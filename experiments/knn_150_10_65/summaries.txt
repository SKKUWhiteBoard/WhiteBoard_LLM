==================== [1/100] ====================
Summary:
CASEY RODRIGUEZ: Theorems: A sequence converges to x if and only if the limit as n goes to infinity of the absolute value of xn minus x goes to 0. We're going to prove it by introducing limsup and liminf, because these are also two important objects that arise in analysis. So let's get to the definition of these guys. Theorem: Every bounded sequence does have a convergent subsequence. And we have to throw in this bounded part, because there are sequences that don't have any convergence. Binomial theorem says that [? for all ?] [? N, the ?] natural numbers, x, y in R, x plus y raised to the n is equal to the sum from k equals 0 to n of n choose k. We'll use a different inequality to get a little bit of a different limit. We want to prove limit as n goes to infinity of p to the 1 over n equals 1. And we'll use 3 for number that we'll show converges to 0. we've only defined what it means to take a real number to an integer power, but-- and n-th roots. So using that, we can then define how totake a real numbers to a rational power. You don't need the exponential and the logarithm to make sense of a positive real number. So in summary, the liminf of 1/n, which is the inf-- the limit as n goes to infinity of the inf of this set, is equal to 0. converges to 0, which implies that. 1/n is an element of the set that's bigger than or equal to everything else in the set. If you have a set that contains an element which is an upper bound for the set, then that has to be the supremum. So this supremum is equal to 1/N. So as n goes to infinity, the limit of the supremUM here, which equals the limit as n go to infinity of1/n, equals 0. The limit as n goes to infinity of 2 over n minus 1 is 0. Square root of that also converges to 0. So this sequence, which is bounded, has a convergent subsequence. Now, not other and are equal to the limit of the original sequence. But that's not just a one-way street. It's a two-ways street. If the limsup equals the liminf, then the original. sequence converges. And we saw here that in-- on display, that the lim Sup and the lim Inf don't equal each other. And that completes the proof. all sequences have convergent subsequences. For example, if you look at the sequence x sub n equals n, that will not have any convergent. subsequences, because any subsequence of that has to be unbounded. And we know that a convergent sequence is bounded. So this is for this direction-- namely, that convergence implies the limsup equals the liminf. And that will give us the proof of this Bolzano-Weierstrass theorem. And I think that's all of the remarks I want to say. are going to be certain limits, so it's not clear that they exist at all to begin with. But we'll show that they always do exist. We define limsup x sub n. And the liminf is similar, except it's now with infs. So I want to show that there's something converging to the limit as n goes to infinity of the a sub n's. And we show that by proving that these two sequences have these monotonicity. properties. on hold just for a second, and now prove a very simple theorem that, if A and B are subsets of real numbers, A, B, both not equal to the empty set, and is A subset of B-- so also need to be bounded. So if we take two non-empty subsets, such that [INAUDIBLE] bounded and A is a subset of A, and the conclusion is that the inf of B is less than or equal to A, then that increases the sup. So the sup of a smaller set is smaller than the Sup of the bigger set. And that inequality reverses for infs. The inf of the smaller. set is bigger than orequal to the Inf of the larger set. the n. Then, if I am looking at this set x sub n, n bigger than or equal to k, and writing-- instead of x sub N, let me just write what it is, minus 1 to the n. So what is this set? This is just a set consisting of two elements, 1 and minus 1. And therefore, the sup of this set is just 1. Now if I change all these sups to infs, then the inf of thisSet is going to be the inf, which is just minus 1, and therefore, we also get-- OK? So the limsup is 1. The liminf is minus 1 for this set. set, is just the limit as n goes to infinity of 0 equals 0, all right. So the limsup of 1/n equals 0. The liminf equals 0 as well. Now, what's the difference between these two sequences? What's the property that one holds, but the other doesn't? This is a convergent sequence and this one is not. And we'll see this is a general fact, that if we have a converge sequence, then the lim Sup and the liminf equal each. to these two numbers, which may or may not be the same. OK? So the reason this is so powerful and so strong is that it-- to get your hands on something, it doesn't require you to show something as strong as showing there is a sequence converging to that. So first, take a sequence approaching-- so that the values are approaching that-- the outputs are approaching the maximum. Then you could pass to a subsequence, which actually does converge to something by this theorem, and proceed in that way. to a sub n sub k minus 1 plus 1. OK? And here, if you like, we didn't define what n sub 0 is, so-- so really, we're only interested in the n sub 1, n sub 2. But for the sake of this whole thing making sense for all integers k, with n sub0 being defined to be 0-- OK? That's just the first case that we dealt with. So we obtain this subsequence x sub nSub k that's sandwiched in between this subsequences of a's and minus 1 over k.

ROUGE-1: 23.70, ROUGE-2: 22.69, ROUGE-L: 19.59
BERTScore: 69.99

==============================================
==================== [2/100] ====================
Summary:
The configuration of a particle is given by, or described by, a wave function. In 3D, the wave function would be a function of all three positions x, y and z. All reasonable, or non stupid, functions psi of x are equally reasonable as wave functions. There's no primacy in wave functions or in states. Some wave functions are more equal than others. Probability is basically 0. It's nice and smooth. It converges to 0 infinity. The superposition principle. If there are two possible configurations the system can be in, which in quantum mechanics means two different wave functions that could describe the system. In a quantum mechanical system, something with short wavelength is something that carries large momentum. This is a state which is equal to all others in the sense that it's a perfectly reasonable wave function, but it's more equal because it has a simple interpretation. The probability that I measure the momentum to be h is h is 0, correct? But I can always consider a superposition which is 1/10 over 2 eikikx plus the momentum. And these are going to turn out to contain all information about the probability of the system. Fourier: Any function can be expressed as a superposition of wave functions with a definite momentum. We know that given a wave, a plane wave e to the iks, how do I get h bar k out of it? If I take a state, x0, which is equal to the state of x, this is a state that has definite momentum h bar. And I can write this as p e to ikx. So first check the units off, what are the units of h bar? Here's the super easy to remember. a definite position? AUDIENCE: Delta. PROFESSOR: A delta function, exactly. So I claim that any function psi of x can be expanded a sum over all states with a definite position. So is this true? Is it true that I can take any function and expand it in a superposition of delta functions? Absolutely. In 2D, this is a perfectly good vector, right? Now here's a to the derivative being continuous. That's the optional problem 8 on problem set 2. question I want to ask you. Is that a superposition? Yeah. I mean every vector can be written as the sum of other vectors, right? And it can be done in an infinite number of ways. So in particular we often find it useful to pick a basis and say look, I know what I mean by the vector y, y hat. It's a unit vector in this direction. And now I can ask, given that these are my natural guys, the guys I. want to attend to, is this a. superposition of x and y? Or is it just x or y? Well, that's asuperposition. Whereas x hat itself is not. Professor: Which particle has a larger momentum? Think about it, but don't say it out loud. If it has higher momentum, what do you just intuitively expect to know about its energy? It's probably higher. So this is an important little lesson that you probably all know from optics and maybe from core mechanics. Shorter wavelength thing, higher energy. Usually higher energy as well. That's a very good question. The question here is look, there's two different things you can be talking about. of length l, 0 outside of that region. Which is closer to true? f has a single well defined wavelength for the most part. Or f is made up of a wide range of wavelengths? Think it to yourself. Ponder that one for a minute. OK, so learning happens. That was an empirical test. So does anyone want to defend this view? Sure, bring it. It's closer totrue. This doesn't have to be exact. And now and I want you to vote. of sines and cosines, do you ever get a discontinuity? No. Do you get something whose derivative is discontinuous? No, you don't. So it's going to take an infinite number of sine and cosine to reproduce that little kink at the edge. So how would you ever reproduce a thing with a discontinued thing? Well, you'd need some infinite sum of sines & cosines where there's some technicality about the infinite limit being singular. Quantum mechanics professor: Average age is the sum over all possible ages of the number of people with that age times the age divided by the total number. Professor: The average need not be an observable value. Average value of the square of ages is, well, I'm going to do exactly the same thing. It's just a squared, right? 14 squared, 15 squared, 16 square, 16 squares, 16 squared. And so that's all I've written here. But notice that I can write this in a nice way. That's just the probability that any given person has a particular age. Is a squared equal to the expected value of a squared? PROFESSOR: Right, in general no, not necessarily. For example, suppose we have a Gaussian centered at the origin. How do you pronounce the last notation that you wrote? Good, that's a good question. Now we have pure numbers and pure probabilities per unit of space and time. We can either write the probability of writing it as the standard deviation of the distribution, or we can write it in this notation. The uncertainty is the square root of the standard deviations squared. Different probability distributions are going to give me different delta a's. The average value of some x, given a probability distribution on x where x is a continuous variable, is going to be equal to the integral. If you see me dropping an exponent or a factor of 2, please, please tell me. All of that is just straight up classical probability theory. And I just want to write this in the notation of quantum mechanics. And this has a different flavor, and we'll see what that is later. couple of ways to improve this notation. One way is to write the expected value of x in the state psi, so you write psi as a subscript. Another notation that will come back-- you'll see why this is a useful notation later in the semester-- is this notation, psi. And we will give meaning to this notation later, but I just want to alert you that it's used throughout books, and it means the same thing as what we're talking about. OK? Yeah? of measuring momentum p. And that's the value p. We're summing over all p's. This is the probability, and that's actually p. So the Fourier transform is a function of the momentum in the same way that the wave function is afunction of the position, right? It's norm squared defines the probability. And then the p on the right is this p, because we're computing the expected value of p, or the averagevalue of p. That make sense? Cool. In quantum mechanics, p is represented by an operator, it's represented by the specific operator h bar upon I derivative with respect to x. And this is a declaration. OK? It is simply a fact. And when they say it's a fact, I'm simply going to declare it. And I've given you some reason to believe that this isn't totally insane. But I didn't derive anything. I gave you no derivation whatsoever of the relationship between d dx and the momentum. Noether's theorem underlies an enormous amount of classical mechanics, but also of quantum mechanics. To every symmetry is associated a conserved quantity. Noether, a mathematician, did all sorts of beautiful stuff in algebra, looked at the problem and was like I don't even know what it means in classical mechanics. So she went back to classical mechanics and, from first principles, came up with a good definition of momentum, which turns out to underlie the modern idea of conserved quantities and symmetries. The potential breaks that translational symmetry. Translations in x are generated by derivative with respect to x. But through Noether's theorem translations are associated to conservation of momentum. So you shouldn't be so worried about the potential breaking the symmetry. The potential breaks the translationally invariant symmetry of the [? system ?] The potential is not translationally invariant in the same way as the classical particle. It breaks the translational symmetry of the potential. It breaches the translational symmetry of the potential. Translate by L takes a function of x and translates it by L, but leaves it otherwise identical. F of x minus L can be written as a Taylor expansion around the point x. The point -- if you have some function like this, and it has a peak at 0, then after the translation, the peak is when x is equal to L. But this is a series that you should recognize, a particular Taylor series for a particular function. It's a Taylor Expansion for the AUDIENCE: Exponential. eyes, of foot, of eye, of brow. I see the antique pens do but express the beauty that you master now. So are all their praises but prophecies of this, our time. All you prefiguring. But though they had but diving eyes, they had not skill enough you're worth to sing. Your praise shall still find room, even in the eyes of all posterity. So no judgment arise till you yourself judgment arise. You live in this and dwell in lover's eyes.

ROUGE-1: 24.44, ROUGE-2: 22.80, ROUGE-L: 18.92
BERTScore: 62.30

==============================================
==================== [3/100] ====================
Summary:
the Lord Byron George Gordon Lord Byron very interesting very outgoing very flamboyant personality he stood out. The literary celebrity down at the bottom had spoken that he was a you know kind of born into not necessarily nobility like a monarch or anything but he was born into his title he didn't do anything to achieve it okay Hinda and he was an individual that you know probably was a bit of a celebrity to some degree he had the money he was wanting it he it said that he got in trouble because he had a lot of love affairs going on. in dark blue ocean roll ten thousand fleets sweep over the in vain men marks the earth with ruin his control stops with the shore upon the watery plain the wrecks are all thy deed nor doth remain a shadow of man's ravage save his own. I have loved the ocean and my joy of youthful sports was on thy breasts to be born like thy bubbles onward from a boy I want and with thy breakers they to me were a delight. I was as it were a child of thee and trusted to thy billows far and near and laid my hand upon thy mane. essence and a storm can just mess that thing all up if there's a hurricane coming that ship heads the other way tries to get around it okay because it would turn it into like a toy in a bathtub. The 100-year anniversary is coming up here next month of the sinking maybe not the there rerelease in the movie and I'm accessed off 3ds so that's just for you a little bit of extra stuff for today but the apostrophe on page 845 um.

ROUGE-1: 22.25, ROUGE-2: 21.82, ROUGE-L: 22.25
BERTScore: 60.63

==============================================
==================== [4/100] ====================
Summary:
When you do not have unlimited amount of everything you will have to make a choice, what to produce? In what quantity you should produce a particular good. Allocation is nothing but assignment, allotment, share, but in economics we are more technical about this particular term allocation. So, allocation here means solving these three fundamental or basic questions of economics, and the first question is what to production? The second is how to produce and the third is for whom to produce. Depending on who makes these decisions we have different form of economy, who makes this decisions. but after 1991 after economic liberalization we are slowly moving towards market based economy, but we are not yet there. One concept that I would like to emphasize here is laissez faire, this is a French word it means leaving it alone. This is an extreme version of market economy where private parties are absolutely free from government interaction, intervention such as tariffs, tax, regulations. So, individuals transact without any interference of the government. But still I would say that we have mixed economy.

ROUGE-1: 32.53, ROUGE-2: 30.44, ROUGE-L: 30.38
BERTScore: 64.94

==============================================
==================== [5/100] ====================
Summary:
Albert Meyer: Random variables are an absolutely fundamental concept in probability theory. He says in a game called the bigger number game, two teams try to pick the larger number. Meyerer: You can only win with probability 50/50, and that's true no matter what Z you take. "You pick Z in a way that can't be predicted or made use of by Team 1," he says. "When you see a small number, you'll stick. But what's going to be large and small is going to vary depending on what you play" naive thing to do would be to choose Z to be in the middle of the interval from 0 to 7. If you're Z was 3, they would always choose their numbers to be, say, 0 and 1. And that way, when you were switching, your Z would tell you that you had a small number, you should switch to the other one. And you'd only have a 50/50 chance of winning. So if you fixed that value of Z, Team 2 has a way of ensuring that you have no advantage. The most interesting case is the middle case. That is, when your Z, which was chosen at random, happens to fall in the interval between low and high. And then in that case, your Z is really guiding you correctly on what to do. If you turn over the low card, then it's going to look low because it's less than or equal to Z so you'll switch to the high card and win. But if you turn the high cards over, it'll be greater than Z so it'll look high and you'll know to stick with it.

ROUGE-1: 22.64, ROUGE-2: 21.25, ROUGE-L: 18.98
BERTScore: 61.72

==============================================
==================== [6/100] ====================
Summary:
foreign and I should turn off the zoom background blur Ry options like this oh it does show up uh yeah there's like speaker notes on your screen but there's be careful because I accidentally just put something else in the first longer okay. I apologize that was kind of a rough introduction that was uh that was me making a couple of last minute edits that probably hurt more than they helped so I want to just apologize. I just think I was just too ready to go I usually uh yeah as our slides were they and put which is the product describing to replace the names. review um convolutions and and the architecture of a CNN to make this more clear and put it put it into perspective how it relates to just standard um dense neural networks I think it's fine. When you have images and a CNN you can simply just do convolutions instead of your normal matrix multiplication. If you want to decrease the size of this volume because it can get quite unwieldy you have pooling layers. There's only five convolutional layers and the next slide should have an updated drawing that's hopefully a lot easier to understand. and we also have a bias term that gets added to the output of moving each window on each location of our input we refer to it as a volume simply because it sort of looks like a cube. If you have a whole bunch of different filters you're going to end up stacking up all of the different outputs from taking each one of your different filters and running it over your input. The big the big thing I really want to get across is that you treat it just like another layer um you're just gonna stack a bunch of them. Resnets are the only architecture that you really need to take away from here which is going to be resnet we'll get to that. The motivation behind most of these architectures start off with a bunch of data a full dimension image with all the features that you have in whatever your input is. Most of these architecture one in their respective year um vgg in 2014 I think Inception net in 2015 a lot of really cool advancements. We want to be able to have our model learn higher order feature maps and by that I mean low level features and edges. like color spaces um how edges lead to other forms and things like that yeah this is basically about what like what I was talking about um there's a lot of space that we want to have and we want  to uh go through this architecture um so yeah this this is hopefully a lot easier to understand than the previous drawing and it's saying the same thing. So you have um a this is a 5x5 image with three channels um similar to what Jake is drawing here actually um this is your original image in like RGB or something it's passed in through to a convolutional layer it's Max pooled. soft Max will scale more logarithmically um and it'll give you a final like probability map um are there any questions on Alex now actually before uh we move on yeah what's upYeah so uh if you don't specify a certain type of padding valid padding is going to be applied to make sure that as you're sliding your kernel across an image uh you're left with the same dimension is there anything you want to add Jake or no that's I I should have mentioned adding two yeah. The motivation behind this is that again we want to learn low level features in earlier stages of the classifier. Vanishing gradients is a common problem as you add a bunch of layers stacked together and that the learning signal or the gradient computation becomes extremely weak the model struggles to learn. The solution is make it easier to learn at least the identity so keep information from previous stages into future computation that's the key motivation behind residuals so yeah this is what I was talking about earlier um which is some identity that you want to keep in mind. Problems that come with that that Inception and resonance um try to fix and that is adding residuals yeah uh what do you mean by branched uh yeah the classifier is at the end uh by this uh again there's there's a drawing for it but essentially you have a multi-headed yeah so you're you're taking whatever your uh input is in a certain step and applying it to the output of another step. This maintains kind of a about it's a backwards way of maintaining a residual value. residual um that's being computed. A 34 layer residual will have jumps between every two. This is a process known as bottlenecking versus if it was after every layer. Adding residuals will increase the time to convergence because you're increasing the number of backwards considering computations that you have so if you're if your bottleneck isn't as big your time to converge will be smaller. It matches Inception of D3 accuracy just by using depth and point wise convolutions and combining. event like a low dimensional projection s yeah yeah this is like probably like really important thing for today but like this idea of like why it'd be important to sort of be able to learn the identity like it's sort of a weird thing um are there any questions or comments or concerns about that yes yeah for sure right so like if you have a dent snail Network like like let's just ignore convolutions right now if you like a dense neural network trivially you have the identity Matrix which is just ones along the diagonal and it spits out the exact same thing that it took in. step in your network if you just multiply the partial derivatives of all of those steps you can find the the derivative of your loss with respect to a given parameter just with the chain rule. If all of these different things that you're multiplying are even a little bit smaller than one immediately like at a certain point at acertain number of multiplications your partial derivative uh your chain rule that you've gotten as a result of many many multiplications just gets sent straight to zero. That's just not helpful. shouldn't produce accuracy when in reality like if this is scaled it can yeah awesome group wise uh your Dimensions so yeah this is a really good point um so if your layer is a convolution um the dimension can change which is why often this is result like kind of viewed as f of x plus W of X where W is a transformation that you do on X two to make it the same Dimension exactly. To make sure your Matrix addition stays the same like like basically you have like the X number of layers and things like during the state. at the end it's very easy to overfit to the data that I have provided uh for training um so this kind of prevents that. This comes more into play also in Mobile nuts and uh the efficient that's that will be talked about as well. There any questions on the previous kind of topics all right all right that was kind of the meat of this lecture uh but mobile nuts are very cool in that you're you're using depth wise convolutions and point-wise convolutions to reduce the number of computations. connected layer passing it through relu and with a fully connected layer you can also expand this back to whatever Dimension you originally had um rescaling according to the layer output is also not as computationally intensive. I think the slides are pretty good and compressed in a very visual way uh the remainder of the piano more impressive art it's actually the other way. I I hope the main takeaways are that you'll like understand those rules and that you you see that we've like adding all of these different sort of like tools to your tool belt now. so these are some things that these models wanted to optimize over time accuracy performance and model size um model size is something that has a trade-off if you get too big you lose out on other metrics like accuracy. performance is something directly corresponds to depthwise convolutions and mobile nuts for Edge Computing and things like that. You want to drastically reduce the number of computations that you want to do yep that is basically everything for today thank you guys for coming oh and there will also be a quiz.

ROUGE-1: 30.77, ROUGE-2: 29.89, ROUGE-L: 27.30
BERTScore: 69.05

==============================================
==================== [7/100] ====================
Summary:
The principles of baking show us how to make bread with and without a bereavement. We'll make three breads one with the brief mint and two without one of the breads without the bereavement will be bulk fermented in the fridge for 24 hours. The third one will be made with a brief mint which will be left for 12 hours to ferment then it'll be mixed into the main dough. The breads will then take the same amount of time to make but why would you add the extra step of making a briefment when cold fermenting?

ROUGE-1: 13.99, ROUGE-2: 12.17, ROUGE-L: 12.58
BERTScore: 67.22

==============================================
==================== [8/100] ====================
Summary:
Bayard Rustin was the chief organizer of the 1963 March on Washington for Jobs and Freedom. Rustin grew up in a Quaker household, and began peacefully protesting racial segregation in high school. He was jailed in 1944 as a conscientious objector to World War II. In the 1980s, he publicly came out as gay, and was instrumental in drawing attention to the AIDS crisis until his death in 1987. In 2013, fifty years after the March On Washington, President Barack Obama posthumously awarded him the Presidential Medal of Freedom. are or who we love.” “I’m so proud of you,” she says. “You’re so beautiful.’ ““I love you, too,’ I say. I love you so much.“” I’ll always love you. ”I will never forget you. ””“We’ve been through a lot. We’d rather be here than there. ’’”

ROUGE-1: 27.06, ROUGE-2: 22.61, ROUGE-L: 22.81
BERTScore: 56.37

==============================================
==================== [9/100] ====================
Summary:
Deuteronomy is the first book in the Bible to mention the election of Israel as the chosen one. In Deuteronomy, Israel is a holy people, separated from the common or the ordinary. Separation entails separation from alien peoples and practices that are inconsistent with the worship of God. The most salient feature of the Deuteronomistic School is the conviction that Israel's residence in the land of Jerusalem is going to be Jerusalem, says Professor Hayes. In Genesis, there is no mention of a king or monarchy. In Numbers, the law of Moses says that when you have a king, you have to obey him. Moses warns the Israelites: Don't be tempted to say to yourselves, "My own power and the might of my own hand have won this wealth for me" He emphasizes, it is only because the wickedness of the Canaanites is so great that the Lord has to drive them from his land, and now he is giving you a chance. Don't fail him or he will a few decisive battles under the military leadership of Joshua. And the disunited Canaanites put up little or no resistance: they're paralyzed by a fear that is sent by God. In Deuteronomy 32:10, the image is that of an eagle that bears its young on its wings. It almost seems to play on the idea that when teaching its young to fly, the eagle will push them out of the nest, swoop under them, bear them up for awhile over and over until they get the idea. Some have suggested that this is quite purposeful. It points to an exilic date for the work's final composition, that is to say when it was finally redacted, the redactors were in exile. We have been discussing the Torah, or Pentateuch, and now we are moving on to the section of the Bible that is referred to as the Prophets. This section of redaction of these books, would put the materials together by inserting verses and speeches that would frame the older sources and link them together, give them some sort of common uniting thread. This is an important point for us today, as we begin to go through to the end of 2 Kings. It is making an argument and attempting to communicate the meaning and significance of the events of that time. There is a great deal of ideological baggage that is involved in the dating of the sources. Anti-priest, anti-cult sentiment is apparent in the history of biblical scholarship. P espouses a communal ethic, and post-exilic priests are going to turn to an individual ethic. Many sections of P do not seem to assume a central sanctuary. The idea of the central sanctuary really took hold in 622, Josiah's reform: that the central reform was Josiah and not Israel. The Bible is divided into two parts we refer to as the "Former Prophets" and then the "Latter Prophets." The Former Prophets include the books of Joshua, Judges, 1 and 2 Samuel. They read as a historical narrative from the conquest of Canaan to the destruction of the state by the Babylonians in 587-586 BCE. The Latter Prophets is a collection of books, each of which bears the name of the individual whose prophecies it purports to contain. In the first half of the book of Joshua, alongside the call for the destruction of all Canaanites, we find interesting tales of alliances and incorporation of various Canaanite groups. Yehezkel Kaufmann uses the term "historiosophy" which I have written up here, historiosophy, to describe this material. It's seeking to ascertain the meaning of events to draw larger philosophical, ideological conclusions from the events of history. We're going to return to this when we reach the conclusion of the Deuteronomistic history in 2 Kings. In the past 4000 years more wars have been fought for the possession of the tiny strip of land known as Canaan than for almost any other area in the world. In times of peace it would bring prosperity, but, of course, in times of war the land was perpetually invaded as armies would crisscross the land. The structure of Joshua is really somewhat simple. We can really divide it into two major parts. The first 12 chapters form a unit that conveys the invasion and conquest. The story is really a composite of two accounts that have been woven together into a single narrative. in Israel, it rises about 10,000 feet above sea level. As you move from the central area over to Jerusalem, that area is dramatically lower. The Dead Sea is nearly 1300 feet below sea level, which is the lowest point on the earth's land surface. The area around the Sea is basically semi-desert. We call this the wilderness, the wilderness of Judea between Jerusalem and the Dead Sea. It is a very desolate area. And tradition identifies this as the site of Sodom and Gomorrah. According to the Book of Joshua, Israel's tribal structure assumed its classical form at this time. This is a very neat picture of the rapid conquest of Canaan, but it's at odds with statements elsewhere in Joshua and in the book of Judges. Excavations at Jericho and Ai indicate that both of these towns were laid waste at least 200 years before the probable time of Joshua. The Israelites weren't even occupied in this period, the late Bronze Age, beginning of the Iron Age. of the places from which the Canaanites were not expelled. Also archaeological evidence contradicts the picture in Joshua. In the Ancient Near East, destroyed cities tended to be leveled, and then a new city would just be built on top of the ruins. Archaeologists have found really no evidence of extensive conquest and destruction in thirteenth and twelfth century archaeological layers. Some of the sites by the hosts of the Lord? Some of them were destroyed by the forces of God, not the sword or the bow. to have been peasant farmers, like other Canaanites. One interesting difference is the absence of any pig bones, which is kind of interesting. This suggests that these settlements were established peacefully, not by a group coming in and conquering. Maybe they emerged from within, rather than being established by peoples immigrating from without. The revolt model proposes that Israel began really as a social revolution within Canaan. The Yahweh cult may have been introduced by people escaping slavery from Egypt. Most scholars see the Exodus story as evidence for the presence of some escaped slaves among this community. perhaps not exclusively, and adopted the national story of the Exodus as its own at some point. The Hebrew tribes, themselves, were likely still in the process of formation. But the tribal structure of Israelite society that would develop would be strengthened by the natural division of the land into these separate geographical areas. And these local tribes probably did assimilate elements of the local population. We've really seen already the ethnic mix of various elements reflected in religious imagery and institutions, says David Perry, professor at the University of California, Los Angeles. century BCE, written by King Mesha of Moab Moab is to the southeast of the Dead Sea. In the inscription he writes, he boasts: "And the god Chemosh said to me, go, take Nebo from Israel" It is likely that such claims are hyperbolic in Moab, and it is likely they were hyperbols in Israel. But that does not lessen the shock value for a modern reader, says Andrew Keen. "War in our time is no less savage and no less brutal," he says.

ROUGE-1: 25.38, ROUGE-2: 23.49, ROUGE-L: 20.50
BERTScore: 60.43

==============================================
==================== [10/100] ====================
Summary:
HMG-CoA synthase is a key enzyme in central metabolism responsible for making the five carbon building blocks from which all sterols, such as cholesterol and steroid hormones are made. HMG- coa synthase catalyzes the following reaction: It takes acetyl-Coa, which we're going to encounter a lot in the central metabolism, and combines it with acetoacetyl- CoA, another thioester. In this process, the one molecule of CoA is lost, and we get the product hydroxymethylglutaryl CoA. The reaction will start by forming this thioester between the acetoacetyl-CoA carbons and the cysteine in the active site of the enzyme. The reaction first involves activation of the cy Steine, is deprotonated, and then cysteines can attack the thioesters and form a tetrahedral intermediate. The next step is to form a carbon-carbon bond between this methyl group here and this carbon of the carbonyl of the other substrate. Then the depotonate the methyl from the acetyl, moiety here, and generate an enolate. enolate is going to attack this carbonyl. Once the enolate is formed, it's going to attacks this carbonium. This oxygen is then going to start developing a negative charge, which needs to be compensated by a general acid. And it looks like this histidine 233 is close enough to donate a proton and generate a hydroxyl group here. All right. So let's try to write a mechanism based on what we just said. Here is the thioester between cysteine 111 and acetyl. And let's show the general base. It'sGoing to be glutamate 79. energy contributes to this reaction. Both of the substrates need to bind to the enzyme. In order to do that, they need to be desolvated, that is to remove all the water molecules that surround them. The binding energy also manifests when we're stabilizing, for example, the transition state of the reaction relative to the binding of the substrate. So if, for. example, for our tetrahedral intermediate that will form here, if that transition state or the tetra.hedral intermediate is stabilized more than the substrate, then the reaction is accelerated and. proceeds towards that pass. a same kind of chemistry in multiple steps over and over again. The advantage of having a coenzyme A thioester is that it may provide some additional binding energy. A much smaller thiol like the one we just saw will not have that kind of interaction available. The resonance that we observe in oxygen esters is almost completely absent in thioesters. And this fact makes the thiaesters less stable and therefore, more reactive. This sums up Problem 2 of Problem Set 4. I hope you now have a much better feel of how we can use crystal structured data to propose a reasonable mechanism for enzyme catalyzed reactions. experimental work and evidence. The study of the effects of drugs on the human body was conducted in the 1970s and 1980s. The results of the study were published in a book called "The Effects of Drugs on the Human Body" The book was published by Oxford University Press in 1989. It was the first of its kind to be published in the U.S. and is published by Simon & Schuster, a division of Simon and Schuster Inc. in New York. The book is available in hardback and paperback.

ROUGE-1: 25.53, ROUGE-2: 22.54, ROUGE-L: 22.58
BERTScore: 64.06

==============================================
==================== [11/100] ====================
Summary:
Alberto Riva: The most important resources for finding and using biomedical information, especially information connected with the study of the human genome. Most of them will be websites, where you can find information, and to talk about how this information is stored and represented. You're going to see a long list of references to sites, websites, with URLs. Don't worry if you can't remember all of them because, of course, I'm going to distribute the slides and will be easier to just look them up. So-called central dogma of molecular biology, as you know, almost all our cells contain DNA in their nucleus. For DNA, you just look at the sequence and you know essentially all that there is to know about DNA. For proteins, you cannot look at. the subsequence of a protein and understand just by looking at it what. the protein is going to do. So how is all this information represented? What are the different ways that we can store and describe this. information? Where does it come from, where is it stored, how do we find retrieve and use it? The genotype is digital, because each base pair in our DNA can be exactly represented using one of four symbols, A, P, G, C. On the other hand, the phenotype is, say, analog. Because most phenotypes are qualitative in nature. They cannot be measured exactly or precise, they cannot even be defined precisely in most cases. You always have to take into account the effect of environmental factors that, again, are very hard to describe in a quantitative way. And we'll talk later about why genotyping is important, what kind of information you can get from that. was discovered, more or less, the same years. Nobody had any idea that there was any connection between these two things, between the DNA and inherited traits. It took over 80 years for this concept to be proven. And finally, the Human Genome Project, that was officially declared a success last year, brought us to the point where we now know the exact base pair sequence of our genome. So we will see as we go forward that we're going to encounter very different forms of data. Your DNA sequence is slightly different from the sequence of any other human being. One of these polymorphisms, every 1,000 bases, adds up to a very large number of differences. If you have enough sequences from the same organism, you can try assembling them, putting them together, and trying to reconstruct the entire genome. And this is what was done to assemble the human genome, for example, and all the other genomes that are being sequenced. And if you find that organism A shares a gene with organism B, and B shares it with C, then you've built what they call a triplet. The process of transcribing the DNA sequences into RNA is, of course, at the basis of expression analysis. Differentiable analysis, clustering, and so on, these are all the usual things that can be done using microarrays. Other resources are found in different PGA projects, PGA are programs for genomic applications managed by the NIH. The Stanford microarray database is a repository of all the-- of a large number of micro experiments performed at Stanford, and a portion of these are public. NCI60 is a famous data set that includes gene expression profiles for 60 human cancer cell lines. GenBank is a database of all the sequences in GenBank with all the orthologs. There are tracks that tell you the location of SNPs, and so on. UniGene is an automated system that looks at all the GenBank sequences and tries to build these clusters. LocusLink is a curated directory, means that there are people who spend their days going through gene records and adding information, checking it, correcting it. This one is partly curated, partly calculated, partly using algorithms to do that. LocusLink is a repository of information about genes, and it collects everything that is known about the genes. It gives you information about the sequence, itself, about the functions of the gene, links to other databases, and phenotypes that are known to be associated with that gene. This is all information that you can find in LocusLink. And the most important thing-- and this is very important-- is to establish an accurate connection between the defining sequence for locus and other descriptors. It basically means, you have a stretch of DNA, you know that there is a gene, let's collect everything. phenotype. What I mean is that, for example, if you have a SNP in the coding. subsequence of a protein, you're going to get a protein that has an abnormal sequence. And that can be a change that doesn't cause any consequence, or it might be a very dramatic change. There are many diseases that are due to the fact that you have SNPs that truncate proteins. They can be used as evolutionary markers, because SNPs arise randomly during the replication, and then they are transmitted from one generation to the next. SNPs get-- how the frequency of the SNP changes in a population. Most SNPs are deleterious. But in some cases, the SNP can also provide an advantage, if it generates something that was not present before, and that works better than the original. If a SNP is neutral, then there is no selective pressure, and it will either go away by chance, or will stay at a certain basic level of frequency. So if you have a SNP that introduces a change is beneficial, then you will-- given enough time-- you will see that thefrequency of the SNPs increases. information mainly from dbSNP, from Golden Path, from TSC, from Alfred, from HGbase. It tries to put everything together in a unified view that allows you to look at the gene. And it provides a way of exporting this data in different formats to make it easier to process later. And I just want to show you one slide from SNPper, but this is a window that describes-- that tells you information about the particular SNP. This is a [? SNP ?] identifier. the gene it belongs to, notch 4. And here, it tells you that this gene is in the coding sequence of the gene, and it actually causes an amino acid change at position 319. It affects protein domains-- this is the list of protein domains that are affected by the SNP. And finally under here, I wanted to show you, this data comes from TSC, andIt's a frequency information data. So they sampled 41 individuals from a population of African-Americans. And then they looked at a different population-- these are Caucasian, I think-- and they found very different alleles frequencies. these domains are overlapping. And this information comes from Swiss [INAUDIBLE] database of protein information. And so you see for example, this first domain covers almost all of the protein. AUDIENCE: So six would be the maximum number? ALBERTO RIVA: No, no, it's just that these domains can be overlapping, just because the Swiss people, they annotate the protein sequence saying OK, from here to here, we know that this happens. But-- well, well, there are some domains that cover the entire protein. LocusLink provides a nomenclature of genes. No LocusLink assigns a name to each gene, and if you stick to that name, then you're sure that everybody knows what you're talking about. In many, many cases, genes have lots of different names, even if it's the same gene, people have been calling them with different names. It's a mess when you try to out which gene is which. So it gives a name, it give a number, and you can use these as identifiers to look up your gene in other databases. a software system for the automated annotation of genomes. It's basically means it's a system that discovers genes and tries to find as much information as possible about these genes. It provides information about genes, about proteins, diseases, SNPs, cross-species analysis, microarray data. They claim they receive 30 million searches per month. Gene ontology is something that stays at a slightly higher level, above everything that we've seen so far. It is to build a dynamic controlled vocabulary that can be used to describe biological concepts. is a consequence of the fact that there is a very complex machinery behind it that determines which genes are active or not, and how much, in different conditions. This is actually a system that integrates a lot of different factors that might include the following, in no particular order-- The tissue, we know very well that the set of genes that are expressed in one tissue is very different from the set that is expressed in another tissue. External signals, of course, all response to external stimuli. And it also depends on the expression state of any number of other genes. The first thing you need to do is need to be able to reliably identify which transcription factors bind to a given gene, and where, exactly, in the promoter region of the gene they bind. And transcription factorsbind to locations that are called transcription factor binding sites. So if you know where the binding sites are, you have a first idea of what factors binds to this gene. Most of the information in TransFIC is experimentally validated. So you can actually trust the fact that particular piece of sequence they give you. a view of taxonomy, for example, for biological process. If you are talking about site communication, then response to external stimulus is a subclass of communication. And if you want to talk about the immune response, you can cite this biology term. OK, I think we're out of time. Well, just a conclusion slide that I'll just let you read, because I think it's just repeating what we're saying so far that we are drowning in data and converting this data into knowledge is not easy.

ROUGE-1: 26.75, ROUGE-2: 25.79, ROUGE-L: 22.45
BERTScore: 68.56

==============================================
==================== [12/100] ====================
Summary:
This week in week three, we're actually going to have some human language, and so this lecture has no partial derivative signs in it. The idea of phrase structure is to say that sentences are built out of units that progressively nest. And so if I wanna capture, um, this talking to a cat here, well, that now means I've got a verb, because words like talk and walk are verbs. And then talk to the cat, it seems like after that, it could become a prepositional phrase. So I could write another rule saying that a verb phrase goes a verb followed by a phrase. what you learned about neural networks last week and the content of today, and jump straight right in to building a neural dependency Parser. Um, the other thing that happens in assignment three is that, we start using a deep learning framework PyTorch. So, um, final projects, we're going to sort of focus on those more in week five, but if it's not bad to be thinking about things you could do, if you're under a custom final project. We have under the sort of office hours page on the website, a listing of the expertise of some of the different TAs. Linguists can see patterns, like the cat, a dog, the dog, a cat, et cetera, and then we're gonna put them into bigger units that we call phrases, like "The cuddly cat by the door", and then you can keep on combining those up into even bigger phrases. So, I've write, um, a phrase structure grammar role, a context-free grammar role of- I can have a noun phrase that goes to a determiner, and a noun. phrase goes to a determiner, and then optionally, you can put in an adjective. And then I poke around a little bit further and I can find examples like the cat in a crate, or a barking dog by the door. And I can see lots of sentences like this. And so I want to put those into my grammar. But at that point, I noticed something special, because look, here are some other things, and these things look a lot like the things I started off with. what you find is that prepositional phrases following the verb in English. But if you go to a different language like Chinese, what you find are the prepositions come before the verb. And so, we could say okay, there are different rules for Chinese, um, and I could start writing a context-free grammar for them. Um,so that's the idea of context- free grammars, and actually, you know, this is the dominant approached linguistic structure that you'll see if you do a linguistics class. In this system of dependencies I'm going to show you, we've got in as kind of, um, a modifier of crate in the large crate. And well, then we have this next bit by the door. And as I'll discuss in a minute, well, what does the by thedoor modifying? It's still modifying the crate, it saying, ''It's the crate" And so, for each word we want to choose what is the dependent of and we want it in such a way that the dependencies form a tree. So, if we sort of said, Bootstrapping, was a dependent of,Um, talk, but then we had things sort of move around, then talk is a dependent that, and so I'm gonna cycle that's bad news. Knowing the right structure of sentences is important to understand the interpretations you're meant to get and the interpretationsyou're not meant toget. So, here, is a newspaper article. Uh, ''San Jose cop kills man with knife''. Um, now, this has two meanings and the two meanings, um, depend on, well, what you decide depends on what, you know, what modifies what? Okay. The second meaning the sentence can have is, that's the man has a knife. what is modifying what? Um, here is another one that's just like that one. Um, scientists count whales from space. So again, this sentence has two possible structures, right? [LAUGHTER] That we have, the scientists are the subject that are counting and the whales are the object. And really, those kind of verb phrases they sort of just like, um, prepositional phrases. Whenever they appear towards the right end of sentences, they can modify things like verbs or nouns. if that's not what you want, um, you have to use parentheses or indentation or something like that. Human languages are. this prepositional phrase can go with anything proceeding, and the. hearer is assumed to be smart enough to work out the right one. If we think of something like C or a similar language, it's just deterministically, the else goes with the closest if. If you just get humans to parse sentences and say, "Well, what is the agreement and what they produced?" you know, maybe you're only getting something like 92 percent. For $27 a share is modifying acquisition, right? [NOISE] So now, we leap right back. Now, is now the acquisition that's being modified? And then finally, we have at its monthly meeting is modifying? [Noise] Approved. Well, the approved, right. It's approved, yeah. Okay. So, yeah there are two possibilities, is, um, the results demonstrated that KaiC interacts rhythmically with SasA Ka- KaiA and KaiB. to potentially consider an exponential number of possible structures because, I've got this situation where for the first prepositional phrase, there were two places that could have modified. And so, if you get into this sort of combinatorics stuff the number of analyses you get when you get multiple prepositions is the sequence called the Catalan numbers. Ah, but that's still an exponential series. And it's sort of one that turns up in a lot of studies of the human brain. "We can represent by dependencies, um, these two different structures. That is either that there's somebody who's a shuttle veteran and a long time NASA executive, and their name is Fred Gregory, and that they've been appointed to the board. Or we can say, well, we're doing appointment of a veteran and the longtime NASA exec, Fred Gregory. And so, again, we can start to indicate the structure of that using our dependency. And we're simply going to count up how many of them are correct, treating each of them individually" can kind of think of these two things as sort of patterns and dependencies that we could look for to find examples of, um, just protein-protein interactions that appear in biomedical text. Okay. So, so that's the general idea of what we wanna do, and so the total we want to do it with is these Dependency Grammars. And so, I've sort of shown you some Dependency grammars, and I just want us to sort of motivate them a bit more formally and fully. sort of put the words in a line and that makes it. He see, let's see the whole sentence. You draw this sort of loopy arrows above them and the other way is you sort of more represent it as a tree. So, in addition to the arrows commonly what we do is we put a type on each arrow which says what grammatical relations holding them between them. Some of the earliest parsing work in US Computational Linguistics was dependency grammars. But I won't go on about that um more now. In the later parts of the first millennium, there was a ton of work by Arabic grammarians and essentially what they used is also kind of basically a Dependency Grammar. There was this guy Wells in 1947 who first proposed this idea of having these constituents and phrase structure grammars, and where it then became really famous is through the work of Chomsky. So, in modern work, uh, there's this guy Lucie Tesniere. He sort of formalized the kind of version of dependency grammar that I've been showing you. Treebanks are very reusable. Once you have a treebank, it's reusable for all sorts of purposes that lots of people build parsers format. But also other people use it as well like linguists now often used tree banks to find examples of different constructions. So that if right action is taken, the tree bank can be used to train a machine learning model to predict sentences. It can then be used by humans to build better parsers. It was used by Google to develop the Parsey McPa- parseFace model of parsing. for this sentence in context. Most dependencies are fairly short distance. There's a question of what's in between. If there's a semicolon in between, there probably is an a dependency across that. The other issue is sort of how many arguments do things take? So, here we have was completed. If you see the words was completed, you sort of expect that there'll be a subject before of the something was completed and it would be wrong if there wasn't. So, you should be building a machine learning model which will recover that structure. In the 60s, 70s and 80s, the mainstay of parsing was a crummy search for possible parsers. Joakim Nivre came along with a clever idea, he said "Yeah, that's true, um, but hey, I've got a clever ideas, because now it's the 2000s and I know machine learning" So, what he did was build a machine learning classifier and that classifier is gonna tell me whether to shift with left arc or right arc. shift, left arc or right arc. Um, if we also wanted to put labels on the dependencies, and we have our different labels, um, there are then sort of 2R plus actions because she is sort of left arc subject or left arc object or something like that. But anyway, there's a set of actions and so you gonna build a classifier with machine learning somehow which will predict the right action and Joakim Nivre showed the sort of slightly surprising fact that actually you could predict the correct action to take with high accuracy. For each word, we're going to represent it as a word embedding, like we've all what already seen. And in particular, um, we are gonna make use of word vectors and use them as the represent- the starting representations of words in our Parser. So we had parts of speech like, you know, nouns and verbs and adjectives and so on. Well some of those parts ofspeech have already been joined with the dependency of another part of speech. And so you had to even though at the end of the day, it was looking at weights that went into a support vector machine. MaltParser was Joakim Nivre's Parser that I sort of, uh, we started showing before. And they've got, um, a UAS on this data of 89.8. And the reason they loved it is it could parse at 469 sentences a second. There had been other people that have worked out different more complex ways of doing parsing with so-called graph-based dependency parsers. So, what we were able to show is that using the idea of instead using a neural network to make the decisions, we could produce something that was almost as accurate as the very best parsers available. There's still room to do better. I mean, at the unlabeled attachment score, it's actually starting to get pretty good. Um, and so then, what's the residual rate in which, um, people can actually disagree about possible parses? I think that's sort of more around three percent. But there certainly are cases and that includes some of the prepositional phrase attachment ambiguities. Sometimes there are multiple attachments that sort of same clause although it's not really clear which one is right.

ROUGE-1: 27.48, ROUGE-2: 26.17, ROUGE-L: 22.89
BERTScore: 66.17

==============================================
==================== [13/100] ====================
Summary:
okay folks let's get started how's everybody doing pretty good Santa Claus has come to town and you know Santa does with naughty kids Hees them finals he gives them finals. He gives them very evil finals is what he does okay so look out for Santa Claus he's really a really a bad guy uh let's see let's think about a couple of things in terms of announcements and we have a couple surprises today one of which is standing in front of you with all this on and there's more surprises as well. The final exam is in this room on Monday at 9:30 a.m. so uh get here in plenty of time remember to position yourselves with seating as I said before. Heather started go for SES work almost identically Amino a TR tight changing their structure when they S1 then there are elect shs at the AC as theaction next [Applause] theide elak without ACH so one piece is bound to it the get set free has to act next toag where it started waiting for a pepti chain to go and start all again. Glycogen phosphor is U an enzyme that's regulated in several ways. It exists in two forms it exists in the glycogenosphor a form which is the form that has the phosphate on that people describe as the more active and it has the form without the phosphate known as the glycogens phosphor B that is less active. Def phosphorilation involves a kise or phosphatase right the r&t involve allosteric affectors okay all right okay so that's uh where we start now um. protein we've used the term G protein before to refer to proteins that bind to guanosine nucleotides this is not a g protein it's just holding on to phosphoprotein phosphatase phosph protein phosphat enzyme. Epinephrine is bound by the cell surface receptor all right so we've got an active pp1 and we're going to ultimately convert it over here on the other side to an inactive form this is the most active form all right well and again we're looking in muscle what happens is when we have epinephrine being synthesized we see that this protein GM gets phosphorilated. put them in the same tube I've got purified pure glycogen phosphor. I've also got Pure glycogen synthes I put them together in a tube and I want to see what happens I add glucose and something very odd happens. The first person that answers that gets a free metabolic Melodies calendar for 2012 are they different active sites in the the same nope what' you say def phosphorilation come by and get your a calendar okay this Santa Claus is here okay all right. There are a variety of what are called glycogen storage diseases and these diseases arise as a result of efficiency of certain enzymes either in that pathway or related to that pathway okay so um there are several of these voner disease uh this one lacks a glucose 6 phosphatase and affects liver and kidney and what happens to the glycogen. Pomp's disease it's lacking alpha1 14 glucosidase okay and my this is not going here um the one I think is the most interesting is actually mcardle's disease gunite. Liver has a normal enzyme right and so when the muscles start running out of energy what happens oh wow we need some glucose they can't get glucose from breaking down of glycogen but the liver sure as he can do that and the Cory cycle kicks in. Cory cycle is a a backwards thing to the lactate that you talked about but it's the fact that the phosphor in the liver is perfectly normal that makes sense. You don't know how much you might make from your knowledge here okay so knowledge is power knowledge may be money so you might meet some exciting people from having these things too. only the muscle cells are lacking the glycogen phosphor. A person lacking glycogenosphor in their muscle loses ability to do strenuous exercise. The person with McArdle disease sees ADP levels go high and then it falls meaning that the cells are catching up and making ATP now for that last chance at a CD my question to you is what's making this possible it's something we learn during the term something we learned a very important process that allows these people who have this disease to lead a fairly normal life.  heard before okay so um you guys know I like the Beatles I write a lot of stuff to Beatles music and you know the Beatles were like this but I really think they should have been like this. What succeeded The Beatles was this group called The Beggs anybody hear the bgs oh you heard the BS okay Night Fever Night Fever right okay so anyway and what succeeded the bGS was a really important group known as right the back street boy what a group huh and of course what succeeded them we know of course was um yeah low moment in music I think but if you thought you've seen low moments in music You Ain't Seen Nothing Yet. the first song that we think about is the fact that how many people in here are really sick and tired of that Sunshine there we go. The bar is very high very high okay are we going to Belt it out I can't hear you all right all right let's go with it it's called BB Wonderland Heather take us no we're all starting all right ready one Anna two mil Hall dirty and they gety he walks to and not louder [Music] started MP3's got added to my iPod some sometimes were and exams when the Cur turned out I don't think it's so my scores are too low sliding by finally there's examination on December.

ROUGE-1: 28.49, ROUGE-2: 27.69, ROUGE-L: 24.22
BERTScore: 65.38

==============================================
==================== [14/100] ====================
Summary:
JUDY HOYT: Hopefully everybody's recovered from their Thanksgiving feast. What I'm showing up here is the schedule to orient us. This is lecture 22. We'll talk about silicides, device contacts, and I've added in a new material this year on novel gate materials. And then next week we have two class periods scheduled. There'll be four speakers in each. And those will be the oral reports and the student reports given on Tuesday and Thursday. And I just want to remind people, if you're doing an oral report, you're expected to provide handouts. The contact itself is still generally considered part of the frontend because you're contacting the silicon. And then beyond, that everything is in the 6.773, which is the backend technology. The advantages of aluminum has a low resistivity. It's the second lowest of all the metal candidates, copper being the lowest. And it has very good adhesion to silicon and to silicon dioxide. It makes good electrical contact to heavily doped silicon, as long as the silicon is heavilyDoped. In the early days, if this was the drain are going to be silicides, either cobalt silicides or nickel silicide. We'll talk more about that. But the point is they're a contact between a metal and a heavily doped semiconductor, which is silicon. In 2003, they wanted to have about 2 times 10 to the -7th ohm centimeter square. But if you go to 2004, you want to lower it according to ITRS about 1.6 times10 to -7. The modern interconnect material is copper. A few places are still using aluminum. Copper oxidizes very readily, even at room temperature. So you need to protect it, if you're all aware of that. But copper is the material of choice for today's interconnect. And in fact, this was annealed for 10 minutes at 425. So this was all pretty copper colored. You can see all the silicon in aluminum from the substrate has actually gone up into the substrate. And when you etch off the aluminum off, you see all these holes. Aluminum was a convenient and a good material for people to use for many years. The only other metal of consequence that has a lower resistivity is copper, which is shown here. Copper has a much high melting point. Copper, if it gets into silicon, can be a deep level. So we're coming up against the red brick wall here because the device scaling and the need to make the contact size smaller means that we need to drop rho C at this rate. It's just nobody knows yet how to make a contact that has 0.8 times 10 to the -7 ohm centimeter squared for the specific resistivity. is not considered desirable for making a good contact. An ohmic contact on the other side, or a tunneling contact, occurs when you can actually tunnel right through this barrier. And if you do make high doping right near the metal, you can reduce this depletion layer width. And it enables this tunneling. This is a problem as we scale, unless we scale roh sub c. Unless we do something to make better contacts as we Scale devices, just because of this geometric factor. That means the resistance is going up as devices are scaled. it's got a units of resistance times an area-- and divided by the area of the contact. And basically what this tells you is-- and you notice you it's the total resistance of a contact in ohms is inversely proportional to area. So if I make a smaller contact area for a given specific contact resistivity, you're going to have a higher resistance. And this is a problem if I'm scaling devices. If I'm packing more and more devices on the chip, it means I want to make the total area occupied by every device smaller. A Schottky contact is governed by thermionic emission. So it's a thermionic process emitting carriers over this barrier. So if we change that barrier height, we can change rho sub C. But rho C is not really scaling because of the doping electrical solubility limit. So the contact passivity doesn't scale as we shrink technology. And this is a major problem. People are looking for new methods, new materials, whatever, some way of getting the doping up, or a new method of making contact. this number, is about 10 to the minus 7 ohm-- that's ohm centimeter squared for an aluminum and silicon contact. So the question is if I change the doping from 1e19 to 1e20 by a factor of 10, how much does rho C go down? And again, this is an exponential relationship. So you expect a big change. The problem is we know that there are electrical solubility limits. We cannot just arbitrarily keep increasing doping. But you know if you do an implant and you do a certain anneal, you get doping up to a certain value. The metal contact is assumed to be 0.2 microns in this direction in width and 1 micron in the other direction. When the metal, the titanium, is contacting the silicon, you form a metal silicide, say TiSi2. It only forms where it's in contact with silicon, which is right here, this dark region, and on top of the gate. Everywhere else you have still remaining some unreacted metal. So we've created a material, a titanium di silicide or a metal disilicide, that because of the virtue of its chemical structure, it now stands up to the etch. The resistance of the current just to flow through this contact is 100 ohms on the drain side. There is another resistance called the spreading resistance. This is a major contributor-- it can be a major contribution to the total series resistance. It's a very two-dimensional problem. You can't calculate it by hand. But I just want to point out, so couple of days. It is not a homework assignment, but I think it's something you should do-- how to calculate these three resistances. And then we can talk about it and go through it next time. Figure 11-35. There's a reference to it in your textbook, a paper that an article that talks about it in much more detail on how it's actually made. And what you do is you have these dark regions here that are funny shaped are considered to be the N plus region that you want to contact. So that would be called the n plus diffusion, has this particular shape. The dashed lines represent the metal. So this is going to take at least three masks to powder. And these little square regions with the X's going through them are the contacts. 1 and 4 and you measure the voltage drop across that face. And then you just divide V divided by I, whatever you measure. And that's going to be equivalent to rho C divided by the area of the contact, L squared. So this is a very common way to do it. You might say, well, why do you go to the effort of having separate probes? Why don't you just measure the current and the voltage on the same probe points? And the reason you do this is because you don't want to have extra contact resistance, say, of your probes going down. in the 1980s or so, but again, it's still not something that people typically do because you're going to get a higher contact resistance. The way people go today is to form what they call a barrier layer. It forms a barrier between this aluminum and the silicon. The barrier has a very low specific contact resistivity. So it still makes a good contact, but it doesn't allow the aluminum or the silicon to talk to each other. So you cannot get this void formation. You can't get the spiking. his book how he's saying titanium may work as a sacrificial barrier. May or may not exactly work this way, but it's one potential methodology. Ti-silicide also has a tendency to agglomerate when it's very thin at higher annealing temperature. Industry moved a number of years ago primarily from Ti-Silicide, although some people may still use it, to cobalt bisilicide. The beauty of Ti is, again, the titanium eats its way through things, through oxides. I did not have to do any photolithography to pattern this metal. The metal was deposited over the entire chip. It was reacted with the silicon, and then it was just etched off in a blanket etch. So this kind of a salicide process was a big breakthrough in CMOS technology, say around the 80s or so. It's very good alignment. You form a low resistance contact to the source drain and to the gate simultaneously. And finally, it can be used as a gate material. The latest silicide that people are exploring in research and development-- and at some point will probably be in production-- is nickel silicide. Nickel silicide has the lowest silicon consumption. It can be formed at very low temperatures. It also avoids the PVD damage to the gate oxide. It's self-aligned, reduces the sheet resistance of the deep source drain region. It reduces the gate sheet resistance because you put you put silicide on the gate. And it also provides a local interconnect layer. Polysilicon doesn't have enough carriers. A metal has 10 to the 23rd. So it's not going to get depleted. People want to remove the semiconductor from the gate. But poly is an extremely easy material to integrate. It's not reactive with SiO2. That's not the case for metals. So people are thinking of replacing poly, but it's got a lot of complex process integration. The work function between the metal and silicon determines the threshold voltage of the transistor, the voltage at which the transistor turns on. FUSI gates are also interesting, not just because it's a little easier to integrate the processing. The frontend processing is what we're all familiar with. Everyone knows how to etch a poly gate and make a good poly gate, and you can convert it to silicide. But depending on how you dope it, you may be able to adjust your threshold voltage. It can be varied by whether you put doping in or not and by how much doping. And this is important because you need this flexibility. control the PT or adjust the PT a little bit. It's still a very tricky process. The reaction temperature or the thermal stability temperature is reasonably low. So you cannot take these and then take them to a backend process that is too hot. So they're very much a research. It was only published by IBM a couple of years ago. They're certainly not ready necessarily for manufacturing right now. Perhaps in the near future, but just to give you an idea of the types of things that people are concerned about. about your oral report or whatever, please get back to me. OK, thanks. About your oralReport.com. About. Your oral report. About Your Oral Report. Please get. back tome. about your oralreport or whatever. about. your oral Report or whatever,. please getback to me about.your oral Report. about Your OralReport.org. OK. Thanks, thanks, thanks for your report. You can send it to me via e-mail at jennifer@dailymail.co.uk.

ROUGE-1: 24.17, ROUGE-2: 23.09, ROUGE-L: 18.06
BERTScore: 63.98

==============================================
==================== [15/100] ====================
Summary:
The dagger algorithm aims to provide a more principled solution to the imitational and distributional Shi problem. The idea in dagger is to actually run the policy in the real world see which states it visits and ask humans to label those States. The goal is to collect data in such a way that P Pi Theta can actually learn from the data it's trained on. The basic version of dagger works like this and that's the version that you will all be implementing in your homework. It's a very simple algorithm to implement if you can get those labels. It can actually get up to fly pretty reliably through a forest dodging trees. Humans need to provide data for imitation learning which is sometimes fine but deep learning works best when the data is very plentiful so asking humans to provide huge amounts of data can be huge limitation. If the if the algorithm can collect data autonomously then we can be in that regime where deep Nets really Thrive without exorbitant amounts of human effort. One of the most exciting things we can get out of learning based control is emerging behaviors behaviors that are better than what humans would have done. In that case it's very desirable to learn autonomously. The cost function and the reward function are really the same thing they're just negatives of one another and the reason that we see both sometimes is the same kind of a cultural distinction that I alluded to before remember I mentioned that we have S a which comes from the study of dynamic programming that's where the reward comes from in optimal control. In optimal control it's it's a bit more common to deal with costs I don't know if there's a cultural commentary here well you know optimal control originated in Russia maybe it's more common in America.

ROUGE-1: 29.36, ROUGE-2: 27.48, ROUGE-L: 27.73
BERTScore: 60.12

==============================================
==================== [16/100] ====================
Summary:
Jonathan Gruber: What stops people from bingeing on everything? It's their budget constraint. He says budget constraints can help with a lot of kind of decisions in life. Gruber says we're not going to tell you what to eat. People like to have choices, and they like to let choice drive things, he says. Grubert: We're not saying it's right or wrong. It's positive economics. This is not normative economics. We're talking about how consumers use budget constraints to maximize utility. The median American household has $400 in the bank. If you're below the poverty line in America, roughly speaking, you get help with buying food. The SNAP program gives you a debit card, and that debit card can be used to buy food and food only. If your income goes up, but your parents decide to send you more money, what does that do to your choices? If you get $500 in cash, now all we've done is shift out your budget constraint from 5,000 to 5,500. You can essentially devote your income to some combination of pizza and cookies. The rate at which you can trade off pizza for cookies is minus 1/2. Every extra cookie that you buy, holding your income constant, lowers the amount of pizza you can have by p sub p. This is a really good slice of pizza, OK? Well, what happens to the budget constraint? Let's look at figure 3-2. You have your original budget constraint BC1. Now we're going to have MRT, the marginal rate of transformation. The more you spend on one, the less you get of another, Jonathan Guber says. Gubers: By having more of one, we're getting less of the other. "You are choosing six cookies and three pizzas. That is the best off you can get given your budget," he says. "We are not literally transforming pizza into cookies. That would be kind of cool, but we're not doing that. That's somewhere else at MIT, OK? But it's effectively doing the same thing" Weight Watchers has developed the best method of weight loss in America. They set up a budget constraint and ask you to follow it. They essentially assign point values to every good you might consume. So, for example, they say, if you want to achieve a weight loss of x over y days, then you've got to limit yourself to z points. So the slope has fallen from negative 1/2 tonegative 1/3. The y-intercept has not changed, but you can have fewer slices of pizza. A 10-piece nugget is 12 points, apple slices are one point, and a Diet Coke is zero. Now you have 13 points and plenty of room for dinner. "I'd much rather a Big Mac and fries and a Coke than nuggets and apple slice and Diet Coke. Give me a break," he says. "But I'd also much rather have dinner, OK? So, basically, this lets you make a choice." "Anyone who tells you that second lunch is as good as that first lunch is a liar," he adds. Jonathon Gruber: How do you determine your marginal rate of transformation? How do determine your-- like say it wasn't just pizza and cookies. How would you determine that value? JONATHAN GRUBER: That's a great question, and we're going to actually answer that question next lecture very explicitly. Next lecture, we'll compare explicitly why income changes differ from price changes and what are the underlying mechanisms. You want the bang for the-- you want to put your next dollar where it's going to make you happiest. budget constraint, which runs from 6 pizzas to 12 cookies. That's the original budget constraint. And you have a series of indifference curves. And these indifference curves, I1, I2, I3, I4, they all come directly from this utility function. And what we see is that point D is the furthest out indifference curve you can achieve while still meeting your budget. And, therefore, we say that the optimum, graphically, is the tangency between your indifference curve is minus 1/2 at point C. point A better? Why isn't it better to have two? Maybe you like cookies a lot and don't like pizza a lot. How can we say that point D is better than point A? Yeah? JONATHAN GRUBER: It's on a higher indifference curve. So point D dominates point A because it's a higher indifferent curve. Well, fine. Same person, by that logic, why not choose point E? It's above the budget. OK, likewise, point C you wouldn't choose. The marginal utility of pizza is 1 over square root of 10. Marginal utility of cookies is 2.5 over the square root. Utils are not, but that is a meaningful concept. So we only care about it in ratios, so we need the ratio. Remember, marginal rate of substitution is MUc over MUp. So your marginal rateof substitution is minus2.5. What does that mean? Can anyone tell me what that means? Your marginal rateOfSubstitution is 2,5. Jonathon Gruber asks audience to trade two pizzas for one cookie. "That's what that number means. And that is a meaningful number," he says. "You are willing to give up 2.5 slices of pizza to get one cookie" "So what should you do? Eat less pizza. Eat more cookies," he asks. "We can use that. That's not an ordinal. that's cardinal" "You're willing to trade. Yeah, say it loudly so we can hear," he adds. how we do constrained optimization. OK, that is hard and very important. If you understand this, you're sort of done with consumer theory, OK? This is sort of the core of what consumer theory is all about. It's all about this balancing act. The whole course is fundamentally all about one equation, which is marginal benefits equals marginal costs. OK? Everything we do is going to be about weighing the marginal benefit of an activity against its marginal cost. We want to set them equal. And this sort of example I hope explained why. Jonathon Gruber: Food stamps are not actually called food stamps anymore. They're basically a program the government has that provides money for individuals to buy food if they're low income. The point of SNAP isn't really with contentedness or happiness, but rather like what would be to a more sustainable life, he says. GRUBER: If you really just care what makes people happiest, you should give them cash, OK? He says a way people could get around food stamps is to buy and sell it. it makes it a lot easier to have two goods. Your two goods are food and shelter. And, actually, if you're that poor, that probably is the only two goods you have to-- you can worry about at that level of income. The fundamental-- one of the important things is people always get to the point that makes them happiest, OK? We call it the robustness of economic equilibria. They want-- they always before had the choice of spending $500 on food, and they chose not to. you die, that would be really bad. The reason to do this is because we think they don't know best. In that case, maybe we don't feel so bad about forcing the guy to buy food instead of cocaine, OK? In other words, this a program which might make sense if we are paternalistic. If we think that people won't necessarily make the right decisions for themselves, then it may be worth actually making them worse off because they're not worse off. Their perceived benefits are worse. in practice. Are they making a difference? Well, actually, we've run an experiment on this, OK? We're going to talk in this class a lot about empirical results in economics. This class is mostly going to be a theoretical class. That is we'll talk about models and ideas. But we're also talking about empirical economics, which is results and testing the theories we develop. And the empirical evidence is that, basically, the price of our paternalism is 15%. We are making people, effectively, 15% worse off. Vox.com followed these women up nine years later, and the effect had totally gone away. They're not worse off, but it looks like, at least what in the short run made them better off, well, that effect fades over time. But the bottom line is, at this point, I think the evidence is sort of probably in favor of being less paternalistic and just giving people cash, but that runs into a lot of difficulties in terms of our concerns about how people will spend it. stop there. We will come back on Monday, and we'll talk about how we actually go from this stuff to the demand curves we started the class with. Back to the page you came from. back to CNN.com home. Follow us on Twitter @cnnireport and @CNNOpinion. Follow CNN Living on Facebook and Twitter. For more, go to www.cnn.com/lifestyle and www.dailymail.co.uk/lpin.

ROUGE-1: 28.41, ROUGE-2: 25.87, ROUGE-L: 21.98
BERTScore: 63.55

==============================================
==================== [17/100] ====================
Summary:
Professor Shelly Kagan: Life on the experience machine is perfect as long as you've got the right tape playing. But the vast majority always says, no, there's something missing from that life, she says. Different theories of well-being might answer that in different ways, Professor Kagan says. We don't have any kind of accomplishments we don't want, not in the right kinds of loving relationships, he says. Rather than trying to work out the details of these complicated views, we should try to add up all the positive experiences along with all the negative ones, says Kagan. All think accomplishment's important, but it's not as though any old accomplishment is important. We want a life in which we're accomplishing things, there's agency, and the life of knowing things. We need the right kinds of accomplishments and the right kind of knowledge. Without predictability you don't know where to put the peak. If you try to aim for peaking later, you might not make it to that. if you put it too soon, you may stick around for longer than that, and then the peak has come too soon. LZ Granderson: We add up the goods and subtract the bads of our lives. He says we can generalize across all humans, but it varies from person to person. LZ: We care more about being short-changed than we do about being overcompensated. He asks: Would it be better to know when you were going to die? Lz: In our world, not only do we have variability, we've got unpredictability. of chosen to become a doctor. The deprivation account says, death is bad for you insofar as, by virtue of dying now, what you've been deprived of is, another chunk of life that would've been good for you to have. We've now seen that that's a bit crude, right? We have to not talk--just talk about the good things in the life, but the good of life itself. And we have to notice that perhaps on certain views, for certain cases, it's not really the case that when I die I'm being deprived of a good life. Hedonism is a version of the "neutral container theory" of the value of life. Hedonism says life itself has no value in and of itself. The value of being alive per se outweighs that, he says. So more life would always be better, no matter how horrible the contents might be, Hedon says. "I find the fantastic valuable container theory fantastic in the sense of incredible. I can't bring myself to believe it, which--I also have some sympathy for neutral container theories" subtract the ignorance and deception. Doing that in terms of the contents gives you a subtotal, but that subtotal is no longer the entire story. If we accept a valuable container theory, we also have to add in some extra positive points to take account of the fact that, well, at least you're alive or have the life or a person--or whatever it is that you think is valuable in and of itself. Even if your content subtotal was negative ten, that doesn't mean you're not better off alive, because negative ten plus the extra hundred points for the mere fact that you're Alive is still going to give you a positive total, plus 90. On the modest view, if we ask ourselves, would it have been good to be immortal? the answer's going to depend on whether we accept Bernard Williams' claim that immortality would be bad for you. So on balance, being immortal is a good theory myself--I don't find it particularly attractive. But let me remind you that saying that does not rule out the possibility of consistently going on to say that even though it's a good thing that we die. For some of us, death does not come too soon. David Frum: The fundamental badness of death is that it deprives me of life worth having. He says it's arguable that this isn't the only bad thing about death. Frum says there are other features of death, as we experience it, that are separable from the deprivation account. He asks: Does the variability of death make it better or worse? He says perhaps there's some comfort in the inevitability of death and the fact that most people die too soon. Dostoyevsky: It's not merely the fact that you're going to die, it's inevitable that we're all going to death. He asks: What about this inevitability of death? Does that make things worse? He says if we see that something is just necessary, we--it reduces the sting of it. He says variability, although it's a requirement for unpredictability, doesn't guarantee unpredictability. The fact that there's variability and so some people get less than average--that extra bad, I suspect, outweighs the extra benefit of some people having more than average. in life is necessary, then we'd get a kind of emotional distance from it; it would no longer upset us. We could no longer be disappointed, because to be disappointed in something presupposes that it could've been some other way. Spinoza thought if you see that it couldn't go any other way, then you can't be sad about it. Well, maybe that's right, but going back to the firsthand, I don't know how many of you have read Dostoyevsky's short novel The Underground Man. of view of somebody who gets less, this is obviously a bad thing. It's bad enough that I'm going to die too soon. I said I wasn't going to keep saying that remark, and here I am saying it anyway. That's clearly an extra-bad. But we might then wonder, for every person who gets. less than the average amount of life, there's another person who has more than the median amount of. life. That person gets to say, hey. Well, you know, it's a pity that I am going. Because of unpredictability, you can't really know, it's difficult to make the right kinds of plans. You decide to go off to medical school, become a doctor. And that's a very long commitment. It's a long-term plan, which can go wrong if you get sick and die in your early 20s. If only you'd known you were only going to have 20 more years instead of 50 more years, you would've picked a different kind of life for yourself. Somebody starts out poor and makes his way through hard work and dedication and effort to riches and success. That's the Horatio Alger life--H.A. Great life. But notice that in terms of the contents of the life, at The narrative arc, as I put it. The story "bad to good" is the kind of story we want for ourselves, while the story "good to bad" we don't want. Why is that? And this of course should remind us of the puzzle about Lucretius. Why do we care more about future non-existence than pastNon-existence? A lot of us might feel that a life like this, where we peak but then we stick around--you know, isn't--can at least fail to be as desirable in which we end with a bang. If you start thinking about narrative arcs--imagine a novel, right? It's one thing to have--it's not to say that the best--if you want your life to be like the plot of a great story, it's not as though you think, "All right, the dénouement must occur at the very last page" ask--so I'll throw the question out and we'll call it a day, start with this next time--then we have to ask, would it really be better to know? would you want the birthmark? Would you want to know exactly how much time you've got left? All Right. See you next time. Back to the page you came from. Click here for more from CNN iReport. Back To the pageyou came from, back to thepage you were from.

ROUGE-1: 29.73, ROUGE-2: 27.68, ROUGE-L: 23.41
BERTScore: 62.60

==============================================
==================== [18/100] ====================
Summary:
Michael Short: How do you know what, let's say, your dose distance relationship is? And how do you calculate all this stuff? Short: In these sorts of experiments, if you want to know how much more radioactive is one place than another, you have to take a background count. Short: We're going to be measuring today, which is why I have my big bag of burnt bananas. These are the ashes of roughly 50 pounds of bananas charred to a crisp at about 250 Fahrenheit. dose and distance or measured activity and distance? Yeah, Luke. AUDIENCE: [INAUDIBLE] r cubed. MICHAEL SHORT: Close. It's, let's say, the measured activity would be proportional to 1 over r squared. Does anyone know why this formula would break down? What happens to our solid angle or our approximation for ourSolid angle is kind of the analog to regular old angle, except in 3D. So instead of looking at things in radians, this has the unit of what's called steradians. One banana contains a minuscule but measurable amount of radioactivity. To boost your confidence on any sort of radiation measurement, boost your signal strength or to boost your counting time. We've concentrated the ashes of 50 pounds of bananas in here to boost our signal strength. We'll take a look at the spectrum in a minute. We're not going to see anything while we're here. And we just hit the gammas anyway. We were playing around with cobalt 60 lines and a cesium 137 line. There's no better concentrated source of smoking radioactivity than a smoke shop. The background in the case of the smoke shop just the area right outside of it. In New Hampshire, the background count's quite a bit higher, because there's a lot of granite deposits. There's a little overlap on my shorts and longs. That helps me do QA on things. And if I run two standards, I'll check the concentrations from one standard to the other. That's another little QA thing. What else we got? know. Most of the Boston area is 21. But once you leave Boston-- MICHAEL SHORT: It varies. I don't think it is where I'm-- from Swampscott. But that's kind of up on the commuter rails. You don't want to go to Swampsc Scott. At any rate, I would think that, OK, it's 21. You can buy them. It's still late-stage. it's like town-to-town. But you have to be 18 to smoke. Michael Short: How long would you actually have to bring a detector in and count in order to be sure that there's any sort of measurable difference? Short: From Poisson statistics, you can say that the standard deviation of that count rate is actually just the square root of the count rate divided by time. The more error you allow, the shorter time you have to count for, he says. Short: As you increase the counting rate, it takes less time to distinguish your source. increase your counting time. By counting for longer you can decrease your standard deviation. Use this formula to estimate how much time you'd need to count for an extra 28 minutes to nail that net count rate with 95% confidence. Let's say what happens if you go in there and you get a 100% rate of 100 counts per minute? That would do that would surprise me. You'd only have to count. for an additional 28 minutes. That would only surprise me, but I believe the homework doesn't actually tell what we don't know. If you go plus or minus 1 sigma away from your true average right here, you've filled in 68% of the area under this normal distribution. 3 sigma is getting towards 99 point-- what was the number, again-- I think it's 6.5%. And then so on, and so on,. There's actually societies called 6 sigma societies. And the way that they get their name is we're so confident of things we can predict them to 6. sigma, which is some 99 point. Michael Short: We have numbers for C b and t b, but not C g and t g. Short: How do we relate t g and C g? Well, let's start with the easy stuff, right? What can we cancel, or square, or whatever? Just somebody yell it out. Cool. So we'll take that expression and substitute in everything we can. So 0.05 C n equals 2 sigma. And there's our sigma expression, which I'll rewrite right here. Cobalt 60 gives off two gamma rays per second. Activity is measured in disintegrations, not in number of gamma rays emitted. Tungsten 186 activates into tungsten 187, which has a half-life of 23.9 hours. The value of that actual surface integral gives you the real solid angle of a detector. It's the super simple one if you just know the area of something and you know that you're kind of far away. But again, whenever possible, use the exact formula. per disintegration. So you've got to know what material you're looking at in order to know how many gamma or how many betas or more that you're going to get. Who here has heard of this uncertainty in quadrature before? There's a couple folks. OK. The idea here is that, again, if you just add the errors up, you're probably overestimating the error and selling yourself short. In that case, if there's no questions, let's go do this. a couple thousand volts across it. When a gamma ray goes into it, it makes some electron hole pairs. So you collect the current from that, and you get a little pulse of current. The height of the pulse tells you how many hole pairs you had, and then back it up to what the energy or your gamma was. That works fine if you collect all of the gamma energy. You don't always quite do that. Anyway, so that's how-- You all can scooch up. There's not a whole lot to see in there. NAA is good for rare earth elements, which are hard to measure by other methods. By picking out various rare earths and the ratios, NAA can help identify where things are from in the world. NAA's Mike Short takes the audience on a tour of MIT's nuclear reactor. "I've got steel toast. If I miss the toe, I'd probably break my-- I don't want to think about it," he says. "We'll see what we got. We'll look at what comes in, and I might veto some things" how long has that been going? Half a day-- less than that. Anyway, so this is a sample of quartz that was irradiated next to the reactor. You guys are going to do shorts in like a month-- did you bring your samples? MICHAEL SHORT: We're getting them. I think this ran six hours. And it's just a little tiny piece. And count it, I'll also have whatever elements are in the vial on the thing. And so then I take a couple of samples, I throw them in a lead pig-- so I've got a whole bunch of these floating around. is these guys are going to calculate what's in their samples. Michael AMES: But that's not how I do NAA. MICHAEL SHORT: [INAUDIBLE] We're doing a simplified version. OK. So you've run a standard, which means a material that how much tungsten is in it or how much a whole mess of other things are. So I'll give you a background spectra. I will provide the efficiency for this geometry, which is pretty poorly defined. samples. But we'll figure out how many samples we'll run. MICHAEL SHORT: It's one per person. [INAUDIBLE] MICHAEL AMES: That's a lot of shorts. In pairs, right? MICHAEL ShORT: Yeah. So I'll show you how the shorts get run. So when we run your shorts, we'll running your samples and we'llrun standards, and then you can do the comparative method. Or, if you feel like it, you can doing the other method. lake sediments. Other analytical methods have gotten a lot better, and so they've kind of caught up to NAA, and you don't need a reactor to run those. The environmental side of this has kind of quieted down a lot. But it's still useful for a bunch of things. I also work in the NCORE group. So that's a lot of my time, rather than just this lab. Practical things-- let's go take a look at a couple other labs. that looks at zinc deficiencies, and fingernails and toenails will give you a good record of how much zinc you've had over the last week, or month, or whatever-- depend where you cut the nails. And so I was going to get a couple of hundred African children's toenail. That didn't happen. But I did analyze my own toen nails. Well, if you went to somebody who was a little suspicious of you, asking for toenailed is a lot easier than asking for a blood sample. Because people would give up toenailing-- it's not a big deal. This is some soil from Montana next to a mine, so it's nicely contaminated with some metals. This is my IAEA mercury and hair standard. And this is kind of what everybody uses for standards. And you just kind of have a whole collection of them. And depending on what elements you're looking for, you try to mix and match them so you cover what you want without having to run five or six of them, says Michael Ames. And so that's how I do the comparative method. The experiment we're doing is basically change reactor power by half a megawatt. To actually do this experiment, we need two licensed people in here, one at least has a senior reactor operator. The only way you can actually do these manipulations are if you're in a program that needs you to actually operate the reactor. We'll show you the proper way to make the entries as a trainee on console. And then we'll go ahead and then do the actual movement itself. If you don't feel comfortable doing something at any time, let us know. Two MIT students test neutrons not leading to fission in a nuclear reactor. The reactor is controlled by a shim blade controller and a regulating rod. The blades can only move at the exact same rate at all times. The regulating rod can drive outwards to increase the amount of neutrons making the reactor power go up. It's not a big factor as it normally would be after doing one of these lowering reactor power. The power would actually want to go down on it's own, so you would have to do a lot of re-shims. power level is based on a chart that we create. So it's not showing you megawatts, or kilowatts, or anything like that. And that current is then converted into megawatts and so forth. You're going to be bringing a record up to 1. Megawatt and since it's linear, it'll be double that-- so 17.1. Now, you want to be careful when you raise reactor power. So when you start to add power to the reactor by raising a regulating rod, you don't want to keep raising it until you reach your value. The reactor is on autocontrol. When we do these manipulations, the reactor operator is going to take manual control. That'll cause an alarm to come in. And this will only happen for the first time. And that should be the only time you hear this alarm, because we'll leave it on. The last person will make an announcement that we're done with power manipulations. We'll do that at the end-- is she'll take Manual control of the reactor, and she'll answer it. where it started, the 13.42 inches out of the bottom of the core. It might not make it all the way back up to [INAUDIBLE]. FRANK WARMSLEY: It'll be close. Compensate with the reg rod if you need to. 30.8. OK. And that's the end of the exercise. We'll be back in a few minutes with the results of the test. We hope to see you on "Larry King Live" next week.

ROUGE-1: 24.94, ROUGE-2: 23.29, ROUGE-L: 18.93
BERTScore: 62.22

==============================================
==================== [19/100] ====================
Summary:
Professor: We're talking about evolutionary game theory in the case of, well, biological evolution. He says it's not that we think that the cells are engaging in any sort of weird puzzle solving. Instead, he says, they're just mutations. And the more fit individuals spread in the population, and somehow, you evolve to the same or similar solutions in the context of game theory, he adds. "From my standpoint as an experimentalist, I don't forget about the basic insights about the experimentalist," he says. Professor: We're going to start with an entire population at 0, 0, and now these mutations will be occurring randomly at some rate. Somehow the population is going to climb up this fitness landscape, and we're trying to figure out the relative probability that it's going to take kind of one path or another. Professor: The answers to all these questions are in principal already on the board. But in another way, you have to keep track of lots of different things, and which regime we're in and so forth. short genome that's string length 2. We're going to start in the 0, 0 state with 1,000 isogenic individuals. And the question is, what's going to happen eventually? In particular, what we want to know is the probability of taking this path. We'll also discuss whether it somehow is very likely is going to kind of have to go through one or from 0,0 to 0, 1. The probability for that first path would be the S for 0,1, so it's 0.02. a minute. But if you don't understand what's going on, it'll take you an hour. Ready? Three, two, one. OK, all right, so we do have a fair range of answers. I'd say it might be kind of something like 50-50. And that's great. It means that there should be something to talk about. So turn to a neighbor. You should be able to find somebody that disagrees with you. And if everyone around you agrees, you can maybe-- all right. So there's a group of D's and aGroup of B's here, which means that everybody-- AUDIENCE: Let's fight. Professor: "I think the arguments there-- there's a lot of truth to the arguments that you're saying" "I really like drawing the graphs of these things, because I think it's just much more clear" "You can't keep just the first term in a series. If the terms grow with number, 3.98 or something" "There's less than a 1% probability of it fixing. Is this believable?" "I never thought that my calculator would become so controversial" that are present maybe in one copy. In order for this individual to fix, he has to survive stochastic extinction, which happens with the probability of 2%. And the 1, 0 individual has to go extinct, which happening 90% of the time. So this is, indeed, answering the question that if you had one copy of each of these two mutant individuals in the population, that's the answer to what is the probability that this 0, 1 mutant would fix in thePopulation. Professor: No clonal interference corresponds to mu N log NS much less than 1. He says as you get more and more mutations, when clonal interfered is really significant, then you're pretty much just guaranteed to take the 1, 0 path. Professor: Once you have multiple mutations that have established, then it's likely that one of them is going to be this. And if it's established, it's going to win, he says, and it's not even nearly neutral. of those two steps-- going to 0, 1 or 1, 0. And in particular, this is like a chemical reaction, where we have some chemical state here. And what we know is we know the ratio of those rates. And that's everything we need to know to calculate the relative probabilities of taking those states. So this is how we get 1/6 instead of 1/5. Because this thing is 1/ 5 of that. So it's like 1, and then 1, 5. And this is cohered at this peak in the finished landscape. rate, you don't even do successive fixations. So it may be that neither state ever actually fixes, because it could be that the 1, 0 state is growing exponentially, but is a minority of the population. And it has a lot of the same behaviors, in the sense of exponential suppression of probabilities as a function of the depth and the width of the valley you're trying to traverse. And there's some very nice papers, if you're interested in looking at this stuff. we have a 0, 0 state that has some fitness. 0, 1 has a higher fitness, and so forth. But in general, these fitness values may depend upon what the population composition is. And in that situation, then you want to use evolutionary game theory. For example, you can have situations where the population evolves to lower fitness. In that case, everybody's getting to fitness 1. Now, as a lone individual, what can you do? All you can do is switch. In evolutionary game theory, what we can do is plot as a function of the fraction of the population that is cooperator or defector. And we can imagine that the only thing that's important are how these lines cross each other. And surprisingly, that does not mean that strategy that is higher fitness, in the sense that you may evolve to a state of low fitness or low coexistence. You can have one strategy that dominates, which is what occurred here. That's what's weird. be in trouble, et cetera. So the idea of the prisoner's dilemma is that if you set up these jail sentences in the right way, then it could be the case that each individual has the incentive to confess. And we'll call this-- so this is for individual one, say and individual two. So there are different strategies you can follow. And do you guys remember from the reading slash my explanation how to read these charts? All right, now the question is, just to remind ourselves, what is the Nash equilibrium of this game? In a Nash equilibrium, if everyone's playing that strategy, then nobody has the incentive to change strategy. If both individuals, or an entire population, say, is playing A, they're getting fitness 5. Question is, as a lone individual, you can choose to switch over and get fitness 3. Do you want to do that? No. So that means that A is going to be a Nash equilibria. But the funny thing is, what that means is, it doesn't matter what you do at the equilibrium. cooperator and for the defector. Do you understand? So what should these things look like? I'd like to encourage you to-- I'll give you 30 seconds to try to draw what this should look like. So this is the payout or the expected payout. So we're assuming that you're going to interact randomly with the other members of the population as a function of the fraction cooperator. So then 1 minus that will be the fraction defector, do you understand what I'm trying to say? you to be influenced by this. Does that mean that this game has no Nash equilibrium? Yes or no, verbally-- ready, three, two, one. AUDIENCE: No. PROFESSOR: No, it does not mean that. This game has a Nash equilibrium. And indeed, all games like this have Nash equilibria. This is what Nash won the Nobel Prize for, so this is the famous one-page paper published in PNAS. And if you have questions about this, I'm happy to answer it. In a population, if you have genetic A's and genetic B's that are each giving birth to their own type, then you evolve to some coexistence of genotypes. Whereas in this situation over here, we have coexistence. Does not matter where you start. So long as you have some members of both A and B in the population, you'll always evolve to the same equilibrium. Whereas the mixed Nash equilibrium is a situation where you have, in principle, genetic homogeneity. So this is a single genotype that is implementing phenotypic heterogeneity. about exploring in my group is this distinction here, where it's known that in many cases, isogenic populations of microbes can exhibit a diversity of phenotypes. So that's a molecular mechanism for how you might get heterogeneity. Another question is, what is the evolution explanation for why that behavior might have evolved? Now in general, we cannot prove why something evolved, but we can make educated guesses that make experimentally testable hypotheses. In the coming weeks, we'll talk about this idea of bet hedging-- that given uncertain or fluctuating environments, it may be advantageous for clonal populations to have a variety of different strategies.

ROUGE-1: 25.51, ROUGE-2: 24.35, ROUGE-L: 19.65
BERTScore: 60.80

==============================================
==================== [20/100] ====================
Summary:
The sa node is located in the upper part of the right atrium. It is the starting point of the electrical conduction system. The sa node acts as the pacemaker of the heart. If it's working like it should it should cause your heart to beat at about 60 to 100 beats per minute. The reason this rhythm is occurring is because our sa node was firing rapidly and it's causing this heart to beating rapidly. The heart will start beating fast because it knows that the body is in dire need of oxygen. With sinus hack the atrial rate is going to be greater than 100 beats per minute and then whenever you're looking at the p wave take your calipers and go from p wave to p wave and make sure there's the same distance between those because that means it's occurring at a regular interval. The p wave represents atrial depolarization so with that you want to make sure that you count your p waves. The qrs complex represents the ventricular rate so just like with the p waves you're going to count your qrs complexes and that's going to give you the rate. p wave so i'm going to go from r wave to r wave with my calipers and i'm just going to confirm that they are regular. Now let's measure the qrs complex we want to make sure it's not too wide or too narrow so it should be less than 0.12 seconds. Now we have a regular rate and it's 110. Now i want to check out that pr interval and that is found at the beginning of the p wave to the begin of theqrs complex. the heart rate now this can be disease related or it can be non-disease related so we want to increase our heart rate whenever we exercise because that gives us a good workout so in that case sound attack isn't really that bad or if we're in a fearful stressful situation let's say you're about to be robbed your sympathetic nervous system is going to kick into gear and it's going to increase your heart rate so it can help hopefully protect you from danger and that's a good thing. whenever a patient develops sinus attack and they have a disease it could be that their disease is worsening. A patient who is post-op from surgery especially some type of bone surgery like hip surgery or they're in the postpartum period where there's a high risk of blood clots after you have a baby or they are immobile. Let's say you're patient develop sinus tachycardia well you'd want to be thinking okay possible pulmonary embolism. You also want to look at their extremities how do they feel are they cool check that capillary refill is the time increase that could mean you definitely have decreased cardiac output. that and see what's possibly going on with the patient assessing blood levels like the thyroid level making sure they don't have hyperthyroidism looking for anemia or maybe infection they have a white blood cell count that is high and looking at that medication history making sure that they're not on any medicines that could be increasing their heart rate. This leads me to medications that can be ordered to help slow down that heart rate one group of medications are beta blockers and one type is called metoprol beta blockers.

ROUGE-1: 26.18, ROUGE-2: 25.14, ROUGE-L: 24.48
BERTScore: 63.31

==============================================
==================== [21/100] ====================
Summary:
In this problem, we're given a collection of 10 variables, x1 through x10, where each i, xi, is a uniform random variable between 0 and 1. And we'd like to develop a bound on the probability that some of the 10 variables being greater than 7 using different methods. In part A we'll be using the Markov's inequality written here. And in part B, we'll use the Chebyshev inequality, which takes into account the variance of random variable x. a better bound. To remind you what a central limit theorem is, let's say we have a summation of i equal to 1 to some number n of independent and identically distributed random variables xi. We take the sum right here, and subtract out its means, which is E of the same summation, and further, we'll divide out, what we call normalize, by the standard deviation of the summation. As the number of terms in the sums goes to infinity, we will actually see that this random variable will converge in distribution in some way.

ROUGE-1: 20.28, ROUGE-2: 19.63, ROUGE-L: 20.17
BERTScore: 73.37

==============================================
==================== [22/100] ====================
Summary:
So now that we've combined pulley A, string 2, platform, and washer as our system, we can now address our question. If we measure the acceleration of the person, what is the force that the person pulls the rope down with? Well, of course, that will just be the tension in the string. And with this simple system,we can now apply Newton's second law, F equals ma. And so by thinking about how to choose a system, what could be a very complicated problem, with lots of equations, is simply one equation.

ROUGE-1: 49.74, ROUGE-2: 48.95, ROUGE-L: 49.74
BERTScore: 80.28

==============================================
==================== [23/100] ====================
Summary:
In order to be successful whenever you're drawing blood or starting IVs you really have to know a couple things number one you need to know the name of the vein that you're going to use and its location along with what can that vein actually handle. Some veins can only handle about a 20 or a 22 gauge IV cannula versus some of them can handle 18 gages 16 gages. I also like to use accessories cephalic vein along with the median vein of the forearm and of course those hand mains the dorsal venous network.

ROUGE-1: 13.00, ROUGE-2: 12.62, ROUGE-L: 13.00
BERTScore: 67.49

==============================================
==================== [24/100] ====================
Summary:
When we talk about complement and substitute, we have to be clear whether we are talking about the demand side or the supply side. So, what do we mean, when do we say a good is complement in production or complement in supply? Can you give an example, first substitute think about it? Plastic chair to iron chair like. Boeing is a manufacturer of airplanes; it makes civilian airplanes as well as military aircrafts. If there is an increase in price of military aircraft, what would happen? Boeing devote more a space to military. To manufacturing, to manufacture military. aircrafts, it would go down.

ROUGE-1: 28.57, ROUGE-2: 27.27, ROUGE-L: 28.57
BERTScore: 66.31

==============================================
==================== [25/100] ====================
Summary:
Nancy: Social intelligence is a range of phenomena that govern the interactions of large groups of people, like war. Ken: I think that when people talk about social cognition they do actually mean all of those things. Nancy: Trying to get a coherent account of everything from your hand motions and your perception of other people's hand motions all the way to politics and sociology is daunting and, frankly, deeply unlikely. Liz: I'm going to focus on three ways to use modern techniques in fMRI to study human mind. Psychologist David Frum: How much blame do we assign to people based on our beliefs? Frum says we assign blame based on what we think we deserve, not what happened. He says we can change the meaning of knowingly murder than to unknowingly murder. Frum's work focuses on how we assign thoughts or internal mental states to people around us. The more we know about someone, the more we can blame them for their actions, he says. The less we know, the less we should blame them. The scope of tests of our ability to think about other people's thoughts or internal states is very large. I share the worry that you could never make progress on this. So what I want to tell you guys is two phases of my attempt to make progress. I'm almost exclusively going to talk about the first one, so how we think about what other people see, think, and know-- but not want or feel. And then a more in-depth look at how I'm using more you thinking about thoughts. false belief task looks like. This is being given to a five-year-old human child. Do you know what pirates really like? CHILD: What? REBECCA SAXE: Pirates really like cheese sandwiches. Here comes Ivan. He says, I want my cheese sandwich. And he takes this one. Uh oh-- why did he take that one? OK, and so the traditional read of what just happened there is that's a kid who gets wanting. But he doesn't get believing. MVPA is a technique for getting more information out of the same data. It's an analysis technique, not a way of getting data. MVPA could be used to rediscover all of the things Nancy already discovered using the traditional analyses. If you use them uncarefully, what you're most likely to do is just re-go over old territory with new math. It gave us a measure that was sensitive to small distinctions in the stimuli. It was a totally stable set of examples within a totally different set of stories. The right TPJ is selectively involved in theory of mind, and so selectively depends on all the experiments I didn't show you. "involved in" is a euphemism that I think a lot of cognitive neuroscientists use and are satisfied with. But after a while, I found it deeply embarrassing, like-- what on earth is " involved in"? And so what I want to talk to you about is how to get beyond the euphemism in using fMRI to understand the mind. MVPA is an old, discredited theory of concepts, but nevertheless a powerful strategy in neuroscience, including in this context. The idea that we're going to look at is that populations of neurons will respond differentially to features or dimensions of our stimuli. And we can infer something about the representation underlying-- the representation that this brain region participates in-- and that is the representation of theory of mind. In every trial, you read a long story that sets up a complicated story that you want to show off your culinary skills for one of the dishes. For the first 15 years of fMRI, the only thing it could tell you was the average amount of response. Haxby method uses spatial correlations to get a more general way of thinking about fMRI data. It took a long time for other people to recognize what a cool technique this was, but he had this idea a very long time ago. For some future that I wonder if it's represented, is the correlation across neural responses more similar when the stimuli share that feature than when they don't sharing that feature? V1 is called V1 because information goes from your eyes to the LGN of your thalamus. It's the first cortical stop of visual information. One way that we know that it's very involved in vision is that if you're seeing visual stimuli, you get a big response in V1. If you're not seeing stimuli, like you're hearing auditory stimuli or feeling tactile stimulus, you don't get a large response. That's a selectivity type measure. We want to know what transformations over the information coming from LGN is V1 implementing-- what computational transformations, what representations. story by just changing your mental state. Whether you knew or you didn't know about your cousin's peanut allergy is really important to the moral judgment of what happened. And the right TPJ is tracking the important information about what you think. And so it's activated for both of these kinds of stories. So that's a univariate analysis. Now what's a multivariate analysis? The idea is, think in a very abstract similarity space. They're all unique. So if we find the same result, then it generalizes across all these incidental features. person who wrote the essay was in the room when you said that publicly shaming thing? A different story is about demonstrating your karate skills and knocking out your classmate-- again, totally new moral scenario. But again, this one feature-- did you know or not know that your classmate was there when you did the kick? Now here's the idea. Even though each of those new scenarios is completely different, if there are different subpopulations within your right TPJ responding when you knew you were going to cause harm, then a little part of that response will be the same. The amount of activity in the right TPJ is a big signal. The relative activity between one voxel and another is a tiny signal. And it's superimposed on a lot of noise. But if there's anything there at all, then you'll still be able to pick up a little more similarity for pairs that are matched on the feature of interest compared to pairs that aren't matched on that feature. We've actually found this a whole bunch of times. The rightTPJ doesn't care about valence. Other regions do. a Haxby style correlation is what's called the within-condition correlation. It's the spatial correlation of the response to two independent sets of stories that share this one feature. The more that you represented knowing harm as different from unknowing harm in your right TPJ, the more you judged them as different when we asked you for moral judgment. The pattern difference in your TPJ accounts for 35% of the variance in your moral judgment, which is pretty amazing, because that's a pretty noisy measurement. MVPA is a new technology that allows scientists to test their theories in real-time. In a new experiment, scientists used MVPA to test the theory of knowing and unknowing harm. The researchers found that the effect was hiding in the data that they'd already collected. The scientists say that there's something more real about it than if you knew the hypothesis before you ran the experiment, even though that makes no sense whatsoever. It's like, if I had the hypothesis in my head, maybe it somehow got from my head to the data.

ROUGE-1: 22.40, ROUGE-2: 19.90, ROUGE-L: 16.98
BERTScore: 61.80

==============================================
==================== [26/100] ====================
Summary:
Joanne Stubbe's lab works on the only cool enzyme in the world-- ribonucleotide reductase. It's the only way in all organisms that you make the building blocks de novo that are required for DNA biosynthesis and repair. If you inhibit this enzyme, you have no building blocks. You can't survive. So from a practical point of view, it's the target of drugs they use therapeutically in the treatment of cancer. And I think in probably not so distant future in the antibacterials because there are sufficient differences between humans and bacteria reductases. and do the same chemistry, but they have different metal cofactors depending on where they evolved. The function in all cases is to generate a radical in the active site and then the chemistry is the same in all these things. And the function of the metalcofactors in all case is to create a radical, which is the key to the chemistry in all of these cases, says Dr. Michael Bociurkiw, a professor of chemistry at the University of California, San Diego.

ROUGE-1: 38.51, ROUGE-2: 33.92, ROUGE-L: 31.74
BERTScore: 70.88

==============================================
==================== [27/100] ====================
Summary:
GILBERT STRANG: Differential equations is the big application of calculus. He says it's interesting to see what information and ideas from calculus actually get used in differential equations. Strang: You really do need to know basic derivatives. The derivative of e to the x of functions that really blow open the functions or we can deal with, he says. "I just like that the use of the fundamental theorem of calculus we need," Strang says. 'It's not everything by any means, but not all the details you learned' is e to the x. Dy dt equals y. And then the inverse function related to the exponential is the logarithm. With that special derivative of 1/x. But you know those. Secondly, out of those few specific facts, you can create the derivatives of an enormous array of functions using the key rules. Derivative is a linear operation. The product rule fg prime plus gf prime. The quotient rule. Who can remember that? And above all, the chain rule. The derivative of this-- of that chain of functions, that composite function is the derivative of f with respect to g. be nice, I just think if you plug that in, to that differential equation it's solved. OK so I want to take the derivative of that. That's my job. And that's why I do it here because it uses all the rules. OK to take that derivative, I notice the t is appearing there in the usual place, and it's also inside the integral. But this is a simple function. I can take e to the t-- I'm going to take eto the t. tells us about bending? That is delta t squared times the second derivative. One half shows in there. So this is the term that changes the tangent line, to a tangent parabola. It notices the bending at that point. So it curves up. It doesn't follow it perfectly, but as well-- much better than the other. OK. Now finally, what if we want to do even better? Well we need to take into account the third derivative and then the fourth derivative and so on. If we get all those derivatives then, all of them that means, we will be at the real one.

ROUGE-1: 34.19, ROUGE-2: 32.42, ROUGE-L: 30.68
BERTScore: 73.84

==============================================
==================== [28/100] ====================
Summary:
okay so welcome to the last lecture of this course here in this winter term and what we discussed so far in the course were mainly the so-called backends or optimization engines or probablistic estimation techniques. Today I would like to give a very very of course brief short overview about front ends and one important aspect inside successful front ends on how to determine if a constraint is likely to be a correct one so we are still interested in avoiding to add roam constraints although we've learned that we there are techniques which can deal with outliers in the data Association. to begin by matching observations so we have different observations depending on what platform that can be whatever stereo camera or laser rangefinder or different types of sensory modalities. Depending on what we see what we assumptions we make about our observations this tarz can be very hard or not that hard. Other approaches use features for example we had those the Victoria Park where trees have been extracted in this case from the laser range data so the trends of trees and every trunk of tree was seen as one feature or one landmark and the robot map those landmarks. Large databases of images using those descriptors so these are three popular techniques or sensor information that front-ends use in order to make the data Association and way we actually look into those who is very short example examples during this course today okay so you can say okay given I'm here at the area of my scanner and say okay say if the robot was sitting standing over here that's the area it may have observed then I can simply check is the current sensor range is there an overlap between the current sends a range and the possible observation that I obtained from b1 on b2. moment and that's my sensor range I can compute where are those other pulses so in this case B 1 and B 2 just two examples could be more obviously and then I can also estimate what is the uncertainty of those poses B 1 or B 2 relative to a do that by eliminating the note a from my linear system and then inverting the resulting Hessian and looking to the main diagonal blocks this gives me the uncertainty here indicated by these dashed lines where b1 isrelative to a and the same here where is b2 relative toa. I reach all the posts I'm interested in looking into but this does is ignores the loop closures so the uncertainty estimates are too big but you can still argue that okay uncertainty estimates I get are toobig but I can compute this extremely efficient and I may inspect a few places too much but I should get all the places which I need to inspect in order to make sure I find the course with whatever 95 percent probability this is what is done. What is done in practice to what inverting this matrix age there so far we really tried to explicitly invert it. ICP is a combination of the iterative closest point algorithm and the initialization so ICP depends on the initial guess and it just finds one solution and may be the right solution but may be wrong solution. P is sensitive to the initial guessing and as a result of that we may end up in a local minima so in something which looks like a match but in reality is not a match. If you have descriptors like feature descriptors it can actually help you to find good estimates where you can be so you don't have to try all camera polls and see if the camera poses match. showing you three different examples of systems that we have built here in Freiburg. Some of the mapping techniques we developed here have been used to at least tested on that car so this is a pioneer a two robot which has a two d-day the rangefinder sitting here and sits on a pencil unit so it moves always like this song so it's called a nodding laser and this way generates 3d data you get 3d information about the scene and then it tries to build sorry a 3d map of the environment using this technique. then we can do is we can take those two local maps and try to align those twoLocal maps it's typically Nanban so depending on how many scans you integrate either you could take these skins so if it's just kind of one 3d scan you would although in practice it's a number of 2d scans for matching. Sometimes it's easier to match full maps like versus individual scans this depends on the data that you have and how many ambiguities you may find your environment so if you have a local blast slightly bigger view so you really match a map against the map that may be easier. darkest stripes these are simply small alignment errors of these individual maps they can they can see kind of small steps over here here that was also probably an alignment error which simply leads to her step which was let's say bigger than and all five centimeters in the ground and therefore it's classified as not reversible anymore and therefore everything is red over here. That's the way you can actually use 3d data to build a map of the environment the next example this is an autonomous car is a parking lot or a 3d model of a parking lots where yellow again means drivable areas and red means non drivable area. done with the grid for 20 by 20 centimeter grid cells and this by lining those grid cells you can actually get maps off and say this quality that's something you can expect to get with this technique. System which was flying on the prototype for a helicopter so never made it to the blimp in the end this was just a self assemble stereo camera system with two webcams assembled in a stereo setup and a small IMU there's an initial inertial measurement unit and one of the advantage of this system is it gives you also the gravity vector this quite accurately. Surf features provide a local description of the scene of a small noise a scene of the small part of the image and so if you see every of those for each of those points here one of those descriptive Alice is computing their computer kind of from a local window around them doing some local operations and returning typically a 128 dimensional vector. So we kind of we take we take a pair of feature out computes the transformation based on them and then take all others in order to evaluate how good this proposed transformation was so super can repeat this process until I am until I let's say happier or a good pose that's been found. stereo camera and the in this case the the camera is looking downward to me of the IMU on top we know the gravity vector and so this eliminates the roll directionally cursus will change the gravity vectors and the pitch direction. So by knowing thegravity vector I kind of get rid of the roll in the pitch and this reduces my problem from six dimensions to four dimensions for every node or for every camera pose which makes my life easier and therefore it is kind of exploited here. Based on that based on that I can buy a triangulation compute where are the points in the 3d space. is a procedure which is very very similar to Rancic it's actually a variant of good sake I think which was used here but this is just you you sample a few parameters that you need in order to compute the solution and then use all other informations to evaluate this solution. Then you try that multiple times and see how often do we find a consistent match. That's the way this works this technique is used in three different ways in this approach the first one is for for visual odometry so there is no wheel encoder on the camera. an existing part of the environment has a good estimate where it is that is what we refer to as localization and the last part which is loop clothing so given I kind of I don't know where I am some a large uncertainty and I you can use this approach to see how well do the features that I see at the moment mattress features have seen in the past and try to find an alignment for this this is a good alignment you may accept that or you may try this for a couple of consecutive frames that not just kind of one bad image screws up everything. Tasker builds a map online and use the map in order to make navigation decisions of where it should actually go. The platform looks for potential loop closures based on the uncertainty and then it tries to find them and collects a few of them groups M sees that there are groups of people in a certain area. The system then tries to close the loops as early as possible so don't let the uncertainty grow so much and the area smaller it's more likely did you find a match the other thing is you could simply cover the whole area. talk which I Neff was more over more whatever like wait overview about how different approaches work was not going to too many details. Second part of the talk today I would like to talk about ambiguities in the environment and what are good ways for dealing with them. How can we actually build accurate maps consistent maps of the environment so they are are so or the main assumption here is not we simply ignore all n big you T's and say the environment has no ambiguisms and I just consider they are none of them. There are multiple hypotheses how it can match inside and they overlap therefore it's local the other ones non-overlapping its global and this is this is our overlapping matches is global so I don't know how this a fits in here so does this guy over here fits this one this one or this one so either here here or here simply something I it's what V is it's just kind of V times a constant C and just to determine this constant so that this expression is maximized but V is already given so that's that can be easily done. hard for me to you to to identify and this is also called what's called the picket fence problem so good offense you seem you don't know which part of the fence matches to what you see so far it's a very very long repetitive structure and these are things where you also don't want to add a constraint the curses simply do not know is this is this locally ambiguous or not if it is just really don't wants toAdd a constraint we say can I say it's either here here or here what you could do is you could use the max mixture approach. The key trick in here is we have a large number of constraints pairwise constraints between nodes. The goal is that among one group within one group they all consistent with each other. This is done with a single graph partitioning approach this was kind of the techniques there the technical - Olson which uses this and yeah again so regarding the the position uncertainty of the platform the higher the uncertainty is in an area the moon bear I need to know the area in order to make the decision easier is this a global is their global. these are those add edges which result from odometry or incremental scan matching if I start from this node over here I can take my little madama tree constrain to go here I take my constraint HJ to jump into the second trajectory for the point in time when I visited the place a second time move along the odometry again and then go back kind of with the inverted H I and go back to the same place. If I have this kind of loop of constraints if they are all perfect and agree and consistent I should add up at an identity transformation if I concatenate all of them. and if you don't understand what the matrix means it will be hard so every entry of this matrix IJ tell us how well do hypothesis hypothesis J agree with each other just looking to this this pair of it's just a pairwise consistency mention the small if they're small Wireless in there I mean they don't agree they are high values and Daisy but they agree that may be good again so the goal is just to find this vector and then later on find a way and how can we determined what V this vector be all right. once once in this vector and I will get a high score if I have ever two groups in there I have I get one. I get scores among the groups but not between each other so we get and I divide by again a large number of apostasy. So we get a small value so I have this function just high values for both elements and low values for bad hypotheses so what can I do again treated as an optimization problem I try to find the vector B which maximizes this fraction that's exactly what is done. that might seem a topological grouping is done in a good in a fair manner and the includes all the relevant constraints but under this assumption that's exactly what I get out here so what I do is I take compute the first eigen value in the second eigenvalue and I compare them. If the solution 1 is locally unambiguous that means there is no picket-fence problem where is the high probability of course still may be the case I made a decision but that's my assumption here what have they need to do it I need to discretize V 1 to 0. Ambiguity or not it's kind of if the uncertainty lips of speakers need to seen all that area to make this is no that's the only place where I actually can match and about the special clustering technique for the original paper where you find all the information here's the work back in also recognizing places using spectral clustered local matches. This is exactly the approach that I presented here and actually a couple of the slides that I use in here or of the images material at least comes from a tin Olsen.

ROUGE-1: 31.12, ROUGE-2: 30.31, ROUGE-L: 27.87
BERTScore: 68.32

==============================================
==================== [29/100] ====================
Summary:
Marketing is about four things creating communicating delivering and exchanging value. The first thing is to identify an unmet need that consumers have. The key message in our advertising is going to talk about quality. We have to provide proof in our commercials in our print ads on our website. We need to identify we need to determine a need that is not being met in the market so it's not like what are some of the problems that they have when cooking or baking and what do you think they're going to say food sticks to the pot. for something to be of a good value it doesn't need to be a low price it could be a high price but it's a very high quality and it has a lot of benefits do you agree who could explain that further good tell us your name theresa bad um i mean would you consider that clothing yes go ahead like so like i find people saying like buying a pair of jeans and zara is expensive because there's like 65 but they last longer than buying these at all maybe unless unless. marketing is about creating communicating delivering and exchanging value the way that we communicate the value is through the brand. The brand is what's wrapped around the product so all products in a given category have the same generic functionality which is transportation. What makes one car different from another is the brand so every car is wrapped in a brand and we're going to talk more about perceptual maps where our brand is positioned in the marketplace relative to our competitors. We could look at that through market research to understand the perceptions that consumers have for our brand importantly relative to other brands. You're not going to be able to have a commercial that's going to resonate with everybody in your target audience or more specifically with your target market so he said all men so when you're showing a commercial do you think that those men that are seeing the commercial between the ages of 18 and 29 want to see somebody in the commercial who's 60 or 65 that's probably not something that's gonna resonate with them. A thousand can be statistically significant a thousand to fifteen hundred but in most categories in the united states it doesn't need to be more than that. being met so what would be a good example how about shampoo i know right but an unmet need is a shampoo that is safe for hair that's what that's dry or that that's curly oily that's perm. The way we identify the need is how guess guess guess oh you're not they're not good at guessing that's not good for exam day you got to be good guessers supply and demand right the way we're going to determine the need  is through marketing research. complete that questionnaire now if 350 million people complete the questionnaire that's called the what census a census is when a hundred percent of the population participates in the research only the government does that they're really the only one that could afford to do that. What we're trying to do is get a representative random sample from that population so it doesn't need to be 350 million how many does it need toBe 349 million 300 million 200 million what do you think a quarter of what's the number how many is that?

ROUGE-1: 32.73, ROUGE-2: 31.83, ROUGE-L: 27.05
BERTScore: 60.67

==============================================
==================== [30/100] ====================
Summary:
Bentham's principle is "the greatest happiness of the greatest number" He thinks all utility is quantifiable. Bentham allows interpersonal comparisons of utility. If you imagine a status quo, a perfectly egalitarian world in which each person has six units of utility, you can start asking yourself, "Well, let's imagine if we could redistribute things." What would that mean as far as Bentham's doctrine is concerned? It's a potential departure from the status quo the utility monster talked about last time. Prof: In the first instance we say that utility is quantifiable and expressible through money. Then related to that, we can work with a doctrine of revealed preference. We can vary the price that we charge admission for the course. And we could even influence your behavior without actually changing your preferences. Your enjoyment from coming or not coming to class wouldn't change, but your behavior would change if we varied the price. Student: Diminishing marginal utility, the principle of diminishing marginal utility of all good things. Professor Ian Shapiro: Bentham was a fairly radical guy. He was a supporter of democracy, which was a radical thing at that time. But he wasn't as egalitarian sticker, he's saying the rich will burn their crops before giving them to the poor, and that is a common argument in politics. Shapiro: In principle absolute equality would maximize the greatest happiness of the greatest number, but in fact if a government set out to do that, the rich would rebel. It seems to allow ethnic cleansing, even genocide, all justified on the grounds this is maximizing the total utility of society. Econ 101: How many of you have done ECON 101, the first econ course? Yeah, so what is the principle that would tell you if you have no food and I give you a loaf of bread, your utility goes up a lot to say it would have to be pairs of shoes, right? Student: Yeah, I guess. Prof: Okay, but that's a great example to start us off on this. What else? Anything else anyone might find problematic? yeah, over here. diminishing marginal utility says that this line will get flatter and flatter. Each new Porsche is less valuable to you than the previous Porsche. If we just kept giving you lots of right shoes, there'd be a problem. So it's the best assumption you can make given that you've got to assume something. But now, and now I want to come back to the sophisticated point that was made in the middle at the back there a few minutes ago, when you made the point that the principle of diminishing marginal utility is true. just need more money to get the same amount of happiness. For Donald Trump to get more utility, you have to give him a huge amount of new money. So the more money you have, the more you will want in order to of diminishing marginal utility. So we should take the dollar from Trump and we should give it to the bag lady, and the greatest happiness of the greatest number will have increased, right? But then maybe we shouldtake another dollar, shouldn't we? I mean it worked the first time, so we shouldn't stop. When are we going to stop? Prof: If you had one and I said, "I'll give you my one, it's right out there," you wouldn't want it? Student: It's not that I wouldn't. want it, but maybe the utility for the second one in some cases would be more than the utility. for the first one. Prof: Okay, so it's a possibility. Any other examples of where this becomes problematic? I mean, think about beer. One beer increases your utility a lot. The next, and the next and the fourteenth. Spitzer's integrity is blown it's not like there's some--it's a binary good. People either think he's either a hypocrite or he's not, it' a binary thing. So there might be some goods like integrity that are not easily capture-able in this logic. We should put that out there, but yeah, over here? Student: What about health? It's not quite binary because you can be in medium health, but I think it would be pretty useful to be healthy and then super healthy, ad infinitum. start to think about the utility that people at the bottom of the social order derive from a particular good, versus the utility of those at the top. Let's suppose a two-person society, again, and let's suppose it consists of Donald Trump and a homeless woman living out of a left luggage locker in Grand Central Station. And the question is, should we take a dollar from Trump and give it to the bag lady? What? Should we? Yes? No? How many think yes? Okay, yeah, almost everybody. as all that, and he wanted to temper the downward redistribution that flows from his principle, and so he makes a distinction between what he refers to as "absolute" and "practical" equality. When have we passed the point of practical equality, to use Bentham's terminology? Are we close to it? Have we gone by it? are we nowhere near it? There have been periods in our history when we've had top marginal tax rates of 90 percent, right? Reagan thought a topmarginal tax rate of 40 percent was beyond the point. because it's where you start to see our old friend the workmanship ideal creeping by the backdoor into utilitarianism. Bentham says, "Law does not say to man, Work and I will reward you but it says: Labour, and by stopping the hand that would take them from you, I will ensure you the fruits of your labour" So another way of thinking about this is, that Bentham's idea of the state is essentially regulatory. It stays the hand of somebody else who would steal your goods, but the government cannot itself create utility. Ronald Reagan came in and said, "If we cut taxes, the pie will get bigger for all and they'll be actually more revenue," and so utilitarianism says do it. And the Democrats say, "No, they won't," and it's an empirical argument. And you will find, if you go back now and look at what happened during the 1980s, perfectly credible economists will line up on both sides because they cut the taxes, but, of course, eight other things happened as well that affect the macro-economy. classical to what we're going to all neoclassical utilitarianism is a subject with which I will begin on Wednesday. See you then for the next installment of this week's Daily Discussion, which will focus on the role of utilitarianism in the development of the modern world. Back to Mail Online home. back to the page you came from. Click here to read the first installment of the Daily Discussion. Follow us on Twitter @CNNOpinion and @jennifer_newton.

ROUGE-1: 32.63, ROUGE-2: 30.60, ROUGE-L: 24.13
BERTScore: 60.15

==============================================
==================== [31/100] ====================
Summary:
Lipids are mostly hydrophobic, which can also be referred to as lipophilic. Water is a good hydrogen bond donor and acceptor, so there will be hydrogen bonding. Salts are going to need ways to get in and out of cells. Neurotransmitters, such as this, this is GABA, or gamma aminobutyric acid. It just can't get through without a transporter of some kind. It's actually proteins that end up doing the heavy lifting of the transport processes. Semi-permeable membranes are made up through the non-covalent association of phospholipid monomer units. These molecules assemble into supramolecular structures that form the boundaries of your cells. If they were fully permeable, anything could come and go and they wouldn't be much use frankly. So let's take a look at the boundary here. It's like a-- it looks like a ballerina or something. Anyone want to tell me what the answer is, and why? Yeah, did you-- are you-- yeah. Other small hydrophobic molecules can pass readily in and out through the semi-permeable barrier, but other things, things that are charged, Things that are big, need a different mechanism. Collagen is a structural protein that provides mechanical support for tissues. In the next class, we'll talk about transporters and enzymes, and as we move on to signaling, things like receptors and membrane proteins and so on. And we will see later on how proteins provide the opportunities to cargo things into cells or out of cells, even very large entities. The amino acids that are encoded in our proteins are all what are known as alpha amino acids. The amino acids are also chiral, but you'll learn more than you ever wanted to know about chirality in 512. Proline is a little odd because its side chain is kind of in a cyclic structure, and towards the end of the class, I'll talk to you about collagen, whose structure is totally dependent on the involvement of proline. And then the last sorts of unusual amino acid is cysteine. Compact, globular structure that's functional is encoded in that primary sequence. The primary sequence determines the fold, and it's the fold of the protein that mandates its function. It's not simple because what you're doing is you're solving a massive energy diagram, where as you fold a structure up, you're trying to maximize all those non-covalent forces for maximum thermodynamic stability, right? It's kind of a three-dimensional puzzle where they are maximizing interactions. When we write out peptides, we always write them N to C. If you don't always remember to write things in this order, and you tell your friend, oh, go and get this peptide made, they'll make the wrong peptide. When I write the MIT peptide, I write M first, I second, T third. If I wrote TIM, it would be a completely different chemical structure with different chemical properties, so the directionality is important to understand. Protein tertiary structure is not complete spaghetti. It's like spaghetti with little bits that haven't been cooked. What I'm showing you here is what's known as the alpha helix. It is an ordered structure exclusively made up of hydrogen-bonding interactions of the peptide backbone. And you can look at this helical structure. It’s a continuous strand of peptide, but there are hydrogen bonds between COs and NHs. And there are a couple of major forms of secondary structure. showing it in detail, they might show it as a cylinder, so you might need to pick that out of a structure. All right, so we've seen primary. Secondary is just with backbone. And things start to get much more interesting when we get to tertiary structure. Tertiary structure is enabled by all these other interactions, electrostatic, hydrogen bonding, hydrophobic forces, that can be put to use. OK, so this is like taking your very extended stored of polymer, knowing there are different kinks in it, because of the backbone bonds, but folding it up in a structure that maximizes the opportunity. GB1 is a very small protein that holds reversibly under appropriate conditions. This is a simulation. It's not looking at anything by spectroscopy or in solution or anything like that. This might describe the folding possibilities of that small motif. And it does that for about 30-- 60 seconds of the simulations, so I made a point to myself to take it to the next level and do it on a larger screen. And what I'm going to do is just show you for a few seconds, you know, this thing's like trying to find its thermodynamic minimum. is the green fluorescent protein, which is a cylindrical structure made up of anti-parallel beta sheets. And then in some cases, proteins may be a mixture of a secondary structure elements. Here it's a little hard to tell. This is triose phosphate isomerase, but if you look down it, you can see the helices. And these rolled together into a three helix bundle that has a fibrillous structure, and then all these structures come together to make the macromolecular structure that is collagen. A single amino acid change in the primary sequence of collagen can destabilize the structure, so it is no longer viable. The disease type I'm going to talk to you about is a set of diseases known as collagenopathies, and the particular one is called osteogenesis imperfecta. A lot of babies with this defect can't even be born through the birth canal because it would crush the bones, and many of them don't survive very long at all. So I think that's a good place to stop and I'll pick up next time with hemoglobin.

ROUGE-1: 22.59, ROUGE-2: 21.60, ROUGE-L: 17.55
BERTScore: 59.36

==============================================
==================== [32/100] ====================
Summary:
Professor: angular momentum is just some geometric constant times the angular momentum. If you send an electron through a Stern Gerlach Apparatus, it always hits one of two spots. This is a different form of angular momentum, which is purely half integer, and we call that spin. The legacy of these little L equals 1/2 states, is that they represent an internal form of. angular momentum that only exists quantum mechanically, that you would have never noticed classically. Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free. The value of a wave function must be equal to minus itself at some point. The energy depends on l but not on m in the energy eigenvalue equation. There's always a special point anytime you have a rotational symmetry around some particular point. So, the wave function, in fact, is non-zero, which is the physical thing is the distribution, which the physical norm is multiplied by 1.2pi to get a 0 at the origin. The origin is not the same as saying that little u has a 0, but it gets by 1 over 1 over r. to the very first lecture, and so, we'll do this in more detail, but I'm going to quickly tell you-- imagine take a magnet, a little, tiny bar magnet. In fact, well, imagine you take a little bar magnet with some little magnetization, and you send it through a region that has a gradient for magnetic field. If there's a gradient-- so you know that a magnet wants to anti-align with the nearby magnet, north-south wants to go to south-north. So, you can't put a force on the magnet, but if you have a gradient of a magnetic field, then one end a dipole-- one end of your magnet-- can feel a stronger effective torque then the other guy. And you can get a net force. Take an electron and send it through a magnetic field gradient. You want to see whether the electron is a little rotating thing or not. If it gets deflected, you will have measured the magnetic moment. The magnetic moment must be proportional to the angular momentum. But, the universe is rotationally invariant. So, in any direction if I measure the angular on this one-- this is also known as 1 over r dr squared r. And this is nothing other than L squared-- except for the factor of h bar upon i-- but if it's squared, it's minus 1 upon h bar squared. measured the magnetic moment, you'll have measured the angular momentum. If the electron weren't rotating, it would just go straight through. And so, the observation that the electron must carry some intrinsic form of angular momentum with one of two values, neither of which is 0, was actually an experimental observation-- quasi-experimental observation-- long before it was understood exactly how to connect this stuff. The intent of the experiment wasn't to solve-- AUDIENCE: No, no. The experiment was this-- there are the spectrum-- Well, I'll tell you what the experiment was in a minute. know of a fundamental particle. If super symmetry is true, then there must be a particle called a gravitino, which would be fundamental, and would have. spin 3/2, and four states, but that hasn't been observed, yet. So, we still need a reason for why the hydrogen system, quantum mechanically, is stable. We'll talk about that a little more when we talk about hydrogen, but it was observed and deduced from experiment before it was understood there was such a physical quantity. we're gonna talk about real, physical systems in three dimensions. And as we'll discover, it's basically the same as in one dimension, we just have to write down more symbols. But the content is all the same. So, the energy for this is p squared upon 2m, plus a potential, which is a function only of the radial distance. And along the way, we'll solve a toy model for hydrogen. And when you go through the equation, you get is minus minus some dimensionless energy, epsilon. guys cancel, right? 1 over r times dr. So, why is this a particularly useful form? We'll see that in just a minute. Any time you see a differential equation that has this form-- two derivatives, plus 1 over R a derivative-- you know you can play some game like this. You can do this sort of thing generally by rescaling r. And more, if you have a differential. equation that looks like that, that's something like a. differential equation with a derivative. over r? PROFESSOR: Oh shoot! Yes, that's supposed to be one of our-- Thank you. Yes, thank you for that typo correction. Thanks OK. So, anytime we have a system which is rotationally. invariant, we can write the energy operator in this fashion. And now, you see something really lovely, which is that this only depends on r. This depends on the angular coordinates, but only insofar. as it depends on L squared. Professor: "I'm going to do it in an abridged form, and it's a good thing for recitation" "So, in this spherical well, what's the potential here? So maybe here's a spherical well. And if it's 0, then it's an infinite potential. And the potential is going to be v or v effective, just v of r" "And finally, can r be negative? No. It's defined from 0 to infinity. So, our effective potential is the sum of these contributions" AUDIENCE: [INAUDIBLE]. PROFESSOR: Which one? AUDIENCE: Middle. Middle.PROFESSor: Up, up. Right there! Up! There. AUDience: [CHATTER] There! PROFessor: Excellent, so the thing that isn't here, would have a bar under it. Ah! You wouldn't think it would be so hard. OK, god, oh! That's horrible! Sorry guys, that notation is not obvious. My apologies. name for this. If you're spinning, and you pull in your arms, you have to do work, right? You have to pull those guys in. You're increasing your kinetic energy due to conservation of angular momentum. As you approach the origin from any direction, the function is going like 1 over r, OK, so it's rotational symmetry. But what the rotational degeneracy is saying is, look, if you've got some total angular momentum, the energy can't possibly depend on whether most of it's in Z. The wave function, phi sub E, which goes near r equals 0, like u of r over r. So, what should be true of u? Can u diverge? Is that physical? Does u have to vanish? Can it take a constant value? So, I've given you a hint by telling you that I want to think about there being an infinite potential, but why? Why is that the right thing to do? Well, imagine U of r went to a constantvalue near the origin. That's maybe not so bad. case-- when there's zero angular momentum, little l equals 0. Eu sub 0 is equal to h bar squared upon 2m. This is saying that the energy, a constant, times u is two derivatives times this constant. So, u0 can be written as a cosine of kx-- or sorry-- kr plus b sine ofkr. And so, this is just like the 1D system. It's just exactly like when the 1 D system. Professor: We can build something with units of a radius. We have the mass, which has units of mass. And from this, it's easy to see that we can build a characteristic energy by taking e squared and dividing it by this length scale. And if you actually take mu e to the 4th over h bar squared, this is off by, unfortunately, a factor of 4. This is equal to 4 times the binding energy, which is also called the Rydberg constant. Professor: "Dimensional analysis gives you this answer without ever touching that equation" "I just want to emphasize how much you get just from doing dimensional analysis" "This is something you should either do in recitation, or see-- go through-- on your own" "It's not different in any deep way, but it's a little bit easier. This is gonna be the easier way to deal with this, because I don't have toDeal with any stupid constant" "So, when rho goes to infinity, which terms dominate? Well, this is not terribly important" because solving it is a sort of involved undertaking. So, we had this differential equation-- this guy-- and we want to solve it. We extracted the overall asymptotic form, at infinity and at the origin, to get a nice regular differential equation that didn't have any funny singularities. And we do a series expansion. And the series expansion has a solution, which is a sub j plus 1. And in order for this terminate, we must have that some aj max plus 1 is equal to 0. As rho goes to 0, asymptotic analysis is gonna tell us that u goes like rho. Well, two derivatives is a constant. And if we get that d rho squared plus u, rho go to infinity, these two terms dominate.dominate. If l is equal to0, then this is the only term that survives, so we'd better make sure that that behaves gracefully. For normalize-ability, I picked the minus, I could've picked the plus, that would've been divergent.

ROUGE-1: 22.93, ROUGE-2: 21.73, ROUGE-L: 15.96
BERTScore: 61.96

==============================================
==================== [33/100] ====================
Summary:
The 60002 course is the second half of the 600 program. The main topic is computational models. The final exam will be based upon all of the above. The lectures will be a bit faster paced than 60001. The course is really less about programming and more about dipping your toe into the exotic world of data science. It will be taught in Python, but there will be additional bits of Python as well as some comments about software engineering, how to structure your code, more emphasis in using packages. Science is moving out of the wet lab and into the computer. Increasingly, there is an increasing reliance on computation rather than traditional experimentation. We'll talk about three kinds of models-- optimization models, statistical models, and simulation models. An optimization model is a very simple thing. We start with an optimization model and then go on to other types of models, such as statistical models or simulation models, which can be used to predict the future of a given area of science. For example, a climate change model. We can build models that sort of explain how the climate has changed over the millennia. with an objective function that's either to be maximized or minimized. We then often have to layer on top of that objective function a set of constraints, sometimes empty, that we have to obey. So for, example, if I'm going from New York to Boston, I might want to find a route by car or plane or train that minimizes the total travel time. A greedy algorithm, as we'll see, is not guaranteed to give me the best answer. Let's talk about a specific optimization problem called the knapsack problem. There's more stuff than you can carry, and you have to choose which stuff to take and which to leave behind. The 0/1 knapsack problem means you either take the object or you don't. But you can solve it with what's called a greedy algorithm, and we'll talk much more about this as we go forward. There is no algorithm that provides an exact solution to this problem whose worst case running time is not exponential in the number of items it can hold. John Guttrag: brute force can't solve optimization problems where n is something closer to 1,000, sometimes even a million. On Wednesday, we'll talk about how do you actually guarantee finding an optimal solution in a better way than brute force. See you on Wednesday at 9 p.m. ET on "This is Life with John Guttrach" and "This Is Life With John Gut Trach" on CNN.com/ HLN, 8 p. m. ET. Got this class food. I have a getValue, a getCost, density, which is going to be the value divided by the cost, and then a string representation. Then I'm going to have a function called buildMenu, which will take in a list of names. And it will build the menu. And I build each food by giving it its name, its value, and its caloric content. Now comes the fun part. Here is an implementation of a greedy algorithm. I called it a flexible greedy primarily because of this key function over here. actually works? I hope not because I think it does work. Let's ask the next question. How efficient do we think it is? What is the efficiency of this algorithm? Let's see where the time goes. So I deleted the comment, so we'd have a little more room in the slide. Who wants to make a guess? By the way, this is the question. So please go answer it. We'll see how people do. But we can think about it as well together. See who remembers. Lambda is used to create an anonymous function, anonymous in the sense that it has no name. Lambda does is it builds a function that evaluates that expression on those parameters and returns the result of evaluating the expression. So I'm taking the function getCost from the class food, and I'm passing it the parameter x, which is going to be what? What's the type of the argument to getCost? We'll go back and we'll look at it. We do have a tradition in this class that people who answer questions correctly get rewarded with food. The problem with greedy algorithms is that you can get stuck at a local optimal point and not get to the best one. Let's say I'm feeling expansive. I don't want to go down. So I'm here and I'm happy. On the other hand, if I had gone here for my first step, then my next step up would take me up, up,. I'd get to here, and I'd stop and say, OK, no way to go but down. I'm done.

ROUGE-1: 27.05, ROUGE-2: 24.34, ROUGE-L: 21.79
BERTScore: 57.64

==============================================
==================== [34/100] ====================
Summary:
Expectations play a huge role in economics, says Ricardo Caballero. He says they play a big role in the decision of all economic actors, including investors, consumers and firms. He gives a shortcut to think about the role of expectations in the kind of models we have already discussed. CaballERO: I want to revisit the consumption function and the investment function, now taking into account expectations and motivate how you should really think about consumption and investment in a more realistic model than we have been discussing. Milton Friedman called it the permanent income theory of consumption. It says what really matters to you in a consumption decision is what you expect to get on average during your lifetime. How wealthy you are will pin down more or less the consumption you have more than your current income. The very rich often have no income, [CHUCKLES] at least labor income. All the income comes from returns on assets, and they mostly borrow against that. But in any event, what really brings out your consumption is your wealth, not the current flow of income. expect to inherit or whatever, minus the debts you have. So very much as we discussed in the previous lecture in the context of asset pricing, the expected present discounted value of the cash flows of all the assets you have, that's your financial wealth. And that's important. You have more financial wealth, even if you have no income today, you will probably borrow against that wealth to the extent that you can. And so you're going to fund the consumption, which is above your current income just because you have morefinancial wealth. Investments that give you a return, a quick return, are worth more than things that have a pay-off in the very long run. So the decision, for example, of buying a machine needs to look at the price of the machine right now and then at the expected present discounted value of the cash flows, OK? So suppose you buy a machine for a price. Let's normalize that price to 1. The first thing you need to know is, well, how long will this machine last. much or is not as optimistic as the firm is and so on. So it may not borrow-- the firm may not be able to borrow as much as it would want given how optimistic that particular firm is on its own project. And one way that firms use, actually, to get around financial constraints is simply by returning-- retaining their earnings, meaning they generate a cash flow, and they save. So, for example, if you have a fiscal contraction that leads to an anticipation of a big cut in interest rates in the future, that may be expansionary. firms, especially smaller firms, have deposits and cash flow and so on mostly because, if they get a good opportunity, they may face financial constraints. So if current activity is high, sales are high, firms are going to be less likely to be financially constrained. And that's the reason current profits also end. But if interest rates go up today, and I expect them to remain high for a long time, that's going to affect a lot more the present value of profits. time, because otherwise it would be irrelevant. If the Fed cuts the interest rate but doesn't persuade anyone that this rate will remain low in the future, then it is going to get very small effect on output. However, if we convince people that there will be future changes, that the rates will remain lower for a long time, that means that this IS now will shift to the right, OK? That's what we have here. So again, expectations mattered quite a bit. If you convince the markets that-- and the markets and consumers, households, and so on-- that you're cutting interest rates and that, with that, you'll be successful. but are the variables we expect of those-- are the values of respect for those variables in the future and, again, with the same sign. So if output-- so if taxes go up today, aggregate demand will decline, and output will decline. But if I expect future taxes to go up as well, then that's going to depress aggregate demand even more. That's the type of logic I want you to develop. So this is the IS in the same space I had before-- interest rate and output, current output. the static model. Changing government expenditure, same idea-- it will also move aggregate demand to the right. But will it do it by more or less? Well, think how government expenditure worked in the basic model. It increased aggregate demand, and that then led to a multiplier. And we got a lot more income and so on. Now, if we expect this government expenditure it to be temporary, that multiplier also will be a lot smaller because, yes, it will increase income, but people are not going to spend all their income today. Alan Greenspan is known as one of the biggest central bankers that the US has, at least in recent memory. He went through a period which was called-- was known as the Greenspan conundrum. He kept hiking interest rates, but the loan rates kept coming down. So he couldn't cool off the economy. There was no way around that because they couldn't persuade the markets that this would be a long-lasting effect. The reason was a different one. It happens that, at the same time, you had China sending massive capital flows to the US. reason, the fiscal deficit, the perception of fiscal deficit,. was really dragging the economy down because people didn't know when there could be a financial crisis in the near future. Then you can get a situation in which the contraction, fiscal contraction today, improves the perception. of stability of the country in the future, which in turn may increase expected future income and be expansionary. So most of the fiscal contractions are contractionary. But there are some famous episodes of what are called expansionary fiscal contracts.

ROUGE-1: 25.02, ROUGE-2: 24.08, ROUGE-L: 21.53
BERTScore: 64.38

==============================================
==================== [35/100] ====================
Summary:
Fiscal policy has three different views of how well it works or how well pieces of paper to buy stuff at a than it does at sea but there's the same amount of output similarly if we're in a situation like this so here we would use expansionary fiscal policy we could also be in a boom. Once you start spending that money it's hard for people to stop is that look you got this deficit right here but you're eventually gonna have to increase my taxes to pay it back the bonds do eventually come due you can't continue to borrow money forever. it doesn't work so let's kind of look at them in the atas model right we have this Keynesian view. We have our kind of a classical view and we have our what would be called supply-side view. There's more views than that obviously but we're just gonna look at these these three stir them the three main ones. We're at some full employment level of output YF and here we are initially we're in some recession right we're at output level say y1 and some price level just call it P l1. exactly right with that Marg propensity to consume and thatMarg propensity to import and knowing all of that lets us know the multiplier and we know how large autonomous expenditures are and all that stuff we can essentially just it's just an Excel spreadsheet you just plug in the number that you want for these changes and taxes or these changes in government spending now you've got a political problem here. Once you actually start spending money it becomes difficult to take money away once you start giving people something taking out money away now becomes very very difficult so politically speaking it's easy to do this it's difficult to this. money aside for college that was the loanable funds market they're putting money in there for savings they're saving it they're the suppliers you guys are borrowing money to go to school and you're now demanders you're the ones taking money out. When the interest rate goes - I - what happens to private borrowing what is it at now do it q2 is the total that's private and public how much is private borrowing spending the money they want their money right but we could even make this guy go back down to a hundred still have this deficit. if this guy's going down what does that mean for the long-run do what well of investments decreasing what else is decreasing investment goes down we don't build the factory your interest rates are too high I'm not building a factory in 2017. If investments going down the quantity and quality of our resources are smaller the capital stock is smaller the future output will be smaller you will have less stuff in the future and here's what's really interesting how are people going to know that here they are on the red line and they could have been at the blue line. Paige: There's some tax rate here between zero and 100 that makes tax revenue as large as it can possibly be. Paige: Every single Monday that I come to work I'm essentially not doing anything I do all the day's work I deal with all of the kids. If you go labor every Monday right Monday Tuesday Wednesday Thursday Friday 20 40 60 80 100 % I don't teach on Saturdays all right and so if they're taking 20% it's like they're like forget it it's not worth it. run doesn't really matter it's the long run stuff that matters is their view and there's some evidence that people respond to these incentives right. Which view is Keynesian new classical supply I don't know if you figure it out you let me know I'll write the paper and win the Nobel Prize and I can go to Sweden and get the nice little 1.2 million dollars and the nicelittle bitty gold medallion that I couldn't wear around my neck for the rest of life at such a such recession.

ROUGE-1: 24.48, ROUGE-2: 23.65, ROUGE-L: 20.31
BERTScore: 66.61

==============================================
==================== [36/100] ====================
Summary:
Professor: We're going to look at this unitary time evolution and calculate this operator u, given the Hamiltonian. Then we will look at the Heisenberg picture of quantum mechanics. Professor: We'll find the He Eisenberg equations of motion and solve them for a particular case. And then we are going to define a new way of thinking about this, which is called the Heisinger picture of the quantum mechanics, or the Schrodinger picture. It's a pretty useful way of seeing things and makes the relation between classical mechanics and quantum mechanics more obvious. The Schrodinger equation can be solved in three ways. When h is time independent, h has a little time dependence. This is an idea, the sign to make it possible for you to solve the equation. So you could have Hamiltonians that are time dependent, but still have a simplifying virtue. For example, the spin in a magnetic field is minus gamma B dot the spin. So here it is. U of t and t0. T it's called the time ordered exponential. This operator does something to the exponential function. U of t t0 is going to be e to the minus iHt over h bar, some constant matrix. When t is equal to t0, this matrix becomes the unit matrix. And therefore from here, U0 is the inverse of this matrix, which is nothing else but e. And that's our solution. There's very little to add to this. We discussed that in recitation on Thursday. This unitary operator you've been seeing that from the beginning of the course in some sense. The Heisenberg operator is actually time independent. It just doesn't depend on time. So if you have a magnetic field that is fixed in one direction but change in time, you can have a situation where your Hamiltonian is time dependent, but still at different times it commutes. But later on as we do nuclear magnetic resonance, we will have the more interesting case in which the magnetic field rotates. So let me call this quantity R of t. And then, armed with our unitary operator, we know that As is here. the derivative of this quantity with respect to time. When you differentiate an integral, you get just the integrand evaluated at the time represented by the upper argument of the upper limit of integration. So that's basically our solution for H and for the unitary operator U in terms of H. And what we're going to do now is turn to the Heisenberg picture of quantum mechanics. Yes, questions? AUDIENCE: Why does R dot [INAUDIBLE]? PROFESSOR: Because that's really a property of integrals. of this, you will get H times that thing. So since it's a power series, you'll differentiate the first term, and you'll get the right thing. Then the second term and you will start getting everything that you need. It's reassuring that something like this success, but in general, you would want to be able to do all these integrals and to sum them up. So it's of limited usefulness. But when you have a practical problem, generally that's not the way you solve it. We will try to figure out the solution some other way. T is not an operator in the usual sense of quantum mechanics or anything like that. When t is equal to 0, U of t-- of 0 0 is the operator propagates no state, so it's equal to the identity. So this is a wonderful relation that tell us you that time equals 0 the two operators are really the same. And another simple remark. If you have the unit operators in the Schrodinger picture, what is the unit Operator in the Heisenberg picture? Well, it would be exponential of e to the minus iHt over t with the x operator and another exponential. solutions? PROFESSOR: Yes, pretty much. Because at the end of the day, this is a first order matrix differential equation. So it's a collection of first order differential equations for every element of a matrix. It's pretty much the same as you have before. If you know the operator at any time, initial time, with the differential equation you know it at a little bit time later. So the operator is completely determined if you knows it initially and the differential equations. So I think it's completely analogous. U t 0 dagger 1 U t 0. U dagger with U is 1. This is a 1 Schrodinger, and therefore it's the same operator. So the unit operator is the same. It just doesn't change whatsoever. OK, so that's good. But there are some cases, as we will see immediately, in which some operators are the same in the two pictures. So we will have an interesting question, in fact, whether the Heisenberg Hamiltonian is equal to theSchrodinger Hamiltonian. We'll answer that very soon. C Heisenberg is just A Heisenber times B Heisenburg. So there's a nice correspondence between those operators. Also you can do is for commutators. If the Hamiltonian is time independent, does that work for any operator that commutes with the Hamiltonians? PROFESSOR: Hamiltonians is [INAUDIBLE]. AUDIENCE: Because then you can push the operator just through the exponential of the Hamiltonia. PROFessor: Yeah, we'll see things like that. Expectation values are the same as Schrodinger expectation values, except that the states are taking up time. When you're computing the expectation value of the Heisenberg operator, you're using the time equals 0 version of the states. So this is something very useful and we'll need it. The.whole thing is the same. We right it in the bottom, but we mean the top equation. And we use it that way. And now we know that the expectation values of Schrodingers operators are the. same as the expectationvalue of their Heisenburg counterparts. P Heisenberg times the commutator of X and P, which is ih bar times a factor of 2. And then what do we get? The ih there and ih cancels. Well, it actually looks like an equation in classical mechanics. So how do we solve for them now? Well, you sort of have to try the kind of things that you would do classically. Take a second derivative of this equation. And the dPh dt would be [INAUDIBLE] 1 over m times minus m omega squared Xh. still doesn't look like what you would want, does it? No, because you haven't used the time equals 0 conditions. At time equals0, the Heisenberg operators are identical they to the Schrodinger operators. So any expectation value of any power of X and P that you will want to find its time dependence, just put those Heisenburg operators, and you will calculate things with states at time equals zero. It will become very easy. So let's see. I hope I didn't make mistakes. m omega squared cosine squared omega t X squared. And the cross term. Plus 1/2 m omega squared over m omega times cosine omega. t sine omega t XP plus PX. Schrodinger Hamiltonian. So you confirm that this theoretical expectation is absolutely correct. And what's the meaning? You have the Heisenberg Hamiltonian written in terms of the He Eisenberg variables. But by the time you substitute these Heisenburg variables, you get the same result. variables in, it just becomes identical to the Schrodinger Hamiltonian. All right, so that's all for today. I hope to see in office hours in the coming days. Be here Wednesday 12:30, maybe 12:25 would be better, and we'll see you then. [APPLAUSE] We'll be back at 12:50 on Wednesday, and be here at 1:30 on Thursday. Back to Mail Online home. back to the page you came from.

ROUGE-1: 27.24, ROUGE-2: 25.60, ROUGE-L: 21.36
BERTScore: 66.87

==============================================
==================== [37/100] ====================
Summary:
Borat: I want to talk about my favorite part of the Second Discourse, a book that never grows old, that never fails to produce. I focused on a famous passage in which Rousseau claims it was the establishment of private property that was the true formation of civil society. But in fact, that's not really true. Rousseau understands that even for institutions like property and civil society to be possible there must be huge and important developments that go on or take place even prior to this. Rousseau's Second Discourse describes inequality as the result of amour-propre. The French term is untranslatable, but it is related to a range of psychological characteristics. Rousseau says the desire to be recognized and respected by others is at the root of our sense of justice, justice and justice. The desire for recognition or respect is a part of our feelings, beliefs, attitudes and opinions, he says. It is also related to something positive, in many ways, in the development of humanity. in such a state to feel pride or vanity that requires human sociability and requires the esteem of others. How could pride have arisen in a state of nature which on Hobbes' own account is solitary? Rousseau speculates about this and, again, this is part of his hypothetical or conjectural history. He speculates that amour-propre began to arise and develop as soon as people began to gather around a hut or a tree and to look at one another. Only a very few people, Rousseau writes, are capable of finding their way back to nature. gaze of another, and it is from that gaze, from the look or gaze of another that the passion of vanity was born. The one who sang or danced the best, the handsomest, the strongest, the most adroit or the most eloquent became the most highly regarded. From these first preferences were born vanity and contempt on the one hand and shame and envy on the other. Our own sentiment of self and existent comes entirely from the judgment, as he puts it, of those around us. Toleration in many ways is a liberal virtue because it requires us to distinguish between beliefs that we may take with the utmost seriousness in private life and yet bracket them in some way once we enter the public world. Rousseau's political philosophy begins, at least he believes, with the realistic or even empirical assumption that each individual has a deep rooted interest in securing the conditions of their own liberty. He does not presuppose altruism on the part of any human being or any other kind of self-other regarding characteristics. Rousseau: Amour-propre and society gave rise to the state of war. Do you remember that, about the cartoons of the prophet Muhammad and the outrage and the protests, often violent, that occurred about that? To some degree, I think, Rousseau would believe the protesters over those cartoons had a point. Their views were not being respected and to which you might say a Lockean or a liberal formulation of the problem or response would be, "Well, so what?" The task of government, according to Locke or the liberal view, is to ensure the respect.proportionate to the esteem in which he held himself. Government's job is not to impose a gag order on what can and cannot be said, he says. This is a respectable, sort of liberal line of thought going from Locke to John Stuart Mill, he adds. But there is something powerful and true about what the government is supposed to do, he writes. It is to protect you from harm and provide you with the freedom to practice what religion you like, consistent with others' freedom to do so too, he argues. The Danish prime minister has refused to apologize for the cartoon. Rousseau: Government is a con game that the rich play upon the poor. Political power simply helps to legitimize economic inequalities. Let us take men as they are, Rousseau says, following Machiavelli. He will not begin, he tells us, by making any heroic assumptions about human nature, he says. The Social Contract begins with itself in many ways as a utopia, an ideal city, in some respects an answer to the Calipolis of Plato's Republic. of preserving it and protecting it. Each of us has a desire to preserve his or her own freedom and that social order will be rational or just, that allows us to preserve that freedom. The state of nature quickly becomes a state of war based on conflicting desires and conflicting again means of liberty preservation. So how do we preserve our liberty without lapsing into anarchy? This is the question that the Social Contract sets out to answer and to which his formulation, his famous formulation of what he calls the general will, is the solution.

ROUGE-1: 26.45, ROUGE-2: 24.55, ROUGE-L: 20.70
BERTScore: 63.75

==============================================
==================== [38/100] ====================
Summary:
So I think I just spend like five minutes, just briefly review on the backpropagation last time. So I didn't have time to explain this figure, which I think probably would be useful as a high level summary of what's happening. This is how you define network and the loss function. So you start with some example x, and then you have some-- I guess, this is a matrix vector multiplication module, and you take x inner product multiplied with w and b. And then get some activation, the pre-activation. And you get some post activation. one of those three lemmas. If you know how to compute the derivative with respect to the output of some module, suppose this is a module, tau is the output. And now you can see what this does kind of like lemma are for. Those lemma, basically, are saying that if you know dg over the d tau, how do you computedg over da? And there's another lemma which says that ifYou know how. to compute dgover da, howDo you compute dG over dz? All of those lemma is about this kind of relationship. Some of the practical viewpoint of ML, like how do you relate to your model, what you have to do in this whole process. So far, we only talk about training. We have some examples, which we have seen when they are training data sets. And often, this is called test distribution. And then you evaluate what's the expected loss on this new test example. So this is a typical situation of overfitting. In the next 20 minutes, I will spend 20 minutes to talk about a new picture that is actually challenging. time. When you test, you find that your model is not as good as you thought before on the training set. Sometimes, it's probably a little worse. But generally, you shouldn't expect that your test performance is dramatically better than the training performance. So we're going to discuss what will happen if you change your model complexity, and whether in what cases, you may underfit. In what cases you may overfit, and what is the best response to that? So then what you-- In some sense, you care about two quantities. You care about the training loss and the gap. You want both of these two to be small. And typically, when l theta is big, there are two failure mode in some sense. One of the failure mode is called overfitting. The other is called underfitting. And whether you are in the overfitting regime or the underfitting regime, depends a lot on different things. And one kind of decision we are trying to discuss today is that what is the right model complexity. Bias is the best error or loss, you can get with even infinite data. Mathematically, one way to define a bias is that you can say this is the-- So bias is-- I guess, actually, there's some approximation here, depending on what exactly your model is. So the bias is something like this. And now let me talk about the variance. And here, there is -- I'll come back to the variance for this model. But here, the variance is, in some sense,you can say, it's not very important. Only the bias. OnlyThe bias is the way if I were to prove that test is equal to a bias plus variance. Bias is the distance from the [INAUDIBLE] model or something like that? I think that's pretty much-- so for this case, they pretty much are the same. So the bias, the trade-off, depends on, for example, how many data you have as well. So this bias and various is not don't think it's required for the exam or anything, but it's a relatively simple word if you're interested. But the intuition is still kind of fun. So if you don't care about what exactly definition of bias is. In the lecture notes, actually, there are some visualizations of the real models you're going to fit. So for linear models, I guess, you can see a bunch of properties. There's a large training error, training loss or training-- let's call it loss just for consistency. What's your prediction on cannot be mitigated by more data, as I said. And actually, it can also not beMitigated by less noise, even though there is-- and by less data data. the training data set. This is your prediction for this x. And you look at the distance between the prediction and the true label. The training error is pretty big. So this is underfitting, by our definition of underfitting because the tuning is already big. And now let's think about so what you should blame. Why the training is big? What's the culprit? The culprit, I would argue, is that it's just because no any linear model can fit your data. It's not just-- no anylinear model can work. And it's not because you don't even have enough data. well. So when in this kind of settings things happens, like you have the bias. So the bias is basically like it's saying that the reason why-- I don't know exactly why people call it bias in the very first time. But I think you can-- see kind of the relationship. So you are imposing a linear structure, but the true data is not linear. So it doesn't matter how many data you see, as long you insist that I just believe that this thing is linear, you're going to fail. So suppose you have a million data, roughly. There's a little bit fluctuation, of course. So now you want to fit a fifth-degree polynomial. What happens will be that this is probably not entirely obvious-- OK. What you really will fit, like if you minimize the error on the training data with this so many training examples, then what you will get is probably something like this. Maybe there are still some small fluctuations. It's not like necessarily matching exactly minima [INAUDIBLE] spurious patterns are the fluctuations in some sense. And so in other words, I think you are explaining the noise instead of the ground truth. How do I formulate this? Like one way to kind of formulate this a little bit more mathematically is that you can consider to redraw the samples. So you redraw some new samples with different spurious patterns. They are spurious because they are noise. And that's a good question. That's exactly what I'm going to talk about next. Same distribution. From the same distribution. Yeah. So like if you collect more data from-- yeah. So how do you know that you are having a large bias? You cannot really exactly know. When you don't know ground truth, so all of these are so far are for analysis purpose. When we don't knows I didn't even tell you what this is. I'll go back to come back to this. Are we [INAUDIBLE] For highly imbalanced data set? Maybe let's discuss this offline. of bias and variance first change, did you use different type of model [INAUDIBLE] So I think this figure, so this is the-- OK. You ask a good question. So probably, the best thing is to use quadratic. Quadratic is, in principle, expressive enough to express our data. So that's why quadRatic has small bias. And also, quadratics is probably, among all models, the most expressive of all models. But where the trade-off comes from, where the sweet spot is would depend on the ground truth. The phenomenon is called double descent. This phenomenon actually dates back to something like 1990. People realize that if you increase your model number of parameters even more, at some point, you will see that it will be like this. And it turns out that, actually, in many cases, what happens is that the test error will look like this, or increase, and it will decrease again. This is the so-called double descent phenomenon. It's about less mysterious these days like after people have studied this in the last five years very carefully. For most of the vision experiments, you are in this regime, where you have more parameters than a data point. This is something that is really like empirically irrelevant. So that's why stochastic gradient descent for linear models. So the existing algorithms underperform dramatically when it's close to d. So both these two peaks are basically like this. It's just that you are changing the axis in some sense. So this is also when n isclose to d, when the number of data points is close to number of features. people really care about it. And what I mean by that is that even within linear models, you can try to change the model complexity. So what that means is that you try to decide how many features you use. So you can start with only using one feature or two features like for example, in the house price, where you can use the square foot as the single feature, or you can collect a bunch of other features. So keep adding more and more features. That means you have more. The peak is caused because the algorithm was suboptimal. The reason is that some random matrix is not well behaved when n is close to d. So regularization would mitigate this to some extent. But here, it really just means that you don't only care about training loss, but also you try to find a model with small norms. And you have some kind of balance between them. So you can sacrifice a little bit of training error, but you insist that your norm is small, then you can see this rise. and this model. So this model seems to have less parameter than this. That's by definition. The norm is actually very big. So in some sense, if you use the norm as the complexity, actually, these peaks have large complexity. So what is the right measure for complexity? So this is a very difficult question. Like for different situations, you have different answers. But there is no universal answer. But norm could be one complex measure. And for linear model, it just happens that for mathematical reasons, I think l2 norm behaves really nice. going to discuss this more next time. So the high level thing is just that something else is driving the norm to be small. Thanks. Going to talk more about this in the next few days. Back to Mail Online home. back to the page you came from. Back To the pageYou came from: Back to thepage you camefrom. Back into the page You came from was from: The Daily Mail. Back onto the pageyou came from, the DailyMail.com page you were from.

ROUGE-1: 24.24, ROUGE-2: 23.00, ROUGE-L: 18.53
BERTScore: 70.40

==============================================
==================== [39/100] ====================
Summary:
Vladimir Ilyich Ulyanov, AKA Lenin, helped overthrow the Russian tsar Nicholas II in 1917. He founded the Soviet Union, one of the worst dictatorships of the 20th century. But was he a hero who toppled an oppressive tyranny or a villain who replaced it with another? It's time to put Lenin on the stand in History vs. Lenin, says Alexander Nekrassov, the author of the novel "The Death and Life of Vladimir Lenin" how would I have sounded?" We can never be sure how things could've unfolded if different people were in power or different decisions were made, but to avoid the mistakes of the past, we must always be willing to put historical figures on trial. We must never forget that history is not an exact copy of the present, but it can be a guide to the future. It can be used to learn from the mistakes made in the past and to make better decisions in the present. It is never too late to put a historical figure on trial for their actions.

ROUGE-1: 26.44, ROUGE-2: 18.07, ROUGE-L: 16.46
BERTScore: 59.46

==============================================
==================== [40/100] ====================
Summary:
The goal of this course is to give you the tools to interpret complicated phenomena. We take what we know about atoms, and build a minimally-complex interpretive picture. When we deal with n electrons, what we are going to discover is that in order to anti-symmetrize the wave function, we have to write a determinantal wave function. The matrix elements of operators that we care about correspond to how we observe atomic and molecular structure, Zeeman says. Bob Field: I was convinced that I had collected some stuff that told an interesting story. He says Klemperer showed him, on a scrap of paper, how to do it. That has been the foundation of my career for the last 50 years, he says. Field: You can understand the hydrogen atom in rather complete detail. You can't solve helium or any more-than-one-electron problem exactly. But you can do it really well, and it just costs computer time. can't-- just the question of indistinguishable electrons is such a subtle thing that you can't say, well, they have to be anti-symmetric. So what we know is this permutation operator, operating on any two-electron function, has to make-- OK, I'm skipping steps, and my notes are really kind of stupid sometimes. [LAUGHTER] OK, so we know that the Hamiltonian has to commute with-- these capital letters mean, many electrons' angular momenta. This is the spin, this is the projection of the spin. The world of spin orbitals is where I live, but we do that for a reason. And so this two-electron thing can be expressed as a space part. And alpha 1, beta 2, and then we have minus or plus beta 1, alpha 2. And that's just the combination of the name of the orbital with whether the spin is up or down. And the reason for this is it's easier to do the algebra, because there are certain selection rules, and stuff like that. can reduce it simply by, instead of writing psi every time, just writing the state. And you can also use lowering operators to generate all the states, once you know stuff. OK, so Hund says, which one of these has the largest S? which one? And that's easy to know. And for example, if you had 2p squared, you're going to get singlet D, triplet P, and singlet S. And I could tell you why you would do that. But I don't want to cause insanity at this stage, either. Stick diagrams are great, because it's easier to see on a picture, who are the actors, and have I included all of them, or have I left something out? And so now we're interested in the stick diagram for the 1s 2s configuration. And there are several kinds of 1s/2s configurations, depending on what the alpha and beta are. So there's four guys, and we can put our arrows on these things. And we know everything we need to know about these guys. It tells us what to do. "We want to know the trends of things, and we want to be able to do something like what you did in freshman chemistry on shielding. Now you probably memorized some rules about what shields what. But I'm going to give you a little bit more insight into that. So there is this. What time is it? I have a few minutes to talk about shielding, and I will. OK, so we have a nucleus. And it has a charge of z. And so if we penetrate inside of it, what we see is only the plus z, minus the number of electrons inside. Hund's rules is all about, of all of the states that belong to a particular configuration, which one is the lowest? One-- which one, not the second lowest. And why do we care? Because in statistical mechanics everything is dominated by the lowest energy state. So if you can figure out what is the low energy state, you've basically got as much as most people are going to want. The highest possible value of J is equal to L plus S-- so for L, N greater than 3. Z effective is a charge that is dependent on distance from the nucleus. It goes from the integer value that you know, from the atomic number, down to 1, because you've taken one electron away from a neutral atom, and taken it outside. And so beyond r0, the charge that you see is plus 1. Add 0, you see a charge of z, and so what ends up happening is you get z effective. And now we have this wonderful thing called the centrifugal barrier.

ROUGE-1: 25.07, ROUGE-2: 23.92, ROUGE-L: 19.41
BERTScore: 65.04

==============================================
==================== [41/100] ====================
Summary:
As a nurse we will transfuse a patient who is low on red blood cells with new blood cells via a venous access of some type. It takes anywhere between two to four hours for a unit of blood to transfuse. You typically want an 18-gauge or larger IV site some hospitals will allow you to transfused through a twenty engage. It's also important that as the nurse you know the patient's compatibility and their Rh factors. What blood they can receive and can't receive and I have a transfusion. Red blood cells are very vital for our survival and how our body works. What can cause a person to be low on red blood cells and what can cause them to need blood transfusions. Who is the universal donor who can donate to all types that's oh now who's the universal recipient they can take from everyone that's a B next you'll want to get informed consent tell the patient what they're going to be receiving assess their understanding of it also this is a good time to ask about their allergies and if they have received any blood transfusion in the past. When is a patient transfuse well this really depends depends on what's going on with the patient their vital signs how are they tolerating that low blood level and recent guidelines by the American Association of blood banks recommends transfusing blood when hemoglobin levels fall to 7 to 8 grams per deciliter. A lot of times physicians like to pre medicate them and you'll want to let the physician know if they do have a history of that and sometimes they're pre-medicated with benadryl or Tylenol acetaminophen before hand orally. From the time that that blood bag leaves the refrigerator it needs to be in the patient's body within no more than four hours because there's a risk of it developing bacteria and we can give the patient septicemia so you want to transfuse it in that time frame. Before you even start the transfusion you're gonna be doing this verification process so as the nurse you're going to be getting another nurse it's usually to our ends cuz ourian's are usually the ones who can transfuse. matches up perfectly you're gonna look look at the patient's blood type versus the donors tie and the Rh factor. You'll be collecting urine urine on them looking for the free hemoglobin that's came from those red blood cells I have lysis. If there's a discrepancy you'll need to notify the blood bank immediately and just from personal experience this has happened with one of my patients I was doing the whole verification process with another nurse and there was one letter that they had did a clerical error on so I had to send the blood. If someone's having an anaphylactic reaction to something another type is febrile. This is non hemolytic so you don't have the breaking up of those red blood cells. This causes the body to build antibodies so you can see that increased temperature like one degree Celsius or one point eight degree in Fahrenheit from the baseline. Another transfusion reaction you can have is the GVHD the you start having this rash from head the this fever diarrhea all this stuff for several weeks after you've had this blood transfusion. graft-versus-host disease and again like I said this is rare but it's deadly and it tends to occur days to weeks after the transfusion. This is where the donors T lymphocytes cause an immune response in the recipient but actually in grasping in the marrow of the recipient and attacking the recipients tissue. What can happen is they can have a fever and this really peculiar rash all over the body it'll be on the hands and the feet as well with GI issues diarrhea nausea inflammation of the liver. signs every five minutes looking at them watching them looking at that respiratory status it's not compromised or they have an allergic response what's going on now whenever you contact the physician depending on what type of reaction they suspect the patient's having or how severe it is they may order some medication so it varies some things they can orders like corticosteroids which is going to suppress that immune response along with fluids helping flush out that free hemoglobin that's in the body getting it out we want out of the body.

ROUGE-1: 28.68, ROUGE-2: 27.69, ROUGE-L: 23.36
BERTScore: 64.20

==============================================
==================== [42/100] ====================
Summary:
In particle physics and in nuclear physics, we use a system called natural units. This system is based on fundamental concepts of quantum mechanics and special relativity. In some examples, we'll use SI, in others, use natural units, such as Heaviside-Lorentz units. We'll use those natural units as we go through class. This will be a discussion in 8.701 on units, and we'll end with a question-and-answer session on the use of SI units. equals m c squared and all those things, m, E, and also the momentum have the same unit. That simplifies quite a bit. You might think that you lose information by setting fundamental constants to 1, but you actually do not, because you carry with you, in your equations, the dimension of the problem. If you want to do a quick exercise here, I invite you to calculate the charge radius of the proton, which is 4.1 over GeV, or per GeV. always be clear from the problem we're looking at. Always be clear about what you're trying to do. Always show us how you're going to solve a problem. Always. Show us how to get to the solution you're looking for. always. show us the way to get there.always. be clear of the problem you're aiming to solve. always be clear. of the goal you're seeking to achieve.always be. clear of your goal. of getting to the answer you're after.

ROUGE-1: 38.68, ROUGE-2: 29.65, ROUGE-L: 26.69
BERTScore: 56.89

==============================================
==================== [43/100] ====================
Summary:
Jeffrey Grossman: We're getting to the point where we're electrifying everything. He says most electricity will play a role, but most of it is useless unless you can store it. Grossman says energy storage is so appealing because you don't really pay a penalty. He also talks about the chemistry of batteries, and why this matters to the future of energy storage, he says. He ends with a look at some of the things we can do with energy storage today, and how we can use it. Electricity is more and more important in our lives, and if you look at just sort of like the major sectors, so you have the power generation, you have transportation, and you have industry. That's how much electricity matters here, but the thing the watts. These are the things that matter in batteries, but so many other things matter, and this is why this is a complicated problem. The world at large scales unless you're running AC units, depending on how many AC units you're using, you're not going to store energy. would be like, I've got energy storage technology and it's got this much density, this much power-- well, then it's going to last if I use it at that power for 41 days and so forth. Now the interesting, though, there's lots of stuff on here. Why don't I just want high and high? That sounds like a good thing, right, so shouldn't we be pumping water up hills all over the place? Well, obviously, if you think about it, you don't do that in your phone. Entropy is about accessibility to states. It's about how many states you have to be in. A messier room does not have higher entropy, and in this case, it's just simply the rules of the algorithm. There's a lot of reasons why this is so appealing. No penalty on the second law of thermodynamics is no entropy. No entropy is not just disorder. No disorder is just smoothness. No smoothness is just disorder, right? No, no, no. In the 1930s, a physicist named Galvani tried to make a connection between things that move and electricity. He hooked a frog up to lightning rods and watched as the storm's coming, and he's like, the lightning rod hits and he hooked him up to some metals, and the frog went crazy. He deduced that the motion itself is something that generates electricity. The story of Frankenstein was written because they went around and electrocuted things and showed that they moved, and often they weren't alive, and so Mary Shelley saw that. Zinc and copper exchange electrons in a reaction that's called a redox couple. The zinc atom at the surface sees a copper 2 plus and it says here, have 2 electrons. Oh, and then it's like, wait a second. I gave two electrons away. I've got to go into solution. And that's exactly what you see over here, right. The copper is plating the zinc so what did I do? Well, I got my zinc strip but now I'veGot a copper strip. Batteries are all about neutrality. The power isn't coming from you as Galvani thought. No, the power is coming from the difference in potentials of those metals wanting or not wanting electrons. It all comes back to the same stuff we talked about. It's standard hydrogen electrode. So sometimes you'll see it as the SHE in the battery world. And what it is it's just a very nice platinum electrode that doesn't change. And it's a reaction that on that electrode happens with hydrogen, hydrogen gas. This is an ACL, so for every copper 2 plus that plates onto the copper electrode, two NA plus atoms go into this beaker from the bridge. Dissolved salt, sodium chloride is in water, so I got all these sodium ions. And when a charge imbalance happens because something plates here, I can draw sodium atoms out of the salt bridge and I canDraw chlorine atoms from that side. That completes your circuit. That's an iron neutralizer, the salt Bridge, and this is allowing the electrons to keep everything neutral, and that's how you get the current. Just that now we're talking about the metals and whether they're more interested than the other metal in having those two electrons. That's what a potential is. And so that's how you do battery chemistry is you look things up. You look up these potentials in tables, and you think about which voltage you need. And it's by a setup that is very similar to the one I showed you that is related to hydrogen. So what you can see is that the potential difference is all about chemistry, right. The Nobel Prize was given to the development of lithium ions as a storage technology. The flexibility of where you can put that other electrode and what properties it has gives you enormous room. That's why lithium-ion batteries can can be used in a variety of ways. And those things combined make it a beautiful battery technology, and that's why it was given the Nobel Prize. It's chemistry. We're in the class. And what I wanted to-- and there's a whole bunch of chemistry. need. So that's a real challenge for chemistry. I hope you guys have a great Thanksgiving and maybe hook up some different metals. And I'll see you all next week. Back to the page you came from. Follow us on Twitter @CNNOpinion and @cnnireport. Back To the pageyou came from, back to the CNNOpinions page.Back to thepage you came From. Back from the CNN Opinion page. Back in the page, click here for the latest from CNN.

ROUGE-1: 21.31, ROUGE-2: 18.78, ROUGE-L: 16.12
BERTScore: 63.14

==============================================
==================== [44/100] ====================
Summary:
Last time, we talked about some of the kind of the bigger questions in deep learning theory. Today, we are going to start talking about the optimization perspective in deeplearning for two lectures. The main focus is to analyze what the functions you are optimizing look like so that you can use some trivial or some standard optimization algorithm for it. The bigger question we are trying to address here is that why many optimization algorithm. are designed for convex functions. But why they can still work for nonconvex functions? In most of the cases, people observe that nonconvex functions in machine learning can be optimized pretty well by gradient descent or stochastic gradient descent. And we are trying to understand why we can optimize reasonably well. And maybe just before talking about more details, let's first quickly review kind of like what gradient descent is just in case. So suppose g theta is the loss function. And the algorithm is just something like sets 0 is some initialization. And you have something like theta t plus 1 is equals to thetaT minus eta times the gradient of g of theta T. This is gradient decent. There is some caveat about whether you can even converge to a local minimum. This is actually somewhat nuanced. So I'm going to formalize this converging to aLocal minimum. But I'm not going to prove any of the theorems here. So the next part is the convergence to localminimum. We're going to show some very, actually, simple cases where we can prove this. And then, basically, I know this is probably not making that much sense for all of you if you're not familiar with how you prove NP-hardness. It's not like, if the gradient is 0 and the Hessian is PSD, then you are local min, right? So why? I guess a simple example here, I cannot-- it's easy to come up is that you just say maybe-- actually, I'm not taking the simplest one just for some reason because I'm going to use this again. So let's say suppose you have f of x1, x2, which is something like x1 squared plus x2 cubed, OK? And in this case, x1,. x2 is 0. The origin satisfies the gradient 0 and satisfies the Hessians. But actually, it's not a local minimum, as you can see. NP-hard. But actually, finding a local minimum is also NP-hard, right? So we have to consider these kind of pathological cases, which makes things harder. So the way to go beyond it is that there is a way to also remove some of the pathological cases as well so that you can find a local Minimum in polynomial time. So far as can converge to a local min with epsilon error in Euclidean distance in time poly d-- d is dimension-- 1 over alpha, 1 over beta,. 1 over gamma, and 1 over epsilon. I think I wrote a book chapter about this kind of optimization thing for our book. So I can send that to the person who take the Scribe notes. And that probably help you to have some references. But the materials are not exactly the same as the book, so you still have to do the Scribes kind of from scratch in some sense. OK, cool. The third strict-saddle condition sounds hard to check. So you cannot check. There is no way you can check whether empirically your function satisfies this condition. But I think you can prove that, if you are just given an arbitrary function, differentiable functions, you should then be able to check whether it satisfies strict-Saddle. somewhat big, almost kind of larger than 0. So then, actually, it's close to a global minimum of the function f, right? So this condition is just a slight different way to say that you have all local minimum global and strict-saddle together, all right? And then I know this condition. Then optimizers-- again, the same set of optimizers which can converge to local minimum. All right, so it's not a big deal. Many optimizers can convergence to aglobal min of f up to, say, delta-error and Euclidean distance in time poly 1 over delta, 1 over tau 0, and d. do linearized network, there is a little bit more things to do beyond that. And the second example I'm going to give is matrix completion. This is an important machine learning question by itself as well, right? So before deep learning, this was one of the most important topic maybe in machine learning, like especially if you think about nonlinear cases. So we're going to talk about that. OK, cool. I guess let's talk about PCA first. So I guess I'll maybe more precisely say matrix factorization. So I'm just trying to find M. Let's find a vector x such that M minus xx transpose in Frobenius norm is the smallest. And this becomes a nonconvex objective function because you have a quadratic term here. And our goal is to show that, even though it's non Convex, all local minimum of this g are global minimum under the assumptions that we have mentioned. So how do we prove this? So as you can imagine, the proof is pretty simple. You first find out all stationary point, the first order stationary points. And then you findout all local minimum, and you prove that they are all global minimum. So basically, it's just more or less like we solve all of these equations and see what are the possible local minimum you can have, right? So let's firstly use the stationary points, a gradient condition. So gradient of x is 0. And what is the gradient of g of x,right? So this is equal to minus this times x. So basically, you have to-- maybe one way to think about this is that you first find out the unit eigen vector. And then if you have unique eigenvectors, so suppose, let's say-- let minimum because that one is also a global minimum. So how do we do this? And also, we don't necessarily want to assume all the eigenvalues are distinct. So let's compute Hessian, right? So we need to use the Hessian. methodology also applies here when you talk about the Hessian. So in this case, from this quadratic form you can figure out what the corresponding matrix is. But for many other cases, actually, it's very hard to write out that matrix of the Hessians. So pretty much you'll still be looking at the different specific quadratics form. So we have to have this quadRatic form. And we know that the Hessia is larger than 0 is equivalent to that, for every v. In the past, one reason why people care about this question is that it has this relationship understand what each user's preference is, want to know that each user likes which item, right? So the Amazon has an incentive to just fill in the entire table. So that's why this problem was important. And it's still kind of important these days, but I guess there are many already existing methods to solve this. And the most used method is basically nonconvex optimization to find this ground truth matrix M. the theorem is that suppose p is something like poly mu and log d over d epsilon. Recall that we are in a regime that p is roughly 1 over d. And this is a poly factor in mu and also poly log in d, OK? And then we assume the incoherence. And then our local of f are when we are-- so actually, you can prove that they are all exactly global minimum. But for the moment, we only prove that the error will be exactly 0. The answer is no especially if you look for a global property, like globally, all local minimum are global. I don't think we have any proofs for any real neural network models. I guess there is a proof for linearized network models, like all the activations are linear. If you have more than two layers, you don't have strict-saddle conditions. You have a lot of [INAUDIBLE] points. Otherwise, I think we are good today.take some questions if anybody has any questions. assume that the input are linearly separable, then there is a proof for this. And there are a bunch of other cases where you can have some partial results. Next week, in the next lecture, maybe the second half of next lecture,. I'm also going to give another result, which is somewhat more general. It applies to many different architectures, but it has other kind of constraints. First of all, it doesn't really show exactly these kind of landscape properties. It shows that these kinds of properties holds for a region, for a special region in the parameter space.

ROUGE-1: 26.25, ROUGE-2: 25.52, ROUGE-L: 24.82
BERTScore: 70.56

==============================================
==================== [45/100] ====================
Summary:
Iceland is one of the world's most active volcanic hotspots crafer has erupted 30 times in a thousand years and last blue in the 1980s. Scientists are now preparing to drill into it the is to learn more about how volcanoes behave so that we can better predict eruptions and also tap into a super hot source of energy volcanoes can be spectacular but they're also devastating around the world millions of people live close to them here in Iceland. Researchers here hope their work will change that helping to save lives and money while also pioneering a form of volcano power.

ROUGE-1: 23.42, ROUGE-2: 22.80, ROUGE-L: 23.42
BERTScore: 66.24

==============================================
==================== [46/100] ====================
Summary:
We're gonna have aggressive so with our regressive tax as people earn more income base they are attacked a smaller amount of smaller percentage of the rounding so here we see people paying a small person their income tax as their income goes up. With our progressive tax people pay the same percentage whether progressive its detect ops as progressive people pay lark there. We want to know do we tax people or to be tax household so let me show you an example because the units that were taxed and can have all kinds of different applications [Music] that's hot. Sales taxes are regressive because people are going to spend different amounts of their anymore. As we earn a dollar from it some amount of that dollar is going to be paid in tax but that percentage that you're paying tax changes. The difference here between 86 and 36 has been to thousands with $50,000 we've got 4,000 yes this guy is easy the decimal point four thousand five so 12,500 plus we're going to pay 18,000 submit an $85 your taxes on $90,000 and we can figure out what our average tax rate is. here don't beat 607 of those dollars so does that make people everyone agrees the answer is yes we just don't know what the number is it's probably different for different people. At one point in time Sweden had a marginal tax rate it was 102 percent so what does that that if you had $50,000 and then you earn $10,000 there is your went from 50,000 to 60,000 not only did they take all 10,000 what what they do they take more right. 17,000 Orlando will go to top Bill and Hillary will both oh eight thousand each so our total BAM attacks if we put this way 19,000 if we tax the household this goes back to this idea we discussed the horizontal language all right we want to treat people that are in similar circumstances. If we tax them as a family they're both a 20-3 if they're married and they're not they're paying 16,000 these people are paying 23,000 this is what you may have heard about this when your refund their tax that's called the marriage the tax code penalize you for being there. when we're looking at fiscal policy Keynes's and we can look at things wide changes and taxes or changes in government spending. When we look at the national debt that's just some eight of all these government debt assistants are possible just a doll and we saw for our country so we have all the deficits and surpluses that we packs and some of you maybe want to add all that up that adds up to 17 trillion dollars deficit in any given year. We can engage the fiscal policy that makes these guys change on purpose we can have called discretionary fiscal policy.

ROUGE-1: 28.81, ROUGE-2: 27.77, ROUGE-L: 26.93
BERTScore: 65.88

==============================================
==================== [47/100] ====================
Summary:
PhilipPE RIGOLLET: I want to have a small bias, hopefully a 0 bias. If this thing is 0, then we see that the estimator is unbiased. So am I planning-- yeah. So if I do, for example, X1, Xn, there are iid Bernoulli. And I'm going to write it theta so that we keep the same notation. Then theta hat is the average of Xi's. So what is the bias of this guy? Well, to know the bias, I just have to remove theta from the expectation. that we are going to be looking for in an estimator, trying to find them to be unbiased. But we'll see that it's actually maybe not enough. So unbiasedness should not be something you lose your sleep over. Something that's slightly better is the risk, really the quadratics risk, which is expectation of theta. And so for example, if the quadratic risk goes to 0, then that means that theta hat converges to theta in the L2 sense. for sum of independent random variables, now it's time to wake up. So we have the variance of something that looks like 1 over n, the sum from i equal 1 to n of Xi. So it's of the form variance of a constant times a random variable. We would like somehow to say that this is the sum of the variances. And in general, we are not allowed to say this, but we are because my Xi's are actually independent. And that's by independence, so this is basic probability. Rigollet: When you have an unbiased estimator, it's simple. It's just telling you it's the variance, because the theta that you have over there is really-- so in the definition so there's this weird notation. You want 1 minus alpha to be very close to 1, because it's really telling you that whatever random variable I'm giving you, my error bars are actually covering the right theta. And I want this to hold true for all possible values of the parameters that nature may have come up with. a vector of parameters, a multivariate parameter, the bias increases when you're trying to pull more information across the different components to actually have a lower variance. So the more you average, the lower the variance. As n increases, the variance decreases, like 1 over n or theta, 1 minus theta over n. And so this is how it happens in general. In this class, it's mostly one-dimensional parameter estimation, so it's going to be a little harder to illustrate that. what a confidence interval is. And so we fixed a statistical model for n observations, X1 to Xn. The parameter theta here is one-dimensional. Theta is a subset of the real line, and that's why I talk about intervals. An interval is just a one- dimensional conference region. And it has to be an interval as well. A confidence interval of level 1 minus alpha is actually called it's level. The closer to 1 it is, the better the confidence interval. One of the properties that we wanted. Strongly consistent means that as n goes to infinity, it converges almost surely to the true parameter. It is consistent also, because it's strongly consistent, so it also converges in probability, which makes it consistent. We've actually computed its quadratic risk. And now what I have is that if I look at-- thanks to the central limit theorem, we actually did this. We built a confidence interval at level 1 minus alpha. two ways of getting rid of this. Since we only need this thing-- so this thing, as we said, is really equal. Every time I'm going to make this guy smaller and this guy larger, I'm only going to increase the probability. And so what we do is we actually just take the largest possible value for p1 minus p, which makes the interval as large as possible. And by Slutsky, we know that this is actually converging not Slutky, right? as much from your data as you can. So it depends on how comfortable and how critical it is for you to put valid error bars. If they're valid in the asymptotics, then maybe you're actually going to go with Slutsky so it actually gives you slightly narrower confidence intervals. Now, if you really need to be super-conservative, then you'reactually going to going with the P1 minus P. So that's what the function here-- the function you're interested in is 1 over square root of X1 minus X. Its derivative really is what matters. The goal is to estimate a true theta star, the one that generated some data, X1 to Xn, in an iid fashion. If I tell you the KL divergence is 0.01, it's not clear what it means. The goal of knowing theta stars is so that you can actually know what P theta. Otherwise, it has-- well, sometimes we said it has some meaning itself, but really you want to know what the distribution is. The proof is just one step Jensen's inequality, which we will not go into details. When you compute probabilities on one distribution, you should have the same probability on the other distribution pretty much. So what we can do is say, well, now I have two candidate distributions. And so that means that we have this actual total variation distance between probability distributions. But here, I still cannot compute it, because I have this P theta star that shows up. And if you've heard of KL, you've probably heard of entropy. And that's a quantity that just depends on theta. are you to compute this maximum over all possible events? I mean, it's just crazy, right? There's an infinite number of them. And so there's actually a way to compress it by just looking at the basically function distance or vector distance between probability mass functions or probability density functions. So throughout this chapter, I will make the difference between discrete random variables and continuous random variables. It really doesn't matter. All it means is that when I talk about discrete,. I will talk about probability mass function. And when I talks about continuous, I'll talk about probabilities density. But they're all the same thing. to some event A. The probability of X falling to some subset of the real line A is simply the integral of the density on this set. Since for each possible value, the probability at X-- so I hope you remember that stuff. But essentially, we know that the probability that X is equal to little x is 0 for a continuous random variable. There's just none of them that actually gets weight. So what we have to do is to describe the fact that it's in some little region. 1/2 really doesn't matter. It's just if I want to have an actual correspondence between the maximum and the other guy, I have to do this. So this is what it looks like. And so we have a couple of properties that come into this. The first one is that it's symmetric. TV of P theta and P thea prime is the same as the TV between P thena prime and PThena. I just flip those two, I get the same number. Those things are completely symmetric in theta. You can just flip them. The total variation equal to 0 implies that P theta is equal to P theTA prime. If this thing being small implied that P. theta could be all over the place, that would not help very much. The problem is that this will still not satisfy the triangle inequality. But the divergence is doing a pretty good thing for us. And this is what will allow us to estimate it and basically overcome what we could not do with the total variation. The fact that you need two definitions of the [INAUDIBLE],, is it something obvious or is it complete? I'll do it for you now. Philip Pirollet: The integral of f is equal to 1 and the integral of g isequal to 1. Pirollett: If I take any other A in this integral than this guy A star, it's actually got to decrease its value. Piollet: This A star is the one that actually maximizes the integral over the set of X's such that delta of X is non-negative. It's actually exactly the same area above and below that's actually equal to 0. RIGOLLET: It is. Just look at this board. So this is definitely at most the maximum over A of Pf of A minus Pg of A. Right? We agree with this? Because this is just for one specific A, and I'm bounding it by themaximum over all possible A. So that's clearly true. So now I have to show you that the max is actually this guy, A star. So why would that be true? Well, let's just inspect this. when I take A to be the set where it's positive. Just need to make sure that there is someplace where it is, but that's about it. So it's a distance. It's symmetric, non-negative, equal to 0, if and only if the two arguments are equal, then it satisfies the triangle compared to the convex function of the expectation of a random variable. If it's not satisfying this thing, it's called pseudo-distance or quasi- distance or just metric or nothing at all. estimator is. And I'm going to try to minimize this estimator now. And if the two things are close, then the minima should be close. That's a pretty good estimation strategy. The problem is that it's very unclear how you would build this. estimator of TV, of the Total Variation. But for this guy, we will be able to build an estimate for it, because it's actually going to be of the form expectation of something. And this constant, I don't know. It depends on P theta star. it is. And that's now where the log plays a role. If you actually pay attention, I said you can use Jensen to prove all this stuff. You could actually replace the log by any concave function. That would be f divergent. That's called an f divergence. But the log itself is a very, very specific property, which allows us to say that the log of the ratio is the ratio of the log. If I change theta, this thing is never going to change. It depends only on theta. this constant is 0 for my purposes, or 25 if you prefer. All right. So we'll just keep going on this property next time. And we'll see how from here we can move on to-- the likelihood is actually going to come out of this formula. Thanks. Back to Mail Online home. back to the page you came from. Back from the page where you came From. Click here to go to the show page where we'll be talking about the probability of winning the lottery.

ROUGE-1: 25.76, ROUGE-2: 24.71, ROUGE-L: 21.83
BERTScore: 69.83

==============================================
==================== [48/100] ====================
Summary:
Markus Klute: This class will be taught in an inverted classroom or flipped classroom setting. The course evaluation or your evaluation in this course will be made up 50% out of homework. The great divide-- the great divide or the grade divide-- at 85% between A and B, 70% between B and C, 60% between C and D, and below 50% earns you an F. The grading scheme will not be worse than what I've given you here, Klute says.

ROUGE-1: 23.03, ROUGE-2: 21.64, ROUGE-L: 19.24
BERTScore: 55.97

==============================================
==================== [49/100] ====================
Summary:
In this problem, we are given a joint PDF for x and y. And then we are asked to compute the variance of x plus y. We're told we should compute this variance by using something called the law of total variance. So first, I want to focus on this term-- the conditional expectation of X plus y conditioned on x. So this is actually just x plus the expectation of y given x. And because it's uniformly distributed and because expectation acts like center of mass, we know that the expectation should be the midpoint.  memorizing formulas might seem like cheating, but there's a few important ones you should know. And it will help you sort of become faster at doing computations. And that's important, especially if you guys take the exams. So that's it. See you next time. We'll be back in a few days with a new episode of "The Daily Discussion" to talk about the week's top news stories. Back to the page you came from. Click here for the next episode of The Daily Discussion.

ROUGE-1: 18.96, ROUGE-2: 16.11, ROUGE-L: 15.88
BERTScore: 65.28

==============================================
==================== [50/100] ====================
Summary:
George Church: How do we get that multisequence alignment? He says the indels for any pairwise combination may not be optimal for the triple. Church: You can get something that's very close to the true optimum if how to prune this hyperlattice. He says there are others which are more heuristic. They are not guaranteed to be optimal, but on the other hand, they don't necessarily require arbitrary pruning, he says. The two that we'll illustrate in the next couple of slides is a tree alignment and a star alignment. will be a recursive algorithm where the score of a two-character string is defined in terms of the maximum of various shorter strings. The number of different cases we have here-- before, it was 3 for a global alignment, which was k, being the number of sequences, was 2. Now k is three for a three-way comparison. All possible subsets is 2 to the k minus 1, in this case, so it's 7. And you get the best score is S1 with S3, which has a score of 9. general is going to be 2 to the k minus 1. And so the time complexity is have to do 2 to k comparisons per node. And the larger k is, the more you can explore. It's like doing a huge mutagenesis experiment and exploring viable mutants. And it's not like we only want to do k equals 2. There are very good reasons for inferring structure or function without experiments, just from sequence. So we use all the power that we developed for the pairwise comparison and we're just generalizing it. The final branching closest to the trunk of the tree or the roots of tree is called the dendrogram. The longer branches indicate greater divergence. And they're in their own cluster. The common ancestor for all the sequences, which would be the common ancestor of the common ancestors of the first two clusters, is represented by this final branching. And here, distance is this horizontal axis. And then, once you have this dendprogram, the next step, or the full steps, are aligning each of the sequences. best score for S1 with each of the others, and have S1 in red in each case, and use that as the anchor. And so then in the multialignment, you take all the indels relative to the red one, and introduce them so that it's the anchor, and so on. So those are two radically different ways. And we'll get to the Gibbs sampling later. The Gibbs sampling, just in a nutshell, is in general when you have a hard problem, where you can't comprehensively go through the entire space, what you do is sample it. or RNA-coding region, you'll have regulatory elements such as promoters and so-called CG islands. Promoters and CG islands are sort of degenerate. They're weak sequence signatures. There's a high variety, and they're used in combinations. If you need longer ones, then you'll miss tiny proteins. And we'll talk about this in just a moment, specific examples. So now the final topic, which talks about a very simple and short motif, is the CG motif, which we claimed is over-represented in promoters in vertebrates. cell type and the rare [INAUDIBLE],, rare messenger RNA within a cell type. Let's talk about the sizes of proteins. How is it that it precipitously drops off at 100 amino acids? Why are there so few proteins that are short? And there are slightly more short proteins in Mycoplasma? Any guesses why they're so few? Why does it drop off at100 amino acids?" STUDENT: There are more but we can't find them? GEORGE CHURCH: Right, there probably are. Stop codons tend to be made up of As and Ts. The stop codons are TAG, TGA, and TAA. So if you have an AT-rich genome, you're going to tend to have a lot. You tend to run into a stop codon at random quite frequently. And it turns out that-- again, experimentally biases. And I just elaborated on one of them here, which is the triplet bias, documented here that this 10 times lower frequency of ATG than some of the other arginine codons. row happen to be-- the next gene down is a histidine biosynthetic gene. And not only that, but about eight histidine genes in a row come after that. And the same thing with phenylalanines. This weird excess of tryptophan is upstream of tryPTophan biosynthesis genes. So what does this all mean? What it means, probably-- and there's actually quite a bit of experiments on this-- is that this is an excellent feedback loop, where you want to do feedback in the most relevant way. Bayes' theorem is a Bayesian idea that says the probability of models in your collection of models, irrespective of sequence, should sum to 1. Now let's see what all this Bayesian stuff is useful for. We're going to be doing-- of the various applications, we had recognition discrimination and database search. A question might be like, what are all sequences in [INAUDIBLE] that look like a serine protease? This would be asking for recognition multiple times, over and over. P of s/m, s bar m-- is the probability that you would get that sequence given a model. So the model might be this weight matrix we've been talking about or it could be something more complicated. And as with any good probability, as we mentioned in the first class, they should sum to 1. If you sum up the sigmas of s, you sum over all sequences, then the probability given the models shouldSum to 1, that will be true for the p of the sequences. the dinucleotide level, and so on, and rather than just dump this on you as a mathematical fact, I want to give you some biological rationale for why you can have nonrandomness at every order of a Markov chain. And so here's one that's illustrated, this dotted, brown line, where it says probability of a C minus-- meaning in an ocean-- given that you have an A plus. So that would be a transition point going 5-prime to 3-prime, from an island into an ocean. are 0s. 0s are a problem, both for the CG dinucleotide in the ocean and for the transitions between oceans and islands. The way you handle it is called pseudocounts. You basically say, what if we just missed finding that thing? We're going to add 1 to it because however big the counts are, you can always add 1. And you can recreate these numbers with that simple formula. Now this, is a real training set based on 48 known islands, again annotated by some person.

ROUGE-1: 26.68, ROUGE-2: 25.20, ROUGE-L: 22.78
BERTScore: 63.48

==============================================
==================== [51/100] ====================
Summary:
in this module of tools and equipment we're going to explore cookware and storage wear. There are five basic metals that are used in cookware aluminum copper stainless steel cast iron and carbon steel aluminum pots and pans are the ubiquitous pans that you'll find in most every kitchen. A rondo is an amalgam of several different metals cladware sometimes referred to as allcloud because of its brand name is using different layers in of metal including stainless steel copper and aluminum to be able to provide the benefits of each one of those metals. interchangeably in many ways but believe it or not though there is a difference between these two does it make a difference in your cooking if you use one instead of the other. A saute pan or satwa is one with the straight sides and has a larger surface area which makes it ideal for tasks like searing meat or reducing a pan sauce. A sawtooth or skillet has a slope side and is used mainly in sauteing the slope sides providing the ample and perfect angle for flipping your food or as it's referred to as saute or to jump a rondeau. These are the basic pots and pans you need to get started in the kitchen. The best rule of thumb is quality over quantity make sure you invest in good quality pots and pan that will last for years. These heavy duty polycarbonate food storage containers often referred to by the trade name cambro are the workhorses of the restaurant kitchen. They can store produce in the cooler liquids in the freezer and dry goods at room temperature. They wash well by either hand or dishwashing and can stand up to years of usage. notice that this two inch hotel pin will fit directly into this and it'll fit snugly. This is because you'll have water underneath in your four inch hotel pan that will be simmering. You also have perforated hotel pans which are basically hotel pans with holes in them. These can be used as steamers you can use them to drain things in a steam steamer setup. There's a lid you have a lid with it takes up too much room in your coolers and takeaway number six you can never have enough hotel pans no matter what size you have. Sip top bags are an indispensable storage device for kitchens. They allow the storage of dry goods and other small quantity items. They can be used in the freezer or refrigerator as easily as they can be in dry storage and they can even be used for sous-vide cooking. They come in a variety of different sizes but most common sizes are the full size and half size. The speed rack is essential for storage and movement within the restaurant. The ziploc can be left open wrapped with plastic for transportation and air protection or draped with a plastic bag.

ROUGE-1: 26.28, ROUGE-2: 25.08, ROUGE-L: 25.28
BERTScore: 60.16

==============================================
==================== [52/100] ====================
Summary:
Charles Brockden Brown: The Enlightenment is a critique of a kind of impoverished view of mind or consciousness that over-relies on reason. We started to look at it a little bit with Barlow's [assumed spelling] Raven poem with Poe. The idea that in fact there is something wrong with reason as a faculty, and this can take a couple of forms. One is that all of those nice ideals that come along with the enlightenment may in fact be parasitic precisely on the kinds of negative types of thinking that they are designed to get rid of. Franklin thinks about moral perfection, all right he's gonna use enlightenment ideals, but he has a little bit of you might say that Calvinist self-denial about him. He believes that you can identify error. And in so doing he's able as author to pick out the defective font [phonetic] of type those errors, and either talk about the ways in which he was actually able to correct them in real life in his real life. He still is getting a kind of do-over as he rewrites his life. Charles Brockden Brown's The Power of Sympathy is thought to be the first American novel. It is set in Philadelphia in 1787, the same time as the Constitution is being framed. Brown is an interesting character because he is one of the first US writers to try to take advantage of first American copyright law. The copyright law does something interesting; it makes a profession of authorship possible, because it suggests that writing is property, and if you can sell what you write then you can live off your writing. Wieland: or, The Transformation, the title about it a little bit but there is a certain kind of haste that is evident in the plotting of Edgar Huntly. Brown is writing in order to transform the novel into a higher form than it's thought to be, and he's writing something that is not within the main line of novelistic writing. This is part of the larger project of romanticism of gothic fiction. And we'll take it up again on Wednesday, when we look at the short stories of Irving Clitheroe. page. An American Tale, right? He's stressing that he's doing something American. If you could read the little epigraph here it says from virtues blissful paths away, the double-tongued are sure to stray. Good is a forthright journey to still, and many paths but lead to ill. All right again a kind of moral note. And yes, if you can actually see it, that is not an F,. that is an old fashioned way of setting an S. So the Gothic is, [inaudible] good true taste. fiction. And what is fiction, if not the opposite of truth? Fiction means lies, right? Why would you read this? If you do it's got to be only a guilty pleasure. There can be nothing improving by it. So fiction, as opposed to fact, fiction somehow has falsehood. Therefore early novels try to insist on their basis in fact. Don't worry about this, it's based on a true story. As if there were something that would then could be morally improving for that. There's a kind of weird conflation of supernatural demonic activity, sexuality, violent and disgusting death. Ann Radcliffe makes a distinction between what she calls, let me see if I have it here, the distinction between terror and horror. So Monk Lewis' Gothic is horror. Her Gothic she would've offered was a form of terror, okay? Finally, this is a rather famous if somewhat overstated account of what's going on in the Gothic novel. It comes from a moment when Freudian criticism was very much in the [inaudible] Gothic becomes a pejorative term by this time that is, that takes on the medieval association and makes you think of you know it's so Gothic, it's medieval, right? Romantic writers are gonna mobilize the medieval and Gothic and dark over against The Enlightenment to show you what's wrong with The Enlightenment. The novel, The Castle of Otranto, this Gothic novel which seems like kind is bad taste. Walpole starts to reclaim it. Brown wants to do something similar to reclaim the Gothic in the American context, okay. Classic Scooby-Doo is Ann Radcliffe. There appear to be manifestations of the supernatural. She often finds herself in the various kinds of settings crumbling castles, dungeons, graveyards, darkened churches. Usually she almost escapes her persecutors, then they catch her. They are trying to get her out of greed or lust or both. In the end, all of the ghosts and supernatural manifestations that have tyrannized her are shown to be fakes. It's a very popular formula and it continues to this present day. Ambrosio is a man who's so repressed and severe that Shakespeare calls, what does he say? He scarce confesses that his blood flows. He's soon readily corrupted by a satanic woman named Matilda who gains access to his cell by disguising herself as a young nun. They become lovers, naughty, naughty. But he soon gets tired of her and of course she's promoting this, and he dreams of possessing a 15 year old girl named Antonia, who is the daughter of a story about enlightenment. every eye was a loathsome and disgusting object. To every eye but a mothers. In vain did human feelings bid me recoil from this emblem of mortality. I was not insensible at that moment of the impulses of vengeance. But they were transient. I detested the sanguinary resolutions that I had once formed. Yet I was fearful of the effect of my hasty rage and dreaded an encounter in consequensive wish I must, I might rush into evils which no time could repair, nor penitence expiate. The sublime is kind of like the romantic emotion par excellence. It's what they are try, they are striving to recreate. In a sense, Matthew Lewis's you know disgusto Gothic is actually making fun of that. But you can see that this discourse of the sublime, is part of what Brown is drawing on in Edgar Huntly. The disappearance of Clithero had furnished new incitements to ascend its cliffs and pervade its thickets. The past is suddenly beginning to impinge on Edgar. There's grounds of hope for everybody, he says. But it also means destroying old systems of obligation. What if the total depravity of human beings is true, not because of something called the fall of human kind, but just because that's human nature? Then, we've got a big problem. All of this you might say is the context for Charles Brockden Brown's Edgar Huntly. Okay? Now, let's take a look at the first pages of this. This is the preface in my rambles with some traces of this man. Be impossible to arm myself with firmness? If forbearance be the dictative wisdom can it be so deeply engraven on the mind as to defy all temptation? My late experience has been of use to me. It has shown me my weakness and my strength. Having found my ancient fortifications insufficient to withstand the enemy, what should I learn from thence but that it becomes me to strengthen and enlarge them. No caution indeed, he says, can hinder the experiment from being hazardous. Curiosity is vicious if undisciplined by reason and inconducive to benefit. Clithero's story in these 4 chapters act as a kind of narrative counterweight to the primacy of reason that Edgar is trying to say this larger story is about, is a dramatization of. Take a look on page 88. Clithero is proof you might say that a rationality even madness is alive and well in the world and very powerful. Edgar refuses to believe this. On page 88, he describes himself as a dispassionate observer still. And later on, take a look at this on page 90, he still is unwilling to accept the evidence of what is in front of him. When Edgar looks for these letters, on page 128, he finds that they're missing. And when he can't find the letter he finds himself lost in horror and amazement. We start to see that there's a whole property motif that is sort of hidden and starting to come up to the surface here. He's been banking his whole future on the fact that Mary is supposed to inherit from Waldegrave. What doesn't, what happens if that doesn't happen? All right? the hand and by which force could be exerted. Some spring therefore secretly existed which might forever elude the senses, but on which the hand by being moved over in all the directions might accidentally light. The process was effectual. A touch, casually applied at an angle drove back a bolt and a spring at the same time was sent in action by which the lid was raised above half an inch. No event could have been supposed more fortuitous, by chance than this. No measure that I could adopt enabled me to place the lid in the same situation in which I had found it. enabled me to discover, to discover enabled me to push forward the bolt and thus to restore the fastening. I now perceive that Clithero had provided not only against the opening of his cabinet, but likewise against a possibility of concealing that it has been opened. So this is one of these da da daDa scenes that you should be looking at as kind of an exemplary moment. What's going on here? What's the immediate ideas that this scene conjures up for us? It's a box. by me which I had culpably neglected, to inspire my zeal, inspirit my zeal to awaken my remembrance and insight to me to the performance of this duty, did this glimmering messenger, this half indignant apparition come. And now some dodgy things come to light. Bottom of the page, and again this is maybe a critique of The Enlightenment. Waldegrave like other men early devoted to mediation and books had adopted a different [inaudible] different systems of opinions on topics collected with religion and morals.

ROUGE-1: 24.02, ROUGE-2: 23.31, ROUGE-L: 20.44
BERTScore: 63.80

==============================================
==================== [53/100] ====================
Summary:
Markus Klute: We're starting a new chapter in which we look at tests and implications of special relativity. One experimental test is stellar aberration, which we discussed can be explained by special relativity and by velocity addition. In all of this experimentation and experimental verification, it's important to understand the importance of uncertainties in the scientific process overall, Klute says. He says one needs to be open minded and scientifically open minded to study and grow to the question of whether or not Einstein's theory is correct.

ROUGE-1: 17.82, ROUGE-2: 14.03, ROUGE-L: 15.91
BERTScore: 60.23

==============================================
==================== [54/100] ====================
Summary:
In the colonial era, most American women of European descent lived lives much like those of their European counterparts. Lower and working class women were actually more equal to men of their own classes, but only because they were, like, equally poor. As production shifted from homes to factories, it shifted away from women doing the producing. This led to the so-called “cult of domesticity,” which like most cults, I am opposed to. The idea of true equality between men and women was so radical that almost no one embraced it. expected to marry and have kids rather than, like, pursue a career. Under the legal principle of “coverture’ actually husbands held authority over the person, property and choices of their wives. Since women weren’t permitted to own property and property ownership was a precondition for voting, they were totally shut out of the political process. Citizens of the new Republic were therefore definitionally male, but women did still improve their status via the ideology of ‘Republican Motherhood’ domesticity decreed that a woman’s place was in the home, so rather than making stuff, the job of women was to enable their husbands to make stuff. “Woman is to win everything by peace and love; by making herself so much respected, esteemed and loved, that to yield to her opinions and to gratify her wishes, will be the free-will offering of the heart.” “The moment woman begins to feel the promptings of ambition, or the thirst for power, her aegis of defense is gone. All the sacred protection of religion, all the generous promptings. of chivalry, all of the poetry of romantic gallantry, depend upon woman�'s retaining her place as dependent and defenseless, and making no claims, and maintaining no right” 19th century. Women gave many temperance lectures featuring horror stories of men who, rather than seeking refuge from the harsh competition of the market economy, found solace at the bottom of a glass or at the end of a beer hose. Now don't get me wrong: Prohibition was a disaster, because 1. Freedom, and 2. It’s the only time we had to amend the constitution to be like, “Just kidding about that other amendment,” but it’S worth remembering that back then people drank WAY more than we do now. in comments where you can also ask questions about today’s video that will be answered by our team of historians. Thanks for watching Crash Course and as we say in my hometown, don’t forget to be awesome. Back to the page you came from with this week's Crash Course video. Follow us on Facebook and Twitter @CrashCourse and @CNNOpinion. For more Crash Course videos, visit CNN.com/Crashcourse and follow us on Twitter @cnncrashcourse.

ROUGE-1: 28.60, ROUGE-2: 26.76, ROUGE-L: 24.58
BERTScore: 63.42

==============================================
==================== [55/100] ====================
Summary:
ae houseman 998-999 uh very intellectual individual um with these two authors today um we're really kind of bridging the gap between the victorian and the modern. We're really transitioning and that's why these are the last authors that we'll focus on in this time period some of their philosophy some of the writing styles and things that motivated them. Most of his poetry especially towards the end if you look on the right there the grief and poetry era you know especially the last of his poems. Houseman can tap into that pain and that darkness and help you know other people and and help spread emotion through and through. To an athlete dying young is an article that i'm going to have you take a look at later. Look at what he says about that individual and how they you know can celebrate and reflect on that individual. The time you won your town the race we cheered you through the marketplace man and boy stood cheering by and home we brought you shoulder high today the road all runners come shoulder high we bring you home and set you at your threshold down. where glory does not stay and early though the laurel grows it withers quicker than the rose eyes the shady knight has shut cannot see the record cut and silence sounds no worse than cheers after earth has stopped the ears now you will not swell the route of lads that wore their honors out runners whom renown outran and the name died before the man so set before its echoes fade the fleet foot on the sill of shade and hold to the low lintel up the still defended challenge cup and round that early laureled head will flock to gaze the strengthless dead and find unwithered on its curls. heart away give pearls away and rubies but keep your fancy free but i was one in twenty no use to talk to me. When i was 1 and 20 i heard him say again the heart out of the bosom was never given in vain just paid with size aplenty and sold for endless rue and i am two and twenty and oh tis truth is true okay the first few lines when i was 2 and 20 and i heard a wise man say give crowns and pounds and guineas but don't give your way. Don't fall in love you can spend money on them you can give them all this stuff but what ultimately don't you want to give them at this age? here in when i was 1 and 20 the kid kind of learns his learns his uh his issues and still 22. that's still pretty young to learn um to learn a life lesson. That's when i learned a lot about myself. That was when I was 1 to 20. I'm 22 now. I've still got a long way to go, but that's a good thing. I still have a lot to learn. I don't think i've learned all of my lessons yet, but I'm getting there.

ROUGE-1: 46.30, ROUGE-2: 42.04, ROUGE-L: 41.21
BERTScore: 64.14

==============================================
==================== [56/100] ====================
Summary:
In quantum mechanics, the spin of a particle with a vector is quantized, and in terms of its length and its components. You find that it's square root of f times s plus 1 in units of h-bar. The components, and along any axis, actually-- and in this case here, the d-axis-- have eigenvalues, and they are listed here. And we find that there is 2s plus 1 possible values, so I'll pick here, just arbitrarily, the z axis. physical state of particles, which axis are the right ones to choose-- or, sensible? There's no right and wrong in this discussion. Let me motivate this. If you look at the orbital momentum of a particle, that's given by r cross p, where p is the momentum vector of the particle. So now, if you're looking at the total momentum, we have to look add the angular momentum and the spin of the particles together. So this is a nice choice of coordinate system or of axis.

ROUGE-1: 38.96, ROUGE-2: 38.60, ROUGE-L: 38.96
BERTScore: 77.91

==============================================
==================== [57/100] ====================
Summary:
In the 5th Century BC Athens was a direct democracy that encouraged wide participation through the principle of ho boulomenos, or anyone who wishes. This meant that any of its approximately 30,000 eligible citizens could attend the ecclesia, a general assembly meeting several times a month. The Athenian system also relied on a 500 member governing council called the Boule, to set the agenda and evaluate proposals. Some ancient philosophers, including Plato, disparaged this form of democracy as being anarchic and run by fools. poll, all examples of how the democratic principle behind sortition still survives today. Polls show that the principle of sortition is still very much alive and well in the United States today. The majority of Americans support sortition, with the majority of people in the U.S. voting in favor of it by a wide margin in the last presidential election in 2008. The most popular type of vote in the 2008 election was for the Democratic Party, followed by the Republican Party and the Libertarian Party. The Democratic Party won the popular vote.

ROUGE-1: 36.82, ROUGE-2: 24.94, ROUGE-L: 25.12
BERTScore: 62.20

==============================================
==================== [58/100] ====================
Summary:
The Fourier transform is the product of a Gaussian and a sine wave, or cosine. Convolution in the time domain looks like multiplication in the frequency domain. We're going to talk about the convolution theorem, noise and filtering Shannon-Nyquist sampling theorem the function into the imaginary part of the Fourier transforms. We'll talk about how to now right? So there's a little Gaussian pulse in time, and when you multiply those together, you get this little pulse of sine. and spectral estimation. And next time, we're going to move on to spectrograms and an important idea of windowing and tapering, time bandwidth product, and some more advanced filtering methods. And we'll end up on the Shannon-Nyquist theorem and zero padding. And there may be, if there's time at the end, I'll talk about a little trick for removing the line noise from signals. And I'll show you how to do this in more detail after we talk about tapering. Fourier transforms have an interesting property about scaling in time and frequency. When you take the Fourier transform of a function, symmetric functions, even functions, are always real. The faster something moves in time, the more stretched out the frequencies are. If you take this pulse and stretch it out for 500 milliseconds, then you can see the same sinc function, but it's just stretched out in the frequency domain. If we take that pulse and make it narrower, then we can make it 4 times narrower. If this is 4 times wider, then this will be 4 times larger. we plot power in log base 10. A difference of an order of magnitude in two peaks corresponds to a unit called a bel, b-e-l. If we plot this on a log plot in decibels, you can see that on a Gaussian, which is e to f squared, that's minus f squared. We're going to spend a lot of time in the next lecture addressing how you solve that problem in a principled way and make a good estimate of the signal. called the time bandwidth product. You can see that as you make the width in time narrower, the bandwidth in frequency gets bigger. Any signal that has discrete components and frequencies is periodic in time. Heisenberg uncertainty principle comes from the wave functions are just-- you can think of wave functions as just functions in time, he says. The Shannon-Nyquist theorem is a very important theorem for acquiring signals in the lab. The Wiener-Khinchin theorem, very cool. that particle can be computed as the Fourier transform of the wave function. So if the particle is more lo-- [AUDIO OUT] in space, then if you compute the Fouriers transform of that wave function, it's more dispersed in momentum. So this concept of time bandwidth product in the physical world is what gives us the Heisenberg uncertainty principle. The sampling rate needs to be greater than twice the bandwidth of the signal. The higher the sampling rate is, the further these spectra are in time. Micheal Fee: Imagine that we have three functions of time, y of t, x of t and X of omega. He says the Fourier transform of y is just the product of g and x. Fee: So shifting the inside of it by a small amount tau isn't going to do anything. He shows you how to derive the transform by reverse the order of integration, which is a function of tau, rather than the other way around. The derivation is kind of cute, and I enjoyed it. Fourier transform of Gaussian noise is just two peaks, with wiggly stuff around them. Power spectrum of filtered signal is just the power spectrum of your original signal times the power Spectrum of the kernel. High pass or low pass? What kind of filter is that called again? High pass. Low pass. All right, so let me play you what those sound like. [LOWER STATIC] It got rid of about the high frequency parts of the with each other? wanted to show you what the autocorrelation function of this looks like, which I think we saw before. So if you look at the distribution of all the samples, it just gives you a distribution that it has the shape of a Gaussian. And the standard deviation of that Gaussian is 1. Now, what if you plot the correlation between the value of value of this function at time t and time t plus 1? Is there any relation? So they're completely uncorrelated with each other. Using these methods, you can pull tiny signals out of noise at a very bad signal to noise ratio, where the signal is really buried in the noise. So it's a very powerful method. And we're going to spend more time talking about how to do that properly. So now let's turn to spectral estimation. How do we estimate the spectrum of a signal? So let's say you have a signal, S of t. And you've got a bunch of short measurements of that signal. What you can do is calculate the power spectrum for each of those signals. Filtering in the frequency domain means multiplying the power spectrum of your signal by a function that's low at high frequencies and big at low frequencies. So convolving our original blue signal with this green Gaussian kernel smooths the signal. It gets rid of high frequencies. Any questions about that? Well, yes-- AUDIENCE: So why is it that like-- you need to like-- of I guess when you filter a signal, either high pass or of that Gaussian? It's just another Gaussian. low pass, by convolving a signal with a kernel. The kernel for a high-pass filter is a delta function that reproduces the function. You can take a signal like this, Fourier transform it, multiply it by a square window to suppress high frequencies. What would be the corresponding temporal kernel that that would correspond to? It would be convulsing your function with a sinc function. So we're going to talk about how to smooth things in frequency with signals with kernels that are optimal for that job. signal has some bandwidth B that in order to sample that signal properly, your sampling rate needs to be greater than twice that bandwidth, 1, 2. There was recently a paper where somebody claimed to be able to get around this limit. And they were mercilessly treated in the responses to that paper. So don't make that mistake. Now, what's really cool is that if the sampling rate is greater than two, something amazing happens. You can perfectly reconstruct the signal. Now that's an amazing claim. see something at the wrong frequency. That's an example of aliasing. OK? OK, so here's anexample. We have a 20 hertz cosine wave. I've sampled it at 100 hertz. So I'm, you know, 5-- so what frequency would I have to sample this in order to reconstruct the cosine? I'd have to samples at least 40 hertz, so those are the blue points. And now, if I do this zero-padding trick, I Fourier transform. I do zero- padding by a factor of 4. That means if I take the Fourier. transform signal and I'm now making that vector 4 times as long by filling in zeros, then I inverse.

ROUGE-1: 23.82, ROUGE-2: 22.26, ROUGE-L: 17.15
BERTScore: 64.84

==============================================
==================== [59/100] ====================
Summary:
The science of classical mechanics establishes an important principle of cause and effect. Newton's Laws of Motion established the scientific principle of analyzing observed phenomenon through the use of clearly articulated mathematical models rather than through intuition. Developing a command of mechanics is a powerful tool for understanding the world around us. 8.01 assumes a strong background in high school level physics and mathematics, so a previous course in calculus is not a prerequisite. It is a rigorous and technically challenging course aimed at MIT undergraduates. In each of these lessons, you will find a series of short lightboard videos that will help you understand concepts.

ROUGE-1: 49.32, ROUGE-2: 46.33, ROUGE-L: 44.75
BERTScore: 74.77

==============================================
==================== [60/100] ====================
Summary:
In this video, we'll look at an application of the probabilistic method to graph theory. An independent set in a graph is a subset of vertices with no two adjacent. Caro-Wei's theorem says that every graph G contains a large independent set of size at least the following quantity: summing over all v among vertices G, 1 over the degree of v plus 1. We'll see an application and some ways to interpret this result in graph theory as well as the corollary of Turan's theorem. case when n is divisible by r, you need to do a similar construction, turns out to be best possible, and this bound can be improved slightly, but not by too much. Let us take the n vertices and split them into equal parts. And there are r parts. So here in this illustration r equals to 3. And let's put in all the edges between parts, but no edge within a part. So this is called the complete r partite graph. So you see that this graph here, one can do a quick calculation to see that it has exactly this number, 1 minus 1 over r times n squared over 2 edges.

ROUGE-1: 26.34, ROUGE-2: 25.22, ROUGE-L: 25.96
BERTScore: 67.42

==============================================
==================== [61/100] ====================
Summary:
Professor: Games from last year will give you a sense of what the scope of this class actually is. Professor: You're not constrained to building games that look exactly like those. "I really, really hesitate not-- try not to put any live digital components in your game," he says. "If you make a decision and the outcome hasn't changed, it wouldn't terribly be meaningful," he adds. "That doesn't necessarily make it not a very good game, which is a different thing, right?" I want to-- I try to cover quite a bit of it from the first reading during class itself. Was anyone here not here on Wednesday? OK, all right. So we need to make sure that your name's on the attendance sheet. And you should come and talk to me after class. And we'll make sure you get a copy of the syllabus and everything like that. Make sure that you get expectations for this class. Anyone want to guess what that is, someone who hasn't played the game? a game? AUDIENCE: (COLLECTIVELY) The players. PROFESSOR: The players, right, not the designers, not you. But you when you actually play someone else's game. Then you're the most important person. If that person's experience is problematic or exciting or engaged or outright hostile to other players, or something like that, that's making it the way that they want to take it. If they say, hey, I really, really wanted this to be a lighthearted game, and you gave me this hardcore game. And it made me hate the people around me, the table. And I don't like it, that is fine. In games, the die roll basically tells you what to do, not actually thinking about it. If you don't let the player know what changed the game state, is it that meaningful a decision anymore? Some numbers changed inside the system. It's good to affect how things go out later. But more as a puzzle solving thing. So it's like something changes right away, and you need to figure out what changed. There's feedback somewhere in the world. But I want to say I probably didn't enjoyed games. Candy Land was invented to keep kids from getting polio from each other, professor says. Professor: "There's a huge inversion from the get out and get some exercise" "I've played a number of adventure games that actually have this," he says. "It's a serious game. It has health benefits. Let's there" "You can't really see what your opponent is doing," professor says of competitive games. "You're doing it yourself 200 turns later, like hours later in this game" you don't actually know what happened. And also if you expect players to get very into your game, this is the stuff that you can leave with because they will understand it and figure it out on their own. And then there's an interesting phenomenon that goes with that in a lot of strategy games called save scrubbing. And that is where you know that the outcome of something is based on a probability. And so if you save and reload often enough, you will always be successful. When I do this one attack. And it's like, yes, because you saved the random number seed. So that is actually a problem because people have this concept about how the mechanics of what does 90% probability of success mean? And I am specifically thinking of XCOM right now. If anyone wants to play that. 90% chance of success to a lot of people means that most-- means that it's going to succeed. But for a game whose seed has been saved and you fail and then you reload from the start, it means you will always fail. Don Norman's first chapter in the design of everyday things. The book used to be called The Psychology of Everyday Things. He talks about things like single control, single function. And he asks this question-- why is it so hard to actually make something right? Anyone remember? Or anyone thinking of-- AUDIENCE: Doesn't the designer never really communicate one-to-one with the user? They're communicating through the object that either was designed or being used? PROFESSOR: That is definitely true. So iteration is the reason why design starts off as being very clunky. in a visual game, that's telling me visibly, right now, that this might be the way I get to do that. In a lot of strategy games, some of these things are very literal, right? So and so technology gives you this bonus. But visibility has something to do with the intent of the player of the user in Norman's case. He's talking about the information from other people. And then there's the affordances of the system. Now if I wanted to hide my cards from you, then I will hold my cards in a way that only you can only see the side that doesn't reveal any useful information. design of everything. And what the system can actually do, the actual operations of the system. The other thing is this concept of mapping, that there is this-- again, it has to do with the player's intent of what they want to do. But mapping, instead of the actual Operations of the System, actually has toDo it for the player. So I need to be able to convey to the player that this is not something you can actuallyDo in the game. You can tell which hex to move your tank, but that's it. what you can see of the system. So there are affordances and there are constraints. These are both words that are introduced in that reading. I think affordances is introduced in this reading. What's an example of an affordance? If you can sit on a chair? What about this thing tells you that you can sitting on it? What is one of the things that this-- not in the rule book completely. If you have this in your game, what is this thing good for? face on it, which I always thought was a little bit strange about-- that might have been designed, too, actually. That might have something because if you-- this won't kill you, really. So you should put it in your home. I want to find out more about the history of the Edison plug. AUDIENCE: It's actually-- the design with the ground on the bottom is a bad idea because if something starts falling out that are too exposed to it, it will not be the ground one. recognition is really easy. The way how they've been designed makes it possible for you to do pattern recognition. Pit's one of those games where exchange is a real time thing. It's easy to mix them up too if there was a really good random aspect to the game. The fact a hidden back gives quite a lot of different possibilities. This rule in particular uses mostly because of this game's shuffling and this rule. You don't know what you're going to draw for people who haven't played this game. stationary stores actually do sell punches, corner punches around of your cards. You're not really supposed to have more than one tile at a time in Carcassonne. So it's OK that the tiles are designed in such a way that it makes it hard for you to hold on to do. For the cards, for instance, you're notreally supposed to stack them. They are not very useful to you when you've got them face down because you can't really see the information. couple of things in this game that is this board. There's a back of the board, which is not colored. And it has a design on it. We could just pass this one around. It's got little playing pieces that are referred to as meeples by the hardcover board game fact base, I guess. They look like little people. Actually, there's probably enough in that for everyone to grab one or two. And then you can just take a look. a whole bunch of ways that you can help people with these sorts of mappings. I guess Donald Norman would describe a lot of these as spatial, metaphors to use. Spatial mostly to describe things like driving in a car and you turn the wheel to the left specifically toward the top of the wheel. And you car it's directed to turn left, that sort of thing. There are sort of bodily metaphors as well. Things that are high are either supposed to be good and happy, things that are low when you're feeling depressed. deal and trade your cards to corner the market, et cetera. When it comes to Carcassonne, there is actually a deep, deep problem with this game despite how popular it is. And that scoring is actually pretty difficult to do. It's a math intensive problem. It does largely map on to how many of the meeples that to get all the CarcASSonne bits back. So and then we'll pick this up in about five minutes. We'll be back with the next episode in about 30 minutes. a couple of board games where you have to put pieces together, and that gives you an idea of maybe those two pieces don't go together. What others things look for in the realm of feedback when it comes to board games? AUDIENCE: Maybe a track? PROFESSOR: Hm? You mean a board track? Audience: Yeah, you want to stay on the track. If you fall out of it, you are no longer-- that's not a legal place for you to place your token. Other players are your primary feedback mechanism on whether you've done a move that's OK or not. Everything that you know about the thing that you are trying to guess is something that only other people can see. In Mafia, all the information you're getting in the game is through other people in Mafia. In Battleship, there's information that's hidden from you, but it's completely available to your opponent and vice versa. In Charades and Pictionary, usually your teammates are the ones who are guessing. your objectives from the player that knows the information. The person who's running knows where they're going at any given time. The detectives are working on partial information to try to corner and close this dragnet. So I think that's on the list. Do you remember it? That was on the syllabus. It used to be-- Scotland Yard. But we may have changed the game. But that also falls in mastermind category of games where it's like, here's this person with all the information, only that person is changing the information as the game goes on. That's a big difference in Scotland. order problem. You're designing something that then becomes this manifestation that somebody else uses. And that's actually when all the problems occur. Market forces push you to add things, to do additive design in order to distinguish yourself from the competition. That naturally leads to complexity in interface. And a lot of products, you said, don't get through that process because it'll take five or six times to get it right. But if it's not good by the second time, people just won't buy it. what he's talking about only, I doubt that actually imagined that this was something to be possible at the time when he wrote it. That was 1980. And cell phones obviously have gone through a lot of criticism. But it is it has been successful through a number of different reasons. Don't discount marketing as being a big part of it. It's expensive for what it does. But then, arguably, by locking out a many things that you might want to do but maybe don't have to do, they are trying to make it easier for you not to do the wrong thing.

ROUGE-1: 33.19, ROUGE-2: 31.64, ROUGE-L: 26.57
BERTScore: 61.62

==============================================
==================== [62/100] ====================
Summary:
The two-level problem is one that's exactly solved. It's one of our favorite exactly solved problems, although it doesn't seem to have any physical relevance. So we can take a two by two Hamiltonian and exactly diagonalize it. And that's done using a unitary-- or actually, in the case that we looked at, orthogonal-- transformation. But the good thing about it is it's solved by a computer. You just have to have a computer that's patient or fast. And it will crank out the results. Robert Field: What do spectroscopists do? Field: I want something a little more profound. Field: We record spectra. We get energy levels. And we get intensities. We're not allowed to determine the wave function by any experiment. But we are able to observe the energy levels and properties of the Hamiltonian. And that's the subject of this lecture and the next lecture. It's called non-degenerate perturbation theory. And so you want to know J-- sorry, C-- extra lines and extra lines. you the tools to be able to take any arbitrary spectrum and extract from it the crucial information. In the last lecture, we talked a little bit about matrix mechanics, and that involved linear algebra. And you have a wonderful handout on linear algebra, which will give you more than enough to do any quantum mechanical problem we're going to be facing in this class. And there is notation. And the notation is unfamiliar, and you have to learn how to use it. And so we have for example-- the analogy to the Schrodinger equation in matrix language. put parentheses around things. So this is now what we were calling h tilde and this is c tilde. This is now an equation that says, OK, we can transform the Hamiltonian into diagonal form-- E1, En, zeros. And when we have this in diagonal form,. we can say, well, this equation is just E1,. 100 et cetera. So for any eigenvalue, we have an eigen vector. Now what we'd really like to know is, how do we get these eigenvectors from the unitary transformation that diagonalizes H? Morse oscillator is a cheap way of generating something with this shape and then doing the perturbation theory to understand how the nonharmonic aspect of the Morse can affect the energy levels. Now for polyatomic molecules, if you have n atoms, there are 3n minus 6 vibrational normal modes. And so even if you could excite a pure overtone or combinational level, the energy moves around the molecule. And that's called intermolecular, vibrational redistribution. And those there are processes that you could names that any educated physical chemist will know to say, oh, that's the Bixon-Jortner. to know J, CJ eigenstates. Once you have this, then you know how to write the time dependent wave function. And then there's the origin of life. You need two particles to come together and start to condense into a liquid. That's the beginning. Perturbation theory explains the long range interactions by which all gas phase particles attract each other weakly. So that's important too. And so you'll be able to do all of this stuff. So here we have non-degenerate perturbation Theory. And it is important too, because it is just mechanical and boring. we do is now we write the full equation and we sort it into sub equations corresponding to powers of lambda. So the lambda to the 0 equation is really easy. It's just H0 psi 0 is equal to E0 psi0. We could put n's on this. And this is the exactly solved problem. It says, yeah, you build your foundation from the Lambda to 0 equation, and it's just what you know already. And what you're going to do is use the psi n 0 and en 0 to do everything else. energy levels that we're sampling in our experiment. So the molecule more or less tells you how to focus on the part that you care about. And it just contaminates the wave functions a little bit. And if you wanted to know what that contamination is, you could deal with it. So this is your handy dandy key. And you don't need a computer, although when you see the horrible complexity that will result when you start dealing with these sums, you will say, well, I do want to use a computer. talk about the primary paths for the energy to flow and the rates. And that was a major area of research for the last 30 years. But it started out being IVR, yeah, anything can happen. It's statistic or whatever. But no, only specific things can happen, and they're controlled by these specific coupling terms. And you could calculate them. Now there's an interesting other thing. I told you that the first-order correction to the energy is equal to a diagonal matrix element of the correction term to the Hamiltonian. isn't just blowing smoke. This happens an amazing number of times because stretches are higher frequency than bends. And it's very common for the bending modes to be roughly half or one third the frequency of a stretching mode. And so you get a resonance. So this is special because now it's violating the fundamental approximation of non-degenerate perturbation theory. But it's a two-level interaction. And these resonances have names. There is a Fermi and there is a Darling-Dennison. level V1 minus 1 V2 plus 2 V3. Because of the energy denominators, these two guys are nearly degenerate. One gets shifted up, one gets shifted down a little bit. And it might also be that this state is what we call bright and this is called dark. This state might be connected by an allowed transition from a lower-- an initial state and this might not. So the levels repel because they're interacting and they're out of position. But because of the interaction between these two levels, the eigenstates have mixed character.

ROUGE-1: 30.61, ROUGE-2: 29.52, ROUGE-L: 28.82
BERTScore: 65.90

==============================================
==================== [63/100] ====================
Summary:
The key word is feasibility technology. It represents the feasibilities that the combination of inputs and outputs, that can be achieved in this world even the current level of technology. The second is no free lunch. At least 1 input is required to produce some output, or more than you know more than 1 kind of output. Third is non reversibility, what it means is that a production process cannot be reversed. So, if you obtain half kg of curd from 1 kg of milk, you cannot obtain 1kg of milk from half kg. Student: To produce 1 kg of rice ok, you need either let us say 100 grams of fertilizer or 50 liters of water. We need land, but that is fixed those are fixed, we are not talking about it only these 2 are variables. Student: On average we are saying this is what it is not always true, but I am saying on average, what we can do is that we can produce 1kg of rice here, using t by 100 and 1 minus t by100, 2 comma 1. If y bar and y bar dash both are feasible, then what additivity says that y bar plus y bar hat is also feasible, is it clear. How we can explain it if it's possible to produce it is possible to have this production plan. Let us say let us take an example y bar is nothing, but 1 2 minus 1 minus 2 comma 1. And next the production process says that seem 1 unit of output can be produced ba using 2 units of input 1 and 2 Units of input 2. same proportion, then output will also decrease in the same proportion and this is possible this is feasible fine. The easier is easier way easier although they these two are not same, but the similar 1 implies the other, but not the other way around. The combination is all given here and this exhibits convexity. Here we are not concerned about the amount of output, what we feasible here, y is taking care of not only inputs but also output. It is also possible to combine lambda x 1 to lambda x 2 of course.

ROUGE-1: 22.74, ROUGE-2: 21.34, ROUGE-L: 20.83
BERTScore: 65.29

==============================================
==================== [64/100] ====================
Summary:
The Great Wall began as multiple walls of rammed earth built by individual feudal states during the Chunqiu period to protect against nomadic raiders north of China. Under the Han Dynasty, the wall grew longer still, reaching 3700 miles, and spanning from Dunhuang to the Bohai Sea. After the Ming dynasty gained control in 1368, they began to refortify and further consolidate the wall using bricks and stones from local kilns. The wall was formidable but not invincible. Both Genghis and his son Khublai Khan managed to surmount the wall during the Mongol invasion of the 13th Century. main body and expanding this remarkable monument to human achievement. Main body is made up of three parts: the head, the torso, and the legs. The main body of the main body is the most important part of the body. The second part is the lower body, which includes the legs, the arms, the legs and the feet. The third part is made of the lower torso, which is the largest body of its kind. The last part is called the lower legs, which are made of marble and marble.

ROUGE-1: 36.14, ROUGE-2: 27.54, ROUGE-L: 26.49
BERTScore: 66.09

==============================================
==================== [65/100] ====================
Summary:
Peter Solovits: I got a call from a committee of the National Academy of Science, Engineering, and Medicine. The committee is chaired by David Baltimore, who used to be an MIT professor until he went and became president of Caltech. Solovitz: They convened a meeting to talk about the set of topics that I've listed here. He says the group of us that talked about AI and decision making, I was a little bit surprised by the focus because Hank really is a law school professor. Trevor Noah: Algorithmic technologies may minimize harms that are the products of human judgment. Noah: We know that people are in fact prejudiced, and so there are prejudices by judges and by juries that play into the decisions made in the legal system. He says we should look for people with broad educations, like Trevor's father is white and his mother is African-American and his father is a Swiss guy. Trevor: What do you like about being able to defend a marriage that they were not allowed to marry six years ago? In California, the decision of whether you get bail or not is going to be made by a computer algorithm, not by a human being. The critique of these bail algorithms is based on a number of different factors. The data collection system is flawed in the same way as the judicial system itself, Peter says. Irene: If we're going to define the concept, what is fair? What characteristics would you like to have an algorithm have that judges you for some particular purpose? Yeah? When I was an undergraduate at Caltech, the Caltech faculty decided that they wanted to include student members of all the faculty committees. In those days, Caltech only took about 220, 230 students a year. So one day, one of the professors said here's what we ought to do. We ought to take the 230 people that we've just offered admission to and we should reject them all and take the next 230 people, and then see whether the faculty notices.look like they're a better bet. Peter Zolovits: Disparate treatment and disparate impact are really in conflict with each other. He says we don't have a nicely balanced set where the number of people of European descent is equal to people of African-American, or Hispanic, or Asian, or whatever population you choose. Zolovich: The good news is that there's a terrible piece of news, which is that you can prove that it's not possible to achieve any of these pair of conditions jointly. In the US, there were tests of this sort done, but the problem was that a lot of African and African-American populations turned out to have this genetic variant frequently without developing this terrible disease. And it was only after years when people noticed that these people who were supposed to die genetically weren't dying that they said, maybe we misunderstood something. And what they misunderstood was that the population that was used to develop the model was a European ancestry population and not an African ancestry population. So you go, well, we must have learned that lesson. The study looked at how machine learning models can identify disparities in general medical and mental health. It found a racial bias in the data that we have and in the models that we're building. In psychiatry, when you look at the comparison populations, you see a fair amount of overlap. The models are not going to give us as accurate predictions, but you still see, still, a huge gap in the confidence intervals between them. The study was published in the American Medical Association's Journal of Ethics, which I didn't know existed. is choose some family of models to try to fit, and then I'm going to use some fitting technique, like stochastic gradient descent or something, that will find maybe a global optimum, but maybe not. And then there is noise. And so his observation is that if you count O as the optimal possible model over all possible model families, then the bias is essentially O minus L. The variance is like L minus A, it's the error that's due to the particular way in which you learned things. can't define a universal notion of what it means to discriminate because it's very much tied to these questions of what is practically and morally irrelevant in the decisions that you're making. And so it's going to be different in criminal law than it is in medicine. And it's feature-specific as well, so you have to take the individual features into account. The government has tried to regulate these domains, and so credit is regulated by the Equal Credit Opportunity Act, education by the Civil Rights Act and various amendments. scoring function is independent of the protected attribute. So that says, can we build a fair scoring function that separates the outcome from theprotected attribute? So here's some detail on those. If you look at independence-- this is also called by various other names-- basically, what it says is that the probability of a particular result, R equal 1, is the same whether you're in class A or class B. That tells you that the scoring function has to be universal over the entire data set. And then the final criterion is sufficiency, which flips R and Y. Hiring is based on a good score in group A, but random in B? So for example, what if we know a lot more information about group A than we do about group B? Or alternatively, it could be caused by malice also. There's also a technical problem, which is it's possible that the category, the group is a perfect predictor of the outcome, in which case, of course, they can't be independent of each other. And so they say, well, we could use an optimal separated score. which is some combination of X and A, and you do this by maximizing the mutual information between X and Z. So this is an idea that I've seen used in machine learning for robustness rather than for fairness, where people say, the problem is that given a particular data set, you can overfit to that data set. One of the ideas is to do a Gann-like method where you say, I want to train my classifier, let's say, not only to work well on getting the right answer, but also to work as poorly on identifying which data set my example came from. does in other populations, and the FDA has actually approved the marketing of that drug to those subpopulations. And if you think about the personalized medicine idea, which we've talked about earlier. The populations that we're interested in becomes smaller and smaller until it may just be you. And so there might be a drug that works for you and not for anybody else in the class. But it's exactly the right drug for you, and we may get to the point where that will happen and where we can build such drugs. degree of calibration will give you a good approximation to this notion of sufficiency. These guys in the tutorial also point out that some data sets actually lead to good calibration without even trying very hard. So for example, this is the UCI census data set, and it's a binary prediction of whether somebody makes more than $50,000 a year if you have any income at all and if you're over 16 years old. It's almost exactly along the 45 degree line without having done anything particularly dramatic in order to achieve that. The probability that you visited the Grace Hopper Conference is dependent on your gender. Computer scientists are much more likely to be programmers than non-computer science majors. The optimal score is going to depend basically on whether you have a computer science degree or not. If you're a historian, you're not likely to have been interested in going to that conference. It's a really cool conference. Grace Murray Hopper invented the notion bug or the term bug and was a really famous computer scientist starting back in the 1940s. We used LDA, standard topic modeling framework. And the topics, as usual, include some garbage, but also include a lot of recognizably useful topics. And so what we found is that, for example, white patients have more topics that are enriched for anxiety and chronic pain, whereas black, Hispanic, and Asian patients had higher topic enrichment for psychosis. And we were speculating on how this relates to sort of known data about underdiagnosis of COPD in women. So again, stereotypes of what's most common in these different groups. Public insurance patients often have multiple chronic conditions. Public insurance patients have atrial fibrillation, pacemakers, dialysis. Private insurance patients has higher topic enrichment values for fractures. The error rates on a zero-one loss metric are much lower for men than work here and embarrassing myself. So this is modeling mistrust in end-of-life care, and it's based on Willie's master's thesis and on some papers that came as a result of that. It could be any of a lot of different factors, but that's the case. The study looked at trust between patients and their doctors. It found that black patients were more likely to be distrustful of the medical system than white patients. The study also found that mistrust was related to severity, not to social differences. The researchers are still trying to understand what the correlation is between mistrust and severity, and how it's related to the patient's ability to be trusted by the medical care system. They are also trying to find out how to make the study more useful for the general public.

ROUGE-1: 28.46, ROUGE-2: 26.10, ROUGE-L: 23.62
BERTScore: 61.60

==============================================
==================== [66/100] ====================
Summary:
Professor: Today we're going to talk about partial differential equations. Next time I'll be lecturing to you will be a week from Friday. homework involves a COMSOL problem which might take some extra time, since it's the first time you're doing it. There's no lecture on Monday, but you have the quiz, too, Monday night. Wednesday, next week is Veterans Day so it's a holiday. There're no classes at MIT. The homework following the quiz will be posted and so you can get started on that. an equation like this, is that whatever partials you see-- When I have this and it has a term that's like d squared, dx squared, is one of the terms in this thing. This says t, this says x. The convention is, oh boy, I better hold x fixed. And here must be, I must hold t fixed. Because there's another one in the same equation. Now, you can change coordinates however you want. Just really watch out that you carefully follow the rules about what you hold fixed. Otherwise you can cause all kinds of craziness. methods. So all those methods basically look the same. You just get equations that have more unknowns in them. And so fundamentally, there's no problem, but there's a big problem if the number of unknowns gets to be so large. 10,000 unknowns are a lot harder to solve for than 100 unknowns. And once you get up to a million unknowns, we're starting to talk serious up to three dimensions, you might be able to solve it or might not. math, now. In the Navier-Stokes equations, what variables do you have? So you have pressure. What else we got? Density or temperature. So then you'd have all these species, right,? The mesh fractions are all the species. So that's a pretty big set of variables and you need to know all these numbers at each point. So the issue, to a large extent, has to do with just, can I solve it? Can I solve a system of 10 million unknowns? A lot of combustion problems are very sensitive to the physical initial conditions. A very small spark can make a really big difference in a system like that. Another class of PDEs where it's stable in some sense, but it causes a problem. A lot of these things, light a spark on a stove, it ends up basically how you light it. So I do a lot of work on combustion problems. So that's one idea about what to do for initial guesses. Another idea is the time marching idea. If it does, then this is a good idea. ones where there's some dissipation that's very significant. And so, in an elliptic problem, you introduce some numerical noise and as you solve it, the numerical noise kind of goes away. But if you try to do it the other way around, it would be kind of weird, right? You wouldn't expect that what in the future really affected stuff in the past. So it naturally leads us to sort of pose it as an ODE IVP, or IVP kind of problem. And the methods that we're going to talk about are ones for solving parabolic and elliptic problems. a flow in a pipe. And you have a nice, fast velocity flow down the pipe. What's happening downwind is very much affected by what happened upwind, but not vice versa. So it has a directionality, even though I might do it as a steady state problem and not have time in it at all. And so if you look at figure 6.8 in the textbook, you see you get perfectly stable solutions when you do that. But where you do this one, you get crazy, oscillatory, unphysical solutions. If you look in the textbook at figures 6.7, 6.8, it's kind of a very famous problem. If you make the local Peclet number too large, actually anywhere bigger than 2, and you try to solve this equation, what you get is oscillations, unphysical oscillations. It's crazy. It looks like the simplest equation in the world, right? It's a linear equation, so what's the problem with this? But it doesn't work. And so, then you could think, well, why doesn't it work? And there's, like, multiple ways to explain this. a diffusive flux coming in here. I don't have any drug up here, so there's no drug coming into here. But I have drug leaving. So basically, it's coming up and it's making a right-hand turn. And so then I could think, well, if I was going to try to figure out, what's the steady state concentration of drug in there, I could say, there's a certain amount of drug entering. And then this part over here would be the velocity, actually, just times the concentration. In this case, we have a 100 million squared matrix with 10 to 16th elements in it. Most of the numbers, are those 10 to the 16th numbers in this matrix are zero. But there still might be quite a few. Probably the diagonal events are non-zero, so that's like ten to the 8th of them right there. So, even if, initially, you can write down j and store it in your computer, you could easily overwhelm it with the third iteration or something. So what do you have to do? You want to find a method. Direct methods to solve this kind of problem. solve this. So you just try to minimize that, right? You know at the solution this is going to be zero. Right, jy plus [INAUDIBLE] zero when this is solved. And then this method, it turns out that the conjugate gradient, if everything works great, this is guaranteed in n iterations to go directly to the solution of this kind of problem. And so, they've worked out better methods. And there's one that people use a lot for these problems called bi cg stab. too big. Now, this is really good if an explicit solver can solve it. And also, if you want a time accurate solution. But suppose we're trying to really solve the steady state problem over here. Then we really don't care about the time accuracy. In the end, we're only going to report our steady state solution. That's all we cared about. So in those cases, you might instead want to use-- So this is time accurate. There's another method. I don't know if I should call it time inaccurate. set up is right, so you can use it. But if you can, it can be pretty good. All right. See you on Friday. Back to Mail Online home. back to the page you came from. Click here for the latest from CNN.com. Back To the pageYou came from the page You came from: Back to the Page You Came From: Back To The Page You Were Originally From: The page you were originally from: The Page you were initially from: the Page you started from.

ROUGE-1: 24.84, ROUGE-2: 23.19, ROUGE-L: 21.64
BERTScore: 65.08

==============================================
==================== [67/100] ====================
Summary:
elizabeth the first is considered one of the greatest rulers in england's history if not the greatest for women. She never married can you blame her i mean look at what she grew up with around her dad and all of those moms and stepmoms she never had any kids which that causes a problem come her end time okay where we have i don't have an heir so she eventually does set on a cousin up in scotland which is the country on the same island of england. She names him as the heir so when elizabeth died king james of scotlands comes down and he is now the king of englands and the king  of scotland it's like he united two kingdoms. know how to read latin or french you were stuck by solely getting your material this is what the bible says solely from your clergy solely from the church so you were kind of a mindless robot with regards to comprehending the the word of god or whatever the bible said on your own. Now that something is in english and if you can read english you are able to read it over and over get your own theories your own arguments and make your own decisions. So you were less mindless page 414 is a really nice page that if you need to come back to review the significance of this or you can't remember um it's pretty good you can see down at the bottom.

ROUGE-1: 55.53, ROUGE-2: 53.91, ROUGE-L: 55.53
BERTScore: 78.48

==============================================
==================== [68/100] ====================
Summary:
The average velocity depends on the time interval t to t plus delta t while the person has displaced a certain amount of vector delta r. So, as a vector, we have delta x over delta t i hat. This component here is what we call the component of the average velocity. And, again as before, this component can be positive, zero, or negative depending on the sine of delta x. And now what we want to do is consider what happens in the limit as delta t becomes smaller and smaller. And that will enable us to introduce our concept of instantaneous velocity.

ROUGE-1: 55.17, ROUGE-2: 53.33, ROUGE-L: 55.17
BERTScore: 82.08

==============================================
==================== [69/100] ====================
Summary:
This week we'll look at the super magic formula. The formula is a compaction of a lot of math. We'll use it to derive a general formula for something moving on something that's moving. The class will also work on the spider problem and the Frisbee problem. The course is free and open to the public. Use the weekly Newsquiz to test your knowledge of stories you saw on MIT Open Courseware. For more information about MIT open courseware, visit MIT opencourseware at ocw.mit.edu. The Machinery I provided you is powerful enough to solve pretty much any kinematics problem because it's based purely on Geometry angular velocity is a kind of a madeup concept that just simplifies the math and it's kind of intuitive stuff is rotating Theta Dot multiplied by you know the Le lever arm right gives you a velocity. The velocity of the frame doesn't mean anything unless I give you a reference point so points have velocities frames have angular velocity good points of accelerations frames haveangular accelerations. This what is the English statement here it is the angular of velocity frame B with respect to frame a right okay so we saw that for the first time now a little bit on the low of angular velocity that you've seen in previous classes but we we Plunge in anymore. If a frame's rotating some point different points will have different velocities so yeah a frame can have a velocity but doesn't really mean much and you tell me which point in the frame you're referring to right look this thing is a frame. If I said look I'm standing at this right I'm going to rotate about my vertical axis and then do this my hands up here right but if I rotate my hand first and oh that did the same thing but you know what I mean angles don't add up right. But infinite decimal angles do add up you remember this you did this in physics. If I rotate about one axis a certain amount 30° and then rotate about another axis 30° then the sequences do actually add up. In the last class I revealed to you this magic formula I said if you have a vector let's call it n and you want to take its derivative with respect to the frame a now if you the way we did it was we would rewrite in in terms of the basis vectors of frame a right. We introduced the the velocity and then the next thing we did last last in the lastclass was I revealed the magic formula. We can rewrite this as what so um first of all L is a scalar so there are a couple of routes we can go here okay. but you need this correction term which is very beautiful done done okay so this was the magic formula that I introduced in the last class how would we go about proving this anybody yeah do the math how would I go about doing it. Here's what we'll do let's prove it let's consider a frame a by the way you notice I will always show frames with this little circle a saying that it's a frame and with a little wiggly line pointing to it. You can always assume if I have a capital A that little A1 and little A2 are the basis vectors in b. yeah let's Express exactly we're going to express B so here's what we're looking for we'reLooking for a d by DT of N and look n is expressed in terms of B1 and B2 that's really not nice we need expressed interms of A1 and A2 to do it old way the long way right. Theta do B3 cross product with u 1 + B1 U1 sorry B1 + U2 B2 sorry right and if you do the math does it come out to be these two terms yes or no yes. I'm not doing it because I want you guys to stay awake in class I don't want to just blindly copy down the notes here right. you're confused that is fantastic because that's a symptom of something else and I'd like to surface it and help you figure it out any confusion about this anything. So far I've used these words coris and all that I kind of called them out without telling you a lot more about them right so what we're going to do now is take a break in the following way I'm going to show you some videos showing you the coris force when I'm when you're done with these videos. let's do that so let's imagine I'm spinning a basketball on my finger I could never do that but let's just imagine I's doing it right. The room the world the Earth is a frame the basketball is another frame the kind of the U you know you guys need to be like Neo in Matrix have you seen that movie right it's very very important if you understand Matrix algebra you need to see Matrix all right so go see Matrix and you know how Neil kind of you know he when he's totally you know after Lawrence fishburns you know kind of trained him Etc. Yeah okay using um frames uh using angle of velocity so the problem was we have the ground and the spider. With those three parameters we're going to capture the blocation of the spider but we're using three parameters but in the end the SP spider is only going to have two degrees of freedom right but that's okay because when you get the velocity of theSpider it'll have only two components when get the acceleration only have two components so it'll work itself out what are the three param the three parameters are u v and L the way we're doing our geometry. can kind of do it that way but the simple way to do it is to simp well you know it doesn't matter we can just write this as B take the whole term d by DT of lb1 plus what quick hm I'm hearing a hesitant murmur not the yeah I was mumbling something a moment ago I'll tell you what I mean what I was trying to say uh cross lb1 what is a Omega B so we can rewrite a Omega b as Theta dot B3. What happens with things like hurricanes and we'll show you I'll show some videos later if we have time is. What is the force that the astronaut feels you know or what force must the astronaut exert in order to move in a straight line with respect to a spaceship or a space shuttle or something? That's the question we seek to answer and what we're going to do is basically generalize the Frisbee problem and the way we're gonna do it is by looking at the Coriolis effect. Water rotates in One Direction or not in the northern hemisphere another Direction on the southern hemisphere is that right is that valid yes or no why not yeah it's turns out that in that scale in fact there is a Coriolis effect but the slight deviation angle is so slight it's like a fractions of a degree per meter at that scale that it doesn't build up enough to really impact the direction in which the vortex takes place. The vortex is created far more by asymmetries in the sink or by rotation that was created earlier that can of remained right. Theta Dot and an l dot term get it then there is a deflection in the B2 Direction in the way we've written it now I'll give you the general formula and you know B2 what it is ETC it become more clear any questions about coris by the way in the last class I told you this anecdote I said that artilleries artillery guns right in the 1700s 1800s um they would have to compensate for directionality because of the coris effect it turns out that that is true. rpq so this is the velocity version of the magic formula and I'm sure you'll admit that I did nothing particularly profound in deriving this okay and then that handout the typed handout that you you hopefully downloaded and read from the web you will see this EXP expand it out for you okay so that means that if you have a frame a which is stationary and something's moving on frame B then the velocity of that thing on frame A is simply the Velocity of uh a point on frameB with respect to frame B. yeah sorry and plus this thing okay we'll end now because Professor Socrates needs to take over this is it this is correct acceleration of P acceleration of Q with respect to B Oiler coris centripedal and you know why it doesn't go to zero because I'm not taking a Omega B cross a omeg B. I'm taking a omega B cross this whole term which is at 90° to a OmegaB got it got it? Yeah sorry and Plus this thingOkay we'llend now because professor Socrates need to takeover.

ROUGE-1: 27.28, ROUGE-2: 25.89, ROUGE-L: 23.12
BERTScore: 70.71

==============================================
==================== [70/100] ====================
Summary:
Al shalot is the CEO of GT Sports. He is also a member of the Olympic commission the international Olympic Comm. He has been involved in the conception and Advising the I on what's not public the Olympic sports games. The turning point for him are the Asian Games the people from the Olympic Committee when they attended this event they're like okay now we get [Music] it hi there my name is Al shalots. I'm the CEO of GT Sports uh it's an official Olympics Esports organization pretty Global they call us the real Madrid of Esports. to connect with the audience the youth audience where they are and to leverage gaming and Esports to do that so it was strategically cited and we're just part of a discussion about how to implement it. I think uh it's totally possible that in the future uh finding the balance as always between popularity relevancy and a certain fit with the values that the Olympic Committee wants to project this kind of game will be futureed. I'm I have to be optimistic and and I'm very positive one of the big criticisms of the Esports World Cup is that there were 22 events just one of them specifically for women. Saudi Arabia is the fastest growing region of the world right now for gaming and eort so this is why PC always brought the idea to bring a property and events to a country to create some bridges. I remember when when Beijing was elected to host for some Olympics it created kind of like a a moment of like creating Bridges with like China and I I see the same way gaming entertainment in general could be a breach now for Saudi Arabia. There's been a lot of criticism about Saudi Arabia as a host uh some LGBT players uh women saying that they don't feel like they'd be welcome. There's not a huge amount of female participation at the moment at that event what are your hopes for how to improve that. Paris Olympics this summer will to perform with the highest level and then it will be the job of the rest of the industry to build the the step stones to get there. There might be some people who might feel a bit disappointed about that potentially it's not my role to say yes or no because I'm not decision maker but I would their country wherever the country. Olympic opening ceremony will be similar to the summer and winter games. There will be something you know with a flag ceremony probably at the beginning. I think it's going to make a huge difference of like touching emotionally people that have not been touched so far by easts. I'm French so I I will cheer for the the national team of League of Legend uh potentially with one of my players on representing my country. We have been dreaming of this like France against Spain with the best player of the World. "Everybody wants to see what a dennish team will do against Corin I I would love to see it too you know so um I've seen a lot of players making the the guess or what the team could look like so his aspiration to be part of is is clearly part of the discussion," he says. "You follow a bit like the conversation about but uh rocket league and OverWatch back in the days at the at the World Cup for for their game and and and League of Legend also often have this conversation about creating the Euro Cup or World Cup"

ROUGE-1: 27.73, ROUGE-2: 26.31, ROUGE-L: 21.36
BERTScore: 66.46

==============================================
==================== [71/100] ====================
Summary:
"Unlockable" can mean either it's possible to unlock it, or it is not possible to lock it. There's an "-able" that changes verbs into adjectives, and there are "un-"s, one that combines with verbs. "Un-" can combine with either a verb or an adjective, and so we get this ambiguity. And the ambiguity is what we would expect it to be, yeah? This is all review. Go ask some people whose minds have not been contaminated by linguistics. in the following way. We'll say some things which are clearly true. There's a "-able" suffix that changes verbs into adjectives. And then, we said, there are two "un-"s. An "un-" that combines with verbs and makes verbs. That's an "un" that means something like undo the effects of, or change something so that it is no longer in the state that it would have been if the verb had applied to it. "Un-" number two combines with adjectives and makes adjectives that mean more or less "not" horrible thought, and it's random. When Alexander cut the Gordian knot, he wasn't untying could imagine that it would, but that's not what it means. If I untie a shoelace, first of all, it has to start off tied, is that right? So if I have a Shoelace which is not tied, I can't untie it. But if it's tied and then I take scissors and I cut it into many small pieces, have I untied it? No, surely not. true, but it is not a complete description to say that those are two sentences that consist of a noun, a verb, a preposition, a determiner, and a noun. We had this operation, we were calling it merge, that takes pairs of things and puts them together into a larger thing. And similarly here, what we're going to do to create a sentence like "I will find the red book," we'll start with just the end of it, "find" You can ask questions like, "Up which stairs did John walk?" It's a strange way to ask the question, but you can say it. "Up the cats" is not a prepositional phrase. You can say, "I will wake him up" but not "up the cats up" "We up" is different from "I'll walk the stairs up," I think. I think maybe "up" is modifying "to her" or "to the student" I think I'm using her as a ladder. Theory of syntax would divide sentences into three kinds. There are sentences that you've heard a zillion times before, like "We're going to class" And on the other hand, sentences you have possibly never heard anyone say, but that are fine. "My anteater is hula dancing," you may never in your life have heard that. But it's an OK sentence. As opposed to, "We's class going to," which I've given a star there. Recall that the star is what syntacticians give to things that are bad. We're not going to be talking about prescriptive studies of English grammar. We're going to try to get sentences like, "This is the answer" or whatever. Flies, sudden heart attacks, and then other kinds of things that are maybe less clear to think what to say about them. Nobody ever performs an infinitely long sentence, but we're competent to produce them. And then, yeah, eventually people die, and so nobody ever says a sentence that's infinitely long. In English, there's this distinction between the examples where leaving a preposition behind is what you prefer, "Who are you talking to" But you can kind of say, "To whom are youtalking?" And others, were "About what are you Talking?" Is either of those acceptable at all? "What did you leave despite?" and "Despite what did you left?" Who would do anything to avoid saying either of these things? Yes. So here's another example of one sentence that's quite bad and another sentences that's only bad-ish. Vince Richards: We can have English sentences that consist of two adjectives modifying a noun. He says certain types of words are categories that you can add more words to, even if you don't know what they like. Richards: The problem here is that there are facts about how English pronouns and English particles work that mean you can't get to the meaning of a sentence that doesn't mean anything, but you feel as though it doesn't, right? He says it's possible to have feelings about sentences of the form, well, I don't understand what this is supposed to mean. NORVIN RICHARDS: English speakers end sentences with prepositions every day. "Who are you talking to" where, without the "to," it doesn't have to be about redundancy, he says. "To whom" is part of this phrase that's at the beginning of the sentence, he adds. "Why does "to" have to come along, according to Mr. Richards? I don't know about you guys," he says to the audience, "but it's not my go-to way to say this" your English teacher? Yes? AUDIENCE: Is this another example because that's how it's done in Latin? NORVIN RICHARDS: Yes. English is quite rare in being able to do this. Most of the languages of the world can't. Some time in the 15th, 16th century, a number of grammarians decided that English would be way cooler if it were more like Latin. And that's why your English teacher told you that you can't say this, which is ridiculous. here's another useful distinction. It's sometimes called competence versus performance. Imagine that I'm standing up here talking to you and I say, "This is the--" and then I Inhale a fly. And then imagine that this experience is so traumatizing for me and also for the fly, that I just I never complete that sentence. So we're going to be talking about speakers' competence, what they would do if there were no distractions and no problems. There's also performance. That's the study of what people actually do. Journalists know that the best way to make someone look like a complete idiot is to quote them accurately. What journalists, in fact, do is to clean up all that stuff so that people sounded like they were talking in complete sentences. So we're going to develop a theory of what English speakers say, but it's going to be a theory that's divorced from reality to a certain extent. We're Going to Imagine what English Speakers Say is going to look like to you and me. There is no bound on the length of English sentences. For any sentence in English, it's always possible to create a longer sentence. You can always say, "She thinks that S," where she maybe refers to different people in every clause. When we say it that way, you can tell that I am talking about competence because no matter how long you say it, it can keep going arbitrarily long. It's a possible sentence of English. "Mary thinks that John thinks that it's raining" is a possible English sentence. begin doing some syntax? Here's the sentence, "I will find the red book." grammatical sentence. It's clear what it means, although we've just said it doesn't matter whether it means anything. We're going to look for these units. We'll give it the label "noun" Kind of like when we added "un-" to "lock" and got "unlock," we said, oh yeah, this is also a verb. It acts like a verb in every other way. will find the red book," for example, we'll see that syntax treats that string, "The red book" as a unit. It's possible to say things like "The blue book, I will leave right where it is" but not "Find the red" "Thered book," we want it to be a substring that has certain privileges, as opposed to "find the red," which you can't do those things with. This way of talking about words has the virtue of giving us a vocabulary for talking about those kinds of kinds of observations. "Find the red" is an acceptable answer to a question that you're finding this-- suppose you have this fictional-- this children's game where there's a bunch of little tiles. So we have some cases where we have things that you certainly-- should be adjectives, that either we're getting to use them as nouns, or we're using them as adjectives. It's kind of the same thing-- "land of the free," "home of the brave," NORVIN RICHARDS: Oh. a unit that we've created as we've been putting these pairs together that consists just to "find the red" Do people see that in this tree? So there's a node in the tree, if you want. It's the one that I circled in red that consistsjust of the words "the red book" But there is no thing that I could circle that would consist just of the word "find" That's a unit that topicalization gets to make reference to. that's a property of this tree. is that it contains a noun. "Find the red book" we're going to give that the label verb, because having a verb is the important part for that. "In the garage" is a unit, it's a constituent. It's the kind of thing syntax gets to care about. And again, if I say, "I will find the book in the garage," and you're amazed, you can say "in the garage?" It's a prepositional phrase. just contain a preposition, like, "I will look up," where, again, you can say, "Up?" (Why will you look up?) "I said I would look up, and up I will look"-- Maybe. Maybe this was your point, maybe we want "up" and "to your room" to be separate adverbs. But just like you can walk up to your room,. you can climb a ladder up toyour room. The ladder isn't-- AUDIENCE: --or the stairs. don't want to think of "up the cats" as a unit that has the cats as an object. We want "up" to not be a preposition that's combining with the cats to be aprepositional phrase. "I will wake up the cat" is OK. But when you're done, the cats are up. And possibly also red. It depends on how you did it, I guess. And so we are learning-- this is why I'm glad I'm not a physicist. So I work in the domain in which up and down can be the same thing. her room. I will-- but similarly, you can walk the student-- so I keep going to the student from her, because there's a difference between pronouns and non-pronouns. You can't walk up the student, unless the student is lying down and you're walking on her. But if you mean you're going to walk with the student so that the two of you are up, so we want "walk the student" up to be different from "wake the cats up" construct syntactic trees for strings of words. It's often the case that we get ambiguities like the "unlockable" ambiguity. There is more than one way to combine things. And what we'll do is develop tests that allow us to see which way we've combined words in different ways. And we'll find cases where, depending on in what order you combine things, you get different meanings, and our tests will combine with that. All right. We will do this again on Tuesday.

ROUGE-1: 25.93, ROUGE-2: 24.54, ROUGE-L: 19.87
BERTScore: 67.99

==============================================
==================== [72/100] ====================
Summary:
William Wordsworth is a very famous individual along with the next person we talked about Samuel Samuel Taylor Coleridge probably the two of the most influential of this particular era. The piece that we'll read today it's heavily influenced by his past you know they say that he was a true literary pioneer he defied the conventions of his time by insisting that poetry should be should express deep feelings about everyday experiences. The next piece we read is called The Rime of the Ancient Mariner forColeridge that was as we will read here and for the intro for that. Sister I've never lost a parent um I don't know if anyone has but I would imagine that's a big gap in your life and you may be bond a little bit more with your siblings maybe she being the only female he has a sense of protection for and so being separated you can imagine the struggle that one might have in dealing with it with the loss and so he you know he longed to be with her and such and it wasn't until it said the mid-20. This is probably one of the most difficult readings that we have throughout the semester I don't think it's impossible but you're going to have to you're gonna have to work a little bit ok. Just kind of grab onto those moments those moments of nature and such but also as this is kind of like a monologue where he's going on all of a sudden we find out at the end that he's actually talking to somebody. If the rhythm gets choppy and you start to get lost refocus this is only about eight minutes so it's not a long one but focus on what he is saying about nature from time to time. different than the typical every line it's kind of its own little thing and so when it's read in a different way it's it might see a little Jill at times button jamming is something very easy to visually identify a once you understand oh that's in JAMA it's a piece of cake okay. It might give you a little bit of reservation here and there but it shouldn't okay what I want to talk about first is a 786 lines composed a few miles above Tintern Abbey on Abbey you know a religious building. a little bit you know there's one look at you know a grass I think there's no roof on it but imagine what it'd have been like to walk through this when it was a practicing monastery and such what was the like with that big roof on there with the stained glass windows all still in there I mean we don't have much of this here in Fort Wayne you knowthere might be a couple bigger churches sprinkled throughout that are kind of still Gothic influence but you know things like this it's pretty pretty amazing. he talks a lot I mean just describing you know leaning against the tree and looking out amongst the field on looking at the orchard with the you know it talks about the all of the which at this season with the unripe fruits are clad in one green hue. He almost has kind of a teleportation back to some degree of when he was a child okay he goes into great details about when I found through around the river like a like a row like a deer and I would play and when I was little I was having a great time. you distinctly remember a visual image have you ever gone back to that same place and like oh yeah this is the place one of my earliest memories was I was like two or three and I went to Disneyland on California I remembered nothing throughout my life except I remember visually what it looked like and so I took some journalism students to a convention there four or five years ago and I remember standing there in the front of Disneyland in California and watch the train go across. It's kind of a nice little Wow think about how I've changed in my life think aboutHow I've grown up things that I've experienced. hitting it wasn't now we know it's not just because he wants to record all the beautiful panoramic surround it's recording my time with my sister as well okay and so to see that they can appreciate that you know a few lines in your nature never did betray the heart that loved her nature's power over the mind that seeks her out is such that it renders that mind impaired impervious to evil tongues rash judgments and the sneers of selfish men and instilling instead a cheerful faith that the world is full of blessings.

ROUGE-1: 33.44, ROUGE-2: 32.70, ROUGE-L: 29.46
BERTScore: 64.51

==============================================
==================== [73/100] ====================
Summary:
Professor: We're going to be taking this IV curve that we've so laboriously set up and understood-- sorry about that. And now we will subject it to illumination. And as part of today's lecture, we have some wonderful little kits over there in the corner where we'll actually be testing IV curves of solar cells. We laboriously filled this out last class. We're just going to refresh ourselves to make sure we're all on the same page and redo it this class right at the beginning. Our goal is to understand solar cell conversion efficiency, which is the ratio of output to input energy. For most solar cells, this breaks down to the following progression, from the solar spectrum to charge collection. If any one of these is low, the efficiency for the entire system is low. We're going to be focusing on charge separation, incorporating elements of either side. We'll be in a good position to understand the different technologies and finally the cross-cutting themes. The efficiency of the solar cell is the product of each individual efficiency. Fermi energy, the chemical potential, is the same on either side of a solar cell. To get the solar cell to go into reverse bias, you really do need to bias it. Electrons can now diffuse over from the n-type into the p-type side, and that's why we have this diffusion current dominating in the dark. In the illuminated case, we'll have all of our carriers traveling from left to right. And in the increasing the number of electrons over here, which will naturally cause the energy of those electrons to increase on this side. In a pn-junction dark, this is the only case in which we have carriers traveling from right to left. And it's happening because we're using that battery in the dark to change the chemical potential on either side. So you can think about it as forcing carriers up the junction. And this is a very useful technique because, in effect, what it's doing in a real device, when we you have a two-dimensional device within homogeneities, the current will travel through the weakest point. let's try to imagine what will happen under illuminated conditions, and let's start out in a very simple case. We'll assume that the principle of superposition applies here, that the photo-excited carriers-- in other words, when light shines into our device. What will happen to those electrons now that they're in the conduction band? Where will they want to go? OK. So what you'll do is set up what's called an illumination current. And that's what shifts our entire curve down. The illuminated IV curve is, to the first order, is just your dark IV curve built up across the p and the n side of the solar cell. And it would be the maximum voltage that could be supported by that solar cell device under illumination conditions. Under all other conditions of operations, we're putting power into the device, not getting power out of it. This IV quadrant over here, this forward bias illuminated case is the only case in which power is coming, usable power is Coming out of our solar cell that we can use. In the dark is important because we can test our devices in the dark, and we can still learn a lot about our solar cell device characteristics. We can force carriers from one side of the junction to the other the wrong way and probe for weaknesses in the pn-junction regions. In the illumination condition, obviously we're testing the total amount of power that's coming out of our device. It's off of this red curve here that we defined the efficiency or the performance of the solar cell. your solar cell, you have a bias. Whether that's generated by light or a battery, that's a matter of detail. Photons are forward biasing the device. Can some ever reverse bias? That would be very difficult. What you could do, though, is have a bunch of solar cells connected in series with this one, right, that are producing forward power. You could shade this device, and then power it from the sun, for example. It's just photons. The maximum power point is the point at which the solar cell is producing the maximum amount of power output. Most often, PV researchers will report a current density, in other words, a current per unit area instead of the actual current coming out of a solar cell device. To ensure that the solarcell device is operating right here, you need to have the right illumination conditions, and youneed to have a good light source, the professor says. He says the ideal diode equation is a combination of the Jsc, Voc, and Jmp variables. Open-circuit voltage is just as you would think it is. When your solar cell is in an open circuit, when you-- say you took a pair of scissors-- please don't this-- and cut the leads. There's no energy gain of the electron traveling through the external circuit, but there's a maximum current. And no power flowing through that external circuit because there's no potential to be dropped across the external resistor. So remember, the Fermi energy is the same on either side. have the right load. So the two need to be matched to each other. And that's where some of the power electronics come into play. And the rationale for that assumption is as follows. The open-circuit voltage, this point, is generally between 0.35 and 0.4 volts minus the band gap, or lower than theBand gap. So you have the band. So it's a very interesting question. Let me repeat it so that the microphone can hear it. The homework question in the last homework, there was one question that inquired. not the current density, the current at the maximum power point. So take this current density and multiply by area, and that's effectively-- the units work out better that way. So be very careful whether you use total power in or normalized by unit area power, right? Just keep track of your units. Don't do like the professor. OK. So we have efficiency here as point divided by the solar insulation, fill factor being defined as the ratio of Vmp Imp product divided by Voc Ioc product. power out versus power in, the power out being the maximum power point power and the power in being the illumination from the sun. OK. This is starting to get interesting because it's beginning to click. Pieces from lecture number 2 come together with what we're seeing now. So this is solar cell output power at themaximum power point and sunlight coming in. And that box looks like this blue one right here. The area of that is the area of the solar cell that is producing power. box is Jmp times Vmp. OK? And notice I have another box around here. I have this clear box that starts at the Voc point and the Jsc point. And now I have two rectilinear shapes, this blue one and the clear one right here, the bigger one. The bigger one has an area of Jsc times Voc. And I'm going to define a parameter called fill factor, which will be the ratio of these two areas. If this is 1, which is virtually impossible to do, but if this were 1, it would mean that these two boxes were the same size. pay more for a high-efficiency cell because I'm using less area, you can use this type of calculation to get to the answer quickly. This is a really back-of-the-envelope envelope engineering approach to estimating costs of a solar system. So I think this is a great place to stop. And if anybody has an idea, a fun idea, for a class project, I'd invite you to give a pitch up here at the front of class, or you're welcome to send it on an email to the class listserv.

ROUGE-1: 26.90, ROUGE-2: 25.70, ROUGE-L: 22.59
BERTScore: 68.98

==============================================
==================== [74/100] ====================
Summary:
The world woke up this morning to Global chaos massive Tech outages are impacting Airlines businesses offices thousands of flights grounded globally long cues frustrated passengers. Problems first reported in Australia before spreading across the world including here where there were delays and big cues at airports. American cyber security firm crowd strike did finally come forward to admit a defect in a software update it had issued which crashed Windows devices the company is deploying a fix but not before widespread Mayhem tonight we'll be looking at exactly what happened and how it's affected patients and businesses. Passengers should have been on planes were forced to wait out delays so people are tired they've been handing out water the boards don't really say anything so it tells you where to go but there's no departure. Many airlines found themselves unable to use their normal systems we've had to revert back to pen and paper basically and manually check each of our customers in of course that takes longer for our customers so we've seen a good operation but it's a slower operation. from chaos at Amsterdam to planes stuck on the ground at Newar in the US cancellations and delays spread around the world Edinburgh stopped accepting incoming flights. At some health centers only the sickest patients were being seen with other appointments cancelled. Only written prescriptions are available with pharmacists warning patients the electronic system has failed anybody who's coming in for their prescriptions. "We have to tell them to go away uh go to back to the surgery and then get a the oldfashioned fp10 uh which is the old green prescriptions," says one pharmacist. "I can't get my sick note um updated and unfortunately I was about to be sanctioned by the um Social Security office"

ROUGE-1: 26.72, ROUGE-2: 25.70, ROUGE-L: 23.69
BERTScore: 61.67

==============================================
==================== [75/100] ====================
Summary:
In the year 2000, four-year college grads actually earned more with their entry jobs than they're earning today. Changing technology has made wages rise more at the top, but has held wages down for a lot of other jobs. A third set of factors has to do with slower economic growth, slower productivity growth and slower dynamism in the U.S. economy. The Great Recession starting in 2008 meant that output was declining, employment was declining and people were laid off, he says. kinds of labor. The computer enhances the productivity of the skilled laborer. Information technology makes it possible for skilled labor to sell their products around the entire world. When supply and demand are ruling labor markets, the people who do well are those who have an economic understanding of where is demand high, and where is supply scarce. Check out our practice questions to test your money skills. Next up, we'll show you where to find data to help you decide which career to choose. ♪

ROUGE-1: 23.04, ROUGE-2: 20.90, ROUGE-L: 20.33
BERTScore: 62.34

==============================================
==================== [76/100] ====================
Summary:
Rationality, rationality axioms - completeness, reflexivity and transitivity. When our when someone’s preference exhibit these three properties, what does it mean that he is able to completely rank all his choices all his potential choices, with also possibility, with possibility that there are more than one bundle at some ranks this is a possibility. One thing also I should add that he will be able to rank only if he has finite consumption set and also what we have learned that this will translate into a utility function. Monotonicity tells us that indifference curve has to be downward sloping. The other thing that we can get from monotonicity is that indifference Curve cannot be a thick curve. So, whenever we have convexity the indifference curve will look like this. If I do this let me take this zone and look at it in, at the bigger scale what will happen, what we are basically seeing is that. indifference curve is very thin, fine, it is clear. And now we have convexity plus concexity.

ROUGE-1: 26.12, ROUGE-2: 24.94, ROUGE-L: 24.91
BERTScore: 71.54

==============================================
==================== [77/100] ====================
Summary:
Theory: Every connected triangulated manifold -- this is a very general result. We're going to think about two dimensional sources in 3D, but it could be D dimensional surfaces in D plus 1 dimensions. The only catch is that every face has to be a triangle. We've even implemented this algorithm here. It actually holds in any dimension. We don't know about edge unfoldings. The other thing that could go wrong is once you have octagons, even if you could find a way to fit them together. Theory of the facet-path: A path that alternates between visiting faces, triangles, and vertices. Professor: It's an open problem for something like a cube where you actually have quadrilateral faces. He says the claim is facet- Paths always exist for any triangulated, connected surface. The theorem works not only for a polyhedron-- it works for any manifold, any sort of locally, two-dimensional thing, he says. "It's useful because then, what's called a non-crossing Euler tour. Point is you've got a bunch of cycles that touch" to work for discs just as well. So ideally, I cut it all apart into lots of little triangles, but it has to stay connected. So I'll unfold it. Why not? So I can add some cuts until I get down to a spanning tree of the faces. That will cause you make two vertices of odd degree. But that's OK because there's still an Euler path that starts at one of the vertices and visits all the other edges. And I just need a path. You can actually characterize when you get a cycle. It's when the original thing is not too colorable. In the case of a triangulated polygon, we usually call them ears. So imagine those guys as being done. I'm left with these four triangles, actually, a little boring because I don't get the Mickey Mouse case. But then, this will be an ear. This will be a second-level ear. And so I'll end up doing this. And this is actually the base case, if you will. So in general, I just pluck off two or three triangles, repeat until I get one of the base cases. original thing, we think of this triangle as being an ear, and this triangle is an ear. I like ears because they're kind of on the boundary, on the surface, so they're just triangles that are adjacent to only one other triangle. Now, the next step is to color what are called the second-level ears. If you remove those ears, what would, then, become an ear? That's it. Unless there's only four pieces left. Right? PROFESSOR: Sorry, what do you mean? even attached to these cycles, so it's kind of a problem. That's step five is we're going to fix all the problems. Connect cycles together. So that's a local change, and now, it will be one big cycle. We've probably seen this trick once or twice before, I think in the Mountain Valley assignment stuff. So, that'd be a great title. That would be a good title for a book. It would be called, Connecting Cycles. The general approach is always proceed rightward in the unfolding. The amazing thing is that this is possible. You could only do it in a computer because you'd beicing things, and it would fall apart. You can unfold every orthogonal polyhedron this way. I would love to see an implementation of this algorithm. Jason? You've been making these gadgets. I guess I'm not going to describe how this works, but you could reconstruct it from these diagrams. Joe Oogonal: This is what we call an Orthogonal terrain. you can see like a mirror on the bottom. And our parent tells us you have to start at s, and it says you better finish at t. There's no flexibility there, but it could be there are pluses and minuses. But if there's going to be a problem with this theorem, there have to be plused and minused. That's the proof by contradiction. It doesn't look good. It would mean this is like a linkage. It just happens to be on a sphere. Forget this on. face at a time, you can't do that, but if you visit faces multiple times and kind of weave around in a clever way,you can do it. So if you follow along here, I just turn right here. So now, I go down here. And that is a left turn because it's on the bottom, a little hard to think about. So I turn left here, and then, we can zoom out, and you get the unfolding. They're so much fun. It's like exploding a city. Boom! work if it was rotated 90 degrees. It's really powerful. It also works if t is on the other side of s. You could do sort of the mirror image traversal. Now, obviously, I didn't cover the entire surface. I'm leaving room for later, but if this was all I was going to do, I would actually sort of fill out all those strips, just kind of extend them. It just makes this kind of fatter. So this got a little bigger. I've got the first half and then, the second half. This is really glued up there. The algorithm works by recursively undoing everything that you did in this diagram. At every level of the tree, you're going to double what was below you, so that's why you get exponential. If your tree happens to be nice and balanced, doubling is not so bad because here you'll have constant. This will double everything below, but there's only log n levels. So is that linear? It should be about linear. It's certainly 2 to the theta log n, and it matters what this constant is. So ideally, k is one, and you're not subdividing at all. But maybe, you take every rectangle, divide it in half. Maybe that's enough to then be edge unfoldable. That would be sort of a refined level two grid-like unfolding. There are a ton of results about this. They're all partial. One thing you could do, with merely 5 by 4 refinement, is something called Manhattan Towers. Let me show you a picture of Manhattan Tower. how to grid unfold. That's an open problem, but if you refine just in z by a factor of 2, that's enough to unfold. So 1 by 2 refinement is enough for orthostacks. Now, you could go a little crazier and allow vertex unfolding of Orthostacks with some grid refinement, and in that case, you don't need any refinement. So grid vertex unfolding orthostack was the title of paper. These guys, John Iacono and Stefan Langerman. And it looks something like this. other open problems. This was genus 0. Interesting question is can you do genus higher than 0? Orthogonal polyhedra. I would guess so, but I'm not sure. I think the biggest question is, can you make this non-orthogonal? But then, the bands get messy. Haven't been able to do that. All right. I'm going to take a break from unfolding now and switch the other direction of folding. So with folding, we're imagining we're given some polygon, and we'd like to make a polyhedron out of it. It's exactly the reverse of what we've been thinking about. When is this possible? glue whole edges to whole edges. If you want something sphere-like, in fact, those gluings have to be non-crossing. The gluing tells you locally what the surface looks like, even though you don't know what it looks like in 3D. If I want to make something convex, there's only one thing it could possibly make. Finding out what that one thing is is quite a challenge, but at least, we can prove that it's unique. If you have two convex polyhedra and they are combinatorally equivalent, they have the same way that things are connected together, and they have congruent faces, then, they are actually the same thing, the same polyhedron. Different faces can be different, but they're identical in pairs. This is not true if you allow non-convex realizations. These have exactly the same combinatorial structure, same geometry on each face. But as long as they're convex, they're going to be the same. a sphere. Think of it is as almost flat. What that would mean is there's some other way to draw this thing. Basically, there's a way to flex this linkage so that all of these angles increase and this one stays the same. How could I get a polygon where all the angles increase  and still be convex? Ain't possible. Why is it not possible? I think we've used this fact a couple lectures ago. It's not possible by something called the Cauchy Arm Lemma. And here's the thing. If you have a convex chain but open chain here. There's a missing bar. true just viewing p prime as p and p as p prime. If there's anything in there other than zeroes, there has to be at least one plus and one minus. Alternation is either going from plus to minus or from minus to plus. If I little bit because now, we can talk about number of alternation is at most two times the number of triangles. 7 f 7, 8 f 8, 9 f 9-- I'm going to be clever at 5 and 3. might think, well, what happens if there's only three vertices. Well, yeah, you can't have those four alternation because if you have a triangle, even on the sphere, triangles are rigid. We're only interested in cases where it might flex locally at a vertex like the pentagon, like a quadrilateral. All right. So what? This was true at every vertices that was not entirely zero. This works as long as there's at least one face. And that is Cauchy's rigidity theorem. a capital V. We're going to use a trick. It has many names, I suppose. I usually call it double counting in combinatorics, where you have one quantity, namely, the number of alterations. We'll get two different answers, but we know they must end up being the same. And then, we'll get a contradiction. The other natural way to count angles is by looking at the faces. It's sort of the dual perspective. Every phase has a bunch of angles that have some degree or whatever. have a face of 2 k or 2 k plus 1 edges, then it will have, at most, 2 k alternations. I'm going to try and prove an upper bound, sandwich it between, and show that, actually, the upper bound is smaller than the lower bound, and that's a contradiction. So this is kind of obvious. Right? If you have 2 k vertices, no more than 2k alternations, slight, the place where we're making a little improvement is for the odd case. is half-- what is the degree of degree 3 faces? 3. What is thedegree of degree 4 faces? 4. And so on. So now, things are starting to look similar, and I want to get some cancellation going on. Use my cheat sheet here. I'm going to rewrite this formula as V equals 2 plus E minus F. OK? All I did here was decrease by-- well, because there's a half out here, I decrease each coefficient by 2, nothing surprising.

ROUGE-1: 28.66, ROUGE-2: 27.28, ROUGE-L: 23.60
BERTScore: 64.32

==============================================
==================== [78/100] ====================
Summary:
For a particle traveling in a circle, we've seen that the velocity can be written as r d theta dt. Let's now look at rotation in an arbitrary plane. And so what we're going to do is use the right hand rule to define a direction that tells you both the direction it defines the plane and it also tells you what the positive direction of rotation is for that plane. Now, in the k hat direction, in this case, I'm going to call it in the arbitrary n hat direction.

ROUGE-1: 19.13, ROUGE-2: 18.37, ROUGE-L: 18.73
BERTScore: 73.16

==============================================
==================== [79/100] ====================
Summary:
In week four, we take a break from learning more and more on neural network topics, and talk about final projects, but also some practical tips for building neural network systems. So first of all, I'm going to introduce a new task, machine translation. And it turns out that our task is a major use case of a new architectural technique to teach you about deep learning. And so we'll spend a lot of time on those. And then there's a crucial way that's been developed to improve sequence to sequence models, which is the idea of attention. Machine translation is the task of translating a sentence x from one language to another. In the early 1950s, there started to be work on machine translation. It was hyped as I don't think we'll ever replace the translators of that type of material. But despite the hype it ran into deep trouble. And so in retrospect, it's not very surprising that the early work did not work out very well. And then we use as the translation. OK. Get straight into this with machine Translation. Machine could be able to translate one to two million words an hour. This would be enough to cope with the whole output of the Soviet Union in just a few hours of computer time a week. But neural machine translation systems also have some disadvantages compared to the older statistical machine translation system. They're less interpretable. They also tend to be sort of difficult to control. It's hard to know what they'll generate. There are various safety concerns. So there's still tons of stuff to do. When they were in the period of statistical NLP that we've seen in other places in the course. And then the idea began, can we start with just data about translation i.e. sentences and their translations, and learn a probabilistic model that can predict the translations of fresh sentences? So suppose we're translating French into English. We can say, what's the probability of different English translations? And then we'll choose the most likely translation. And it's not immediately obvious as to why this should be because this is sort of just a trivial rewrite with Bayes' rule. The European Union produces a huge amount of parallel text across European languages. The Canadian Parliament conveniently produces parallel text between French and English, and even a limited amount in Inuktitut, Canadian Eskimo. And then the Hong Kong parliament produces English and Chinese is working out the correspondence between words that is capturing the grammatical differences between languages. So you can have words that don't get translated at all in the other language. So in French, you put a definite article "the" before country names like Japon. So when that gets translated to English, you just get Japan. One French word gets translated as several English words. You can get the reverse, where you can have several French words that get translated as one English word. So here we sort of have four English words being translated as two French words. But they don't really break down and translate each other well. These things don't only happen across languages. They also happen within the language when you have different ways of saying the same thing. So another way you might have expressed the poor don't have any money is to say the poor are moneyless. That's much more similar to how the French is being rendered here. We start with a source sentence. So this is a German sentence. And as is standard in German. You're getting this second position verb. So that's probably not in the right position for where the English translation is going to be. So we might need to rearrange the words. And so we explore forward in the translation process. And we could decide that we could translate next the second word goes, or we can translate the negation here, and translate that as does not. And in the process, I'll go through in more detail later when we do the neural equivalent. We sort of do this search where we explore likely translations and prune. we have is based on the translation model. We have words or phrases that are reasonably likely translations of each German word, or sometimes a German phrase. And so then inside that, making use of this data, we're going to generate the translation piece by piece kind of like we did with our neural language models. And if we're guided by our fairly small, something like 5 to 10. And at each step of the decoder, we are going to keep track of the k most probable partial translation. "he." And so then doing LSTM generation just like last class, we copy that down as the next input. We run the next step of the L STM, generate another word here, copy it down, and chug along. And we've translated the sentence, right? So this is showing the test time behavior when we're generating the next sentence. And so our representation of the source sentence from our encoder is then this stack of three hidden layers, whoops. And then that we use to then feed in the next word. The models have also been applied not just to natural languages, but to other kinds of languages, including music, and also programming language code. So you can train a seq2seq system, where it reads in pseudocode in natural language, and it generates out Python code. And if you have a good enough one, it can do the assignment for you. So this central new idea here with our sequence to sequence models is we have an example of conditional language models.everywhere else as well. each step, as we break down the word by word generation, that we're conditioning not only on previous words of the target language, but also each time on our source language sentence x. Because of this, we actually know a ton more about what our sentence that we generate should be. So if you look at the perplexities of these kind of conditional language models, you will find them like the numbers I showed last time. They usually have almost freakily low perplexities, that you will have models with perplexities that are something like 4 or even less.  multilayer or stacked RNNs are more powerful. It's almost invariably the case that having a two layer LSTM works a lot better than having a one layer L STM. After that, things say, I'm going to use this hidden representation to look back at the source to get information directly from it. And so we'll be training the model here to be saying, well, probably you should translate the first word of the sentence first, so that's where the attention should be placed. are somewhat flimsy terms. The meaning isn't precise. But typically, what that's meaning is that lower level features and knowing sort of more basic things about words and phrases. So that commonly might be things like what part of speech is this word, or are these words the name of a person, or a company? Whereas higher level features refer to things that are at a higher semantic level. So knowing more about the overall structure of a sentence, knowing something about what it means, whether a phrase has positive or negative connotations. become much less clear. It's normally very hard with the model architecture that I just showed back here to get better results with more than four layers of LSTM. Normally to do deeper L STM models and get even better results. You have to be adding extra skip connections of the kind that I talked about at the very end of the last class. Next week, John is going to talk about transformer based networks. They're typically much deeper. But we'll leave discussing them until we get on further. So that we have our LSTM, we start, generate a hidden state. It has a probability distribution over words. And you choose the most probable one the argmax, and you say "he", and you copy it down and you repeat over. So doing this is referred to as greedy decoding. Taking the most likely word on each step. And it's sort of the obvious thing to do, and doesn't seem like it could be a bad thing toDo. But it turns out that it actually can be a fairly problematic thing todo. Stuck with it. And you have no way to undo decisions. So in this case, so I can fit it on a slide. The size of our beam is just 2. Though normally, it would actually be a bit bigger than that. And the blue numbers are the scores of the prefixes. So these are these log probabilities of a prefix. So we might generate he hit, he struck, the most likely following word. And then for each of those, we generate the k most likely next words tart, pie, with. In greedy decoding, we usually decode until the model produces an end token. In beam search decoding, different hypotheses may produce end tokens on different time steps. And so we don't want to stop as soon as one path through the search tree has generated end. So what we do is sort of put it aside as a complete hypothesis and continue exploring other hypotheses via our beam search. And then we'll look through the hypotheses that we've completed and say which is the best one of those. And that's the one we'll use. In a newspaper, the median length of sentences is over 20. So you wouldn't want to be having a decoding model when translating news articles that says, huh, just generate two word sentences. They're just way high probability according to my language model. So the commonest way of dealing with that is that we normalize by length. So if we're working in log probabilities, that means taking dividing through by the length of the sentence. And then you have a per word log probability score. BLEU gives a score between 0 and 100 where your score is 100. If you are exactly producing one of the human written translations, and 0 if there's not even a single unigram that overlaps between the two. For assignment 4 this year, we've decided to do Cherokee English machine translation. Cherokee is an endangered Native American language that has about 2000 fluent speakers. It's an extremely low resource language. So it's just there isn't much written Cherokee data available period. And particularly, there're not a lot of parallel sentences between Cherokee and English. Many languages don't distinguish between things masculine or feminine. When that gets translated into English by Google Translate is that the English language model just kicks in and applies stereotypical biases. So if you want to help solve this problem, all of you can help by using singular they in all contexts when you're putting material online. And that could then change the distribution of what's generated. And people also work on modeling improvements to try and avoid this. Here's one more example that's kind of funny. People noticed a couple of years ago. That if you choose one of the rarer languages that Google will translate, that the gender neutral sentences get translated into, she works as a nurse. such as Somali, and you just write in some rubbish like ag ag ag. Freakily, it had produced out of nowhere prophetic and biblical texts, as the name of the Lord was written in the Hebrew language. As far as I can see, this problem is now fixed in 2021. So there are lots of ways to keep on doing research. NMT certainly is a flagship task for NLP and deep learning. And it was a place where many of the innovations of deep learning NLP were pioneered, and people continue to work hard on it. For languages for which there isn't much parallel data available, commonly the biggest place where you can get parallel data is from Bible translations. So this is a piece of parallel data that we can learn from. Cherokee is not a language that Google offers on Google Translate. So we can see how far we can get. But we have to be modest in our expectations because it's hard to build a very good MT system with only a fairly limited amount of data. There is a flipside, which is for you students doing the assignment. The advantage of not too much data is that your models will train relatively quickly.

ROUGE-1: 33.33, ROUGE-2: 32.33, ROUGE-L: 29.15
BERTScore: 65.68

==============================================
==================== [80/100] ====================
Summary:
Nuclear materials are the biggest research field in the department, and yet, it's not talked about in 22.01. The basic mechanism of radiation damage is like you might imagine. If you want to actually say how long will it take to get materials to the end of their useful life, this tends to be anywhere from 0.002 DPA to hundreds of DPA in proposed fast reactors. The best we can wait for is 20 years to start building the reactor, which is 40 years for a return on investment. So what we really need is what is the full population of every single type of defect in an irradiated material? Stiffness is more of a response function, so it's how much does it deform in relation to how much stress you put into it. Radiation creates pretty much any and all defects, it's a great way to stiffen and strengthen the material. Dislocation movement is irreversible. You can't just snap it back when you relieve the stress. And by making something stronger and stiffer, you make it more difficult for those dislocations to start moving. If you throw absolutely anything in the way from solute atoms to interstitials to vacancies, all of a sudden it's harder for that dislocation to move. A measure called DPA, or displacements per atom, is a simple measure of how many times has every atom left. It's not actually a unit of damage, and I'll be giving a talk at MRS, the materials research conference tomorrow, railing against this DPA unit. Damage is some measure of the number of messed up atoms at the end of the game, and they operate in very different timescales. The DPA is all over in less than a picosecond, but it can take years for all these different defects to diffuse, to cluster up and to form these super structures. energy E and imparting kinetic energy T to another struck atom? That comes right from-- remember our treatment-- I think I've drawn this probably 50 times now. Our hollow cylinder treatment of a charged particle with charge little ze interacting with a particle a big ZE at some impact parameter B. We wanted to know well, for all possible approach paths, the area of this hollow circle, or the probability that this particular approach path is taken, is just the area here 2pi b db. not just ranting against it, no I am, but I'm doing so with evidence, so it's justified. Here's a nice experiment I like to show in every talk for this case. These folks took pure nickel and put it in the same reactor, at the same temperature, and got the same amount of swelling. All the conditions were the same. Same temperature, same materials, same microstructure. Same reactor, same neutron energies. Just a different dose rate. A 30% difference in the rate at which neutrons arrived at the nickel. Russia has a fleet of sodium cooled fast reactors that can get you 25 DPA per year. If your reactor is going to go to 500 DPA, you have to know whether or not your materials will survive. The reactor vessel looks like a gigantic forging of really thick carbon steel with a very thin liner of stainless steel. And the stainless steel is there to prevent corrosion from the reactor water. But if you were to take something out of the vessel and release it from the plant, the part would annihilate their stored energy. You can see it in real time. Most of the defects that cause these reductions in material properties are too small to see, even in a millisecond. Most defects are very, very small, and it turns out that-- first of all, the resolution of the screen is funny. I think I know how to fix that. Clone the screen and then jump back to presenter mode. That usually-- that's not what I wanted. That's what he wanted, great. That would be the worst with a gun, you are an expert marksman and deserve the delicious and tiny treat. build up, then what Jared said could happen. You could crack the material because it could get weaker, less ductile, less tough. Weaker is the opposite of strong, and what's the other one? Toughness-- oh, and harder, actually. So the origins of material swelling are the humble vacancy. A void is nothing but a bunch of vacancies or a pocket of vacuum or gas in a material. As they cluster together, they reached this threshold in terms of free energy where putting a few of them together is not quite energetically favorable. pocket of vacuum, then that pressure differential goes down and that void becomes a bubble, and that bubble is more stable. So there's this one here called Z alpha, which means neutron comes in, alpha particle goes out. alpha particle is just an ionized helium ion, which very quickly pulls in two electrons from anywhere else in the metal and becomes helium gas. At higher energy starting around 2 MeV, there is a small, but non-negligible chance that a neutron will go in, and a helium atom comes out. When you apply radiation, you can just create dislocations. This is a pretty slick image of a single crystal of cadmium being pulled in this direction, and you can actually see every plane there, that's a slip plane. That's a plane where disLocations have been moving all the way to the outside of the material, which is pretty cool, and this is the process that you want to happen. You get a mixture of bending and rotation to make it look like the bar is bending uniformly straight, but on the microscale, it's not. the metal that all those guys thought was going to be ductile like metal was more brittle than glass. And any sort of bump would cause just complete shattering of this metal and catastrophic release of radioactive material. So this took them-- let's see-- I think Kazakhstan is smaller than the US. So who here has done a cross-country trip? How long it take you? AUDIENCE: Six days from Seattle to here. MICHAEL SHORT: OK, so this trip took them 13 days because they went slow. what we call the ductile-brittle transition temperature. This is the property that people worry about for reactor pressure vessels. If this material's got a bunch of defects already in it, then you should release that defect energy by heating it and that would take a little less energy to heat it up. We're doing this process on nanograms of material and seeing if you can irradiate something and measure its stored energy because if you could, you could take a tiny little razor blade, take out the smallest sliver of the vessel. These out and you hit them with a very well calibrated hammer. And you can measure by actually turning this dial and letting the hammer turn it. So it breaks right through the material, in this case, it's in a quenched or brittle condition. And by doing this test at a number of different temperatures, you can recreate this ductile-brittle transition temperature curve. So they'll take a few Charpy coupons, they will test them at, let's say, every 25 Celsius, get a bunch of points, and decide where is the material brittle. that gets the most damage, you'd be taking out some of the stainless steel, which is a problem. You could take a piece out from here, maybe the outside, but then you've got a stress concentrator. Any sort of chunk that is missing is where a crack is going to preferentially form. The only way to do that is to go nano. Take that out. Anyone else have any ideas? Yeah? AUDIENCE: Is it impossible to just replace the vessel? MICHAEL SHORT: Yes. Scientists have developed a way to measure the stored energy in a material. It's called differential scanning color imagery. You simply heat two materials, one of which contains your sample, and look how much energy it takes between them to keep them at the same temperature. The heating is so fast that the defects don't even have time to show up, the researchers say. The next step is to use a nano a DSC, or nano differential scanning calorimeter, that can heat about 10,000. Snipes are real. You pretty much have to be British to know it, because they hunt them there for sport, and apparently, they're delicious. That's actually where we get the term snipe because the actual size of the sniper compared to the sniper is about that. If you can shoot that bird, if you can get that shot, you're a snipe hunter. You're not looking for a bird that doesn't exist. It's what we call the ultimate snipe hunt. It actually fits on a chip. There's one that we put our material on, and one as a reference that we both put in the accelerator being irradiated at the same time. This is what they actually look like. The scale bar here is 100 microns, and that transparent spot is a little bit of aluminum that we vapor deposited onto the calorimeter. Right there. That whole thing just went from zero to 450c. That pulse right there. It slowed down by a factor of 1,000. And so the way this process works, is we take our DSC chip, we put a mask over it. doing Charpy coupons of one place, which is what we do now, you can get a map. We don't have that information now, but if you take pieces from all over the vessel, then you get an actual 3D map. When some type of defect gets high enough in temperature that it goes from stuck to mobile, and as that moves, it encounters anything else. How do you know? You don't. You make measurements like these. Any other questions? Yeah? the vessel too much, it's no longer a code stamp vessel. Pretty tricky spot that we're in, huh? But we're trying to science our way out of it. I don't want to keep you longer, but I'll open on Thursday with a little story about how mass attenuation coefficients can get you out of apartheid South Africa. I'm serious. And then we'll move into dose and biological effects. It's a couple minutes after. Well, it was just a few minutes.

ROUGE-1: 29.81, ROUGE-2: 28.36, ROUGE-L: 21.80
BERTScore: 59.78

==============================================
==================== [81/100] ====================
Summary:
In Western astrology, it's a constellation determined by when your birthday falls in the calendar. But according to the Chinese zodiac, your shēngxiào is the animal assigned to your birth year. Of the many myths explaining these animal signs and their arrangement, the most enduring one is that of the Great Race. The first twelve animals to make it across the river would earn a spot on the zodiac calendar in the order they arrived. Each year is associated with one of the animals in this order, with the cycle starting over every 60 years.

ROUGE-1: 25.00, ROUGE-2: 23.79, ROUGE-L: 24.49
BERTScore: 67.07

==============================================
==================== [82/100] ====================
Summary:
Professor: In almost all cases when you address atoms, you do two photon courses because a photon is scattered. In reality, it is a two photon process and not two single photon processes. Professor: With two-photon emission with two lasers, we have the situation that we start in what we're doing with the atoms, cannot done on the d1 or d2 line. But I think you get the gist, and later on it will become clearer, even on a two-level system. many terms do you get if you write down the Hamiltonian? No approximation without. How many do youget? AUDIENCE: Four. PROFESSOR: Eight? I think it's multiplicative because we have four processes involving one-- oops. I wanted to say 16, 4 times 4. But now I would say in the first step, which is to the intermediate level, we have 4 at frequency one, four at frequency, which makes eight. But then I think in the second step you get eight more. But they're just all combinations, all combinations of frequencies. lowest order perturbation, second order perturbedation theory, in two steps. But if you're asking, what is the transition probability? The transition probability, we have to get the probability to be in the excited state. And when we divide the probability, the amplitude squared by t, we get a rate. And this is Fermi's Golden Rule. It is exactly the same you have seen probably more than 100 times. So this is the correct description of resonant fluorescence and Rayleigh scattering. Any questions? Yes. We've just cleverly defined our quantities. The rate to go from a to b is Rabi frequency squared, but it is the two photon Rabi frequencies. So therefore, our result looks almost indistinguishable from the result built on single photons. So in other words, if you were interested just in the physics of two levels-- Rabi oscillation, you name it-- you can just say the same thing happens. The only difference is that instead of having a coupling, we have stacked up the two photons. The Raman process only in the external degree of freedom, in the motion away function. The only change is you change the momentum. We need our intermediate state, which is often an electronically excited state. And now we have one photon going up and one photons going down. And it doesn't really matter if the power is delivered by a laser or a light bulb. The rate for this process is the same. And this actually was the discovery by Raman, which was rewarded with the Nobel Prize. say if omega 1 is very, very strong, you can exactly diagonalize the Hilbert space of states k and a. But again, what happens is you mix those two states. And it is the admixture now in a non-perturbative way of state k, which is sort of the stepping stone. And from this stepping stone on,you can absorb a photon omega 2. So we could actually-- let me just redraw this-- that we want to go to the final state b. But in this kind of picture I just suggested, I start with a dress state a. Spontaneous emission occurs with Einstein's a coefficient connecting the intermediate state to the ground state. This is actually also-- this kind of spontaneous Raman process-- has been very important historically. The system involves with the following operator, and this is the operator which completely describes the interaction of an atom with the electromagnetic phrase-- the randomness of spontaneous emission. But this operator will actually lead to final states of the photon field, which may not have a specific phase. So what I suggest is when we, really at the fundamental level now, discuss spontaneous emission, we allow this part-- excited atom, empty cavity-- to undergo half a vacuum Rabi oscillation. Two-photon absorption or Raman processes are like single-photons transitions. If you have a single photon, you always transfer momentum to the atom. But if you have two photons, the two momenta can cancel. And this is actually a powerful method to avoid Doppler broadening in spectroscopy. Really will talk about it in a second about the process you absorb or two photons from the left or the right. But you have sort of a broad pedestal where you take one photon from the right, whereas the peak is where you have free photons from counter-agating directions. You can't know for certain that the velocity distribution of the atoms in your laser beam is exactly at room temperature. You really have to go to low temperature, go to cryogenic temperatures. The lifetime of the 2s state is actually due to two-photon emission. Two photons in series. And you will, actually, with this rather simple description, get a fairly accurate estimate for the lifetime, which is a fraction of a second. It really depends what you want. If you simply want an atomic clock, all you want is a very stable reference point. line, which is sufficiently sharp, sufficiently narrow, and also insensitive to magnetic fields and electric fields. People who want to measure fundamental constants-- the Rydberg constant-- want to compare lame shift with first principle QED calculations. This is the case for hydrogen. And, actually, with some advance in the numeric calculation of wave functions and all that, it may also be possible to do it with helium. But so far it hasn't kind of-- helium has not replaced hydrogen. It's still is an important technique and important tool for measurements. Coherence exists if there is a well-defined phase. It's always a phase between quantum mechanical amplitudes. Coherence can be two amplitudes describing two different atoms, or it can be the amplitudes of two states within the same atom. When we observe an interference, that means we obesrve-- and this is how we read out of a coherence-- one observes a physical quantity, the population in a certain quantum state, total electric field emitted, but this quantity is usually proportional to the square of the total amplitude. There is a certain randomness in spontaneous emission when we go to the laboratory and look at the spontaneously emitted photons. And this is actually what I want to work out with you in-- maybe even today, I think ten minutes may be enough-- what is really the information-- the phase information-- which we have in a photon, which has been spontaneously emitted. I know how to least the diminishment of the read out. There may be situations where we have a laser beam which has a well-defined phase, photons are scattered, and we just cannot retrieve the phase of the laser beam by looking at the photons. When we scatter light for many, many atoms, we're not even asking for the phase. You immediately perform an incoherence sum because you sort of know deep in your heart that there won't be any interference. So it's always possible, of course, to lose the phase by not controlling every aspect of your experiment. So B is always possible. But for so many reasons that I don't want to discuss it, but the measurement process is very relevant. And so, therefore, I would have answered C here. one is we know for sure the atom is in the ground state. However, the excited state with zero photon will actually do Rabi oscillation. And so we have now our knowledge from the vacuum Rabi. oscillation that this part of the wave function does nothing. And if we start out with the superposition of ground zero and excited zero, we can get the same result with ground zero, excited zero and ground zero. And we can say that the atom in this state is in a state of superposition.

ROUGE-1: 23.66, ROUGE-2: 22.33, ROUGE-L: 21.00
BERTScore: 66.49

==============================================
==================== [83/100] ====================
Summary:
Lecture eight is about deep learning software and how the hardware works. We'll talk a little bit about CPUs and GPUs and then we'll talk about several of the major deep learning frameworks that are out there in use these days. This is one way that you can attack a lot of problems in deep learning, even if you don't have a huge dataset of your own. We're in the process of assigning TA's to projects based on what the project area is and the expertise of the TA's. It's kind of hard to most people in deep learning when we talk about GPUs, we're pretty much exclusively talking about NVIDIA GPUs. For all of the several notebooks it's just in Python and Numpy so you don't need any GPUs for those questions. The downside of a GPU is that each of those cores, one, it runs at a much slower clock speed. And two they really can't do quite as much. You can't really compare CPU cores and GPU cores apples to apples. The midterm will be in class on Tuesday, five nine. It'll be sort of pen and paper working through different kinds of, slightly more theoretical questions to check your understanding of the material that we've covered so far. And I think we'll probably post at least a short sort of sample of the types of questions to expect. So just, Yeah, yeah, so that's what we've done in the past is just closed note, closed book, relatively just like want to check that you understand the intuition behind most of the stuff we've presented. About fancier optimization algorithms for deep learning models including SGD Momentum, Nesterov, RMSProp and Adam. And we saw that these relatively small tweaks on top of vanilla SGD, are relatively easy to implement but can make your networks converge a bit faster. We also talked about dropout, where you're kind of randomly setting parts of the network to zero during the forward pass, and then you kind of marginalize out over that noise in the back at test time. So this is kind of out of the box performance, but it's not really like peak, possible, theoretical throughput on the CPU. The CPU is the Central Processing Unit, that's this little chip hidden under this cooling fan. The GPU is called a graphics card, or Graphics Processing Unit. These were really developed, originally for rendering computer graphics, and especially around games and that sort of thing. So if you're writing programs that run directly how you want it to work, these are the basic ideas even if you want to write it yourself. This is the kind of thing that you can find more details on GitHub, but my findings for things like ResNets, ResNet 19, Resnets 16 and VGG 16 were all very similar. This is kind of the prototypical type of problem that like where a GPU is really well suited, where a CPU might have to go in and step through sequentially and compute each of these elements one by one. So you could imagine that for a GPU you can just like blast this out and have all of this elements of the output matrix all computed in parallel and that could make this thing computer super super fast on GPU. Another thing to point out here is that these lines of code are not actually computing anything. There's no data in the system right now. We're just building up this computational.  Torch is actually in Lua, not Python, unlike these other things. Torch is also older, so it's more stable, less susceptible to bugs. In PyTorch it's in Python which is great, you've got autograd which makes it a lot simpler to write complex models. In Lua Torch you end up writing a lot of your own back prop code sometimes, which is a bit of a turn off for some people. They're about the same speeds, that's not really a concern. that can be really bad and slow you down. Some solutions here are that like you know if your dataset's really small, sometimes you might just read the whole dataset into RAM. You can also make sure you're using an SSD instead of a hard drive, that can help a lot with read throughput. Another common strategy is to use multiple threads on the CPU that are pre-fetching data off RAM or off disk, buffering it in memory. And finally you want all this stuff to run efficiently on GPUs so you don't have to worry too much about these low level hardware details about cuBLAS and cuDNN and CUDA. The first generation of deep learning frameworks that really saw wide adoption were built in academia. Now industry is giving us these big powerful nice frameworks to work with. So today I wanted to mostly talk about PyTorch and TensorFlow 'cause I personally think that those are probably the ones you should be focusing on for a lot of research type problems. I'll also talk a bit about Caffe and Caffe2. But probably a little bit less emphasis on those. Although I think Caffe is still pretty commonly used in industry again for production. In Numpy, you can just kind of write down in Numpy that you want to generate some random data. And in TensorFlow, we actually need to construct some concrete values that will be fed to the graph. So here we're just creating concrete actual values for X, Y, w1 and w2 using Numpy and then storing these in some dictionary. In PyTorch you can define your own new autograd functions by defining the forward and backward in tensors. And it has some other implications that we'll get to in a bit. of these frameworks. TensorFlow has this magic line that just computes all the gradients for you. So now you don't have go in and write your own backward pass and that's much more convenient. The other nice thing about Tensor Flow is you can really just, like with one line you can switch all this computation between CPU and GPU. So here, if you just add this with statement before you're doing this forward pass, you just can explicitly tell the framework, hey I want to run this code on the CPU. TensorFlow lets you build a graph and then run it over and over again. You can then feed data into the graph to perform whatever computation you want it to perform. In this example we're only feeding in the data and labels X and Y and the weights are living inside the graph. And here we've asked the network to compute the loss for us. And then you might think that this would train the network, but there's actually a bug here. So we've had to tell TensorFlow that the loss and updates is not actually a real value. In the forward pass we can use both our own internal modules as well as arbitrary autograd operations on variables to compute the output of our network. So here we receive the, inside this forward method here, the input acts as a variable, then we pass the variable to our self.linear1 for the first layer. And now the rest of this code for training this thing looks pretty much the same. Where we build an optimizer and loop over and on ever iteration feed data to the model, compute the gradients with loss.backwards, call optimizer.step. TensorFlow is smart and it only computes the parts of the graph that are necessary for computing the output that you asked it to compute. So that's kind of a nice thing because it means it's only doing as much work as it needs to, but in situations like this it can be a little bit confusing and lead to behavior that you didn't expect. So the solution in this case is that we actually need to explicitly tell TensorFlow to perform those update operations. There's a little trick you can do instead. The question is why is loss a value and why is updates none? That's just the way that updates works. So here after you run updates, then the output is none. So it's kind of some TensorFlow magic that's going on there. Maybe we can talk offline if you're still confused. So but now we've kind of got this, again we've got this full example of training a network in Tensor Flow and we're kind of adding bells and whistles. In the previous example we were computing the loss explicitly using our own tensor operations. TensorFlow gives you a bunch of convenience functions that compute these common neural network things for you. So in this case we can use tf.losses.mean_squared_error and it just does the L2 loss for us so we don't have to compute it ourself. And in this example we've actually not put biases in the layer because we're not using biases. So another kind of weirdness here is that we had to explicitly define our inputs and define our weights and then like chain them together in the forward pass using a matrix multiply. TensorFlow is an open-sourceensorFlow framework. It's used by Google to train neural networks. The code example shows how to use TensorFlow to train a neural network. It uses the xavier initializer object to set up an initialization strategy for the data and labels. It also sets up variables for those with the right shapes that are kind of inside the graph but a little bit hidden from us. And in fact if you run this code, it converges much faster than the previous one because the initialization is better. There's like a lot of different higher level libraries that people build on top of TensorFlow. When we're working with neural networks we have this concept of layers and weights and some layers have weights associated with them. So that's what these various packages are trying to help you out and let you work at this higher layer of abstraction. There's these three different ones, tf.layers, TF-Slim and TF.contrib.Learn that all ship with Tensorflow, that are but Tensorboard. can compose together these modules to build big networks. The PyTorch variable is similar to the TensorFlow tensor or variable or placeholder, which are all sort of nodes in a computational graph. Rather than using torch.FloatTensor, you do torch.cuda.Floattensor, cast all of your tensors to this new datatype and everything runs magically on the GPU. You should think ofPyTorch tensors as just Numpy plus GPU. That's exactly what it is, nothing specific to deep learning. a chance to play around with this myself so I can't really speak to how useful it is. Tensorboard actually lets you visualize the structure of the computational graph. And Visdom does not have that functionality yet. PyTorch is kind of an evolution of, kind of a newer updated version of an older framework called Torch which I worked with a lot in the last couple of years. It is pretty much better in a lot of ways than the old Lua Torch, but they actually share a much of the same back end C code for the back end. different where we're actually building up this new computational graph, this new fresh thing on every forward pass. That's called a dynamic computational graph. With a static graph you can imagine that you write this code that builds up the graph and then once you've built the graph, you have this data structure in memory that represents the entire structure of your network. And now you could take that data structure and just serialize it to disk. And then you could later rear load that thing and then run that computational graph without access to the original code that built it. The problem is that because we only build the graph once, all the potential paths of control flow that our program might flow through need to be baked into the graph at the time we construct it before we ever run it. In this case this tf.cond.kind of needs to be an explicit operator in the TensorFlow graph. But what this basically means is that you have this sense that Tensor Flow is almost building its own entire programming language, using the language of computational graphs. have loops. We can just kind of use a normal for loop in Python to just loop over the number of times that we want to unroll. Now depending on the size of the input data, our computational graph will end up as different sizes. But that's fine, we can just back propagate through each one, one at a time. Now in PyTorch this is super easy. We just want to compute this same recurrence relation no matter the length of our sequence of data. Super new paper that's being presented at ICLR this week in France. Initial impression was that it does add some amount of dynamic graphs to TensorFlow but it is still a bit more awkward to work with than the sort of native dynamic graphs you have in PyTorch. So one option is recurrent networks. So you can see that for something like image captioning we use a recurrent network which operates over sequences of different lengths. In this case, the sentence that we want to generate as a caption is a sequence and that sequence can vary depending on our input data. inner product, we compute some loss and the whole structure of the graph is set up in this text file. One kind of downside here is that these files can get really ugly for very large networks. So for something like the 152 layer ResNet model, which by the way was trained in Caffe originally, then this prototxt file ends up almost 7000 lines long. So people are not writing these by hand. People will sometimes will like write python scripts to generate these prototext files.

ROUGE-1: 26.86, ROUGE-2: 25.73, ROUGE-L: 21.02
BERTScore: 66.53

==============================================
==================== [84/100] ====================
Summary:
Part 3 in our series on distributed word representations. We're going to be talking about vector comparison methods. To try to make this discussion pretty intuitive, I'm going to ground things in this running example. On the left, I have a very small vector space model. We have three words, A, B, and C. And you can imagine that we've measured two dimensions, dx and dy. And then you can see graphically that B and C are pretty close together. And A is kind of lonely down here. corner, the infrequent one. We can measure the Euclidean distance between vectors u and v if they share the same dimension n by just calculating the sum of the squared element wide differences, absolute differences, and then taking the square root of that. And the other class of methods that you might see come up are probabilistic methods which tend to be grounded in this notion of KL divergence. KL divergence is essentially a way of measuring the distance between two probability distributions. To qualify as a proper distance metric, a vector must be valued in a certain way. we changed the space as I showed you before. So they're all up here kind of on the units here. And notice that the actual values that we get out are the same whether or not we did that L2 norming step. And that is because cosine is building the effects of L2norming directly into this normalization here in the denominator. There are a few other methods that we could think about or classes of methods. I think we don't need to get distracted by the details.

ROUGE-1: 23.31, ROUGE-2: 22.40, ROUGE-L: 20.61
BERTScore: 71.96

==============================================
==================== [85/100] ====================
Summary:
liyan blake a little bit about him he's kind of out there some of his contemporaries thought he was insane you know nowadays maybe he would be heavily medicated. People think that he's out there and there was that who was William Wordsworth who will read and discuss in the next day or two he said that there is something in the madness of this man which interests me more than the sanity of Lord Byron and Walter Scott so something about this man is more interesting than these other people. anything else that we've seen before that it's almost like a new art form does that kind of make sense? He was an individual who you know was into art and was into eventually you know religion and such became a big part of his life but his as an artist you know he was a very prolific poet but he was also big-time into painting we'll see some of his paintings later on but also a new type of art form kind of relief fetching where you kind of sketch out a negative of what you should have and so he can kind of do that same thing but fetching into a metal. and your parents don't know just throw out William Blake you might be right okay very very famous individual so we'll be covering in the next couple days the lamb by William Blake. okay this is probably the sweetest of the poems that we have to some degree okay looking at it at face value one they have a little picture and these are the the etchings that he does okay you know with the picture that they give us to to help place things you know we have a shepherd talking to his flock okay he's out there alone he's probably has conversations with them. the field but somebody that is passionate about their religion and their faith you know the nature and the common man the shepherd person of the flock or even a child in a sense of children so it's a very short one and it's one that I believe is pretty easy to to comprehend and understand okay. "I believe it is one of the easiest to understand and understand," he says. "It's one of those things that is very, very easy to understand. It's very simple to understand"

ROUGE-1: 47.51, ROUGE-2: 44.63, ROUGE-L: 44.57
BERTScore: 73.60

==============================================
==================== [86/100] ====================
Summary:
Cú Chulainn, hero of Ulster, stood at the ford at Cooley, ready to face an entire army singlehandedly. The army in question belonged to Queen Meadhbh of Connaught. Enraged at her husband’s possession of a white bull of awesome strength, she had set out to capture the fabled brown bull of Ulster at any cost. Cú Chulpainn invoked the sacred rite of single combat in order to fight the intruders one by one. broken heart, leaving behind a land that would remain ravaged by Meadhbh’s war for years to come. “I will never forget you,” he wrote to his son. ‘I will always love you.’ “You are my son,’ he replied, “and I will always be your son.” “And I will never leave you, my son. I will forever love you’, he said, ‘even though I’m no longer your son’.

ROUGE-1: 26.13, ROUGE-2: 22.31, ROUGE-L: 23.54
BERTScore: 64.46

==============================================
==================== [87/100] ====================
Summary:
The Underworld is actually a lovely place to "live" It boasts historic charm and eccentric neighbors with eternal ties to the area. Tartarus is reserved for a select few who some might call the greatest sinners of all time. Elysium is the Underworld’s exclusive VIP section— and your permanent home. The Underworld also features four other waterways: Acheron, the river of woe; Cocytus, river. of wailing; Lethe, river of oblivion; and Phlegethon, a. great source of natural light.

ROUGE-1: 21.21, ROUGE-2: 19.65, ROUGE-L: 15.30
BERTScore: 65.12

==============================================
==================== [88/100] ====================
Summary:
Fluorescence is the absorption of light energy by a molecule. Luminescence is the general term. Fluorescence is a little bit more specific. There are different types of luminescence. And you'll get to see some of those varieties of fluorescence. And then we'll talk about the application of fluorescent dyes in the electromagnetic spectrum in the next couple of slides. And we're going to see a little more detail in a minute in a detail on some of the fluorescent proteins that are used in biology. Luminescence is the interaction of a chemical with another chemical to give luminescence. In a crime scene, luminol interacts with the heme of blood at an amazingly sensitive level, such that when the lights are turned out, the room's just sort of this battlefield of bright luminescent that indicates that this was a crime Scene. Another pretty useful type of luminecence is bioluminescent, which is a biological reaction that causes luminesence. And many, many marine organisms actually undergo biolueminescence, and it's a whole party there at night in the ocean. The first thing to learn about fluorescence is how to spell fluorescence. It actually is fluor, F-L-U-O-R-E-S. The photo physics of fluorescence involves the excitation of a molecule with light of one energy. Once that molecule has absorbed light, there's a very transient period until the molecule lets out energy in the form of light. And returns back to its ground state now. And I'm going to redefine all these terms properly in a moment. When you excite a molecule, you'll take it to the excited state. It'll sit and vibrate there a little bit. Then it will kick back energy out at a longer wavelength. For the majority of the fluorescence experiments that we do in biology, the wavelengths that you see emission at are in the visible range. Whereas the wavelengths you might excite your molecule at are often in the UV or a bit longer. And these things are going to come up. This isn't the first time that you're going to see them. And it's not the last time. very often related to what's around it. Why is that the case? It's because the excited state may behave differently in different environments. That's why you might see fluorophores experience a change in their fluorescence as a function of their environment. So the molecular environments, if I'm a fluorophore and I'm in water, I'm going to feel pretty differently in my excited state. It's pretty dramatic when you see it. So here, say molecules. Now, ethidium bromide is a dye that can get into cells. And we can look at DNA within cells. Here's a picture of how it would look. DNA microarrays exploit the complementarity of DNA sequences. They can spot 40,000 distinct sequences of DNA in grids to recognize these sequences. And they could be used for profiling genetic material, not just DNA, but RNA and other genetic material. The next class will discuss what the limitations of that are, but we've already talked about the fact that we have to use antibodies with fixed, not living anymore, cells. And a particularly important technology is known as a DNA canores, which can be used to probe for a particular sequence of DNA. dye does when it gets to the DNA. What would that do to things like replication and transcription? It just kind of messes it up. And so these are toxic dyes that can only be used in fixed cells to do observations of cells. So we use it a lot. We need to be careful of it because if it gets absorbed through our skin, it could get into our cells. And it could interfere with replication and other cellular processes. Because it would accumulate on our cellular DNA. in those grooves and you're dissociating easily, you're not going to interfere so much with replication. Does that make sense? So it's a weaker force. Now, I moved this slide up. I realized he was in the wrong place in the deck. This is just an application of the DNA minor groove binder CEOCHST. And in this case, we're looking at three cells. These two are not actively dividing. But take a look at this cell, it's actually clearly in the state preparing for cell division. Professor Martin will talk about antibodies, which are agents of the human adaptive immune system. Antibodies are biological macromolecules that particularly evolved to recognize target antigens. They have been exploited intensively to study biology. In two or three lectures, Professor Martin will learn more about the nuts and bolts of the immune cells, and how it mounts a response to disease and other features. The lectures will take place in the U.S. at the University of California, Los Angeles. B cells are the cells of the immune system that produce soluble antibodies. When you challenge a B cell population with a foreign entity, the B cell will go into gear to produce antibodies that very specifically recognize that foreign target. B cells adopt a very classical shape. But another part of the structure is variable. And when B cells mature, there's loads of rearranging in that variable section in order that it adapt to bind to target. This is what you want to think about is it's a combinatorial system to take little pieces of DNA into a super molecular structure to give you antibody combining sites. The way you make antibodies is by injecting.at cells through the use of antibody structures. The way you generate antibodies is through laboratory animals. And very commonly we use either mice or rabbits for antibody production. The rabbit is used when you need a lot more antibody material. The mouse will satisfy for some experiments, but it's not necessary for all experiments. It's just a way to get the antibodies to bind to the foreign agent or antigen that you want to make an antibody to, which would normally bind at a variable region of the molecule.

ROUGE-1: 26.14, ROUGE-2: 24.32, ROUGE-L: 21.51
BERTScore: 62.46

==============================================
==================== [89/100] ====================
Summary:
 RAFAEL JARAMILLO: Today, we're going to discuss the many D's of thermodynamics. We'll talk about lowercase d, lowercase Greek d, and uppercase Greek D. Jaramillo: Of the three D's, the U.S. D is the one that contains the most physical, unstated assumptions. He says it is often the hardest for students to understand when you first encounter it. JARamillo says to illustrate the concept of transformation quantities, it helps to draw state function surfaces. In thermodynamics, we have many different transformation quantities that we keep track of. In material science, our most common independent variables are pressure and temperature. But for any transformation quantity, you can at least imagine, if not draw out on a piece of paper, state function surfaces corresponding to the quantity that you're trying to measure, independent variables corresponding to those variables that you are regulating, and a vertical distance. So this is illustration for an isobaric, isothermal transformation, and we're illustrating this for entropy.

ROUGE-1: 28.70, ROUGE-2: 25.36, ROUGE-L: 25.49
BERTScore: 67.39

==============================================
==================== [90/100] ====================
Summary:
In this video I'm going to be going over lung oscilation specifically the sites of where you osculate. We're going to go over normal breath sounds versus abnormal breath sounds. In the next video I'll be performing an assessment on a patient and show you how to listen with your stethoscope to these sides. I'll also give you some landmarks to make your job easier so you'll know what intercostal space correlates with which lobe of the lung you're listening to. We're going to start from top to bottom and compare sides and work our way down. We'll start right above the scapula right where the Apex is and we'll listen here. Then we'll just inch a little bit down maybe to the seventh space down the lung and just listen in those lower loes. We will inch down to the sixth intercostal space but mid axillary so where their armpit is go Midway. We're down in the lower lobe of the lungs and we will have them in inhale and exhale and then we'll compare it on the other side. on women you want to have the woman raise up her breast so you can get underneath those sights so youcan hear those lung sounds because the tissue will muffle the noise and you won't be able to hear that. Third type of continuous adventitious breath sound is called stri spider and this is heard on inspiration because what's happening is that the airway is being obstructed by inflammation or some foreign object. Once you hear this you will never forget it's very unique sounding it is a high pitch whistling or gasp with a very harsh quality. remember to get the best sound quality so you can hear so you're not listening over the shoulder blades because that will muffle your sound and you won't be able to hear. Have your patient put their arms in their lap or just separate those shoulder blades from each other so they can get in between that spine and shoulder blade area. From C7 to T3 your cervical and thoracic spine that is where your upper loes are and as you can see here's your Fishers right here. and discontinuous now first let's go over continuous what does continuous mean this is a extra sound that you're hearing that is lasting SE more than 2 seconds with a full respiration okay the first type is called a high pitch polyphonic whe let the name help you okay so what is it it is mainly heard in expiration but it can be in Inspiration as well. A lot of times you'll hear it whenever the patient's breathing out it is a low pitch whistle sound and this is what Strider sounds like.

ROUGE-1: 25.48, ROUGE-2: 24.28, ROUGE-L: 19.55
BERTScore: 68.50

==============================================
==================== [91/100] ====================
Summary:
Professor Donald Kagan: We were examining Sparta, the most important, I think, of the early poleis. In short, the average Spartan did not ever speak in the assembly, it appears. So it's not a democratic assembly, even though every single citizen is there, if he wants to be. Let me turn now to the ephors. These, according to Spartan tradition, were invented somewhat late in the development of the Spartan constitution. When we see them they are usually engaged in dealing with foreign policy. mentioning that that assembly--you want to distinguish that assembly from what I'll describe shortly about the Athenian Assembly. In this assembly, it is true that all adult male Spartans were participants, and let me also say that they came to the meeting dressed in their military uniform. When a question was put to the Spartans, the way they responded was by shouting and banging on their shields. Whereupon, the presiding official would try to determine which side had the loudest noise. It's like a voice vote in one of our own meetings, only a little bit more colorful. Most issues that came before the assembly were probably infrequent, says Thucydides. But when it involved something fundamental like war and peace or alliances, then they would have to go to the assembly to get their approval, he says. "My guess is that it would have been wildly reckless and therefore never done for the ephors not to going to the gerousia first," he adds. "The idea was to sort of have a representation of the ordinary Spartan to carry on the functions that I have talked about" Aristotle tells us that they in fact were just any Joe Spartan, that they were ordinary people, not distinguished in any way. Dorian's versus Achaeans still seems to have some meaning to the Greeks. It suggests that that division among the Greek peoples hadn't died down yet and I think that's what's going on and that's the system that was the Spartan way of life. Nothing could be greater as a contrast to this way of thinking than the way that the Athenians will develop when they go through their growth as a polis. Sparta became the first state to be in command, or in control, or to be the leaders of a coalition of states. The Spartans gain a reputation of being--because they often fight against tyrants--they gain a reputation of being hostile to tyranny. Sparta is by far the strongest military force among the Greeks, but they are more than usual reluctant to fight, and they don't like to fight wars if they can avoid them. War, if you win, you are going to have booty; loot of some kind will come back into Sparta. An ancient alliance between the Spartans and their allies is called the Peloponnesian League. The Spartans took on other Greek states trying to establish their domination. They defeated the powerful and important state of Argos. Spartans are trying to claim union with the Achaeans not dominance over them. The League was formed around 570 B.C. and lasted until the end of the fifth century anyway, says Herodotus, the ancient Greek historian and author of "The Histories of the Ancient Greeks" It's a very important state for the Spartans, not just because it's the neighbor right to the north of them, but because remember what I told you, if you want to get to Mycenae from Sparta you can't go across those mountains. So, its strategic importance is very great. The Spartans got into this war with Tegea and they claimed to have discovered the bones of the great Homeric hero, Orestes and taken it away. Also, there was a legend that maybe they propagated that showed up in some poetry we have. According to Thucydides, the fear of the Helots was at the core of it all. If we take our whole army, leave town, go three days march away, how do we know we'll find anybody alive when we get back? That's always on their minds, although they don't take place every day, take place very sparsely, but they keep happening so that the fear is never irrational. To that is added the permanent enmity of Argos, which never gives up the idea of returning to the great days of Pheidon. The city is Athens; the region in which they live is Attica; the people are Athenians. Everybody who is a citizen who lives in Attica is an Athenian, no matter if he lives sixty five or seventy miles away from the city. Their tradition, and this one is surely right, is that at an early time in their history, Attica became a refuge for people escaping what they would have regarded as the Dorian invasion. Some of the most important Athenians traced their ancestry to the Athenians who were there before the moon was created. Attica contained silver mines in the south of the peninsula, and that gave the state a source of income. The availability of that silver would turn out to be crucial at various moments in Athenian history. Just as it does elsewhere, it leads to new wealth and new class distinctions, which are now based not on birth but on wealth. We hear about Athenians divided into different classes, including the Eupatridae, the well-born, and hippeis, the cavalryman. Aristocratic implies means ruled by the best, and best in that time means simply best by birth. Nobody in Athens holds an office at this time, or as far as I can tell, at any time. In the year 632, an Athenian nobleman who had become famous because of his victory in an athletic contest. married the daughter of a very wealthy and powerful tyrant in Megara, right next door to Attica. Cylon, attempted a coup d'état trying to establish a tyranny in Athens. terms of the place where you sort of established your belonging. The phratres, because it was established by birth and tradition, was an aristocratic stronghold. The Greek religion chief jobs in religion, they were the government, because as early as we can tell that there was a regime after the legendary kings are gone, the council of the Areopagus is the number one governmental organization, you might call it. It's important though to realize that in these early days of the polis they probably had very had very strong ties to the aristocracy. Aristotle tells us that beginning in the early seventh century, the date he gives us, is 683 B.C. He tells us we are introduced to a new thing, magistrates are chosen from the aristocracy to do various jobs in the city. In Athens, the magistrates were called archons. The year after their archonship they automatically go into the areopagus and remain areopagites for life. They will be looking very carefully over the shoulders of the aristocratic archons whenever they are in power to see that they're not screwing up. they named the year after the leading archon, the leading magistrate of the state. Mesopotamian cities did the same thing. So that archon was called the archon eponymous, the one who gives his name to the state and was the most important. His responsibilities were mainly religious, but I should point out that all the archons, whatever else they did, every one of them also did justice, that is, they had courts to which people could come to get their quarrels settled. than a year. The only thing in town that has continuity, that can develop power and influence over a period of time, is the council of the Areopagus. Aristocracies love equality; equality among aristocrats, and then tremendous inequality between them and everybody else. Yalies are very nervous about anybody sticking his head up above the crowd, because the question is always why not me? You have high expectations of yourself and so sometimes unless you're invaded by later religious ideas that the Greeks didn't have, you's not humble, you're vying for honor. what we will see in the future is that men who are hippeis who are not necessarily aristocrats. At the bottom of the barrel we hear about people called thetes. They've always been around, they are the poor; they don't own land. They live at the mercy of chance; they work for other people. But now comes the new thing, people called zeugitai. What does it mean? It means yoke fellows. These were men who were sufficiently well off that they could own a team of oxen. opponents that he was defeated. The leader of the resistance was the family known as the Alcmaeonidae. They went up there, locked up Cylon and his supporters in the Acropolis, in a temple. They cut the cord and killed the Cylonians. That put an end to the Cylonian conspiracy but it brought something to AlcMAeonidae as well, a curse. They were declared accursed and driven from the city. Later on we will hear they're back again and they're very important. But the curse continues to be attached to the family.

ROUGE-1: 27.53, ROUGE-2: 25.69, ROUGE-L: 20.77
BERTScore: 67.08

==============================================
==================== [92/100] ====================
Summary:
Ka-Yen: "I walked into MIT not knowing a single thing about nuclear energy" "What causes nuclear resurgence? This is the perfect time to talk about why nuclear power's cool" "I'm going to be honest with you. I don't know all that much about potentially spewing out that much information" "How do you calculate this? Bozeman Tzeman. Cool? Whew." "What is the difference between fissile, fertile, and fissionable material?" In 1951, the first nuclear reactor to produce electricity was the experimental breeder reactor, the EDR1. The first nuclear powered submarine, the USS Nautilus, was launched in 1954. The real heyday of nuclear was between 1960 to 1975, when people like Westinghouse were creating nuclear reactors. Today, we're entering what people like to call a nuclear renaissance, which is between this chunk over here and 2002. But basically there's been a whole new push for creating more about what I've mentioned. Nuclear creates 75 times less carbon emission than coal does, and 35 times less than natural gas does. Nuclear power is able to provide a good baseload source of energy. The main thing that holds us back from just having nuclear power everywhere and creating about 90% of our electricity as we would hope it would is economics. It's just about $0.01 per kilowatt hour, as compared to natural gas, which the majority of electricity comes from. But if you look at the operation and maintenance, which is the same as maintaining a coal power plant, it's about the same. The fuel core is basically just a bunch of rods of uranium, sometimes it's clad in something like zirconium. Neutrons can simulate other fissions, and the control rods are there to make sure that there's not too many fissions happening in the fuel core at a certain time. The heat that gets created during these nuclear fissions goes and heats up the water. It creates steam so the steam goes and spins a turbine. The turbine creates electricity. And it comes back and gets recondensed. The next kind of reactor that falls under the light water reactor category is the pressurized water reactors. With BWRs there is a higher chance of leaking radioactive material into the environment. PWRs comprise about 60% of the reactors the United States. They are functionally essentially the same, and it's just slightly more complicated. The main downside of this is that deuterium is really expensive, which is really ridiculous because a kilogram of water is really much at all, you know? So even though you're using your reactor with lower enriched uranium, you're actually replacing it more often. Heavy water reactors use heavy water instead of light water. Heavy water has a much lower absorption cross-section than light water does. This means that when neutrons are flying around in the reactor there is a chance of it hitting a fission product and a piece of fissionable material and undergoing fission. But there's also a chance that the water that surrounds it will absorb that neutron. If that neutron gets pulled out of the system you're not able to create any more fissions. This is actually kind of a bad thing because the whole point of nuclear reactors is to create heat and fission, right? keep adding more fissile materials, because it's not as perfect as they want it to be. And then finally we're going to move on to generation four reactors. Generation four reactors are all the new kind of reactors that people want to build. Here are the six kinds of generation four reactor types that were deemed to be the most promising. So there's gas-cooled fast reactors, lead- cooling reactors, molten salt reactors, sodium-cooling reactors, among others. After a nuclear accident you can see a pretty steep decline in the amount of nuclear reactors that are being commissioned. This is especially noticeable at Three Mile Island, which is essentially the first nuclear reactor accident that we all had to go through. The next reactor accident, the big kahuna I like to call it, is Chernobyl. And again Fukushima, once again, with the number of reactors being commissioned after the accident just declines dramatically. So these are mainly the things that people who don't really have any nuclear energy energy think about. At Three Mile Island there was containment that prevented radioactive isotopes from leaving the system. They didn't realize that it became stuck because their equipment and their instrumentation wasn't able to detect that. So now there's water leaking out so the core is getting hotter, but then they also took out the water that is usually used to cool the reactor core, so again it's also getting hotter. This combination of events led to a core meltdown. So the core melted down. That's never a good thing, by the way. 28 highly exposed reactor staff and emergency workers die from this radiation or from thermal burns during this time. Officials also believe that there is about 7,000 cases of thyroid cancer that occurred because of Chernobyl. No one really lives near Chernobyl at the moment. It's kind of been deemed unlivable because these radioactive isotopes literally went everywhere in this environment. It is not safe to live there. Luckily we see that there are animals coming back now now. But it's been about-- how long has it been, like 30, 40 years? People aren't advised to live here. The U.S. wants to bury nuclear waste deep into the ground and never be able to retrieve it again. It turns out that if you were to just keep all the spent fuel that we create in fuel casks, it'd take about 300 acres of land, which is absolutely insane. Yucca Mountain is the primary push by the United States to find a deep geological repository somewhere in the US so we can deal with our spent fuel. The main push for this was-- well, first of all, it's a permanent method of disposal. Yucca Mountain is located in Nevada. People in Nevada weren't happy about this. There was a lot of opposition. Because of the social opposition there was government opposition and many loopholes we had to jump through. They also realized that it wasn't as geologically sound as they had hoped. There's a lot more groundwater running through and seeping through Yucca Mountain than they thought there would be. So there's a huge debacle. Basically the costs are rising, nothing much was happening. And then 2011, under the Obama Administration, he just called it quits. can I have a billion dollars to build this nuclear reactor? It's going to take five years to get your profit back. No investor is going to be like, yeah, that's a good idea. That's the main reason why we can't get nuclear up and running. We have the possibility to create a lot of plants, but we just don't have the money to do so. And also, if for some reason something happens, you have to stop building your reactor. Like, there's no turning back, right?

ROUGE-1: 23.10, ROUGE-2: 21.46, ROUGE-L: 19.56
BERTScore: 53.46

==============================================
==================== [93/100] ====================
Summary:
Professor Steven Smith: I want to talk today about Aristotle's discovery of America. Smith: In many ways, as it is for every student of politics, the most difficult issue one confronts is the problem of faction. He says, in a way, Aristotle has discovered long before James Madison's famous article in Federalist Number 10, the remedy for the control and containment, so to speak, of faction, he says. Smith says the purpose of the best regime is directed not Tolstoy, Henry James, and perhaps the greatest of all, Jane Austen. any sensible reader of Aristotle would reach is that Aristotle, in fact, discovered the American Constitution 1,500 or 2,000 years before it was written. Aristotle understands the mixed constitution as a balance of classes--the one, the few, and the many. For Aristotle, it is not the liberty of the individual so much as the functioning or functional well-being of the city that is the highest priority. He would regard elections as merely exacerbating the tendency to demagoguery, where each person seeking office plays shamelessly to the mob. Aristotle understood the importance of property and private property and commerce for a flourishing republic. He would never endorse the view stated by a famous American president that the business of America is business, he says. The political partnership must be regarded for the sake of noble acts performed well, he tells us. If wealth were the purpose of politics, Aristotle writes, the Phoenicians, you might say, in the ancient world, would be the best regime. But he denies that. He says the aim of the city is not wealth, is not the production of wealth. to war, but in fact to peace. The citizen of the best regime, he says, must be able to sustain war if duty requires, but only for the sake of peace and leisure. Leisure does not simply mean rest or inactivity, but leisure is necessary for education or what he sometimes calls by the term philosophy. By philosophy, he seems to suggest not so much the capacity for abstract or speculative thought, but rather a kind of liberal education that he regards to be the preserve of the megalopsychos. Aristotle's best regime differs from Plato's intransigent demand for the rule of philosopher-kings. The megalopsychos, the gentleman, whatever else he is, is not a philosopher in the strict sense. Most importantly, you might say, what distinguishes the gentleman as a class from the philosophers is a certain kind of knowledge or practical intelligence. The gentleman may lack the speculative intelligence of a Socrates, but he will possess that quality of practical rationality, of practical judgment necessary for the administration of affairs. Aristotle describes a quality of judgment called phronimos. He says it is a kind of knowledge most appropriate to politics. How is this knowledge acquired? Are we just born with it? Do some people just have it or is it a product of experience? Aristotle doesn't say, but I think the answer is clearly some of both. It is a quality, as I agree with Berlin, possessed by some of the great psychological novelists. To refuse to participate in that conversation is either to be below humanity or above it. To be human is to be part of that conversation. animal means first to possess speech or reason that allows us to participate in a community or a way of life governed by shared standards of justice and injustice. To be a political animal, for him, is to engage or for praxis, is his word for action. All action, all human behavior is aimed at achieving some type of good, is all aimed at action. When we act, we seek to preserve or to change. All political action, you might say, is guided by the idea of better or worse. Aristotle suggests that there will always appear to be something ad hoc about the methods used in the study of politics. We will have to let the method fit the subject, rather than demanding the subject matter fit a kind of apriori method. To insist on that kind of methodological purity would be to impose a false sense of certainty or absoluteness on politics, which is variable and contingent and always subject to flux and change. He lays out a set of common questions that political scientists have to address. These four questions are intended to guide inquiry, to shape and direct inquiry. They are not intended to yield sure or certain results, but to guide and inform statesmen and citizens in the U.S. The political scientist must have a grasp of the best regime, given the most favorable circumstances. He must also know something about the techniques of reform and persuasion, what we might call the area of political rhetoric by which existing regimes can be brought closer to the best. And he must have some knowledge of how to render any regime, no matter how imperfect, more stable and coherent. Most contemporary political scientists tend to be liberals. Their values are liberal values. This raises a question. Whether the relation between contemporary political science and liberalism is merely accidental or whether there is some intrinsic, some necessary connection between them. One might do well to ponder which political science is really more scientific--Aristotle's, which is explicitly and necessarily evaluative or that offers advice and exhortation to the statesmen and citizens about how to care for their regime, says Julian Zelizer. the back door. On Friday, let me just remind you, Il Principe. We'll study Machiavelli. On this very partisan note I conclude. On Thursday, we'll study the life of the Italian statesman and play a game of chess with him. We're going to see how well he can play the game of poker. I'm looking forward to it. I hope you'll join us for the game on Friday night at 8 p.m. ET on CNN.

ROUGE-1: 30.10, ROUGE-2: 27.37, ROUGE-L: 24.71
BERTScore: 62.78

==============================================
==================== [94/100] ====================
Summary:
The Stanford Natural Language Inference Corpus is a big data set. It has over 550,000 training examples and 10,000 examples for dev and test sets. The premises are all image captions from the image Flickr30KData set. The train premises, in this case, are going to be much more diverse and drawn from five genres-- fiction, government reports, letters, travel guides, and phone and Berlitz things. And then, interestingly, the mismatched condition is what they call the "9/11 Report" unanimous gold label. And we rate the overall human level of agreement at about 91.2% for the gold labels. 92.6% is the traditional measure of human performance here. For MultiNLI, the test set is available incredibly productive. How are we doing on MulitiNLI? So again, we're going to have our score over here and on the x-axis, time. And since it's on Kaggle, we can look at lots more systems. crowdworker had to come up with three sentences. One may be correct-- that is our gloss on neutral. And one definitely incorrect, which is our Gloss on contradiction. And here are some examples from the validated set. And I think they're sort of interesting, because you get high rates of agreement, but you do find some examples that have a lot of uncertainty about them. And that might be a hallmark, actually, of NLI problems. And Adversarial NLI is exciting because it's given rise to a whole movement around creating adversarial datasets. The train set is a mix of cases where the model's.pair is independently validated. So in this way, we're kind of guaranteed to get a lot of examples that are very hard for whatever model we have in the loop in this process. And so what we're hoping is that as we progress through these rounds, these examples are going to get harder and harder in virtue of the fact that the model is trained on more data and is getting better as a result of seeing all these adversarial examples.

ROUGE-1: 23.19, ROUGE-2: 21.59, ROUGE-L: 16.51
BERTScore: 61.92

==============================================
==================== [95/100] ====================
Summary:
In a perfectly competitive market, the firm is a price-taker. No matter how many units they produce, they're just going to be able to get that same market price. A firm in an imperfectly competitive market will have their own firm-specific demand curve. The price that they get in the market is higher than the marginal cost and the marginal revenue at that point. Folks are willing to pay more than that marginal cost, but you still have no motivation to produce more.

ROUGE-1: 21.16, ROUGE-2: 19.73, ROUGE-L: 21.16
BERTScore: 65.43

==============================================
==================== [96/100] ====================
Summary:
"Between the Folds" is a weekly, off-beat look at the world of engineering. This week, the professor talks about infinitesimal rigidity. He explains how it can be used to test whether a 3D graph is generically rigid. The professor also says goodbye to origami creator Eric Joisel, who died on Sunday. He says Joisel was a master of origami, and he never met him in person, but he had a great spirit, and his photos will be shown Tuesday. of an informal notion-- a valid first derivative of a motion. I want to take such a motion, essentially, take its derivative with respect to time, And evaluate that derivative at time 0. If the motion exists, surely you can take this derivative and evaluate it. You need some smoothness. But if there's a way to get started moving, that doesn't actually mean you could actually move. If you have two vectors, a and b, you take their general, we want this thing to equal that thing. And if you rearrange terms, that's the same thing. In linear algebra, we need to understand how this length changes to the first order. This is a dot product in d of w-- that vector-- dot product with C of v minus C of w. If I project this vector onto this one, that's going to be the first-order change to this edge length. If we can ever find something and show that it's infinitesimally rigid, then we've determined it's rigid, professor says. The idea of a rigidity matrix is a matrix equation, if you want. There's a fun theorem called the rank nullity theorem from linear algebra. It says if you have a matrix, you take its rank-- which is another quantity-- and its nullity, and you add them together, it will be the number of columns in your matrix. In two dimensions, ideally your nullity is-- if you want rigidity, your Nullity should be 3. So this is telling us something we already knew, but essentially for free, which is kind of fun, if you know all thislinear algebra. it's another way of thinking about essentially everything we've seen. And if you know linear algebra, it's useful. Otherwise, I'll tell you how we can use it. OK, for this definition we need the notion of a minor. So we have some matrix. Actually, our matrices are pretty rectangular. And I just choose, let's say k columns from the matrix-- let's I'm saying is if it's non-zero for some choice of C, then it should be non- zero for this choose of C. say 3, and I choose k rows-- the same number of rows and columns. I look at those elements in the intersection, that's a square matrix. That's a minor-- that 3 by 3 square submatrix is called a minor. Then we take the determinant of that matrix, which I won't define. It's a number associated with that matrix. And if it's non-zero-- now remember, rigidity matrix is defined as a function of C. It depends where you put the vertices. Tensegrities are things like this-- here's a tensegrity built and given to me by Bob Connolly. And it has well, I guess, there are three kinds of edges physically. There are the springs, the struts, and there are these wire cables. And its rigid. It's a generalization of a linkage where we allow three kind of edges. It can be a bar-- and this is the old case, which is that the length is fixed. We can have cables, which are the reverse-- you can increase but not decrease. Theory of duality goes back to Roth and Whiteley, 1981-- good year. If I look at all the equilibrium stresses, and I find some stress that is non-zero on a particular structure or cable-- I should mention there's always a trivial stress, which is you set all of these S's to zero. Then everything will hold. If there's a stress that's nonzero on that edge, then in fact, it can't change length at all. It effectively acts like a bar in this configuration. But it's rigid if and only if this is true and this istrue. Professor: I have linear equations. I also have linear inequalities. I can write it as some other matrix R prime. Actually, it's the same matrix. R prime times d is greater than or equal to 0. This is the general form of a linear program. You can write an equality constraint in this world just by taking it and its negation. That forces them to be equal to zero. All you need to know is it there are fast algorithms to solve this, also. If I give you a tensegrity in polynomial time, you can tell whether there is a valid infinitesimal motion that does all things you want on the struts, cables, and bars. In a force polygon, all the stresses are positive, so you're automatically infinitesimally rigid. A polyhedral lifting is going to work if I have a non-crossing configuration. This is a brand-new result from this summer at [? OSMI ?] by Robert Lang and Alex Bateman. In particular, you're going to have to have non-triangles. Those should all remain planar. That's a lifting. If you have any lifting or any lifting, you could not lift them at zero. them before. A spiderweb is something where every edge is basically a cable. You can see under wind and tension, they get a little bit shorter than they're supposed to be. They are pretty strong at preventing expansion. A positive stress corresponds to a mountain, which is the same picture upside down. A zero stress corresponds. to a flat angle-- doesn't have to be horizontal, but it has to be flat. Theorem is there's a one-to-one correspondence between the equilibrium stresses and polyhedral liftings. a little bit, but all by the same amount. And then you twist them a little bit. So here they are twisted, and then you just connect the dots. This algorithm has been around forever, as far as I know. It's how most tesselations are folded. But when does it work? And there is a brand-new result from the summer that it works for some choice of those angles the chain. For convex cycles, it's a little less obvious, but it's also true. can't translate around, but you have one two-degree two-rotational freedoms, I think. So say remove rigid motions by forcing the outside face to lie in z equals 0. So in this particular example, this guy can go to any value, positive or negative, but that's it. Now, this is a lot like stresses-- in fact, it's identical to stresses. It's probably not obvious, but just like stresses where you could set everything to 0, here you can also setEverything to 0. This is called Maxwell-Cremona Theorem. configurations where no two edges cross. This is a smaller version of our old configuration space. It's still defined by polynomial inequalities. What we allow-- and in this case I'm just thinking about linkages for the moment. Two edges either share a vertex or not. And that's it. But they're not allowed to touch anywhere else. And now what we care about is something called a locked dimensions, and maximum degree 2. We can think about what happens with degree 3. Then you can get locked things, and other fun things like that. Claim this tensegrity is infinitesimally flexible, meaning it is not rigid. And that will imply that there's at least an infiniteimal expansive motion. And then you have to use some fancy tricks-- not fancy, some tricks I don't want to talk about. You basically integrate that vector field, and that will give you an actual expansive motion and straight or convex. But how do we actually prove this thing? Let's not worry about that. The interesting part is show that at least infinitsimally, this tense grity moves a little bit. In a typical case, we're going to get a polygon. Wherever it has a convex angle, you get a mountain here. Every polygon has at least three convex vertices. So that's the general picture. And the only thing that can happen is you actually prove that all of this stuff has to be flat. That's the one way it can happen. You can't have strict valleys, but you could have them all 0. And that is the Carpenter's Rule Theorem. still buy them at the hardware store. That's it. You can't buy them online. You have to buy them from the store. They are not available online. They can be bought at the store, but you have to pay for them in cash. They're not available on the internet. You must buy them in the store to get them. You cannot buy them on the Internet. They must be bought from the hardware stores. That is it. They have to be bought in the stores.

ROUGE-1: 24.23, ROUGE-2: 22.05, ROUGE-L: 19.52
BERTScore: 66.87

==============================================
==================== [97/100] ====================
Summary:
Chess has been known as a tool of military strategy, a metaphor for human affairs, and a benchmark of genius. The game was originally known as chaturanga– a Sanskrit word for "four divisions" With its spread to Sassanid Persia, it acquired its current name and terminology– "chess," derived from "shah," meaning king. Chess-playing computers had been developed for decades, but Deep Blue’s triumph over Garry Kasparov in 1997 was the first time a machine had defeated a sitting champion.

ROUGE-1: 21.34, ROUGE-2: 20.34, ROUGE-L: 21.34
BERTScore: 63.36

==============================================
==================== [98/100] ====================
Summary:
presenting okay share your screen that's what I'm doing oh you are okay hopefully it is yeah stop sharing yeah you should be sharing my screen under your camera until I can decide if I click on slideshow this is still show my camera uh it does I guess I can minimize it do screen sharing are you recording tooYeah great baseball back yeah I mean it's my first time giving my lecture so I'm as good as I can be do you want that cheers I mean I have to write something to hear me okay okay you know that it works. this week will probably go live tomorrow uh not quite sure yet but you'll try to get it up as soon as possible so I guess like without further Ado let's Jump Right In. I'm gonna go somewhat into detail into what representation learning is and I think this should sort of cap out the last few weeks of deep learning um and probably give you a more comprehensive understanding of what deep learning actually is doing. We'll also talk a little bit about what transfer learning is. I'll cover convolutions and CNN's in greater depth very soon. The machine learning pipeline you start with an input X you extract all the relevant features from it and then you push those into a machine learning algorithm should get an output Y and you sort of optimize based on that. The process of choosing the right features can get really complicated really fast and this is also kind of a compromise solution in the sense that you are learning the weights of your model but you're still hand programming the feature extractor yourself. Deep learning says that hey we don't need to hand program feature extractors we can all learn those as well in fact we can learn the entire pipeline from feature extraction to um training and you could you just need to pass in this like raw image input and it will spit out an output. There are special feature extractors for images so this is sort of what classical machine classical CV look like. Once you train this model you can take um can you see my cursor you can once this model has been trained these three like set up neurons in the middle of the model can then be 16b. Even without any labels we can still learn something meaningful about the structure of the raw data how many have you guys taken 16b before so yeah you might have you might recall something called PCA from the class of principle component analysis it's actually one of the most common unsupervised learning algorithms. but luckily huge models have already been trained before so the sort of question is can we leverage them in some way and the answer is yes absolutely. There's an idea called freezing so that's basically what you guys both mentioned one idea is to freeze certain layers of our neural network so basically we use our pre-trained Network which is like the already trained resnet classifier this card a few of the layer layers and then freeze the remaining earlier layers and so we can add and train the later layers to basically customize them. The layers are all network but a lot of the times they'll be like 100 layers for example depending on how deep we want it to be. In this example we start sort of abstracting in layer three um where we see objects and humans and then once we reach layer five draw like decision boundaries based on that. Even if you don't have any labels the model can still learn that okay these points are dripping up together they're forming clusters and this is still like meaningful information that themodel can learn so yeah hopefully it's sort of this picture makes clear the difference between unsupervised and supervised learning. in the past few lectures can be basically avoided so the speed of our model increases by a lot by doing this. transfer learning we basically apply the knowledge that we've learned from one pre-trained network and apply that to the second task. transforming learning is especially huge in NLP as you can imagine with natural language once you've trained on a large large amount of text there's no reason for you to relearn all of that um so you already know the meaning so there's like some semantic meaning. um and the similarity of the new data set to the original data set so for example in the case one where you have um a small small data set or a lot a large data set. Since it's larger we have more confidence that we won't overfit if we fine-tune. On the other hand if we have a smaller data set even though it's similar to theOriginal model it's not a good idea sometimes to fine tune because you can definitely overfit. And then in the third case in which you have like a smallsmall data set and then it's pretty different from the first task um. In terms of neural networks embeddings are pretty important so they are often described as lower dimensional learns continuous Vector representations of discrete variables. Embeddings are useful because they can reduce the dimensionality of your categorical variables for example and meaningfully represent them in the transform space. The idea is that if we can directly work with the significant somehow find a way to represent this line using just one variable instead of three that's going to be better. We often prefer to work with lower dimensional data so a common task is transforming them in from high dimensional data into lower dimensional instructors. Using pre-trained networks and using transfer learning is a really really good idea but not only does that apply to NLP it applies to almost other domains every other domain xcp. Downstream tasks can be something like you learn the representations from some pretext tasks and you'll use those representations for image classification or object detection or semantic segmentation or whatever for NLP. The next part of the lecture is going to be on self-supervised free training so before I tell you what self- supervised is let's clarify some terminology. notion of a loss that sort of Compares how far apart this prediction is from the ground truth label y right and your goal is to optimize the network such that this error decreases and your model is trying to output something that is very close to the actual labels y . so in a sense your training process is receiving supervision from the labels your super your labels are guiding what the model must learn. Some examples of supervised learning can be your typical classification problem like the one that we just showed. It can also be something like regression if you have kick in say 16 a or means. using that and turns out this is a pretty good idea also but it turns out that these large data sets are usually not labeled you can you could like scrap text or image or images from the web but you can't really label them automatically right so a lot of the time you're working with unlabeled data. We want to see if we could use unsupervised learning techniques to the sum level data sets and learn representations using that and this is also appealing because labeling in general is a very time consuming and tedious process. anymore but it's also trying to understand what's going on in the image it's it's going to learn that an image can be made up of different parts and those parts are going to be related to each other. There are some more technical details on the slides um I won't go into those but something that the authors actually did I actually go mention this is when they sample the patches and they divide it into a grid instead of taking the grid directly they actually generate each patch a bit.  SSL can also be applied to audio it's a very broad sort of paradigm and I think the currency of the art and audio classification is Wave 2 Vector Q which I think came out a few years ago okay so what if we go back to this idea of where to work we are predicting a single word from some surrounding context right. What if you predict a word from the entire sentence that it is a part of and as I think I think you might imagine that this white might work better because the sentence will give you more context than just like those two surrounding words. the slide is it turns out that the current state of the art for CV is actually very similar to Burke so that's a teaser for the lecture that we discuss Advanced Techniques and as software CV. There is a homework for this entire cluster which is the high Crush notebook that should be due next Tuesday even though this lecture doesn't have homework I mentioned before that there will be a lecture on on Advanced SSL for CV and that will have a homework so to work on that homework this slide deck should be up on the website again if you feel free to do that that is it for today a second pause.

ROUGE-1: 30.86, ROUGE-2: 29.87, ROUGE-L: 25.86
BERTScore: 72.00

==============================================
==================== [99/100] ====================
Summary:
All right welcome to the third lecture on Foundation mulative AI. Today we're going to cover chat GPT. chat GP was the the tool or the the AI that really made people understand this is different now. Next time we'll talk about stable diffusion image generation and then we will talk about emerging Foundation models basically Foundation models generative AI in the commercial space. Then we'll end with the lecture on AI ethics and regulation as well as a panel okay so what have we talked about before? Foundation models geni apply this self-supervised learning where we learn without label data so we can we can get as much data as we want because there's no human being in the loop. What we get from this you know by learning from observation and learning from the data directly is a very contextual and relational understanding of meaning. The robot learns to rate responses that's supervised learning because yes it is supervised learning and then the actual model is generating the response that's a great Point. Force things to comply to Simple Rules right it kind of abandons our ability to understand and compress what we're seeing and deals with that chaos directly that's why AI is so powerful and so humanlike um so also like when I talk about this in CHP we try to make very high level um statement but of course the nuances matters and I think it's quite interesting. I took this quote from a general from the 18 and 1700s and he says this quote that P Theory which sets itself in opposition to the mind and what he meant was that he's a general so he fights in battles and War. so we have a sequence of words and and then we're just going to try to predict uh the next word based on previous words so let's say we have uh we start with i here as input. Then we want to someh predict the Target right so we know we know or the computer knows somehow by just downloading the text that what this whole sequence is. When it trains this AI model it hides part of it right so it just inputs I to the AI model and then it's supposed to do something with it. We're going to create scores or predictions for all words in the L like in the human you know vocabulary in the English vocabulary that sounds extremely expensive and it is quite expensive and so I have different tricks to make this work.then it kind of gets it right and then you give some positive feedback back and we'regoing to kind of do this um we'reGoing to maximize this. Then you give this feedback to to the model it's called back propagation so you given some feedback through model it should push the score or the probability distribution for the correct one to be bigger or larger and then reduce all other ones. like a a specific token that says we're happy until we complete a complete sentence for example um and this is kind of expensive uh to do because you have to generate one thing at a time but of course training is is much faster because then you can just you don't need to generate and run on your own input. Al: We have a great model that's been fine tune on dialogue and it's able to generate really good answers still uh to different prompts so uh we're going to run this model now on a collection of prompts. Transformer was trained at a scale with an amount of data and parameters that we never seen before. Just training the final model cost around $5 million just in in Compu electricity bills right that's how huge and much compute they spent on this. Transformer has much less structure and has to relearn a lot of this structure. Since now we have St super plus learning we can train by just downloading text from the internet and there's no human being to train it. It's a very very simple approach but it's a certain scale that's that's never been seen before and really that's what a big part of open eyes Transformer is. well we're just going to go all in and just make this bigger and bigger and big and and and then like in hindsight like maybe it makes sense but it could have been a case like it wouldn't work and then people like oh that's a stupid bet like why would you think such a simple idea and approach would lead to to such sophisticate intelligence but it did okay so we covered thetive pre-train part right so uh you know we've now said how we basically are going to uh train our model but how does this model look like likeHow does this kind of engine look like. is that for every step here that's label with the same uh digit you know they can all be done in parallel. step three can be run after step two is been run because step threes don't rely on each other and so on. This is key because in in deep learning we use this uh um computer is called gpus. If we can make multiple steps into single step in parallel this is a single cost we want to run things in parallel as much as possible. This works only during training now right okay and I'll come to that actually later. these are extremely extremely popular and a version of them called uh lstm long short-term memory networks um it performs really really well and some people say it performs you know almost better than Transformers a lot of times. But they just take them longer to train because we're going to realize why it one a point but they work really really good and also notice here somehow that uh this was very very intuitive for researchers to say like well text we read text from left to right we process words one at a time and therefore our models should to to to tble to learn effectively. things flow forward this in kind of this sequential way right so to get from uh you know for the information from I to go to the information prob being processed step number nine basically right when you want to predict the period has to travel eight or nine steps here to to to uh be used so let's think about this a little bit start we start off now in a Transformer which basically starts off the same way so we we let the first we know we just process the first word uh and we prict the next Target based on that. and it's going to be much faster to run so it's much less uh uh well that's a modification but uh it's a little bit less sensitive in a sense uh we care about both being fast uh and yeah I mean but somehow H this is going tobe much much faster than a recurr network so you're going to get much much better performance. During training we can do this because we not upend the words we just see them in the sequence we can doing this. just goes off a little bit like here for example when you say you know I went to the financial and then just you know some small error happens and it goes off the road to restaurant like somehow you know they started seeing that okay now it's basically go Haywire because it went off and it's in a different space than it's been trained on. So somehow we want to be able to say like well if you find yourself you know alittle bit off the the the path you should be able. to find your way back to be as as robust as possible. to any prompt that we have so let's say we and this is very cheap to do so we have a million prompts that we found online now we run our model four times on each prompt with different random seeds we we sample uh four different answers so now we have 1 million prompts with four uh candidate answers okay and then we're going to pay people to actual human beings to label these they'reGoing to rank this this uh prompt or the answers that these models produce to these prompts so we'regoing to pay pay actual human being to score them and say are they good or not. good or bad dialogue so we've solved that okay and the last two problems we are going to solve by using reinforcement learning so what is reinforcement learning well we talked about this a little bit before but uh something is very important and characteristics of reinforcement learning is this delayed feedback. In order to generate you know this exploration you you know you want to be a do a very very targeted exploration around language is still kind of make sense so the robot gives you good feedback and you actually can start you know making progress. gratification actually leads to very non- GRE and and independent robustness so these are the consequence of applying reinforcement learning where you only get feedback at the very end so there's less you know supervision right you're more on your own. H deal with an uncertainty of not having constant feedback you have to figure out things by yourself which leads to you being more robust and also again in reinforcement learn here the only thing we care about is the signal at the end so we don't care about making the best next step. We care about optimizing the whole output so we're now addressing these things. well because if you now have a better model you kind of want to you want to go to the human beings and get more feedback that's more relevant to this model because this model now is is doing better than the previous one. I think open a runs this two or three times um okay cool. We can just run this step uh and do it all again and you know you can done you can do this as much as you want of course uh maybe with some uh you know decreasing returns I don't know exactly. and self Suess learning care more about both aspects somehow somehow. Next time we will talk about uh we'll do a similar Deep dive into stable diffusion there will be self supervised learning and Foundation mod an AI but uh I think it's going to be slightly more conceptually interesting um so should be a lot of fun and yes please go to the website for more information Etc and if you have any questions feel free yeah can I something can I assume that probability on that after there changes based on the subject of the totally yes yes yes great question okay. word mask it and try to predict it based on the surrounding words um yeah the answer to that is we used to do that way and it works better uh but due to engineering you can Bas basically kind Transformer you can maybe this can be like a homework for you but if you look at Transformer Works uh if you mask a word uh then you will like then you'll only um okay I if you do this you know if only predict the last word based on previous words you can make this okay attention. oh actually works better given the same amount of compute yeah does that give you some sense of answer we can talk more about it offline as well. What are like the main challenges of these models nowadays and are there other like language models that are like better but they do like more expensive than or like this is like the best um yeah I think. I mean we want to able to rely on it as much as possible and if they if they don't behave like we want them to we don't want to make up things right. the World by looking at videos right you can even sort to understand how human beings work even better because you can see people being upset or sad or happy whatever right in in a video and start picking these cues up. You can connect the vision part to the text part and get a multimodality model that's able to do both in a really really sophisticated way uh also something that I think these these people are working on all right thank you thank you for your time and good luck with your book.

ROUGE-1: 30.82, ROUGE-2: 29.78, ROUGE-L: 28.72
BERTScore: 69.26

==============================================
==================== [100/100] ====================
Summary:
The EM algorithm is a framework that we use throughout most of the class. It's for dealing with various different notions of latent variables. Maximal likelihood is just a framework. It happens to be the one we're going to use. In GMM, we could guess randomly an assignment of every point to the cluster, the probability. In EM, we just keep running those two loops again and again, and hopefully, it will improve. You'll see this weird picture of a curve that we go up, and that's the loss function. The decomposition is quite important. And we're going to try and kind of abstract that away. That's what I mean by a kind of traditional supervised thing. What they're doing, basically, is they have that decoupling property. If we knew this thing that we couldn't have observed, then, all of a sudden, it becomes a really standard statistical estimation problem. And somehow, we are assuming structure, and that's what we're putting into the latent variable. And then we'll exercise it, basically exercise the notation. The EM algorithm is based on convexity and Jensen's inequality. We're going to draw some curves of a likelihood function that will hopefully be easier to optimize than the original function. And we'll try an iterative algorithm that will look exactly like we talked about before. And then we will is going to be equal to L of theta t-- sometimes call this the tight property, OK? Our hope is Lt is easier to optimized than l. Then we maximize that, and this is formalizing the back and forth. We take that new maximum that looks that new thing, which is our new algorithm. A set is convex if for any a and b element of omega, the line between them is in omega. A function is going to be conveX if its graph is, OK? As I said, let's draw an example of this. So here's 0. Here's minus 1, and I draw this character. Let me erase 0 and 1 because we don't really need them. Their values are kind of unimportant to us. It was just so you knew what I was drawing. We'll draw a. All right, awesome. This is a picture, which maybe won't make perfect sense to start with, but we'll get there. If f is twice differentiable and, for all x, f double prime of x is greater than 0, then f is convex, OK? So this says these functions really are bowl-shaped, right? Second derivative being positive means that they have this kind of positive curvature that looks like the U's. Their first dimension-- first derivative goes up and down, but they're kind of always trending. take what is the obvious thing to do. I'm going to multiply this by lambda. I have to make a statement about this, right? That's what's in my definition above, OK? Well, that's just the same as adding f of z for Lambda, plus 1 minus Lambda. So that shows that this thing, this inequality, holds, f ofZ. Oh, so you've seen the double, is that [INAUDIBLE]? Yeah, this is Taylor's theorem. is, otherwise, this thing, which we actually do care about, is strongly convex. This definition feels like it comes from space aliens otherwise. So for example, f of x equals x squared, which I told you was in my head. Well, this gives me a simple test, right? Its second derivative is 2. That's greater than 0. It is the prototypical strongly conveX function, OK? You also saw those This is to make this parameter sometimes with the curvature one. Doesn't really matter, but OK. I pick a curve, one way to define a curve. And that curve is going to be as a result of sweeping some parameters in a high-dimensional weird space. No matter how I pick the parameters of that curve, anywhere that lives on this thing, that's a probability distribution, a bunch of numbers that sum to 1 in the discrete case. That's going to allow me to build a lower bound for my function, and I'm going to hillclimb using it. We'll see that in just a minute. Are there any questions about this piece here? do something fancier if you want something that's a full probability distribution. This holds even if E is a continuous distribution. The reason you'll always get the inequality the right way is you'll draw the picture of the function and see the chord is always above it. We actually don't want to use a convex analysis. We'll stop at kind of high school calculus. Sound good? All right. Now, everything is defined in the literature traditionally for convex. If you take convexAnalysis, it's the way we define things. structure. In a real sense, when we make a modeling decision, and we say, there exists some structure out there. One version of the prior is, I tell you exactly where every photon comes from. That's clearly a very strong prior. If you knew that, Godspeed. Go do it. You just solved GDA. So let's take a look. What does EM actually do here? So what is EM? EM is very general. You can instantiate it. So what does it mean here? Be theta t plus 1, OK? And we're going to, again, create some new curve, Lt plus 1 of theta, based on that point. And the key aspects of the point that I'll write in a second is this point is a lower bound. This curve is always below the loss, so it's kind of a surrogate that I'm not overestimating my progress, and it's tight. So I wouldn't think and get fooled that there was a higher loss function somewhere else. about the algorithm, but hopefully, it's clear what's going on. Easy-to-train surrogate, and we kind of slowly hillclimb with that. And this is what we were doing in K means. This is the rough algo. I'm just restating what's on the thing, I'm not giving you enough math. But we'll see. All right. Cool. Please. Just could you reiterate? Like, why are we not using gradients on the original turbulence? Right, so we could imagine doing some kind of gradient descent here. just means I have an internal solver that's fast and I kind of trust, and I have something on the outside that's a latent variable that I'm like splitting up the modeling. It's one of a number of decomposition strategies. Doesn't mean it's the only way to solve it, though. All right, so the question is, how do we construct L of t? And I claim we know everything else. So we'll come back to that claim in a second. Jensen's is a probability distribution over the states such that the sum over Q(z) equals 1, and Q (z) is greater than or equal to 0. So we're going to shoehorn Jensen's into what we're doing, and there's some motivation, but it's kind of opaque, let's say. So how does it work? So we have this character-- copy-- in here. This can also be written as an expected value, where z is distributed like Q of this weird-looking quantity. It's just the definition of expectation. Each data point is going to get its own different Q, which is the log of how likely this thing is, OK? And we picked those for each i. So because we did this term by term, we can pick that Q-- Q1, Q2, Q3, all different. And we pick them all so they satisfy this equation. So the ELBO of x, Q, z equals maximizing here over all the parameters, phi and mu and sigma, sigmas, sorry, all the covariants. sequence that is monotonically increasing or nondecreasing, OK? So it's possible that it would grind to a halt. But eventually, it has to be strict. And so to derive a counterexample, you would just find a likelihood function that had those two bumps. And what it will do is it will gradually hillclimb. And this is actually not great. Like, it can't go back downhill, right? It's got to just continue to go up. This just says that it's not going to oscillate wildly. entire curve because we wanted to optimize it. So it wasn't enough to find a point in a lower bound. We needed to find the whole thing that was underneath it so we could run our argmax step. And that was the setting where we would learn all of the parameters and estimate that in a way that was hopefully nice and easy to do. It's a lot of notation because we're abstracting out a huge number of things that we're doing. We'll run through an example of that. model, where we were saying, the way we're going to think about the world was to maximize the likelihood. We didn't get to a global optimum. So I don't mean that we definitely guaranteed that we got the maximum likelihood estimation, just that you can phrase what's going on as MLE. And so when you get into other estimation problems and the subproblems, you just apply the MLE stuff you learned from the first half of the class. And we'll see that in an example. This is a Gaussian distribution with center j. The notation doesn't change except for this is what the Gaussian looks like instead of a square. And then there's the phi j, which is just multiplied times this horrible expression. And this exp parentheses is so I don't have to write it in superscript, right? Just expo the function, just a bad habit that I have. Oh, and this whole thing is unfortunately-- snap 2? Yeah-- over w(i)j. phi 2 was hugely bigger than phi 1, right-- a billion points came from the second source. So to automate this, this is Bayes' rule. It just weighs those two probabilities and tells us what should happen. If you haven't seen this before, this will trip you up in some way. You need a Lagrangian, OK? If you want, I'll post notes about how to compute Lagrangians as well. But the piece is here that it gets you back to an expression which makes sense in this setting. The z(i) notation was still very abstract-- wj(i), which was summing over this part here, log-- and help us all, 1 over 2 pi-- this is a covariance, 1/2. Oh, I decided to write this in four general things. OK, I see why. Transpose sigma inverse x( i) mu j times phi j. On a whiteboard, that's really catastrophic-- phi J, OK? Let me make sure the brackets are clear. so if you just take this and compute the derivative, it doesn't account for the constraint. So you have a bunch of numbers that must sum to 1. If you've never seen this before, I just want to flag for you, when you minimize a function that's constrained to make sure you use Lagrange multipliers. I will put up a little tutorial about them. You do not need to spend a lot of time on them. It's just if you see some challenge where you're actually supposed to do it, just have a little light bulb to go off.

ROUGE-1: 22.92, ROUGE-2: 22.03, ROUGE-L: 19.81
BERTScore: 63.12

==============================================
