This week in week three, we're actually going to have some human language, and so this lecture has no partial derivative signs in it. The other thing that happens in assignment three is that, we start using a deep learning framework PyTorch. And so we hope that you can put together what you learned about neural networks last week and the content of today, and jump straight right in to building a neural dependency parsers. And since I missed my office hours yesterday, I'm gonna have a shortened office hour tomorrow from 1:00 to 2:20. Linguists have thought about the structure of sentences in two ways. One is called phrase structure, or phrase structure grammars. The other is called context-free grammar. The idea of phrase structure is to say that sentences are built out of units that progressively nest. So, we start off with words that, cat, cuddly, et cetera, and then we're gonna put them into bigger units that we call phrases. And then you can keep on combining those up into even bigger phrases, like, "The cuddlely cat by the door" In this system of dependencies I'm going to show you, we've got in as kind of, um, a modifier of crate in the large crate. And well, then we have this next bit by the door. And as I'll discuss in a minute, well, what does the by thedoor modifying? It's still modifying the crate, it saying, ''It's the crate by the doors.'' Okay. So, the structure you get may be drawn a little bit more neatly when I did that in advance like this. to your friends is that you just blab of something, and I understand what you're saying, and, um, what goes on beyond that, is sort of not really accessible to consciousness. But well, to be able to have machines that interpret language correctly, we sort of need to understand the structure of these sentences. Unless we know what words are arguments and modifiers of other words, we can't actually work out what sentences mean. And I'll show some examples of that as to how things go wrong immediately, because a lot of the time there are different possible interpretations you can have. "What can go wrong?" is a way of saying, ''What can't go wrong?'' Okay. So here, is a newspaper article. Uh, ''San Jose cop kills man with knife''. Um, now, this has two meanings and the two meanings, um, depend on, well, what you decide depends on what, you know, what modifies what? "The cop stabs the guy. The second meaning the sentence can have is, that's the man has a knife" what is modifying what? Um, here is another one that's just like that one. Um, scientists count whales from space. [LAUGHTER] Okay. So again, this sentence has two possible structures, right? That we have, the scientists are the subject that are counting and the whales are the object. So, obviously what you want is this one is correct and thisOne is here wrong. So this choice is referred to as a prepositional phrase attachment ambiguity, and it's one of the most common ambiguities in the parsing of English. if that's not what you want, um, you have to use parentheses or indentation or something like that. If we think of something like C or a similar language, it's just deterministically, the else goes with the closest if. That's not how human languages are. Human languages are this prepositional phrase can go with anything proceeding, and the hearer is assumed to be smart enough to work out the right one. If you want to have artificial intelligence and smart computers, we then start to need to build language understanding devices who can also work on that basis. acquisition by Royal Trustco Limited of Toronto for $0.27, $27 a share at its monthly meeting. Boring sentence, but, um, what is the structure of this sentence? Well, you know, we've got a verb here, and we'veGot exactly the same subject, and for this noun,Um, object coming after it. But then what happens after that? well, here, we have a prepositional phrase. You've just got a see fourPrepositional phrases in a row. And so, well, what we wanna do is say for each of these prepositions what they modify. For $27 a share is modifying acquisition, right? [NOISE] So now, we leap right back. Now, is now the acquisition that's being modified? And then finally, we have at its monthly meeting is modifying? [Noise] Approved. Well, the approved, right. It's approved, yeah. Okay. [NOise] I drew that one the wrong way around with the arrow. Sorry, it should have been done this way. I'm getting my arrows wrong. Um, um. of places when they're tree-like contexts. So, if any of you are doing or have done CS228, where you see, um, triangular- triangulation of, ah, probabilistic graphical models and you ask how many triangulations there are, that's sort of like making a tree over your variables. And that's, again, gives you the number of them as the Catalan series. But- so the point is, we ha- end up with a lot of ambiguities. In English, you can use kind of just comma of sort of list intonation to effectively act as if it was an "And" or an "Or", right? So, here, um, we have again two possibilities that either we have issues and the dep- and the dependencies of issues is that there are no issues. Um, and then it's sort of like no heart or cognitive issues. So, "Heart" has a depend- has a coordinated dependency of "Issues". That's one one. "Mutilated body washes up on Rio beach to be used for Olympics beach volleyball." Um, wha- what are the two ambigui- What are the. two readings that you can get for this one? [NOISE] We've got this big phrase that I want to try and put a structure of to be use for Olympic beach volleyball, um, and then, you know, this is sort of like a prepositional phrase attachment ambiguity. Um, and you have it's the first experience and it goes like this. that, um, we can have here is another noun phrase muti- mutilated body, and it's the mutilatedBody that's going to be used. Um, and so then this would be, uh, a noun phrase modifier [NOISE] of that. Okay. So, you know, this is back to the kind of boring stuff that we often work with of reading through biomedical research articles and trying to extract facts about protein-protein interactions from them or something like that. But it's, it's sort of, okay, youknow, I was using funny examples for the obvious reason, but, you Know,This is sort of essential to all the things that we'd like to get out of language most of the time. can kind of think of these two things as sort of patterns and dependencies that we could look for to find examples of, um, just protein-protein interactions that appear in biomedical text. Okay. So, Dependency Grammar postulates the what is syntactic structure is is that you have relations between lexical items that are sort of binary asymmetric relations. We have a system of dependency labels. All we're doing is making use of the arrows. And for the arrows, you should be able to interpret things like prepositional phrases as to what they're modifying. In the later parts of the first millennium, there was a ton of work by Arabic grammarians and essentially what they used is also kind of basically a Dependency Grammar. There was this guy Wells in 1947 who first proposed this idea of having these constituents and phrase structure grammars. And where it then became really famous is through the work of Chomsky, which love him or hate him is by far the most famous, um, linguist and also variously contributed to Computer Science. Computational linguistics. Some of the earliest parsing work in US Computational Linguistics was dependency grammars. Universal Dependencies is project I've been strongly involved with. The goal of universal dependencies was to say what we'd like to do is have a uniform parallel system of dependency description which could be used for any human language. If you have a- a big calling to say I'm gonna build a Swahili Universal Dependency um, treebank, um, you can get in touch. We want models that can kind of capture what's the right parse. For each word we want to choose what is the dependent of. We want to do it in such a way that the dependencies form a tree. Most dependencies are fairly short distance. They not all of them are. So, if we sort of said, Bootstrapping, um, was a dependent of talk, but then we had things sort of move around, this goes to here, and so I'm gonna cycle that's bad news. We don't want cycles, we want a tree, and this example here is actually an instance. is a transition based parser. It was mainly popularized by this guy, walk him Joakim Nivre, he is a Swedish computational linguists. Um, and what you do it's- it's sort of inspired by shift-reduce parsing. And this is sort of like a shift- reduce parser, apart from when we reduce, we build dependencies instead of constituent. So, what I wanna to do is parse the sentence "I ate fish". And yet formally what I have is I have a why I start, there are three actions I can take. Joakim Nivre showed that you could predict the correct action to take with high accuracy. In the simplest version of this, there's absolutely no search. You just run a classifier at each step and it says "What you should do next is shift" and you shift. He proved, no, he showed empirically, that even doing that, you could parse sentences withhigh accuracy. Now if you wanna do some searching around, you can do a bit better, but it's not necessary. And in the last bit of lecture I want to show you what people have done in the, um, neural dependency world. Using a neural network to make the decisions of Joakim Nivre Style shift-reduce parser, we could produce something that was almost as accurate as the very best parsers available at that time. The secret was we're gonna make use of distributed representations like we've already seen for words. So for each word, we're going to represent it as a word embedding, like we're all what already seen. And in particular, we are gonna use word vectors as the represent- the starting representations of words in our Parser. more to do with each other than others. So maybe we could have distributed representations, a part of speech that represent their similarity. And so, we built a representation that did that. Now for- so, you know starting from- starting from the next lecture forward, we're gonna sort of s- start using a more complex forms of neural models. But for this model, um, we did it in a sort of a very simple straightforward way. We said, well, we could just use exactly the same model. Our neural network is just a very simple classifier of the kind that we are talking about last week. So based on the configuration, we create an input layer. We do Wx plus b and then put it through a ReLU or a non-linearity to a hidden layer. And then on top of that, we're simply gonna stick a softmax output layer. So multiplying by another matrix, adding another, um, bias term, and then that goes into the softmax which is gonna give a probability over our actions. right action. By having it use these dense representations, it meant that we could get greater accuracy and speed than Nivre's parser at the same time. Google said, "Well, maybe we can get the numbers even better if we make our neural network, um, bigger and deeper and we spend a lot more time tuning our hyper-parameters" All of these things help when you're building neural networks. Sometimes the answer to making the results better is to make it bigger, deeper and spend more time choosing the hyper- parameters. There's still room to do better. I mean, at the unlabeled attachment score, it's actually starting to get pretty good. But there's stillRoom to doBetter. Um, yeah. So Beam search, the final thing that they did was- that we're not gonna talk about here, is the sort of more global inference to make sure it's sensible. [inaudible] [OVERLAPPING] [NOISE] So that's a good question which I haven't addressed. um, humans don't always agree. Google developed these models that they gave silly names to, especially the Parsey McPa- parseFace, um, model of parsing. So that then- that's sort of pushed up the numbers even further so that they were sort of getting close to 95 percent unlabeled accuracy score from these models. And actually, this work has kind of, you know, deep learning people like to optimize. So we- we're still going on but I think I'd better stop here today.