Machine learning is about how to acquire a model and acquire the parameters of a model, from data and experience. In the next few lectures, we're going to work through a sequence of different takes on machine learning that are going to highlight different subsets of the big ideas on this topic. We'll see today exactly how data kind of goes through the mill and gets turned into a model. And we'll see more in-depth examples and more structured examples of these kinds of problems later on when we talk about applications. Machine learning can be used to predict labels of new images that are not the ones we've already seen. It's increasingly the case that we feed in low-level features like pixels and higher level features like edges. We'll talk about that in a couple weeks when we talk about narrow nets. There's also tons of applications of machine learning like clustering, or classes, y, inputs called x, outputs are called y, and there's tons of this. The input is the image, the output is the classifications. some account activity and you want to red flag accounts that are suspicious. Automatic essay grading, auto grading, this can be a machine learning problem. Customer service email routing. You'd like to automate the routing of that. Review sentiment. Here's a bunch of reviews of my product. Which ones are good and which ones are bad? Have they gotten better in the past 10 days since the new announcement? And so on. You can do that with classification. You gotta do that before you can do things like translation. In model-based classification, rather than directly learning from errors that you make in the world from experience, think like reinforcement learning model-free. The Naive Bayes assumption is an extremely radical assumption as far as probabilistic models go, but for classification, it turns out to be really effective and it goes something like this. You build a model where the output label and the input features are random variables. There's going to be some connections between them, and maybe some other variables too that might help you build a better model. Machine learning theory is based on trying to say something precise about the connection between what's going on in your data and this future used to which you're going to put the classifier to. The main worry is that in picking the parameters of your model, you do a really good job of capturing that training data, but it doesn't generalize. This is like you download all the exams from past years, and you optimize. You learn all those answers, and then you go to the final exam and it's totally, totally different questions that look nothing like those. You go through an experimentation cycle, it looks something like this. Get your model and learning already, and then you're going to learn the parameters. parameters are things like what's the probability of pixel 73 for the number eight? Then there's hyper-parameters, like, do I want to have features for the lowercased version of the words in case I've seen the word, but never uppercased? Right? These are questions about, is this or this orthis going to work better? You don't want to test your classifiers on the data that was used to train them, because they will do surprisingly well. There can be a slow leak of your test data into your training data if you're not careful. So you try not to peek at the test set, and that's another reason why we have held-out data, which gives you something you can peek at. To see that today, go to CNN.com/soulmatestories and click here. cost-- of different kinds of mistakes may not be the same. And so accuracy isn't always what you want. What you really want, is you want a utility here. You want to know what was my utility, and you should have different costs for these things. There are also cases like machine translation, where you're always going to be a little bit off, a little word here or there, but there's a difference between being completely off and a tiny bit off. And again, we're going talk a lot today and next time about over-fitting and generalization. Spam detection is, in some ways, a very poor example of a canonical classification problem. If you're trying to build a classifier to detect the number 7, the number7 is not like trying to squirm its way out from those pixels to avoid detection, right? You can just get better at the task. And if you get perfect at thetask, great. But you don't want to under-fit either, because then you're going to fail to learn the information that's actually in your training data. In Naive Bayes probabilistic models, over-fitting usually shows up as zeros in your probability table. For other methods, it's actually going to show up in totally other ways. We need to smooth or regularize our estimates, and we could take that polynomial and limit the degree of the polynomials. We already know one kind of over-fits to limit that, so let's just look like what it would look like to just illustrate it. you shrink a hypothesis space, you fit less. Using it too much, you under-fit. So let's take a look at the distribution of a random variable, just to sort of show why we need to do these kinds of things. We can do elicitation, right? You can ask a human. You can go to a doctor and say, hey, I'm building a classifier. What fraction of people with meningitis will present with a fever? And a doctor can give you a guess. You could also do that empirically. The maximum likelihood estimate, or relative frequency estimate, is a way to estimate the likelihood of an outcome. The more samples you draw, the more accurate your estimate will be. But in practice, you need some smoothing to prevent things like zeros in these estimates. This is actually due to a philosopher who kind of worried about things like things like how do I estimate the probability the sun will rise in the morning? Every morning it's risen, so far so far. Every morning the sun's risen. So I need some way of incorporating that into my estimate. example, I can go into my spam, and instead of computing odds ratios on the maximum likelihood-- or empirical relative frequency estimates-- I can instead do some smoothing. And suddenly things that only occurred once, they don't percolate to the top, because they haven't occurred enough to overwhelm that flat prior that I'm associating them with. So this is the top of the odds ratios for ham on the left, and favoring spam on the right. Some of these maybe make sense. Like, there it is. Free is probably in there somewhere. If you see money, that's a good sign that it's spam. In general, your model is going to make errors. The k that's going to be most accurate on my training data is zero. That's the maximum likelihood estimate. We learn our parameters from the training data. We tune them on some different data, like some held-out data, because otherwise, you'll get crazy results. And then eventually, you're going to take the best value, do some final tests, test run. We're talking a bit more about features, because it's important for when we start to get to neural nets. In general, you're going to need more features. In spam classification, we found out that it wasn't enough to just look at words, you've got to look at other sort of metadata from the ecosystem. For digit recognition, you do sort of more advanced things than just looking at pixels, where you look at things like edges and loops and things like that. You can add these as sources of information by just adding variables into your Naive Bayes model, but we'll also talk in the next few classes about ways to add these more flexibly.