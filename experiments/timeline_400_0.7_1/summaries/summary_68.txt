Today we're gonna talk about learning in the setting of games. What does learning mean? How do we learn those evaluation functions that we talked about? And then, er, towards the end of the lecture, we wanna talk a little [NOISE] bit about variations of the game- the games we have talked about. So, uh, how about if you have- how about the cases where we have simultaneous games or non-zero-sum games. And an example of that is rock, paper, scissors. So can you still be optimal if you reveal your strategy? It's actually not the size that matters. It's the type of strategy that you play that matters, so just to give you an idea. So, so the question is more of a motivating thing. We'll talk about this in a lot of details towards the end of the class. But, like, the reason that we have put this I guess at, at the beginning of the lecture is intuitively when you think about this, you might say, "No. I'm not gonna tell you what my strategy is, right?" at the utility, er, so if you're an- at an end state, we are gonna get utility of S, right? Like if you get to the end of the game, we get the utility. And so that was the recurrence we started with, and, and we looked at games that were kind of large like the game of chess. So, so instead of the usual recurrence, we decided to add this D here, um, this D right here which is the depth that un- until which we are exploring. And then when depth is equal to 0, we just call an evaluation function. In the game of chase- che- and chess example is, you have this evaluation function that can depend on the number of pieces you have, the mobility of your pieces. Maybe the safety of your king, central control, all these various things that you might care about. So, so the hand- like you can actually hand-design these things and, and write down these weights about how much you care about these features.and figure out what is a good evaluation function. So to do that, I can write my evaluation function, eval of S, as, as this V as a function of state parameterized by weights Ws. In the first part of the lecture, we're going to look at backgammon. And then towards the end of the class, we will talk about simultaneous games and non-zero-sum games. And, and that kind of introduces to this, this, um, temporal difference learning which we're gonna discuss in a second. It's very similar to Q-learning. Okay. So let's think about an example and I'm going to focus on the linear classifier way of looking at this just for simplicity. can actually, like, roll two dice and based on the outcome of your dice, you move your pieces various, various amounts to, to various columns. Uh, there are a bunch of rules. So your goal is to get all your pieces off the board. But if you have only one piece and your opponent gets on top of you, they can push you to the bar and you have to start again. So, so what are some features that you think might be useful? Remember the learning lecture? How did we come up with feature templates? Yes. These indicator functions. You might ask number of O's on the bar that's equal to 1, fraction of Os that are removed. So, so we have a bunch of features. These features, kind of, explain what the sport looks like or how good this board is. And what we wanna do is we wanna figure out what, what are the weights that we should put for each one of these features and how much we should care about, uh, each one, these features. "We generate episodes and then from these episodes, we want to learn. These episodes look like state action reward states and then they keep going until we get a full episode" "The reward is going to be 0 throughout the episode until the very end of- end of the game. Until we end the episode and we might get some reward at that point or we might not" "We go over them to make things better and better. So that's, kind of, the key idea" So s, take an action, you get a reward. You go to some s prime from that and you have some prediction. Your prediction is your current, like, your current V function. And then we had a target that you're trying to get to. And my target, which is kind- kind of acts as a label, is going to be equal to my reward, the reward that I'm getting. So I'm gonna treat my target as just like a value, I'm not writing it as a function of w, okay? The TD learning algorithm is based on gradient descent. It is very similar to Q learning. There are very minor differences that you'll talk about actually at the end of this section, comparing it to Qlearning. So, so I wanna go over an example, it's kind of like a tedious example but I think it helps going over that and kind of seeing why it works. Especially in the case that the reward is just equal to 0 like throughout an episode. So it kinda feels funny to use this algorithm and make it work but it work. Is it possible to have, an end state and not end state have the same feature vector, or no? If you use like, uh, initialize rates do not be zeros which you update throughout instead of just to the end. If there were kind of the same and have same sort of characteristics, it's fine to have feature that gives the same value. If it is always 0, it doesn't matter like what the weight of that entry is. So in general, you wanna have features that are differentiating and you're using it in some way. to 0.25 and 0.75 then it kind of stays there, and you are happy. All right so, so this was just an example of TD learning but this is the update that you have kind of already seen. And then a lot of you have pointed out that this is, this is similar to Q-learning already, right? This is actually pretty similar to update, um, it's very similar, like we have these gradients, and, and the same weight that we have in Q- learning. The idea of learning in games is old. People have been using it. In the case of Backgammon, um, this was around '90s when Tesauro came up with, with an algorithm to solve the game. And then more recently we have been looking at the game of Go. So in 2016, we had AlphaGo, uh, which was using a lot of expert knowledge in addition to ideas from a Monte Carlo tree search and then, in 2017, weHad AlphaGo Zero, which wasn't using even expert knowledge. all, like, based on self-play. Uh, it was using dumb features, neural networks, and then, basically the main idea was using Monte Carlo tree search to try to solve this really challenging difficult problem. So, um, I think in this section we're gonna talk a little bit about AlphaGo Zero too. So if you're attending section I think that will be part of that story. All right so that was learning and, and games. Now, so the setting where we take our games to simultaneous games from turn-based. And then, theSetting where we go from zero-sum to non-zero-sum, okay? So, so we have player A and player B. We have these possible actions of showing 1 or 2. And then, we're gonna use this, this payoff matrix which, which represents A's utility. If A chooses action A and B chooses action B. So, so you can kind of see like a whole mix of strategies happening. And this is a game that you are gonna play and talk about it a bit and think about what would be a good strategy to use when you are solving this game. Someone tells me it's pi A and pi B, I can evaluate it. I can know how good pi A is, from the perspective of agent A. But with the challenge here is we are playing simultaneously, so we can't really use the minimax tree. So what should we do? So I'm going to assume we can play sequentially. So that's what I wanna do for now. So, so I'm gonna focus only on pure strategies. I will just consider a setting- very limited setting and see what happens. If we have pure strategies, all right, going second is better. What if we have mixed strategies? Are we gonna get the same thing? So, so that's the question we're trying to answer. If someone comes in and tells me, "This is a mixed strategy I'm gonna follow," I'll have a solution to that and that solution will be a pure strategy. So that's actually what's happening in this general case. So I'm actually gonna make a lot of generalizations in this lecture. If you are playing a mixed strategy, even if you reveal your best mixed strategy at the beginning, it doesn't matter if you're going first or second. If you're minimizing or are maxim- or maximum or min- minimum of that value, it's going to be the same thing. So this is called the von Neumann's theorem. For every simultaneous two-player zero-sum game, with a finite number of actions, the order of players don't matter. So no matter what your opponent does, like you're gonna get the best thing that you can do. just telling you what's the pure strategy you're using, right? So that was kind of the first point up there. And then if you's using mixed strategies, it turns out it doesn't matter if you're going first or second. You're telling them what your mixed- best mixed strategy is and they're going to respond based on that. Okay? All right. So next 10 minutes, I want to spend a little bit of time talking about non-zero-sum games. In real life, you're kind of somewhere in between that, and, and he wants to motivate that by an example.