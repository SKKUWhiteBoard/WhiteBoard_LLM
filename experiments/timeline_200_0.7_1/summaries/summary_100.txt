In this problem, we're going to be dealing with a variation of the usual coin-flipping problem. But in this case, the bias itself of the coin is going toBe random. And we're told that the expectation of this bias is mu and that the variance of the bias is some sigma squared. So how do we go about calculating what this is? Well, the problem gives us a int. It tells us to try using the law of iterated expectations, but in order to use it, you need to figure out what you need the condition on. The problem also defines the random variable x. X is the total number of heads within the n tosses. Or you can think of it as a sum of all these individual xi Bernoulli random variables. And now, remember that we're flipping the same coin. And so each of these expectations of xi should be the same, no matter what xi is. And each one of them is mu. We already calculated that earlier. And there's 10 of them, so the answer would be n times mu. different scenarios, one where i and j are different indices, different tosses. So we have to consider both of these cases separately. Let's first do the case where x and i are different. So i does not equal j. In this case, we can just apply the formula that we talked about in the beginning. So this covariance is just equal to the expectation of xi times xj. All right, well, how can we simplify this inner-conditional expectation? Well, we could use the fact that the problem tells us that, conditioned on Q, the tosses are independent. we're told is sigma squared. All right, so what we found is that for i not equal to j, the coherence of xi and xj is exactly equal to sigma. And so, because they're correlated, they can't be independent. The second case is when i actually does equal j. And in that case, well, the covariance of x i and xi is just another way of writing the variance of Xi. And again, we know what the second term is. part A is just mu, right? So that's just second term is justmu squared. But what is the expectation of xi squared? Well, we can think about this a little bit more. And you can realize that x i squared is actually exactly the same thing as just xi. And this is just a special case because xi is a Bernoulli random variable. So the answer isjust mu minus mu squared. OK, so this completes part B. And the answer that we wanted was that xi and xj are in fact not independent. Right. of xi is mu. And we also want to remember what this covariance is. The covariance of xi and xj is equal to sigma squared when i does not equal j. So we'll be using these facts again later. And the variance of x i is equal  to mu minus mu squared. So now let's move on to the last part, part C, which asks us to calculate the variance of x in two different ways. So the law of total variance will tell us that we can write the variety of x as a sum of two different parts. of these Bernoulli random variables. And now what we'll do was, well, use the important fact that the x's, we're told, are conditionally independent, conditional on Q. And in fact, all these are the same, right? So we just have n copies of the variance of, say, x1 given Q. Now, what is the Variance of x1given Q? Well, x 1 is just a Bernoully random variable. But the difference is that for x, we don't know what the bias or what the Q is. Because it's some random bias Q. were i is not equal to j. Well, there are actually n squared minus n of them. So we've use two different methods to calculate the variance, one using this summation and one using the law of total variance. All right, and now if we compare these two, we'll see that they are proportionally exactly the same. So what do we learn from this problem? Well, we saw that first of all, in order to find some expectations, it's very useful to use law of iterated expectations. But the trick is to figure out what you should condition on. An art that you learn through more practice. But one good rule of thumb is, when you have kind of a hierarchy or layers of randomness, it's useful to condition on the layer above where that is, in this case, the random bias of the coin itself. Once you condition on that layer above, that makes the next level much simpler. Because you kind of assume that you know what all the previous levels ofrandomness are, and that helps you calculate what the expectation for this current level.