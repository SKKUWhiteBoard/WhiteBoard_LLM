==================== [1/100] ====================
Summary:
John ESSIGMANN: I measure my blood sugar at different times during the day. Gluconeogenesis technically means new synthesis of glucose from non-carbohydrate precursors. The medicine I take is called Metformin. It has a number of targets, but one of them is one of the enzymes, called PEPCK, Pyruvate Carboxykinase, that's in the gluconeogenic pathway. The liver provides a constant stream of glucose to organs that absolutely require it, like our brain.

ROUGE-1: 28.97, ROUGE-2: 26.69, ROUGE-L: 28.97
BERTScore: 70.94

==============================================
==================== [2/100] ====================
Summary:
In this lecture, we introduce and develop the concept of independence between events. If I tell you that a certain event A has occurred, this will generally change the probability of some other event B. In such a case, we say that events A and B are independent. We will then proceed to define the independence of a collection of more than two events. Finally, we will close with an application in reliability analysis and with a nice puzzle that will serve as a word of caution about putting together such a collection. probabilistic models. Probabilism models are models based on a set of assumptions about the future of the economy. The models are based on the assumption that the economy will continue to grow in the future. They can be used to make predictions about future growth. For more information, visit: http://www.cnn.com/2013/01/29/30/science/features/top-10-most-futile-probabilities-in-the-world/story/story.html.

ROUGE-1: 63.01, ROUGE-2: 50.14, ROUGE-L: 50.41
BERTScore: 70.32

==============================================
==================== [3/100] ====================
Summary:
In today's video i'll show you the importance of de-gassing your bread dough as it's fermenting. No matter how gentle you try to handle your dough you will always de-gas it if only a little bit in today's comparison video we'll make 4 breads they will be made from the same dough but they will all be treated differently. The first one of the four breads will be left alone from the beginning of fermentation until it's baked. The final one will be folded shaped and degassed three times and we won't be fermenting them for the same amount of time. why are you punching it punching is actually quite aggressive and you should never punch your dough the best thing to do is deflate it by pressing it gently as the dough ferments the gas pockets inside the grow larger and larger and the membrane of dough between those pockets can tear and the pockets of gas can fuse into larger pockets. If this process keeps going undisturbed the crumb of the bread can end up with large bubbles surrounded by denser areas of dough this of course not always a bad thing some high hydration breads are specifically made to have that texture as we punch down or deflate the dough the Gas pockets break down and split up resulting in a more tightly packed and even chrome structure. proof i tried to shape them similarly as i could so that the main difference between these breads would be the steps that we took or skipped now degassing folding and shaping is not just about what the crumb will be like it's also a way of controlling fermentation the loft on the left is almost ready to be baked while the other three they still need about an hour until they go in the oven as we know fermentation builds flavor and it helps develop texture of the crumbs and the crust by degassing the dough we are forcing it to rise back up again it basically has to start over.

ROUGE-1: 32.37, ROUGE-2: 31.94, ROUGE-L: 32.37
BERTScore: 67.94

==============================================
==================== [4/100] ====================
Summary:
Of the nearly 11,000 amendments proposed in the centuries since, only 27 have succeeded as of 2016. The founders of the United States were trying to create a unified country from thirteen different colonies. For an amendment to even be proposed, it must receive a two-thirds vote of approval in both houses of Congress. To actually change the Constitution, the amendment must be ratified by three-quarters of all states. The result of such high thresholds is that, today, the American Constitution is quite static.

ROUGE-1: 24.96, ROUGE-2: 23.60, ROUGE-L: 24.96
BERTScore: 67.92

==============================================
==================== [5/100] ====================
Summary:
50 years ago, John McCarthy and Marvin Minsky coined the term artificial intelligence. Progress has been made, especially in the last 20 years. But we need different expertises. Not all in computer science, but in other ones. And so, this was the people that we put together from different labs, from neuroscience, from computer science,. from cognitive science, and from a number of institutions in the US. Especially MIT and Harvard. Let me tell you a bit more about the background here. This idea of merging brain research and computer science. We are still very far from understanding how people can answer questions about images. We want to have a system that does it in the same way as our brain does it. And we want to compare your model, our system, with measurements on the brain of people, or monkeys, also during the same task. So that's what we call Turing plus, plus questions. I think it's a golden age for intelligent applications. If people want to make a lot of money with useful things, that's the time. able to answer how people do understand images. We start with vision. We are not limited, eventually, to vision. But in the first five years of the center, that's the main focus. And answer the question about images. And we want to understand how the answers are produced by our brain at the computational, psychophysical, and neural level. It's ambitious. And I think there are probably, in terms of having all these different levels, levels of really understanding from the what, where, the neuroscience, to the behavior.

ROUGE-1: 36.85, ROUGE-2: 35.53, ROUGE-L: 34.81
BERTScore: 65.21

==============================================
==================== [6/100] ====================
Summary:
Chef Todd Moore shares with you the seven basic skills that I think everyone should have to cook food consistently in the kitchen and be proud of the results. If you already have all seven of these skills and cooking techniques great you can work for me on the other hand if you only have one or two of these Skills that's still fantastic why because I know you'll want to add other skills and learn to cook basic methods know the methods behind all written recipes then you'll be making your cooking a winner every single time. like to cut things up you'll be using fresh vegetables uh anyway uh second in my chef test to anticipate when oil is about to smoke now the skill here is understanding the convective cooking process. When the chef notices the oil starting to change from being perfectly smooth to beginning a convection process then adds the protein product to the pan just before there's visible smoke this is where you get the splattering reaction in the pan. If you give me a chicken breast with a beautiful brown plate appeal that shows your ability to control heat so that the item develops color but doesn't lose moisture or burn you fail this test. one that's lost moisture one that's burned one that shows the lack of involvement in the preliminary steps of the saut√© process Chef test number four here's the answer thicken a liquid to make a sauce. A chef that can show me that they pass they take a cup of milk they turn it into a thickened sauce that's shiny that's velvety that has no lumps. Without an understanding of how starches thicken it's difficult to make consistently great sauces this is a skill of someone that knows how to cook. every single time because understanding these methods will allow you to make sense out of any recipe or not even use a recipe. Your increased understanding of how different cooking techniques work not what to cook but how the techniques work and then you'll be creating the things that you want you want them to be. Make sure you get a seed at my webinar [Music] and then come back again and again to try and create the things you want to create. You'll be able to create them again andagain and again.

ROUGE-1: 29.45, ROUGE-2: 27.66, ROUGE-L: 27.64
BERTScore: 65.57

==============================================
==================== [7/100] ====================
Summary:
as a nurse you want to be familiar with heart blocks and in this review i'm going to be talking about third degree heart blocks also known as a complete heart block. The reason is occurring is because electrical signal from the atria isn't making it to the ventricles. The person could be born with it so it could be congenital or the person has severe heart disease or they have a myocardial infarction or they're taking some type of medication that they become toxic on like digoxin. block and if you'd like to watch more videos about heart blocks in this series you can access the link in the youtube description below. Click here to read the first video in the series about a heart block. Read the second video to learn more about the heart block and how to block it. Click the third video to see the next video about the block and learn about the different types of block that can be used to block a person's heart. The third video is the final video of the series and is about the blocking of a heart valve.

ROUGE-1: 39.81, ROUGE-2: 29.93, ROUGE-L: 27.96
BERTScore: 64.02

==============================================
==================== [8/100] ====================
Summary:
The final contest, which is due tonight, is to design an agent that plays together with another agent to try to collect food pellets while not getting eaten by ghosts. submissions for that, your last chance to submit are tonight at midnight. And on Thursday in lecture, we'll discuss the results. What else is left? I think there is a project due next week. There is still a section this week. And I think that homework is all wrapped up, but you would still have a self-assessment of your last homework that will be due next. And then there's a final exam the week after that. be mostly on advanced applications. We will not quiz you on these application lectures, on the final exam, or anything like that. It's more meant to give you more perspective rather than extra study materials for the final. So far, I've looked at foundational methods for search, for acting adversarial environments, for learning to do new things, and for dealing with uncertainty, noisy information. We'll look a bit more at AlphaGo, arguably the most prominent result in gameplay in the past few years. For a game like Tic-Tac-Toe, you will find out that you can force a draw, and that means fully solving the game. For Go, this is actually pretty hard to do, and it's even much harder than chess. And why? Let's take a look at chess. It's a 19 by 19 board, so there's 19 times 19 positions to choose from in the first move. And then one less, of course, next move. But the branching factor is enormous. So if you tried to run an exhaustive search through this kind of game tree, it's not going to work. DeepMind's AlphaGo is a computer program that can play the game of Go. It uses a deep neural network to learn how to search for the best moves. It can also learn to predict who is likely to win from a given situation. DeepMind is now working on a system that can learn to play Go without human help. It's called AlphaGo Zero, and it's being developed by DeepMind's Sergey Brin and Yann Leibovitz. It will be unveiled at the World Economic Forum in Davos this week. AlphaGo Zero has no prior knowledge. It starts at a pretty, actually, negative elo rating. Loses all the time initially. It's playing itself, but then gets tested against a set of players. After 21 days, it goes past where AlphaGo Lee Sedol was. And then it was still creeping up after 40 days. Once you reach that level, essentially, there's no further to go, because you solved the game. With reasonable compute power, it traverses the whole tree, even with alpha beta pruning. Berkeley student Woody Hoburg took charge in seeing how far we can get without human experts for some simpler things, not for all the maneuvers here. He was able to have it learn to hover reliably with the only human input being shut it off when it looks like it might start doing something dangerous. We did not push that further to flying those maneuvers. There is some work. If you look at Woody, Woody was shutting things off.to this behavior. Then recently at OpenEye, there's been some work on robots learning to do back flips. it has more time, and if it already has a recovery controller, then you can imagine that. And Claire Tomlin's group here at Berkeley has done some work in that direction, where they have a safe controller and a learned controller. And the learning controller is learning on its own while the safe controller keeps things in check so the helicopter doesn't crash. So what we used there is a model-based reinforcement learning method. So we learned a dynamics models for simulator from data that was collected. To learn a good controller in the simulator, we used something called iterative LQR. is a separate linear feedback controller for each time slice. If there is no wind, you can actually just run the linear feedback control. It will be fine. But if there's some wind gusts that could throw you off, you want to use the value functions and the two second look ahead against those value functions to do the controls. Training a unified policy across the entire space might work. It might take some work, exactly, figuring out how to do it. It could be interesting to revisit that now and see what the current understanding of how to train these networks. In 2015, there was the Doppler Robotics Challenge, which was held in Pomona, just east of Los Angeles. The robot had to, essentially, drive a car or walk, but driving the car was recommended. It turned it's very complex to get a robot to do that. The thing is modeling these situations proved even harder than modeling helicopters, because your sensing needs to understand whether or not you're already making contact, and making contact or not. You can be very close, but not have contact. It's a very subtle thing. The Dartmouth Grand Challenge is the first time autonomous vehicles have been put to the test. Stanley is about to become the first vehicle in history to drive 132 miles by itself. Just five robots remain on the course, and to finish, they must wind through a treacherous mountain pass. The first Grand Challenge, where many faltered within sight of the start, and no robot went beyond seven miles, was held in 2005. It was won by a Berkeley entry. Only motorcycle was allowed in the race. tail of special events that can happen when you're driving. You can measure progress by just demo videos, which is one way, and it gives you some kind of feel for what's going on. But the 2013 video is already very impressive. So another way to measure progress is to see how are these cars doing relative to human drivers. So left and right are the same plots, but the ride is on a log scale so you can see more detail. It's a number of events per 1,000 miles driven. Red there is human fatalities. Then yellow is human injuries. is between 10 and negative 2 and 10 negative 3 per 1,000 miles of human driving. In green is the Google slash [? wave ?] mode disengagement. It's when the driver decides they want to take control because they don't trust the autonomous system right now to avoid an accident. And we see that it's going down how often that needs to happen, but still a bit removed from where humans are at. Where does this data come from? If you test in California, you have to report this data to the DMV. so many decisions. If they're gigantic, use a lot of power. That's a problem. Let's see what we can do to build smaller networks to make decisions. What else did we not cover yet? Personal robotics. I want to spend a little more than two minutes on that, so let's keep that for Thursday. that's it for today. Bye. [SIDE CONVERSATIONS] [Side CONversation] [sideconversation.com: Do you know more about this topic? Email us at jennifer.smith@cnn.com].

ROUGE-1: 19.07, ROUGE-2: 17.75, ROUGE-L: 17.77
BERTScore: 70.74

==============================================
==================== [9/100] ====================
Summary:
So let me say here just the repetition, normal, inferior. Let us say we are talking about price of good 1 has gone up substitution, income these are the effects and overall. When P 1 goes up subs because of substitution effect x 1 will. Come down. And for income also because of income effect it will come down. So, overall it comes down. While inferior goods substitution effect quantity demanded would come down while income effect is Up. Up. Can you give me an example its very very difficult to find Giffen good in real life why? Sir, like if Gucci is a brand and it is. Prices comes down by significant amount. No one could buy a Gucci, it is at that level. only because of it is prices. I am not certain about this statement then when price of these goods would come down then people would not buy what you are saying. No, no if some purse is very expensive like 10 lakh rupees and it symbolizes your status. And other day it comes down to 1 lakh rupee. Then there is no point that the set of people who were consuming that purse that good for 10 lakhRupees to show their status would buy the same purse at 1 lakh Rupees because it will become more common. During Irish famine the potato was Giffen good because people were using most of their income to buy potato because that is what they would consume. So, when the price of potato went up they could not even spare little bit of money for meat and other food products. To get enough calorie; they had to increase the consumption of potato. But the problem is with this the second requirement that income effect is larger than the substitution effect. Typically this is not fulfilled that is why we have very-very hard time getting finding out, figuring out Giffsen good in an economy.

ROUGE-1: 53.74, ROUGE-2: 52.50, ROUGE-L: 53.74
BERTScore: 75.55

==============================================
==================== [10/100] ====================
Summary:
which we are to for next week and here and I'll talk about that actually week from today because it's callay on Monday so it will get behind but if we get any further behind I'll have to make it up sometime. If you don't have that you be copies available upstairs is it c um come to my office okay to dany's office he doesn't have copies of it there that's six six upstairs in the first floor immediately after CL and then on the next Friday I will talk about the parts in the in the inquiry that were assigned and also a that essay of of the oral concept well last time you call. I was attempting to formulate what I call social compact Criterion of the legitimate form of government. There's a distinction between express consent and P consent Express consent incorporates us into society as full member and as a Perpetual or presumably taxic consent does not do this. There's a point while it's a bit vague what the details are from the text that the point for us that's important is that when you read hum's account of the original contract he gives arguments against unlocks View and you have to ask yourself how effective is K's argument against each of these two parts of lock view. The English Constitution was at that time widely regarded this a sort of accepted view on all sides it's a hex Constitution that this problem as I say arose during the exclusion crisis of 1679 and 81. The original part of the second CH was written it believe about 79 and 880 and then other chapters that were were added in 81 83 and two chapters added after. James was Theos James and Harry this room in I think February of 1689 so those parts were add it afterwards but really I men talks about the earlier crisis. means that is there's no legal framework within which that is within the Constitution for settling the conflict there is no way to do it at law. In paragraph 168 he says if there is a conflict of the sort then the people have the right to appeal to Heaven which is a euphemism for for go to war that is Civil War and uh the power then refs to them and they have theright of resistance I'll come for that the moment now I might mention that in our constitution there are devices that tend to prevent this sort of conflict from happening. of Andrew Johnson although didn't past and it tends to make it politically impossible for president to persist over a period of time and a long drawn that conflict of with Congress. Mar's problem is to justify existence of the crown in this kind of situ situation. The source of Lock's constitutional Doctrine the source and thought is a book where at least is belied to be uh a book and many evidences of this uh by man Nam George wson written in 1657 published in 1660 calleda sais. power as a p power it connects up with the idea that political Authority is the right of making laws only for the public good says that the very beginning in three since given all of the constraints on parties to the social compact it can not be given uh for any other purpose now I don't know whether I ought to do this I might Ting out kind of what the form of the social Compact might be suppose we're imagine ourselves to be sort of loocking constitutional lawyers and we want to write a Social contact what are all the Clauses it might contain. changed his mind he thought perhaps it was not all together safe never knew in England the English were regarded in those days as very unstable people. Mark says nothing about the details of of this process that is how does he Enis that this constition power is actually to take place he doesn't give any institutional account of how he supposes actually might be done laon is also vague on this he says something has the idea that it would begin in the county courts and they organized there and would then they presented Des sent to a parliament. L's view was that James had just advocated or meod from bacon and the Parliament finding a bacon Throne had simply found an occupant for it and that act did not imply the parliament was Supreme that is a view that they prepered to maintain. Why did L refuse to alter his view I think we have to assume that he didn't change his mind and he persisted in the doctrine that he formed earlier in 79 and 880 I think partly because his views are more radical than the standard view of his college that would be one reason but also because he felt I think that it was inconsistent. protected it's the importance of this notion now I'm not I want to mention at the end I gu about the last for some minutes um I'm going on to mention a witness in lock view from from our standpoint. Mark is presupposing and taking for granted that only a few people vote relativelyFew people vote to whole certain amount of land or Capital assets that add up to a certain some of money and so the in accurate I don't know the exact size of the scent I me to look it up rather small percentage of it is able to vote on this view.

ROUGE-1: 26.45, ROUGE-2: 25.77, ROUGE-L: 26.31
BERTScore: 68.88

==============================================
==================== [11/100] ====================
Summary:
David KAISER: Today we're going to be talking about the kind of invention or the hybridization of a whole new subfield within physics that now is often called particle cosmology. It studies the smallest units of matter, the fundamental forces and elementary constituents of matter. It asks about what role they might have in shaping, really, the fate of the entire universe. The field is doing pretty well these days by other measures. Its annual budget just within the United States is on the order of $1 billion a year. This is a $1 billion area of study with devoted colleagues all over the world, not just here in the United States. There are on average two new physics papers, two new preprints posted to the central preprint server archive.org every hour of every day just in this subfield. And I find that all the more astonishing since this field literally didn't even exist 45 years ago. So how did this come to be? There's a very compelling story and the story that we will take some time to look at today. Einstein was inspired by some of Mach's writings on this. Mach himself didn't call it Mach's principle. But it was attributed to Mach by Albert Einstein as early as 1918. So from the early days of the study of general relativity, it was often framed as a question. Do we have to think about the distribution of matter throughout the whole universe in order to make sense for why this block slides down this inclined plane at a certain rate? Local inertial effects, are they somehow tied to cosmological distributions? In the 1950s and '60s, a number of high-energy theorists were trying to put together these highly symmetric models to account for things like the nuclear forces. These nuclear forces are somehow mediated by particles exchanging certain kinds of force-carrying particles. There were other instances of that, cousin models, that were getting a lot of attention throughout the 1960s for the other main nuclear force, for what's called the weak force rather than the strong force. That is to say the force that causes things like radioactive decay. self-evidently of short range, unlike gravitation or electromagnetism, which, in principle, can extend arbitrarily long distances. The nuclear forces really exert themselves across nuclear dimensions, very tiny fraction of the size even of a single atom, let alone macroscopic scales. The question of mass turns out to have been on many specialists minds in the 1950s and '60s, but as embedded in quite different-sounding conversations. So let's look a bit more at some of the proposed models of the nuclear force. The Brans-Dicke theory of gravity was published in 1961 by Carl Brans and Robert Dicke. It was an attempt to modify Einstein's theory of general relativity in response to this challenge of Mach's principle. The idea was that instead of having a single constant unit strength of gravity, Newton's so-called constant actually could vary because it was actually a dynamical field. And this was basically to control how much their version would depart from the ordinary version of relativity. In 1979, two separate theorists working independently of each other suggested that the two fields might be literally the same, not just comparable or considering side by side. Alex asks, was it an accident that both parties chose phi? Not really, I see something come up in the chat. And so what's changed over time? That's what we'll pick up on in the next part of the show. Back to the page you came from. Click here for the next episode of The Physics Show. It was not a rule but a pretty widespread convention by that point that a field that had no spin-- so a scalar field-- would often be labeled by the Greek letter phi. The Greek letter psi was often reserved by this point for spin 1/2 particles. So the notation isn't super surprising that it was so similar. But the rest of it that is a new hypothetical state of matter, it pervades all of nature, everything else interacts with it, that's what gives rise to mass. more than just the letter that they chose. There was a lot of what we might have considered similarities. And yet, the two sets of ideas really were treated so separately. By the end of the '70s, things did not look very good for Brans-Dicke gravity experimentally. In fact, Einstein's theory was not highly favored, and that's why everyone else started paying attention on the gravity side of the field. So we might wonder, well, was it changes in data? Did experiments force a new evaluation? evidence, a big fat goose egg nothing in favor of the Higgs-Boson until July of 2012, or if you're extra generous, maybe December of 2011, the first hints experimentally. So it wasn't a new experiment. The main story that's mostly given is actually hearkens to changes in ideas and, in particular, on the particle physics or particle theory side. And these are brilliant and beautiful ideas. These ideas are well worth appreciating. I just don't think of the whole story. asymptotic freedom. And actually, it's the reason why our friend and colleague here at MIT, Frank Wilczek, received the Nobel Prize. So this was work introduced by Frank and his then advisor David Gross, and independently by a different very young grad student at the time, David Politzer. And what they found was that the strength of the strong nuclear force, that QCD force that we talked about quite a bit at the end of last class session, that the force actually decreases with the energy scale. The effects that Frank Wilczek, David Gross, and David Politzer were talking about would be noticeable exponentially higher energies, right? Well beyond anything that can be achieved even today, let alone in the 1970s. So it's suggested that if one could ever get to really super crazy high energies, you might see some very qualitatively different behavior among nuclear particles-- really cool, a very interesting set of ideas. The present-day experiments at the Large Hadron Collider are here. idea from asymptotic freedom. The force strengths look like they might converge. It's not just that these two get stronger with high energy, and this one gets weaker. The idea was that maybe all of these highly symmetry mediated forces that we see as very different at these low energies-- they have very different behaviors and characteristic strengths-- maybe they're all signs of a single force. That would unite these three forces of nature into a single one modulated by a single strength, a single effective charge. That was called GUTs. In the late '60s and early '70s, physicists saw a really dramatic change in the kind of infrastructure for the discipline. The US budget for that subfield fell in half in just four years. The field that had, so to speak, the most to lose or lose during this reversal of fortune was high-energy particle physics. We can ask a bit more that's very coarse-grained, but we can dig in more and look at subfield by subfield. the discipline, including explicitly more focus on gravitation and cosmology. So that means that more and more departments, including very elite trend-setting departments across the country, start rushing to offer new graduate courses in general relativity. Questions on that field, really for the first time in the U.S., start showing up regularly on the general exams for physicists across all fields, all specialties, not just those who wanted to study relativity. And you see a market response as well. You see a flood of new graduate-level textbooks on general relativity on Gravitation and Cosmology-- twice as many published in the 1970s versus the 1960s. vast majority of those came really in the later '70s, in the wake of these pedagogical reforms. So remember that big report comes out in 1972. You start seeing curricular changes as early as '73, '74. By '75, '76, '77, you start seeing, in some sense, the market respond with many textbooks being really rushed into print. Some of these textbooks were basically mimeographed lecture notes. And now there's very, very fancy books published in a more typical way. for more than half of my life, to his great chagrin. He's won a number of awards, gold prizes and that big award-winning fella. But the award that I take most pleasure in is the fact that he won Boston's Messiest Office. So yes, we'll work on that. Let's see. Fisher says, on the chart of interactions with the field strengths, are those groups? Yes. Fisher, thank you. That's right. So I was avoiding the nomenclature, but you're quite right. a continuous unitary symmetry, which is like saying you could rotate the electron field by any continuum amount, and the equations remain unchanged. The photon only has to mop up a relatively simple symmetry, the U1 gauge symmetry. Whereas SU2 was what I was pointing to when I was referring to the weak nuclear force. That's a more complicated symmetry structure. And that's the symmetry group that these force-carrying particles-- the W and the Z particles-- are invented to enforce. Tony Zee and John Wheeler came up with the idea of a field phi pervading all of nature, all of space, with which other matter interacts. The idea was that this local strength of gravity, Newton's constant, would get anchored to a very small value when phi gets stuck at a relatively large value. They suggested maybe it's because it's arising from some broken symmetry, much like the Higgs-Goldstone mechanism, the field is dynamical, but it's getting stuck. Lee Smolin was the other person who independently introduced that broken-symmetric theory in 1979-- same year as Tony Zee. He was actually, from the start, combining the two fields, both in the courses he took and eventually with his advising team for his thesis and for his dissertation itself. So unlike this accident of trading apartments in Paris and reading a few preprints, more and more members of Lee Smolin's generation were going through a training like his, partly by design, in the wake of the National Academies report. does exactly the kinds of things that Smolin and Zee had been doing, trying to unify the Brans-Dicke and Higgs field. And in turn, these new folks, especially people like Mike Turner and Rocky Kolb, went on to become real institution builders in their own right. Not only had they been trained to think carefully at this new interface, they helped really accelerate the trend. So it goes from who even thought of it to who wouldn't even try that? What counts as natural can shift in a pretty short time scale. the hall from Alan's. And by a quirk of the old building 6, we had the same key. A single key would open the whole hallway. Couldn't get rid of it now. And one time, my parents were visiting. And I basically broke into Alan's office. They couldn't believe me when I described what it was like to try to work with this person. So I actually broke into his office to show them the safety violation, fire code violation, horror show that was the den of entropy. So that's a true story. soon, everyone. Soon, everyone will be able to play together again. Soon. Everyone will be playing together.soon. Everyone. Will be. Playing together again soon.soon,everyone. Everyone, playing together againsoon.soon,. everyone. soon, everyone, will be. playing together once again. soon.everyone. will be Playing Together again.soon! everyone. will. be.playing together soon. soon! Everyone. will play together soon! everyone will.be. playing again soon! soon.

ROUGE-1: 25.43, ROUGE-2: 23.74, ROUGE-L: 23.15
BERTScore: 64.24

==============================================
==================== [12/100] ====================
Summary:
Frida Ghitis: Ferdinand Magellan may have been the first person to actually circumnavigate the globe. She says Spain and Portugal had their eyes on the same prize: trade routes to the Spice Islands. When a Portuguese defector claimed that a westward route existed, King Charles made him captain of a Spanish armada, she says. Ghitis says Magellan's legacy lingers: galaxies and space programs named after him, and he was celebrated in Spain. of the "Victoria" sailed into harbor in southern Spain in September 1522.

ROUGE-1: 17.48, ROUGE-2: 13.88, ROUGE-L: 12.94
BERTScore: 60.28

==============================================
==================== [13/100] ====================
Summary:
Ani was a real person, a scribe from the Egyptian city of Thebes who lived in the 13th century BCE. His Book of the Dead was a 78-foot papyrus scroll designed to help him attain immortality. Ani's epic journey begins with his death. His body is mummified by a team of priests who remove every organ except the heart, the seat of emotion, memory, and intelligence. It's then stuffed with a salt called natron and wrapped in resin-soaked linen. The wrappings are woven with charms for protection and topped with a heart scarab amulet. can imagine him happily tending his crops for all eternity. Can't imagine him being happier than when he was tending to his crops. Can imagine him growing his crops all day and all night. Couldn't imagine a better way to spend the rest of his life than in the fields. Can picture him growing crops all night and all day long. Could imagine him tending his crop all day, all night, all day. Could he be happier than he was right now? Could he ever be happier?

ROUGE-1: 34.15, ROUGE-2: 25.37, ROUGE-L: 29.48
BERTScore: 63.27

==============================================
==================== [14/100] ====================
Summary:
Judy Hoyt: We're going to discuss the accurate control and placement of active dopant regions through a process called dopant diffusion. The placement of those regions determines many of the so-called short channel characteristics of MOSFETs that we'll talk about. The doping of other materials, not just the silicon itself, but of the polysilicon gate affects things like gate depletion and limits how well the gate voltage controls the channel potential, Hoyt says. The equation numerically, essentially, for an arbitrary doping profile is 1 over the integral of the carrier concentration minus the background concentration. There's a fundamental physical limit on how much dopant we can put in the silicon and how much it will be electrically active. We need to find new ways to activate dopants to higher levels if we're going to be able to manage this design tradeoff. Being able to scale the device really amounts to, in the front-end processing to a large extent, to being able to control very precisely the shape of the doping profiles where the dopants end up. And what I'm going to spend some time in the next few slides is giving you examples from the present literature on device scaling. detailed device physics, but it's just to give you a flavor for why studying dopant diffusion is such an important topic. So let's go on to slide number 8 and talk about a topic called the short-channel effect. This basically takes place when the distance between the source and drain-- that is the channel length L-- becomes comparable to the MOS depletion width in the vertical direction. And then that the source-drain potentials themselves from theSource and drain regions end up having a strong effect on the control of the current in the device. refer to different lateral source-drain gradients. For lateral gradients larger than about 4 nanometers per decade, the Vt roll-off is just too large. You wouldn't be able to make a 25 nanometer MOSFET-- so, again, illustrates the importance of controlling the lateral doping profile and of controlling diffusion processes themselves. So given that brief introduction to the electrical effects, let me go on now on slide number 15 and talk about dopant diffusion fundamentals. In silicon IC processing, there are two different steps that we refer to in diffusion historically. Dopant diffusion is described by Fick's first law, which describes how the flux or the flow of dopant depends upon the doping gradient. When the concentration gradient goes to 0, essentially, the dopant or the atoms are uniformly distributed, say, in the solid, and the flow would stop. Later on, we'll talk about the more atomistic diffusion mechanisms and effects of dopants in the silicon lattice. We're going to consider macroscopic first-- macroscopy models for diffusion. given by that constant dose Q divided by 2 times the square root of pi Dt times the exponential of minus x squared over 4 Dt. So that's what's known as a Gaussian profile. And the important consequence of this are that one, of course, the dose Q remains constant. That means then that the peak concentration-- so the concentration at the origin-- is going to decrease according to the squareroot of Dt over time. So the peak Concentration goes down and the width of the profile or the diffusion distance from the origin increases. In semiconductor processing, linear scales for dopants are not all that useful. So we often care about how the dopant falls off over many, many orders of magnitude of concentration. In the second case, which is a fixed dose Q, just like we talk about, constant in time, but now we're diffusing near a surface. And the third case, essentially, that we can solve analytically is called the case of an infinite source. And what this is essentially is a infinite source of dopant which is made up of small slices, essentially each diffusing as a Gaussian. along the x-axis. That exponential squared over 4 Dt. So we're summing up all these Gaussians at the bottom of slide 30. The solution which satisfies Fick's second law is written down at the top of slide 31. The concentration is actually equal to concentration C prime over 2 times the quantity in square brackets 1 minus the error function of the argument x. And we can write this as C sub s times the complementaryerror function of x over2 times the square root of Dt, where the second equation and third gives you the definition of what we mean by the errorfunction. In the predeposition case, at the surface of the silicon wafer, the concentration of the dopant is fixed at the solid solubility. The longer you would do the pre-dep, the more dose you would deposit into this silicon surface. Let's go on to slide number 35 and talk a little bit about dopant diffusion coefficients themselves. And we'll talk about some interesting Fermi level effects that come into play as the dopants become modulated by the carrier concentrations of the species. of cases where there are analytic solutions. We talked about the diffusion of a Gaussian profile with a fixed dose. We apply this diffusion to a constant surface concentration. And finally -- we talk about the¬†diffusion¬†of a complementary error function, which we apply for a constant level of surface¬†concentration. The diffusion of this error function can be applied to a fixed level of¬†surface concentration, or a constant¬†level¬†of¬†substance¬†on the surface, for example.

ROUGE-1: 18.01, ROUGE-2: 17.03, ROUGE-L: 16.79
BERTScore: 68.59

==============================================
==================== [15/100] ====================
Summary:
Atas model shows what happens if aggregate demand increases and firms respond to this by saying we want to make more output. As people demand these higher wages shortening our supply curve decreases basically all the way back to essentially where it was before. The price of tuition to Missouri State goes down but my wage is fixed under contract right and so they're gonna have to pay me the Saints of tuition declines and my input prices stay the same that's bad for them and bad for all of the firm's too. it's gonna be trust me I've been doing this for 20 years questions on the AAAS wanna yes so that's what we're going to talk about when we talk about fiscal policy and monetary policy. Changes in taxes and changes in government spending are also pretty obvious right if your taxes go up and the government doesn't give you more goods and services for that you have less stuff. If taxes go down and thegovernment doesn't change the amount of services that they give then you have more money in. your pocket right so you can change people's wealth by changing their taxes or by changing the services that they're getting in exchange. Keynes comes along and says famously and the long run we're all dead in other words yes eventually we'll get back to a but why wait why not go ahead and do something change aggregate demand and get you here to see because there's really no difference between a and seeing anyway other than the number of green pieces of paper it takes to buy the goods and services.

ROUGE-1: 13.38, ROUGE-2: 12.92, ROUGE-L: 13.38
BERTScore: 59.65

==============================================
==================== [16/100] ====================
Summary:
As a nurse you play an important role in teaching the parents about car seat safety and this education actually starts at birth before the child even goes home from the hospital in their first car ride. In this lecture we're going to concentrate on the main concepts that you need to know as a nurse and for exams first let's talk about the four types of car safety restraints that you can use in a motor vehicle. The back seat of the car is actually the safest place for a child 12 and under.

ROUGE-1: 7.06, ROUGE-2: 7.00, ROUGE-L: 7.06
BERTScore: 64.70

==============================================
==================== [17/100] ====================
Summary:
Machine learning is about how to acquire a model and acquire the parameters of a model, from data and experience. In the next few lectures, we're going to work through a sequence of different takes on machine learning that are going to highlight different subsets of the big ideas on this topic. We'll see today exactly how data kind of goes through the mill and gets turned into a model. And we'll see more in-depth examples and more structured examples of these kinds of problems later on when we talk about applications. The boundary between what is actually spam, unsolicited commercial email, and emails you just don't want can be a fuzzy boundary. Some people just click on an email they wish people hadn't sent them, even if it's like from their mom. Machine learning is going to do some amount of work, but something has to power this. There has to be something about those first emails that's going to give you the clues that something's fishy here. And so, what kinds of features can you include? Defining these features is a big part of deploying machine learning systems. Machine learning can be used to make predictions about spam or ham. The training set for machine learning is very noisy, so it can be hard to label. Machine learning is not perfect, and some inputs are just really, really hard, and they're going to look like this and we're just not all even going to agree on what that's supposed to be. The goal is to be able to predict labels of new images that are not the ones we've already seen, OK? So that's actually subtle, but it's super-important. representations that if the thing gets tilted or it's a little bit lighter. It's not the exact pixels being on that we care about. But the pixels are something we could use. We could look at how many connected components of ink there are. What's the aspect ratio? How many loops are there? It's increasingly the case, especially for problems like this, that we feed in low-level features like pixels, and higher level features like edges. We'll talk about that in a couple weeks when we talk about narrow nets. some account activity and you want to red flag accounts that are suspicious. Automatic essay grading, auto grading, this can be a machine learning problem. Customer service email routing. You'd like to automate the routing of that. Review sentiment. Here's a bunch of reviews of my product. Which ones are good and which ones are bad? Have they gotten better in the past 10 days since the new announcement? And so on. You can do that with classification. You gotta do that before you can do things like translation. In model-based classification, rather than directly learning from errors that you make in the world from experience, think like reinforcement learning model-free. The Naive Bayes assumption is an extremely radical assumption as far as probabilistic models go, but for classification, it turns out to be really effective and it goes something like this. You build a model where the output label and the input features are random variables. There's going to be some connections between them, and maybe some other variables too that might help you build a better model. 1 to 0 is equally likely. If you're looking at lots of round numbers, maybe it's 0. You can think about why that might be. So these come from data. And this actually underscores the point that depending on the data, depending on where you are, it depends on what you are looking at. And that's what we're trying to figure out here in this article. We want to know what you think is most common in real data. Or are they all equally common? So 0 might be common. How you collect the data, it can actually shape the distributions that you are imagining are going to exist at test time. In addition to the prior probability of each label, we can compute things like, what is the probability that pixel 3 comma 1 is on, given each class? This isn't a distribution over on or off. These are just the probability of that pixel for each class. And it's going to be some number. So for example, the pixel in that position might be pretty likely for the number six, but pretty rare for thenumber one. be weighed, and that's what's going on here in the conditional model. It's actually very common when you're multiplying probabilities to just add log probabilities instead. In the end, when you want to turn it into probabilities, you do need to sum them. And summing the logs won't do that. You need to do a sort of log sum, which one way to do that is to convert them back to probabilities by taking exponentials. That's actually not the way you would do it. You would sort of shift them by their minimum or their maximum as appropriate. word depends on the class and also the previous word. In general, it will be a little more accurate, but at a cost of having significantly more complicated conditional probabilities to estimate. How much more accurate will depend on to what degree the bag of words assumption is dangerous. If you're looking for a class which is not kind of strongly connected to the actual ordering, Naive Bayes is really good. Otherwise, you add other correlations like this to fix it. OK? Other questions? Good questions. Machine learning theory is based on trying to say something precise about the connection between what's going on in your data and this future used to which you're going to put the classifier to. The main worry is that in picking the parameters of your model, you do a really good job of capturing that training data, but it doesn't generalize. This is like you download all the exams from past years, and you optimize. You learn all those answers, and then you go to the final exam and it's totally, totally different questions that look nothing like those. You go through an experimentation cycle, it looks something like this. Get your model and learning already, and then you're going to learn the parameters. parameters are things like what's the probability of pixel 73 for the number eight? Then there's hyper-parameters, like, do I want to have features for the lowercased version of the words in case I've seen the word, but never uppercased? Right? These are questions about, is this or this orthis going to work better? You always know you're training data. The question is, do you generalize? This can happen to your classifier too, so you always want to test your performance on data that was not used to train it. And there can be a slow leak of your test data into your training data if you're not careful. So you try not to peek at the test set, and that's another reason why I say don't test your classifiers on the test data.to see that today. we have held-out data, which gives you something you can peek at. I ran 20 experiments. How did they go? Am I doing well? Is this thing good enough to release? You need to have some metric, and there's a lot of possible metrics. An easy one is accuracy. For how many of these emails did I make the correct decision? Fraction of instances predicted correctly, but actually, that's actually not a great metric for spam detection. Any ideas why? What's wrong with accuracy? cost-- of different kinds of mistakes may not be the same. And so accuracy isn't always what you want. What you really want, is you want a utility here. You want to know what was my utility, and you should have different costs for these things. There are also cases like machine translation, where you're always going to be a little bit off, a little word here or there, but there's a difference between being completely off and a tiny bit off. And again, we're going talk a lot today and next time about over-fitting and generalization. Spam detection is, in some ways, a very poor example of a canonical classification problem. The problem here is not that you're test accuracy is low, but your training accuracy was also low because you didn't learn anything. We'll investigate these things formally in a few lectures. I had a really good question during the break, which I want to answer for everybody, which is, couldn't you just defeat this Naive Bayes spam classifier by pasting the word Gary 100 times to the end of your offer to lose weight while you sleep? Spam is being generated by people who are trying to defeat spam filters. Spammers are going to double down on what's working. And so if you have features that are like, did the same email get sent to a lot of different people? What do spammers do? They're going to start modifying that email in some templated way. Now you have some feature that detects templates. Now there's sort of an arms race here. and so in that sense, in a sense, there's a spam arms race. over time, spam classification doesn't actually look like a standard classification problem because it's adversarial. OK, so in these images, you want to fit the hat right. You don't want it to be too small, because if you over-fit, you're not going to be able to generalize. Here's an example of this tradeoff. In general, we're going to do discrete classification. But for this example, let's imagine the thing we're trying to do is to fit a curve to this data. the data points of the squared distance or something. So what is my constant approximation to this? Does anybody want to hazard a guess? Let's call it five. OK, did I fit something about this data? Yeah, I felt something about the data. I fit basically it's mean. Did I capture the major trends? No. I would call this under-fitting. All right, let's try again. Let's fit a linear function. OK. It's close, right? It's a better fit than the constant function. Notice that when I went to linear function, the space of hypotheses grew. Instead of just lines, now it's like lines with slopes and intercepts. In Naive Bayes probabilistic models, over-fitting usually shows up as zeros in your probability table. For other methods, it's actually going to show up in totally other ways. We need to smooth or regularize our estimates, and we could take that polynomial and limit the degree of the polynomials. We already know one kind of over-fits to limit that, so let's just look like what it would look like to just illustrate it. you shrink a hypothesis space, you fit less. Using it too much, you under-fit. So let's take a look at the distribution of a random variable, just to sort of show why we need to do these kinds of things. We can do elicitation, right? You can ask a human. You can go to a doctor and say, hey, I'm building a classifier. What fraction of people with meningitis will present with a fever? And a doctor can give you a guess. You could also do that empirically. The maximum likelihood estimate, or relative frequency estimate, is a way to estimate the likelihood of an outcome. The more samples you draw, the more accurate your estimate will be. But in practice, you need some smoothing to prevent things like zeros in these estimates. This is actually due to a philosopher who kind of worried about things like things like how do I estimate the probability the sun will rise in the morning? Every morning it's risen, so far so far. Every morning the sun's risen. So I need some way of incorporating that into my estimate. example, I can go into my spam, and instead of computing odds ratios on the maximum likelihood-- or empirical relative frequency estimates-- I can instead do some smoothing. And suddenly things that only occurred once, they don't percolate to the top, because they haven't occurred enough to overwhelm that flat prior that I'm associating them with. So this is the top of the odds ratios for ham on the left, and favoring spam on the right. Some of these maybe make sense. Like, there it is. Free is probably in there somewhere. If you see money, that's a good sign that it's spam. In general, your model is going to make errors. The k that's going to be most accurate on my training data is zero. That's the maximum likelihood estimate. We learn our parameters from the training data. We tune them on some different data, like some held-out data, because otherwise, you'll get crazy results. And then eventually, you're going to take the best value, do some final tests, test run. We're talking a bit more about features, because it's important for when we start to get to neural nets. In general, in general, you're going to need more features. In spam classification, we found out that it wasn't enough to just look at words. For digit recognition, you do sort of more advanced things than just looking at pixels, where you look at things like edges and loops and things like that. Try to do things that are more advanced than just pixels, like looking at loops and edges and edges, and try to look at other metadata from the ecosystem as well as just words. are invariant to rotation and scale and all of that the vision folks think about. You can add these as sources of information by just adding variables into your Naive Bayes model. We'll also talk in the next few classes about ways to add these more flexibly, and also ways to induce these. All right, I'm going to stop there for today, and as you go, please come up and grab some more candy. Thank you. Back to the page you came from.

ROUGE-1: 28.13, ROUGE-2: 26.72, ROUGE-L: 25.50
BERTScore: 61.77

==============================================
==================== [18/100] ====================
Summary:
in this video we're gonna talk about how a country can gain from exporting goods or services through international trade. We're gonna look at how consumer surplus producer surplus and total surplus are going to change when we introduced the idea of trade in allowing Chile's copper manufacturer producers to trade on the global market. The world price of copper is five thousand four hundred and forty dollars a ton. Because the world price is higher than the price in Chile Chile will export copper. There is a shift of some of the consumer surplus is going to go to the producer surplus.

ROUGE-1: 14.64, ROUGE-2: 13.69, ROUGE-L: 13.40
BERTScore: 67.04

==============================================
==================== [19/100] ====================
Summary:
Thiazide tells us that this medication works in the early part of the distal convoluted tubule that's found within this nephron. This transporter is called the sodium chloride co-transporter and it is considered a cyanide sensitive transporter so hence why this drug works so well. While loop diuretics are a lot more effective than a thiazide diuretic but the thiazid does provide a nice diuresis effect. They're less effective in patients who have a compromised GFR a go merrill ER filtration rate. the high uric acid level and hyperglycemia again teach your diabetic patients to monitor their blood glucose really closely while taking a thigh. Avoid giving doses of diuretics at night because we want our patients to sleep at night we don't want them up using the bathroom all the time so make sure you're not doing that. orthostatic hypotension this is where the when the patients maybe they've been sitting or lying down they get up they can fall they become dizzy you want to teach them to change position slowly. weight or losing weight so we play a huge role with that as well okay so that wraps up this review over thighs I diuretics. thank you so much for watching don't forget to take the free quiz and to subscribe to our channel for more videos. Back to the page you came from. Follow us on Twitter @CNNOpinion and @bbcopinion. Follow our Facebook page and our YouTube channel for all the latest from CNN.co.uk and CNN iReport.

ROUGE-1: 12.89, ROUGE-2: 11.54, ROUGE-L: 11.90
BERTScore: 60.24

==============================================
==================== [20/100] ====================
Summary:
Future John Green tells you that in a stunning turn of events the 2020 presidential election will be won by - Harry Styles. We‚Äôre going to change the constitution to make it possible. Because‚Ä¶ that‚Äôs how much we love Harry Styles in 2020. The U.S. was facing what turned out to be the 2nd worst economic crises in the past 150 years. The Wall Street Wamboozle, the Financial Fartstorm. And the U.N. Security Council. In the early 2000s, many millions of Americans bought real estate assuming that its value would increase rapidly and forever. It turns out this was essentially a pyramid scheme and, my friends, I was not at the top of the pyramid. When banks stop lending, business can‚Äôt function. So the stock market collapsed, with the Dow Jones Industrial Average dropping from above 14,000 to around 8,000. By mid-2009 more women than men held paying jobs for the first time in American history. And World Trade cratered and that led to unemployment and misery worldwide. In 2008 Obama‚Äôs election seemed a political watershed and not just because he was the first African American president. Obama promised to change the culture of Washington. He would end partisan squabbling‚Ä¶. sorry I couldn‚Äôt even get through that sentence. He also wanted a foreign policy based on diplomacy, he wanted to reduce inequality and increase access to health care. And he also wanted to end the wars in Iraq and Afghanistan and, as critics mocked, reverse global warming. So how has he done? Not bad. Well, some would say not great either. For instance he launched diplomatic outreach to the Muslim world, but a lot of this was more rhetoric than action. He signed into law the Lily Ledbetter Fair Pay Act, which made it easier for women to sue when they had been systemically underpaid. And speaking of women he appointed two of them to the Supreme Court, Elena Kagan and Sonia Sotomayor. He also followed through on his promise to end the war in Iraq, although to be fair the Bush administration had really set him up for success there. Obama has been criticized internationally for backing off his promise to close the Guantanamo Bay detention camp. But the Obama administration has deployed far more unmanned drones to kill suspected militants around the world. Despite provoking outrage on the left and the right, Americans generally appear to support the use of drones and extra-legal assassination of accused terrorists. Obama was fortunate to have a Democratic Congress for his first term in office, so he could push through a lot of legislation. The Affordable Care Act is arguably the most significant piece of social legislation since Medicare. ceiling. Things that Congress used to be able to hash out back when their business was governing not ideological rigidity. Meanwhile, the economy has slowly added jobs and looks halfway decent at the moment mostly because Europe looks so bad. That qualified questioning yay is about the last word I have to say on American history. We have to ask ourselves again, ‚ÄúWhat does freedom really mean?‚Äù Can you be free when you live in poverty or when you‚Äôre one injury away from bankruptcy? Can you being free when the government can go to a secret court to read your text messages? job to protect you not only by having a standing army but also making you wear your seat belt? Those are ultimately ideological questions, but we have to grapple with them in a real practical way. And the great story of American governance is compromise. But that is also often been the tragedy ofAmerican governance as when the Constitutional Convention compromised over whether African American people were people. So if you‚Äôve learned anything this year, I hope its been that the American story that we find ourselves in now isn‚Äôt entirely novel. And I think we have much to learn from those who came before us.

ROUGE-1: 34.31, ROUGE-2: 32.89, ROUGE-L: 34.21
BERTScore: 62.51

==============================================
==================== [21/100] ====================
Summary:
Simple graphs are directed graphs where the arrows have a beginning and an end. A directed graph might have a self loop, an edge that starts and begins at the-- starts and ends at the same vertices. A simple graph has a nonempty set, v of vertices, and it has a set E of edges, but the edges now are somewhat different since they don't have beginnings and ends. An edge just has two endpoints that are in V, and we don't distinguish the endpoints. Study: Men have 30% more partners than women, according to the US Department of Health. David Frum: "We're going to come up with a very elementary graph theoretic argument that says that this is complete nonsense" He says there is a fixed relationship between the average number of partners of men, the average degree of the M vertices and the average degrees of the F vertices. Frum says the answer seems to be that people are lying about the number of women and men.

ROUGE-1: 15.32, ROUGE-2: 13.95, ROUGE-L: 14.21
BERTScore: 66.87

==============================================
==================== [22/100] ====================
Summary:
Robotics is a really cool and important direction for the future. I really believe that in the future we will have AI assistance whether they are embodied or not to act as our guardian angels. These agents will help us with cognitive and physical work. With AI we will see such a wide breadth of applications for instance these technologies have the potential to reduce and eliminate car accidents. We have three types of learning and you have seen different aspects of these methodologies throughout the course we have supervised learning, unsupervised learning and reinforcement learning. The images that get fed from the camera streams of cars are fed to the decision-making engine of the car. Machine learning is very powerful for building perception systems for robots but as we employ machine learning in the context of robots it's important to keep in mind the scope when they work when they don't work and then it'simportant to think about what my what kind of guard rails we might put in place at the decision time so that we have robust Behavior. "With all small perturbations you can turn the stop sign into a yield sign and you can imagine whatkind of chaos this would create on a on a physical Road" to do given input reinforcement learning is causing a huge revolution in robotics. Reinforcement learning is concerned with how intelligent agents ought to take action in an environment in order to maximize the notion of a cumulative reward. In order to get the simulation to drive a real robot we actually need to think about the Dynamics of the robot and so in other words we have to take into account what the vehicle looks like and its kinematics what are its Dynamics and so it's really cool because really we are now able to train in simulation. In 1995 a Carnegie Mellon project called nav lab built a car that was driven by a machine learning engine called Alvin and Alvin drove this car all the way from Washington DC to Los Angeles. The car was in autonomous mode for a large part of the highway driving but there was always a student there right ready to um to take control and the car did not did not drive inonomous mode when there were when it was raining or when there was a lot of congestion or when the car had to had to take exits. In 1986 a German engineer Ernst Dickman started thinking about how he could turn his van into an autonomous vehicle. Alexander: We have very effective and Deployable solutions for robot cars that move safely in Easy environments where there aren't many static nor moving obstacles. Many companies and research teams are deploying and developing self-driving cars. Many of these preconditions revolve around certainty in perception planning learning reasoning and execution before we can get to Robo taxi but we can have many other robot solutions that are much that that can happen today. Alexander: We can use deep learning and reinforcement learning to take us from images ofroads onto steering and throttle and what you can do with this is really great. Machine learning can be used to look for the presence of language which is a major sign of intelligence. We are trying to understand the phonetics the semantics and the syntax and the discourse for whales. We have a big data set consisting of about 22 000 clicks. Using machine learning we can identify coded types. We can identify patterns for Coda exchanges and we can we can begin to really ask ourselves how is it that that that Wales exchange information and if you're interested in this problem please come see us. that model to run on edge devices or on huge devices uh you have seen that many of our Solutions are Black Box Solutions and sometimes we have brittle function we have we have easily attackable models. There is so much opportunity for developing improved machine learning using existing models and inventing new models. If we can do this we can create an exciting work world where machines will really Empower us will really augment us and and enhance us in our cognitive abilities and in our physical abilities so just imagine waking up enabled by your personal assistant that figures out the optimal time. Robots can help us with cognitive and physical work. There's the garbage ban the garbage bin that takes itself out. After a good day when it's time for a bedtime story you can begin to enter the story and control the flow and begin to interact with the characters in the story. These are some possibilities for the kind of future that machine learning artificial intelligence and robots are enabling. I'm personally very excited about this future with robots helping us with Cognitive and physical Work but this future is really dependent on very important new advancements that will come from all of you. Thank you very much and uh come come work with us.

ROUGE-1: 17.40, ROUGE-2: 16.45, ROUGE-L: 16.54
BERTScore: 60.84

==============================================
==================== [23/100] ====================
Summary:
Tatiana and her team eagerly learned a few of the new recipes last night. The menu has been completely revamped. The message is clear nobody scared to walk through that door and get their hands dirty in that kitchen no we're not tomorrow is a big day let me tell you I need everyone on their game good night guys get some sleep thanks thank you very much chefs oh my God this is amazing last night last night Tatiana andHer team eagerly learn a few new recipes. The new menu has a dynamic simple clean menu right now.

ROUGE-1: 26.11, ROUGE-2: 23.21, ROUGE-L: 24.04
BERTScore: 62.53

==============================================
==================== [24/100] ====================
Summary:
HONG LIU: This is a key relation between the bulk and the boundary. The more you go to the interior of the space time, then corresponding to the lower energy process when viewed form the field theory. So here, the same process is happening here compared to happening here, and here corresponds to the IR process, and the [INAUDIBLE] boundary corresponding to UV process. This is the IR-UV connection between the Bulk and the Boundary. It's called the IR/UV connection. the boundary theories. And also, this gives you an intuitive understanding where does this actual dimension come from from the field theory point of view. Then from a field theory perspective, this actualdimension can be considered as a geometrization of the energy scale. So any questions regarding this? Good. So now let's talk about some further aspects of the duality. The duality is that once you realize there's such relation, since the two sides are completely different objects, so the game is that you really have to do lots of guess work. How does this quantity translate into that quantity, et cetera? And then check the consistency. Just like you don't know two languages, and then you have to guess between the two languages and then build up the dictionary. We will be doing the same. So this is more like a review of what we already sad. We have N equals 4 super Yang-Mills theory. And then here, you have type IIB string in Ads5 times ds5. So here on this side, there is a conformal symmetry which we explained before. The low energy limit, as we said before, just has to be super gravity. But the interesting thing is that by definition, the supersymmetry on the gravity side is actually local. So if you look at this correspondence between each other, then you actually see a pattern. So the isometry is a subgroup of diffeomorphism. So why we are only talking about isometry? Why we don't talk about other parts of the diffeomorphicism? So what's special about the isometric? But let me just save time. HONG LIU: Large gauge transformation means the gauge transformations which don't vanish at infinity. The ordinary gauge transformation is just corresponding to redundancy of degrees of freedom. Gauge freedom is just redundant freedom. You never see it on the other side. And those large gauge transformations essentially is like the global part of the u(1) gauge symmetry, in some sense can be considered as.asymptotic geometry of the space time. But the corresponding part associated with the boundary is with the gauge symmetry on the gravity side. previously we said, from the relation of the d-brane, so the G Yang-Mills square here is related to the 4 pi GS here, string coupling. So the N is the same N on this side. And we can also, instead of using GS, as we said before, you can also use the Newton constant. Then you find, once you plug all those relations in, how the GS and alpha prime, et cetera related to N, then you find here actually, G YangMills have disappeared in this relation. expanding 1 over N squared. So as we said before, we often do dimensional reduction on S5. So these relations are often useful in the future. So now let's look at [INAUDIBLE] limits of this relation. So let's first look at the classical gravity limit on the gravity side. By classic, we always use h bar equal to 1. But quantum gravity is captured by this parameter, h bar times GN. So even though h barequal to 1, in the limit when GN goes to zero, then you are in the regime in which you can ignore the quantum gravity effect. they all should be treated quantum mechanically. It's just that you should consider this small. So let's consider what this means. So GN small as a dimensionless parameter, this translates into field theory side if we use this relation. So that means N goes to infinity. So this is the large N limit of the Yang-Mills theory. And then alpha prime goes to zero. From the relation between the alpha prime and here, when alpha prime go to zero, so this is in the downstairs. side is simple, we can just deal with quantum field theory in the curved space time, which we know how to do. But on this side, it's highly non-trivial because this is an infinite coupling limit. So this will tell you that the strong coupling limit is described by classical gravity. So that means that we can actually use classical gravity to, in principle, solve problems which are strongly coupled. So also, of course, there are corrections beyond this. So quantum gravity corrections on this. side, so this is a classical gravity limit if you take those parameters to go to zero. essentially the semi classical gravity limit because we still treat the matter fields essentially as quantum. HONG LIU: Yang-Mills theory lives on Minkowski space. And then you say you can imagine that this is the boundary, this relation is related to the bulk and the boundary. And this is a postulate based on that fact. Yes? AUDIENCE: I thought one of the motivations for thinking about the holographic duality was to try to escape [INAUDIBLE] And all of a sudden, it strikes me, so we're trying to get on the boundary of AdS. Then they will also exist in the massless particles. will give you a direct way to argue that. Also, in your p-set, you have checked this holographic bound. And so that's a confirmation of this. Yes? AUDIENCE: So what does the massless [INAUDIBLE] field on the right map to if it's the same representation of SO(d, 2)? HONG LIU: Sorry. Say it again. We are going to talk about it. So even though this relation was discovered in '97, actually in the '80s, people already worked a lot to consider this type to be supergravity on this space. On the string theory side, you always have this dilaton. And on the N equal to 4 super Yang-Mills theory, that's a local operator. And it turns out that operator is mapped to this Dilaton. So I won't go through those details. But let me just mention the most important such kind of mapping for these two theories. And actually does have consequences for the general story. So the mostimportant mapping. And then you can immediately see they actually map to certain representation of operators. theory side, we have this SO6 gauge symmetry. Then we have the SO6 conserved current. And it turns out this, on the gravity side, just maps to sO6 gauge field. And then another universal operator on the field theory side is the stress tensor. And this is mapped to-- turns out, to the metric perturbations. It's a deviation from the AdS metric. But physically, this is also natural. Physically, alsonatural. a few minutes. So now, given this mapping, any operator is due to a bulk field. Then you can ask some immediate questions. For example, the quantum numbers of these operators will map to the quantum number of the bulk fields. And that's something I said you can check their symmetries. So do you have any questions regarding this? So for local operator on the field theory side, so once we have this mapping we can immediately ask questions related to operators on this side, and try to ask what's the counterpart on the other side. When phi 0 is equal to constant, then this corresponding to a change in the coupling for this operator. If your Lagrangian previously already included this operator, and then if you add such a term, then you are just changing the coupling. So this is a natural thing to do. But in general, you can make it space time dependent. And the immediate question is, what does this operation-- so in the field theory point of view,you can always do this operation. guesswork is based on some very small clues. Good physicists do is that they can see non-ordinary things from ordinary things. So let me start with that relation. Related to the GS. GS string coupling can be considered as the expectation value of the dilaton field. The bulk field phi due to O has a boundary value phi 0. Any questions about this? Yes? AUDIENCE: Your choice of operator O should be consistent with all symmetries, right? HONG LIU: No. O can be anything. You can choose any O. methods. Now I'm saying because I already know it's true, but in real life, what you will do from this example, you will say, ah, this must be the case. Then you will start trying to find examples to check it. And we will describe it later. It's that any conserved curve in the boundary theory must be equal to some gauge field in the gravity side, and the stress tensor should always be due to the metric. So if we assume this, I can also use this to argue. I can make star and star star natural for any duality. and the field theory. And that also tells you that if you have a theory which due into a higher dimensional theory, and then that theory has a stress tensor, then this bulk theory must have gravity. So you can say, if any field theory is due to a theory of one higher dimension, that theory must involve gravity-- nothing about quantum gravity. Let's stop here. We're not going to get into quantum gravity right now, we're just going to talk about the theory of gravity.

ROUGE-1: 31.79, ROUGE-2: 30.50, ROUGE-L: 29.60
BERTScore: 72.33

==============================================
==================== [25/100] ====================
Summary:
Bolek Wyslouch: How do you convert a given physical system with all the forces, et cetera, into some sort of fixed form, fixed type of notation? Bolek: We will work on two, again, simple physical systems, one that consists of two pendula driven by forces of gravity, each of them. And we will discuss various interesting-- even though the system is very simple, just two masses, a spring, a little bit of gravity on top of that. me do everything. Let's write down everything in the matrix form, because it turns out that linear matrices are very useful for that. So let's introduce to them and show vector, which consists of x1 and x2. So we will be monitoring the change of this x2 as a function of time. We will introduce a force matrix k, which is equal to k plus mg over l minus k here. And then we need a third matrix, mass matrix, which simply says that masses are mass of first object is m. Use complex notation to find solutions to coupled oscillations. Complex notation is a mathematical answer, how to solve a mathematical equation. The physics answer is to find fixed frequency modes us such that the system, the complete system, oscillates at one frequency. The solution is written as two numbers, oscillatory term, with both x1 and x2 oscillating with the same frequency, and this is our postulated solution. It's a slightly different way of doing things, but we can assume this for now, right? to obtain the solutions to at least one normal mode, and we expect that there will be two normal modes, because we have two masses. So mathematically, the way to find out the oscillating frequency is you take a determinant of m minus 1 K minus i omega squared must be equal to 0. So we have a-- what this says is that if I set my frequency to g over l, then it will be able to set things up to set such that one frequency, one frequency forever. so it's stretch from both sides. And the whole system oscillates at the same frequency, and because of this additional force of spring, the frequency is actually higher, it's larger. It oscillates faster. All right, so that's the first step in understanding the system. We now know that there are two oscillations and two normal frequencies. The next step to finish our understanding of the system in a mathematical way, to describe it fully, I have to know what is the shape of oscillations. In principle, we know, now, at the end of the day, I still want to know how much 1 moves, how much 2 moves. So we have to put it all together. We have identified the frequency and the kind of, in the matrix notation, shape of the node. But of course, the final solution is a linear superposition of all possible normal modes. And then you calculate a shape of a normal mode. Is that clear? Any questions at this time? Right? The shape, 1 and 1, and 1 minus 1 is fixed, because these are the shape of normal modes, which corresponds to those frequencies. And the superposition of x1 plus x2 gives you the most general combination of possible motion. So if I write this down now in terms of position of number 1 and number 2, so I have a position of X1 as a function of time. In general, it will look like this. It will be some sort of constant alpha, cosine omega 1 t plus phi. The magenta is normal mode number 2. And blue and the red are the actual pendula. And the motion of blue and red is simply a linear sum of the two. And this is exactly what-- this is the computer simulation that shows you that one of them is going up, the other one down, et cetera. Now, is there a way to disable one of the normal modes? How would you disable a normal mode? Is there a quick way to set things up such that the second normal mode, BOLESLAW WYSLOUCH: What you do, is you just change the initial conditions. So for example, one possibility is I move both of them at the same distance, and I just let them go like this such that the spring is irrelevant, right? How would I do it in my program? I don't know. So what I did is I just changed the initial condition. And you see, this is the type of motion where one of the modes has stopped, and the other one is going on. could put it with me some spaceship, and go to a place where the gravity is different, right? Why not? So what would happen? So if gravity changes, then basically what will happen is both this term and that term will change. So let's try to see what happens on the Moon. It's a little bit not completely clear what's going on, but you see, actually the motion is kind of a little strange. Look at the red one. The red one is stopping. Then it's going halfway out. It looks kind of messy, doesn't it? center mass of the system or just one of the two --? BOLESLAW WYSLOUCH: This one, I think, this one is justOne of them. Actually, the one-- on the difference-- it normally doesn't matter. What matters this is the frequency and how these move to the other. So let me now talk about this thing, which is called beat phenomenon, because when you look at the motion of one of those objects, or the difference between them or whatever. BOLESLAW WYSLOUCH: I don't have to go to Jupiter to modify it, because this one is just a little mass here, right? [TONE] Ah, cool. So now, this thing is probably-- I know it's a period, a fraction of a second,right? Yes? AUDIENCE: Should both of those sine and cosines have Ts in their arguments? BOLESlaws: Of course always. They are both time dependent, yeah. sort of early on, to instead of, so far, when we talked about pendula, we describe their motion in terms of motion of number 1. It turns out we can rewrite the equation into some sort of new variables, where, so-called normal coordinates, where you'll simultaneously describe both of them and then kind of mix them together to have a new formula. So you do change of variables. So instead of keeping track of x1 and x2 independently, you define something which I called u1, which is simply x1 plus x2. then you can say, ha, ha,. I can I can introduce normal variables and make things simpler. But at the end of the day for complicated systems that work is the same. But for simple systems like this one where there is a good symmetry, you can do it. Anyway, so I think we are done for today. And on Tuesday, we'll continue with forced oscillators. All right? Thank you. "I think we're done for day one. We'll continue on Tuesday," he said.

ROUGE-1: 21.45, ROUGE-2: 20.69, ROUGE-L: 20.37
BERTScore: 63.24

==============================================
==================== [26/100] ====================
Summary:
GILBERT STRANG: Take a bar, a material bar. The ends of the bar are kept at temperature zero, they're frozen. Heat is flowing around in the bar, and where is it going? It's flowing out the ends. So I insulate the bar and I put it in the freezer. And I have an ordinary bar and an ordinary temperature. And the heat is escaping out the sides, so the end x equals 0 and the end f equals 1. And that's the solution. differential equation. We have a whole function to match, so we need all of those. And Fourier series tells us how to do that matching, how to find these Bk's. So that's a separate and important question, Fourierseries. Thank you for your time. Back to Mail Online home. Back To the page you came from. Back into the article you came From. The story behind the story: Click here to read the full transcript of this article. Back onto the page of the story you come from.

ROUGE-1: 20.68, ROUGE-2: 16.05, ROUGE-L: 15.75
BERTScore: 54.96

==============================================
==================== [27/100] ====================
Summary:
MIT OpenCourseWare continues to offer high-quality educational resources for free. To make a donation or to view additional materials from hundreds of MIT courses, visit MIT OpenCourse Ware at ocw.mit.edu. The following content is provided under a Creative Commons license. Your support will help MIT Open courseWare continue to offer free, high- quality educational resources in the U.S. and around the world. For more information on MIT Open CourseWare, visit opencourseware.org. We've got two lectures left discussing linear algebra before we move on to other topics. We looked at one type of transformation we could utilize for solving systems of equations. Today, we'll look at another one, the eigenvalue decomposition. And on Monday, we will look at a different decomposition called the singular value decomposition, which is a different type of matrix. We want to make sure that we're answering those questions in a timely fashion. We don't want anyone to get left behind. be stuck. It won't proceed after that. So it's the difference between getting a solution and writing a publication about the research problem you're interested in and not. So how do you do reordering? Well, we use a process called permutation. There's a certain class of matrix called a permutation matrix that can swap rows or columns. So if I want to swap columns, I multiply my matrix from the right, IP transpose. If I swap the rows and then I swap them back, I get back what I had before. This is a form of preconditioning. It's always done via Gaussian elimination if we want an exact solution. You're studying one of them in your homework assignment now, where you know the matrix is banded with some bandwidth. So you don't do elimination on an entire full matrix. You do it on a sparse matrix whose structure you understand. We discussed sparse matrices and a little bit about reordering and now permutation. I feel like my diffusion example last time wasn't especially clear. So let me give a different example of diffusion. seen The Price Is Right? This is a game where you drop a chip into a board with pegs in it. It's a model of diffusion. The Plinko chip falls from level to level. It can go left or it can go right with equal probability. So the probability that I'm in a particular cell at level i is this Pi plus one. And there's some sparse matrix A which spreads that probability out. It splits it into my neighbors 50/50. And at the next level it all gets passed down by 50%. For a real N-by-N matrix, there will be eigenvectors and eigenvalues. They're special vectors that are stretched on multiplication by the matrix. The amount of stretch, however, is unique. It's associated with that direction. And that describes the eigenvector-eigenvalue pair. But because there's N equations for N plus 1 unknowns, that means they're not unique. We don't know what an eigen vector is uniquely. We can only prescribe its direction. But we'll find them in a second. We want to know the eigenvector of the rate matrix having eigenvalue 0. This should correspond to the steady state solution of our ordinary differential equation. Can you do that? Can you find this eigen vector? Try it out with your neighbor. See if you can do it. And then we'll compare results. Are you guys able to do this? Sort of, maybe? Here's the answer, or an answer, for the eigenector. It's not unique, right? It's got some constant out in front of it. James W. Swan: Try this example out. See if you can work through the details of it. I think it's useful to be able to do these sorts of things quickly. Here's a matrix. It's not a very good matrix. But it's all 0's. So what are its eigenvalues? It's just 0, right? And they're 0. That eigenvalue has algebraic multiplicity 2. Can you give me the eigenvectors of this matrix? Knowing what those eigenvectors are requires solving systems of equations. If I know the eigenvalues in A, then I can easily diagonalize my system of equations, right? So this is a useful sort of transformation to do. We haven't talked about how it's done in the computer. These are ways you could do it by hand. There's an alternative way of doing it that's beyond the scope of this class called-- it's called the Lanczos algorithm. And it's what's referred to as a Krylov subspace method. triangular form for this matrix. We'll talk next time about the singular value decomposition, which is another sort of transformation one can do when we don't have these complete sets of eigenvectors. You'll get a chance to practice these things on your next two homework assignments, actually. So it'll come up in a couple of different circumstances. I would really encourage you to try to solve some of these example problems that were in here. Solving by hand can be useful.

ROUGE-1: 20.09, ROUGE-2: 18.99, ROUGE-L: 18.90
BERTScore: 66.79

==============================================
==================== [28/100] ====================
Summary:
A random variable is a number that's produced by a random process. In physics, the number of alpha particles detected by a Geiger counter in a second is believed to be a random number. The number of faulty pixels in a monitor is also produced from an unpredictable randomness in the manufacturing process. The only time that C is not greater than or equal to 1 is when you have all tails, so there's a 7/8 chance, 7 out of 8 outcomes involve 1 or more heads. A random variable is not a variable, it's a function that maps the sample space to the real numbers. Usually this would be a real valued random variable. We have another concept of independence that holds for random variables that was motivated by the definition of independence for events. If I have an event A, I can package A into a random variable, just like the match random variable was really packaging the event that the coins matched into a [? 0-1 ?] valued variable. And it means everything that matters about event A is captured by the random variable IA. number of heads we can ask whether the event M, which is the indicator variable for a match-- the random variable M-- and the indicatorvariable IO are dependent or not. Now both of these depend on all the three coins. IO is looking at all 3 coins to see if there are an odd number of heads, M is looking to see whether they're all heads or all tails. And it's not immediately obvious that they're independent, but as a matter of fact they are. this can have value 0 and 1. If R is independent of S then R is really independent of any information at all that you have about S. If I have k random-- if I have a bunch of random variables, a large number much more than k, they're k-way independent if every set of k of them are mutually independent. And of course as with events we use the 2-way case to call them pairwise independent. We'll be making use of it in an application later when we look at sampling and the law of large numbers.

ROUGE-1: 25.27, ROUGE-2: 23.75, ROUGE-L: 23.80
BERTScore: 63.43

==============================================
==================== [29/100] ====================
Summary:
s gawan and the Green Knight um I believe this is the last piece where we don't know who the author is where it's unknown um I find the background interesting and that through textual Clues they're able to figure out who wrote it. It's interesting when they talk about some writer character character character is whatever that word is SAR Gowan as a ruthless bloodthirsty Warrior um we really don't see him in that regard. Throughout the story of seral when we again come back to what a knight truly is. if you look down at the bottom of uh uh uh well it's actually on 176 now he comes in and addresses everybody and everybody gets kind of kind of quiet uh because this is a huge monstrous man with a monstrous Axe and he saysi come to uh speak to the person in charge and off Arthur King Arthur says that would be I and he goes I've come to search you based on reputation remember how we talked about the the honor okay people don't do you know if the knights do something it's a reflection of Arthur and Arthur as we know from all of our you know illusions of movies and cartoons and stuff that you know he is a good guy. your reputation Royal sir is raised up so high and your castle and Cavaliers are accounted the best the mightiest of male clad men in mounting fighting the most warlike the worthiest the world has bred most Valiant to VI with in viral contest and as chivalry is shown here so I am ass assured at this time I tell you that has a track here so he goes on in great detail say I've come here because of the reputation did you notice some of those alliterations sprinkled throughout there hopefully it did. I so a game but to see if people here are as good as their word and so he says on page 177 that I am here not for war and I shall offer to him this fine axe freely that they strike a blow in return for another. Arthur says oh well surely you just this is ridiculous but I will do this if this is what you truly want I'll do it and we all know that Arthur based on what we know of him from the past you know is true to his word. see a big difference uh Raper and this guy big difference okay and so he says I will do this and I will take the axe um where's the actual beheading there on line 182 or so uh they're kind of getting the groundwork going goes when I have taken the blow after you have duly dealt it then you may keep your Covenant and call on me and if I waft you no words then well may you prosper stay long in your own land and look for no further trial so if after the deal I'll tell you where you can find me I will give you my name there are no tricks here. bath he didn't know that he was going to have to marry that woman did he no would he have still done it well how much does he value his life you know probably. The Green Knight graciously stood on the ground with his head slightly slanted he even got down exposed his neck the naked neck for the business now doe go gripped his ax gathered up and slashed down and what happened well line 203 the fairhead fell from the neck struck the floor and people spurned it as it rolled around. his Steed snatched the bridal stepped into the sturup and swung a loft holding his head in his hand by the hair he settled himself in the saddle as steadily as if nothing had happened to him though he had no head. Then the head will obviously have to tell him his name and where you can find him are you envisioning this cut the head off the body walks over I mean you've seen it comically where a headless body is looking for a head on the ground right. will be able to pick it up and still live and survive and so oh crap this is not going to turn out well. The Knight tells him you have a year to find me you can find me I the Knight of the green Chapel green Chapel no kidding Everything Green line 232 233 come or be called a coward and it's not just sir GNE being called a cowardly we've got to understand that mindset who would that be calling a coward Arthur and everybody else the Knight came here specifically to challenge the best that Earth has ever bred. mind um in this you know kind of lengthy introduction just to set up the fact that this guy is having some supernatural abilities that the normal person like probably GNE does not have. So that will lead us into uh into the uh the coming up here with him looking for the night so at the bottom of 181 uh he's gone off on his journey had a year and a day to find it and throughout the book you I'm sure there are tons more Adventures probably more of those uh you know uh battles or actions. you receive the the hunter the owner of the house the husband says whatever you receive you must give to me that seems kind of weird what possibly could he receive while he's there well we find out that the wife really kind of starts to come on to him over time okay real flirty she's she's a temptress okay very seductive uh she gives him a kiss what does he do when the husband comes home that day cuz the husband's going to share his his food share his everything with him as long as GNE you share with me. you know of his pact uh with King Arthur to be a good individual. She eventually does give him a green girdle um a green corset a a sash some sort of clothing that he can wear under his stuff and she gives that to him and he accepts it okay which isn't necessarily so bad. She says you wear this and no harm will fall upon you that's about the best thing you can say to somebody who's getting ready to go get hit in the head with an axe. up my sleeve come time to get that shot in the head um and so he does leave and he keeps that GLE for himself um and doesn't give that away page 185. He roamed looking for this man he roamed up to the roof of that rough dwelling then from that height he heard from a hard rock on the bank beyond the brook a barbarous noise. What it clattered amid the cliffs fit to cleave them apart as if a great sight were being ground on a grindstone. thing and so he hears it you know happening and um so he screams out and the and the um and the Green Knight comes he says by there said one on the bank above his head and you shall swiftly receive what I once swore to give you. He chastises him and tells him take off your helmet uh and offer no more argument or offer noMore argument or.thing and he gets ready to hit him and it's one of those kind of like you've had friends before probably have come up to you and go like that. Justin: The Green Knight is a coward and you were just proved to be a coward. The Knight knows about the girdle and so he knows all about the wife giving that to the Green Knight. Justin: If he never violated his oath the most he was ever going to do was scare him and probably wasn't even going to hurt him. He says do it again I will not flinch go on game on 185 187 excuse me um I shall stand your stroke not starting at all till your axe has hit me. Green Knight is the protagonist but yet isn't he that antagonist early on? The purpose of the Green Knight is simply to challenge challenge King Arthurs and his crew and see if they truly are as honorable as they say as legendary as Legend has as the stories have it. We do have the Damsel in Distress we have that woman the temptress that's trying to corrupt the hero and take him down the path maybe uh satanic in some degree with regards to trying to lure the good person away with the forbidden fruit.

ROUGE-1: 48.77, ROUGE-2: 47.39, ROUGE-L: 46.73
BERTScore: 64.21

==============================================
==================== [30/100] ====================
Summary:
hey what's going on YouTube boy Robert and my mission is to teach you everything in the kitchen now earlier this week at work I learned and I was wondering what are the fruits and vegetables I can turn in the salt as well. Today we have blueberries strawberries Kiwis red beets yellow beets pineapples right in fruit and cucumber this is the equipment you'll need for today you don't need one blender sheet trays mason jars parchment paper aluminum foil a spice grinder a bowl with a strainer and a plastic spatula.

ROUGE-1: 27.87, ROUGE-2: 27.37, ROUGE-L: 27.87
BERTScore: 66.38

==============================================
==================== [31/100] ====================
Summary:
The six Vital Signs are pain oxygen saturation temperature heart rate respirations and blood pressure. Before you start you want to perform hand hygiene and provide privacy to the patient and tell them what you're going to be doing. If they do have pain ask them the quality what does it feel like and where it is at so hi Ben my name is Sarah and I am your nurse and I'm going to get your Vital Signs and I'll be getting your hand hygiene as well as your hand movements. first thing I want to ask you what your pain rating is are you having any pain rate on a scale of0 to 10 yes pain in my shoulder and it's a three okay and what is it feel like it's just a sharp pain when I raise my arm okay now I'm going to get your temperature there's several ways you can take a temperature every facility has a different system set up. axillary and temporally the readings are going to run about one degree lower than oral. For tanic and rectal temperatures it's going to usually run about 1 degree higher than your oral reading. In this video we're going to go over the one step blood pressure of how to obtain it manually. We are going to palpate the brachial artery this is in the bend of the arm and make sure you ask the patient which arm you can take their blood pressure in because you don't want to take it in arms with if they've had blood clots or they have a shunts things like that. We're listening for whenever it stops and whenever it's over that's our diastolic blood pressure.

ROUGE-1: 30.53, ROUGE-2: 28.21, ROUGE-L: 29.40
BERTScore: 74.34

==============================================
==================== [32/100] ====================
Summary:
Grenade algorithm uses a minimal set of three-point correspondences to solve the camera pose estimation problem in the grenade formulation. The disadvantage of this particular formulation is that it ends up with a four degree polynomial which means that it could give up to a total of four possible solutions. The question becomes whether can we find the solution directly from any four point correspondences such that the solution is unique? The answer is yes and it was formulated by quan at all in a paper that was published in the 1990s. furthermore we have three polynomial equations with the same unknown variable so there's no guarantee that all this are going to be the same solution due to noisy data and probably the most important part is that we cannot profit from this data redundancy which should increase the stability. In this case here because get solutions doesn't agree well due to the noise so it means that the solution is not stable at all and a better solution here is proposed by uh kwan and lan in the paper published in active tipami in the year 1999. over here which is in the form of a x equals to zero uh we have seen this homogeneous linear equation many times so now this means that we get this form of equation over here. A t which is written as a t equals to 0 over here and what it means is that we simply have to solve for the unknowns over here so this matrix here is known and the unknown contained in this particular vector here and we know that since a is a three by five matrix uh at most have a rank of uh three this is the same as solving ax equals tozero. s1 we know that t5 here is actually a vector that is made up of the same entry but of different order so x4 x3 x2 x and 1 of different entry. We know that by observation any two elements the product of tij is going to be equal to tk and tl for this constraint to be valid where i plus j must equal to k plus l. We can substitute the respective components of t5 into this particular constraint to get this equation over here. where we can solve for since we know that x equals to s1 squared we can solved for the final depth by taking the square root of x. Once s1 is solved we can back substitute s1 into the polynomial equation of f i j s i and s j equals to zero. After we have gotten all the unknown depths we can do the same thing to to apply absolute orientation to recover the camera pose as in the grenade algorithm and here what's interesting here is that because we have four point correspondences we'll get the unique solution provided that the four points are not degenerate. as what we have seen earlier on now uh it happens that the linear four-point algorithm can also be applied to more than four points so for example when n equals to five when there are five point correspondences and in this particular case uh we uh we will end up to have a system of polynomial equations that is in this form a uh t equals to 0 where t here still remain as a 5 by 1 vector that consists of x that we saw earlier on where x here is simply equals to s 1 square. equation we can stack them all up so one two all the way to six equations and we end up with a coefficient matrix with an a matrix of of dimension 6 by 5. in order for non-trivial solution to exist then this guy here better be of a maximum of rank four so what we can do here is that we can take the svd of a and this will give us u sigma v transpose where we simply the vector that corresponds to the least singular value in sigma. The epmp algorithm mitigates the problem of the linear endpoint algorithm that was shown earlier on by quan and lun that was published in the year 1999. The algorithm has cubic complexity in the order of the number of points that is used to form a. The cubic complexity here becomes a limiting factor for us to apply for this particular algorithm to a number of point correspondence which is significantly large now in 2006 the paper published by vincent lapati which is called the epMP algorithm was first published. proposed by lapati in the year 2006 is that instead of using every single point correspondences that is given to us the the core idea is that here we'll make use of all these points to define four control points. Even for a very large n the number of control points still stay constant as uh fall. So now this control point becomes our unknown so that we also need to solve for in addition to the camera rotation and translation where these 3d points they are given and they are known.

ROUGE-1: 18.38, ROUGE-2: 17.84, ROUGE-L: 17.75
BERTScore: 68.58

==============================================
==================== [33/100] ====================
Summary:
Time dilation is a change in the light frequency. If there is a time dilation effect due to gravitational fields, then there's also a redshift which is of gravitational fields. The way to think about this is first to say, OK, now the light-- the delta t equals the light to travel-- is l divided by c. The change in velocity is g, acceleration. So the Doppler shift then is c divided by l, which is the speed of light. And I would like you to get a feeling. How big can this effect be, the effect of redshift here? the frequency, the new frequency, divided by the initial frequency. And that can be approximated by 1 plus delta v over c. So we find that it's 1 plus g times l over c square. Now the speed of light is pretty fast, 3 times 10 to the 9 meter per second. And this distance is only 22 and 1/2 meters. But nevertheless, experimentalists at Harvard tested this effect. So Pound, Rebka, and Snider in the 1950s and '60s were able to show this very tiny effect.

ROUGE-1: 72.00, ROUGE-2: 68.07, ROUGE-L: 57.52
BERTScore: 81.34

==============================================
==================== [34/100] ====================
Summary:
During the semester we have a few recitation instructors they help with the students during the recitation section. During those sections your the the instructor will solve a similar problem like what is actually covered during the same during the lecture and that give the students another chance to look at more example and to get for media will get used to the calculation which we carry how for the first time during the the lecture. We did not record the Recitation sections during the fall semester in 2016 on the other hand we included problem-solving videos from Professor with Busha.

ROUGE-1: 67.77, ROUGE-2: 66.22, ROUGE-L: 67.77
BERTScore: 85.45

==============================================
==================== [35/100] ====================
Summary:
In this video, we're going to compute some useful quantities for the exponential random variable. The CDF of x is the probability that X is less than or equal to little x. We use the standard formula, which is minus infinity to infinity t times fx of t dt. For this, if you evaluate the balance, 0 makes this 0, and 0 to get 1 over. And so the expectation is 1 over lambda, and so the variance is part c, so OK, so far so good. The limit as x goes to infinity-- the exponential will beat x squared. We're asked for the PDF of z, which is the max of x1, x2, and x2. The probability of the max being less than or equal to z is actually also the probability of each of these random variables individually beingLess than orequal to z. The trick is to flip it and say we want to compute the min of x 1 x2 being greater than w. In that case, let's check if we can do this trick. If x1 and x 2 are individually bigger than w, then the min's also bigger. So it's going to be-- Notice the similarity between this and this. The only difference is this has a 2 lambda in there. That means that w is an exponential random variable with rate 2 Lambda. You can also take the derivative of this and find that you get this. OK, so we're done with the problems. We computed some interesting quantities for the exponentialRandom variable in this. So then the PDF isgoing to be an exponential, whatever it is for an exponential.

ROUGE-1: 24.71, ROUGE-2: 22.49, ROUGE-L: 22.83
BERTScore: 69.78

==============================================
==================== [36/100] ====================
Summary:
Professor: Can you explain the physical significance of the crystal momentum? Professor: Let me answer that in a slightly backward way. Let's step back and think about the momentum, and ask what the momentum is. Professor: If you have a wave function, sine of x, such that, the expectation value of the wave function is x, then the momentum of that function is 1/x. The momentum is the same as the speed of light, which is the speed at which light travels through the air. in the state SI of x in the stateSI is equal to x naught, and the expectation value of p in SI is p naught. Then if you want to change the momentum, increase momentum by h bar k, the way to do that is to take SI and build a new wave function, SI tilda. So all the intuition you have about momentum, you can translate into intuition about the spatial variation of the phase of the wave function. So let's turn all these facts around into the crystal momentum. just constantly increases. For the crystal momentum, that's not the case. You turn on a force, it increases according to the conservation law. But it's not increasing constantly. It's periodically defined. So it increases then it ends up at a smaller value. It increases and ends up in a phase. So developing an intuition for the crystal. momentum, I think, is best done by just playing with examples. And you'll do that more in the course on solids, which I encourage you all to take. Because it's really beautiful stuff. Glossing over in the entire story here. Which of the following. So, is u of x a real function? Well, so when we started out asking what are the eigenfunctions of the transit by l operator, all we showed was that, and I'm going to do this on a separate board just to make it clearer. Tell me if this turns off, because it kept bumping. OK. So we've determined is that if we take q l is equal to alpha, then Phi sub q if eigenvalue label by its eigen value, q, can be written in the form e to the i q x u sub q of x. Professor: "I need to do this carefully, because it's incredibly difficult to get the straight" "I have a hard time drawing these things" "So for every value of q, there's an allowed energy that's different than what it would have been for the free particle" "If we wanted to put it in the fundamental domain, this is what we get" "You can't stop me. You hand me a function, I will hand you a different function" "This is what the first band and the second band together look like" to plot u with respect to k instead, would that just be a parabola dotted line? If so, why do we not have really-- PROFESSOR: If we just wanted-- sorry. Say it again? AUDIENCE: E as a function of k instead of q. PROFessor: Oh. But k is not a well-- so what is k? K is just defined as h bar squared, k squared upon 2 m is equal to e. So this doesn't tell you anything. This diagram is telling you is which e's are allowed. If you put on a capacitor, played across your perfect lattice, you don't get any current. So the particle, the charged particle in your lattice just oscillates back and forth in a block oscillation. And that is manifestly what happens with copper. But the experimentalist comes back to you and says look dude. That is a ridiculous model because the copper isn't in fact perfect, it's messy. So how do you test the model? Well there are two ways to test the situation. One is you improve the model to incorporate properties that copper actually has. And see if you can actually get the same conductivity. The equation for light going through a dielectric can be put in exactly the same form as the Schrodinger equation for the time evolution of a wave function. And so you can build a system which incredibly, cleanly, has a periodic dielectrics constant and no disorder. The most elegant experiment that I know of was done by Wolfgang Ketterle, who's here at MIT. And he got three data points because it was preposterously difficult and declared victory. But it really needs to be done well. this part of the field right now is we know that it's true, but we want to see it. We want to feel it, so various people around the world are working on making a truly beautiful demonstration of this bit of physics. But, the basic question is how robust is this. And the answer is it's not robust at all. But which you can tell because everything in the real world has enough impurity that it conducts. Or as an insulator. Yeah. And that's actually, it depends on the lattice. situation, it depends on the system. And exactly how it depends is something that is an active area of research. So don't throw away the model. Observe that you've modeled the wrong system. If you find a system that fits your-- that is-- that shares the assumptions of your model, that's when you ask did it work. And it worked like a champ. OK. So now let's talk about real materials. This is going to close up our discussion bands and solids. But that's OK. There are lots of questions and they were good questions. In order for the system to be neutral, I must have one electron for every well. In order to induce the current, I. must put the electron into a higher energy state and in a particular superposition of higher energy states. If I put in the n electrons I need to neutralize a system, where do those n electrons go? Yeah, they fill up the first band. And if we let the system relax with the lowest energy state in this band, every state will be filled. In order for light to scatter off a crystal, you must have electrons in superposition states so that they can have a dipole and absorb and radiate that energy. In order for that to happen, the light has to excite an electron across the gap. Light along wavelength will not have enough energy toexcite an electrons across this gap into the next band to allow there to be a current, which could oppose the electric field. So we have these two materials which one has the larger band gap? Diamond, because it's transparent.  spin in one dimension is little-- I'm lying about spin. Electrons spin up, and electrons spin down, will generically have different energies. In 3D, this isn't such a big deal, because those splittings are tiny. But in 1D they can't. So I mean, that's also not exactly true, but it depends on exactly the details of the system, is what I wanted to get to. Curse you. But do you really want me to get in spin? Man. In three dimensions, you guys did an interesting thing, when you studied, you didn't know this was about the structure of solids, but it really was. When you studied the rigid rotor, you found that you had energy eigenstates and they were degenerate with degeneracy 2 l plus 1. And what you found is that these guys split. There's one that changes, one that doesn't. And then this guy has five. One, two, three, four, five. is the length of the energy of the last electron that you put in. How much energy do you have to give the system, do you. have to add the system to excite the electrons into excited states, in. particular into superpositions so that the electrons can move? AUDIENCE: [INAUDIBLE] PROFESSOR: Yeah. Preposterously small amount. An amount that goes like one over the number of particles. So in the continuum limit, it's zero. There's an arbitrarily nearby energy. called a band insulator. Because there are other ways of being an insulators. So what determined the exact band structure in for a 1D periodic potential? Two properties. One was l, the periodicity. And that came in the q l and k l. And the second is the detailed shape of the potential. Now in three dimensions, the things that are going to determine the potential are not just the distance between atoms, but you have a three dimensional lattice. And so when you solve the problem for the energy eigenvalues is a function of now the three different components of the crystal momentum, you'll get a different set of equations. the atomic orbital structure of the individual atom, the crystal structure, and the resulting band structure. You will almost always find overlapping bands in three dimensions in sufficiently high energy. What we need is one of two things. We need either the band gap coincidentally is ridiculously small, or we need a free particle that has no band gaps at all. That's a good example of that? A free particle, these band gaps go to 0. And so that's a conductor. Just an electron. It conducts, right? OK. The temperature controls an energy scale for a real material. If you have a hot piece of copper, then the lattice is wiggling around. And every once in a while, an ion can hit one of the electrons and excite it, give it some momentum. And so there's an available reservoir of energy for exciting individual electrons. So when I say small, that doesn't mean anything. I need to tell you small compared to what.what that means. OK. Delta e is very small. Now delta e has dimensions. It has units. excited above the gap. And now it's in a super-- and generically, it's going to be in asuperposition state of one of these excited states. It can radiate. It will eventually fall back down. The sea of electrons is constantly being buffeted by this thermal fluctuation. And as a result, you constantly have electrons being excited up, cruising around, falling back down, so you end up with some population of electrons. And they can ask-- and both when asked, although not quite in this language, how likely are you to get an electron up here? Photonic crystals are periodic arrays of dielectrics. They have bands of allowed energy and gaps of disallowed energies where no waves propagate through. The structure of a photonic crystal on the surface of a butterfly wing makes it shiny and blue. It looks like it's a crystal reflecting in a specific frequency. At some sharp blue. And the reason is, it is exactly this form. If you look at it under a microscope, you see little rays of protein which have different dielectric than air. and metallic without actually being shiny and metallic. And it's not a pigment, so it doesn't absorb light and decay over time. It's like the best thing you could ever do if you wanted to be a shiny, fluttery, flying thing. OK. So that's it for band gaps. And I want to move on to the remainder, the last topic of our course. Which is going to be entanglement and quantum computation. And here I need to give you one quick observation. The probability of finding the particle at point A is given by chi a squared. This is normalized, so when we integrate against it, we get 1. And similarly, the probability that we find the second particle at b is this thing norm squared. And it's independent of what a is. But we also studied the symmetric configuration, which was equal to 1 over root phi, root 2. And this tells us something totally awesome. We either find it at chi of a or chi. So there's a factor of one half. Quantum physicist David Wheeler: Measurement of one particle tells you something about the second particle. Wheeler: I could've taken these particles, put them in this entangled state, and sent one particle off to a distant planet. And my sister measures this second particle and determines what state it's in and is immediately determined what state the first particle is in over in this distant planet Zorg, right? So that's deeply disconcerting, he says, to those of us who have studied quantum mechanics up to this point. Einstein created a thought experiment which we're going to study in detail next week called the EPR experiment. There's a beautiful historical story about the setting and the meaning and the particular person. Einstein moved to Princeton in 1933. He left Germany in 1932 December, about three weeks before Hitler took power. And he did so with decisiveness and dispatch and a head of almost all of his-- in fact, I think all his German-Jewish physicist colleagues and those German physicists for whom the Hitler regime was unacceptable. Hard to find. It's really sweet. Jeremy Bernstein, who is a physicist. A physicist and writer. He's in his eighties now. He lives in Aspen. He worked with CERN for a number of years. He wrote for the New Yorker. Bell had a friend named, I think, Bartelstein. Who had two quirks. An unusual color sense and a taste for mismatched socks. Bell used to say, if you saw one leg and that sock was pink, you knew to a certainty that the other sock was not pink. I think-- I'm trying to remember who this is originally attributed to. Same thing. If you have a coin and you cut it in half down the-- so you've got two coin shape disks. And you see the head that you know somebody-- somebody at some other casino is cheating by tossing in the half coin that only has a tail on it. So there are lots of ways to represent this. And many physicists being very witty indeed have come up with different metaphors for it. if you're sort of approaching it naively. What Einstein, Podolsky, and Rosen argued was actually something a little bit-- in fact, the paper comes to an end on that note of queasiness. And they have definition for what reality is. And that is something whose-- if you can perform a measurement, you know that quantity absolutely. But you can't do the-- so on the one hand, quantum mechanics says you can‚Äôt know physical reality to this level of precision. they claimed was a paradox. And this paper was published. And it received a range of reactions from indifference by younger physicists who said, we don't care that it's weird. And most notably Niels Bohr found this paper really troubling. And he spent about six weeks, apparently, discussing this and trying to come up with a response to it. And what he responded was essentially that-- in some ways, it was in some way a paradox, as well as a good thing. Einstein had argued that the EPR paradox suggested that quantum mechanics was incomplete. Bohr essentially responded in effect that Einstein's description of quantum mechanical explanation was inadequate. The experiments were done, and I imagine are still being done, as sort of demonstrations. And they showed that Bohr's interpretation was correct and that yes, quantum mechanics produces results that are non-local just as Allan described to you. And that the world really is as strange as people first glimpsed in 1925, '26, and '27.

ROUGE-1: 31.08, ROUGE-2: 29.65, ROUGE-L: 29.35
BERTScore: 68.14

==============================================
==================== [37/100] ====================
Summary:
First up the proper way to chop fresh herbs to get maximum flavor chopping herb the secret is to chop them not bruised them now basil this is a soft herb so treat it with some respect when people go mad chopping herbs all the goodness comes out on the board I want the goodness left inside the basil. How to cut the perfect pepper stalk off pepper down and get the knife start from the top and slice all the way around basically we're gonna be slicing around the seeds look no faffing around but this isn't perfect Christmas tree of seeds and have got the mess all over your board.

ROUGE-1: 24.84, ROUGE-2: 24.45, ROUGE-L: 24.84
BERTScore: 63.64

==============================================
==================== [38/100] ====================
Summary:
hey everyone it's sth register nurse rn.com and in this video we're going to be going over our weekly inlex practice question. Let's see what our question says a patient who has a health history of uncontrolled hypertension coronary artery disease and diabetes militis is prescribed to take propanolol. The question asks which statement by the patient is correct about this medication propano wal that they're Going to be taking. The answer is that the patient should take the medication every morning with grapefruit juice and monitor their blood glucose level closely. look and see why D is wrong this patient says that they're going to stop their beta blocker immediately if they experience cold hands and feet well this is a normal side effect with these non-selective beta blockers. You would never just immediately stop taking a beta blocker they need to be tapered off of this because if they just all of a sudden quit taking that medication they can go have cardiac death or something worse can happen so that answer is wrong for that reason okay so that wraps up this inlex practice question.

ROUGE-1: 23.76, ROUGE-2: 21.83, ROUGE-L: 23.39
BERTScore: 68.65

==============================================
==================== [39/100] ====================
Summary:
This is part 2 of a guide to clinical reasoning or how to create an accurate differential diagnosis from a patient's presentation. In the first part I reviewed a practical five-step bedside approach to clinical Reasoning. In this part I will demonstrate how to use this approach with an actual patient case at the student level. I present this patient to you the same way in turn might present the patient to his or her attending on rounds or to their colleagues during a morning report or teaching conference as I present the case I'll keep a running list of the key features of the presentation. The patient is a 75 year old woman presenting with epigastric pain for four hours. The pain is relatively well localized to the midline in the region between her ankus and xiphoid process. There did not seem to be any particular trigger and the duration from initial onset to its maximal intensity of 8 out of 10 was about 45 minutes. The patient has refused to attempt to eat or drink anything since the pains onset because she is concerned that it will concert a vomit which she has not yet done. The patient is a 75 year old woman with abdominal pain for four hours it is epigastric well localized progressed over 45 minutes is constant with no exacerbating or alleviating factors and associated with nausea from the rest of her history. She has hypertension diabetes moderate alcohol use smoking and is drinking water from a newly drilled well on exam she is MA in moderate distress she has a borderline temperature tachycardia and tachypnea. The next step is creating the problem representation using precise medical terminology of the most highly relevant aspects of the patient's history exam and diagnostic tests. There is PUD in the absence of an ulcer perforation there is not a very reliable means of distinguishing PUD from gastritis on clinical grounds. She simply sounds just a little bit too sick for this diagnosis then there is food poisoning and gastroenteritis. Wiles certainly can become contaminated with enteric bacteria but for this to happen with a brand new well it would imply that they literally drilled it into a patch of pre-inoculated earth. Her cardiovascular risk factors put her at risk of this. A pulmonary embolism can cause abdominal pain but as with acs would not be expected to cause abdominal tenderness. The patient has no major risk factors for RPE and has no shortness of breath. There are no causes of four hours of epigastric pain in which we would expect the troponin or CK to be abnormal even in the event of RPE. It's possible to present with a PE with just pain this case is just not what a PE looks like either in its classic presentation or even atypical variations. that the patient had an acute MI it really is too soon for these enzymes to become elevated and so therefore they probably should not be key features. The guaiac negative stool was critical in establishing the problem representation in step three. The history of the newly drilled well depending on where you are in your training is the most interesting. The combination of acute abdominal pain nausea and a possible contamination of well water is all consistent with heavy-metal poisoning specifically arsenic and lead this brings up an important point. element does not fit into the framework yet still seems to be a key feature the framework must be incomplete in this case I would add another category of diagnosis to our four existing categories of epigastric right upper quadrant left upper quadant and chest that fifth category is acute abdominal pain secondary to systemic toxic metabolic problems. The four major members of this group are heavy-metal poisoning a rare genetic disorder called acute intermittent porphyria another virgin etic disorder called familial Mediterranean fever and finally angioedema for any of these to be the final diagnosis this patient would need to have an atypical presentation of a rare disease.

ROUGE-1: 26.02, ROUGE-2: 25.08, ROUGE-L: 24.38
BERTScore: 69.83

==============================================
==================== [40/100] ====================
Summary:
Today is the day that you have to have done the mid-quarter survey by. Hundreds of people have, but if you haven't, this is your last chance to get the half-point for that. Final project proposals are due. We really encourage you to try and hand them in on-time or nearly on- time. And then today, delighted to have our first invited speaker. And there is a reaction paragraph talking about something that the speaker talks about. There is also assignment 5, which we're giving you one extra day for. Danqi Chen is one of the foremost researchers in question answering. She was the head TA of CS224N once upon a time. So she's quite familiar with the context of this class. Here's my plan for this lecture. So first, I would give a brief introduction of what is question answering, and what kind of problems that people are studying today. Then I'm going to spend the most of this lecture focused on one type of question answering problems called reading comprehension. And at the end of the lecture, I'm hoping to spend hopefully like 10-ish minutes to talk about a more practical, and in my opinion, more exciting problem called open domain question Answer. able to handle more complex questions like how-to questions. People actually really like to ask questions on these digital assistants. Ask a question is actually the second most used case. Only ranks after listening to music and before the check the weather and set up. The best way to prevent illness is to avoid being exposed to this virus. And to help prevent the spread of COVID-19, you can do the following. If you just click this link and read through the article. So this is also one type of the question answering problems. Question answering is probably one of those fields that we have seen the most remarkable progress in the last couple of years driven by deep learning. In this lecture, I will be mostly focusing on the text based, or textual question answering problems. And another class, bigger class of the question Answer problems is called visual question answering. So this problem basically requires both understanding of the questions and also images, and is actually a very active field between the computer vision and NLP. So if you have interest in these type of problems, I encourage you to check out those problems, but I'm not going to go into them. dig into these problems today. So next, I'm going to start with a part 2, reading comprehension. I just want to quickly check if there are any quick questions I can answer before I start us on part 2. OK. So let's talk about the reading comprehension then. So reading comprehension is a basic problem that we want to comprehend a passage of text and answer questions about the content. So here is one example. So basically to answer this question, so you need to find this sentence, like, in 1861, Tesla attended this school where he studied. he studied German, arithmetic, and religion, and only German is a language. So the answer to this question should be German. OK, here is another example, OK? Another passage of text. And the question is, which linguistic minority is larger, Hindi or Malayalam I think 5 seconds. OK. So next I'm going to talk a little bit so why do we care about this problem? So why do you care about the reading comprehension problem? It has actually many useful real-world practical applications. Reading comprehension has been also viewed as a very important test bed for evaluating how well computer systems understand human language. So this is really just similar to how we humans actually test the reading comprehension test to evaluate how well we actually understand one language. And also there is another interesting and important reason that reading comprehension is important. So in recent few years, some researchers actually found that, OK, well, there are many other NLP tasks. So we also reduce them to a reading comprehension problem. Stanford Question Answering Dataset is a supervised reading comprehension dataset. It consists of 100K annotated passage question and answer triples. Each answer is a short segment of text, or we called it span in the passage. And this kind of a large scale supervised dataset are also very key ingredient for training the effective neural models for reading comprehension. But also just to caveat, that this is also a limitation. Because not all questions have the answers in this way. But today, so this dataset, I forgot to say that. This dataset was collected in 2016 by several researchers at Stanford. Stanford, so it's called Stanford Question Answering Dataset. Today, after four or five years now, so SQuAD still remains the most popular reading comprehension data set. So it's actually very clearly a high quality dataset, but is also not a very difficult dataset. So today, basically the SQuad dataset itself has been almost solved, and the state-of-the-art already exceeds estimated human performance. So we can see that there is an exact match score between the predicted answer and any of the gold answers. the F1 score would be taking the max. Danqi, one question you might answer is, so if you can do other tasks like named entity recognition or relation extraction by sticking something on top of BERT and fine tuning for it or do it as a question answering, does one or the other method work better and by how much? That's an interesting question. So I haven't really seen the-- OK. So there has been some claim, OK, that all the tasks can be converted into question answering task. But I'm not sure if there is really a very fair comparison. answer to that. So next, I'm going to talk about how to build neural models for reading comprehension, and in particular, how we can build a model to solve the Stanford Question Answering Dataset. So the input of this problem is let's take a context or paragraph. And also we take our question, Q. And the question consists of n tokens q1 to qN. And because the answer has these constraints, the answer must be a section of text in the passage. We are going to predict a start and then end. just not with us using the both directions. In the bottom right, we sum over i, so why does the i remain in bi? Is that correct or is that a typo there? This is another typo. So the output of gi will actually range from the 1 to N, which is the number of the context words. There are lots of questions about this. What is the rationale for the expression of the gi? How does one come up with such an expression? OK, OK, I'm sorry. The attention layer is basically modeling the interactions between the query and context. The query-to-context attention is trying to measure the importance of these context words with respect to some question words. The final output layers are basically just two classifiers just trying to predict the start and end positions. And the whole BiDAF model can be just trained in an online training set, in terms of the log likelihood of the gold answer and the end position of the answer. It's basically taking the product of these two probabilities, but it is the sum of the two negative probabilities. the models are actually a very similar ballpark. So numbers range from the highest number here, 79.8 until after the ELMo was introduced, the numbers have actually improved quite a bit. Each model actually improved the previous model by one point or two points. And now here is our attention visualization to show that how this smorgasbord of attention actually can capture the similarity between the question words and the context words. So it show the actual question word here. And each column's matrix basically indicates the attention score, the similarity score that has been learned by this model. BERT is a deep bidirectional transformer encoder pre-trained on large amounts of text. It is trained on the two training objectives, including masked language modeling and the next sentence prediction. The BERTbase has 110 million parameters and the BERT-large model has 330 million parameters. If you just take this BERT model, and by just optimizing all the parameters together, it can give you a very high performance. And even if you use a stronger pre-training models, they can even lead to better performance on SQuAD. pre-training has been so important. So next I will quickly talk about-- OK, a question here is that can we actually even design better pre-training objectives for reading comprehension or question answering? And the answer is actually yes. So this is actually a work I did with Mandar Joshi and other folks one year ago called SpanBERT. So for SQuAD and other a lot of extractable reading comprehension datasets, the goal is trying to predict the answer span from the passage of the question. a lot of evidence showing that the current systems still perform poorly on adversarial examples or the examples from out of domain distributions. So today we compute a very good reading comprehension data set on the individual data sets. But these systems trained on one dataset basically cannot really generalize to other datasets. So I have 10 minutes left. Chris, is there any question I should answer at this point here? I think you can go on. OK. So in the last 10 minutes, I'm going to give you a very, very brief introduction of what is open-domain question answering and what we have been trying to do. a large collection of documents. So one example is just taking the whole Wikipedia, which has five million articles. And we're going to return the answer for any open-domain questions. So this problem, there isn't any single passage, so we have to answer questions against a very largeCollection of documents or even the whole web documents. This is actually a much more challenging and also more practical problem. So if you look at the example of Google example I showed at the beginning, so these techniques will be very useful in the practical applications. Today, because she doesn't have a Stanford login, we're going to do questions inside Zoom. So if you'd like to ask a question, if you use the raise hand button, we can promote you. And if you hang around and don't leave the Zoom for more than a few minutes, maybe we'll just promote everybody who's still there into people in the regular Zoom for some bits of discussion. There are now four people who've been promoted. OK. Should I read those questions? Should I look at the chat or? No. Most existing question answering datasets or reading comprehension datasets have been collected from Mechanical Turk. So it is very difficult to avoid some kind of artifact though, like a simple clues or superficial clues-- let's say not superficial but some simple clues that are for the machines to pick up. So that's the reason that more specialized models that have been trained very well in one data set, it's very hard to generalize this kind of thing to another data set. So if you do the remnant questions in your train, dev, and test set, there's a debate of just debate of it's inevitable that it's going to overlap. in the training set that is not really generalization, right? Yeah, but this is more on the open-domain setting not in the really [INAUDIBLE] here. Do you want to ask a question? Yes. So you mentioned in the last part of the presentation that the reader model may not be necessary and you presented the DensePhrases which also work well on CPUs. So do we know how well it performs on the question and answering datasets compared to other models including BERT and those on computer of course. The goal of this project is trying to ingest all the phrases in the Wikipedia. So these conversations are built using the training set of the question answering datasets. So if we use say a different data set that does not present the information using the structure presented in Wikipedias, this model may not work as well as. what do you mean by structure represent? So say if we lean more towards structures like the passages we see in standardized tests where the answers to the question may not be in close proximity to where the information was first introduced. while asking a question if you want. So next person is [AUDIO OUT]. All right. Thank you for taking the time to teach this. My question is kind of quick. So you mentioned work, they brought up a set of relatively simple questions that show how brittle or poor the current models can be, right? I'm curious if that-- yeah, exactly. Did that kind of change the community to improve how to evaluate the models? Because they're actually doing pretty poorly on some of those. Next question is about the future of NLP. Do you think that in order to solve NLP in the sense that you can perform on par with humans on all NLP tasks it's sufficient to only interact with text data? Or do you think we'll eventually need the sort of experiences and common sense that. you get only from seeing and viewing the world and having a set of interactions that we as humans have? Yeah. I mean, common sense is a very difficult-- even in the context question answering. So all these things need to be resolved. encountered this paper called Learnable Quantizers, which essentially learns baseless representation for the quantizers jointly with the leads of the network. And while this would be extremely effective if you were to just like, say, train from scratch, I was just sort of curious, do you think there is some way to do this say a pre-trained BERT model or something like that? I had a few ideas with like beam search for instance, but I don't see a very clear way of doing that. Next question is from Danqi, who was one of the co-organizers of the EfficientQA task. Danqi: How concerned should we be about potential encoding sort of biases into these record labels or how we evaluate them, or is that just more of a concern for more open ended questions? John: I'm not sure I have a good answer to that. Some people do a de-biasing of the pre-trained language models, all these things are very important. yeah, I guess I'm just a little worried about who comes up with the test cases? Who determines what the right answer is? I mean, we will have more discussion of toxicity and bias coming up very soon, including actually Thursday's lecture as well as a later lecture, not specifically about QA though. OK. Next person is-- Thank you for the lecture. Yeah, my question is also related to the open domain question answering. So there is some very specific designs like domain server alignments and efficient level disentanglement techniques that has shown some interesting performance on other tasks. definitely an interesting point. At least for the work that I have seen so far, it all applied or operated at a very simple sentence classification task. I feel like QA is a more structured task and also handles longer bar sequences. Yeah, so I don't know if it works unless people have tried that. OK then we've got-- and maybe we should call this the last question. Hi, I'm just wondering what is the intrinsic difference between solving question answering with generative models like T5 versus encoders like BERT. The key difference between the generative model and the extractive model is that for generative models, you can actually leverage more input passage together and do the generation. So for this model, there isn't any retrieval. So this model really has you relying on all the parameters you memorized. So by just taking this input, it has to just rely on the parameters to infer this answer. So it's actually very hard to-- yeah, it's a definite balance between memory and the generalization from it, yeah.

ROUGE-1: 28.41, ROUGE-2: 27.51, ROUGE-L: 27.29
BERTScore: 69.14

==============================================
==================== [41/100] ====================
Summary:
 homework two is out now. This weekend sessions will be having some more background on deep learning. We're also gonna be reaching, uh, releasing by the end of tomorrow, what the default projects will be for this class. Um, and those proposals will be due, um, very soon, er, in a little over a week. The assignments, [inaudible] are they limited to TensorFlow? asked the question if, if the assignment is limited toTensorFlow. I'm pretty sure that everything relies that you're using Tensor Flow. Deep Q Learning is a form of reinforcement learning. It uses deep neural networks to learn to make complex decisions. We'll be covering the basics of Deep Q Learning in this tutorial. You can look at the tutorial, or you can reach out to us on Piazza with any other questions. We hope to see you in the next class on November 14th and 15th, at 10:30am and 11:00am. Back to the page you came from. Click here for more information about the class. when we thought about doing this, we're gonna focus on function approximations that are differentiable. Um, and the nice thing about differentiable rep-representations is that we can use our data, and we can estimate our parameters, and then we can take use gradient descent to try to fit our function. And that information now could be [NOISE] in the form of episodes or it could be individual tuples. When I say a tuple, I generally mean a state-action reward next state tuple. is the same as the full gradient update. And the key hard thing was that we don't know what this is. So, the two ways we talked about last time was inspired by a work on Monte Carlo, or on TD learning. We could either plug-in the return from the full episode. This is the sum of her words. Or we could put in a bootstrapped return. Where we look at the reward, the next state, and the value of our next state. all for linear value function approximation, but there are some limitations to use the linear valuefunction approximation, even though this has been probably the most well-studied. So, if you have the right set of features, and historically there was a lot of work on figuring out what those rights set offeatures are. They often worked really well. And in fact when we get into, I think I mentioned briefly before. When we start to talk about deep neural networks you can think of, a deep neural network is just a really complicated way to get out features. Non-parametric approaches, where your representation size tends to grow with the number of data points. These have a lot stronger convergence results compared to linear value function approximators. But they're not gonna scale very well and in practice you don't tend to see them, though there's some really cool work by my colleague, Finale Doshi-Velez, over at Harvard who's thinking about using them in the future. The number of points you need to tile that 180 degrees space, generally scales exponentially with the dimension. These for things like, um, health care applications and how do you sort of generalized from related patients. So, they can be useful but they generally don't scale so well. What we're gonna talk about today is thinking about deep neural networks which also have very flexible representations but we hope we'll be able to scale a lot better. Um, now, in general we're going to have almost no theoretical guarantees for the rest of the day, and- but in practice they often work really really well. Deep neural networks are artificial connections with artificial neural networks inside our brain. They are used in unsupervised learning like predicting whether or not something is a cat or not or, you know, an image, uh, of a particular object, um, or for regression. They combine both linear and non-linear transformations, and they need to be differentiable if gradient descent is to be used to fit them. In the last 5 to 8 years, there's auto differentiation. So, you don't have to derive all of these gradients by hand instead. Convolutional neural networks are used extensively in computer vision. They can represent any function with the deep neural network. You can use exponentially less nodes or parameters compared to using a shallow net which means not as many of those compositions, um, to represent the same function. The final thing is that you can learn the parameters using stochastic gradient descent in like five seconds. It's very likely that we're going to want to use similar sorts of input on our- our robots in our artificial agents. for really high dimensional input and kind of average and slow down until we can get to, um, a low dimensional output. So, the final layer is typically fully connected. We're kind of computing this new feature representation of the image, and at the very end, we can take some fully connected layer, where it's like doing linear regression, and use that to output predictions or scalars. So these type of representations, both Deep Neural Networks and Convolutional Neural Networks, are both used extensively in deep reinforcement learning. how we could use these type of approximations for Atari. And then we had the results that were kind of happening around like 1995 to maybe like 1998, which said that, "Function approximation plus offline off policy control, plus bootstrapping can be bad, can fail to converge" So I think for a long time after that, the, the community was sort of backed away from Deep Neural Networks for a while. And so, perhaps it was natural that, like, around in like 2014, DeepMind and DeepMind combined them and had some really amazing successes with Atari. I think there's also a couple algorithmic ideas that we're gonna see later in this lecture, that help the performance kind of avoid some of those convergence problems. So with the Atari case specifically, did you- did you avoid that problem? Well, sort of, that if you tried it by having on policy control? I just don't know. Um, and it's a great question for me. We'll see how it works here. Anyone else? Okay, cool. So, we'll- we'll see an example for breakout shortly, um, of what they did. are the important things that they, um, did in their paper, this is a nature paper from 2015, is they use the same architecture and hyperparameters across all games. Now just to be clear, they're gonna then learn different Q functions and different policies for each game. But their point was that they didn't have to use totally different architectures, do totally different hyperparameter tuning for every single game separately. It really was the sort of general architecture and setup was sufficient for them to be able to learn to make good decisions for all of the games. DQN, deep Q-learning addresses these is by experienced replay and fixed Q-targets. Experienced replay, prime number if you guys have heard about this, if you learned about DQN before is we're just gonna stroll data. So this is nice because basically it means that you reuse your data instead of just using each data point once, you can reuse it and that can be helpful. And what we'll look at that more in a minute. [inaudible] It's a great question which is this equivalent to keeping more frames in our representation? It's not that interesting. I am in the world, I'm now in state four. It's like I suddenly pretend that I'm back in s1, took a1, got r1, and went to s2 and I'm gonna update my weights again. The reason that that update will be different than before is because I've now updated using my second update and my third update. So, it'll cause a different weight update. In general, one thing we talked about a long time ago is that if you, um, uh, do TD learning to converge it, which means that you go over your data mu- like, like, an infinite amount of time. learn to model, a dynamics model, and then the planning for that which is pretty cool. But we don't wanna do that all the time because there's a computation trade-off and particularly here because we're in games. There's a directtrade-off between computation and getting more experience. Can we use something similar to like exploitation versus exploration. Um, essentially like with random probability just decide to re-flag [inaudible]. The question is about how would we choose between getting new data and how much to replay et cetera. based on the experience replay versus getting, um, putting new samples into there. So, generally right now is really heuristic trade-off. Could certainly imagine trying to optimally figure this out but that also requires computation. Um, but if, you know, your agent thinking about how to prioritize its own computation which is a super cool problem. Which is what we solve all the time. Okay. The second thing that DQN does is it first has- it first keeps route this old data. minus. I'll call it minus because, um, well there might be other conventions but in particular it's the older set of weights, the ones we're not updating right now. Those are the ones that we're using them as target calculation. So, when we compute our target value we, again, can sample and experience tuple from the dataset from our experience replay buffer, compute the target value using our w minus, and then we use stochastic gradient descent to update the network weights. help, um, in terms of stability? In terms of Stability, it helps because you're basically reducing the noise in your target. If you kept your target fixed forever, you would learn the weights that- that minimize the error to a constant function. That would then be stable because you always have the same target value that you're always trying to predict. And eventually you'd learn that, and that would eventually be stable. And that's what we're trying to do with GT. This is just reducing the noise and the target that we're trying to sort of, um, if you think of this as a supervised learning problem, we have an input x and output y. The challenge in RL is that our y is changing. If you make it that you're- so your y is not changing, it's much easier to fit. This is really just about stability and that's- that's true for the experience replay too. Experience replay is just kinda propagate information more- more effectively and this is just gonna make it more stable. It stores the transition in this sort of replay buffer, a replay memory, um, use sample random mini-batches from D. So, normally sample in mini-batch instead of a single one. You do your gradient descent given those. Um, you compute Q learning using these old targets and you optimize the mean squared error between the Q network and Q learning targets, use stochastic gradient descent, and something I did not mention on here is that we're typically doing E-greedy exploration. of what the agent is doing. So, remember the agent's just learning from pixels here how to do this. Um, and one of the interesting things about it is that as you'd hope, as it gets more and more data, it learns to make better decisions. So this is really cool that sort of it could discover things that maybe are strategies that people take a little while to learn when they're first learning the game as well. Yeah, so, um, you might see, uh, I think she is talking- she is referring to the fact that the paddle was moving a lot. clearly sort of an inexperienced player to do that. That would be a strange thing but from the agent's perspective, that's completely reasonable. Um, and it does not give him positive or negative reward from that. So, it can't distinguish between, you know, stay in stationary versus going left or right. If you put it in a cost for movement that could help. This might become a little bit of [inaudible] but is there a reason to introduce a pulling layer? Puling layer? There might be one in there. uh, they're not talking about how long it took them or their agent to learn and as you guys will find out for homework two, it can be a lot of experience. So, they did very well on some domains. Some domains, they doing very poorly. Um, I think that it's clear that the really important feature is replay. We'll probably talk- uh, we'll talk a lot more about exploration later on in the course. so, what was critical? So, I- I like the, uh, there's a lot  lovely things about this paper and one of the really nice things is that they did a nice ablation study. Replay is hugely important and it just gives us a much better way to use the data. Double DQN is kind of like double Q learning, which we covered very briefly at the end of a couple of classes ago. Greedy Policy is where we average networks, average reward networks, and then use one of the Qs as the target for the other network. Then we update Q2 with 50 percent probability, we pick the next action from the next network, this is a pretty small change. back to the Mars Rover example. So, let's say you get to choose two replay backups to do. Vote if you think it matters which ones you pick, in terms of the value function you get out. If you pick backup three, so what's backup three? It is, S2, A1, 0, S1. So that means now you're gonna get to backup and so now your V of S2 is gonna be equal to one. So you've got to back-propagate from the information you're already [NOISE] have on step one to step two. The number, of, um, updates you need to do until your value function converges to the right thing can be exponentially smaller. If you update carefully and you, you could have an oracle tells you exactly what to do. But you can't do that. You're not gonna spend all this. It- it's very computationally, expensive or impossible in some cases to figure out exactly what that. oracle ordering should be. But it does illustrate that we might wanna be careful about the order that we do it. The idea is that, if you want to, make decisions in the world, they're working some states are better or worse, but they're just gonna have higher value or lower value, but- figure out what the right action is. One method basically takes these priorities, raises them to some power alpha, um, and then normalizes. And then that's the probability, of selecting that tuples. So you prioritize more things that are weights. Doesn't freezing. in the old ways were a counter to propagating back the information there? It could be super tempting to try to start, by like implementing Q learning directly on the ATARI. Highly encourage you to first go through, sort of the order of the assignment and like, do the linear case, make sure your Q learning is totally working. Even with the smaller games, like Pong which we're working on, um, it is enormously time consuming. It's way better to make sure that you know your Q Learning method is working, before you wait, 12 hours to see. whether or not, oh it didn't learn anything on Pogge. So, that, that- there's a reason for why we, sort of build up, the way we do in the assignment. Um, another practical, to a few other practical tips, feel free to, to look at those, um, and then we were on Thursday. Thanks. Back to Mail Online home.back to the page you came from. Back from the page where you come from. back to MailOnline home.

ROUGE-1: 29.26, ROUGE-2: 27.57, ROUGE-L: 27.12
BERTScore: 65.67

==============================================
==================== [42/100] ====================
Summary:
 salt makes up a tiny part of any bread though which has a huge effect on it and most bread is made with salt nowadays. salt has a tightening effect on the gluten it strengthens the dough and makes it more cohesive as yeast consumes the sugars in the dough. salt helps with controlling fermentation it draws moisture through the cell walls of yeast in a process called osmosis. salt can help with preserving the color and flavor of flour unbleached flour has carotenoid pigments which give the crumb of our bread the creamy color and a wheaty aroma. starter or yeast you would mix your flour water yeast or starter and then leave to ferment for several hours ahead of time before making the final dough pre-ferments add a great deal of flavor improve the texture and the keeping quality of your bread. Normally brief mints don't contain any salt that's why in hot kitchens or hot climates they can ferment too rapidly. There are ways of controlling this you can lower the temperature of the briefment you can place it in a cooler area or even lower the hydration of it by adding salt. it's rising more slowly whilst the one on the left is already collapsing. the one in the right is still pushing on. what did you think of this experiment did you learn something new let me know down in the comments see more videos like this one click right here that's all i have for you today thank you for watching i'll see you in the next one. i'll be back with a new video in a week or so. I'll let you know what it's about.

ROUGE-1: 23.84, ROUGE-2: 22.13, ROUGE-L: 22.03
BERTScore: 59.98

==============================================
==================== [43/100] ====================
Summary:
In this section, we're going to talk about the relativistic Doppler effect. We make good use of our space-time diagrams, which we discussed earlier. The question is how is this being observed by an observer which is moving with a relative velocity v with respect to the source? So we have to apply Lorentz transformation. And we find then-- this is a little bit of an algebra exercise here-- that the period now is given by 1 plus beta over 1 minus beta square root of that times tau.

ROUGE-1: 32.62, ROUGE-2: 31.29, ROUGE-L: 32.62
BERTScore: 72.64

==============================================
==================== [44/100] ====================
Summary:
Hollywood Legend Will Smith owns a team in the new E1 racing series that aims to prove the potential of electric power in the Marine industry. The nine teams have the same boat but they're working out how to push the tech and try to get ahead of the competition. The boats can reach 50 knots that's around 93 km per hour so how do they reach those speeds? The key bit here is getting up on the thin bits of the foil and staying above the water to have the speed that's right. as we go there's no set plan and it completely depends on where we are during the race and yeah it can change in a matter of minutes [Music] at the end of the day you know the teams really have to work together both Pilots have to be consistent you know One Pilot can be really fast if the other one is slower then it may come down to the wire. Having two very consistent Pilots that can work together is how you're really going to win let's po that champagne [Applause] hope it continues for many years to come. We are proud to be a part of the history of the city. We hope to see it continue for many more years. Thank you for all your support and good wishes. We will continue to support the city in any way we can over the next few years. Back to Mail Online home.Back to the page you came from. Back To the pageyou came from, click here for more information on this story and to read the rest of the article.

ROUGE-1: 26.35, ROUGE-2: 20.82, ROUGE-L: 18.98
BERTScore: 59.93

==============================================
==================== [45/100] ====================
Summary:
Marginal rate of substitution of good 1 with respect to good 2 is infinity. When we are talking about marginal rate of. substitution of milk and cola, it would still be infinity, but marginal rate. of substitution. of cola withrespect to milk is 0.75. So, when we do not say when we say just MRS, we are not using any particular term, you can use both way. Instead of getting x you get 1 by x, and why? Let us see what do we mean by marginal rates of substitution mathematically. To get 1 good you will have to give up the other good, both what we are assuming that both these items are good, means they give certain satisfaction or certain you know utility to the person, and what we want. We want this person to have the same level of utility by consuming one of these two bundles. So, of course, when we are increasing the amount of 1 good to bring what will happen using this, if we use the monotonicity what will happening. If more of 1Good what is happening let us see. From here you are moving here in this direction. see, that MRS is given as a positive number, that only means that the author has introduced a negative sign here to convert the MRS into apositive number. So, it does not matter. Is it clear? Do you know the answer to this question? If so, please email us at jennifer.smith@mailonline.co.uk. If you don't, we would like to hear from you. Please send us a photo of your MRS and we will send it to you.

ROUGE-1: 33.95, ROUGE-2: 29.12, ROUGE-L: 29.87
BERTScore: 65.60

==============================================
==================== [46/100] ====================
Summary:
in this video we're going to discuss what externalities are in economics. An externality is when you do something that affects the well-being or the good of another person or a company but you're neither harmed or rewarded for what you did to that person so the externalities can be positive they can be negative. A negative exTERNality is where you've harmed someone you've done something to somehow impose a cost on someone or some some company or something and you haven't reimbursed that person. your lawn and you mow your lawn and stuff but you don't really spend a l a lot of time making your house look pretty now if your neighbor is trying to sell their house they have a for sale sign up they might appreciate if you went out and really did a great job maintaining your home they would really just love that because then when people come to see their house which is for sale that would increase the value of their home right if the neighboring properties like yours look really really nice then that would help them sell their home. where you have a negative externality like pollution or something like that it would be overs supplied relative to what is socially efficient or optimal. Where you have an over-supply of something like pollution, for example, you would have it over supplied in a way that would be socially efficient and optimal. For example, if you have pollution, it would have to be over supplied to avoid it becoming a problem. This would be a way to reduce the amount of pollution or other negative externalities in the environment.

ROUGE-1: 38.65, ROUGE-2: 33.09, ROUGE-L: 31.61
BERTScore: 65.98

==============================================
==================== [47/100] ====================
Summary:
Political philosophy is the oldest of the social sciences. It can boast a wealth of heavy hitters from Plato and Aristotle to Machiavelli, Hobbes, Hegel, Tocqueville, Nietzsche. The study of the great books or great thinkers of the past can easily degenerate into a kind of antiquarianism, into a sort of pedantry, says Steven Smith. Smith: Political philosophy is not just some kind of strange historical appendage attached to the trunk of political science; it is constitutive of its deepest problems. This course will be devoted to the study of those "academic scribblers" who have written books that continue to impress and create the forms of authority with which we are familiar. But one thing we should not do, right, is to approach these works as if they provide, somehow, answers to the problems of today. Rather, the great works provide us, so to speak, with a repository of fundamental or permanent questions that political scientists still continue to rely on in their work. Again, we still think in.influences, are usually the slave of some defunct economist. terms of the basic concepts and categories that were created for us long ago. Among the oldest and still most fundamental questions are: what is justice? What are the goals of a decent society? How should a citizen be educated? Why should I obey the law, and what are the limits, if any, to my obligation? And of course, the all important question, even though political philosophers and political scientists rarely pronounce it, namely, quid sit deus, what is God? Does he exist? The concept of the regime is perhaps the oldest and most fundamental of political ideas. Broadly speaking, a regime indicates a form of government, whether it is ruled by the one, a few, the many, or as more common, some mixture of these three ruling powers. Regimes are necessarily partisan, that is to say they instill certain loyalties and passions in the same way that one may feel partisanship to the New York Yankees or the Boston Red Sox, or to Yale. The study of regime politics is in part a study of the distinctive national character types that constitutes a citizen body. A regime is more than simply a set of formal structures and institutions, it consists of the entire way of life, the habits, customs, and sentiments that make a people what they are. The regime constitutes an ethos, that is to say a distinctive character, that nurtures distinctive human types. The study of regimes is inseparable from the study of political science, and we will consider this further over the term. see you back, and have a very good but thoughtful September 11^(th). See you back on 9/11/11. I'll see you back in a week or so. Thanks for your support and good wishes. I love you all, and see you on 11/11 /11. Back to Mail Online home. back to the page you came from. See you in a month or so, and I'll be back to see you in the U.S.

ROUGE-1: 20.60, ROUGE-2: 18.01, ROUGE-L: 18.42
BERTScore: 63.27

==============================================
==================== [48/100] ====================
Summary:
Ahern: So the exams are not graded. The TAs have exams of their own and I have told the TAs that, fingers crossed, I would like to have exams back sometime on Friday. That may not be until afternoon, I don't know at this point. But the aim is to get things back by Friday. When exams are available, what I will do is I will send an email out to the class announcing that they're available and announcing where to pick them up. exam. Ahern: I have never had an exam where I had fewer questions. There were maybe 10 questions I got on the exam and that was for a class of this side, really unusual. "I find most students are honest. I've only had a handful of situations where in this class, where I've had dishonesty as an issue," Ahern says. "There's just too many eyes here and not enough of our eyes," he says of his class. "So that's why I videotape those" Ahern: How many amino acids can form zwitterion? None. That's not trick. There are no amino acids, everything, so think about it. Ahern: What did you have in mind for the last question? The one about the Kcat? Oh,Yeah, yeah, yeah. okay. So the question, and by the way, I will post the key outside my door after we give the exams back. I don't post those now because students get all anxious until they see their exam. Ahern: How can you have two apparent Kcats? and the answer is it depends on how you calculate the concentration of the enzyme. If you take Vmax and divide it by the total concentration of enzyme, you will see a reduced Kcat compared to the uninhibited enzyme. Why? Because much of that concentration of. enzyme is not active, it's inhibited. However, if you take the inhibited out of it and you take. Vmax to divide it, you'll see exactly the same Kcat as if you have an uninhibited. enzyme. where time is sometimes a factor, and so I was going to have 3 of the longer answer questions and I decided not to do that and I made them shorter. Next term, the format of the exam changes in 451. Most people like 451 better because the maximum number of points on a question I think is 3. If you're going to spend a fair amount of time on something, it should be worth more points. Anybody hate the exam? You can say it. As I said, I don't...okay, there you go. same manner that the enzyme is binding a substrate. And so the parallels of those with respect to concentration and so forth, that's really the reason you see those two curves being essentially the same. Everybody's all [Ahern makes groaning noises]. Alright, I hope everybody got how I start my lecture. Right? Student: No. Ahern: No!? Student: I said if you said something about starting the exam, we would give credit. Or starting the class, we'd give credit, yeah? S1 proteases are a class of enzymes that are called serine proteases. They all have serine, histidine, and aspartic acid as a catalytic triad at the active site. It's very easy to alter the genetic code for any of these proteases and change which amino acid is presence at any given place. When they do that, and they compare the activity, so this is the log of Kcat. The wild type enzyme has an activity up here. One 10 millionth as active when that serine is changed to an alanine. Ahern: There are other types of proteases that behave very much like S1, well like serine proteases. One of these classes is known as cysteine proteased. Ahern: This class of protease is essentially identical to that of theserine protease, at least for our level of understanding. He says the aspartyl proteases, at first glance, look somewhat different. But those similarities aren't all the way through like we see with the cystine prote enzymes, he says. of a sudden realizes, "Whoa! What's that?" He grabs it, you know. Wipes the dust off of it. Of course in the process polishes this thing. And out pops this magic genie. And he says, "oh master, thank you, I will grant you 3 wishes." and the guy says, 'oh, this is really great' he says,. "I guess I want a billion dollars "so that I can be a very rich man." and poof, a certificate appears in his hands and it says he has a billion. dollars in a Swiss bank account. Carbonic anhydrase can catalyze the conversion of a million molecules of substrate into product per second. Most enzymes have a fairly narrow pH range where they work that's ideal. At pH 9, the enzyme is far more active than it is at pH 7, indicating that a very, very important step is the removal of that proton. The rate of formation of the nucleophile is the critical step in the catalytic action of this enzyme like that of the proteases that you saw before. alkoxide ion. In the case of the restriction endonucleases, that shape change is more dramatic. What it does is it actually causes a bend to occur in the DNA. So we think of the DNA molecules of being straight and linear, but when the enzyme is bound to that proper site, the enzyme goes "oh, whoa!" and it bends. The DNA molecule is physically bent at that point. It's physically bent. Now that bending turns out to be critical for the catalytic action. like we saw with the peptide bond and everybody's happy. I will very briefly go over that next time and I will see you on Friday. Thank you for listening to our show on CNN.com. We will be back on Friday at 10 a.m. ET for a new episode of CNN Live in the U.S. and 2 p.M. ET in the UK. For more, visit CNN.co.uk/live and follow us on Twitter @cnnlive and @CNNLive.

ROUGE-1: 23.15, ROUGE-2: 21.37, ROUGE-L: 21.32
BERTScore: 71.71

==============================================
==================== [49/100] ====================
Summary:
AA and I are going to show you how to make vanilla extract so easy so yummy so good for all the things that you would use it for cakes whatever really really good isn't it so let's go [Music] simple now all you want for this is about 1 oz or roughly 30 G of vanilla beans so they'll be you know long Dobby whacker things and just cut them up into I don't know a couple of cenm you may maybe maybe qu an inch half an inch or so. cup of vodka that's it all you do is put on the lid and just give this a shake and you just have to shake this once a day for at least a month preferably 2 to 3 months cuz you'll get more flavor it will ferment and it'll be a nice strong flavor just store this in a dry cool place and just as I said just shake once aday as you can see it's nice and dark I've been shaking this every day for about 2 months now a month will be enough but the longer you leave it the more flavor you'll have. like this no no you don't want to try it oh it smells so good do I smell this no you sure no it's really good can I smell it no oh let's give this a little little taste hold on oh yummmy do you want totry no no I'll see you next time for my next [Music] meal. Like this noNo, you won't even try it. No, you're not going to even smell it. You're not even going to taste it.

ROUGE-1: 72.43, ROUGE-2: 68.28, ROUGE-L: 68.11
BERTScore: 78.76

==============================================
==================== [50/100] ====================
Summary:
RAFAEL JARAMILLO: Let's talk about intermediate phases and line compounds. He recalls intermediate phase in a three-phase system. He says the solution model becomes very narrowly shaped, like a pin. All possible common tangents are going to converge at the same point, he says. "I no longer need a solution model. I only need one point," Jaramillo says. 'I can't help myself. I got to draw the mouse-face plot' Magnesium nickel system has a number of different phases. At high temperature, this Laves phase develops some width. But when you drop down to low temperature, both of these intermediate phases appear as line compounds. These are very distinct structures, and they're only occurring at very distinct compositions. That's a hallmark of an intermediate phase. That is a line compound. Let's talk about the size of these vertical segments. And it's often written as delta form. So what is the compound formation energy? It is the free energy change for formation of 1 mole. of compound from the elements in their reference states. So, for example, I might have 2 moles of magnesium in. its alpha phase plus alpha-- let's say HTP-- plus a mole of nickel in its alpha FCC. phase. And these can react to form magnesium 2 nickel. And there's a normalization that you need to apply in order to use formation free energies in free-energy composition diagrams. For example, we need normalization to use delta formation free energy on a free energy composition plot. algebraically-simple point, but it's a conceptual point that it's easy to mess up. So you have to apply that normalization if you need to draw such plots. All right, I want to give you some examples of line compounds in nature and in technology. And then we'll come back to discuss the thermodynamics a little more. Before I move on to some examples, are there questions about this algebra, this arithmetic, the concept of formation energy, the mouse-face plot, anything, anything of that nature? There's an infinity of examples of line compounds. Silicon tends to be a pretty useful additive for copper-based alloys. Doping semiconductors is why we're able to talk to each other over there. Without doping, there is no semiconductor devices, and no electronics revolution. So the fact that you can dope some metals into silicon is as important as the solubility limits can be in the parts per billion. It's a very famous line compound, so on the last P set, you're going to do some problems. gallium arsenide. Where silicon isn't fast enough are the transmitters and receivers of your phones, gigahertz RF networks. Another place you're going to find it is anywhere you need light. And so gallium arsenicide and alloys, thereof-- which we don't show here-- are the basis for all optoelectronics and photonic technology. So right now, we're Zooming. But likely, some part of the data between me and you is carried by fiber optic. Silicon carbide is a refractory-- some people will say it's a ceramic. Some people say no because it doesn't contain oxygen. It's of enormous industrial importance. It also is an emerging semiconductor material for high-power electronics. In 50 years from now, if the idea of a power substation is a thing of the past, it will be due to silicon carbide and similar high- power electronics that are being developed today. And this last one, this is the titanium-sulfur system. This is a neat system because, first of all, it has everything in it. third of the Nobel Prize work that was recognized recently in 2019 with the chemistry Nobel Prize was for work on titanium-sulfide-based cathode. It's a layered material, so you can shove a lot of lithium in it. So these are-- for example, systems that have line compounds and other things. Do we have any questions on reading these, interpreting these, using these? AUDIENCE: Yeah, I have a quick question. So on the bottom left one and the top right one, do they both contain three line compounds? doesn't have to be that way. Here's a case of a line and an element that actually does have a pretty wide, solid solution, great range. At low temperature-- actually, similarly to gallium arsenide at low temperature, it's going to be a really boring free-energy composition diagram. Let's draw what that looks like. So would anyone like to take a stab at how I would draw a free- energy composition diagram for the silicon-carbon system at low temperatures? the free-energy composition diagram. OK, then we move back to the board. I'll just make a couple of conceptual points and then we'll finish up. Comparing solutions at equilibrium to line compounds at equilibrium. Let's imagine an alpha phase and a liquid phase at equilibrium, and I'm going to draw just a representative phase diagram just to have something in mind. Here's alpha. It is liquid. Here is temperature. And I'mgoing to imagine now some equilibrium, alpha liquid equilibrium. I drew one tie line there, alpha Liquid equilibrium. 1/7 of the Gibbs free energy of the IV-III phase. And these phase factions are determined by lever law. This is really the point. When you have line compounds in coexistence in equilibrium, there are no internal composition variables. Whereas when you have solution phases in equilibrium,. the internal compositions are variable. And that's what's led to everything we've been enjoying over the last month and a half. And now we have these cases where the compositions are not there [INAUDIBLE] anymore. Z is determined by charge balance. Oxygen is always O2 minus in compounds. Metals have various oxidation states. Some metals have more than one oxidation state. Z is not a variable. That z is an integer or a rational fraction, and it's fixed-- SiO2, magnesium oxide, Al2O3, so forth. When we return on Wednesday, we're going to talk about the thermodynamics of this reaction. We'll use this property of being line compounds, and we'll use a bunch of other things.

ROUGE-1: 33.87, ROUGE-2: 32.14, ROUGE-L: 33.32
BERTScore: 63.29

==============================================
==================== [51/100] ====================
Summary:
A random variable can take different numerical values depending on the outcome of the experiment. Some of the possible numerical values of a random variable will be more likely than others. We will describe these relative likelihoods in terms of the so-called probability mass function, or PMF. The PMF is also sometimes called the probability law or the probability distribution of a discrete random variable. We use a subscript, X, to indicate which random variable we're talking about. And it has a probability. And in our case this probability is equal to 1/2. the form of the answers. And it's always convenient to also provide a plot with the answers that we have. The answers come in the form of a plot. The plot comes in the shape of a map. The map is a map of the world as seen from the top of a mountain. The top of the mountain is a mountain with a waterfall at the top. The waterfall is a waterfall as well as a river. The river is a river that runs through the middle of the village. The rivers run through the village and the valley.

ROUGE-1: 20.12, ROUGE-2: 15.51, ROUGE-L: 17.20
BERTScore: 69.39

==============================================
==================== [52/100] ====================
Summary:
Protein three-dimensional structure and its implications for the binding of small molecules such as drugs. The ABCs -- the alpha helix, the beta sheet, and the coil -- can be characterized by the hydrogen bonds that hold it together. How can we use these to recognize other macromolecules, other proteins and nucleic acids? We will in short order get to the scary pumpkin-like molecule. It's a catch-all phrase that includes everything except alpha and beta, but a particularly well-formed type of coil has its own nomenclature. We have these motifs that we could find, weight matrices for them by aligning lots of sequences. Now instead of aligning sequences, let's see what we can do by mutating both the protein part and the nucleic acid part. This is a way. of getting a really good empirical data set, which in principle, you can combine it with similar functions on the flanking ones, and you can dial up sequence of a protein interaction, at least with this class of proteins. you can get the binding at a lower concentration, which means a stronger binding. How you relate that to the binding constant we had in the previous slide is the subject of this slide number eight. Now we call this the apparent equilibrium association constant because these experiments, just like many binding in living cells is not at equilibrium. It's a dynamic process in the cell and in vitro. There are ways that you can measure the equilibrium constants, but what this is apparent is that you need to wash off the excess fluorescence in order to detect the fairly low signal. of the protein maintaining almost perpendicular to the DNA axis. But again, so on the left is the three tandem repeats, and on the right is a dyad axis. These are the two major symmetry classes, and it's amazing how many nucleic acid protein interactions fall into one of these two classes. Can we extend this to RNA? This is a much more complicated situation with RNA because you don't have these long perfect double helices anymore. You have these very short RNA helices that I found. showed in the last couple of classes. This is transfer RNA, one of our favorite molecules here, with the anticodon at the bottom of each of the pink structures. And the amino acid acceptor three prime end of that 70-some nucleotide-- 70 to 80 nucleotide nucleic acid. So the pinks are all the tRNAs, and there are at least 20 different types of amino acid and has 20 types of transfer RNAs and 20 type of proteins that add amino acid onto the three prime ends of the transfer RNA. If you wanted to create a new code, as these authors have, you'd have to find homology among the proteins or graph domains of recognition between each one. You can arrange to make a new amino acid by carving the pocket the amino acid recognizes and grafting on the appropriate nucleotides. OK, you've had some programming experience that hopefully will prepare you for the real world of interacting with input and output from various devices. The topic today is proteins, and this really is the main contact between the exquisite regulatory mechanisms. that you can basically program the almost digital nucleic acid world inside the cell but via clearly analog inputs and outputs. We also-- I've listed some of the scariest proteins that I could think of. And we're going to talk about three of them. One of them in the slide, which is the proteins that are actually involved in causing the symptoms that come from when you're worried about anthrax. And then we'll talk about HIV yet again, this time, polymerase mutants that cause drug resistance. not inside the cell. But the whole complex gets internalized. Still, topologically, it's as if it were outside the cell when it's inside this little vesicle. So you can see that when we're talking about protein three-dimensional structure, whether we're predicting it or solving it, protein is not a static object. Here, it associates with one factor. It associates seven of itself. It interacts with lethal factor. You need to think of these as dynamic systems with many different states. minus 9th seconds. That's atomic motion. Transcription that we talked about, all of the regulatory mechanisms of transcription last time, the rate of the constant for that process is around 50 nucleotides per second. Not entirely coincidentally, that's about the rate at which it is translated into protein. That could be used as a timer in a circuit of these longer time frames, like cell cycle, circadian rhythm, very long time frames in ecological systems with bamboo and various pests. then development and aging, which can be on the order of hundreds of years, at least for humans, turtles, and whales. So what we think proteins are good for depends on the accuracy. And the accuracy depends upon the method. As you get to 1 Angstrom or better in your accuracy, as you can get from NMR and X-ray crystallography, you now are in a position to study catalytic mechanism and design and improve ligands, such as drugs. There may be a day where we can do this all from ab initio prediction or modeling at very great distances. HIV is one of the most sequenced molecules on Earth. The HIV gene encoding the reverse transcriptase polymerase. It's been sequenced many times because as a patient takes the drugs, their population of the AIDS virus changes. And that causes a drug resistance in the HIV, with unfortunate consequences for the patient. We can take-- now, making mutations in polymerases is not entirely of negative consequences. And I'm going to show you a really beautiful example where a DNA polymerase, you want to change it so that it can now handle what would normally be an inhibitor. This has an 8,000-fold effect on the specificity of this polymerase. Now, that's how we program a particular atom to achieve an important goal. Point mutants are not the only way to generate conditional mutants. Many of them historically were. But there are ways that you can program, and conditional, meaning you can regulate under what conditions. It's the ultimate where we go in and if the protein is already present, we can change that particular nucleotide in situ in the correct place so it's properly regulated and everything. the protein is expressed or not or active or not with an entire domain, or with single nucleotide polymorphisms. Now, so this is one way. This is the nucleic acid way. Another way is by modulating the activity of the proteins from the outside with drugs or drug-like molecules and chemical kinetics. And under the subheadings for that, you can make these by combinatorial synthesis-- and we'll show an example of that. But combinatorially synthesis can be based on design principles, not just completely random. In the case of the zinc finger, we made an altered specificity. We made new zinc fingers with bind to completely new trinucleotides. With the DNA polymerase, by changing one amino acid, we could make it now accept almost four logs better an inhibitor is very useful. These are not that unusual. And those can be designed or naturally occurring. They can have a very important thing to do with conserved charge. They tend to be highly conserved. That's a general way of looking at the conservation of single-nucleotide polymorphisms. That might have impact on pharmacogenomics. The idea of chemical diversity, in a way I hope nicely connects to where we've been with RNA arrays. RNA arrays, and the double-stranded RNA array that we used earlier in class today, can be generated in a commentorial sense. Solid phase comes up again and again in arrays. It's very obvious why you have a solid phase. You want to be able to address it by its positions in x and y on the array. And it's a fantastic way of getting purification of your products simply by washing rather than doing complicated purification procedures.  Synthetic way of getting short peptides, either by directly synthesizing the peptides or synthesizing nucleic acid that encodes that peptide. And you can think of these as drug-like molecules. These are naturally related-- they can be analogs of nucleic acids and proteins, not just straight ones. And we'll talk about opportunities for making these analogs. And the process is cyclic in the sense that each cycle, you return, and the polymer gets a little bit longer. You start with one monomer. on a solid phase, shown by these little hexagons on the far right side of the slide. And you add-- you remove the protecting group on the immobilized polymer, one protecting group. And then you bring in this reactive group, otherwise protected. And there's really one major product that you expect. You wash off all of the excess. You now have one longer. You deprotect. This DMT group is removed. There may be additional steps, such as oxidation, which will stabilize the new bond. I said there's an opportunity here for modifying the nucleotides or oligopeptides or other chemicals to make them so that they're related to, but not identical in every property, to normal constituents of your body or of a bacterial cell that you're aiming at. And why would you want to make derivatives? Why not make the exact thing? And examples are, in the previous slide, you can make modified bases. And in slide 29 here you can change the backbone itself. to a template-independent polymerase, [? thermotransferase, ?] which will extend a few nucleotides of completely random nucleic acid sequence. This is one of the examples in biology where you generate sequence de novo. The proteins themselves, these little arrow-shaped things with boxes in them along the top, labeled Module 1 through 6, those proteins are, of course, made on ribosomes. But then they act kind of like the solid phase synthesis. In next class, we'll talk about ways of getting direct information on protein through cross-linking and mass spectrometry. Another way is indirectly setting up these reporter assays, where you take advantage of the binding properties of known proteins to analyze two unknown proteins. This is a source of information, which is intrinsically computational in the sense that there's a large amount of it. And typically we want to select targets for solving the structures of proteins in order to look at their ligands in more detail.

ROUGE-1: 33.59, ROUGE-2: 32.59, ROUGE-L: 32.39
BERTScore: 61.77

==============================================
==================== [53/100] ====================
Summary:
More than half of large US firms plan to use AI within the next year to automate tasks that were previously done by staff in a bid to cut costs boost profits and make their work more productive. The New York Times reports that generative AI could automate activities equivalent to 300 million full-time jobs around the world. open ai's chief executive that's Sam mman says governments will need to assume the bulk of responsibility in supporting workers AI labor market disruptions and the question will employees just end up training AI systems only to them be replaced by them. One quarter of employees saved a combined 50,000 hours. At.at what that's allowed you to do I mean as you said free up time allow those staff to do other things. I mean how does that work how do we square that Circle well I think like you know any other uh benefits of becoming more efficient or more productive it is now spending more time on higher value impact work in your job those are the core benefits that we've seen from our teams instead of spending you know 10% of your time looking for information. Cisco's chintan: There's a danger that we focus on all of this and on what it can deliver but frankly it needs people that understand it. Salesforce's Natalie: How do you make sure that AI is used to democratize access to jobs access to different experiences. The Vatican's call by the Vatican is really centered on the fact that we can't lose sight of human dignity as we develop systems that can actually create an inclusive future for all. The UK's new government's policy they're very thin on AI and that was a real moment for this new government to say we need to train everybody up in AI. were most at risk because of automation. People are realizing you have to throw out the door any ideas you had about who is really at risk and say every job is going to be changed. We just hope that the workers whose jobs are changing have a voice in saying how yeah it's turkeyy is not going to vote for Christmas are they if those are the ones that could find their jobs being replaced. There's uh there's so much in there and Stephanie just a final thought from you in all of this about briefly if you will. I think we also have to be realistic you're not just going to suddenly go get an engineering degree in two weeks right learn cyber security overnight um these are these are highly complex skills that require usually years of training so this will involve I think a sort of again look at the Danish model of the government and companies working together to sponsor re-education and retraining for workers. This is a total pivot of our economy uh Stephanie really good to have you with us again guing us through all of this and now Emma thank you so much.

ROUGE-1: 22.82, ROUGE-2: 21.44, ROUGE-L: 21.86
BERTScore: 62.31

==============================================
==================== [54/100] ====================
Summary:
Sadhana sadhana is a machine learning scientist at Themis Ai and the lead TA of the course intro to deep learning at MIT. She'll be teaching us more about specifically the bias and the uncertainty of AI algorithms which are really two key or critical components towards achieving this Mission or this vision of safe and trustworthy deployment of AI all around us. Sadhana will talk about how we can build very modular and flexible methods for AI and building what we call asafe and trustworthy Ai. behalf of Themis so over the past decade we've seen some tremendous growth in artificial intelligence across safety critical domains in the Spheres of autonomy and Robotics. A lot of these Technologies were innovated five ten years ago but you and I don't see them in our daily lives so what is what's the Gap here between Innovation and deployment. We'll talk about how Themis is innovating in these areas in order to bring new algorithms in this space to Industries around the world after we talk about the ramifications for this for real world AI. Commercial facial detection systems are everywhere you actually played around with some of them in lab two when you trained your vae on a facial detection data set. The biases in these models were only uncovered once an independent study actually constructed a data set that is specifically designed to uncover these sorts of biases by balancing across race and gender. There are other ways that data sets can be biased that we haven't yet talked about so so far we've assumed a pretty key assumption in our data set which is that the number of faces in ourData is the exact same as theNumber of non-faces in our Data set. to dbias a model so what we want is a way to learn the features of this data set and then automatically determine that samples with the highest feature bias and the samples with lowest feature bias we've already learned a method of doing this in the generative modeling lecture. Now we'll walk through step by step a de-biasing algorithm that automatically uses the latent features learned by a variational autoencoder to under sample and oversample from regions in our data set um before I start I want to point out that this debiasing model is actually the foundation of themis's work this work comes out of a paper that we published a few years ago. good representation of what a face actually is so now that we have our latent structure we can use it to calculate a distribution of the inputs across every latent variable and we can estimate a probability distribution depending on that's based on the features of every item in this data set. This allows us to train in a fair and unbiased manner to dig in a little bit more into the math behind how this resampling works this approach basically approximates the latent space via a joint histogram over the individual latent variables. data point x will be based on the latent space of X such that it is the inverse of the joint approximated distribution. As Alpha increases this probability will tend to the uniform distribution and if Alpha decreases we tend to de-bias more strongly. This gives us the final weight of the sample in our data set that we can calculate on the Fly and use it to adaptively resample while training. Once we apply these this debiasing we have pretty remarkable results. Keep this algorithm in mind because you're going to need it for the lab 3 competition which I'll talk more about towards the end of this. lecture so so far we've been focusing mainly on facial recognition systems and a couple of other systems as canonical examples of bias however bias is actually far more widespread in machine learning. Consider the example of autonomous driving many data sets are comprised mainly of cars driving down straight and sunny roads in really good weather conditions with very high visibility. In some specific cases you're going to face adverse weather bad um bad visibility near Collision scenarios and these are actually the samples that are the most important for the model to learn. Uncertainty or when a model does not know the answer is the core idea behind uncertainty estimation. In the real world uncertainty estimation is useful for scenarios like this this is an example of a Tesla car driving behind a horse-drawn buggy which are very common in some parts of the United States. The exact same problem that resulted in that video has also resulted in numerous autonomous car crashes. There are multiple different types of uncertainty in neural networks which may cause incidents like the ones that we just saw. epistemic uncertainty so both of the methods we talked about just now involve sampling and sampling is expensive ensembling is very expensive but even if you have a pretty large model um having or introducing Dropout layers and calling 24 word passes might also be something that's pretty infeasible. At Themis we're dedicated to developing Innovative methods of estimating epistemic uncertainty that don't rely on things like sampling so that they're more generalizable and they're usable by more Industries and people. Themis is unlocking the key to deploy deep learning models safely across fields. We can now answer a lot of the questions that the headlines were raising earlier which is when should a human take control of an autonomous vehicle. What types of data are underrepresented in commercial autonomous driving pipelines. We now have educated answers to these questions due to products that Themis is developing and in spheres such as medicine and health care we can now answers questions such as when is a model uncertain about a life-threatening diagnosis. compete in the competition which the details are described in the lab but basically it's about analyzing this data set creating risk-aware models that mitigate bias and uncertainty in the specific training pipeline. At Themis our goal is to design advance and deploy a trustworthy AI across Industries and around the world. We're hiring for the upcoming summer and for full-time roles so if you're interested please send an email to careers themesai.io or apply by submitting your resume to the Deep learning resume drop and we'll see those resumes and get back to you.

ROUGE-1: 21.28, ROUGE-2: 20.58, ROUGE-L: 20.87
BERTScore: 57.43

==============================================
==================== [55/100] ====================
Summary:
HONG LIU: So if you want to compute, say, some scattering amplitude from alpha to beta-- so alpha's some initial state and beta's some final state. Say alpha consists of momentum p1 and PN-- or pm, and beta, say momentum p m plus 1 and pn. And then you can get this scattering amplitude just by taking your momentum-space correlation function, OK, for the n points. So this is the relation, and here I have stripped out the momentum conservation on both sides. I will explain a few things. OK, so remember this Gn, so let's go back to the definition of this Gn. So this is obtained by doing a Fourier transform. Say-- I think it would be minus sign. By doing the Fourier transforms, the coordinate-space correlation function can be written as the following. So-- yes? AUDIENCE: Right, so time-ordering of x1 to xn right here? HONG LIU: Yeah. It doesn't matter. It just has to act to the right. HONG LIU: To derive that theorem, the time-order matters. So we mentioned that if you have a diagram like this, say minus P1, minus P2, to P3, P4, and then the square root and amplitude are just given by minus i lambda, because you need to throw away all the external propagators. OK, you just need to-- you have to truncate all the External propagators, so that means you also throw away diagrams like this. So all these diagrams, you can ignore them, OK. HONG LIU: Z is the same for different processes, but it's different for different particles. Z is a constant for all of them, yeah. If it depended on momentum, then it would be kind of useless, right? So we will not go into details of the Z. And that is what is called the "Z" principle. It is the principle that all such things don't change the momentum. OK, the momentum don't changed. Yeah.things, they-- yeah, all these things, essentially, they just modify the-- give you the correction to the external propagator and include it in that constant Z. HONG LIU: This is the self-interact-- yeah, just when the-- when you have an interacting theory. When the particle propagates, it actually can interact with the virtual particle. And so that, this kind of interaction, will affect the property of the propagation. It can have the most effect by prefactor, but actually can change the mass, too. But for the-- oh, it can correct for the mass-- and also, that's the subject of the QFT2. The Dirac equation is one of the most beautiful equations in mathematical physics. It's actually describing electrons, so it's not only beautiful, but it's actually useful. The Dirac theory is a prime example of how people make great discoveries often for the wrong motivations. But the key is that if you are good enough, you will find something new and that something new will be useful. We'll talk about fermions in the next section of the lecture. We're going to talk about the Dirac equations and its covariance. Go to a different frame. OK, yeah, that's what we mean by-- just when you go to a Different Lorentz frame, the equation, the form of the equation looks the same. Just different observers in different laboratory, they see the same equation. So your first-order derivative, then, has to have the following form, alpha minus i-- yeah, so this is the gradient operator. And then this is a vector, have three components, and then have to contract with something. They're just some constant Hermitian matrices. And then, so then he reasoned that for this equation, if we want this equation to be Lorentz covariant, then at least it should have the relativistic plane wave as its solution. If it does not even have that type of solution, then you, yeah, of course cannot be covariant. OK, so the minimal requirement-- so before we really try to see how can we make this into a covariant equation, we say let's consider minimal requirement. solutions. And then the simplest way to do this is-- this is the first-order equation. And now imagine if we square this equation, and then this should reduce to the Klein-Gordon equation. If you can make that work, then this property will be satisfied, OK? So this can be satisfied if square of star, OK, satisfies reduced to Klein- Gordon equation. So now we want this equation to be given by minus partial x square psi plus m square psi. HONG LIU: We said the alpha and the beta, they must be Hermitian. So if we satisfy all these four conditions, and then we will guarantee that that equation star should have the plane-wave solution. And so now you just try to find matrices, satisfy those conditions. OK, you see that to satisfy them needs at least a 4-by-4 matrix, so n has to be 4. So for example, here is one solution. Say you take beta to be 1, so all matrices here should be understood as 2- by-2 matrices. So all together, it's 4-By-4, so 1 and minus 2. HONG LIU: The Dirac equation can be written in terms of gamma matrices. He explains how to write it in a more compact form. HONG LIu: It takes a little bit time to get used to it, OK, and I know some people develop psychological fears for fermions because you have to deal with those Gamma matrices, OK? For a long while, actually, I have this psychological fear myself. [LAUGHTER] When I look at fermion, I want to be away from it because I don't want to dealing with those gamma matrixes. the meaning of all these different solutions for alpha and beta or for gammas, OK? So as I mentioned, you can have infinite number of solutions. What's the meaning of them? So first, let's imagine when we look at this equation-- so as I said, this is a matrix equation. In this matrix equation, then you have this psi which is a four- component vector. So now if we make a basis change in this four-component vector, OK, so imagine, just say, consider making a basischange in psi. actually in the-- opposite regime, it's convenient for the ultrarelativistic regime. OK, so it depends on which regime, sometimes you use different gamma matrices. So now if you look at the Klein-Gordon equation, let's see how this is covariant. And this is a Lorentz scalar, so this is also equal to that. OK,. it's the same in any frame. Now we want to show that when we go to a new frame, partial prime square-- OK, this means in the prime coordinates. to show, OK, so now we want to show that the Dirac equation has the same property, OK. OK, that's much more nontrivial. Again, it's really ingenious, ingenious, yeah, but we see, actually, it works.OK, so we will do it next time. Next time, we will show you how to do the same thing in a different way. We will do that in a few minutes. We are going to take a break. We're going to have a drink.

ROUGE-1: 24.13, ROUGE-2: 22.76, ROUGE-L: 21.38
BERTScore: 70.11

==============================================
==================== [56/100] ====================
Summary:
There's no age, education, profession, or even native-born citizenship requirement. Most presidents nominate individuals who broadly share their ideological view. So far, six justices have been foreign-born. At least one never graduated from high school, and another was only 32 years old when he joined the bench. For example, when President Eisenhower, a Republican, nominated Earl Warren for Chief Justice, Eisenhower expected him to make conservative decisions. Instead, he chose to appoint him to the Supreme Court. A US Supreme Court justice is expected to be, in the words of Irving R. Kaufman, "a paragon of virtue, an intellectual Titan, and an administrative wizard" This job is for life, barring resignation, retirement, or removal from the court by impeachment. Of the 112 justices who have held the position, not one has yet been removed from office as a result of an impeachment. One of their roles is to protect the fundamental rights of all Americans, even as different parties take power. to be debated and dissected by the ultimate judges, time and history. To be debated, dissected and debated, we need to look to the past, present and future. We need to see how far we have come and what we can learn from the past. We also need to learn from our mistakes and learn from each other. We want to be the best we can be, and we want to do it in a way that honors the past and the present. We hope you will join us in this quest.

ROUGE-1: 49.21, ROUGE-2: 38.64, ROUGE-L: 33.11
BERTScore: 61.95

==============================================
==================== [57/100] ====================
Summary:
Jake Xia: This is the second time we are having this class. We had it last year in a smaller version. That was for six units of a credit, and we had it once a week. And mostly practitioners from the industry, from Morgan Stanley, talking about examples how math is applied in modern finance. And so we got some good response last year. So, with the support of the math department, we decided to expand this class to be 12 units of credit and have twice a week, as you know. WebEx.is has added math lectures focusing on linear algebra, probability to statistics, and some stochastic calculus. The purpose of this course is to give you a sampling menu to see how mathematics is applied in modern finance. Last year when we finished the class, we had a few students coming to work in the industry. Some work at Morgan Stanley, some work at elsewhere. So it's open door. You're very welcome to attend the classes here. And at the same time, obviously, you will further solidify your math knowledge and learn new content. and the Sloan school here. So anyway, thanks for that. We will be doing a bit more polling along the way, mainly to get feedback of how you feel about the class. Last year we had it online, so if you feel the class is going too fast, or the math part is. going too slow or the finance part is a bit confusing, the easiest way is really just to send us. emails, which you will find from the class website. So Anyway, I will start today's lecture with a story, and a quiz at the end. Don't worry, it's not a real quiz. Just going to ask you some questions you can raise your hand and give your answer. my personal story. I want to tell you why I tell the story later. But the story actually was in the mid '90s. I just left Salomon Brothers -- that was my first financial industry job -- to go to Morgan Stanley in New York. So the first day, I sat down, I opened the trading book, I found something was missing. So, I turned around, I asked my desk quant. I said, where is the vega report? So, let me show you. Kappa is actually a Greek letter. Kappa is also called vega by some uneducated traders at the Salomon Brothers. So obviously, I learned how to call kappa very quickly. And I called it kappa in the last 17 years, but you will hear people calling it vega. But anyway, so that's my first day at Morgan Stanley. But why did I tell you the story? What point I try to make? So this story is actually-- when you think about it, mathematical or quantitative finance is a rather new field. from mostly under-educated traders. Some of them typically joined the firms in the mail room and became trader later on. And to nowadays, if you walk on the trading floor, you talk to the traders, most of them have advanced degrees. So, financial products is really just one form of trading. There are many other ways of trading aside from exchanges. And the different regions have different exchanges and markets, as well. And they typically specialize in local products, local company stocks, local bonds, and local currencies. After 1933 Glass-Steagall legislation, there were two main types of banks. Commercial bank is supposedly, you're taking deposits and lend out the money. Investment bank supposed to focus on the capital markets, raising capital, trading, and asset management. So that's how banks are organized. Outside banks, other players, basically, the asset managers, are obviously a very big force in the financial markets. So the really bank.type of player is really the investment bank. The investment bank is the player that is really bank-like. In the current environment, when you have a savings account, you don't really earn much at all. And so you would have to take more risk to generate more return, or you have longer horizon CDs, other type of products, or trade the stocks. So, when somebody has money, you're essentially-- you're buying a stock, you give the money somewhere. Supposedly, it will go to the company to generate a better return. And for the borrowers, whoever needs money, they need to have access to the capital. Brokers take principal risks, market makers make the trade happen, earn commission. Hedge funds find opportunities to profit from inefficient market positioning or pricing. Private equity funds look to invest in companies and either take them private or invest in a private equity form to improve the company's profitability. Government always have a very large impact on the market, because they are the policymakers, and the different policies they push out will generate different outlooks for the future markets, therefore, profitability. The first type is really just hedging. That means you're not proactively adding risk to what you have. In the new regulation, obviously, proprietary trading is banned, right? And so the third type is really the proprietary trader, the risk taker. These are the hedge funds or some portfolio managers. They need to focus on generating return and control the risk. That's where the beta and alpha, the concept comes in. It's a format. You want to beat S&P 500, so you want to basically have certain tracking of this index, but you. want to return more on top of that. stock, I think a lot of people know pretty much where it is. But if it's not transparent, so what do you do? So, if instead of asking you where Apple is, probably you're going to tell me $495 today. So you're probably less transparent. So that market maker comes in to provide that liquidity, and then takes the risk. They manage the book by balancing those Greeks, which I mentioned earlier. That's called delta. Gamma is really the change of the portfolio. Take the derivative to the delta, or to the underlying spot. "VaR" concept is also, obviously, a very important concept. And we talk about the volatility exposure was vega. And on top of that, what are the tail risks? What are the events that can actually get you into big trouble? So people use value at risk. Then capital. How much capital are you using? It becomes a veryimportant issue nowadays. And balance sheet. You have asset, you have liability. How do you leverage? How much leverage you have? Before the crisis, for example, lot of the banks leverage up 40 times. Vasily Kuznetsov: Arbitrage is really to find the relationships between prices, and try to profit from those relationship mispricing. He says the systematic trader builds computer models. Kuznets: Math is very useful in risk management, which I will give you some quiz -- questions -- to test your knowledge of the market. He adds that math plays a very important role to quantify how much exposure you have to the risks you have in your bank account or in the stock market. facing two choices, choice A and a choice B. Choice A being you have 80 chance to lose $500. You have 20% chance to win $500, right? That's choice A. Or choice B, you basically just lock in you have 100% chance of losing $280. Let me ask you, for whoever likes to choose choice A, please raise your hand. One, two, three, four. About six out of say, let's call it 50. So, that's really the common behavior, which mathematically may not make sense. your bank account balance is-- let's say you are a freshman student. Your choice will be very different from someone who has $100,000 in his bank account. And also, your risk tolerance, how much you can tolerate. So learn the math, learn the finance first, but keep those questions along the way when you are learning during this class. So suggested homework, optional. Go to the course website, read what we have put up for the financial glossary. So if you still have things you don't understand, compile your own list of financial concepts. we got maybe-- how about this? We still got about 15 minutes or 12 minutes left, so I'll pass it to Vasily, then maybe we can leave five minutes for some questions. VASILY STRELA: [INAUDIBLE] mentioned that, Apple trades, that now it's $494.4 Yeah, just a couple of [INAudIBLE]. Well, first of all, no offense to people who were [INA UDIBLE], but I just wanted to give an example. every time. So, if you want to differentiate this functions and get a derivative, then this derivative will be quite noisy. And so, instead of getting the true derivative, you might obtain something quite different from true derivative just because there is a confidence interval around any point. So obviously, there is somewhere balance, and the question was, is there an optimal shift size to get the derivative? And that's what-- uh oh, the slide got corrupted. There was an answer. optimal size, that would be your numerical derivative of this blue function. While if you use an optimal shift size, which [INAUDIBLE] computed, it would be much smoother and much better. So, that's one of example, and that's what he did. And we actually are implementing it in our systems and plan to use it in practice. Another project was actually quite different. And it was about electronic trading and basically how to better predict prices of currencies and exchange rate. We put syllabus there, a short list of literature. We will be posting a lot of materials there. Probably most lectures will be published there. Jake's slides are there already. So, any questions? We like to get your emails so we can put you on the website for further announcements. But it's probably easier if you put your email on the sign up sheet, so that we can [INAUDIBLE]. VASILY STRELA: Yeah, but please visit and sign up here, because there will be announcements to the class.

ROUGE-1: 35.41, ROUGE-2: 33.45, ROUGE-L: 34.25
BERTScore: 68.75

==============================================
==================== [58/100] ====================
Summary:
Climate change is changing us from the inside out in the UK one in four adults and one in 10 children experience mental illness. Half of the people they spoke to experienced negative impacts on their mental health due to the heat. People on Lower incomes and other disadvantaged groups groups were more likely to feel the negative effects of extreme heat because it was harder for them to keep their homes cool. You don't even need to be alive to experience some of these effects a study of expecting mothers who experienced Hurricane Sandy in 2012 the huge storm that hit New York showed that in unborn children who did experience that storm girls as early as preschool were 20 times as likely to experience anxiety. people in that storm showed that about half of these folks had some kind of post-traumatic stress and that's relative to 5% of the general population. Direct effects of climate change on people's mental health there's also knock on effects like loss of income and Rising prices which inevitably have implications for people's well-being. Although the impact of a changing climate on our mental health can be profound both Clayton and Charles's Rec search has shown that there are things you can do to support your wellbeing engaging with nature so spending more time in Green Space. people use that as a way to cope with stress so this is very beneficial social connection is a really big one as well. We are not separate from our environment we are connected not just to the world around us but of course to one another and it and it is only in working with one another that we're going to be able to move forward [Music]"It's a very exciting time for us. We're looking forward to it," says singer-songwriter. "It's going to give us a lot of energy. It's a really exciting time"

ROUGE-1: 45.54, ROUGE-2: 41.98, ROUGE-L: 38.58
BERTScore: 59.95

==============================================
==================== [59/100] ====================
Summary:
Adam Martin: How do you go from something you're interested in learning about an organism to actually identifying genes and mechanisms that are important for that? Martin: The two heroes of today's lecture will be the roundworm, Caenorhabditis elegans, and the fruit fly, Drosophila melanogaster. He also highlights a couple important vertebrate-model organisms-- the zebrafish and the mouse. Martin: Most of them are fairly small, and they're easy to house large numbers of them in a lab. induce mutations. So we're looking for mutations. And these mutations could be spontaneous mutations, meaning you didn't do anything to induce it, but they just appear as a variant in the population. In the way we can induce mutations is by using some type of mutagen. For example, you could have some sort of chemical mutagen that increases the error rate in DNA replication. Or you could use radiation to induce DNA damage, and that essentially accelerates the frequency of mutations that occur in the genome of an organism. Miles Martin: In model organisms, we can find these types of mutations. "Wingless" mutant defined a gene that has a homologous gene in humans. "Notch" gene defines an entire signaling pathway, which is important in stem-cell biology and is also over activated in cancer. Miles: In order to look for incorrect body parts, you need to get more than one fly with the same mutation to get a normal fly with a different mutation, so you can get multiple F2 flies. Drosophila larvae has a segmental pattern that alternate between smooth cuticle and hairy cuticle. Because there's a lot of hairlike projections here, it reminded the researchers of a hedgehog. The hedgehog gene was the founding member of an entire signaling pathway, known as the "hedgehog pathway," that plays important roles in human development and also, human cancer. There are now a number of drugs that are being developed to target the hedgehog pathway, and one was approved back in 2012 for use in treating basal-cell carcinoma. Robert Horvitz's lab identified a pathway of genes that were involved in cell death. They identified a mutation that affected cell death that specifically blocked this engulfment process. They then mutagenized these ced-1 mutants. And what you see in this worm are these bubble-like structures that are cells that are dead but haven't been engulfed. So these dead cells, because they're not engulfed, are visible in the adult worm. And so they identified a mutant, and thus, a gene that causes "3," which is basically a double mutant. Adam Martin: circadian rhythm is a behavior. We are awake during certain parts of the day and are asleep at night. If you're hidden from the light-dark cycle, you continue this cycle for some amount of time. Martin: There's something intrinsic in our system such that we want to exist on this 24-hour wake-sleep cycle. He says the Nobel Prize-winning researchers identified a gene called "period" that is associated with familial advanced-phase syndrome, which is a sleep disorder.

ROUGE-1: 15.77, ROUGE-2: 14.34, ROUGE-L: 14.73
BERTScore: 60.51

==============================================
==================== [60/100] ====================
Summary:
In this chapter we will discuss two applications, one price control and second taxation, so right. Sir, does this slope of this graph denote anything price demand upon, some price upon some quantity? So, wait little later we will talk about that that topic, right now we are just talking about movement and shifts, the direction of movement. We are not talking about the slope. So, what is price control what do we mean by price control? Price control is how we can regulate the prices of the goods in the market. Many people in this country would like to have gas connection, but the suppliers cannot afford to give those many gas connections at the government-determined price. So, what they say you go to, you go for gas connection if it is your turn if somehow you are getting gas connection. What the seller would say that you want gas connection you will also have to buy gas burner from me then well you will get the gas connection ok. You are not interested in buying gas burner at really at the price which is much higher than the price that you will you have to pay in the market for a gas burner. When we have a price floor, we get excess supply then you have non-market rationing mechanism. Like the payment is not done at the time of buy, it is done after year or after few months. And one more thing we have to pay a minimum wage to the laborers. So, all these things happen whenever we have price control. But I am not saying that price control does not have advantage that when we talk about the welfare economics towards the end we will talk about it. But what I am saying definitely that there are definitely better ways to cater to the welfare of public.

ROUGE-1: 18.06, ROUGE-2: 17.50, ROUGE-L: 17.89
BERTScore: 68.55

==============================================
==================== [61/100] ====================
Summary:
Barbara Imperiali: We are going here now from the smallest carbon atom. She says in order to make proteins like hemoglobin and antibodies, we need large entities. The things that will feature today are the transfer RNAs and the ribosome, she says. IMPERIALI: And what to me is intriguing is that the size of theribosome is pretty similar to theSize of the rhinovirus, a little smaller than the hepatitis virus, but quite a bit smaller than some of these other viruses. messenger is single-stranded RNA, obviously. It has a 5 prime cap, which has got this funky 5 prime, 5 prime bridge that's resistant to exonucleases. Somewhere in that sequence is a start codon. It's something that says this is the bit I want to translate. Often, there's a lot of stuff here that you don't translate. Part of what's known as the ribosome binding site. And there are many features in this part of the sequence that are very important for translation. The genetic code, which forms the basis of this entire concept, has some features to it where it does have some degeneracy. But we'll go through the degeneracy, and we'll take a look at the genetic code because it will tell us exactly how the three letter-- the words made of three bases encode everything we need for translation. The ribosomes were discovered a decade later, and that was the work that was awarded a Nobel Prize for. And then, later on, things started to get to the point where you know, these are sort of things you want to get out to. The genetic code gives you the identities of what are known as the codons, which is how we designate the triplet of nucleotides. The way you read this table is you read the first letter. So, if I'm going here, here, it's going to be starting UU. And then, within each block, there are the four alternatives for the other four bases. The third one, basically, just designates those. It is the absolute-- the sort of Rosetta Stone for translating messenger RNA to amino acid sequence using codons. small and large subunits. In orange-- well, that's kind of a burnt orange-- is a sneaky little bit of the messenger. In yellow are the transfer RNAs. And there's one more unit on here that I won't describe too much. It's a protein factor that helps all the processes occur. And, in each step, you're bringing in a tRNA that's loaded with an amino acid where the anticodon of the amino acid is complementary to the codon that's within the messenger RNA. "I think this field is fascinating. Once you get used to the mechanics of it, it's really cool to think of how you go from DNA to RNA to folded proteins," he says. "Don't forget my office hours on Monday if you need them," he adds. "I'll be in the lab all day, every day, if you want to talk to me about it." "I've got a lot of questions for you," she says, "but I'll be happy to answer them."

ROUGE-1: 14.22, ROUGE-2: 12.89, ROUGE-L: 12.75
BERTScore: 64.92

==============================================
==================== [62/100] ====================
Summary:
Professor Donald Kagan: We are living in the early years of a polis sometime in the eighth century B.C. The date that's typical for the general phenomenon of colonization coming out of the mother cities of Greece is 750 roughly. The earliest date according to Greek tradition, if my memory is correct, was something like 773 where the Greeks date the foundation of what they thought was the earliest colony they ever established -- a place that they called Pithaecusa, which is the island of Ischia. The Greeks were terrified of the sea for very good reason. The vast overwhelming majority of people needed to farm land, in order to stay alive. One answer, and it's the one that is most widely believed among Greek scholars, is that the growth of population that we have mentioned in connection with the rise of the polis is still working. But I don't think that's the only answer. I think the desire for commerce would have been also--I agree with the traditional view which is that this would be at a lesser consideration. In any group of people there is a small minority, I want to emphasize small, who just love to do risky things. They just love adventure; they're never happy if they're safe, and so off they go seeking adventure and seeking to make a fortune however they're going to do it. The Greek word for this is, apoikia, and most literally it would mean a home away, an away home. They are establishing for themselves a household, a home someplace away from where they started, and that's the name. Delphi was thought by the Greeks to be the navel of the universe, the center in every way. There the god Apollo had established an oracle. Gases would escape through this gap in the earth and there were priests who worshipped Apollo there. They would place a young woman there who would sit as these gases came up and she would speak in tongues. John Hale of the Yale Class of 1973, who is now an archaeologist at the University of Louisville, investigated this. He found evidence that totally confirmed the Greek story. Greeks and barbarians came to Delphi to consult the Delphic Oracle. Most of the things they asked were questions that really had a yes or no answer. The oracle probably gained its fame for being very good precisely at answering this question. These people knew more than anybody else about these things, and so consulting that oracle was a very act of rational thought, says John MacIntosh, professor of ancient history at the University of California, Los Angeles, and author of the book, "The Lost City of Syracuse" will decide whether it's a good idea for him or not. Recruiting is tremendously important because you need to have a certain number of settlers to make the settlement viable. So however many that is, that is what you try to recruit and you recruit typically at a time when it's easy to get people together so you can tell them the story. There are festivals held in each city just for its own citizens and my guess is that when you could do that, when you felt that you could recruit a full colony from your fellow citizens in Corinth, let us say, that's what you did. were not enough Corinthians who were ready to go with you on your expedition. So, you would try to take your message to one of the Pan-Hellenic festivals which were getting organized about this time. So now you have everything in place, you've recruited your settlement, you get on your ships and sail, in this case out to the west central Mediterranean. You find your way to Sicily, work your way into the harbor at Syracuse and things work out, and now we have this apoikia called Syracuse. The city of Corinth sent out a lot of colonies, which is why we know something about their arrangements. When Potidaea got into trouble with Athens, and found itself besieged, Corinth sent a real army to go in there and fight. At the other end of the spectrum it's again Corinth and they have a colony up in the northwest called Corcyra. The first relationship between them is a navel battle, and thereafter we hear of them quarrelling and fighting with each other just about at least once a century right on down. Kagan: The overwhelming normal situation is the first one I described, friendly relations. colonies to send representatives back to the mother city for the religious observations that were common to them all, so that those create good feelings. Corinth always needs that kind of stuff, so we sell you our wheat, you sell us your pottery, we sell good wine that we can't grow yet and maybe never will be able to grow in our neighborhood, so on and so forth. So you can see why it would be very natural for all sorts of ties to unite these colony and mother city. There are no Greek cities as you keep coming down and pass Palestine. You reach Egypt, and of course, Egypt is one of the great empires of antiquity. The Greeks settle a single colony in the Delta of the Nile of Egypt at a place called Naucratis. You can actually go, now that I realize that Libya is now open; it's no longer a closed territory. You will find very rare cases of a Greek city founded away from the sea; they always wanted to be close to the sea. what can we speculate is the meaning of that? What we find is that the states who are doing most of the colonizing are located where most ofThe trade was going on at this point in history. When I say manufacturing, you understand everything is done by hand, but you see things like shops that contain a number of slaves working for a master. In some cases, especially the later on you go, some shops that have quite a few slaves that worked to produce these things. Well, these places are the ones that have the trade, the industry, and also engage in colonization. Greece's impact was greater in the west and the north than it was in the east and the south. The Greeks lived among people who were more civilized than they. They had very little to teach or to impose upon those people rather than vice versa. It is inconceivable the Greeks could have developed a civilization that they did without contact with these eastern civilizations and learned a great deal from them. It's evident that they borrowed stuff from the Greeks in every part of their lives, in a potent, fundamental way. who have interests rather different from those of the most primitive polis you could imagine. Some scholars early on in the century, moved by Marxist theories, suggested that you had a capitalist class growing up, there's just no evidence of that; it's just wrong. My guess is that the earliest traders of any scope were probably noblemen who also had land and estates back home, but who had the opportunity, the know-how, the connections to make it possible to make a lot of living in trade. is going on. More and more farmers are becoming independent, self-sustaining, hoplite farmers of the kind that I've described. They're not going to feel the same way about it, there's going to be pressure from them for a better participation in the decisions that are made in the state. And also there will be some rich people, very rich people,. rich in a different way from the way people used to be rich; rich meant the best land. There will be people who will have wealth in the form of precious things and I would use the word money. come to mean weights of silver or gold. So now you have a change in fundamental economic things. Well, all of this is tied up with this colonial story I've been telling you. Finally, I think, it works in both directions at the same time in terms of the impact all of these changes have on the political situation. On the one hand the changes, that is (A) the rise of the hoplite class; (B) the development of lots of commerce and industry and wealth in a new kind. especially, some scholars have pointed out, I think persuasively, also for some considerable time provided an answer to that problem in the form of an escape valve. Americans didn't have the kind of terrible class warfare and the terrible warfare within cities that the Europeans had experienced throughout most of their history. Really unhappy and angry people could always go west, get new places. I mean, fundamentally, Kansas is a colony in a certain Greek sense, all of these places are. So, that's part of the story of why America had the very lucky early history that it had. that colonization provided something analogous to that for the Greek people. So now, here we are somewhere in the seventh century, most of these places I've been talking about have been settled, the currents that I have been describing are flowing and the kinds of problems they have give rise to what will be felt in most of the towns. That is the proper introduction to the next topic, which I'll discuss next year. No not--it seems like a year, but it's next Tuesday actually.

ROUGE-1: 27.88, ROUGE-2: 26.53, ROUGE-L: 27.52
BERTScore: 60.95

==============================================
==================== [63/100] ====================
Summary:
Instructor: We are asked, which of the following correctly identifies the areas of consumer surplus, producer surplus, tax revenue, and deadweight loss in this market after the tax? So pause this video, have a go at it. Even if you struggle with it it will make your brain more attuned to when we work through it together. All right, now let's work through this together. And I just want to sort of understand what's going on here before I even try to answer their questions.

ROUGE-1: 16.31, ROUGE-2: 16.15, ROUGE-L: 16.31
BERTScore: 70.34

==============================================
==================== [64/100] ====================
Summary:
Sir Gawain, nephew of King Arthur, was invited to a party at Camelot. A towering knight riding an emerald steed burst into the room and proposed a game. The Green Knight declared he would allow the bravest warrior present to attack him with his own axe. If they could strike him down, they would win his powerful weapon. However, the knight would be allowed to return that blow in one year and one day. Gawain tried to forget this bizarre vision, but despite the strangeness of the knight‚Äôs game, he was determined to act honorably.

ROUGE-1: 24.43, ROUGE-2: 21.68, ROUGE-L: 22.14
BERTScore: 69.18

==============================================
==================== [65/100] ====================
Summary:
well welcome back everybody to uh the last lecture 162. this is kind of a a special lecture um i did get some requests for more information about distributed storage and quantum computing and so i think we're going to do that. i want to make sure that we talk through the chord algorithm since that's a i think relatively simple thing to understand and is very cool and applied pretty much everywhere so if you remember one of the things we talked about last week was basically this cap theorem which was really a conjecture that eric brewer put forth back in the early 2000s. is a good way to understand global storage systems as a result now um at the very end of uh last lecture we were talking about key value stores. Key value stores are very simple in interface excuse me so basically uh you can have an arbitrary key although that's usually a hash over some value. You can have a value associated with it and if you do put a key comma value that goes somewhere into the ether and then when you do get of the key you get back the value that you started with. node except that in reality what happens is this gets distributed over a whole bunch of nodes and so the question is really many parts to this question one is how do we actually do that distributing another is when some client does a get how does it figure out which node to go through. So today i want to tell you about the cord uh algorithm which has been turned into storage systems of many sorts including those used by amazon et cetera okay facebook so um before we get there i wanted to remind you of this notion of recursive versus iterative lookups. Consistency hashing is a way to take your keys and figure out a clean way to distribute them throughout the system without having to know pretty much all of the nodes that are participating. The chord algorithm lets you get by with only knowing essentially a logarithmic number of nodes in the total system and you can still do this well. So we're going to associate each one of those storage nodes is going to get a unique id and that unique id will be in the hash space. These are not evenly distributed in fact probabilistically they're evenly distributed. Just its successor then we can always find the server we're looking for okay now what does this really mean okay so here's this ring and here's you know key 14 stored on node 15 let's say what it really means is something more like this right so these nodes since we we're doing hashes over their ip addresses and some metadata it means that they could be anywhere in the world and then we're connecting them together based on their hash name so fort talks to eight eight talks to 15 and so on so that for instance key 14 happens to be stored here on the east coast node 4 is up in alaska. means that no particular part in the in the world here might be a hot spot it means unfortunately though that we don't have the most uh local of look up because if we start at node four it'd be nice if we could just go down to 15 and back okay now this is a really good question here about redundancy how do we get redundancy out of this for the moment uh suspend that question for just a second certainly we could put raid servers or what you know raid storage on each of these nodes and that would be great if the disks fail but uh we would like something even more powerful. Beachfront property in nevada and then has a plan to basically cause uh california to fall into the ocean and therefore have really expensive properties fortunately superman uh saves the day and it doesn't happen so um okay so if we move um forward with this by the way i'm showing you these clients now to make this a little more clear the clients need to know one gateway into the system in order to talk to the system. So how we can make sure it's connected is we're going to have this dynamic stabilization procedure. The key is to make it even harder to destroy the connectivity of the system. We're going to keep track of a logarithmic number of these pointers and of course the way we find out about them is we just query the ring and ask it oh i want to store each of these keys and what will come back from the ring is which node is responsible the power the powerful thing about this is once i've got all these nodes now i can do a really fast routing process to figure out how to find which nodes is going to store the key i'm interested in. Go on to uh storage fault tolerance for the data does anybody have any questions on this we good okay so um now let's look back at what we had a slide before right so we had key 14s stored on node 15. If node 15 weren't there key 14 would be stored on 20 right that's just the next node up from 14 since the only copy of key 14 it's currently stored on 15 if 15 dies or goes away we don't have the data and so so uh it's fine that the consistent hashing tells us where it should be stored but we can't store it there because we've lost our data. Cord can be used to store locations of data rather than the data and so think of this like a dns built out of cord. The downside of course is performance might hurt if you happen to be too far away from a copy of a chord algorithm. This chord ring is actually used in lots of uh cloud services these days the idea at least. For instance dynamodb and i have a paper for that up on the reading from last time uses the chord rings and you can look down here but it uses them within their machine rooms. have a service guarantee that says we'll get a response within 300 milliseconds uh for say 99.9 percent of the requests okay. This is very in contrast essentially to what we've been talking about a lot of the rest of the term uh which is focusing on mean response time. Instead we want to have guaranteed performance okay and this is again thinking i want you to think back to when we were talking about real time scheduling and what was important there was keeping the predictability of the scheduling time low. it adapts automatically which is pretty good okay so what i wanted to do next uh i'm going to talk a little bit about security and then um talk through a couple of things and then i want to uh try to get to quantum computing as well so we can i know there was some of you asked some questions about that so i'mgoing to leave this topic unless there's more questions okay. Security is kind of dealing with actions of a knowledgeable attacker who's really trying to cause harm and we want to make sure that uh they can't really screw us up okay. security policy built with our protection mechanisms okay so i wanted to point out something interesting i don't know if you've ever seen this before but here is a car in the ditch. Back in july of 2015 there's a team of researchers that took complete control of a cheap suv remotely exploited a firmware attack over the sprint cellular network. They basically caused the car to speed up and slow down and and veer off the road and uh totally wirelessly so this is a little scary to think about now fortunately no humans were harmed. Cryptography has always been confidentiality about encoding information so an adversary can't extract it. The idea of a secure hash function is one where you take data and you run it through a hash function and you get a bunch of bits out of it. If you change the data even slightly you end up with a good hash function with something that essentially roughly half of the bits change. Hashing is pretty powerful and i'm not going to have a lot of time to go through this with you that's a 161 topic but just you know keep that in your lexicon about hashing. Security is all about the data and if you can protect the data then you can protection everything okay and so if you think about the internet of things really we have a whole bunch of devices and compute elements all over the world and it's really a graph of services that we want to connect. We want to make sure that the data can only be written by authorised parties and only read by authorized parties okay and these secure enclaves are a topic for another day as well but this is a special um virtual machine that's in modern hardware that basically allows you to set up a secure channel and do some secure encryption. how we want to be dealing with data all right sorry if that's a lot of information but i wanted to see if there's any questions there before i switch over to some quantum computing all righty give me a second i'll be right back. So first question is how do we know the data is secured so um just like with a blockchain let me just back up to the picture here which i think is a is a good one to be talking about um what we know is the following. bundles of data and if somebody tries to put garbage in there a legitimate person who's trying to look at this can just throw the garbage out because there's no way that that garbage could have been put in there uh in a way that meets the integrity constraints of the data okay. So it's not forgeable um it's uh it maintains its integrity the the transactions can't be swapped or whatever and so it's a unique umly uh high integrity kind of bundle of data. The vision here really is of pretty much everybody using data capsules everywhere okay and if you can get that to happen then you could potentially have a very interesting scenario here. Part of what we're doing is we're working with roboticists and machine learning folks to put their data and their models for grasping and so on inside of data capsules and as a result they can reside securely in the edge in say your robots or whatever in a way that can't be breached okay. This is really targeted at secure edge infrastructure in addition to the cloud so these data capsules can move back and forth. unforgeable all right good so let me say a little bit about using quantum mechanics to compute. It's basically using weird but kind of useful properties of quantum mechanics two of them quantization and superposition. If you're willing to allow things to not be always a one or always a zero what you can do is you can just start doing quantum computing. There are many other algorithms out there now these days these days they've been slowly working on them but these are some pretty good ones that might be interesting. Google and ibm are building quantum computers. The goal is to prove that quantum computers could be faster than classical ones. The machines are currently running at four degrees kelvin or something really cold. There are other types of technologies including ion traps that potentially are pretty interesting that there have been some thoughts over the years might be able to run at something closer to room temperature not there yet the current goal of google and ibM is to do something which they call quantum supremacy which is basically to prove quantum supremacy. Quantum supremacy is the ability of a quantum computer to do something better than a classical machine. This is a demonstration of quantization which is a way of looking at the spin of certain particles like protons and electrons and neutrons. These are particles that have this intrinsic spin and so now i got one and zero or up and down okay and a representation called the heisenberg representation looks at this messy physical situation like this which is either a zero or a one in these brackets and that represents spin up and spin down. something people were looking at okay but the temperature here was less than one kelvin which is really cool okay but let's suppose now here's where the quantum computiness gets pretty tricky okay and and uh bear with me just a little bit i know i'm going a tiny bit over here but um if you think of the zero and the one thing okay this is actually a wave function if you take quantum mechanics representing spin up and spin down and what's interesting is the wave function in quantum mechanics is a complex function. here is actually sort of in one state andsort of in another okay and those are those are two options and it turned out that there was there's a set of famous bell uh inequality experiments that were done that showed that reality is actually the second choice so in fact as weird as it is uh that proton is is a combination of zero and one at the beginning. It's only when we look carefully and force it to be one or the other when we actually try to measure it then it gets forced into a state okay and so if you think about this in terms of building a quantum computer there's some interesting things here. you do a bunch of computing on it such that the probabilities are kept and you measure okay and the way it looks is that you take uh let's say you put an input with all possible combinations of the input input of the inputs being equal values all possible probabilities it looks like you're doing computation on all possible values at once but then when you measure you pick up exactly one and that's the answer you get okay. If you don't do anything very interesting here this is going to look like you randomly picked some input and computed on it so basically what we're talking about here looks like a random computation. high probability some answer that was hard to find that's what we would like okay and so if you look here um you know if the two n inputs are equally probable there could be two to the n outputs that are equally likely. What we'd like is the probability of the outputs to be piled up high on the answer we want and it turns out that something like fourier transform does the trick okay. If we can do a fourier transforms on some input we can actually get an interesting output. So that's the essence of the shortest factoring algorithm. actually investigated if you were to build uh that factoring algorithm and you could do it as quantum circuits that could run on a quantum computer what would that look like. We actually investigated ways of optimizing that and we could actually look at performance of different options for the shortest factoring algorithms. So we built a cad tool to do that so i i don't know i think it's a pretty interesting area right now and there's a lot of interest in it all right so um sorry i kept you guys way over but this is the last lecture i figured if anybody was interested we talked about key value stores.

ROUGE-1: 28.03, ROUGE-2: 27.07, ROUGE-L: 27.45
BERTScore: 76.59

==============================================
==================== [66/100] ====================
Summary:
Early astrologists did not recognize the lymphatics so they thought there are only three system there is a branch of portal or this branch of puerto bean. Originally doctors thought that every corner these three things are present and they call it portal triad. But later on of course the new it is not portal Triad it is portal triads. The liver is really working is that right that blood is again there are two input system hepatic arterial input and what was this portal venous input.

ROUGE-1: 14.07, ROUGE-2: 13.44, ROUGE-L: 14.07
BERTScore: 61.32

==============================================
==================== [67/100] ====================
Summary:
so in the next portion of today's lecture we're going to talk about how we can modify the policy gradient uh calculation to reduce its variance. In this way we can obtain a version of the policy gradients that can be used as a practical reinforcement learning algorithm. The first trick that we'll start with is going to exploit a property that is always true in our universe which is causality causality says that the policy at time t prime can't affect the reward at another time step t if t is less than t prime. the proof is somewhat involved so i won't go through it here but once we show that this is true then we can simply change the summation of rewards. Instead of summing from t prime equals one to capital t simply sum from t Prime equals t to capitalt basically discard all the rewards in the past because we know the current policy can't affect them now. For a finite sample size removing all those rewards from the past will actually change your estimator but it will still be unbiased so this is the only change that we made. that is it's the rewards from now until the end of time which means that it refers to the rewards that you have yet to collect basically all the rewards except for the ones in the past or the reward to go. We will get much more into this in the next lecture when we talk about extra critical algorithms but for now we'll just use a similar symbol with a hat on top to note that it's a single sample estimate all right now the causality trick that i described before you can always use it you'll use it in homework two. think back to this cartoon that we had where we collect some trajectories and we evaluate the rewards and then we try to make the good ones more likely and the bad ones less likely that seemed like a very straightforward elegant way to formalize trial and error learning as a grain ascend procedure but is this actually what policy gradients do well intuitively? We can show that subtracting a constant b from your rewards in policy gradient will not actually change the gradient in expectation although it will change its variance meaning that for any b doing this trick will keep your grading estimator unbiased. The optimal baseline is not used very much in practical policy grading algorithms but it's perhaps instructive to derive it just to understand some of the mathematical tools that go to studying variants. In many cases when we just need a quick and dirty baseline we'll use average reward however we can actually derive the optimal baseline. In order to find the optimal b i'm going to write down the derivative d var db and solve for the best b so the derivative of the second part is 0 because it doesn't depend on b.

ROUGE-1: 32.17, ROUGE-2: 31.31, ROUGE-L: 31.04
BERTScore: 67.87

==============================================
==================== [68/100] ====================
Summary:
Today we're gonna talk about learning in the setting of games. Can you still be optimal if you reveal your strategy? It's actually not the size that matters. It's the type of strategy that you play that matters, so just to give you an idea. And then, er, towards the end of the lecture, we wanna talk a little [NOISE] bit about variations of the game- the games we have talked about. So, uh, how about if you have- how about the cases where we have simultaneous games. In the game of chase- che- and chess, you have this evaluation function that can depend on the number of pieces you have, the mobility of your pieces. Maybe the safety of your king, central control, all these various things that you might care about. And, and potentially, a designer can come in and say, "Well, I care about about nine times more than I Care about how many pawns I have." So the hand-designer can actually hand- design these things and write down these weights. can actually, like, roll two dice and based on the outcome of your dice, you move your pieces various, various amounts to. Uh, there are a bunch of rules. So your goal is to get all your pieces off the board. If you have only one piece and your opponent gets on top of you, they can push you to the bar and you have to start again. So, so what are some features that you think might be useful? Remember the learning lecture? Yes. How did we come up with feature templates? Currently, still bound with the [inaudible]. So maybe like the location of the X's and O's. somewhere. So, so one idea that we can use here is we can try to generate data based on our current policy pi agent or pi opponent, which is based onOur current estimate of what V is. Right. So currently, I might have some idea of what this V function is. It might be a very bad idea ofwhat V is, but that's okay. I can just start with that and starting with, with that V function that I currently have, what I can do is I can, I can call arg max of V over successors of s and a to get a policy for my agent. So, what you have here is you have a piece of experience; s, a, r, s prime, we can try to learn something from each one of these pieces of experience. And then we had a target that you're trying to get to. And my target, which is kind- kind of acts as a label, is going to be equal to my reward, the reward that I'm getting. So, so my target the thing I'm trying to like get to is the reward plus Gamma V of s Prime, w. is the TD learning algorithm. This is all it does. temporal difference learning, what it does is it picks like these pieces of experience; s, a, r, s prime, and then based on that pieces of. experience, it just updates w based on this gradient descent update, difference between. prediction and target times the gradient of V. There are very minor differences that you'll talk about actually at the end of this section, comparing it to Q learning. All right. So I wanna go over an example, it's kind of like a tedious example but I think it helps going over that and kind of seeing why it works. If you use like, uh, initialize rates do not be zeros which you update throughout instead of just to the end. Yeah. Okay and section two, so S4 and S9 are the same future of activities but you said S4 is S9 [OVERLAPPING]. Uh, this is a made up example, [LAUGHTER] so don't think about this example too much though. Well, is it that possible to have, an end state and not end state have the same feature vector, or no? As one, uh, entry that's always isn't [inaudible] like instead of 1, 2, we have 1, 0 leading to the, the final weight then the weight corresponding to that. Is going to- [OVERLAPPING] Yeah. It will never converge. And that kind of tells you that that entry in your feature vector, you don't care about that. If it is always 0, it doesn't matter what the weight of that entry is. So in general, you wanna have features that are differentiating and, and you're using it in some way. on the relationship between the features and the weights. Uh, they always have to be the same dimension, and what should we be thinking about that would make a good feature for updating the weights specifically, like- So, uh, okay so first off, yes, they need to be always in the same- in dimension cause you are doing this, um, dot-product between them. Um, the feature selection, you don't necessarily think of it as, like how am I update the weights, you think of the featureselection as is it representative of how good my board is. to 0.25 and 0.75 then it kind of stays there, and you are happy. All right so, so this was just an example of TD learning but this is the update that you have kind of already seen. And then a lot of you have pointed out that this is, this is similar to Q-learning already, right? This is actually pretty similar to update, um, it's very similar, like we have these gradients, and, and the same weight that we have in Q- learning. The idea of learning in games is old. People have been using it. In the case of Backgammon, um, this was around '90s when Tesauro came up with, with an algorithm to solve the game. And he was able to reach human expert play. And then more recently we have been looking at the game of Go. So in 2016, we had AlphaGo, uh, which was using a lot of expert knowledge in addition to ideas from a Monte Carlo tree search and then, in 2017, we have AlphaGo Zero, which wasn't using even expert knowledge. Minimax sca- strategy seemed to be pretty okay when it comes to solving these turn-based games. But not all games are turn- based, right? Like an example of it is rock-paper-scissors. You're all playing at the same time, everyone is playing simultaneously. The question is, how do we go about solving simultaneously, okay? So let's start with, um, a game that is a simplified version of rock- paper-sc scissors. This is called a two-finger Morra game. So, so you have things like pure strategies, uh, pure strategies. We have also this other thing that's called mixed strategy which is equivalent to, to stochastic policies. So, so pure strategies are just actions a's. And then you can have things that are called mixed strategies and they are probabilities of, of choosing action a, okay? All right. So how do we evaluate the value of the game? Well, I'm gonna use my payoff matrix, right? So 1 times 1 over 2 times the value that we get at 1, 1, is equal to 2. Someone tells me it's pi A and pi B, I can evaluate it. I can know how good pi A is, from the perspective of agent A. But with the challenge here is we are playing simultaneously, so we can't really use the minimax tree. So what should we do? So I'm going to assume we can play sequentially. So that's what I wanna do for now. So right now I'm gonna focus only on pure strategies. I will just consider a setting- very limited setting and see what happens. In a more general case, I'm gonna make a lot of generalizations in this lecture. So I'll show you one example I generalize it, but if you're interested in details of it, like, we can talk about it offline. So, so, so setting is for any fixed mixed strategy Pi A. What I should do as Agent B is I should minimize that value. I should pick Pi B in a way that minimizes that value, and that can be attained by pure strategy. The key idea here is revealing your optimal mixed strategy does not hurt you which is kind of a cool idea. The proof of that is interesting. If you're interested in looking at the notes, you can use linear programming here. So next 10 minutes, I want to spend a little bit of time talking about non-zero-sum games. In real life, you're kind of somewhere in between that, and, and I'm going to motivate that by this idea of Prisoner's dilemma. different players. So the von Neumann's minimax theorem doesn't really apply here because we don't have the zero-sum game. But do you actually get something a little bit weaker, and that's the idea of Nash equilibrium. So a Nash equilibrium is setup policies Pi star A and Pi star B so that no player has an incentive to change their strategy. So, so what that, that means is if you look at the, the value function from perspective of player A. Nash's existence theorem says if any finite player game with a finite number of actions, then there exists at least one Nash equilibrium. In a collaborative Two-finger Morra game, it's not a zero-sum game anymore and, and you have two Nash equilibria. And then Prisoner's dilemma is the case where both of them testify. And yeah, there are other types of games still like Security Games and or resource allocation games that have some characteristics that are similar to things we talked about.

ROUGE-1: 19.76, ROUGE-2: 19.28, ROUGE-L: 19.47
BERTScore: 63.96

==============================================
==================== [69/100] ====================
Summary:
The famous example was posed by Comte de Buffon back in the 18th century. It marks the beginning of a subject that is known as the subject of geometric probability. The problem is pretty simple. We take a needle that has a certain length-- l-- and we throw it at random on the plane. The needle might fall this way, so that it doesn't cross any line, or it might fall that way, and it ends up crossing one of the lines. If the needle is long enough, it might actually end up crossing two of the Lines. much more streamlined. There's not going to be any choices. We just need to consider the event of interest, express it in terms of the random variables that we have in our hands, and then use the probability model to calculate the probability of this particular event. When will the needle intersect the nearest line? This will depend on the following. We have an intersection if and only if the vertical extent-- which is this vertical green segment-- is larger than the distance x. Or equivalently, if x is less than the vertical amount of the needle. our case is four over pi d-- and integrate it over the set of x's and theta's for which the PDF is non-zero. So what are these pairs? This event can occur with any choice of theta. So theta is free to vary from 0 up to pi over 2. How about x? For this event to occur, x can be anything that isNon-negative as long as it is less than or equal to this number. And all we need to do now is to evaluate this double integral. is this 4 with this 2 give us a 2. We have 2l over pi d. And then the integral from 0 to pi over 2 of sine theta. Now the integral of sines is minus cosine theTA. And we need to evaluate this at 0 andpi over 2. This turns out to be equal to 1. And this is the final answer to the problem that we have been considering. And now, a curious thought. How can you figure out the number pi? Take your needle, throw it at random a million times, and count the frequency with which the needle ends up crossing the line. And if you know the length of your needle and of the distance between the different lines, you can use the estimate of that number to determine the value of pi.

ROUGE-1: 32.93, ROUGE-2: 31.64, ROUGE-L: 32.78
BERTScore: 68.97

==============================================
==================== [70/100] ====================
Summary:
Professor: If you have a perturbative treatment, even if you carry to infinite order, you have, formally, divergences if you have resonant interaction. Professor: I want to show you what are the tools to treat those infinity source diverGences in a consistent and a systematic way. MIT OpenCourseWare offers high quality educational resources for free. To make a donation or view additional materials from hundreds of MIT courses, visit MIT Open courseWare at ocw.mit.edu. means to sum up an infinite number of diagrams. And OK, we want to understand the time evolution of this system. Our tool is a time evolution operator. And at the end of the class on Monday, I told you, well, let's simplify things. Let's get rid of those temporal integrations and multiple integrals by simply doing a Fourier transform. And this iterative equation where we get the nth order by plugging the n minus first order on the right hand side. terms, which are 1 over Z minus Eb. Sometimes, scattering and evolution equations are better formulated when you do it in the complex plane. Just remember, Z is the energy. And it is the initial energy. The initial energy is if it's a ground state and a resonant photon, we have a problem. Because the denominator is 0. So in other words, for resonant excitation, we are interested in the case that Z is on the order of this close to Eb. easy part, which has no divergence, we can make any kind of approximation we want without altering the physics. But the resonant part, this needs special attention. We want to figure out which of those expressions include this problematic term exactly twice or three times or four times. So we regroup those infinite sums, these algebraic terms, in such a way that we say, OK, which one has the occurrence of this once, twice, three times, four times? Colin: Is the definition of the S-matrix or the matrix matrix of the Fourier transform? non-trivial expression, the function of the kernel, has no divergences. And therefore, because there is no critical part to it, rather simple approximations can be made. That's an idea you may see often in physics. You have a theory whether it's something complicated, non-perturbative divergence. But you just rewrite the theory, transform the equations in such a way that structure of the equations now accounts for the physics behind it. And you still get the correct physics out of it. light scattering. I just go now and apply to an excited atomic state. So the state we are interested in is the atomic state b and no photons. And the property of the atomic. state is obtained when we know the function Gb of Z. And this is the matrix element between state B0B0 and the time. evolution operator. So we are calculating, of course, the Fourier transform of the time evolution of the state b by the. Fourier. transform through the resolvent G. Fermi's golden rule where we-- well, with a little twist-- have the initial state. The dipole interaction or the [? p.a ?] interaction takes us to an intermediate state with a photon with [INAUDIBLE] and polarization epsilon. We propagate in the intermediate state Ea. And now we have to go back with the same matrix element. So therefore the matrix element is squared. And we have a double sum. We sum over all possible states of the photon. The real part is this matrix element squared, but double sum. But what we use is the principle part of it, which is well defined in the theory of complex functions. So the imaginary part gets us Fermi's golden rule. And the real part has actually-- remember when we discussed the AC Stark shift. And such AC Stark shifts which appear as self energies, as energy shifts created by the state, this is nothing else than the famous Lamb shift. So that's what you have seen. I have already-- do I in this sum? The Markov approximation often means that some relaxation time or some response of a system is replaced by delta function. Well, we've done the same here. If you replace some temporal response function by a delta function, that means the Fourier transform becomes constant. And by neglecting the energy dependence, we are now saying everything is constant as a function of energy. And that means in the temporal domain that we have a deltafunction. And you should now-- well, this is what we may have expected. But there are two things you should learn. infinite summation of diagrams, if we had done a perturbative expansion, we would have never obtained exponential decay. We would have obtained some polynominal decay. If you do lowest order perturbation theory, instead of getting an exponential decay, you would just get a linear slope. And if you fix it, I think you get quadratic terms. So it's not really profound what I'm saying. It's pretty much an exponential function is non-perturbative. we have an atom in the ground state, and a photon comes along, and it takes the atom to the excited state b. Then we go back to the same state-- could be also another state-- by emitting a photon k prime epsilon prime. And the relevant matrix element of the time evolution operator, which is the T-matrix, involves now the matrix element, the initial energy minus the intermediate energy. The critical part is really-- you can do it mathematically. The excited state of an atom comes from an infinite number of absorption, of emission and reabsorption processes. The real nature of this state is that it couples to many, many modes. And you can often neglect that in the simple description of your experiment. But if you take certain expressions seriously, they would have divergences. The next chapter is called Derivation of the Optical Bloch Equation. And yes, what we really need for a number of phenomenon in AMO physics for laser cooling, light forces, and much more. A small system which I just described by Schrodinger's equation, now follows the density matrix equation, has relaxation, the entropy increases, and such. So in other words, we have coupling to the environment. And in this cause in part one, we've dealt with it and looked at vacuum Robi oscillation and a few really neat things. But what happens is that the system is an open quantum system. You can have spontaneous emission. And if your mirrors are not 100.00% reflectivity, some light leaks out. So this is our system. In general, if you write down the total Hamiltonian and do the time evolution, something will come out which, in general, is very complicated, very entangled. So you have to know, to keep track of all the photons, which have been scattered in the lifetime of an atom. But often, what we do is we simply put the photons in a trash can. We trash them. We're not interested in what the photons are doing. We really want to focus on what happens to the atom. How does an initial state of the atom propagate into a final state? This is done by optical Bloch equations. In quantum physics, we really want to find the full quantum time evolution. We have to be careful. The time evolution as a Hamiltonian, if you now bring in the environment, cannot be simply included by adding an imaginary term. And our derivation of the master equation will actually show what kind of operators applied to the atomic density matrix are consistent, are quantum mechanically consistent. This is actually something-- well, we're not doing a lot of equations or work today, so let me rather be a little bit chatty. processes. OK, any last questions? Well then, let's enjoy the open house with incoming graduate students, and I'll see you on Monday. I'll be back on Monday to talk to you about the classes we'll be teaching next year. Back to Mail Online home. back to the page you came from. Click here to read the full transcript of this interview. Back To the pageyou came from, click here to see the full Transcript of this Interview. Back in the page, please share your questions and comments.

ROUGE-1: 27.73, ROUGE-2: 26.31, ROUGE-L: 25.97
BERTScore: 66.85

==============================================
==================== [71/100] ====================
Summary:
David KAISER: In the last few class sessions, we were looking at some changes in high energy particle theory. And then in our most recent class session, we looked at some of the shifts within the fields of study. And today, we're going to focus on a kind of example of that new subfield, a relatively new sub field that's known as inflationary cosmology or simply cosmic inflation. So it's a framework for trying to understand the evolution of our universe over a huge expanse of time, increasingly using tools at the interface. lecture notes on the Canvas site which go into a little bit more detail of some of these parts from the lecture. Again, strictly for your own interest as your interest and time allows. There's some more material there. And again, I'd be glad to chat more about this if questions come up beyond that. OK. So oftentimes, astronomers will describe the most salient features of our universe in terms of what they call large scale structure. It's really quite remarkable. And this [CLEARS THROAT] picture's been emerging really over a century, for 100 years or even more. or our own Milky Way galaxy. Or if we zoom in even closer to home with the solar system or even really in human terms, there are concentrations of enormous matter and energy and activity separated by huge voids. The question is, what could account for that structure across these scales from meters or kilometers up to tens of billions of light years? It turns out that ordinary gravity-- even Newtonian gravity, let alone Einstein's fancier version that we looked at in class, general theory of relativity-- that these gravitational frameworks are sufficient. for an infinite expanse of time. But other colleagues showed at least it was consistent with his own equations to have universes that would change over time, that could either expand or contract. That was actually a prediction made by some of these colleagues even before some empirical evidence began to come in starting in the late 1920s. Hubble found this remarkable trend that the further away from us a given galaxy was, the faster it tended to be moving away from me further still. So you can actually then work backwards and say for how long has our observable universe been stretching? When did this stretching or expanding phase begin? George Lemaitre was an ordained Catholic priest and an MIT trained PhD astrophysicist. He studied briefly in Cambridge, England with one of the first converge to general relativity, Arthur Eddington. He then came to MIT to finish his PhD and then was finding many of these solutions to Einstein's field equations even before Einstein did. And, in fact, Einstein came thinking he must be wrong, and then Lemaitres kept being right. But Einstein started off by always being frustrated that Lemaitr found solutions that Einstein found abhorrent or disgusting. After the Second World War, new groups began coming back to these somewhat old questions. Some of the newer groups had experience with things like the Manhattan Project and in general were much better versed in things like nuclear physics than had been known even in Lemaitre's day. One of the most active groups soon after the war was based at the advanced-- excuse me-- the Applied Physics Laboratory. Here's a famous composite photograph. They're making a not so subtle gesture to the fact that Gamow was widely rumored to enjoy his drink. So his head is emerging from the vapors of Cointreau, of a liqueur. At early times in cosmic history, the universe should've been opaque. You literally wouldn't have been able to see anything because the mean free path of any given photon would be very, very short. The photons would each be trapped, kicked like soccer balls between all these loose electric charges. They begin to piece all this together. At early times, you would have an electrically charged plasma, that the universe would not be filled with electrically neutral atoms because they literally couldn't form yet. Lemaitre: As the volume of space stretches, as you have an expanding universe, the average temperature of all the stuff inside it should fall. At that time, a new phase in the universe would begin to unfold, he says. The universe would be filled with neutral atoms of hydrogen, and now you have a mean free path for light that's arbitrarily long, says Lerman. Lerman: Today, this bath of remnant radiation from that early hot, dense state should be filling the sky in every direction. out I don't attend dance parties very often. This is what the internet tells me they look like. If you just Google "dance party" and throw away the bad pictures-- anyway. So at early times, the DJ's playing some raucous house music, and everyone's just jostling around. The average energy per dancer is very high. That's like the charged particles where the mean free path is effectively zero. No one could cross that dance floor. And then at a calculable moment, if the DJ knows what she's doing, she'll put on some slow music. And you start having couples form like in Harry Potter at the Yule Ball. Scientists predicted as early as 1948 that there should be this remnant glow from the Big Bang. They were using a new horn antenna sensitive to radial microwave and radial band frequencies. This should've been among the most precise instruments available on the planet for that band of the spectrum. But they found this remnant hum in their electronics, and they couldn't get rid of a residual hum. The group here, these folks are rediscovering many of the ideas from George Gamow and his group, actually at the time unaware that Gamow had even done these calculations. In the early universe, there was a huge plasma everywhere, not just in one corner, but everywhere. The photons were everywhere. And it's like sitting in a bathtub full of these photons. And they're just losing their energy as the overall size of space continues to grow. The Big Bang happened at x equals 0, and x equals 1, equals 2. Any place we could put spatial coordinates to on this model, those all experienced the Big Bang at the same time. But that's the reckoning that people like Lemaitre got comfortable with starting in the '20s and '30s. This day or let's just say earnest disagreement over what's called the cosmic distance ladder. We can get the velocity from these very fancy spectroscopic-- I say we. The people who do it are actually astronomers. They kindly share their information. I don't know how to do it well. But one can do it very, very well. What's hard to do is to calibrate. How far away is that object right now, let alone how quickly is it continuing to move away? Astronomer Hubble measured a much quicker average rate of expansion than what we have mostly settled on today. The picture was enough to get a small number of people to pay attention to Georges Lemaitre's otherwise quite obscure mathematical solutions. This is actually called conformal time, which might remind you of our beloved friends, the 19th century Cambridge Wranglers. We're really doing a very similar thing here, to adopt coordinates for the rate at which time takes into account changing rates over time. According to Einstein's equations, the shape of space could either have a positive geometry where it closes back on itself like the surface of a sphere, an open or negatively curved geometry, or a flat geometry. What controls which geometry you have is this ratio of the actual amount of stuff per volume, the actual density of matter and energy per volume. Robert Dicke introduced this conundrum in 1969, so soon after the discovery of the cosmic microwave background radiation when people began to take the Big Bang model more and more seriously. We receive a remarkably uniform signal on the sky today from opposite sides. But those photons were emitted at a finite age when the universe was only a short portion of its current age. So the horizon distance was actually a factor of 100 shorter than the smoothness scale across which we receive remarkably uniform information. And how could that be? If this portion of the sky never had a chance to become in any kind of physical equilibrium or even exchange a single tweet, to have absolutely no information about what the average conditions are in this part of the universe? The Big Bang model has still had to assume by fiat with no real explanation that there was some initial lumpiness, there's some inhomogeneity that over time could then grow to become this cascading hierarchy of scales. So we'd have super clusters of galaxies separated by huge voids and all the rest. That's the point of this series of ping pong balls distributed through space. And that's what it's like to have these causally disjoint regions emitting these photons with the exact same energy. the Big Bang model had some amazing successes but some pretty stubborn quandaries as well. So I'll pause there again and ask the questions about that. Any questions on the shortcomings of the Big Bang as people began articulating them throughout the '60s and '70s? Feel free to jump in or use the chat or either way. And again, there's more on the quantitative details of that in that optional primer you can find on the Canvas site. So Fisher asks, is it useful to think of the universe as spherical still? Yeah. These pictures get pretty hard. like circa 1980. He was not originally asking questions about the cosmos, but he was haphazardly encountering some of those questions, again, very much like Tony Zee around the same time. What Alan was interested in was in things like spontaneous symmetry breaking and the Higgs mechanism. If there were a time during which the matter that's filling the universe could be temporarily stuck in a metastable state in which it had some non-zero potential energy, that could have implications for the global shape of space. even as the volume grows exponentially. That could happen. Alan began wondering if you have this weird state of matter that was at least hypothetical and of right interest to particle physicists. Alex, I'm going to skip the monopolar problem, but it comes from this discussion as well. And I'd be delighted to chat more about that if you'd like afterwards. But in the interest of time, Alan was worried about some exotic features from these Higgs fields that can get twisted up in some topological shape. But he was really just wondering what happens if the universe gets stuck even temporarily. equations exactly in the form that he began learning from Bob Dicke from that series of lectures, then you have these very different solutions for the average size of space. It grows exponentially quickly. And as Alan and others were quick to confirm, this happens very naturally, or at least it's a kind of feature that one stumbles upon readily, if when studying these exotic Higgs-like fields from particle physics. And then again, realizing that if you study the dynamics, the behavior of these exotic quantum fields like a Higgs field. in a stretching space time, if you take that stretching of space seriously, then you don't even need to cook up those exotic Higgs-like potentials that Alan was first thinking about. Quite generically, you'll have a damped oscillator behavior. This comes from the fact that space itself is stretching. And that alone it turns out is enough to find these self-consistent solutions in which the field moves very slowly. You can imagine it rolling down this hill, rolling down-- sorry-- rolling down slowly as a function of time. The latest measurement from the Planck collaboration using a satellite is that this parameter in our actual universe today is 1 to better than a percent level accuracy. In grad school, it looked very much like omega was 0.3. And if you squinted at it, you could maybe make it 0.35. It was not 1 according to the best. In more quantitative detail in the primer, we'll go into more detail about the new measurement in a few minutes. We'll be back next week with a look at some of the other things we've learned about the universe this week. If inflation happened, there should've been a very brief period before what had previously been called the Big Bang. So we're adding more real estate along our time axis. We're unfurling a little bit extra time that hadn't been taken into account in the standard Big Bang model. And so the universe was so tiny, it could very easily have been in a kind of equilibrium or at least a causally self-connected state. So during this tiny blink of an eye, the universe grew exponentially. moment, that field would be subject to slight, slight quantum fluctuations in the distribution of energy across space. That starts to yield this tiny little fluctuation in why there's slightly more matter and energy in this region of space than the other one. So now those very tiny quantum scale fluctuations get stretched as the whole universe stretches. As the scale factor grows exponentially, you have the average length between the distance between crests of those tiny wiggles get stretched to galactic and even super galactic scales all within that blink of an eye. higher energy photons in the CMB and slightly lower energy photons. And the idea now is that the regions of the sky from which these photons were emitted are telling us about the very, very tiny unevenness in the distribution of matter and energy. The photons we receive today had to spend less energy gravitationally to overcome that very tiny gravitational potential. So we can actually map the quantum fluctuations which leave an imprint in this dynamical fabric of space and time that then maps to this distribution. solid green line is the generic prediction from the simplest models of inflation, what's the pattern of bumps and wiggles on the sky you should see today. The red dots are the actual observations from Planck team. And in many cases, the error bars are expanded so we can see them with our naked eye. So now not only do we know do we live in a universe that is indistinguishable from flat as inflation suggests we should, but the actual pattern of those wiggled matches predictions to, again, better than a percent level accuracy. should be slightly more or less energetic depending on the quantum fluctuations of that Higgs-like field. There should be primordial gravitational waves as well. This is now much like the waves that Rai and his huge team found locally from the collision of, say, black holes. Inflation says similar kinds of things should've been happening in the earliest moments everywhere in space through this very violent, rapid stretching of space. So a version of these were found by the LIGO collaboration and announced early in 2016. These are not primordial. this periodic, very particular pattern of squeezing and stretching. So while the atoms are forming, gravity waves would be rippling through them. That should yield a characteristic twisting or curl pattern of polarization in that cosmic microwave background radiation. If you zoom in by a factor of another 20, you should actually see a corkscrew pattern. In March of 2014, a team using the BICEP satellite at the South Pole announced they had actually measured exactly that twisting pattern. Unfortunately, pretty soon after that, it turned out the BiceP team had measured data consistent with local noise. makes specific predictions for what we should see on the sky today, including very minute statistical predictions for things like the cosmic microwave background radiation. The simplest models fit to unbelievable accuracy despite what my mean dormmates used to say in the mid '90s. So why is the universe lumpy? Why is this cascade of scales? Because space time is wiggly, and matter is jiggly. Now, there's an alternate hypothesis, my final set of slides. I mentioned this last time. And I just want to make it clear. why the universe is so messy is actually because Alan's been generating the mess in his own office, and it's expanded to cosmic scales. So if you want to study that part of today's lecture, it's probably the most important lesson, you'll ever take away. And I'll be glad to stay a bit longer if people have questions. Again, I'm sorry for running late. Feel free to drop off if you need. Any questions on that? The photos in Alan's office are on Canvas.

ROUGE-1: 33.06, ROUGE-2: 31.60, ROUGE-L: 30.94
BERTScore: 65.01

==============================================
==================== [72/100] ====================
Summary:
The Peloponnesian War was fought in 431 BC. After the war, Spartan power had grown to an unprecedented degree. For the first time there were lots of Spartans, who had lots of money. The Spartans had choices that they could take. They could either stay in the Pelop onnesus, or they could contest it in their power to control the entire Greek world in the east. Or they could have some control of the Aegean and the Hellespont. the great rhetorician and sophist Gorgias and he was also in the circle of Socrates, along with Plato and Xenophon and various other bright young men of the upper classes in Athens. Also, he was a poet, an orator himself, a philosopher and so on and some of his fragments remain for us to look at. But one thing that he was by 404 was a bitter enemy of the democracy. He had been exiled or had voluntarily taken exile, in order to get away from the democracy, and was determined now that there should be no democracy in Greece. Lysander Lysander set up the Thirty to rule Athens in 404. Lysander agreed to the idea of making the Thirty of twenty men who were oligarchs. Critias' oligarchs were very extreme, but allowing Theramenes, an Athenian general, who had taken part in the democracy, to remain in power. The oligarchic revolution of 400 year brought about the revolution of 411 and again, the Thirty had made something like something like that, although it's fair to call it fair to oligarchs, although moderate. Thrasybulus, the leader of the Thirty, was deposed by a group of 3,000. He appealed to Sparta for help against this democratic army that he had put out. The Spartans did vote to send an army in there to deal with Thrasy Bulus, but they did not put Lysander at the head of the army. Instead King Pausaniasas was sent out to do the job, they met under ThrasyBulus and defeated him. For one thing, as they had in the past, the Athenians fought bravely and well and inflicted serious losses on the army, but it was also obvious that Pausanas was willing to negotiate.

ROUGE-1: 7.21, ROUGE-2: 5.93, ROUGE-L: 6.33
BERTScore: 55.43

==============================================
==================== [73/100] ====================
Summary:
The coherent state has a simple definition, simple but subtle. It's an eigenstate of the annihilation operator, and it has a complex eigenvalue alpha. The person who popularized those states was Glauber, and he got amply rewarded for that. We are now using the coherent states to look at any other quantum state of the electromagnetic field, any statistical operator which describes photons by forming the diagram matrix element of the statistical operator with alpha. And now we realize that coherent states are not as wonderful as I believe. look at a number state-- Well, you often know in quantum mechanics, number and phase are complimentary. If the number of photons is fixed, you know nothing about the phase. And indeed the quasi-probability of a numberState is a ring. It has no phase. It's completely random phase over the 2 pi circle. The energy is sharp of aNumberState, since the energy is e squared. But what you get is also something blurred on the order of unity. In quantum mechanics, the coherent state is a minimum uncertainty state. The product of delta x delta p is just-- is it h-bar or h- bar over two? One of the two. And therefore, you can never have a quantum state which is less fuzzy than the coherentState. If this area is determined by Heisenberg's uncertainty relation, what can be maybe deform the circle into an ellipse, and these are three states of light? That's what you're going to do in the second half of the class. have a product which is fully symmetric i in the ordering of x p, which is anti-normal or normal. The reason why I picked for the course Q of alpha is that it's a real probability, it's always positive. The other guys, P of alpha, can be positive or negative. And also, W of alpha can be negative or positive. But the fact is all three have their advantages and disadvantages. So they all have pluses and minuses. And as a result, it can be written like this. the coherent state is now not this Gaussian. It doesn't have thisGaussian distribution as a course of probability. The probability of the coherent state alpha has a delta function peak at alpha, which is sort of nice. And the number state is not a ring of a finite radius. You would naively expect the energy is sharp. The square root of the energy's electric field, shouldn't it be sharp? And indeed, it issharp. It's actually worse than aDelta function. W stands for Wigner distribution, which is something you actually find in most quantum mechanics textbooks. The Q and P distribution are more common in quantum optics. All the three distributions are the same. It's more sort of on the level of whether something is a delta function or has widths unity. So on the small scale, it matters. But if you map out something on a bigger scale, they are all related to each other. And for the rest off today and the next class, when I project onto the vertical axis to get the electric field, I'm not completely rigorous which of the three functions I've really chosen. Coherent states are minimum uncertainty states. Coherent states were maybe invented by simply saying, We have an harmonic oscillator data. We want to find the minimum. uncertainty states for which the uncertainty in x and p, when expressed in. natural units, is the same. The fluctuations of the intensity are usually expressed by the second order temporal coherence function, which is what we want to introduce now. It's simple. But I always feel that if you really appreciate the quantum mechanical character, you have to know the classic description first. In quantum mechanics, the expectation values would need averaging over time, not over ensembles of I of t. In classical physics, you can determine an ensemble average by taking an ergotic system and observing it at many, many times. The idea is that one system as time goes by will sample all possible states. So in other words, if you switch on a light bulb with a stable power supply, that the light emitted by the light bulb will go through all possible quantum states as time evolves. of tau is we measure the intensity now and a little bit later. But measuring the intensity really means absorbing photons. So what is more closely related to an experiment how you measure the correlation function is you want to look at the probability of absorbing 2 photons. This suggests that experiments where we look at two subsequent clicks of a photomultiplier where we determine the photon correlation, that this is measuring a correlation function, which, for quantum states of light, should be defined as the expectation value of a dagger. Professor: As long as we limit ourselves to a single mode of the electromagnetic field, things are independent of tau. He says if g2 of t Tau changes, it comes because you have several modes. But if you have one mode, there is no coherence time, he says. Professor: In your homework, you will show immediately that the g2 function is related to number fluctuations. It's related to an average and an n-squared average, and it's independent of time. Vladan Vuletan: Coherent states, as I've just shown you, are very classical. They've always a g2 function of 1. And attenuation is not changing it. So a coherent state with an expectation value of 1 photon is not a single photon. But of course, there are ways how you can get single photons. And this is, well, you start with single atoms. Namely, if you have a single atom in the excited state, it can emit only one photon. So that's a way how we can create non-classical states of light. The Hanbury Brown Twiss experiment was the first experiment which really looked at g2 functions correlations. It was the beginning of quantum optics and modern experiments with light. The classical limit is always a limit of high intensity, so at any given time, you have a ton of photons. If you put a light bulb into a cavity or couple the light from a light bulbs into a fiber, the light becomes spatially a single mode. That's the only way how you can distinguish a lightbulb from a laser beam. The only property which distinguishes a laser from the thermal light source-- we're not taking advantage of it. You cannot create an intense enough single mode light source unless you use stimulated emission into a single mode. There are LEDs or semiconductor devices, which provide photons with interesting statistical properties. The correlation function for, for example, an LED light-- that's not really a thermal source. It's not described by a Maxwell-Boltzmann distribution. It would be closer to something for a laser, even though there is no cavity.

ROUGE-1: 20.88, ROUGE-2: 19.75, ROUGE-L: 19.91
BERTScore: 63.06

==============================================
==================== [74/100] ====================
Summary:
This lesson will first dive into some signal Theory and then move on into things that we're more familiar with things like deconvolutions and using Transformers for next note prediction. The first thing we want to talk about is how can we sample and quantize a continuous time signal. We'll then go into some geometric signal theory and with Transformers and finally how how we can kind of generate sounds using these. The lesson will end with a soft introduction to digital signals and then we'll move on to the next lesson. Not very easy for a computer to do given that every operation needs to be in on a continuous time signal. The way that we we can fix that is through the process of discretization or to to make a an analog signal a digital signal for us to be able to process. The two kind of main ways that we can make our signal easier to process is one by taking samples at certain time periods and two by quantizing our level so instead of dealing with A continuous scale we can quantize at certain levels. voice and Pitch it up very fast right what quantization level do we do we want there. Can we do a lossy pitch up with a uh with by filling in the the blanks in some intelligent way through prediction or kind of note fitting which is an interesting consideration I think given the fact that audio is a continuous time signal um the digitization process and the choices you make matter a lot and because of that this field is so interesting and there's a lot of really didactic work around how we can take these continuous signals. The aliasing phenomenon is incredibly interesting this happens both visually and um auditorily. aliasing is the byproduct of poor sampling. A lower wave resolution will result in a modified output signal as compared to the original input that we're trying to process. There's a lot of literature about um aliasing effects including spatial illnessing. The effect of aliasing on digital signal processing can be seen in a number of different ways including in the way that the image is compressed and the way it's displayed. We're able to gain information through this uh this unit process so the way that this works is um our down sampled waveform is is initially sent through eight down sampling blocks. At each layer the number of filter Banks is doubled. We're not losing information but we're developing a new representation of these images. These kinds of techniques are used in a variety of ways to reproduce music from bands in the 1900s. We want to use Transformers for audio we're going to kind of transition now into the next generation of audio we can use Transformers to predict the next note for example. the better your model will be and the more generalizability you have in your Transformer the better it'll perform right we talked about Occam's razor. A generalized Transformer a generalized solution can fit the Goldilocks of what we want in a model. It's easier for machines to predict keys without flats and Sharps um which has you know similar to what humans do it's easier to focus on the regular keys on piano um so this specific example was trained on on those um however the glass andSharps with specific augmentations and specific training processes can also be added to our vocabulary. example um this specifically I believe this uses like a music 21 framework but it's able to you know tokenize a certain item and then you're able to transpose this to a certain amount of notes or a certain key. Just by transposing you'reable to increase your your training sample size which is very cool and can improve model performance by a ton. The next thing to consider is positional beat encoding right we want to include some metadata to feed into our model to give it a better sense of musical timing because the position of the token in our our tokenized representation it doesn't correspond to its actual position in time. Then it won't really learn a a very good representation of the data that you're providing right we want to be able to mask information that it previously had as well as mask info that it'll have in the future. We want to apply an attention mask to keep the model from peaking and essentially leaking information at the next token it's supposed to predict we can do this by kind of observing this model right um so here we at each at each step the model can only see itself right at the first step where zero is a token you can see one is a tokens you can't see. things a try yourself uh yeah thank you guys for tuning in have a good one. Things you might want to try yourself. Things that you may want to give a try. uh yeah. things you might be interested in trying yourself. things that you might like to give yourself. uhYeah. things a try yourselves. uh Yeah. Things a try themselves. Things your might like yourself.things you may like to do yourself. thanks you guys. for tuning into this week's episode of The Daily Discussion.

ROUGE-1: 20.29, ROUGE-2: 17.98, ROUGE-L: 18.41
BERTScore: 69.52

==============================================
==================== [75/100] ====================
Summary:
Professor: The amygdala is closely connected to the basal forebrain. Professor: The abnormal brain connections that we know occur, at least many types of schizophrenia. He says the earlier the lesion, the greater the plasticity, the more chances of sprouting the connections that are the basis for this idea. In green, there are the very widespread catechol-oligosynaptic projections, which show the extent of the brain's plasticity. In blue, the acetylcholine containing neurons that you see in the medial septum, which we mentioned last time. and early damage? The fact is, a number of schizophrenics, particularly the ones that are the hardest to treat, have enlarged brain ventricles. Many of them do have this temporal lobe damage. And in every case these four systems of widespread projections all go to the amygdala as well as to other parts of the brain. The dopaminergic projections are a little more widespread according to recent studies than they were in the earlier studies. But also we know the basal nucleus projections are shown here. Very widespread. that we know if you bind the receptor you'll reduce the effects of the [? rejections. ?] If the prefrontal cortex is functioning abnormally because of sprouting of these axons, also the basal forebrain, then binding to the receptors will move it more towards the normal. Just saying that it does [INAUDIBLE].. Let's talk about the other part of the basal ganglia, the larger part, the corpus striatum. Going back to these earlier pictures where I talked about the evolution, remember very early there was no dorsal Striatum. It was more of an olfactory structure.

ROUGE-1: 9.53, ROUGE-2: 8.60, ROUGE-L: 7.95
BERTScore: 60.70

==============================================
==================== [76/100] ====================
Summary:
Learn how the solar cell device converts sunlight, the input energy, to some usable output energy, which is in the form of electricity, typically, from a solar panel. Learn how to minimize the amount of light reflected or not absorbed into maximizing amount of life that's actually absorbed. Learn about the duality of light, or how to think about light as a particle, or alternatively, as a quantified particle. Use the weekly Newsquiz to test your knowledge of stories you saw on MIT OpenCourseWare. a particular refractive index. It's comprised of a real component which indicates the phase velocity inside of the material and an imaginary component, which can be thought of as an extinction coefficient. We use that information to calculate engineering relevant parameters such as reflectance of light off of a surface. So this is a method for you to gain a foothold in this new area of understanding the refractiveIndex of a material based on something you've already seen before. So I would advise taking this analogy as far as it will go until it breaks down. And you'll see at some point it actually does. We need to understand how light gets absorbed inside of matter. We apply a very simple formulation inside of this class, which is called Beer-Lambert's Law. As we increase the thickness of the polyethylene, we will plot the total transmitted light as measured by that photodiode. And so I'm going to come up with a hypothesis of what's going to happen. And it's a logical thing you might assume. But we're going to test it. And then we'll walk through a derivation that will correct our missed logic. As we keep increasing and put one layer of polyethylene, that drops to 0.75. The amount of light that's absorbed is following a curve looks something like that. So if we assume that light is coming in a medium and light is decaying in some function to that medium, we know, of course, from our little experiment that it follows some exponential function. But how do we justify that to ourselves? Well, first off, we're going to ignore reflections off the front surface. These things don't reflect a lot of light. by some sort of scattering intensity within the medium-- and this sigma here can refer to a variety of processes. If we increase the total thickness, we're going to decrease the total amount of light coming through via that exponential function. The alpha, obviously, is going to be very different for our atmosphere than it was for these little polyethylene sheets. Because the nature of the scattering and absorption processes are very different. for the atmosphere than for here, the density of the material and so forth. Most solar cells that you see of crystalline silicon are on the order of 100 microns. The record efficiency of gallium arsenide solar cell is a few hundred nanometers thick. If you absorb 90% of the light on the first pass, you'll absorb 99% of it on two bounces, right? Or in one bounce, rather, and two trips, two optical path links through the material. And over the next few slides, we're going to learn how we engineer that. electrons instead of the system is changing from one medium to another. The light path will obey what is called Snell's Law, which is the product of the refractive index and sine of that angle, the angle relative to the surface normal. So there are two benefits to texturizing your front surface. One is you have an additional pass, additional bounce, an additional encounter with the material. And the second benefit is that you're able to increase the optical path length by the delta in refractive indices. most often, depending on the angle. You have what is called total internal reflection, which is this case right over here. You might change the nature of the anti-reflection coating on the glass. Even if the panel looks black, there are some really aesthetically pleasing solar panels out there that look completely black. They may still have white back skin, but the glass is just very good at absorbing that light and preventing it from escaping. To engineer front and back surface reflectances, you really have to carefully select your refractive indices. When you look down, the ray of light is traveling like this and it bends toward the normal and likewise symmetric. So you're seeing material down there. What change of property would give you these two images over here? In one of those two images, the refractive index of the medium inside the pool is not 1.3. It's 0.9. And in another one of these two, therefractive index is actually going to be negative. So which of thesetwo do you think is which? There is a limit to how much we can trap light simply by modifying or corrugating the surfaces. A gentleman by the name of Eli Yablonovitch, who's now a professor in Berkeley calculated these parameters I think back in 1982. He came up with an upper limit to the optical path length. That's a pretty good litmus test for the ability of a material to trap light. If you have silicon, for instance, with a refractive index of, let's say, in the infrared some around 3.6, your Yabonovitch limit is around 50. In most solar cells, we want to suppress reflection. We go to great lengths to make sure that this thickness as well as the refractive index of the material is optimized for a particular system. So calculate for me what is the optimal thickness of an anti-reflection coating of silicon nitride? And we'll give it a refractiveIndex of, say, 2.1. Let's call it 2, just make our lives simple. And the peak of the solar spectrum is 550 nanometers. surface of a material, let's say right here, then you can cause each node, each point within your material, to lag by an increasing amount, so that your wave front now bends. And that will cause the light, essentially, if you trace through the points of maximum intensity, say the pink, you'll see that the light is bent. And so it's really exciting. There's stuff coming up every day. This is the point. We can, in principle, if this is hot off the press-- and then of course there's a whole flurry of researchers out there trying to figure out how to use this to our advantage. on light trapping and light management. Mostly it's for photonic devices, but they can be transferred over into solar cells as well. And another example of the photon up/down converters, there's recent reports in SPIE, a lot of interest in the optics community. There was a TR35 award given to a person who studying this topic. So it is, as well, a very exciting and up and coming field. Again, the opportunities there of manipulating light are large, are vast.

ROUGE-1: 17.00, ROUGE-2: 16.06, ROUGE-L: 16.33
BERTScore: 61.72

==============================================
==================== [77/100] ====================
Summary:
Professor Steven Smith: I want to look at two sets of issues. One is Locke's theory of the constitutional state, particularly focusing on the role of the executive, vis-a-vis the legislative branch of government. The other is thinking about Locke and the American regime and the current state of political philosophy, modern contemporary American political philosophy. Smith: Locke doesn't endorse necessarily one particular form of government from any other. He is an advocate of what we have come to call limited government, of constitutional government. John Locke says in the case of a fire in a city it is sometimes necessary, he says, in his day for the fire department to tear down the house of an innocent person to prevent the fire from spreading to other houses. He understands this as a piece of prerogative power acting for the public good. But the question for Locke, as for any constitutional lawyer, is what are the limits of this power? What check, if any, is there on this power to prevent their abuse? LZ Granderson: John Locke gave the modern constitutional state its definitive form of expression. He says Locke's doctrine of consent and legislative supremacy should make him a hero to Democrats, to radical Democrats. LZ: Locke's conception of natural law, rights, government by consent, the right to revolution and all are all part of the cornerstone of our founding. Lz: A judgment on America is very much a judgment on the philosophy of Locke, if anyone is to be considered to be America's philosopher-king. A return to Locke such as it is, even if such a return were possible, is by no means a panacea to what ails us. Locke's effort to build a kind of modern republican government on the low but solid foundations of self-interest and self-ownership could not help but generate its own forms of dissatisfaction. America, as a former teacher of mine once said, is the land where the many facets, the many faces of modernity are working themselves out. We are but a moment in the kind of comprehensive self-dissatisfaction that is modernity.

ROUGE-1: 11.84, ROUGE-2: 11.11, ROUGE-L: 11.37
BERTScore: 64.00

==============================================
==================== [78/100] ====================
Summary:
JACK HARE: Let's do a little recap on electron cyclotron emission. We expect to have multiple different peaks, even from a single particle. These peaks are going to be occurring at frequencies. These tend to be in an optically thick regime. We can go back and we can use these measurements to identify three specific points inside our plasma that we've been using as part of this example. But, in general, you can get it all through the way through the plasma. temperature as a function of frequency. Then we can say, well, that frequency corresponds to a certain magnetic field. And then we can says that magnetic field corresponds to. a certain spatial coordinate. And so, this is a technique which will give us, by looking at the spectrum for lowish frequencies, the first or maybe the second harmonics of this cyclotron emission, we can work out what the temperature is as a. function of space. This is correlation ECE, often called CECE. what we want to do is measure very small temperature fluctuations. We want to measure temperature fluctuations within the plasma that are maybe on the order of 1% of the baseline temperature. If the mean temperature is just Te, and we have some fluctuation around 1%, this intensity will also fluctuate by about 1%. And that 1% is actually extremely hard to measure. And this is because the noise is just too high on these systems. There are lots of different contributions to the noise. But, in general, they all add up to make it very hard toMeasure these very small fluctuations. want to understand in plasmas so that we can build an economically viable fusion reactor. Now, the fact that the noise is too high does seem like a big limitation. But there are some clever tricks that we play where we use correlations. And I'll talk now about what exactly these correlations are and how they provide us with information that allows us to get a signal out despite the overwhelming [INAUDIBLE]. So our setup here is borrowed from ASDEX Upgrade. And, in effect, I referred to Alex Creely's PhD thesis, which you can find online if you want more information. can get very detailed. So in ASDEX Upgrade, we don't really have a circular cross-section plasma, but I'm just drawing it like that, be a nice D-shaped plasma, got our plasma inside here. And our system, at first glance, looks an awful lot like the system that we sketched out up here. We're going to have some sort of special lens. It turns out, you can make lenses for microwaves. And that lens is going to collect light from a region like this. some sort of top hat. It's centered around the frequency where most of the electron cyclotron emission is. And this has a bandwidth of 10 gigahertz. So we've cut out an awful lot of the radiation in bands that we're not interested in. We are no longer going to study those, any bremsstrahlung, any higher order things. This is going to capture all of the information in, say, the first harmonic within some relatively small window. frequency range. Once we've got our bandpass filtered signal here, 110 kilohertz is still too fast for us to digitize. This would be an extremely expensive digitizer. So, what we actually do is we downmix it with the signal at 100 gigahertz. And so, then we get out our beat signal, which we can digitize, which is at 10 gig ahertz here. So this is-- actually, I will write this as 0 to 10 giga hertz. This is the sort of signal that we can actually digitize now. downsampled these is that digitized is now much less expensive. We're doing it something like 4 mega samples a second. And that is quite an affordable digitizer compared to the ones you would need to digitize this signal. So I have not yet told you anything about how correlation ECE works. I'm just giving you an outline of exactly how these measurements are made with an example from ASDEX Upgrade. But there are other similar devices on other tokamaks. many oscillations. And so, it will average out to-- well, because it's power, it'll average out out to 1 or something like that. So it's some DC offset that we can subtract off after. So all this is doing here is mixing the signal down so it gets to a regime that we could effectively digitize. Yeah, another question. [INAUDIBLE] JACK HARE: So if we were doing geometric optics, which we're not, then you would have a lens like this. That lens could collimate your beam. The idea here is that there was some region over which, in the transverse direction perpendicular to your collection volume, you have a very narrow scale. This means that you can actually collect from a very small region on the order of 100 microns. What they were actually doing with this system is all of these were very tunable. And so, if they wanted to look at turbulence in the edge or turbulence near the center, they could actually tune all their bandpass filters to do that. Watts: The technique is used in a variety of applications. He says it's used to extract temperature from the noise of plasma. Watts: The noise is uncorrelated at random, but these two signals will correlate together, and we'll be able to measure it. The temperature and the noise are correlated, he says, and that's what makes it useful in a lot of cases. The technique can also be used to detect cancerous cells in the human body, Watts says. is an example of nominative determinism. Someone called Watts goes around making power measurements. Yeah, and this technique is incredibly powerful because it's enabled people to measure, again, delta Te upon Te on the order of 1%. And they've done this at 13-- on ASDEX Upgrade-- 13 radial locations. So 13 positions they can measure these temperature fluctuations. And they have done this with a time step of 100 kilohertz. So 100,000 times a second they've been able to measure these temperatures. we have positioned these two volumes, which are producing frequencies omega 1 and omega 2. And we think that that distance is smaller than the size of our turbulent eddy. The temperature should be the same going up and down. And if you do this correlation and you get out nothing, that probably means your volumes are too far apart because there is-- this T correlation would just go T1 T2. And there's no good reason to believe those temperature fluctuations are correlated, because they'll be part of a different turbulent Eddy. situation where, at some point, your eddy has moved across, and these two are no longer correlated because the next two are correlated. So it-- yeah. I think we're reaching the limits of my knowledge. JACK HARE: I mean, there's no reason to believe that each of the fluctuations is the same size. So this is going to be a time-evolving system here.Yeah, exactly. Cool. Any other questions? Anything online? Now we're going to go to bremsstrahlung. here with the ions and electrons as point particles. There's a semi-classical approach where you start bringing in some quantum physics and treat, I think, the electron as a wave. And then there's a full quantum approach. What's remarkable about all of these approaches is they all give the same answer with just a very slightly different coefficient. So we get a small change in coefficient. The scalings are the same. So, in some sense, although it's important to get the exact coefficient, it doesn't matter exactly which one of these techniques you use. from the point of view of this course, it makes no difference. Yeah, I think it's kind of remarkable that it doesn't make any difference. So, again, if you want the full treatment, go have a look in Hutchinson. And there's also a long treatment in Jackson of this same problem. I'm just going to quote some results. I kind of already spoiled it now. It's here. For the Maxwellian average, because we can have all sorts of different distribution functions, but our plasma tends towards a Maxwellian. This, therefore, has units of watts per hertz per meter cubed. And these constants are things like e, the electron charge, and the electron mass, and epsilon 0, and h bar, and c, all arranged in some way to make all the dimensions work. And you'll also find this Gaunt factor varies very weakly with the temperature of the plasma here. So this is a pretty weakly varying function. We can change it by two orders of magnitude in this parameter here, but this only changes by one order of. In general, it's reasonable to just treat G as being a constant and then, for this calculations we're doing, where we're going to drop the absolute intensity, we can just drop G with all the other constants as well. But if you want to go back and do this properly for some measurement that you's doing, then you'd have to include this. And so, this emissivity, as a function of photon energy, is a very simple function because it just decays exponentially with photon energy. And that is all you need to do Problem Set 3. In reality, for some plasma, we might have a spectrum that looks like this. So it goes back to being optically thin here for the high energy photons. But the low energy photons will get absorbed as well. There's actually another effect, which is in Hutchinson's book, which I haven't covered here. And so, the spectrum will be even further modified because that wave will be evanescent. But we're skipping over that this year. But it's in there if you're interested. at synchrotron, yes. So cyclotron is when the particles are spiraling around the magnetic field. Synchrotrons are used as a source of X-rays for diagnosing many other things. So it's interesting in its own right, but I don't know whether people use it as a diagnostic. It depends mostly on the magnetic fields and the radius in a tokamak. But maybe there's a really clever diagnostic you can do, like fast particles or something like that. That entire acceleration yields one photon, one energy. And so, initially, we have a kinetic energy, a 1/2 mv squared. Then, afterwards, we're going to have an energy 1/ 2 mv prime squared minus h nu. Now, if h nu is less than the initial kinetic energy here, then we still have some kinetic energy left. So v prime is greater than 0, our particle is still free, and it can continue. But, of course, there's another case where this photon takes away so much energy that this starts, I guess it becomes imaginary. where we have a range of different discrete energy levels that the electrons can occupy. These energy levels are labeled by the principal quantum number n. n equals 1, 2, 3, 4, and so on. And, up here, infinity, this is ionization. If your electron gets this much energy, it becomes free again. And the energies of these levels are given by this unit, Ry, which is the Rydberg z squared of our ion over n squared. And that's the thing that we're going to see with our spectrometer. The spectrum, therefore, looks like we had something like this for bremsstrahlung before, and now we have a spectrum that contains a series of edges, like this. And this lowest edge here-- I should draw this as a straight line-- this is n equals 1, 2, 3, 4, and all the way back up here, where, again, this is our brems result, and this isOur recombination.n of jn here. So if you're asking, what happens if you can't fulfill that, it just doesn't happen. Quantum mechanically, that would be a forbidden transition. we just did it in reverse. So this long tail here is due to the fact that we have electrons not just with v equal 0 but much more than that, yeah. So if you've got some helium with 2 times ionized helium, and some hydrogen, then it will have different recombination lines here. And that's because, fundamentally, the energy levels are in different places inside here. We'll go into some ways of actually making some use out of all this stuff in a moment. JACK HARE: We're going to get to that and the idea of detailed balance and thermodynamic equilibrium and things of that ilk. One use of this is a diagnostic called bolometry. It cares not at all about the detailed spectrum of what the emission is. It just wants to know how much power is being radiated by the plasma. So, bolometry is focused on measuring the total power that's being emitted. And we'll talk more about how to do that in the next step. Bolometry was one of the first diagnostics that we had on many MCF devices. Now, this itself is a very simplistic system, and it does not work because it is very susceptible to noise. So we have to, as always, come up with a clever system, which is noise resistant. And then, using heterodyning techniques, we can then measure it very cleanly without noise. This may be an experimental technique you're familiar with with the head of a bolometer. This is often made out of gold. Gold is chosen because it absorbs all wavelengths relatively evenly. This thin layer of gold has been deposited on top of a substrate. And it's on the back of the substrate that you have your resistor. And depending on whether it is M or R, you either have this open to the plasma or you have a thick block some distance in front of it so it can't see the plasma. It's the heat capacity of the absorber, c, that goes into our previous equation. difference between those. neutron damage leads us to use much thicker substrates, which gives us a longer time response and so, therefore, a worse bolometer. So, ironically, the bolometers that will be used on [INAUDIBLE] are significantly worse than the bolometer used on existing devices. And I'm sure that [? Spark ?] will have exactly the same problem. Almost everyone in the world uses this design of bolometer that they pioneered on ASDEX Upgrade in the '80s. No one has come up with a better system yet.

ROUGE-1: 36.04, ROUGE-2: 34.43, ROUGE-L: 35.36
BERTScore: 62.70

==============================================
==================== [79/100] ====================
Summary:
So, last time we were starting to talk about the sort of the general overview of what reinforcement learning involves. We introduced the notion [NOISE] of a model, a value, and a policy. Can anybody remember off the top of their head what a model was in the context of reinforcement learning? So what we're gonna do today is, sort of, um, build up for Markov Processes, up to Markov Decision Processes. And this build, I think, is sort of a nice one because it allows one to think about what happens in the cases where you might not have control over the world. Markov Decision Processes are the same as the Markov Reward Process except for now we have actions. So, the agent is in a state they take an action, they get immediate reward, and then they transition to the next state. The advantage of this is that each of the iteration updates are cheaper and they'd also will be some benefits later when we start to think about actions. Now, if we think about our Mars Rover MDP, let's just define there being two actions being A1 and A2. expected discounted sum of rewards. Now you might ask, okay well they- are they ever guaranteed to stop changing? And we'll get to that part later. We're going to get to the fact that this whole process is guaranteed to be a contraction so it's not going to go on forever. So the distance between the value functions is going to be shrinking. And that's one of the benefits of the discount factor. So if people don't have any more immediate questions, I suggest we all take a minute and then just compare with your neighbor of what number you get when you do this computation. There exists a unique optimal value function. The optimal policy for an MDP and an infinite horizon finite state MDP is deterministic. There are two choices for every state and there are seven states. The [NOISE] number of policies is |A| to the |S|. It's a mapping from states to actions so it's gonna be 2 to the 7th. We'll talk about- probably a little bit clearer to when we talk about contraction properties later. Um, any one want to take a guess of whether or not the optimal policy is always unique? I think there might be cases where it's not. on sort of the contraction operator. So, if an operator is a contraction it means that if you apply it to two different things, you can think of these as value functions. The distance between them shrinks after, um, or at least is no bigger after you apply the operator compared to their distance before. So how do we prove this? Um, we prove it- for interest of time I'll show you the proof. Again, I'm happy to go through it,. um, I- or we can go throughIt in office hours et cetera.

ROUGE-1: 7.08, ROUGE-2: 6.83, ROUGE-L: 6.81
BERTScore: 65.96

==============================================
==================== [80/100] ====================
Summary:
 angular momentum is a set of operators that provide observables, things we can measure. They're particularly important for systems in which you have central potentials. A v of r that depends just on the magnitude of the vector r is relevant to cases where you have two bodies interacting through a potential that just depends on the distance between the particles. Professor: We showed that any component of angular momentum, be it lx, ly, or lz, commutes with l squared. The eigenvalue of this operator should be positive. are the Legendre polynomials. Solve this equation for m equals 0. Are there any questions? Anything about the definitions or? Yes? AUDIENCE: Why do we care about simultaneous eigenstates? PROFESSOR: Well, the question is why do we want to know what are the properties of the states? In general, you will be led in any physical problem to look for the maximal set of commuting operators. The most number of operators that you could possibly measure. and an electron we can reduce this system to as if we had one particle in a central potential. So that will be also very important physically. And here there is a simple observation that one can make. Is that the differential equation for p l m depends on m squared. We expect to need values of m that are positive and negative. The complex conjugate ones should be thought as having m negative. So we expect positive andnegative m's to be allowed. So how did people figure this out? They, in fact, figured out that if you have these polynomials you can create automatically the solutions for this equation. that don't diverge. So this is very important. It shows that there is one more constraint on your quantum numbers. This formula you may forget, but you should never forget this one. This one says that if you choose some l which corresponds to choosing the magnitude of the angular momentum, l is the eigenvalue. You will have several possibilities for m. There will be several states that have the same l, but m different. So for example you'll have l equal 0, in which case m must be equal to 0. The spherical harmonicas are going to be those wave functions. And they have a normalization, n l m, an exponention, and all that. So let me write, just for the record, what a y l m looks like with all the constants. The only one I really remember is that y 0 0 is a constant. It's 1 over 4 pi. That's simple enough. No dependents. Here is another one. y1 plus minus 1 is minus plus square root of 3 over 8 pi e to the plus minus i phi sine theta.

ROUGE-1: 38.02, ROUGE-2: 36.41, ROUGE-L: 37.69
BERTScore: 70.63

==============================================
==================== [81/100] ====================
Summary:
In order to do that, I basically have to do the integral. So here it is. We have psi of x and t. It's integral dk phi of k e to the ikx minus omega of kt. If you want to see the distortion, you have to keep that [INAUDIBLE]. We'll do that in a week from now. And then, you say, look. There's lots of things making it look like a difficult integral, but it's not as difficult as it looks.

ROUGE-1: 15.65, ROUGE-2: 15.13, ROUGE-L: 15.65
BERTScore: 62.97

==============================================
==================== [82/100] ====================
Summary:
There are three common uses of a rotation matrix. The first is to represent an orientation. The second is to change the frame of reference of a vector. And the third is to rotate a vector or frame. To demonstrate these, I will use these three coordinate frames, representing the same space with different orientations. To help you visualize these frames in 3 dimensions, I‚Äôll use my handy tinkertoy frame. This is the z-axis, this is the x-axis and the y-axis. we will learn how to represent the angular velocity of a frame. We will also learn about how to use the frame to represent a frame's angular velocity. We'll also look at how the frame is used to represent an object's speed. We hope you will join us for the next few weeks of classes on how to work with a frame in this class. The next class will be on the physics of the frame in which we are working. The final class will take place in the next week or so.

ROUGE-1: 35.22, ROUGE-2: 26.36, ROUGE-L: 29.92
BERTScore: 66.09

==============================================
==================== [83/100] ====================
Summary:
 RAMESH RASKAR: So this is a position, and this is superposition. And that concept of a position or superposition applies to all three types, shadows- or refraction- or reflection-based techniques. So we saw this last time, and we'll see how-- we already have some projects that are inspired by biological vision. You know, Matt is trying the chicken. And I think it's going to be-- [LAUGHTER] It is going to. Coded imaging is a co-design between how you capture the image and how you process the image. In a typical film camera, or even it is digital camera, you take the picture, and that's basically the end of the story. Here, you're trying to do something clever about how the picture is taken. You can either take a really short exposure photo. But that's going to be very dark. If you take a high ISO, you can recover some information. Or you can just take a long-exposure photo by keeping the shutter open. Photographer RamESH RASKAR: When you try to recover this information, you start getting this banding artifacts. And we'll see it in the next slide, why that happens. If you keep the shutter open for even longer, it'll blur correspondingly longer. So you can basically represent that as a sharp photo, where there is a convolution of the sharp photo with some kind of a Convolution filter. But then you will get a blurry photo, which is well exposed, but a lot of high-frequency details are lost. have basically a 1D convolution that's converting this image into this image. But the Fourier transform has some zeros, so you cannot divide those frequencies by 0 and recover an image. So the culprit here is really this box function, which is equivalent to-- when you release the shutter, opening the-- release your shutter button-- opening the shutter and keeping it open for exposure duration and closing it. But that's not the most effective. So what if you change that? What if you open and close it in a carefully chosen binary sequence? frequencies-- they're all preserved. Of course, they're attenuated. It's not as high as-- it's not 1.0.0, it's reduced. Maybe it's 0.1 or so. So there is still some hope to recover this photo back from this because, in the denominator, we will not have seen. This is a very simple idea. Instead of keeping the shutter open for the entire duration and getting a well-exposed photo, the shutter is open for only half of the time. And that's your 1010 inquiry. CNN's Ravi Agrawal is a senior producer at CNN.com. He is the founder and CEO of a company that uses artificial intelligence to improve camera technology. He says the technology can be used to blur images based on distance and speed of objects. He shows how the technology could be used in a mobile phone app to help people figure out license plate numbers of cars and cars on the street. The company is based in New York and has been around since the 1970s. have multiple cars, for example, and they're all independent, then it's fine. As long as it's moving in a straight line at a constant speed, you're OK. But if the two cars overlap, what happens? Our model fails again. If two cars are partially overlapping during the exposure, it's possible, but it's more challenging because you don't know exactly how fast the two car are moving. So it's just like-- AUDIENCE: OK, but that's just so you can get more light. When I take a picture, the camera automatically decides what the exposure time should be. Similarly, you should look at the speed of how things are moving maybe with an ultrasound Doppler or whatever. You need to know how much the blur is. Another major disadvantage is let's say I want to take this bottle. And if I just rotate it and motion blur that, it will not work. For any point in the front that you're looking at it, it'll work. But the point that was in the back, that all of the 52 sequence-- maybe for the first 10, it was occluded. And the remaining 42, it's seen. In general, the technique works well when things are moving naturally. But if somebody wants to do this kind of an experiment, or move things behind an occluder and move out, those are very challenging scenarios. If I have a car that's moving, and I tell you how exactly one point of the car is behaving in the image, I can tell you automatically how the rest of theCar is behaving. The point spread function or the blurred function is very critical. And this is what we want to study about half of the class. to recover some information. The goal of coded imaging is to come up with clever mechanisms so that we can capture light. The circuit is very, very simple. You just take the hot shoe of the flash, and it triggers. When you lose the shutter, it triggers the circuit. And then you just cycle through the code that you care about. What can we do for defocus blur that is for motion blur? What can you do fordefocus blur? We, again, want to engineer the point spread function. would you apply spatial coding? AUDIENCE: Coded aperture? RAMESH RASKAR: C coded aperture. So this is coded exposure, coded aperture-- very easy. And all you're going to do is put some kind of a code in the aperture of the lens. And this is how, actually, it started in the days of-- in scientific imaging, especially in astronomy, coded apertures are very well known. And so I thought, it must be useful for something in photography. Ramesh Raskar: Lens has to deal with chromatic aberration, geometric aberrations, such as radial distortion, and so on. When you change your f-stop and decrease it and increase it, it's all happening in the center of projection, he says. RaskAR: If you take a picture of a point light, and everything is a sharp focus? Nothing changes, OK? He says if the dust is all the front, then you start seeing distance, so when it's in sharp focus, you just see the autofocus. the values will be constant. So if we're placing a broadband code, certainly we have an opportunity to recover all the information. It's much easier to think about convolution and deconvolution in frequency domain than in primal domain. The bokehs are-- it depends on your-- I mean, for your average consumer, I don't know whether this matters. But you're right. If you're looking at something that's-- we have bright lights in the scene, take our false photo. They will all look like this. motion case, we had to know how much the motion is. The size of the blur is dependent on what? AUDIENCE: Belt. RAMESH RASKAR: The belt. But not just depth-- depth from the plane of focus, right? So that's an extra parameter you would estimate somehow. Maybe you can use a rangefinder or something like that, or just a software. There are methods you can employ. We said, OK, let me try to refocus. When it comes into sharp focus, my edges, that must be the right depth. I said, by the time I come tomorrow morning, I'll find a really good code. And I came back next morning. Nothing had happened. I waited all day. It was still running. And it never came out of that. So 2 the 52 is pretty challenging. But even if you use a cluster, it's still a pretty big number. And there are all these theories about how to create different codes for different applications. So you can start with some code and do a gradient descent and so on. good solutions for 2D. But for 1D, there are some really good solutions to come up with that. For 2D, for certain dimensions, they call it one more 4 or three more 4. Basically, when you divide by 4, the remainder can be 1 or 3. And there are certain sequences that are beautiful mathematical properties, of which sequences could have broadband properties and which may not. So it turns out you cannot-- there's a little bit of cheating going on here. filter to the beginning of the signal. This particular filter is actually not circular, but it's linear. So when you apply the filter here, when you start applying the filter at the end of the image, you don't go back to the front. It turns out, for circular convolution, the match is very clean and beautiful and smoother course work. Or for linear convolution,. there is no good mechanism. So we came up with our own code called RAT code, R-A-T, which is after three quarters. Ramesh Raskar: Coded imaging is elegant and beautiful and sometimes complicated. He says there are many ways of engineering the point spread function. RaskAR: For any continuous code, there is a corresponding binary code that will do an equally good job. "It's just one of those things. It's like we are sick of it, so we don't want to do it, but I think it's worth trying," he says of orthogonal motion blur. adding small matchsticks on top of the main lens. The way they do it is they actually put one single sheet that looks like that, an additional layer of support, a face mask. If you have a piece of glass, and light is going through, it's going to slow down here and then again [INAUDIBLE].. That means you basically slowed down the light. That's why, as we learned about at the beginning, if you have something very far away, this slows down a little bit. blur is only about 10 pixels, no matter where you [INAUDIBLE]. So maybe that was the matter. If you have a point of access, it's still going to create an image that's blurred 10 pixels. This is, again, very counterintuitive, where you go to make the image intentionally blurred. It's just that it's blurred everywhere. And then we also saw this one very early on, where the point spread function-- typically when something goes in and out of focus, it looks like a point. some point, they'll stay the same. RAMESH RASKAR: The xy still remains traditional microscope 1 micron, 1/2 micron. But the z-dimension is 10 nanometers. They are still working on a lot of these concepts. OK? So let's very briefly look at compressed sensing because it's something you should be familiar with. It's a very cool idea, by the way. As a scientist, I really like it. But when somebody like Technology Review or Wired Magazine says, Top 50, Top 10, of course, I wish I'm listed among them. Ramesh Raskar: Compressive sensing or compressed imaging is taking a picture with a hardware and compressing the software. He says the idea is to take a photo and compress it down to 10,000 bytes. RaskAR: The theory of compressive sensing is that that's the basis for compressive imaging, which can be exploited while I'm sensing, OK? He says if you have a map of your optics, this is your optics map if I have it on a slide 5. Photographer Ramesh RASKAR: Compressive sensing allows you to build a camera with a single sensor. Raskar: Compressed sensing is a very, very active field. He says the secret of success for film, of film photography, is that if somebody had given you this problem before the invention of film, that there is a scene-- and I want to give you a sensation of the same scene-- time shifted or space shifted. Raskar: Can we use sensing mechanisms that are similar to our brain so that we don't do any software? The debate about whether it's really better or not is photography? RAMESH RASKAR: Tomography, yeah. When you have to recover the sky, you want to take as few measurements as possible. So compressive sensing allows you to take less measurements. But the problem is you need to actually have more information about the scene before you take the measurement, which is another measurement. That's a different problem for the dual photography code for the camera. It's a very different thing. which is how to write a paper and wishlist for photography. Which isHow to Write a Paper and Wishlist for Photography: How to Write A Paper and Write A Wishlist For The Camera. For more information on writing a paper or wishlist, go to: http://www.cnn.com/2013/01/30/photography/how-to-write-a-paper-and-wishlist-for-photography-how- to- Write-A-Paper-And-Wishlist.html.

ROUGE-1: 27.10, ROUGE-2: 24.81, ROUGE-L: 24.86
BERTScore: 72.06

==============================================
==================== [84/100] ====================
Summary:
Prof: You know why I am dressed up? When I do this course and when I do the first half of the French course I do a lecture on the bourgeoisie, the middle classes. Middle class was a form of self-identity that was constructed in the way being a worker was constructed, or being a noble. When you look at me dressed like this, please try to think, knowing me a little bit as you do, why it was that it meant a lot to dress like this in the nineteenth century. of class identity for ordinary people, for working people, the bourgeoisie had as strong a sense of self-identity as any social class you could imagine. It was, as I'll make the point in a minute, difficult to get into that class if you weren't born into it. The fear of falling out of it was something that helps motivate lots of political things in the nineteenth century. Once we've got an increase in the wealth of the middle classes, then you wanted the political power. You wanted access to information through the press and print culture. The French Revolution opened the way by removing legal blocks in very many places to the career open to talents. Napoleon used to say tediously that in each soldier's backpack there was a marshal's baton, or staff that you could get promoted with good work, hard work, if you didn't get your head blown off in one of these battles. The bourgeoisie did anything but that. Work was part of how they believed to get ahead, and getting ahead is what they wanted to do. In the 1960s people really didn't study the middle classes because they didn't like them very much. But there's been an awful lot of good work done on the middle class. The middle class formed voluntary associations, and many of these were for extremely charitable purposes, particularly in Britain. In the nineteenth century, over a very long period, laws finally by the end of the century in most places made at least primary education obligatory, and in most cases free. The Society for the Protection of Cruelty to Animals is one of the classic examples of bourgeois voluntary associations doing good things. Religion was a fundamental part of the British middle class's view of itself. The percentage of people who went to church could be exaggerated. There's a massive kind of church building campaign that has its counterpart in almost every country as well. After the Paris Commune of 1871 they start building churches in France, and in other countries as well, including the U.S. and Germany. The church movement in France is well studied, but you still had this de-Christianization. How do we know who is middle-class? When they first started doing censuses--and censuses are really a nineteenth-century phenomenon, and subsequent centuries, as I said before. There was a whole lot of work done in the 1970s on what they used to call the new urban history, which is counting people up and deciding who might well have considered themselves middle class. Inevitably I have to talk some about Paris because the work is so rich there. A woman called Adeline Daumard wrote a dissertation that was subsequently published called Les Bourgeois de Paris. Hamburg, Bremen, and L√ºbeck, and Hamburg above all, have a very enormous bourgeoisie. Lyon has the most tightly closed middle class that you can imagine and still is. Naples is one of the biggest cities in Europe right through the early-modern period. The further east you get, the smaller the middle class gets. At the very top--think of Zurich. Think of any city you want. Zurich has a big middle class. So does Geneva for obvious reasons. The nineteenth century bankers will become much more important. There's a revolution in France in 1830, yet another one that you can read about. Arguably--Marx says this and in a way it's sort of true--what it does is it brings to power in France the big bourgeoisie. They have the ear of the king, Louis-Philippe, who calls himself the Citizen King. He would rule from 1830 to 1848. He was noble. But in the official government he was not any bourgeois. That's what he calls himself. He's still the king. Paintings of him you see people dressed like me who are coming into the throne room. They have power. He wants them in the painting with him. It's terribly interesting. Then you've got other layers of bourgeoisie. Here we have smaller bankers, not in size but in money, industrialists, merchants, these kinds of people. Lawyers rise up rapidly in popular esteem and usefulness. The middle class likes to see themselves as useful. You find lawyers reaching in there and, slowly, doctors. times, as you know, in the French Revolution--;the French revolutions, and in the revolutions of 1848. They're always there. These folks are here, too. People are always dumping all over them needlessly. I will give you some example. If you've ever read the great French novelist--;he was paid by the word, but Balzac is really the novelist of the bourgeoisie. When he describes Paris and the seventeen to nineteen percent of the population who are increasingly living in the western part of Paris, he describes it as a jungle. as a jungle. In several hundred brushstrokes, Daumier captures the look of panic on his face because he's going to go home without his hat. He's got to buy one, and they've got to put the money together so he is not going to fall off the ladder in this jungle. Then you have to imagine this as a ladder, like this. In order to really give an image of what it was like, he's got this one magnificent print called the "Street of the Four Winds" mobility is the goal. You want to have enough money to leave to your 2.2 children. Then you'd have to grease this pole through bad economic times. What happens down below here? Holy cow! That's the big sea. I saw this wretched movie called the Poseidon Adventure once. It had an image where the water is kind of coming up below and it's going to finally get to the top and there's no more room to breathe. This is how the people on the bottom part of this ladder viewed the demands of the working class. Daumier is the greatest caricaturist in the nineteenth century and arguably ever, to make an extreme assertion, but it really is pretty true. This is what he captures, the prevailing mood in much of Europe in that money, more than blood if you were going to exclude places like Hungary, Poland, Spain, and Prussia. What is the.down here? This is ordinary people. The chances are that in these bad years you're going to fall down. But yet lots of people get up and the ranks of the middle class increases everywhere. man doing? He's counting his money. That's a very nineteenth-century profession, as it is for every subject. This guy, if you have extraordinary eyes and can read upside down, you will be able to see that he is reading a newspaper on the price of colonial goods, imports. He's one of these people that's at the very top of my triangle there. Look, this guy's got his coat, too. This is early in the century. You can tell. Some of these images, this is really not very interesting art, but that's not the point. The bourgeoisie didn't kiss and hug a lot. They still had arranged marriages. Love could count for something, but marriages were still essentially, less so for the middle classes than for ordinary people. That's what they were. They were economic relationships, wrangling over the dowry and that kind of thing. Look at our guy on the left. He's working very hard there. He had probably not secondary education. Most people didn't go to high school, secondary, lyc√©e in France or gymnasium in France. in Germany, et cetera, and et ceta. It represents this world. By the way, we also know that this takes place in the center of Paris, right behind a big department store, subsequently the Hotel de Ville, but right near the town hall. This is very common. You see this in the book you're reading, I think. These things can be represented spatially very easily. One of the themes of the long run is the emergence of prosperous western Paris, prosperous western London, prosperous center Vienna and other places. minute. In Zola's great novel, L'Assommoir, Gervaise dies like a dog on a bed of straw, because there was no more mattress. She must be at the very top. Now these rooms then became in the twentieth century student rooms and then were transformed into enormously expensive lofts. But this is a way of visualizing the special concomitance of what I'm talking about. The more you go up there, you're still within the middle class. People were aware of what these symbols meant. This is your classic Hamburg financier's apartment. Ordinary people did not wear slippers. Domestic servants cost almost nothing. It was considered to be a way of moving up the ladder to say that you had four domestic servants instead of three. You've got brass or copper here on the heater. That's a good sign. You're on one of the lower floors. Why? Because you see the trees outside the window. And you've got a domesticated animal. Pianos were expensive, but the middle class has pianos. The piano replaces the harpsichord. The middle class wants privacy. They want their own rooms. There's the kitchen. This is the domestic with her children, who are part of the team who has been hired to help run this. There is more than one room. You'll see in a minute there's even more than two rooms. It's all obvious stuff. There are lots of rooms. These are very good chairs, sort of Louis-Philippe chairs. The notion of childhood, childhood didn't exist for ordinary people. Nobles did not send their children to public schools or even to private schools. The whole salon, the idea of going to art shows starts in the seventeenth century. The middle class wants to be seen rather like the Dutch. They wait in line in line to go to theatres. This is all Daumauau, the piece that you're obliged to swallow after dinner. Here's the little girl being trotted out to play a few notes. birthday, papa." You didn't take time out to celebrate a birthday if you were an ordinary person having to get to the fields at 4:00 in the morning in the summer, or going to work during the day. There's a whole notion, and here again this would probably fit rather awkwardly into the birth control description, but there's this whole sense of being prepared that emerges with the middle class. It was the idea of protecting that one suit. We didn't carry umbrellas, because it rained all the time anyway and I'd just lose it. The bourgeoisie, the middle classes, want the right to bear arms. They want to be in the national guard. The national guard might hypothetically be there in case there was an invasion of France or Germany by, I don't know, some distant place. You had to be defined as a property-owning citizen to have theright to vote. They didn't want to pay a lot of taxes, but property reflects one's belief in one's own social worth. No longer was it the worth of blood. H.D. Daumier's light lines, and this is the last one, disappear in this painting, which is called the Rue Transnonain, April 15, 1934-don't write it down, in Paris. It's a street that no longer exists. It disappeared when Haussmann built the boulevards in the 1850s and 1860s. The middle classes, for all of their insistence that they have access to information, at least in the case of France they cheered on a press law in 1835.

ROUGE-1: 41.17, ROUGE-2: 39.23, ROUGE-L: 37.49
BERTScore: 62.59

==============================================
==================== [85/100] ====================
Summary:
Researchers have confirmed a second smaller space Rock smashed into the sea off the coast of West Africa creating a large crater during the same era. Scientists say it would have caused a tsunami at least 800 M High to tear across the Atlantic Ocean. The asteroid that's believed to have wiped them out 66 million years ago was not the only one researchers have confirmed. The discovery is exciting that it happens to be potentially close to the same time as the chicku event known to be the the main cause of the extinction event that killed the dinosaurs.

ROUGE-1: 38.76, ROUGE-2: 35.80, ROUGE-L: 31.78
BERTScore: 63.39

==============================================
==================== [86/100] ====================
Summary:
Dynamics plays a very important part in automotive design. There's a trade-off between how comfortable the ride is and how tightly the car handles. The subject that we're studying applications homeworks and exams I know that I mean beyond that followingYeah trajectories so missiles stuff like that anything to kill people you need Dynamics what else build a what building a car okay that's good good and what for that's actually that's right but what for well actually in automotiveDesign. MIT opencourseware at ocw.mit.edu. in Dynamics there two sides to it one is here's the system what is its trajectory going to be in other words how will its various degrees of freedom behave over time. You want to figure out how it goes in time that that's analysis so that's the first kind of the yin. The second is you here's a system how do you modify it so that it behaves in a way that you want all right and that is called it starts with a C control so control is given a dynamic system. The thrust is only for the first few minutes of its um journey and then it's ballistic ballistic means trajectory it's all Dynamics after that you control it initially to make sure it's pointing in the right direction compensates for wind Etc. A lot of missiles there's a little bit of fuel left so in the end you can do some adjustment right but a bistic missile most of its journey is ballistic okay okay. In the last class we did um I should also warn you I'm a little woozy today uh I had some serious drugs this morning prescription drugs and the result is that I might I think Sam. was saying if I might start babbling but you won't notice because I Babble anyway right so all right Sam didn't say that I said that I bet but anyway s was very respectful okay so uh in the last class we did uh uh we did a problem essentially the whole Pro class was we looked at you know the skier situation and there's a handout AJ's published right on the web where we do the energy formulation and we solve the problem. So today we're going to kind of flow the accelerator a little bit we've been slow and steady we've kind of pounded the concepts in.  angular momentum concept is our stepping stone into Dynamics of rigid bodies because then you can start looking at two and three particles more easily so we'll start with that. Any questions about the last class comments ridicule jokes nothing okay all right so so today we're going to do angular momentums we're still in points Point masses but this is the stepping stone the link to rigid bodies then we'll do a problem we're very problem oriented in this class and then we're Going to do yet another problem but we'reGoing to do multi- particle okay now just a couple of announcements uh one is that pet is that dog. 4 was due uh is due on Wednesday the 10th and the reason is there's no class next Monday and um we posted the solution to that problem from class uh just one last thing I won't have officers today only because you don't want to hear me Babel I'm really sick um but I'm also going to change my officers um several people suggest so the timing isn't right um so we'll talk about the end of class but I I might go to like a Monday off M um like later on a Monday or maybe Wednesday later or something like that. Just hang out like at 3:00 p.m. on a Saturday is that something that you you folks would find useful okay we'll do that okay um so here's what we going to look at what no okay we're going to see our first angular momentums and torqus today. So consider a frame consider a point fixed in frame and this is an inertial frame and a pointfixed in that frame o okay now let's consider a particle P by the way when I say particle a point mass or in context point I'm referring to the same thing essentially I'm refer to something with no Dimensions but with a finite Mass. derivative do I need to say which frame I'm taking the derivative in yes and this thing is going to be I need I'll put it out big let say scaler does that make sense I'm not doing anything wrong why I'm doing that will be obvious in a second okay everybody Lauren everyone yeah okay so this I can write as this this thing a d by DT of r o minus r oq cross M A VP you understand I haven't done anything wrong I'm just going down. be clear this whole thing is this term and this is thisterm now let's let's write this The Next Step what is this what this m a acceleration of P or acceleration of p with respect to a hm H it's the force on particle P right cross product rqp yes now so I'm just going to put a dotted line so you know that that's what this is is this I don't think you'll disagree let me just write some draw some lines I'm trying to save space as I said. of all engineering graduates who take a car class in Dynamics Berkeley Stanford Princeton might not even see this there is this term and you need to know about it okay it just so happens the term vanishes in many situations but it's key that you know see for Force f is equal to D by DT of P momentum completely coer Crystal Clear when you come to angular momentum it's say artifice yeah there's this funky term which vanishes let's identify the conditions but youneed to know it exists when do you think this vanishes so what is this? it's not Kosher can't do it if I do I need to include that term right and you you've got a figure that a simple thing like my a robot is a very typical Dynamics application okay common mistake okay now that we've done this let's try and figure out why angular momentum is useful let's solve a problem any questions about this any questions bottom line if you're taking angle momenta about moving points be careful bottom line many conditions it'll be okay we'll elucidate we'll Express we'll we'll nail this conditions. A lot of MIT grads old friends work all over the world very important positions around the world for example the foreign minister of the new British government is an MIT grad. One of my friends Works in a congressional he she helps Congressional analyze things from a physics point of view so you know when Katrina occurred someone asked if it would be possible to change the temperature in the when when a hurricane approaches to dissipate the temperature. She did some analysis and showed that you need something like a nuclear weapon but like you know the most the largest the largest. nuclear weapon ever conceived to even you know impact it by like 2% because the energy in a in a hurricane all right I told you I'd Babble all right let's do a problem here's a problem so imagine a um a table right you notice I've drawn it in perspective you know what perspective is right it's very cool it wasn't intentional but looks good I see everything in perspective right now okay I have a hole on this table right so this is a table and and um it's frictionless hockey puck basically a point map attached by a string and the string comes through the hole. call this Theta the initial length is L one and the initial velocity is we'll call it a scalar because this is how I'm defining the problem of V1 when we actually solve it we might have to define a vector okay. As the thing's going around this person is going to pull the string down and as it kind of goes around it's going to spiral in and end up it's a new length L2 and the question is what is V2 going to be okay so I'll let you uh let resolve this for a second. that stuff let's let's examine it from a uh from a basic you know intuition point of view from what we studied so far first of all when is linear momentum conserved forget this in general linear momentum is conserved when what condition occurs no external Force right so let's study this guy this particle as it moves around does it feel an external Force. As the particle moves around kind of intuitively which direction is it accelerating in kind of cental right so it's has an acceleration in this direction what else is happening maybe there's a potentially a tangential acceleration kind of an Oiler acceleration as well potentially right kind of thing.  angular momentum is when is is when things go in circles or kind of circularly it's a nice way to do instantaneous linear momentum conservation get it that's basically what it is you understand so let's so is angular momentum conserved let's think about it is there any talk on this particle yep go ahead well there are no forces in the tangential Direction on this are there right isn't the velocity changes ah but the velocity so let me put it here this way right so let us say X Y there's a hockey uh uh what do you call the surface of a hockey or an ice rink or a rink right now X and Y. would have to do one of two things I would have had to either calculate this term or calculate or make you know make my frame attach it to the truck. I've just done it in a very precise way okay so in the end there's no surprise the whole point is to show you they could be surprises but be careful any questions about this all right snap quiz in the next 3 minutes I want you to calculate for me the final velocity literally 3 minutes because I have toDo the dumbbell problem.  angular momentum is a vector out of plane in 2D you see that okay is energy conserved for this particle why not well think about it you wor yeah work it out has the energy gone up or gone down. The energy kinetic energy is gone up by the square a velocity right and um so energy is not conserved so where did the energy come come from yeah that's because you were doing work by pulling it right potential energy is constant so the can energies because you can also do this with energy by the way right. momentum formulation actually came precisely from FAL to ma let's look at this guy what we did was I just defined terms the conservation yeah sure well this way this is the nice way to do it look angular momentum is simply f is equal to Ma you put an R cross product in front of the F and inFront of the ma that's kind of where it comes from right so effectively that's what I did but I just gave you a canned way of do it instead of doing it you know each time you have a question I meant ABP. is equal to ma there's nothing all this is f is equal ma right so far we've done nothing Beyond it energy comes from f isequal to ma these are all just tricks okay okay so with that now let me solve a multi-particle system so this is a stepping stone into uh into uh rigid body Dynamics in the fullest Glory so what we're going to analyze now is this system this is all think of this in the horizontal plane right like a skating rink um and this is an inertial frame. they always point in in the horizontal Direction in the on the Whiteboard right so they little Gyros they you point that way or maybe they're magnets and there's a big magnet at that end so it's only being pulled in that direction okay and the question I ask now is how does this thing behave so let's examine this first of all how do I parameter or to use the more technical term what nonstandard coordinates do I need to describe the configuration of this dumbbell let me make a proposal to you. p and particle Q is that reasonable it's perfectly reasonable. If instead of a rigid bar if I had a string connecting them what would the kinematic constraint be less than equal to to R right. If I went with four non-standard coordinates X and Y of this I would have fourNon-Standard coordinates and one kinematics constraint right. Is there a better coordinate system General a non- Standard coordinate system you can think about anybody clao are we assuming that the same mass doesn't make any much it doesn't making a difference the problem just makes our lives easier later. different you'll find out later I would have picked the center of mass which would not have been the geometric Center Center okay but you don't know why so I can't tell you why. A lot of terms cancel out that's the only difference I could still have by the way I could have picked a point on that line anywhere. There would still be reasonable generaliz coordinates there wouldStill be three just the math is easier that's it okay so I don't have to pick theCenter of mass but I pick thecenter of mass. understand the uh um right totally understand the reasoning behind kinematic constraints I'm going to call this point c r o That's when one vector now this angle of this thing is Theta and I'm defining um because this thing I've rotate a lot right uh I just you know it's a little confusing but let me just draw it for you like this if I had if if the way I had drawn it I hadn't rotated that dumbbell so much if I drawn it like this it'll make more sense. this R OC which is some component this way plus some components this way and an angle that we know the configuration of this dumbbell at any point in time. First we will do some free body diagrams then we'll figure out the accelerations of both particles. We'll write f is equal to Ma and we have differential equations we'll make sure we have as many equations as we have unknowns and we'll the do all right so from a free body diagram point of view I'm not going to do it right now but very simply there are each bar applies a force. Clarence: We should be able to equate the accelerations of these particles multiply them by m and equate them to the forces do a free body diagram we should we done right nothing really special here any questions about this anybody okay so I'll just write it out now I won't do it because we have only about 2 minutes left but let me do this this side this white board sorry now let's do the free body diagrams on each particle so there are two particles p and Q and I'm applying a force from the Rockets right. we'll show is that the acceleration of the center of mass is uh related to the total force and the angular acceleration of. the whole rigid body is related to torque and we'll show it okay and then we'll generalize it. Define angular acceleration and moment of inertia and a more General sense okay so let's stop here because we are over time. We're going to take a break. We'll be back in a few minutes. We've got a lot to talk about.

ROUGE-1: 42.71, ROUGE-2: 41.62, ROUGE-L: 41.81
BERTScore: 69.09

==============================================
==================== [87/100] ====================
Summary:
In this module, I'm going to briefly introduce the idea of differentiable programming. Differentiable programming is closely related to deep learning. I've adopted the former term as an attempt to be more precise in terms of highlighting the mechanics of writing models as you would code. So let's begin with our familiar example, a simple neural network. And this is the programming part of differentable programming which allows you to build up an increasingly more sophisticated model without losing track of what's going on. In a three layer neural network, we start with our feature vector. In this case, it's a six dimensional vector. And we left multiply by a matrix. Now I have a vector and now I can do the same thing again. I apply a matrix, add a bias term, apply an activation function. Apply a matrix which happens to be a vector, so I get a scalar and I add a simple scalar bias term. And I getting a score which then I can happily drive regression or take the sign to drive classification. you want to do image classification. We need some way of representing images. So the FeedForward function that we just introduced, takes a vector as input. We can represent an image as a long vector by, for example, adding all the rows. But then we would have this huge matrix that we would need to be able to transform this vector resulting in a lot of parameters. And the problem here is that we're not really using the spatial structure of images. To fix this problem, we introduce convolutional neural networks which is a refinement of a fully connected neural network. be learned. The second thing is I also haven't specified the hyperparameters which is the number of channels, the filter sizes, and so on, which are actually pretty important for getting a good performance. But I just wanted to highlight the overarching structure and the idea that you can compose in a fairly effortless way. So now let's turn our attention to natural language processing. So here is a motivating example. Suppose we want to build a question answering system. We have a paragraph. It's from Wikipedia and we have a question. We want to select the answer from that passage, from the paragraph. In NLP, words are discrete objects and neural networks speak vectors. So whenever you're doing NLP with neural nets, you first have to embed words, or more generally, tokens. There's one problem which is that the meaning of the words and tokens depends on context. So we're going to define an abstract function. An abstract function is something that has an interface but not an implementation. A SequenceModel is going to be something that takes a sequence of input vectors and produces a corresponding sequence of output vectors. I'm going to talk about two implementations of the sequence models. Collapse takes a sequence of vectors and returns a single vector. There's three common things you can do. If you're doing text classification, you probably want to pick the average to not privilege any individual word. But as we'll see later if you're trying to do language modeling, you want to take the last. The score for, let's say, binary classification is going to be equal to taking the input sequence of tokens. You embed all the tokens into a sequence and now you can apply a sequence model, for example, a sequence RNN. The attention mechanism takes in a collection of input vectors and a query vector and it outputs a single vector. So mathematically what this is doing is you start with the query vector. I'm going to multiply a matrix to reduce its dimensionality, in this case from 6 to 3. And the attention is going to process y by comparing it to each of these x's. OK. So these types of functions where the input and output have the same type signature are really handy because then you can compose them with each other and get multiple steps of computation. Here is one of the input vectors. x1, x2, x3, x4. I'm going to reduce its dimensionality to also 3 dimensions. And now I can take the dot product between these x's and y's. So that's going to give me a four-dimensional vector of dot products. Now I can turn those scores into probabilities by taking a softmax. So in general, this is kind of softly picking out which input vector is similar to y. be done once the parameters are learned from data. You can think about this as a sequence model that just takes input sequence and contextualizes the input vectors into output vectors. There's two other pieces I need to talk about before I can fully define the transformer. Layer normalization and residual connections. These are really kind of technical devices to make the final neural network easier to train. I'm going to package them up into something called AddNorm and it also has a type signature of a sequences model. processing each xi in context. Now we have enough that we can actually build up to BERT which was this complicated thing that I mentioned at the beginning. So BERT is this large unsupervised pretrained model which came out in 2018 which has really kind of transformed NLP. And the basic building block for generation is, I'm going to call it GenerateToken. And you take a vector x and you generate token y. And this is kind of the reverse of EmbedToken which takes a token and produces a vector. encourage you to consult the original source if you want kind of the actual, the full gory details. Another thing I haven't talked about is learning any of these models. It's going to be using some variant of stochastic gradient descent, but there's often various tricks that are needed to get it to work. But maybe the final thing I'll leave you with is the idea that all of differentiable programming is built out of modules. Even if you kind of don't understand or I didn't explain the details, I think it's really important to pay attention to the type signature of these functions.

ROUGE-1: 34.69, ROUGE-2: 33.43, ROUGE-L: 32.77
BERTScore: 72.78

==============================================
==================== [88/100] ====================
Summary:
Professor Amy Hungerford: Today it is my very great privilege and pleasure to introduce Andrew Goldstone, a TF in this course. Andrew is a fourth-year student in the Ph.D. program in English, and he is writing a dissertation on the autonomy of the work of art in modernism. In preparation for that, for next week I'd like you to finish the novel and then read his essay, "On a Novel Entitled Lolita." It should be bound at the back of your book. The novel invites ethical questions but also holds them off through parody. Nabokov uses the tropes of romanticism and romantic love and parodies them. Humbert's techniques of rhetorical seduction relate to a kind of intellectual problem that Nabokovsky sets himself of trying to make you identify with this villainous character. The novel has only its purpose to afford aesthetic bliss, Nabokovan says. It takes something meant to be serious and turns it in to a dirty joke, he says. sense that civilization itself is being overturned. The idea that culture itself is the saving, most important activity that people can engage in. Fourth--and this goes along with that--a rejection of convention, especially sexual convention, sexual morality. Fifth, spatial form, the idea that in place of a linear narrative you have a system of cross-references and repeated motifs that give the structure of works. Sixth, this is a term from the critic Joseph Frank: spatial form. And then, this anticipates my last points: Modernism is self-consciously international. The strategy of the knight's move is to frustrate your expectations, to leap over the apparently important events into something else characterized by a kind of aesthetic play. Nabokov, in 1966 he said this: "The greatest masterpieces of twentieth-century prose" are, in this order: Joyce's Ulysses, Kafka's Transformation"--that is, "Kafka's Transformation," "Ulysses" and "Lolita" The parentheses are a real icon of that. A critic has counted 450 sets of them in this novel. is, The Metamorphosis--"Bely's St. Petersburg," a pretty obscure Russian avant-garde novel. Proust is himself gay. One of his big subjects is homosexuality, and Nabokov's reaction to this is really homophobic. It's about a relationship to predecessors who are seen as too similar. This should cue you to think about the theme of doubling in this novel, to think of the possibility of desire between men here, says David Bianculli. he's attractive to all women, about his supposed virility. And it should just make you wonder whether pedophilia is in itself a kind of knight's move from homosexuality. In other words, is there another form of perverted desire hiding behind the one that's in front of us? Just a suggestion: look on page 20, still in Humbert's early life, near the bottom. It happened for instance that from my balcony I would notice a lighted window across the street and what looked like a nymphet in the act of undressing before a co-operative mirror. Nabokov's relationship to this modernist past is not just the burlesque that he visits on Eliot. An element of admiration is also present, and that's really part of his relationship to Joyce. I'm just going to name for you four features of Joyce's style that are important to Nabokov: stylistic virtuosity, the ability to imitate any style; at the same time, a scrupulous attention to the banality of everyday life and all its detail. Third characteristic, the constant use of a superimposed structure. A list of names leads up to this aesthetic sensation, the revelation of a poem. The ordinary materials of life become the basis for a kind of artistic achievement. Even the ordinary names are kind of plants, because almost every name on this list comes back elsewhere in the book. McFate is the icon of the difference between the realistic world of Joyce and the already artificial, already aestheticized world of this novel. No one was ever really named McFate. McFates are a parody of real. Nabokov's Lolita is a kind of artificial, processed, bland, easily consumable version of fate. In other words, chance is already fated. The thing that stands for randomness in this book, the thing that looks like ordinary detail, has already been arranged to give you artistic pleasure. This kind of transformation of arbitrary, real fated events into conspicuously artificial tricks is a response to exile, in particular to Nabokov‚Äôs condition of exile. foreign country, lives in a kind of denaturalized world, a world where, instead of everything making instant sense everything has to be decoded. In that afterword to this book, Nabokov says he had to invent America. That's because he didn't know it already; it wasn't given to him. In a way this is a terrible state, a state of discontinuity with the world you exist in. But it has a payoff, kind of, a payoff which is the possibility precisely of inventing. because Humbert is a foreigner--into something you can laugh at, something youCan enjoy, something that you can apply the knight's move to. Gaston Godin says about the school that Lolita's going to go to, the girls are taught "not to spell very well, but to smell very well" The foreigner's love for this kind of move is a response to this denaturalized world of the exile. It's important, in this connection, to remember that the Knight's move as a way of avoiding obstacles keeps skipping over forms of violence. turn of the staircase was glazed with ruby, and that raw wound among the unstained rectangles and its asymmetrical position--a knight's move from the top--always extremely disturbed me. Nabokov will say that his private tragedy is that, let's see: [His] private tragedy, which cannot, and indeed should not, be anybody's concern, is that I had to abandon my natural idiom, my untrammeled, rich, and infinitely docile Russian tongue for a second-rate brand of English. evocation of the landscape: By a paradox of pictorial thought, the average lowland North American countryside had at first seemed to me something I accepted with a shock of amused recognition. "Inutile loveliness" is kind of the key word of Nabokov's technique, and he says the novel has as its only purpose to provide aesthetic bliss. So, a European artist actually appears again there, with Claude Lorrain, but kind of made strange: given that knight's move, given a new twist. Nabokov tried to become an American writer, as Nabokov says he's doing: trying to invent America, trying to bridge the gap between Russian and English. He translated Lolita back in to Russian later on, and he added a second afterword where he said: That wondrous Russian tongue that, it seemed to me, was waiting for me somewhere, was flowering like a faithful springtime behind a tightly locked gate whose key I had held in safekeeping for so many years, proved to be nonexistent.

ROUGE-1: 30.30, ROUGE-2: 28.73, ROUGE-L: 28.36
BERTScore: 67.10

==============================================
==================== [89/100] ====================
Summary:
NORVIN RICHARDS: Today is phonetics, which means that today we begin making funny sounds at each other. Let's see. I'm trying to remember if there's anything that I ought to announce. You remember, maybe, that problem set 1, which confusingly is your second problem set, is due on Thursday. Normally, it would be due on Tuesday. But because I am technologically challenged, it's due Thursday. I just figured out how to get the projector to project over there instead of in the middle. Linguists have a system for writing sounds down so that we'll all know what kind of sound we're talking about. The symbols of the International Phonetic Alphabet resemble letters of the English alphabet. The symbol for the sound of the beginning of "face" is an f, and the symbol of the start of "vase" is a v. We'll talk about other kinds of articulation that English doesn't use, but I think that one just doesn't exist. We have started here with symbols that should all look familiar, but as we go along, we will be seeing weirder and weier symbols. another new symbol. That's the symbol for the sound of the beginning of a "ship," and another new symbol that's the sound in the middle of "azure," the "zh" sound. Both of those are postalveolar sounds. A little further back, there are what are called palatal sounds. These are either even further behind the alveolar ridge, back where the roof of your mouth gets as high as it's going to get. And the one palatal sound that we have in English is the "y" sound at the start of "year" The IPA symbol for that sound is a "j" In the velar sound, the body of your tongue is up against what's called the velum, which is the soft tissue at the back of your mouth. English doesn't make a huge amount of use of the glottal stop, but it's what shows up at the beginnings of words like "uh-uh," that catch that you're getting in your throat. There are places in English where things that we write as "glottal stops" are actually, in fact, sounds. in mind? STUDENT: No. NORVIN RICHARDS: Oh, OK. We'll get to "r," eventually. People investigate this kind of thing in all kinds of ways. These days, people do a lot of MRIs. There are charts of all of the sounds that we're going to talk about plus many more together with MRIs of the insides of people's mouths making these sounds so that you can see the anatomy. You won't just have to think about it.  phonologist: "cat" and "dog" end in sounds that differ in voicing. "z" is voiced and that "s" is voiceless. "v" is more-- there's more air, I guess, than "f" or "v," but it's not the same. If you whisper "safe," and "save," you have the feeling that you can hear the difference between them. "F" is slower, I think my mouth is opening slower, so yes? So yes? Polish has words that end in "k" and words that ended in "g" underlyingly. Polish also has a rule that changes "g," to "k," at the ends of words. Voiced "b" becomes the voiceless version, which is "p," in Polish. "z" and "d" are voiced, and "t" is voiceless, but both are alveolar, so they're both different from "to" And it's a cross-linguistically quite common phenomenon. English has interdental fricatives and alveolar stops, "t" and "d" There are languages out there that have what are called dental stops. Part of your job, if you're learning Tagalog, for example, is to learn to make dental 't's instead of alveolars. There's all this work on what people do to compensate for various kinds of obstructions in the vocal tract. And on the website, there will be a link to charts that will have more official charts by the IPA. There are what are called retroflex sounds. These are sounds in which the tip of your tongue is on your palate. Uvulars are kind of like "k" except more so. Pharyngeals involve constriction near the pharyngeal wall. Arabic has these. The Berber languages have these. And for some reason, the dental stops are still red. Have to fix that? I don't know why. This chart is slightly larger. OK, keep asking me. English has a very large number of vowels and a not-very-good system for writing them. English spelling is so difficult that we can actually have competitions where you watch people spell. In many languages, the spelling bee would just never end because every word is pronounced exactly the way it's spelled. So here are two vowels. The vowel in "bead" and the vowel in ‚Äúbad‚Äù Go ee-ah, eo-ah,. ee, ooh, ee. OK. English has five vowels, but Latin has 12, 14. English doesn't have words that end in "ih," "uh," or "eh," with the possible exception of "meh" Not all speakers of English distinguish schwa from wedge. "caught" and "cot" are different in dialects of English in which I speak a different dialect of which I do not speak a dialect of English. "Ooh" is a high back rounded tense vowel. "d" is sometimes represented with that wedge shape, it's pretty similar. this exercise next time. As we go along, I'm going to be asking you to read things in IPA. So I'll start putting IPA on the slides more and more. So start trying to familiarize yourself with it and get to where you're familiar with at least the symbols for sounds that we use in English. Do you want me to read the rest of them? I'll do some more IPA. OK, so what's the second one? STUDENT: "Sue says he's a bad egg."

ROUGE-1: 15.67, ROUGE-2: 14.15, ROUGE-L: 14.30
BERTScore: 62.38

==============================================
==================== [90/100] ====================
Summary:
Expectation is a basic question that will come up again and again when we look at random variables and probability theory. We're imagining n independent flips of a coin with bias p. The probability of heads is p. It would be biased in favor of heads if p is greater than 1/2. And we want to know how many heads are expected. So what's the expected number of heads? Well, we already know-- we've examined the binomial distribution B n,p.

ROUGE-1: 19.54, ROUGE-2: 18.43, ROUGE-L: 15.40
BERTScore: 71.67

==============================================
==================== [91/100] ====================
Summary:
Aristotle was born 384,15 years after the trial of Socrates. He was sent by his father to go to college. Unlike most of you, Aristotle did not spend four years at the Platonic Academy. He remained attached to it for the next 20, until the death of Plato. Unlike his intellectual godfather, Socrates, who wrote nothing but conversed endlessly, Aristotle wrote disciplined and thematic treatises on virtually every topic. He collected constitutions, 158 of them in all, from throughout the ancient world. animal. "That man" he says "is much more a political animal than any kind of bee or herd animal is clear" Why is it clear? "For we assert," he says, "nature does nothing in vain and man alone among the animals has speech" In other words, he seems to be saying that it is speech or reason, logos, that is able to both distinguish and create certain moral categories, such as the advantageous, the harmful, the just and unjust, and things of this kind. Aristotle says man is political not because we have some biological impulse or instinct that drives us to participate in politics, but because we are possessed of the power of speech. It is a reason or speech, not instinct, that makes us political, he says. To say it's natural for us to do so is not to say we engage in political life spontaneously and avidly, as you might say spiders spin webs or ants build anthills, he writes. The power to know is our ability to recognize, by sight, members of the same polis or city.

ROUGE-1: 9.81, ROUGE-2: 9.33, ROUGE-L: 8.96
BERTScore: 55.42

==============================================
==================== [92/100] ====================
Summary:
Mathematically, a consumer is trying to maximize his utility. And this utility maximization has to be done with respect to some constraint and the constraint the budget constraint we take P 1 x 1; P 2 x 2 should be less than or equal to I. In real life, it is possible that a person derives some satisfaction from having some money left in his pocket, but the way this problem has been framed here the person‚Äôs satisfaction depends only on his level of consumption of good 1, and good 2. Student: What we have here is such that P 1 x 1 plus P 2 x 2 is equal to I. Student: Downward sloping, slope is again going to be equal to minus P 1 by P 2 here also is slope is minus P1 by P2 ok. And similarly, we can draw for different. values of K and this is the optimal bundle, this is. the bundle; let us say x star and here is let us call it also x star in the old problem just to distinguish x star. in the new problem.

ROUGE-1: 17.33, ROUGE-2: 16.60, ROUGE-L: 17.23
BERTScore: 68.12

==============================================
==================== [93/100] ====================
Summary:
The definition is a cylindrical container containing a date covered with a [? basic ?] substance that can be deployed in order to attract and capture insects. Your support will help MIT OpenCourseWare continue to offer high-quality educational resources for free. To make a donation or to view additional materials from hundreds of MIT courses, visit MIT Open courseWare at ocw.mit.edu.The following content is provided under a Creative Commons license. For more information about MIT Open CourseWare, visit opencourseWare.org. i3. A transportation system responsible for moving people and products with an enclosed metal frame equipped with various safety devices using electrically-powered control and locomotion subsystems. A concept for an all-electric vehicle. The i3 and EPFL? Anybody do the i3? Nobody. Anybody else here? Yeah, Veronica. Thank you. There's a lot there. OLIVIER DE WECK: OK. That's pretty good. I would throw a question. Justice. defined could almost apply to a Tramway as well. If this was like a streetcar, don't you think it would apply to that as well? So I think the fact that it's a personal vehicle, I think it's important. So the key in this is, describe the concept using few words precisely, but to set it apart from neighboring concepts. What about Rolex Center? It's a single-layer building with multiple straw used as a library for people to meet and study. built. The first thing you do is understand where is the value-- the stakeholders and the stakeholder analysis and the requirements definition. And then you interpret and incorporate some of the needs into goals, which become requirements. And so the goals then are an instrument of the primary delivery value delivering process, which is your value proposition. To then actually deliver that value you need to design the product, the product system, and understand the operand, the thing that is being operated on or transformed by the primary value delivery process. In order to chill, we need a chiller, and there are different types of chillers, like a cooler or refrigerator. The key idea is start thinking in this abstract way, and all of a sudden all these other possibilities become possible. Once we have that, we can start managing complexity, decomposing function and form. And then our system object, you can decompose it into different elements, supporting systems, the operand, the operator, and so forth. It's the combination of this specific way you're going to operate the system. for something very simple like a cooler. architecture selects the concept, the decomposition, mapping of function to form. Design, then, given that, selects the actual values for those design variables, and then you can optimize. People in the US, we like to have big refrigerators, big gallon of milk, and refrigerators in Switzerland are much smaller. The real complexity comes in when you look at the form form mapping. So this is then the decomposing of the refrigerator in terms of all the elements of form and how they relate to each other. is concept generation. So take the requirements and think creatively about how these requirements could be fulfilled. And then once you have several concepts, you've got to select among them, which we'll talk about next week. It's about starting with the operand. What is the thing that the beneficiary, the stakeholder cares about, and how do we transform that? Concept then elaborate these into architectures that have form function and structural complexity. And the goodness of an architecture is really a pretty complex concept. System design and management program is a full-year program. It's essentially focused on decomposition, which is an important part of architecting. So let me talk about methods and tools for concept generation. So what are different ways of stimulating or organizing creativity? And what I'm showing you here is-- that's essentially a mind map of how to think about the creativity space. And then we have this whole area here, which I'm going to mention, but we're not going to do as part of the class. There's got to be a driving question for the brainstorming session. There's an ideal group size, and it says 5 to 10 here, but I should probably revise this to be-- what do you think? 7 plus minus 2. The idea is to produce a large amount of ideas and then at some point, maybe 30 to 60 minutes, Brainstorming session that last four hours is probably really good and then the rest is kind of shot and there's not a lot of new ideas coming. Leonardo was a head of his time in many ways. So he's really been identified as an exceptional individual. How to Think Like Leonardo, Seven Steps to Genius, is a book on creativity. The seven da Vincian principles of creativity are here in Italian. They include curiosita, lifelong quest for learning. Dimostratzione, testing your knowledge through experience, trying things out. Sensazione, continual refinement of the senses. Mastering ambiguity, paradox, uncertainty. Arte/Scienza is the whole brain thinking, left-right brain. Corporalita, balance of body and mind. or an architecture. So the key decisions are the rows. There are factors in the rows, and then for each row you think about what are the number of possible alternatives for doing this. And then you enumerate all possible combinations. And the big challenge with this, of course, is if you have many factors, you could generate many infeasible architectures. Not all these combinations are actually feasible. So that's architecture enumeration, and there's different ways of doing this at different layers of abstraction. A3 assignment A3 asks students to come up with new ways of brainstorming. Students must use mind maps, morphological matrices, and architecture enumeration. The assignment is due in two weeks and will be graded on a scale of 1-10. The final assignment will be given to students at the end of the month and the results will be published in the next few days. For more information on the assignment, visit the assignment website. It is open to students from all over the world and can be downloaded from the assignment site.

ROUGE-1: 26.00, ROUGE-2: 23.35, ROUGE-L: 22.41
BERTScore: 66.96

==============================================
==================== [94/100] ====================
Summary:
then we are going to continue with the second part of the lecture today which focuses on the problem what actually happens if the gaussian assumption that i have about my constraints doesn't hold. One of the questions actually how to handle that so as we said what we are doing here we are minimizing the sum of the squared errors terms and as we have seen so far this is the same or strongly related depending on how you formulate that. If we would have the possibility to integrate a multi-modal distribution here that would actually be a nice beneficiary. If you have structures in the environment and there's a lot of clutter in the scene the clutter even if it has a repetitive pattern may lead to a multimodal belief about what the relative transformation between two poses let's say. Other things is gps can even be problematic if you have this called gps multi-pass problems you have reflections of the gps signal based on larger buildings. You may get beliefs or you may get outlier measurements and the question is how can we actually take that into account. have also experienced and if you look to those poses over here in the poses down here how those individual structures match um if you just apply let's say scan alignment you may say this may match so maybe someone has opened the door which was closed before or here is a door now closed which was open all the other scans map actually quite well the same holds here. Even as a humanism you say okay there is definitely a misalignment between the skin so they don't fit perfectly but that's something which actually can result from small changes in the environment. The original number of constraints i don't know how large it was but i think it's around three thousand something like this along those lines. If you put already ten wrong ones in there it's quite likely to screw up it was one hundred which is still a very small number compared to 3000 or just a small fraction. It will actually end up in dramatic mapping errors so the system is unusable so having good data cessations is really important so already screwing up a small number of places is something which can hurt your optimization. the first attempt to to solve this problem this would be our probability distribution what is the problem with this probability distribution so i say okay some of my constraints are these multiple multimodal constraints i will simply go ahead and implement that. What's the problem that you're going to experience if you make this is some so this is we know how to solve that right this is what you know howto solve that that's what we did so far so the thing is we have a waiting term down here so we make sure that it sums up to one so we assume we have normalized this. robot and the ground is muddy the wheels may slip before you get grip and the and the robot starts starts driving if this is the case although you're executing command you're standing and then you start moving so you may get this kind of distribution. In most cases actually the vehicle executes what you tell the vehicle to do but in some cases simply doesn't move. This max mixture idea is actually a pretty easy idea pretty simple idea just reply funny no one has done that in robotics until recently a few years ago. that's actually a nice thing so um another thing is it can handle both things at the same time data station errors as well as multimodal constraints. So the combination of outlier rejection and dealing with wrong data associations is actually kind of nice we also can do this obviously in 3d so this is again this data set with the sphere that we have seen before robot moving in a virtual sphere with constraints. This is gauss newton and this is the max mixture gaussnewton and um so you can see here there's a non-perfect alignment in here because you don't see the regular structure. if he increases to 100 outliers this is just whatever a big mess whereas this one still is able to solve those things quite nicely. The key idea the intuition behind that is if i have a constraint which has a large error so where the current configuration is far away from what the constraint tells me just reduce the uncertainty that is associated to that so decrease um the the information matrix so scale down the information Matrix so they can just a small change by saying the question is still how do we actually compute these so the main changes we go to this formulation. and flatter and flatter gaussian distribution the further the point is actually away there's also one technique which you can find which is also quite easy to implement because you just need to compute the scaling factor and multiply that with your information matrix for every constraint also something you can do quite efficiently. If you have constraint which introduce large errors these are these outliers this can actually screw up the optimization when computing the minimal error configuration. There are different ways of uh kind of row function that we can actually in there and then we're then trying to minimize. setup to another sensor setup can be quite tricky on but the back end itself which sits here doesn't really change that much therefore the focus in this course which was much more on the backEnd. At least i would tell you a little bit about what typical front desk exists and how you could realize a front end if you want to build a slam system. Well that's something we are going to do next week that's it from my side thank you very much and hope to see all of you next week.

ROUGE-1: 27.38, ROUGE-2: 26.77, ROUGE-L: 27.35
BERTScore: 67.06

==============================================
==================== [95/100] ====================
Summary:
John Stuart Mill is the principle expositor of neoclassical utilitarianism. The rights-utility synthesis signals that we're looking for an attempt to put together both a commitment to utilitarian efficiency and respect for individual rights. The transition for classical to neoclassicals utilitarianism really went on in all fields of thinking about the human sciences at more or less the same time. The emotivist doctrine is an endpoint in a philosophical evolution that really begins in the seventeenth century. The theory of value was going to be tremendously influential and important in the way utilitarianism evolved. Hume's famous for the idea that an ought cannot be derived from an is; that there's a fact-value problem. But he thought, nonetheless, people are pretty much the same, and so if you can figure out what makes one of them tick, you can find what makes all of themtick. And that was most emphatically Jeremy Bentham's view. It's presupposed in everything we discussed last time. If you think about the idea of doing interpersonal comparisons of utility, and making the judgment that taking that dollar from Donald Trump and giving it to me would be better, that's what Bentham would have said. it to the bag lady increases her utility more than it decrease his, you're assuming that they basically all have the same kinds of utility functions. Stevenson questioned that idea radically. He said, "We don't actually know. We should take Hobbes much more seriously in his critique of Aristotle than he was willing to take himself" And so that was thought to be a radically relativist doctrine because it seemed to undermine the possibility of making ethical judgments of any sort across people. That is a doctrine to which we will return, as I said, when we get to the anti-Enlightenment and, in particular, Alasdair MacIntyre's book, After Virtue. But today we're going to focus for the rest of our time on the economics of the transition from classical to neoclassical utilitarianism. And I'm going to ask you to suspend disbelief for the whole of today's lecture and just trust me, because what I'mgoing to do is I'mGoing to go into this backwards. I'm Going to look at a very different problem that the neoclassicals economists were concerned with that had nothing to do with utilitarianism, or rights, or anything that we've been talking about. we would know about this person A, as I said, is that they prefer four to three, three to two, two to one, one to zero. But we can't say anything about how much they prefer those things because these distances don't actually mean anything. All we get is an ordered ranking. One other thing we can say is, that this is a no-no. These indifference curves cannot cross. Can anybody tell us why? Why can't they cross? Wait for the mic. you get it that way. But so now we have a diagram with two people on it, okay? So this is person A, and this isperson B. And these axes, the X-axis, here, is A's utility function. Remember A in the previous slide was trying to get from P towards Q, right? A was Trying to go up here. So it looks like A's happier than B, but that's misleading. If the different distances are taken to imply in your mind that A's happiest than B disabuse yourself of that thought right away. use, yeah, let's use J. If I put a point here, J, we would say that A's gone down and B's gone up. Now, to make this a bit more real imagine in here this is the sphere of market transactions. This is where A and B will go voluntarily, right? So A will say to B, "Well, I have all this wine, and you have all that bread, how about I swap you a bottle of wine for a loaf of bread?" And you say, "Okay." The Pareto principle out of which the whole of neoclassical economic theory was constructed depends on this idea of indifference curves. A is trying to get up on those indifference curves, and B is wanting to get along on them. When they get to a point at which no matter what swap A is willing to propose, B says no, then you know they've hit that frontier, what's called the Pare to possibility frontier. Once they wind up anywhere on this indifference curve they're not going to move of of improving one person's utility. diagrammatically, and then if anybody doesn't get it we'll wait up and I'll go through it more slowly. But think about the diagram we just did, okay? Think about A is here in the corner,Okay? But basically now we're putting the two previous diagrams together. We're putting this diagram, where we have two commodities and one person, and this diagram where we've got two people and just utility. And so you'll see why this is helpful once we get to the end of it.

ROUGE-1: 23.91, ROUGE-2: 22.82, ROUGE-L: 22.95
BERTScore: 65.60

==============================================
==================== [96/100] ====================
Summary:
HONG LIU: We are going to talk about chiral fermions. We will look at the specific representation of gamma matrices. When the S is block diagonal, that means, when I write psi-- so, psi-- have four components. That means that the upper two component and the lower two components-- they don't transform to each other, OK? They only transform within themselves. And so there are two ways to reduce it, and one is called the Majorana fermion. HONG LIU: You don't need four components to be able to transform under Lorentz transformation. At least two components can already transform. So this tells you, in a sense, that the LorentZ covariance only requires two component spinors. So we have to do a little bit of work to do much work if you actually find the right trick to do this for any choice of gamma matrices. So you can check yourself that the gamma 5 is actually Hermitian. You need i for this to be true, OK? You can also take the product of all the gamma matrix together and then with a factor of i, OK?" Without the chiral symmetries, there's no pion. Symmetry is only present in the classical level but not in the quantum level. And, again, that plays a very important role in particle physics, actually. It's also important in many condensed matter systems, like liquid helium, et cetera. Will not go into detail. The pions-- they essentially come from the Chiral symmeetries. And actually understanding how the pions come from them was a Nobel Prize to Nambu. HONG LIU: If tilde is just some other constant, then you multiply gamma 5. So, here, we can rewrite a little bit differently. We can consider rewrite this alpha L and alpha R in terms of the following. Let's consider the two transformation-- one transformation, psi L, and psi R transform the same. And the other transformation is that they transform oppositely. They transform in the opposite phase, OK? So I write the alpha-- and this writing is like that. HONG LIU: In physics, actually, the massless case actually gives you very much richer structure, normally, than the massive particle. Mathematically, it's because the representation of the Lorentz group is very different from the massive case. In Euclidean space, you continue gamma 0 to gamma 4. And then you reserve gamma 4 for that. Then the gamma 5 is the next one you take.this? Yes. Other questions? Yes, I'm happy to answer. HONG LIU: In the Dirac spinor, which we have talked about so far, is four components. In the chiral spinor we talked about, you have two complex components. The next one I'm going to talk about is the Majorana, in which case, I would argue, we have 4 times 1 real component. And, now, let's talk about the last case-- this case, which is-- you have four real components, OK? So what do you do? Again, we follow the similar strategy to see whether it's possible to have fourreal components. But, in principle, you don't have to choose them to be unitary. OK, so, now, let's double check. So let's check that star star-- so we show that, here, in this representation, this is compatible with Lorentz transformation, OK? So we still need to check star, star is compatible. So it means that the psi prime star should be equal to the same as B psi prime. So that means this iscompatible with LoreNTz transformation. B when you act on sigma mu B minus 1. So that gives you minus sigmaMu, nu, star, OK? So this is obvious because sigmamu, nu has i there. And, otherwise, the B takes each gamma matrices there into star. So we have used this trick many times. Yeah, and then so we get a very nice relation that, under Lorentz transformation, again, is generated by this B, related to this B matrix. So B, in this case, can only be gamma 2. we have-- so this real scalar theory-- we can see that before. So this theory has a discrete symmetry because this is invariant under phi. It goes to minus phi, OK? And so this is-- if you do it twice, you go back to itself. And there are also spacetime discrete symmetries. So you can have so-called time reversal, which corresponding to your t. Then you revert all the spatial direction. So comment that you can ask why we actually reverse all three directions. How about if I just reverse one direction or reverse two directions? you take that phase to be pi-- say, exponential i pi-- and then you take to be minus phi. And so, in that case, this is part of the continuous symmetry. But, here, there's, nevertheless, another discrete symmetry. Can you see what is the other independent discrete symmetry here? Yes? Good. You can exchange phi to phi star, OK? It's a complex conjugation. And this is often called charge conjugated.

ROUGE-1: 17.89, ROUGE-2: 17.02, ROUGE-L: 16.33
BERTScore: 66.86

==============================================
==================== [97/100] ====================
Summary:
The best-case scenario for expansionary fiscal policy is when there are lots of underemployed resources in the economy. By increasing spending, the federal government can try to counteract falling aggregate demand. In one scenario, government spending doesn't have to be as large as the fall in "C," or consumption, to counteract the recession, and that's because of the multiplier effect. But, as always, shifting lines on a graph is much easier than shifting around real resources in a multi-trillion dollar economy.

ROUGE-1: 30.09, ROUGE-2: 28.77, ROUGE-L: 30.09
BERTScore: 67.40

==============================================
==================== [98/100] ====================
Summary:
Sarah thread sterner shows you how to wear and take off a mask. She explains how to determine which part of the mask is the front versus the back. She also shows how to mold the nose piece of the face mask with the finger tips of both hands. Finally, she shows you the best way to remove the mask by placing it over your ears and pulling it under your chin when removing the mask it's important to remember that the front is considered contaminated so remove it with the fingertip and then perform hand hygiene.

ROUGE-1: 29.04, ROUGE-2: 20.29, ROUGE-L: 23.82
BERTScore: 64.55

==============================================
==================== [99/100] ====================
Summary:
Adam Martin: In this video, researchers are shining light into a mouse's brain to activate specific neurons in order to test whether they function in arousal. In today's lecture, we're going to work towards understanding how this experiment works. We're also going to talk about how neurons function and how researchers are able to control that function to modify behavior-- in this case, the arousal of this mouse. And here, you see the mouse is going to wake up. There it goes. It's awake now. The sciatic nerve extends from the base of your spine all the way down into your foot, OK? So that's an extremely long distance to transmit information along a single cell. And so we're going to go from thinking about how signals are transmitted in single cells, and this will evolve electrical signaling. Then we'll talk about synapses and how synapses function to communicate between neurons. And this is going to involve also sort of understanding how certain antidepressants, like Prozac, work. And then we'll end by talking about how researchers did this experiment to wake up the mouse. In a resting state, the cell's resting potential is negative 70 millivolts. The cell can lose this polarity and not have a charge differential, or it can flip and be positive on the inside. Stephen suggested opening ion channels. If you open these, it's going to depolarize the cell. Because remember, high on the outside, out here. And so if you open them, positive ions are going to flow in to make this less positive. So what is an action potential? Everyone see how it's a transient potential. There's electrical insulation around the axons of these neurons. This is provided by another specialized cell type called a glial cell. In multiple sclerosis, the myelin sheath is damaged. And if you damage this electrical insulation, you greatly slow down these action potentials. That has a significant impact on nerve impulses in the brain and throughout the entire body. And so this neuron will then have to decide whether or not to fire an action potential down its axon. And the way that the neuron decides this is to integrate the signals. Prozac, Zoloft affect this reuptake process. And what that does is it keeps the neurotransmitter in the synaptic cleft for longer, such that it enhances the signaling. So the idea behind these drugs is that if you are suffering depression from a lack of serotonin, then you can rescue that by preventing the rapid reuptakes of the neurotransmitters. All right, now I want to end by just telling you how this experiment works, where we're able to activate specific neurons in a brain and that leads to the brain.  optogenetics is an approach to control the activity of a cell with light. In this case, we're going to have light inducing depolarization. And the way this is done is there's a protein discovered from photosynthetic algae that's responsive to light. And this protein is called channelrhodopsin, specifically ChR2. And if that's expressed specifically in the neurons that you're trying to test, you can then shine a light into the brain of the organism and activate this type of neuron. you to test the function of the neuron in the behavior of an organism. So, in this case, this mouse, the light is shined into its brain, and they're testing a specific type of neuron that is involved in arousal of the mouse. And it's going to wake up right now. There it goes. It woke up. You see now its muscle activity is going, OK? So you can test thefunction of specific nerve cells using this approach, and it's because you have a light-sensitive sodium channel.

ROUGE-1: 18.25, ROUGE-2: 17.46, ROUGE-L: 17.71
BERTScore: 72.94

==============================================
==================== [100/100] ====================
Summary:
In this problem, we're going to be dealing with a variation of the usual coin-flipping problem. But in this case, the bias itself of the coin is going toBe random. And we're told that the expectation of this bias is some mu and that the variance of the bias isSome sigma squared. And what we'll be asked is find a bunch of different expectations, covariances, and variances. We'll see that this problem gives us some good exercise in a few concepts, a lot of iterated expectations. An art that you learn through more practice. But one good rule of thumb is, when you have kind of a hierarchy or layers of randomness, it's useful to condition on the layer above where that is, in this case, the random bias of the coin itself. Once you condition on that layer above, that makes the next level much simpler. Because you kind of assume that you know what all the previous levels ofrandomness are, and that helps you calculate what the expectation for this current level.

ROUGE-1: 10.90, ROUGE-2: 10.35, ROUGE-L: 10.53
BERTScore: 70.24

==============================================
