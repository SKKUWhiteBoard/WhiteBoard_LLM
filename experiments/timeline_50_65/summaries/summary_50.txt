The following content is provided by MIT OpenCourseWare under a Creative Commons license. Additional information about our license, and MIT Open courseWare in general, is available at ocw.mit.edu. Welcome back to the second half, where we'll talk about multisequence alignment, for starters. And I said that I would, and I'm happy to do so. Back to the page you came from. Follow us on Twitter @CNNOpinion and @wiredculture. Show this slide again. This time-- before, it was to introduce how we would go about getting an empirical substitution matrix from distantly related protein sequences. Now, we would like to ask, how did we get that multisequence alignment? This is one way to go about it. The next slide shows how to get an empirical substitutions matrix from protein sequences that are not related to each other. This is another way to get the same result from a protein sequence that is related to another protein. of thinking about it as a generalization of the two-dimensional array that we had before, where we would have, say, two sequences, one horizontal, one vertical. Now, the third dimension is the third sequence. This gets harder and harder to visualize as the number of sequences you put in, but it's not impossible to do. It's just a matter of how many sequences you want to put in at a time, and how you do it, and what you do with them. let's think about it in three dimensions for just a moment here. And when you have a multiple alignment, you can think of it as dynamic programming on this hyperlattice and that the indels for any pairwise combination may not be optimal for the triple. And let's go beyond triple, we can even go beyond two to three dimensions. We can go beyond three dimensions to four or five or six or seven or eight or nine. We're talking about dynamic programming in a three-dimensional world. but to a very simple dinucleotide alignment. And we will say that this is the optimal multiple alignment. You can see here that the multiple examples of AT anchor the A and T as being separate positions, even though normally, if you just did a pairwise alignment with a high high A and high T, they would both be in the same place. But in this case, the A is in the middle of the T, and the T is at the top of the A. gap penalty, there would be a tendency to line the A up with the T. You would not have these canceling indels. But in the context of the multialignment, you now have a different interpretation. So we want to generalize the kind of algorithms we've been using. And again, this is not a new idea, it's just an extension of what we've already been doing. It's just a new way of thinking about how to deal with the gap penalty. will be a recursive algorithm where the score of a two-character string is defined in terms of the maximum of various shorter strings. So at the very top is the case where we have no insertions-- the simplest case. And we just ask the question: What is the best way to write a string of two characters? The answer is that it depends on the type of string you are trying to write, and the length of the string you want to write it to be. For example, it could be a string that has only one character, or a string with two characters in it. what is the score of having a [? VSA, ?] that is to say, this triple single-amino-acid comparison. Now, we're asking a V substitution for an S substituted for an A. Now the number of different VSAs that can be substituted for the A is: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 68, 69, 70, 72, 73, 74, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 94. cases we have here-- before, it was 3 for a global alignment, which was k, being the number of sequences, was 2. Now k is 3 for three-way comparison. And all possible subsets is 2 to the k minus 1, in this case, so it's 7. So seven cases, so seven cases of the same thing. The number of possible sequences is k plus 1, so k is 7 for a three- way comparison, and k is 2 for a single comparison. and you can just walk through them. You can see the first one is no insertions or deletions. The next three are two insertions and deletions in the three different ways that can happen. And then the last three are one placeholder, one of these dashes, which means that the dashes are one of the places where the insertions can go in the code. And you can see that the last one is one placeholder and one of this dashes. The last three places are one placeholders and one placeholder. The first three are no insertion and the second three are deletions and the third three are insertions. other two sequences have insertions relative to the one dashed. So this is the seven cases for a three-way comparison. Now, as k grows, then both the space complexity-- the amount of lattice points that you have to store somewhere, either in RAM, or disk, or somewhere-- grows by n. This is the case where k is the number of points in a lattice that needs to be stored in some way, such as a disk or a RAM. And this is also the case when k is greater than 1. to the k-th power where the sequences are roughly n long and the number of sequences is k. Now to compute each of those nodes-- well, I mean, what will be on the order of 2 to the k power, because remember I said that theNumber of subsets in the subsets is k-k-k. That's the power of the kth power, which is the power that we use to work out the n-th subsets of a set of sequences. general is going to be 2 to the k minus 1 or about roughly 2 toThe k. And so the time complexity is have to do 2 tothe k comparisons per node. And there are n to thek nodes, so it's of the order 2 the k. The time complexity of the general algorithm is about 2 to The k plus 1. The complexity of The General algorithm is 2 to The K minus 1 or about roughly 2 to The K. times n to the k. Now, this is not a straw man. This is using all the power that we developed for the pairwise comparison and we're just generalizing it. And so this is actually a hard problem. This does scale exponentially with k. It is not some naive algorithm. It's using a lot of power that was developed for pairwise comparisons. And it's not a naive algorithm, it's just using all of that power. It doesn't just scale exponentially to k. And it's not like we only want to do k equals 2. There are very good reasons for inferring structure or function without experiments, just from sequence. And the larger k is, the more you can explore. It's like doing a huge mutagenesis experiment and exploring viable mutants. So we don't just want to infer structure and function. We want to explore viable mutants and find out how to make them. We don't want to just infer structure orfunction without experiments. This is the way we deal with most non-polynomial calculations, that is to say, in this case, exponential, which is we approximate. Now, you can get something that's very close to the true optimum if how to prune it. This is how to do multisequence alignments, so how do wedeal with this? This is the how to deal withmultisequences, so this is how we do it. We approximate. We prune. We get the optimum. If you know where the band should start and how wide it should be, you can essentially prune off many of the nodes without really losing any optimality. But you have to be very sure. Remember, one of the examples I showed was you could take this band and make it bigger. You can do this with any kind of data structure you want. You just have to know where to start and where to stop. This is a very simple example of how to do this, but it can be very complex. you know where to start it and how wide it should be. So it's optimal within those constraints. Then there are others which are more heuristic. They are not guaranteed to be optimal, but on the other hand, they don't necessarily require arbitrary pruning. And the two that we'll illustrate are the one and the other that we're going to illustrate today. They're not necessarily the best, but they're the most likely to be the most effective. And that's what we're trying to do here. in the next couple of slides is a tree alignment, as illustrated by ClustalW. By the way, pruning is illustrated by a program called MSA, which is short for multisequence alignment. And we'll show a star alignment. Later on, later on into the transcriptome part of the report, we'll talk about how the genome works. We'll also talk about some of the challenges that we face in trying to understand the human genome and how it works. course, we will talk about the Gibbs algorithm. Let's walk through ClustalW, and then a star algorithm. So here's progressive multiple alignment. And I think most of you, if I had given you the luxury of just thinking this through during the break, how you would do the multialignment, this might be the algorithm you would come up with. Almost always it makes sense to start with the pairwise alignments because that is a solved problem. And so here, you take each of the, let's say four sequences and do all of them. pairwise alignments. And you get this 4 by 4 matrix. It's going to be symmetric, so you only have to do the diagonal and the off diagonals on one-half of it and. You get the best score is S1 with S3, which has a score of 9. And so you get a matrix that is symmetric and you can do it in that way. And the matrix is called the S1/S3 matrix, and it's a symmetric four by four matrix. you can construct a tree. And this is-- basically, we're starting to describe the method by which weconstruct a tree, such as that tree of life that I've shown a couple of times now. And so when you construct a. tree, you take the two closest scoring sequences, and so on, and this is the method we use to create a tree in the first place. This is the way that we construct trees in the computer world, and it's the same way we construct them in the real world. you indicate them as terminal branches of the tree. And you connect them to a fork, a branch point. And the distance of each from the common ancestor is indicated by the length of these lines. And so the second-best score is S2 and S4. It's a little bit weaker than the S1 and S2. And it's the same as the S3 and S3, which is the S4 and S5, which are the S6 and S7. similarity than S1 and S3. So you have these longer branches indicating greater divergence. And they're in their own cluster. Now, it turns out that then the common ancestor for all the sequences, which would be the. common ancestor of the common ancestors of the first two clusters, is represented. by S3 and S1. The common ancestor is represented by S4 and S5, and S6 and S7 and S8 and S9 and S10 and S11 and S12. by this final branching closest to the trunk of the tree or the roots of tree. And here, distance is this horizontal axis. And then, once you have this dendrogram, the next step, or the full steps, are aligning each of the sequences, which you already had to have done. The next step is to align the sequences in the order that you want them to appear in the tree. For example, if you want to align each sequence in the same order, you need to align them in the direction of the trunk. Step 3 is new. You align this alignment, we'll call it the pair S1, S3, with the pair N1, N2, N3. Step 4 is to calculate the similarity matrix. These are a pairwise alignment of S 1, S2, and S4. Steps 1 and 2 were already done to get a similarityMatrix. Now, step 4 is new, and it's to calculate a similarity matrix of the same size. This time, it's the same as the similarityMatrix of the previous two steps. S2, S4. And you could imagine keeping doing this hierarchical process. If there were additional sequences which are even more distant related, let's say S5, you would take this alignment and align it with a single sequence S5. So you can see how you can use this to your advantage, I'm sure you'll find a lot of ways to use it in your work. I'm looking forward to seeing what you come up with next. I'll keep you posted. align not only sequences, but you can align pseudosequences, which have these little indel bashes in them. So that's one method. This is a different method. And here, the premise is that you've got one sequence which is sufficiently close to all the other sequences that you can use it. That's what we're trying to do here. It's a bit of a different way of doing it. But the idea is the same. We want to get the same result. as an anchor sequence. And whatever indels you put in individually pairwise, for that sequence, can be propagated throughout the entire multisequence alignment. So here, we start the same way. Here, we have five sequences instead of four, but it's the same thing. You do all pairwise similarities, and you do all similarities. And you have a sequence that can be used in any way you want. And that's what we're trying to do here. give a score. These scores are the scores that would have come out at the end of that traceback in the pairwise alignments. So this is not a pairwise matrix. This is the results of 5 times 4 over 2 pairwisealignments. Each of these boxes itself is the outcome of each of these alignments, and the score is the result of those alignments combined. The result of each alignment is the score of that alignment, plus the score from the previous one. of a full matrix on S1 versus S2, for example. And you can see from these set of scores that the best score, or the best set of Scores for any sequence, is S1. S1 has the best scores to S2 and it has thebest score overall to all of the sequences in the matrix. The best score for S1 is the score it gets overall, not just for S2 or S3 or S4 or S5 or S6 or S7. the sequences. And so we'll use S1 as the focus of the star geometry. And we'll say OK we've already compared every sequence to S1. But let's focus on that. And now take wherever the indels were that were required to get the get the star shape. That's where we'll focus on. We compared every sequences to every sequence. And then we'll look at where the indel was that was required. And that's where the star was that we wanted to get. best score for S1 with each of the others, and have S1 in red in each case, and use that as the anchor. And so then in the multialignment, you take all the indels relative to the red one, and introduce them so that it's the anchor, and so on. So those are those are the best scores for each case. And then you take the best score for each of those cases, and add them together to get the best overall score. two radically different ways. The Gibbs sampling, just in a nutshell, is in general when you have a hard problem, where you can't comprehensively go through the entire space, what you do is sample it. You say, let's try a few things, and try to randomly sample it, and maybe even develop locally. If, after randomly sampling in certain places look better, then look near there, and find other solutions, and keep optimizing. That's the Gibbs. Now, we have explored the space-time accuracy trade-offs. by having this storing, this pairwise or multi sequence in a matrix-- so that's actually you've done a trade-off where you've taken up computer memory in order to save time. And then if you're willing to sacrifice a little accuracy or a little comprehensiveness, then you can save even more. That's what we're trying to do here. We're not trying to take up memory, we're just taking up time to do a better job of doing a good job. time or memory. Now we want to use motifs, which is the sort of thing that you get out of local alignments, to find genes. And we're going to use the motifs and the finding genes as a way of introducing a particular motif. which is a CG motif, as a new way of finding genes. We're using the CG motif to introduce a new type of gene to the genome. We'll be using this as a tool to find new genes in the future. Genes have little bits of sequence at the beginning, in the middle, or the end, which are distinctive. They have distinctive properties, typically sequence properties. So at the start of the gene, before the protein-coding region, there is a distinctive region. This is where the Markov model is used to look for genes that are unique to that region. A simple example of a hidden MarkovModel is shown below. The Markov Model can be used to find genes in a variety of ways. or RNA-coding region, you'll have regulatory elements such as promoters and so-called CG islands. The CG islands are basically an abundance of the CG dinucleotide. Of the 16 different dinucleotides, CG happens to be one of the most common. It's also the most abundant nucleotide in the human genome. It is also the least common nucleotide among all the other nucleotides in the genome. This is why it's important to have a common nucleucleotide for all the different types of DNA. underrepresented in general invertebrate genomes, and over-represented in promoter regions upstream from genes. And the reason is probably that they bind to transcription factors, and the transcription factors protect them from methylation, and thereby protect themfrom a mutagenic process that would otherwise cause them to become a TG. Now, that's probably not the whole story, but that's the way to think about it. And we'll have to wait and see what happens with the next generation to find out. that's the example of a distinctive sequence element that indicates the beginning of a gene or just before beginning of the gene. Within the gene, especially-- well, only-- if it's a protein-coding region, you'll have preferred codons. These are preferences that are set by the particular abundances of transfer RNAs in a gene. The most common type of gene is the human gene, which is a copy of the human genome. It's a very complex gene, but it's very similar to a human gene. in the cell, as well as other constraints on the sequence. If you're in an organism that does RNA splicing, you'll have RNA splice signals, and they'll have distinctive sequence features. You will have to maintain the translational reading frame across the cell. You'll have-- if you have RNAsplicing, then you will have to maintain the translational reading frame across the cell and the resonance frame across the cell. the splice junctions. That's a hint. If you have multisequence alignments, then you can look for conserved positions and interspecies conservation. The ultimate cheat is if you have a cDNA in the case of species that are spliced. Then you can figure out the splicing just empirically by the presence of splicing junctions in the cDNA. That’s the ultimate cheat. It's the only way to find out if a species has been spliced or not. of actually sequencing the messenger RNA that encodes your gene. Promoters and CG islands are sort of degenerate. They're weak. So you know there's a gene there because you found it present in the messengerRNA population, and you sequenced it. Now, there are problems with each of these approaches, and there are ways to get around them that don't involve using CG islands or Promoters, for example. But there are also ways to use CG islands, which don't require CG islands. sequence signatures. There's a high variety, and they're used in combinations. When we're looking at preferred codons, we need a lot of codons in a row to see a preference over random sequences. Random sequences will also contain some of the same codons. And if you need longer ones, then longer codons are preferred as well as shorter ones. For more information, visit the Unicode Consortium's website or go to: http://www.eNCODE.org/. you'll miss tiny proteins. Similarly for RNA splicing, you can have weak motifs, again. And alternative splicing-- it's not like there's one specific splice that occurs in a particular gene segment. There can be multiple kinds. Conservation requires that we make sure that we don't miss these tiny proteins, which are important for the survival of the cell's cells. We'll talk about this in just a moment, specific examples of different kinds of spliced proteins. you have the right species, that at least some of the species in your multisequence alignment are just the right distance-- not too close, not too far away. And cDNAs are great, if you have them. If you have very rare [? trends, ?] you need to have the cDN as well. But if you don't, you're going to have to find a new way to do it. And that's a lot of work, but it's worth it in the long run. cell type and the rare [INAUDIBLE],, rare messenger RNA within a cell type. So let's talk about the sizes of proteins. If you look here, I plotted the sizes. of proteins in annotated genomes-- two of the first annotated. genomes is the smallest eukaryote yeast and the smallest prokaryote Mycoplasma. The size of the proteins is based on the type of cell that it is in, and the cell type that it's in. and asked what were the sizes of the proteins that are annotated? Proteins in quotes, because this is what humans and computer programs together chose to represent. This is not truth, necessarily. And you can see it goes out to over 900 amino acids. And if you go to humans, the size of the protein is about the same as the human size. And it's around the same size as the protein that is in the human body. And so on and so on. this would go out to 10s of thousands of amino acids long for the largest proteins. But let's focus attention on the smallest proteins. How is it that it precipitously drops off at 100 amino acids? Why are there so few proteins that are short? And there are slightly more short proteins in Mycoplasma? Any guesses why they're so few? Why does it drop off at100 amino acid? STUDENT: There are more but we can't find them? GEORGE CHURCH: Right, there probably are more. And why did the annotators chose not to. they choose not to? They just agreed that they would stop at 100. That was getting too short. And this is what kind of illustrates why. Here, every genome has its own GC content, its own codon usage, and so forth. Here,. we're just talking about just the first order of the genome. That's what we're talking about here. We're not talking about the whole genome. We are talking about a very small part of it, and that's what makes it so special. percentage of GC versus AT. And the genetic code, theoretically and as observed, can restrict genomes so that they really can have a minimum of 25% GC content, or 28%, and a maximum of 75%. And essentially all genomes fall in that range, and yeast is around 39% or so, he says. "It's a very, very complex problem," he says, "but we think we have a pretty good idea of what it means to be GC-rich," he adds. Stop codons tend to be made up of As and Ts. The stop codons are TAG, TGA, and TAA. So if you have an AT-rich genome, you're going to tend to have a lot. You tend to run into a stop codon at random quite quite often, says Dr. David Weinberger, a genome expert at the University of California, Los Angeles, and author of the book "Genome: The Inside Story of a Genome," published by Oxford University Press. frequently. So if you have a long open reading frame in an AT-rich genome, that's very-- if you've got a modestly long reading frame, that is very significant. If you have GC-rich genomes, then you can go for a long time at random without running into the reading frame. But if you don't have a GC- rich genome, you can't do that for a very long time. That's a very significant, AT-Rich genome. stop codon, so it's less significant. So you need to have more codons in a row in a CG-rich genome in order to convince yourself. So it's usually somewhere in between. And you can see that there's this general trend. You need toHave more codon in a Row to convince Yourself. So It's Usually Somewhere in Between. And You can see That There's This General trend. So You Need to Have More codons In A Row to Convince Yourself. convince yourself as the GC content on the horizontal axis goes higher. And basically, the place where you start getting too many false positives is around 100 amino acids. And so that's why the community just decided to cut off there. When we get to proteomics, we'll talk about ways to improve the GC data in the future. And we'll also talk about how we can use the data to improve our understanding of the human body in a more efficient way. Back to the page you came from. that you can empirically, by mass spectrometry and so forth, find those small proteins. And genetically, of course, you can find them. Let's talk about the most extremely small ones, and ask whether these extremely small open reading frames are interesting. And I think they're very extreme examples are very very interesting, and I think that's a very good thing to look at, because they're so small. That's what makes them very interesting to study, and that's what we're trying to do. interesting. So the smallest that I know of is a pentapeptide, which is actually encoded in not just one, but many different phylogenetically diverse, large ribosomal RNAs. So here, Ribosomal RNA normally acts as part of the translation apparatus, but here, it is acting as the messenger RNA, as well as the RNA itself. It's a very unique situation. It is a very exciting time for the field of molecular biology, and I'm looking forward to finding out what the future holds. presumably a separate molecule, maybe possibly a degraded version of it. But in some way or another, the 23 sRNA encodes this pentapeptide, which is not just some junk. But this one actually confers erythromycin resistance at low levels in low levels of the body. It's not just junk DNA, you can have junk peptides. It is not junk peptide, it is a real molecule. It can be found in the human body. wild type. It is not a mutant kind of peptide. It's the normal pentapeptide. Now, here's three examples that are related to one another. They have somewhere between 14 and 16 codons, and they have this very strange amino acid composition. When you do the translation conceptually in the computer, they look very strange to the human eye. They look like they have a very different type of amino acid. It looks like they're from a different time and place. Phenylalanines and histidines are two of the rare amino acids found in the human body. The next gene down is a histidine biosynthetic gene, and about eight histidine genes in a row come after that. This weird excess of tryptophan is upstream of trypticophan biosynthetics, and the same thing with phenylalanine biosynthesized genes, and this weird excess is also upstream of histidine bios syntheticals, and so on. This is an excellent feedback loop. You want to do feedback in the most relevant way. So here, if you want to know whether you need more data, you need to look at this data. This is a very good way to get the data you need. It's a great way to learn more about how to use the data in a more efficient way. It can be a very useful tool for a lot of different things, and it's a very interesting tool to use in your own life. to make tryptophan, phenylalanine, or histidine, you ask whether there's enough of it around to do translation. That's very relevant. And so this has to be sensing the translation process itself. It's asking whether the transfer RNAs are charged up with amino acids enough that you're getting efficient translation. If they're not charged up enough, you're not getting efficient transfer. And that's what we're trying to find out here in the lab. you're not, then you'll pause here. That ribosome will hesitate, waiting for the right transfer RNA. And as it hesitates, this RNA changes. It's folding. And a series of events results in-- if it's hesitating, then it wants to make the biosynthetic genes downstream to make more of the amino acids. That's what happens when you have a protein that's in short supply. That protein is called an amino acid, and it's used to make amino acids in the body. acids. So the tRNA has to be charged up. So you get this nice, little feedback loop that the hesitation causes a change in RNA, which causes change of transcription, and you make more of what you need. So I think these are interesting examples. And of course, if you want to make your own, you have to go to the lab and do it yourself. You have to learn how to do it, and how to get the right reaction in the right place. knew in advance you were looking for runs of histidines, that would be great. But for other open reading frames, there may be a different story. And so you need to have methods for looking for very short motifs. So let's go back to the bigger question of motifs, and let's talk about how to look for motifs in a story. We'll be back in a week or so to talk about the next chapter of our book, The Art of Storytelling. ask, how do we deal with them more rigorously? And the way we dealWith these profiles, we're going to take a multisequence alignment. You now know how to do multiseqence alignments. And we now want to capture that. We want to take that knowledge and use it to create these profiles. It's a new way of looking at how to deal with these profiles that we've been looking at for a long time. We're now going to use that knowledge to create the profiles. information, and deal with these position-specific profiles. Remember I mentioned the PSI-BLAST and other algorithms. You acknowledge that you don't have a generic substitution matrix for all positions in all proteins or all nucleic acids. Because one position might be, say, the position of the C-terminal, you have to deal with that. You have a different substitution matrix. for every position. Because of this, you can't use the same matrix for every part of the genome. an alpha helix. We have one substitution matrix. And another one might be in a coil. So here, this is all about what motifs are all about. Each position has a different set of rules. So the first position in this tetranucleotide-- it doesn't care what it is. It can be a coil or a helix or a protein. It doesn't matter. It just has to be in the same place. And it can be in any position. be A, C, G, or T. These are four different sequences, real start sites, that we've aligned, either manually or by computer. This is dead easy to do the alignment, but the interpretation here is the position upstream of the start codon doesn't matter. So your matrix down below is -- be A,C,G,T, or A,B,C or G, which is the real start site of the alphabet. It's a bit more complicated than that, but it's a good start. A, C, G and T each get a 1, which is a count. The T and the G position at the 3-prime end of the list is at the top of the page. We could do it in terms of frequencies, percentages, but we're doing it in counts here because that's just a restatement of the data. It's not about percentages, it's just about the data, and it's about the way the data is presented to the public. It doesn't matter what the number is, it just matters that it's there. codons are, in this small sample, invariant. And so they get a count of 4 for the correct base and count of 0 for all the alternatives, A, C, and G, for example, instead of T. And the A position is not quite invariant in this sample. GTG is a GTG, not a G, and so it's not quite an A position, but it is a G position, so it counts as a G instead of a T. perfectly good start codon in, say, 1 sequence in 10 or 1 in 4 in this case. And so you get 3 and a 1. So this is the weight matrix or position-sensitive substitution matrix. This is more precise than, say,. say, a consensus sequence or a single sequence from the. single sequence in the sequence above. It's a very precise way to get a good startcodon in a series of sequences. It can be used to solve complex problems, for example, in computer programming. We've lost the dependencies of adjacent bases, or bases that are a few bases away. But let's see how this plays out, this position is position sensitive. But it's not the most precise way of representing this. It's position sensitive, but we have lost the higher order correlations between positions. This position is not the best way to represent this, but it's a starting point for the rest of the article. For more information on this article, or to download a copy of the code, go to: http://www.cnn.com/2013/01/26/science/science-and-technology/science_and_technology/index.html#storylink=c. sensitive. This is another way of representing is in terms-- is an information theory version of this, where the full height of each of the bases is 2 bits. And that's the same 2 bits we talked about in the first lecture, since there are four bases. And this is a way of showing how information theory works in terms of the number of bits that can be stored in a bit. And it's a way to represent information theory in a way that makes sense to people. the same motif, ATG. The T and G were invariant in this larger sample, or nearly invariant. And again, A and G are the predominant ones. You can see a little bit of a T in the sample. The sample size of now instead of just 4, but more than 1,000 sequences. The same motif was the predominant one in the larger sample. It was the same motif in the bigger sample, but it was not the same in the smaller sample. there in the first position. And then the base just upstream from the ATG is almost completely random. And so its information content is close to nil, and so it's 0 bits. Now, this is easy enough that you can just do a big search aligning on theATG, which is in the second position. But it's not as easy as you might think. It's a lot of work to find the right position to align with. It takes a long time. is a very striking thing, and look to see if there's any other residual information content to the side. And sure enough, you find this little blip of Gs and As, mostly, at minus 9 relative to the A of ATG at 0. And it turns out that-- again, experimentally -- it's a very interesting thing. It's a little bit of a surprise, but it's not a big one. It doesn't mean that it's bad. It just means that it doesn't make sense. verified-- this motif-- so the ATG motif binds to transfer RNA, and the GA-rich motif actually binds to a ribosomal RNA sequence. And so then basically, the messenger RNA is coaxed into the right position, to be in the right place of the ribosomes where the tRNA can bind the tRNAs. The tRNA is then coaxed to bind the transfer RNA and the messengerRNA to the transferRNA. And then it's able to transfer the RNA to the Transfer RNA. Initiator.initiator: Here's an example where you can do a multisequence alignment. Here are 1,000 of sequences. k equals 1055-- remember, this is exponential of k. And you can find these motifs that have great biological significance. Now, once you've done multiseqence alignment and you've derived the weight matrix, this will give you a weight matrix that you can use to work out the weight of a protein or a gene. position-sensitive substitution matrix, now you want to be able to search the genome for these things. You know what a start motif looks like. You want to find them all. And it wouldn't just be the ATG, it would be this full, including the GA-rich motif. And the way you search for them is the same way you would search for a human genome. It would be like a search engine for the human genome, but it would work on any genome. do that is now take this weight matrix, and ask for each-- we're scanning the genome, and we run into the sequence [? AAT ?] AATG. Now you want to know, how good a match is that to this weight Matrix, which was taken from either 4 sequences or 1,000 sequences. The weight matrix was either taken from 4 sequences, or 1:000 sequences, and asked for each of those sequences to be compared to the weight matrix. sequences? And the way you do it is for each position, you ask what was the score in the whole learning set? And now this should be a now independent test set you're trying this out on. Here, the learning set and the test site are the same. But you don't have to use the same test site. You can use your own learning set to test your knowledge of a subject. For example, if you're a teacher, you can test your students' knowledge of your teaching style by using your own teaching style. basically have the A is a score of 1, which is not going to be a big contribution because they were all the same. So then the second A isA score of 3, and the T and G are a Score of 4. For a total score of 12, the A, T, G and T are a total of 12. The A, G, and T is the total number of points that a person can make in a game. That is a total contribution of 12 points. for this particular tetranucleotide instance of this motif represented by this weight matrix. And then you can see that the top three sequences, which all have ATG, have the best scores. And the bottom one, GTG, even though it's a valid member of the learning set, it was something which had to be removed from the set. It was something that was not part of our learning set at the time, and we had to get rid of it as a result. was underrepresented statistically. GTG tended to be less frequently encountered than ATG, and so it gets a lower score when you search the genome for it. So if you prioritize these, they would be prioritized in this order by 12, 12,12, 10. So now the final topic, which talks about GTG, is "GTG vs. ATG: What's the difference?" It's a debate that has been raging for some time, but is finally coming to an end. about a very simple and short motif, which is the CG motif. The CG motif is over-represented in promoters in vertebrates. But before we talk about these very short motifs, let's talk about why we have probabilistic models in sequence analysis in general. And there are three main uses. One is to predict how a gene will behave in the future. The other two are to predict gene activity in the distant future. And the third is to help us understand the evolution of genes. of them is recognition-- for example, the recognition that we've been doing is, is a particular sequence of protein start? In other words, does it have a score which is statistically significant? That's basically what we were doing, very anecdotally, in the previous slides. Or another task is discrimination. We are trying to find out how people discriminate between different types of proteins. That's what we're trying to do in the next few weeks, and we'll share the results with you. ask questions like, is this protein more like a hemoglobin or like a myoglobin? The first question is about one sequence relative to, say, a weight matrix. The Other one is about two sequences, asking how-- or three sequences-- whether a particular protein is more like one than another. And finally, ask questions about whether a protein is like a certain gene or a gene that's related to a gene or gene that is not related to that gene, for example, or if a gene is related to two genes that are not related. in a database search, we would go through. A question might be like, what are all sequences in [INAUDIBLE] that look like a serine protease? This would be asking for recognition multiple times, over and over. So here is the basic idea-- which will be a Bayesian idea soon, in a Bayesesian idea. In the future, we'll be able to use this idea to find out more about the origin of proteins, and how they are created. the next slide-- is assign a number to every possible sequence such that the probability of that sequence given a model. So the model might be this weight of s/m, s bar m-- is the probability that you would get that sequenceGiven a model, the number is called the P of the sequence. The P of a sequence is a number such that it is similar to the number of possible sequences that you could get from a given model. The number is known as the P, or the probability, of getting that sequence. matrix we've been talking about or it could be something more complicated. So what's the probability that we get the sequence ATG, given the model, the full weight matrix model? And as with any good probability, as we mentioned in the first class, they should sum to 1. If you don't know what ATG is, go to the Wikipedia page for ATG and look up the word "ATG" in the search engine. You can also find out more about ATG on the Wikipedia site. sum up the sigmas of s, you sum over all sequences, then the probability given the models should sum to 1. We can also have the probability of a sequence in your population of sequences irrespective of model. And here's a very useful theorem, called Bayes' theorem. It doesn't depend on models and sequences. You could just call it 'Bayes' Theorem' It is completely general and doesn't need to be dependent on models or sequences. It can be applied to any collection of models. The probability that the model given the sequence is. equal to the probability of the model times the probability. of the sequence given the model, is called a posterior probability. These are probabilities which are not conditional. They do not depend on something else. Now let's see what all this Bayesian stuff is useful for. We're going to be doing-- of the various applications, we had recognition discrimination and database search. So here's the first example of a Bayesian system. It's called Bayes' system. example of a database search. We'll have two models, a model that we actually have a hydrolase and the model thatWe have randomness. So we call this the null model or n model, and m is the model we're interested in, they're hydrolases. We have random bases, so we have a null base and a randomness base, and we search for the null base. We search for n, which is the null, and for m, the randomness, and so on. This is hydrolase and amino acids. So we want to report all the sequences where the probability that that sequence, given the model, is better than that sequence given a null model or random amino acids, that that is significant, and it's significant by the delta.or random amino acid. This is Hydrolase & Amino Acids, a study by the University of California, San Diego. The study was published in the Journal of Applied Physiology and Biomolecular Sciences. It was published by the journal in the spring of 2013. between just the null versus the probability of the model in general. So if we look, if we, let's say, do a database search where we have scoring metrics just as the ones we developed earlier in the talk, and we score for random sequences, we'll get one distribution in the database. We'll get that distribution in a search of random sequences. That distribution will be the same as the one we got in the earlier part of the talk. We can then use that distribution to get a more accurate estimate of the likelihood of a certain outcome. orange. And if we score for fide hydrolases, we might get this distribution in blue. And we're asking whether the probability of getting a particular sequence given the model this is a hydrolase is better than probability of get that sequence at random, the orange. And you want that to be the case for your hydrol enzyme. For example, if you're looking for an enzyme, you might want to look for a sequence that's more likely to be in the blue area. be statistically significant. So you can rephrase this in terms of bits, or in terms the significance level of probability of 5%, which is typically the case. Now, when we're talking about the probability of a particular sequence, where we can have deviations from randomness at the mononucleotide level, at least 5% is a high level of significance. That's what we're looking at here. We're not looking at the whole sequence. We are looking at a very small part of it. the dinucleotide level, and so on, and rather than just dump this on you as a mathematical fact, I want to give you some biological rationale for why you can have nonrandomness at every order of a Markov chain, meaning every length of sequence. So the first-order chain, the lowest-orderChain, and the din nucleotide level. The first order chain is the most random. The lowest order is the least-random. The most random is the highest order. chain, would be mononucleotides. In organisms that lack a uracil glycosylase, which would then return it back to a C, Cs will change into Us because it's a very common chemical reaction. And you might have a bias where C would be rare because the Cs mutate into Us. And in organisms that lacked a urACil glycolysis, C would mutate back into a C because it would be a rare reaction. It's called cytosine deamination. But a deoxy U is an abnormal base. There's repair in most organisms that [INAUDIBLE],, but there's some that don't. And there's a tendency of those genomes to aim towards high content. The Cs disappear, and hence, take the Us with them. The U is the abnormal base, and the Cs are the normal ones. It's a sign that something is wrong with the DNA. It can't be fixed, but it can be repaired. A T near a T in the presence of ultraviolet light will get mutated to something else. And if you can't repair that back to a T-T sequence, it gets repaired to something other or it gets mutated. And many organisms repair-- well a T near. a T will get mutation to a different T or a new T. or a different G.Gs with them. And so on and so on. It's a very common way to repair T-Gs, and it's very common in nature. so you'll lose that particular dinucleotide out of the 16 possible dinucleotides. CG is rare. This is methylated for various regulatory reasons. And now, because it's methylated, even if you have uracil glycosylase, which would then take all the regular Cs, you won't be able to get CG back. It's rare. And the reason is that this is methylation for various Regulatory reasons. It is rare, and the reason for it is that it is methylating for variousregulatory reasons. that turn into Us, deoxy Us, and turn them back into deoxy Cs, now a 5-methyl C turns into a T, and you can't tell that it's abnormal. T is a perfectly reasonable thing to get. And so every place you've got a methyl CGturns into a TG, and so every time you have a CG that turns into an TG, you get a TG. And you can get a CG with a TG by turning a CG into a CG. you tend to lose the CGs, unless they're not methylated. And similarly, you can have rare codons. And hence, these turn into rare triplets. You can also have rare tetranucleotides if you, for example, have a methylase, the methylase is a pentanucleotide, and every time you do that, you get rare tetra-tetra- Tetra-Tetra. And we'll get to that in a minute. see that-- every time the bacteria sees this related CTAG-G sequence, that says, oh, that must have been one of these methylation deamination problems. Let's fix it up. And the CTAG tends to be underrepresented as a consequence. Similarly, very long stretches of As-- not As -- not CTAGs or CTAGGs, but pentanucleotides. And that's what we're trying to get rid of with this new method of methylation. just tetranucleotides, but you can get excesses of As due to the fact that messenger RNAs end in polyA. They get reverse transcribed, reinserted into the genome, and now you've got a polyA track. Or you canget polymers in general by polymerase slippage. So all these things can cause excesses in the body, and they can cause problems in the brain, as well as in the liver and the heart. Biases. And I just elaborated on one of them here, which is the triplet bias, documented here that this 10 times lower frequency of ATG than of some of the other arginine codons. So now let's talk about a Markov model. This is not a hidden Markov models yet. In In the book, Dr. David Wheeler explains how he developed the ATG gene. In the movie, he explains how the gene came to be known as ATG. in the film, In The Garden of Earthly Delights, he talks about the discovery of the first ATG code. Just a moment, it will be. It's a Markov model because we're asking what is-- the columns that we had kept independent when we were making profiles or weight matrices, we said the two nucleotides, whether CG or AA or whatever, were independent. Now, we're no longer going to make that decision. We're going to have to change the way we think about the nucleotide. We'll have to make a new way of thinking about it. them independent. We will allow them to recognize the co-dependence. Forget the pluses right now. Just assume they'll be explained when we get to the hidden part of this. So they're hidden for now. But what we're talking about is, what's the probability of getting an A given an A? The answer is that it's not very high, but it's better than a B or a C. It's better to get an A than a C or a D. We've got an A in the first, in the 5-prime position. What's the probability now of getting an A dependent on that one? So we're recognizing that dependence. We've said that CGs are underrepresented in the genome as a whole, and they're over-represented in promoters. So this particular transition of CGs to A is a good example of how we're changing the way we think about CGs. It's a step in the right direction, but it's still a long way off. what's the probability of getting a G given a C in the 5-prime position-- this is one of those conditional probabilities. This is a Bayesian that we had set up a couple of slides back. And so this particular arrow going from a C to a G is represented by a G. The probability of a G being given to a C is 1 in 5,000, or 1 in 10,000 for a C-G ratio of 1 to 1. That's a probability of 1 in 1,000. this probability. And you can see going the other way is a different probability. That would be p of C given G. And these little arrows will refer to itself, is example of a p of an A given an A. So this is an AA dinucleotide. and you can find out more about the AA din nucleotide by going to the AA website or the AA Facebook page. For more information on the AA Dinucleotide, go to: http://www.aas.org.uk/. see there's 16 possible transitions, including four homopolymers, AA, TT, CC, GG, and 12 transitions of the other dinucleotides. Now, what do we mean by hidden? We've got CG islands where the CGs have been protected from methylation, and hence, protected from mutations. So they're fairly abundant. They're involved in the creation of new proteins. They are involved in new proteins that can be used to make new proteins, such as DNA. regulation in binding transcription factors. And these islands will be a variable length and just have an increased concentration of CGs. And then outside are the ocean, which are not protected. They're not involved in transcription, and they mutate. And they are very low in CGs, and you want to protect them from the ocean. And you want them to have CGs that are high enough to prevent them from mutating. And so you want the ocean to be protected from them. know where the island begins and ends because that helps you know where the regulatory factors are. So now, the hidden part is when you look at a new sequence, you won't know whether you're in an island or not. And so this Markov model that you have has to be different for whether it's an island. So in that case, you expect the CGs to be high, roughly the same. But you don't know what you are in. So here is thehidden part. as the other dinucleotides, possibly higher. And in the oceans where they're lost, you expect the CG, this particular transition from C to G, to be low, and most of the other transitions to be normal. So there's 16 different din nucleotides in islands on the left. And there are 16 in oceans on the right. In addition, there's a whole set of transitions between islands and oceans. The genome is not just blocks. They're all connected. And so you can make a transition from any nucleotide in an island to any nucleophile in an ocean. an ocean. So that would be a transition point going 5-prime to 3-prime. And so here's one that's illustrated, this dotted, brown line, where it says probability of a C minus-- meaning in an ocean-- given that you have an A plus--meaning in an island-- in the 5- prime position. And that's the probability of being in a 5-Prime position. That's the transition point to being in 3-Prime. It's called the "5-prime transition point" from an island into an ocean, going from an A to a C. Aren't you glad that I picked a dinucleotide to illustrate this? OK, here's a real example. Here's an example where I've cut and pasted a very short sequence with only one ocean on the left, and one on the right. The result is a sequence that goes from an island to an ocean and back again. The difference is that the island goes from a C to an A and the ocean to a B. island on the right, in bold and capital letters. You're given this as a learning set. Somebody has, by hand, decided that the boundary occurs at this first CG dinucleotide. There are no CGs to the left, and there are three CG's to the right. And so when you make the island, you're making the island on theright, in Bold and Capital letters. The island is the first CG in the learning set, and it's the first place you learn how to make it. this table-- we'll call this an A table later on-- this A table has the transition from an A in the 5-prime position to an A on the 3-prime. So that's the p A given A. And here's the CG dinucleotide, C to G transition, all in an island. And this is the p C given CG din nucleotide, G to C transition. And these are the p G given CGdinucleotide and G to CGDynucleotide. indicated by plus. And you can see that's quite frequent. And then below it, let's look at the same CG dinucleotide going from C to G in an ocean. And here it's unobserved in this little toy example that I gave you, so it's a 0.43% in this example. So 43% of the time in this case, it's not seen. And it's quite rare. It's a very, very rare event. But it happens a lot. actual example-- and you can work the numbers out because it's all here-- and there's only one transition between islands and oceans, and that happens to be a CC. And that gives us 0.2. And all the rest of the numbers are the same as what you see in the chart below. So, for example, a C in an ocean going to a A in an island would be a C-C-C. That's a CC in the ocean and a A-C in the island. That gives us a CC-A-A. are 0s. Now, 0s are a problem, both for the CG dinucleotide in the ocean and for the transitions between oceans and islands. And the way you handle it is called pseudocounts. You basically say, what if we just missed finding that thing? We're going to add 1 to it. And that's how you get rid of the 0s in the data. It's a way of getting rid of 0s, and it's a good way to do it. because however big the counts are, you can always add 1, and that would give you some feeling for the-- you don't really have 0s there. You can't trust 0s. And there's even a more rigorous way of doing it called Dirichlet, where you can do these pseudocounts. and so on. And so on, and so and so. It's a very rigorous way to do it, and it's a lot of fun to do. you can see. You can actually calculate these conditional probabilities by hand in the privacy of your home, not while the hordes are waiting to get into the room. And you can recreate these numbers with that simple formula there. Now this, is a real training set based on 48, and you can do the same thing with your own data. It's a very simple formula, but it's a lot of work to get the results you want. But it's worth it in the end, because you'll be able to see the results. known islands, again annotated by some person. And you can see those that this A matrix, focusing on those things that were 43 and 0 before, now more realistic numbers are 27% and 8% for an island and an ocean, respectively. Now we're going to plug these numbers-- basically, I've basically been trying to figure out how much of the world's landmass is made up of islands and oceans. And the answer is that it's a lot of landmass. cut off the transition tables, which are off to the right. Now let's use them to actually do an HMM. In the Viterbi algorithm-- remember we said dynamic program is a hero, and we're going to end on this. The recursion we have here, the Vitibbi score for-- so l l ll l l. l l l l  l l . l l  l. L l l 1 l l 2 l l 3 l l and k are the states. There are two states, island plus, ocean minus. And i is the sequence. Here, the sequence length is 4. i goes from 1 to 4. And the sequence we're testing is, is CGCG in an ocean or an island? What's your guess? That's a pretty good guess. What do you think? Share your thoughts in the comments below or tweet us @CNNOpinion. Back to the page you came from. extreme case. But this is actually using the numbers from the previous slide, which were taken from real oceans and islands. And so you start out with the probabilities being just equally probable that you can start at the C. extreme case. So there are eight different states, and so we just use the probabilities from the earlier slide to do the work for us. We will show you the results in the next few days as we go through the rest of the world's oceans and island groups. divide 1 over 8 is a starting point, or 0.125. And so there are two possible places it can be, and they're equally probable. It's in an ocean or island, just given the C, 1/8. Now you make a transition where you multiply this times the A matrix, A sub, and A sub again. It can be anywhere in the world, but it's most likely to be in the ocean, or an island, or in the middle of an ocean. There is a 0.27 for going to a CG dinucleotide. So the recursion here is you multiply this-- is an emission, which is always 1.k l. So you're going from state 1 to state 1, from an island to an island. And if you look back one slide, you remember there is a0.27 to go from 1 to 2. So, you multiply 1 by 2, and you get 1. k l, so you get state 1. 1. You multiply the maximum of the previous Viterbi, so i plus 1 and i, times the A matrix, which in the previous slide is .27. So the previous one was 1/8, and then times 0.27, you get 0.034. And if you started in an ocean and stayed in an Ocean, you would have stayed in the ocean for an hour and a half. You would have been in the same place for that amount of time. ocean, it would already drop to 0.01. So you can see the better probability is already that you're in an island. And if you carry this all the way out to all four tetranucleotides, you get a much higher probability of being in the island, of 0.032, than being in. the ocean. If you were in the ocean, you would have a better chance of being on an island of at least 0.1%. You would also have a higher chance of not being on a island. the ocean, 0.002. Question. Do you know the basis for thinking that the context for a dinucleotide is either an ocean or an island, in other words, only two states? Why couldn't the context be five states? GEORGE CHURCH: OK. The context is either the ocean or the island. The ocean is 0.02. The island is a state. The state is a city. The city is a town. The town is a village. The village is a suburb. The county is a county.