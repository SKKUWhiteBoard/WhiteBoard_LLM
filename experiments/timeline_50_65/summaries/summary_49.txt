Today we're going to do a fun problem that will test your knowledge of the law of total variance. And in the process, we'll also get more practice dealing with joint PDFs and computing conditional expectations and conditional variances. So in this problem, we are given a set of conditions and we have to work out how to apply them to the real world. Hey guys, welcome back to the Daily Discussion. This week's Daily Discussion will focus on the problem of the Law of Total Variability. a joint PDF for x and y. So we're told that x and. y can take on the following values in the shape of this parallelogram, which I've drawn. So the joint PDF is just flat over thisparallelogram. And then we are asked to compute the variance of x plus y. You can think of xplus y as a new random variable. And because the parallelograms has an area of 1, the height of the PDF must also be 1 so that the PDF integrates to 1. whose variance we want to compute. And moreover, we're told we should compute this variance by using something called the law of total variance. So from lecture, you should remember or you should recall that the law can be written in these two ways. And the reason is that it can be used to compute a number of different things, such as the number of players in a team, for example, or how many players there are in the same team at one time. And so on and so on. why there's two different forms for this case is because the formula always has you conditioning on something. And for this problem, the logical choice you have for what to condition on is x or y. So again, we have again the same problem, but we have two different ways of dealing with it. Here we condition on x, here we Condition on y, and here Condition on x Condition on Y Condition On x Condition On y Condition On Y Conditioning On xConditioning On Y this option. And my claim is that we should condition on x. And the reason has to do with the geometry of this diagram. So notice that if you freeze an x and then you sort of vary x, the width of this parallelogram stays constant. However, if you condition on the x, then the width goes up and down. And so on and so on. And this is the reason why we need to freeze x and vary x to get the shape we want. on y and look at the width this way, you see that the width of the slices you get by conditioning vary with y. So to make our lives easier, we're going to condition on x. And I'm going to erase this bottom one, because we're not using it. So this is the bottom one. This is the top one. And this is what we'll do with this one. We'll condition it on y. And we'll put it on x, and then we'll condition on y, and so on. And that's the way it works. this really can seem quite intimidating, because we have nested variances and expectations going on, but we'll just take it slowly step by step. So first, I want to focus on this term-- the conditional expectation of x plus y conditioned on x. So coming back over to this picture, we'll see that we have a set of expectations that are conditional on each other, and we can use this to work out how to work with the other side of the equation. We'll see this in the next section of the article. if you fix an arbitrary x in the interval, 0 to 1, we're restricting ourselves to this universe. So y can only vary between this point and this point. And the formula for this line is given by y is equal to x plus 1. But we actually know more than that. We know that in the unconditional universe, x and y are equal to 0 and 1. And we know that when we condition on x, we know y varies between x and xplus1. y were uniformly distributed. So it follows that in the conditional universe, y should also be uniformly distributed, because conditioning doesn't change the relative frequency of outcomes. So that reasoning means that we can draw the conditional PDF of y conditioned on x as this. We said it varies between y and x, but in reality it varies from x to y. So we can say that y is uniformly distributed in the universe, and that x and y are conditioned on each other. We can then draw a conditional PDF that looks like this. x and x plus 1. And we also said that it's uniform, which means that it must have a height of 1. So this is py given x, y given x. Now, you might be concerned, because, well, we're trying to compute the expectation of x plus y and this. But we're not worried about that. We just want you to know that we're looking at the expected height of x and y, which is 1.x and 1.1. is the conditional PDF of y, not of the random variable, x plus y. But I claim that we're OK, this is still useful, because if we're conditioning on x, this x just acts as a constant. It's not really going to change anything except shift the expectation of y by an amount of x. So what I'm saying in math terms is that this is actually just x plus the expectations of y given x. And now our conditional PDF comes into play. expectation acts like center of mass, we know that the expectation should be the midpoint, right? And so to compute this point, we simply take the average of the endpoints, x plus 1 plus x over 2, which gives us 2x plus 1 over 2. So plugging this back up, we can see that we have a point at which the expectation is 2x + 1 + 2. We can also see that this point is at the center of the equation. here, we get 2x/2 plus 2x plus 1 over 2, which is 4x plus1 over 2. OK. So now I want to look at the next term, the next inner term, and that is this guy. So this computation is going to be very similar to the one we just did. We'll see what happens when we add this guy to the previous one. And then we'll see if we can get the same result with the new term. We can do it by adding 2x+1/2 to the first term. nature, actually. So we already discussed that the joint-- sorry, not the joint, the conditional PDF of y given x is this guy. So the variance of x plus y conditioned on x, we sort of have a similar phenomenon occurring. x now in this conditional world just acts like a guy, and so does y in the conditional world. So that's what we're dealing with here, I think, in terms of the variance in x and y. It's a bit of a funny thing to see. a constant that shifts the PDF but doesn't change the width of the distribution at all. So this is actually just equal to the variance of y given x, because constants don't affect the variance. And now we can look at this conditional PDF to figure out what this is. We can see that this is a constant that doesn't shift the PDF at all, but shifts the size of the PDF. And we can use this to see that the PDF is the same size regardless of the constant. We have a formula for computing the variance of a random variable when it's uniformly distributed between two endpoints. So say we have aRandom variable whose PDF looks something like this. We can compute its variance by multiplying it by the PDF. The formula is called the "variance formula" and it's used to calculate the random variable's variance. The variance formula is based on the fact that the variable's PDF is uniformly distributed. It's called a "random variable" and its PDF is "uniformly distributed between endpoints" This is pww.this. We have a formula that says variance of w is equal to b minus a squared over 12. So we can apply that formula over here. b is x plus 1, a is x. So b plus 1 is x, a minus 1 is a, so w is b plus a minus 12. This is w plus 12, so b minus 12 is w minus b plus 12. We can apply this formula to w to get the number of square roots of w. We know that the expectation of a constant or of a scalar is simply that scalar. So this evaluates to 1/12. And this one is not bad either. So writing this all down, we get variance of x plus y is equal to variance of this guy, 2x plus 1/2 plus expectation of 1/ 12. So we're making good progress, because we have this inner quantity and this outer quantity. So now all we need to do is take the outer variance and the outer expectation. So similar to our discussion up here, we know constants do not affect variance. You know they shift your distribution, they don't change the variance. So we can ignore the 1/2. This scaling factor of 2, however, will change the variance. But we know how to handle this already from from the previous discussion. We can ignore this scaling factor and just use the previous method to get the same result. We are still going to be using the same method for the rest of this article. We know that you can just take out this scalar scaling factor as long as we square it. So this becomes 2 squared, or 4 times the variance of x plus 1/12. And now to compute the volatility of x, we're going to use that formula again, and we'll call it 2 squared. We'll then compute the varied variance of x using that formula. And this will be the varying variety of x as a function of the scalar scale. we're going to use this picture. So here we have the joint PDF of x and y, but really we want now thePDF of x, so we can figure out what the variance is. So hopefully you remember a trick we taught you called marginalization. To get the PDF, click on the image below and then on the box to the right of the image to see the full size of the picture. To see the whole picture, click the image above and then the box below to the left. of x given a joint PDF, you simply marginalize over the values of y. If you freeze x is equal to 0, you get the probability density line over x by integrating over this interval, over y. So if you integrate over this strip, youget 1. If we freeze x at 0, we get 0, and if we freeze y at 0 we get 1, and so on. So we get a probabilitydensity line of 1 over x, and 1 over y over x. We get a line of probability density over x over y, and this line is called a probability density. move x over a little bit and you integrate over this strip, you get 1. This is the argument I was making earlier that the width of this interval stays the same. So based on that argument, which was slightly hand wavy, let's go ahead and do it. We get 1, and the variance stays the the same, so we get the same number of polygons. The result is the same as before, and we get a new number, 1, with the same width. come over here and draw it. We're claiming that the PDF of x, px of x looks like this. It's just uniformly distributed between 0 and 1. And if you buy that, then we're done, we're home free, because we can apply this formula, b minus a squared over 12, and we'll be done. We'll be able to show you how to do it in the next few minutes, if you want to try it out yourself. Back to the page you came from. gives us the variance. So b is 1, a is 0, which gives variance of x is equal to 1/12. And that is our answer. So this problem was straightforward in the sense that our solution was straightforward. So coming back over here, we get 4 times 1/11 plus 1/15, which is 5/12, and that is the answer. We can now go on to the next section of the book. The next section is about the book’s structure. task was very clear. We had to compute this, and we had to do so by using the law of total variance. But we sort of reviewed a lot of concepts along the way. We saw how, given a joint PDF, you marginalize to get the PDF of x. We were able to see how to do this in a way that made sense to the user. It was a very, very complex task, but we got it done in about an hour and a half. It might seem like cheating to memorize formulas, but there's a few important ones you should know. We got a lot of practice finding conditional distributions and computing conditional expectations and variances. And we also saw this trick.saw how constants don't change variance. And it will help you find the right formula for the right situation for you. It will also help you figure out how to get the right result for your situation. It's a good way to get a better idea of what you're looking for in a formula. sort of become faster at doing computations. And that's important, especially if you guys take the exams. So that's it. See you next time. Back to the page you came from. back to the original page. Back into the page that you came From. Back To the original pages that you Came From.Back to thepage that you Coming From. back into the originalpages that you coming from. Back in the original columns, back to The Daily Discussion. Back onto the original column, front to the front.