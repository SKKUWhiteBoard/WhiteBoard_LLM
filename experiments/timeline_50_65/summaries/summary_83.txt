Lecture eight is about deep learning software. This is a super exciting topic because it changes a lot every year. But also means it's a lot of work to give this lecture 'cause it's after 12, so I want to get started. So today, lecture eight, we're going to talk about deeplearning software. It's a very exciting topic, but it's also very hard to give a lecture about it. So I'm going to give it to you in a few minutes. Project proposals for your course projects were due on Tuesday. So hopefully you all turned that in. It changes a lot every year. But as usual, a couple administrative notes before we dive into the material. So as a reminder the project proposals foryour course projects are due onTuesday. SoHopefully you all have a somewhat good idea of what you're doing. It's going to be a busy week, but we're looking forward to it. We'll see you at the end of the week. of what kind of projects you want to work on for the class. We're in the process of assigning TA's to projects based on what the project area is and the expertise of the TA's. So we'll have some more information about that in the next couple days I say. I'm looking forward to hearing from students about what they want to do with their time this year. I hope to hear from them in a few days. I'll be in touch with you in a couple days. We're also in the process of grading assignment one, so stay tuned and we'll get those grades back to you as soon as we can. Another reminder is that assignment two has been out for a while. That's going to be due next week, a week from today, Thursday. Think. think.think. Think, think, think. Think! Think! think! Think,think. think,think, think! think. think!think! think, Think!think, Think, Think. When working on assignment two, remember to stop your Google Cloud instances when you're not working to try to preserve your credits. And another bit of confusion, I just wanted to re-emphasize is that for assignment two you really only need to use GPU instances for the last few minutes of the assignment. And again, when working on assignments two and three, you will need to stop the Google Cloud instance when you are not working so that you can continue to use the cloud for the rest of the work. For all of the several notebooks it's just in Python and Numpy so you don't need any GPUs for those questions. So again, conserve your credits, only use GPUs when you need them. And the final reminder is that the midterm is coming up. It's kind of hard to keep up with all of these questions. For all the questions, just use Python or Numpy. For the rest of the questions just use Numpy or Python. For more information on how to use these notebooks, go to www.notebook.com. The midterm will be in class on Tuesday, five nine. It'll be sort of pen and paper working through different kinds of, slightly more theoretical questions to check your understanding of the material that we've covered so far. It will be more theoretical than the actual midterm. The midterm is for students who want to understand the material they've been taught so far in the course of the course. It is not meant to be a test of knowledge, but to help students understand what they have been taught. And I think we'll probably post at least a short sort of sample of the types of questions to expect. Question? [student's words obscured due to lack of microphone] Oh yeah, question is whether it's open-book, so we're going to say closed note, closed book. So just, yeah, so just, Yeah,Yeah, so, yeah. And we'll try to answer some of the questions as soon as we can, but we'll have to wait and see. that's what we've done in the past is just closed note, closed book, relatively just like want to check that you understand the intuition behind most of the stuff we've presented. So, a quick recap as a reminder of what we were talking about last time. Last time we talked, we talked about how to get the most out of your mobile phone. This time, we're going to focus on how to use it to help you make the most of your phone. We'll talk more about that in the next few days. Small tweaks on top of vanilla SGD, are relatively easy to implement but can make your networks converge a bit faster. We also talked about regularization, especially dropout. So we saw that these relatively small tweaks are relativelyeasy to implement and can make networks converge faster. So that's what we did with SGD Momentum, Nesterov, RMSProp and Adam. And we also saw that we can use regularization to make our networks converge even faster, especially if we add dropout to our models. remember dropout, you're kind of randomly setting parts of the network to zero during the forward pass, and then you kind of marginalize out over that noise in the back at test time. And we saw that this was kind of a general pattern across many different types of regularization. We saw that it was a pattern that was seen in many different kinds of regularizations. We also saw it in other types of network regularization as well, such as in computer vision and other forms of machine learning. in deep learning, where you might add some kind of noise during training, but then marginalize out that noise at test time. We also talked about transfer learning where you can maybe download big networks that were pre-trained on some dataset and then use them in a new way to train the network. In the video below, we look at some of the techniques used in this type of deep learning. We'll be back in action next week with a look at the next step in the project. fine tune them for your own problem. And this is one way that you can attack a lot of problems in deep learning, even if you don't have a huge dataset of your own. So today we're going to shift gears a little bit and talk about some of the things that we can do to help you with your own problems. We're also going to talk about how you can use your own data to improve your deep learning skills. We'll be back next week with a look at how to use your data to help your own deep learning problems. nuts and bolts about writing software and how the hardware works. diving into a lot of details about what the software looks like that you actually use to train these things in practice. So we'll talk a little bit about CPUs and GPUs and then we'll go into more detail about the software that's used to train the computers. We'll also talk about some of the different types of software that are used in the training process, such as the training software that is used to teach the computer how to do certain tasks. talk about several of the major deep learning frameworks that are out there in use these days. Deep learning uses GPUs, but we weren't really too explicit up to this point. We've sort of mentioned this off hand a bunch of different times, that computers have CPUs, computers have GPUs. But deep learning uses GPU, and that's what we're going to focus on here. We're not going to go into too much detail about how deep learning works, but it's going to be a big part of it. this point about what exactly these things are and why one might be better than another for different tasks. Who's built a computer before? Just kind of show of hands. So, maybe about a third of you,. half of you, somewhere around that ballpark. So this is a shot about what it is and why it's important to build a computer. It's not just a computer, it's a tool. It can be used to do many different things. And it can also be used for a lot of other things. of my computer at home that I built. And you can see that there's a lot of stuff going on inside the computer, maybe, hopefully you know what most of these parts are. And the CPU is the Central Processing Unit. That's this little chip hidden under this cooling fan, and it's the heart of the computer. It's the little chip that powers all of the other parts of the system. And it's also the reason why the computer is so fast and can run at high speeds. right here near the top of the case. And the CPU is actually relatively small piece. It's not taking up a lot of space. The GPUs are these two big monster things that are taking up  a gigantic amount of space in the case, he said. He added that the CPU was a relatively small thing inside the case and that it was not a huge part of the overall design of the PC. He said that the GPU was a much larger piece of the design that was taking up most of the space. in the case. They have their own cooling, they're taking a lot of power. They're quite large. So, just in terms of how much power they're using, in Terms of how big they are, the GPUs are kind of physically imposing and taking up a lot. of space in the case," he said. "They are quite large," he added, "and they have their Own Cooling, They're Taking A lot ofpower. They are quite big, so they're physically imposing" case. So, the question is what are these things and why are they so important for deep learning? Well, the GPU is called a graphics card, or Graphics Processing Unit. And these were really developed, originally for rendering computer graphics, and especially around games and that sort of thing. So it's a very powerful tool, and it can be used for a lot of different things, including deep learning and other types of machine learning. It's a really powerful tool and it's been used in a number of different ways, including in deep learning. another show of hands, who plays video games at home sometimes, from time to time on their computer? Yeah, so again, maybe about half, good fraction. So for those of you who've played video games before and who've built your own computers, you probably have your own opinions on this. You probably have an opinion on this, too, if you've built a computer or have played a video game before. If you've played a game before, you've probably got a view on this too. debate. [laughs] So this is one of those big debates in computer science. You know, there's like Intel versus AMD, NVIDIA versus AMD for graphics cards. It's up there with Vim versus Emacs for text editor. And pretty much any gamer has their own opinions on which of these two should be used in a game. And I'm sure most of you have your own opinion on which one is better for you. I'm not going to get drawn into a debate about it. sides they prefer for their own cards. And in deep learning we kind of have mostly picked one side of this fight, and that's NVIDIA. So if you guys have AMD cards, you might be in a little bit more trouble if you want to use those for deep learning. And really, NVIDIA's been pushing a lot fordeep learning in the last several years. It's been kind of a large focus of some of their strategy. And they put in a lot effort into engineering sort of good solutions to make their hardware better suited for deeplearning. of like what is the difference between a CPU and a GPU, I've kind of made a little spread sheet here. And there's a couple general trends to notice here. Both GPUs and CPUs are kind of a general purpose computing machine where they can execute programs and do sort of arbitrary instructions, but they're qualitatively pretty different. So CPUs tend to have just a few cores, for consumer. For consumer GPUs, for example, there are a number of different types of processors that are available. desktop CPUs these days, they might have something like four or six or maybe up to 10 cores. With hyperthreading technology that means they can run, the hardware can physically run, like maybe eight or up to 20 threads concurrently. So that's just not a gigantic number, but those threads for a CPU are pretty powerful. They can actually do a lot of things, they're very fast. And they can all work pretty independently. For GPUs it's the same. For PCs it's a little bit different. a little bit different. So for GPUs we see that these sort of common top end consumer GPUs have thousands of cores. So the NVIDIA Titan XP which is the current top of the line consumer GPU has 3840 cores. That's like way more than way. more than some of the other top-end consumer GPUs. So that's a crazy number. It's a little different than what we see in other top end products. We see that many of the common top- end consumer PCs have thousands. of cores, and some of them even more than that. The downside of a GPU is that each of those cores, one, it runs at a much slower clock speed. And two they really can't do quite as much. You can't really compare CPU cores and GPU cores apples to apples. But it should give you the sense that due to the large number of cores GPUs can sort of, are really good for parallel things where you need to do a lot of things all at the same time, but those things are all pretty much the same flavor. and GPUs is this idea of memory. Right, so CPUs have some cache on the CPU, but that's relatively small and the majority of the memory for your CPU is pulling from your system memory, the RAM. Whereas GPUs actually have their own RAM built into the chip. There's a pretty large bottleneck communicating between the RAM in your system and the GPU. And for the Titan XP, which again is maybe the current top of the line consumer card, this thing has 12 gigabytes of memory local to theGPU. the actual GPU cores. And that's somewhat similar to the caching hierarchy that you might see in a CPU. So, CPUs are kind of good for general purpose processing. They can do a lot of different things. And GPUs are maybe more specialized for these highly paralyzable algorithms. So the the actual CPU cores are probably not as important as the GPU cores, but they're still very important for some of the algorithms that are being used. And so that's what we're going to focus on in this article. prototypical algorithm of something that works really really well and is like perfectly suited to a GPU is matrix multiplication. So remember in matrix multiplication on the left we've got like a matrix composed of a bunch of rows. We multiply that on the right by another Matrix composed of another matrix. That's what we're doing here. We're multiplying that by another matrix that we're multiplying by a matrix that's composed of rows and columns. And that's what's going to be happening on the GPU. a bunch of columns and then this produces another, a final matrix where each element in the output matrix is a dot product between one of the rows and one of. the columns of the two input matrices. And these dot products are all independent. Like you could imagine, for this output matrix you could split it up completely and have each of those different elements of the output Matrix all being computed in parallel and they all sort of are running the same computation. But exactly where they're reading, I don't know. that data from is from different places in the two input matrices. So you could imagine that for a GPU you can just like blast this out and have all of this elements of the output matrix all computed in parallel and that could make this thing computer super super super. That could be the future of computer graphics. It could be a big step in the direction of making computer graphics even more powerful. It's a very exciting time to be a computer graphics engineer, and I'm looking forward to seeing what the future holds. fast on GPU. So that's kind of the prototypical type of problem that like where a GPU is really well suited, where a CPU might have to go in and step through sequentially and compute each of these elements one by one. That picture is a little bit of a little more of a picture than what you would see on a CPU, but it's still a good example of what a GPU can do. It can do a lot of things that a CPU can't do. caricature because CPUs these days have multiple cores, they can do vectorized instructions as well, but still, for these like massively parallel problems GPUs tend to have much better throughput. Especially when these matrices get really really big. And by the way, convolution is kind of the same kind of a problem as convolution, so it's kind of like the same thing, but on a much larger scale. It's a very different kind of problem, but it's the same sort of thing. In convolution we have this input tensor and this weight tensor. Every point in the output tensor after a convolution is again some inner product between some part of the weights and some parts of the input. And you can imagine that a a.story would be like this: A story would be about a person who had been in a car accident and had to go to the hospital. That person would then go to a hospital and have to go through a series of tests to see if they had survived. GPU could really paralyze this computation, split it all up across the many cores and compute it very quickly. So that's kind of the general flavor of the types of problems where GPUs give you a huge speed advantage over CPUs. So you can actually write programs that run directly on top of the GPU, which is a huge advantage over a CPU. It's a very powerful tool, and it can be used in a variety of ways, including as a replacement for a CPU, or even as an alternative to a processor.  NVIDIA has this CUDA abstraction that lets you write code that kind of looks like C, but executes directly on the GPUs. It's actually really tough to write CUDA code that's performant and actually squeezes all the juice out of the GPUs, he says.on GPUs. He says it's "really really tricky" to write the right code for the right kind of hardware. "It's really really tricky," he says, "to get the best performance out of this code"  NVIDIA has released a new set of GPUs for its CUDA software. It's hard to write performant CUDA code on your own, the company says. "You have to be very careful managing the memory hierarchy and making sure you don't have cache misses and branch mispredictions and all that sort of stuff," says NVIDIA CEO Jensen Huang. "It's actually really really hard to writing performantCUDA code," he adds. "So as a result, we've released a set of new GPUs. a lot of libraries that implement common computational primitives that are very very highly optimized for GPUs. So for example NVIDIA has a cuBLAS library that implements different kinds of matrix multiplications and different matrix operations that are super optimized, run really well on GPU, get very close to sort of sort of the sort of matrix operations. That's a big part of what we're trying to do in the next generation of software. We're going to be able to do a lot more with these libraries.  NVIDIA has gone in there and released their own binaries that compute these primitives very efficiently on NVIDIA hardware. Similarly they have a cuDNN library which implements things like convolution, forward and backward passes, batch normalization, recurrent networks. So in practice, you tend not to end up writing your own CUDA code for deep learning. You typically are just mostly calling into existing code that other people have written. Much of which is the stuff which has been heavily optimized by NVIDIA already. of the main game in town for deep learning. So you can check, there's a lot of different resources for learning about how you can do GPU programming yourself. It's sort of a different paradigm of writing code because it's this massively parallel architecture, but that's kind of fun, too. "There's a bunch of different ways to do it," he says. "It's kind-of fun. ... It's a new way of thinking about programming," he adds. "I think it's going to be a big part of the future of computing" a bit beyond the scope of this course. And again, you don't really need to write your own CUDA code much in practice for deep learning. And in fact, I've never written my ownCUDA code for any research project, so, but it is kind of useful to know like like. It's kind of like knowing how to write a web app in Python or Ruby or something like that. It can be a bit of a learning experience for people who don't know how to do it. how it works and what are the basic ideas even if you're not writing it yourself. So if you want to look at kind of CPU GPU performance in practice, I did some benchmarks last summer comparing a decent Intel CPU against a bunch of different GPUs that were sort of similar to the ones in this article. I did this because I wanted to see what kind of performance I could get out of a decent CPU and a decent graphics card at the same time. So that's what I was trying to do. of near top of the line at that time. For things like VGG 16 and 19, ResNets, various ResNet, then you typically see something like a 65 to 75 times speed. And these were my own benchmarks that you can find more details on GitHub, but my findings were that for things like that you usually see that for those kinds of things. It's a very, very different set of tools than what you're used to seeing in the open source world. up when running the exact same computation on a top of the line GPU, in this case a Pascal Titan X, versus a top-of-the-line CPU, which was an Intel E5 processor. Although, I'd like to make one more comparison, this time between the two processors: the E5 vs. the Pascal Titan, and the Intel E4 vs. Titan, which were both running on an Intel i7-4770K processor. This time, the Titan X was running on the i7 4770K, while the E4-4760K was running the i3-4750K. sort of caveat here is that you always need to be super careful whenever you're reading any kind of benchmarks about deep learning. Because it's super easy to be unfair between different things. And you kind of need to know a lot of the details about what exactly is being being done. And it's kind of hard to be fair about what's being done, because it's so different. It's really hard to tell what's going on and how it's being used. And that's where the benchmarks come in. benchmarked in order to know whether or not the comparison is fair. So in this case I'll come right out and tell you that probably this comparison is a little bit unfair to CPU because I didn't spend a lot of effort trying to squeeze the maximal performance out of CPU. I'll also say that I don't think that the comparison between CPU and GPU is fair at all. I think that CPU is more important than GPU in the current state of the game, and I'm not sure that this is the case here. This was sort of out of the box performance between just installing Torch, running it on a CPU, and just installing it. I probably could have tuned the blast libraries better for the CPU performance. And Iprobably could have gotten these numbers a bit better. This was just running Torch on a PC. I could have done a better job of showing the performance of the Torch app on the CPU. I don't know if this would have made a difference, but it would have been interesting to see. This is kind of out of the box performance, but it's not really like peak, possible, theoretical throughput on the CPU. But that being said, I think there are still pretty substantial speed ups to be had here. Another kind of interesting outcome from this benchmarking was comparing the CPU to the GPU. The result was a very close race between the CPU and the GPU, with the CPU coming out on top in terms of overall performance. The results are shown in the video below. These optimized cuDNN libraries from NVIDIA for convolution and whatnot versus sort of more naive CUDA that had been hand written out in the open source community. And you can see that if you compare the same networks on the same hardware with the same deep learning framework and the only difference is swapping out theseCuDNNs. You can see something like nearly a three X speed up across the board when you switch from the relatively simple CUDA to these like super optimizedcuDNN implementations. code on GPU, you should probably almost always like just make sure you're using cuDNN because you're leaving probably a three X performance boost on the table if you're not calling into cuDnn for your stuff. So another problem that comes up in practice, when you're training these things is training these thing is how to get them to do what they're supposed to do. It's a lot of work, but it's worth it in the long run. It gives you a huge performance boost. that you know, your model is maybe sitting on the GPU, the weights of the model are in that 12 gigabytes of local storage on the. GPU, but your big dataset is sitting over on the right on a hard drive or an SSD or something like that. So if you know that your big datasets are sitting over there on the left on the hard drive, you can use that data. You can use the data on that hard drive to get the weight of that dataset on the model. you're not careful you can actually bottleneck your training by just trying to read the data off the disk. 'Cause the GPU is super fast, it can compute forward and backward quite fast, but if you're reading sequentially off a spinning disk, you can Actually bottleneck yourtraining quite, and it's a big problem for a lot of people. It's a very, very big problem, and you can't do anything about it if you don't know what you're doing. You have to be very careful about it. that can be really bad and slow you down. So some solutions here are that like you know if your dataset's really small, sometimes you might just read the whole dataset into RAM. You can also make sure you're using an SSD instead of a hard drive, that can help a lot with read throughput. Another common strategy is to use multiple threads on the CPU that are pre-fetching data off RAM or off disk, buffering buffering and data read-and-write. it in memory, in RAM so that then you can continue feeding that buffer data down to the GPU with good performance. This is a little bit painful to set up, but again like, these GPU's are so fast that if you're not really careful with trying to feed them, you're going to have a bad time. It's a bit of a pain, but it's a good way to get the most out of your graphics card. It doesn't take long to get used to it. Just reading the data can sometimes bottleneck the whole training process. So that's something to be aware of. And then I wanted to give you a brief introduction to like sort of GPU CPU hardware in practice when it comes to deep learning. It's kind of like a kind of a training tool for deep learning, but it's also a tool to help you learn more about it, too. It can be a little bit of a learning tool for you, too, because you're trying to learn as much as possible. switch gears a little bit and talk about the software side of things. The various deep learning frameworks that people are using in practice. But I guess before I move on, is there any sort of questions about CPU GPU? Yeah, question? [student's words obscured due to lack of microphone] Yes, question. [Student's words obscured due to lacked of microphone]. Yes. Question? Yes, I'm sorry, I was trying to ask a question about the CPU. I was going to ask about the GPU. Yeah, so the question is what can you sort of, what can we do mechanically when we're coding to avoid these problems? Probably the biggest thing you can do in software is set up sort of pre-fetching on the CPU. Like you couldn't like, sort of a naive thing would would be to do it in the first place. That would be like a very, very naive thing to do. That's what we're trying to avoid here. We're not trying to be naive, we're just trying to solve a problem. Bean.be has a sequential process where you first read data off disk, wait for the data, then feed the minibatch to the GPU. Then go forward and backward on the GPU, then read another minbatch and sort of do this all in. The process is called sequential read-write-read. Bean says the process is used to read data from disk to the CPU and back again, and then back to disk again. It's called sequential data read-writing. sequence. And if you actually have multiple, like instead you might have CPU threads running in the background that are fetching data off the disk such that while the, you can sort of interleave all of these things. Like the GPU is computing, the CPU background threads are feeding data. And so you can have all of this happening at the same time. It's a very powerful way to do it, I think, and I've seen a lot of people use it in the past. off disk and your main thread is kind of waiting for these things to. Just doing a bit of synchronization between these things so they're all happening in parallel. And thankfully if you're using some of these deep learning frameworks that we're about to talk about, then some of this can be done in parallel as well. We'll be talking about some of those in the next few minutes. We hope you'll join us for the second half of the show on Tuesday. We're looking forward to hearing from you. work has already been done for you 'cause it's a little bit painful. So the landscape of deep learning frameworks is super fast moving. So last year when I gave this lecture I talked mostly about Caffe, Torch, Theano and TensorFlow. And when I last gave this talk, again more more Tensor Flow. And so on and so on, until we get to the end of the talk. And then we'll talk about the next stage of the project, which is the next generation of deeplearning. than a year ago, TensorFlow was relatively new. It had not seen super widespread adoption yet at that time. But now I think in the last year Tensor Flow has gotten much more popular. It's probably the main framework of choice for many people. So that's a big change. We've also seen a big increase in the number of people using Google's machine learning software. We're seeing a big uptick in the use of Google's Machine Learning software in our products. seen a ton of new frameworks sort of popping up like mushrooms in the last year. So in particular Caffe2 and PyTorch are new frameworks from Facebook that I think are pretty interesting. Paddle, Baidu has Paddle,. Microsoft has CNTK, Amazon is mostly Paddle and CNTk. There's also a lot of other frameworks. I think it's a really exciting time for the open source community. I'm looking forward to seeing what the future holds. using MXNet and there's a ton of other frameworks as well, but I'm less familiar with, and really don't have time to get into. But one interesting thing to point out from this picture is that kind of the first generation of deep learning frameworks that really saw wide adoption were MXNet-based. That's the kind of thing that really got people talking about deep learning. It was the first time that deep learning was seen as a real tool that could be used to solve real-world problems. were built in academia. Caffe was from Berkeley, Torch was developed originally NYU and also in collaboration with Facebook. And Theana was mostly build at the University of Montreal. But these kind of next generation deep learning frameworks all originated in industry. So Caffe2 is from Facebook, PyTorch is from NYU and Theana is from Montreal. It's a very interesting time for deep learning, and I'm looking forward to seeing what the future holds for the field of deep learning. from Facebook. TensorFlow is from Google. So it's kind of an interesting shift that we've seen in the landscape over the last couple of years is that these ideas have really moved a lot from academia into industry. And now industry is kind of giving us these big powerful nice nice tools, says Google's Fei-Fei Huang. "It's really exciting to see how these ideas are moving from academia to industry," she says. "And it's a really exciting time to be a part of it" frameworks to work with. PyTorch and TensorFlow are probably the ones you should be focusing on for a lot of research type problems these days. I'll also talk a bit about Caffe and Caffe2. But probably most of the talk will be about Pytorch andTensorFlow. I personally think that those are the ones to focus on for most research type Problems. For more information on PyTorching, visit the PyTorCh website. For the more about Tensor Flow, visit Tensorflow.org. a little bit less emphasis on those. And before we move any farther, I thought I should make my own biases a little bit more explicit. So I have mostly, I've worked with Torch mostly for the last several years. And I've used it quite a lot, I like it very, very much. I'm a big fan of Torch, and I've been using it a lot for the past several years, so I'm very fond of it. I think it's a great tool. a lot. And then in the last year I've mostly switched to PyTorch as my main research framework. So I have a little bit less experience with some of these others, especially TensorFlow, but I'll still try to do my best to give you a fair picture and a decent picture of what's going on in the world of machine learning. I've also been using Tensorflow a lot, and I've been using it a lot in my own work as well. overview of these things. So, remember that in the last several lectures we've hammered this idea of computational graphs in sort of over and over. That whenever you're doing deep learning, you want to think about building some computational graph that computes whatever function that we want to compute. So that's what we're trying to do with deep learning. We want to build some computational graphs that compute whatever function we're looking at. We're also trying to get a sense of how deep learning works. in the case of a linear classifier you'll combine your data X and your weights W with a matrix multiply. You'll do some kind of hinge loss to maybe have, compute your loss. You will have some regularization term and you imagine stitching together all these different operations into some graph. It's a very complex process but it's a good way to think about how data is being processed and how it can be used to make sense of the data in a way that makes sense. structure. Remember that these graph structures can get pretty complex in the case of a big neural net, now there's many different layers, many different activations. Many different weights spread all around in a pretty complex graph. And as you move to things like neural turing machines then you can can get into more complex structures. For more information on how to build a neural net click here. for more information about how to create a neural nets, visit the Neural Net Project website or the NeuralNet Project blog. get these really crazy computational graphs that you can't even really draw because they're so big and messy. There's really kind of three main reasons why you might want to use one of these deep learning frameworks rather than just writing your own code. The first would be that these frameworks enable you to easily build and work with these big hairy computational graphs without kind of worrying about a lot of those bookkeeping details yourself. Another major idea is that, whenever we're working in deep learning we always need to be able to see what our data is doing. to compute gradients. We're always computing some loss, we're always computer gradient of our weight with respect to the loss. And we'd like to make this automatically computing gradient, you don't want to have to write that code yourself. You want that framework to handle all these back propagation details. We want to make it automatically compute gradient, so that we don't need to write all the code ourselves. We don't have to do all the work ourselves, we can use a framework to do it for us. for you so you can just think about writing down the forward pass of your network and have the backward pass sort of come out for free without any additional work. And finally you want all this stuff to run efficiently on GPUs so you don't have to worry too much about it. For more information on how to write a network pass, see our guide to writing a network forward pass and backward pass in the same way, and for more information about how to build a network, see the guide to building a network. much about these low level hardware details about cuBLAS and cuDNN and CUDA and moving data between the CPU and GPU memory. You kind of want all those messy details to be taken care of for you. So those are kind of some of the major reasons why you might not want to use CUDA in the first place. It's kind of a little bit more complicated than you might think, but it's still a lot of fun to use in the long-term. choose to use frameworks rather than writing your own stuff from scratch. So as kind of a concrete example of a computational graph we can maybe write down this super simple thing. Where we have three inputs, X, Y, and Z. We're going to combine X and Y to produce a graph of the kind that we want to create. That's a graph that we call a "computational graph" This is an example of how we might use a graph to make a graph. A. combine A and Z to produce B. Then we're going to do some maybe summing out operation on B to give some scaler final result C. So you've probably written enough Numpy code at this point to realize that it's super easy to create a Numpy function. Now, let's see how we can use it to create some Numpy functions in a real-life situation. Let's see what happens when we combine A, Z, B and C to produce C. write down, to implement this computational graph. You can just kind of write down in Numpy that you want to generate some random data. You want to multiply two things, you Want to add two things and so on. In Numpy, you can do all of these things in a single line of code. It's a very powerful tool. It can be used to do all kinds of computations in a very short amount of time. For more information, visit Numpy.com. sum out a couple things. And it's really easy to do this in Numpy. But then the question is like suppose that we want to compute the gradient of C with respect to X, Y, and Z. So, if you're working in. Numpy, you kind of need to write out the gradient in a different way. And that's what we're doing here. We're doing it in the form of a function called C, which is a function of C, X, and Y. this backward pass yourself. And you've gotten a lot of practice with this on the homeworks, but it can be kind of a pain and a little bit annoying and messy once you get to really big complicated things. The other problem with Numpy is that it doesn't run on the operating system it's based on, so you'll have to do it yourself. This backward pass is for students who want to learn how to use Numpy. It's also for those who don't want to use it, but want to know how to get the most from it. the GPU. So Numpy is definitely CPU only. And you're never going to be able to experience or take advantage of these GPU accelerated speedups if you're stuck working in Numpy. And it's, again, it's a pain to have to compute your own gradients in all these situations. So, kind of kind of a shame, I'm not sure what's going to happen with Numpy in the next few years. I'm sure it'll get better, but it won't be the same. of the goal of most deep learning frameworks these days is to let you write code in the forward pass that looks very similar to Numpy, but lets you run it on the GPU and lets you automatically compute gradients. So if you imagine looking at, if we look at an example in TensorFlow of the exact same computational graph, we now see that in this forward pass, you write this code that ends up looking very very similar. to the Numpy forward pass. And that's kind of the big picture goal ofmost of these frameworks. of doing these multiplication and these addition operations. But now TensorFlow has this magic line that just computes all the gradients for you. So now you don't have to go in and write your own backward pass and that's much more convenient. The other nice thing about Tensor Flow is you can do anything you want with it, and it's free and open source. So you can use it to do anything with your data, even if you're not using it in a particular way. really just, like with one line you can switch all this computation between CPU and GPU. So here, if you just add this with statement before you're doing this forward pass, you just can explicitly tell the framework, hey I want to run this code on the CPU. But now, now you can run it on the GPU as well. It's really just a very simple way to switch between the two. It doesn't have to be a lot of lines of code, it just has to be something that you want to do. if we just change that with statement a little bit with just with a one character change in this case, changing that C to a G, now the code runs on GPU. And now in this little code snippet, we've solved these two problems. We're running our code on the CPU and the GPU at the same time. It's a very simple code snippet. It doesn't take a lot of time to get it working. It just takes a few lines of code to get the code to work. GPU and we're having the framework compute all the gradients for us, so that's really nice. And PyTorch kind looks almost exactly the same. So again, inPyTorch you kind of write down, you define some variables, you have some forward pass and the forward pass again looks very similar. And we're doing the same thing here. So it's kind of like a very similar thing. It's very similar to the way we do it in our code. to like, in this case identical to the Numpy code. And then again, you can just use PyTorch to compute gradients, all your gradients with just one line. And now in PyTorCh again, it's really easy to switch to GPU, you just need to cast all your stuff to the GPU. You can use the same code as before, but now you can also use it to compute all gradients in the same way. It's just as easy to use as Numpy to compute Numpy.  CUDA data type before you rung your computation and now everything runs transparently on the GPU for you. So if you kind of just look at these three examples, these three snippets of code side by side, the Numpy, the TensorFlow and the PyTorch. You see that the TensorsFlow and Pytorch both work on the same CUDA type. The Numpy data type is used as the data type for the code that runs on the CUDA. the PyTorch code in the forward pass looks almost exactly like Numpy which is great 'cause Numpy has a beautiful API, it's really easy to work with. But we can compute gradients automatically and we can run the GPU automatically. So after that kind of introduction, I wanted to dive into the code a little bit more and see what we could do with it. The result is a very powerful tool that can be used to make complex graphics more accessible to the user. In the rest of the lecture, I'm going to use the training a two-layer fully connected ReLU network on random data as kind of a running example. I'll talk in a little bit more detail about kind of what's going on inside this TensorFlow example. In the next lecture, we'll look at the training of a neural network on a set of images. We'll also look at some of the features that are used to train the neural network in the first place. example throughout the rest of the examples here. And we're going to train this thing with an L2 Euclidean loss on random data. So this is kind of a silly network, it's not really doing anything useful, but it does give you the. It's relatively small, self contained, the code is relatively small and self contained. The code is self contained and it's relatively. small, the network is relatively. large and self-contained. The network is not very useful, it does not do anything useful. fits on the slide without being too small, and it lets you demonstrate kind of a lot of the useful ideas inside these frameworks. So here on the right, oh, and then another note, I'm kind of assuming that Numpy and TensorFlow have already been imported in all these code. That's what we're doing here. We're importing them into our code. And then we're using them to do some of the data analysis. So that's what's going on here. In TensorFlow you would typically divide your computation into two major stages. First, we're going to write some code that defines our computational graph, and that's this red code up in the top half. And then after you define your graph, you'regoing to run the graph over and over again and actually feed data into the graph to perform whatever computation you want it to perform. So this is the really, this is kind of the big common pattern in Tensor Flow. You'll first have a bunch of code that builds the graph and then you'll go and go and do the computation. run the graph and reuse it many many times. So if you kind of dive into the code of building the graph in this case. Up at the top you see that we're defining this X, Y, w1 and w2, and we're creating these tf.placeholder objects. So these are going to be used as placeholder objects for the rest of the graph. We're going to use them to create a graph that can be used many, many times in the future. to be input nodes to the graph. These are going to be sort of entry points to theGraph. When we run the graph, we're going to feed in data and put them in through these input slots in our computational graph. So this is not actually like allocating allocating nodes in a graph. It's a way of getting data into the graph without having to allocate nodes in the graph to get the data out of the graph and into a different part of thegraph. any memory right now. We're just sort of setting up these input slots to the graph. Then we're going to use those input slots which are now kind of like these symbolic variables. We are going to perform different TensorFlow operations on these symbolic. variables in order to set up the graph and then use them in a different way in the next stage of the algorithm. The next step is to create a graph that can be used as a graph for the next step in the algorithm, which is to use the graph as a memory. what computation we want to run on those variables. So in this case we're doing a matrix multiplication between X and w1. We're doing some tf.maximum to do a ReLU nonlinearity and then we're do another matrix multiplication to compute our output predictions. And then, again, we're again using a sort of sort to figure out what kind of prediction we're trying to make. And we're also doing some matrices to compute the output predictions of our analysis. of basic Tensor operations to compute our Euclidean distance, our L2 loss between our prediction and the target Y. Another thing to point out here is that these lines of code are not actually computing anything. There's no data in the system right now. We're just building up this computational power for the time being. We don't have any data to work with right now, so we're building it up from the ground up. We'll be adding more data as we go along. graph data structure telling TensorFlow which operations we want to eventually run once we put in real data. So this is just building the graph, this is not actually doing anything. Then we have this magical line where after we've computed our loss with these symbolic operations, then we can use them to compute our loss. This is the line where we can then use the loss function to compute the loss of our data. This line is called our loss function. We can use this function to calculate our loss and then use it to compute how much data we have. just ask TensorFlow to compute the gradient of the loss with respect to w1 and w2 in this one magical, beautiful line. And this avoids you writing all your own backprop code that you had to do in the assignments. But again there's no actual computation happening here. This is just the result of a line of code that was written in Tensorflow. But this is the result, and this is what it looks like when you write the code in the assignment. just sort of adding extra operations to the computational graph where now the computationalgraph has these additional operations which will end up computing these gradients for you. So now at this point we've computed our computational graph, we have this big graph in this graph data structure in memory. And now we have our big graph. We have this data structure. And we can then use it to compute the gradients. We can now use this graph to calculate the gradient. And then we can use that graph to do the other operations. that knows what operations we want to perform to compute the loss in gradients. And now we enter a TensorFlow session to actually run this graph and feed it with data. So then, once we've entered the session, then we actually need to construct some concrete values that will be used to calculate the loss. And then, we use this data to create a graph that we can use to compute a loss in the loss of a graph. For example, we could use the following graph to calculate a loss of 1:1:1, or 1:2:1. TensorFlow just expects to receive data from Numpy arrays in most cases. So here we're just creating concrete actual values for X, Y, w1 and w2 using Numpy and then storing these in some dictionary. And now here is where we're actually running the graph.fed to the graph in the TensorFlow app. The graph is just a list of all the data points that are being fed into the graph by the Numpy function. The data points are then stored in a dictionary and the graph is run from there. So you can see that we're calling a session.run to actually execute some part of the graph. And that, so we actually want the graph, in this case we need to tell it which part of it we want as output. The first argument loss, tells us which part the graph do we actually Want as output, and that's what we're doing in this example. The second argument loss is what we want to output as the result of the run. This time we want the output to be the whole graph, rather than just a small part. that we actually want to compute loss and grad1 and grad w2 and we need to pass in with this feed dict parameter the actual concrete values that will be fed to the graph. And then after, in this one line, it's going and running the graph and then computing those values for loss grad1 to grad w 2 and then returning the actualcrete values for those in Numpy arrays again. So now after you unpack this output in the second line, you get Numpy array with the loss and the gradients. can go and do whatever you want with these values. So then, this has only run sort of one forward and backward pass through our graph. It only takes a couple extra lines if we actually want to train the network. So here we're, now we're running the graph, and it only takes a few extra lines to run the graph. And then, we're going to do the same thing with the other side of the network, which is the forward side. And so on and so on. many times in a loop so we're doing a four loop and in each iteration of the loop, we're calling session.run asking it to compute the loss and the gradients. And now we'redoing a manual gradient discent step using those computed gradients to now update our current values of the loss. We're now doing a manual Gradient Discent step to update the loss to the current value of the current loss. And we're updating the current values to the new loss of the previous loss. the weights. So if you actually run this code and plot the losses, then you'll see that the loss goes down and the network is training and this is working pretty well. So this is kind of like a super bare bones example of training a fully connected network in the code. The code is available on GitHub here: http://www.gofundme.com/s3s3a7a. It's a very simple example of how you can train a network in a very short time. Every time we execute this graph, we're actually feeding in the weights. We have the weights as Numpy arrays and we're explicitly feeding them into the graph. But there's a problem here. And now when the graph finishes executing it's going to look like this:. TensorFlow.TensorFlow is a free and open-sourceensorFlow software. It was developed by Google and is available on the Google Play Store and the GitHub repository. For more information, visit:http://www.googleplay.com/tensorflow/ andwww.gofundme.com/. to give us these gradients. And remember the gradients are the same size as the weights. So this means that every time we're running the graph here, we're copying the weights from Numpy arrays into TensorFlow then getting the gradient. And then copying theGradients from Tensor Flow back out to the graph again to give us the same result. We're using the weights to get the weights and then getting gradients to give the graph the same results. Numpy arrays. So if you're just running on CPU, this is maybe not a huge deal. But remember we talked about CPU GPU bottleneck and how it's very expensive actually to copy data between CPU memory and GPU memory. If your network is very large and your weights and weights are very large, this could be a big problem for you. If you have a very large network, this may be a huge problem for your network. It could be very expensive for you if you have very large networks. gradients were very big, then doing something like this would be super expensive and super slow because we'd be copying all kinds of data back and forth between the CPU and the GPU at every time step. So that's bad, we don't want to do that. We need to fix that. That's what we're trying to do. We're not going to get there right away, but we're working on it. We've got a lot of work to do to get to that point. TensorFlow has some solution to this. And the idea is that now we want our weights, w1 and w2, rather than being placeholders where we're going to, where we expect to feed them in to the network on every forward pass. Instead we define them as variables. That's the idea behind TensorFlow.that. It's a new way of thinking about how data is fed into a network, and how it can be used to predict what data is going to be used. So now instead of declaring these w1 and w2 as placeholders, instead we just construct them as variables. So a variable is something is a value that lives inside the computational graph and it's going to persist across different times when you run the same graph. So now we just declare these w2 and w1 as variables, instead of placeholders. We can then use these variables in the same way as the placeholders in the previous example. The result is that we have a variable called w2, and we can use it to store a value. But now since they live inside the graph, we also need to tell TensorFlow how they should be initialized. So we need to pass in a tf.randomnormal operation, which again is not actually initializing them when we run this line. So it's a little bit of confusing misdirection going on here. But again, this is just telling Tensor Flow how we want them to beInitialized. We were feeding in their values from outside the graph so we initialized them in Numpy. In the previous example, we were computing the gradients and then using them to update the weights as Numpy arrays and then feeding in the updated weights at the next time step. But now because we want these weights to live inside the graph, this operation of updating the weights needs to also be an operation inside the computational graph. So now we used this assign function which mutates these variables inside the computational graph and now the mutated value will be the updated value of the previous value. persist across multiple runs of the same graph. So now when we run this graph and when we train the network, now we need to run the graph once with a little bit of special incantation to tell TensorFlow to set up these variables that are going to live inside of the network. We need to do this so that the network can be trained on this graph. We do this by running the graph with a special command in the middle of the training process. We then run the network on the graph to train it. the graph. And then once we've done that, now we can run the graph over and over again. And here, we're now only feeding in the data and labels X and Y and the weights are living inside the graph. and here we've asked the network to, we have asked it to use the data to create a graph. The result is a graph that looks like the image of a person's face, with the person's name and age added to the top of the image. TensorFlow to compute the loss for us. And then you might think that this would train the network, but there's actually a bug here. So, if you actually run this code, and you plot the loss, it doesn't train. So that's bad, it's confusing, like what's going on? We wrote Tensorflow.tensorFlow.Tensorflow to compute. the loss. We wrote. Tensor Flow. tensorflow-loss. this assign code, we ran the thing, like we computed the loss and the gradients and our loss is flat, what's going on? Any ideas? [student's words obscured due to lack of microphone] Yeah so one hypothesis is that maybe we're accidentally re-initializing the w's every time we call the assign code. This assign code is supposed to be used to calculate the loss of an object. It's supposed to look like a circle, or a square, or something like that. We need to explicitly tell TensorFlow that we want to run these new w1 and new w2 operations. So we've built up agraph.graph. That's a good hypothesis, that's actually not the problem in this case. [student's words obscured due to lack of microphone] Yeah, so the answer is that we actually need to tell TensorFlow that We want to Run These New W1 and New W2 Operations. We need to build up a graph.graph to do this. this big computational graph data structure in memory and now when we call run, we only told TensorFlow that we wanted to compute loss. And if you look at the dependencies among these different operations inside the graph, you see that in order to compute lost we don't actually need to use all of these operations in the first place. We just need to tell Tensor Flow that we want to compute the loss. That's what we do. We don't need all of the other operations to be done in the same way. TensorFlow is smart and it only computes the parts of the graph that are necessary for computing the output that you asked it to compute. In situations like this it can be a little bit confusing and lead to behavior that you didn't expect. So the solution in this case is that we actually need to explicitly tell TensorFlow to perform those update operations. So one thing we could do, which is what the team at Google came up with, is to tell it to perform this update operation. was suggested is we could add new w1 and new w2 as outputs and just tell TensorFlow that we want to produce these values as outputs. But that's a problem too because the values are again these big tensors. So now if we tell TenseFlow to produce the new values as an output, it will produce the old values instead of the new ones. So we have to change the output to be the same as the original output. This leads to the problem of how to get the values to work together. TensorFlow we want those as output, we're going to again get this copying behavior between CPU and GPU at ever iteration. So that's bad, we don't want that. So there's a little trick you can do instead. Which is that we add kind of a dummy node to the graph. And that's what we do in TensorFlow. It's called a 'dummy node' and it can be used to add nodes to a graph. It can be added to any graph. With these fake data dependencies and we just say that this dummy node updates, has these data dependencies of new w1 and new w2. And now when we actually run the graph, we tell it to compute both the loss and the dummy node. And this dummy nodes doesn't actually actually have any data dependencies. It's just a way to make the graph look a little bit more like a lossy graph, but it doesn't have any real data dependencies at all. It just looks like a graph that has a loss and a dummy node in it. return any value it just returns none, but because of this dependency that we've put into the node it ensures that when we run the updates value, we actually also run these update operations. So, question? [student's words obscured due to lack of microphone] Is there a reason why we want to do this? And if so, what is the reason? And why do we need to do it? And how do we get it to work? And what does it look like? didn't put X and Y into the graph? And that it stayed as Numpy. So in this case, they could have stayed in the graph, but in most cases they will be minibatches of data so those will actually change at every iteration and we will want to feed different values for those at each iteration. So you're right, we could have just also stuck those in theGraph. But in a more realistic scenario, X and. Y will be Minibatches. change, so we don't want them to live in the graph. We had put into TensorFlow that the outputs we want are loss and updates. So when when they change, we want to see that change. We want to be able to see the change in the data. We don't like to see it in a graph. So we want it to be in the real world, not just in the code. We're trying to make it more intuitive for people to understand what's happening. updates evaluates it just returns none. But because of this dependency we've told it that updates depends on these assign operations. These assign operations live inside the computational graph and all live inside GPU memory. So then we're doing these update operations entirely on the GPU and we're no longer relying on the CPU for updates. We're no more dependent on the processor for updates than we were on the previous version of the code. We are no longer using the CPU to update the code, we are using the GPU to update it. The question is does tf.group return none? So this gets into the trickiness of TensorFlow. So when you execute the graph, and when you tell, inside the session.run, when we told it we want it to compute the concrete value from updates, that returns none. So here after you run updates, then the output is none. Does that clear it up a little? Do you have a better idea of how this works? If so, please share it in the comments below. bit? So the question is why is loss a value and why is updates none? That's just the way that updates works. So loss is a value when we compute, when we tell TensorFlow we want to run a tensor, then we run the tensor.bit? [student's words obscured due to lack of microphone] So the answer is that loss is not a value, it's just a value that we use to compute the tensors. So that's why we use it. get the concrete value. Updates is this kind of special other data type that does not return a value, it instead returns none. So it's kind of some TensorFlow magic that's going on there. Maybe we can talk offline if you're still confused. [student's words obscured due to lack of lack of words] Maybe we could talk offline If you are still confused if you are not sure what is going on, or if you would like to talk about it offline, please contact us at [email protected] The behavior is coming from the group method. So now, we kind of have this weird pattern where we wanted to do these different assign operations, we have to use this funny tf.group thing. That's kind of a pain, so thankfully TensorFlow gives you some convenience operations, so you don't have to go through all of that. It's just a little bit easier to do some of the things that you need to do in the code. It doesn't get as complicated as it used to be, though. that kind of do that kind of stuff for you. And that's called an optimizer. So here we're using a. tf.train.GradientDescentOptimizer and we're telling it what learning rate we want to use. And you can imagine that there's, there's RMSprop, there are all kinds of different optimization algorithms here. And now we're going to use that to do our training. We're using it to do the training and then we'll see what the results are. we call optimizer.minimize of loss and now this is a pretty magical. This call is aware that these variables w1 and w2 are marked as trainable by default, so then internally, inside this optimized it's going in and adding nodes to the graph. It's also performing that update operation for you and it's doing the grouping operation forYou. It is like doing a lot of magical stuff inside there. But then it ends, it ends. It ends. The code is actually using tf.group so it looks very similar internally to what we saw before. And now when we run the graph inside our loop we do the same pattern of telling it to compute loss. This gives you this magical updates value which, if you dig through the code they're actually using TF.up giving you this Magical updates value. The code is very similar to the one we saw in the previous example. It looks like the same thing but with a different name. and updates. And every time we tell the graph to compute updates, then it'll actually go and update the graph. So that's initializing w1 and w2 because these are variables which live inside theGraph. Question? [student's words obscured due to lack of microphone] Yeah, so what is thetf.GlobalVariablesInitializer? So that is the tf. global variables initializer for the graph, and then it updates the graph every time it's told to do so. So we need to, when we saw this, when you create the tf.variable we have this tf.randomnormal which is this initialization. So the TF.GlobalVariablesInitializer is causing thetf.random normal to actually run and generate concrete values toinitialize those variables. [student's words obscured due to lack of microphone] Sorry, what was the question? The question was: How do we get these variables to be created? The answer is: We need to create a new variable. the question? [student's words obscured due to lack of microphone] So it knows that a placeholder is going to be fed outside of the graph and a variable is something that lives inside the graph. So I don't know all the details about how it decides, what exactly it decides. The question? The question is, what is the answer to that? The answer is, the question is: What is the solution to that question? Click here to read the rest of the story. to run with that call. I think you'd need to dig through the code to figure that out, or maybe it's documented somewhere. So but now we've kind of got this, again we've got this full example of training a network in TensorFlow. And we're kind of adding bells and whistles to it. We're adding bells, whistles and bells to it, and it's kind of working. It's working. We've got a pretty good idea of how it's going to work. Whistles to make it a little bit more convenient. So we can also here, in the previous example we were computing the loss explicitly using our own tensor operations. TensorFlow you can always do that, you can use basic Tensor operations to compute just about anything you want. But TensorFlow lets you do it in a way that is more convenient for you. For example, in this example we can compute the loss by using a tensor operation. We can also do this by using the whistles function. also gives you a bunch of convenience functions that compute these common neural network things for you. So in this case we can use tf.losses.mean_squared_error and it just does the L2 loss for us. So we don't have to compute it ourself in terms of basic tensor operations. So another convenience function is called tf. Losses. Mean_Squared_Error. It does the same thing for the L1 loss and L2 Loss. kind of weirdness here is that it was kind of annoying that we had to explicitly define our inputs and define our weights and then like chain them together in the forward pass using a matrix multiply. And in this example we've actually not put biases in the layer because we've not defined our weights yet. We're just using the matrix multiply to chain the weights together. We don't have biases in our layer. We just use the matrix multiplier to chain our weights together in a forward pass. that would be kind of an extra, then we'd have to initialize biases, we would have to get them in the right shape. We'd also have to broadcast the biases against the output of the matrix multiply and you can see that that would kind of be a lot of code. It would take a long time to write all of that. It's kind of a big project, but it's not that big of a deal. It just takes a little bit of time to get it working. would be kind of annoying write. And once you get to like convolutions and batch normalizations and other types of layers this kind of basic way of working, of having these variables, having these inputs and outputs and combining them all together with basic computational graph operations could be a great way to work. It could be the future of computer science. It's a very exciting time to be a computer scientist. I'm looking forward to seeing what the future has in store for us. I hope you'll join me. little bit unwieldy and it could be really annoying to make sure youinitialize the weights with the right shapes and all that sort of stuff. So as a result, there's a bunch of sort of higher level libraries that wrap around TensorFlow and handle some of these details for Tensorflow. There's a lot of work to be done to make Tensor Flow more efficient. We're working on a way to make it even easier for people to use it. We'll have more to say about this in the coming weeks. TensorFlow is an open sourceensorFlow library. It allows you to write your ownensorFlow code. Here is an example of how to use TensorFlow in your own code. You can use it to test your own data and labels in the real world. For example, you could test your data by looking at the labels on the data. You could also use it as a way to test different types of data, such as text or images. For more information, visit Tensorflow.org. give it the input X and we tell it units=H. This is again kind of a magical line because inside this line, it's kind of setting up w1 and b1, the bias. It's setting up variables for those with the right shapes that are kind of inside the graph but outside of the graph. The result is a graph that looks like the shape of a circle or a triangle or a pentagon or a circle with a line in the middle of it. It looks like a circle, but it's actually a line. a little bit hidden from us. And it's using this xavier initializer object to set up an initialization strategy for those. So before we were doing that explicitly ourselves with the tf.randomnormal business, but now here it's kind of handling some of those details for us and it's just spitting them out. It's using the xavierInitializer object. So it's doing some of the initializer business for us, but it's also spitting out the random numbers. It doesn't have to do all of it, just some of it. We're passing an activation=tf.nn.relu so it's even doing the activation, the relu activation function inside this layer. We're also passing an H, which is again the same sort of H that we saw in the previous layer, it's just doing some of those details for us. And you can see here, we're alsoPassing anactivation=TFN.Relu so It's even Doing the Activation function inside This Layer. And we're Passing an H to Do The Activation Function inside This layer. And You can see Here, We're Also Passing An H To Do The activation function Inside This Layer, It's Even Doing The activation. for us. So it's taking care of a lot of these architectural details for us. Question? [student's words obscured due to lack of microphone] Question is does the xavier initializer default to particular distribution? I'm sure it has some default, I'm not sure what it is. I think you'll have to wait and see what the final version of xavier looks like, but it's going to be very different from what you've seen so far, I can tell you that. to look at the documentation. But it seems to be a reasonable strategy, I guess. And in fact if you run this code, it converges much faster than the previous one because the initialization is better. And you can see that we're using two calls to tf.layers and this lets this code converge much faster. And this lets us use the same code that we used in the previous example. It's a bit of a mess, but it's a good starting point. TensorFlow is not the only game in town. There's a lot of different higher level libraries that people build on top of TensorFlow. And it's kind of kind of convenient. We. build our model without doing all these explicit bookkeeping details ourself. So this is maybe a little bit more convenient. But tf.contrib.layer is really not theOnly game inTown. It's really not. the onlyGame in Town. We build our models without doing. all of the explicit bookkeeper details our selves. of due to this basic impotence mis-match where the computational graph is relatively low level thing, but when we're working with neural networks we have this concept of layers and weights and some layers have weights associated with them. We typically think at a slightly higher level of abstraction, and we typically think of neural networks at that level. Of course, this is not the case in the real world, but it's a good starting point for us to understand how neural networks work. than this raw computational graph. So that's what these various packages are trying to help you out and let you work at this higher layer of abstraction. Another very popular package that you may have seen before is Keras. Keras is a very beautiful, nice API that sits on top of TensorFlow and handles sort of building up these computational graph for you up in the back end. By the way, Keras also supports Theano as a back end, so that's also kind of nice. And in this example you can see we build the model as a sequence. We build some optimizer object and we call model.compile and this does a lot of magic in the back end to build the graph. And now we can call. model.fit and that does the whole training procedure for us magically. So I don't know all the details of the details. of the model.of layers. We build some Optimizer objects and we called model. Compile. This does a much of the work for us. And then we can use model. fit to train our model. how this works, but I know Keras is very popular, so you might consider using it if you're talking about TensorFlow. Question? [student's words obscured due to lack of microphone] Yeah, so the question is like why there's no explicit CPU, GPU going on here. So I've kind of left it kind of up to you, but you might want to consider using Keras if you want to use Tensor Flow with Keras in the future. that out to keep the code clean. But you saw at the beginning examples it was pretty easy to flop all these things between CPU and GPU. There was either some global flag or some different data type or some with statement and it's usually relatively simple and just to keep it clean. It's usually pretty easy and just a few lines of code at a time to get the code to work on both sides of the divide. That's what we're doing here. We're trying to make the code as simple as possible. about one line to swap in each case. But exactly what that line looks like differs a bit depending on the situation. So there's actually like this whole large set of higher level TensorFlow wrappers that you might see out there in the wild. And it seems that like even even even the most basic Tensorflow apps can use the new toolkit in some cases. The toolkit is available now on the Google Play Store for Android and the iOS App Store for iOS. It's not clear yet if it will be available for other platforms. people within Google can't really agree on which one is the right one to use. So Keras and TFLearn are third party libraries that are out there on the internet by other people. But there's these three different ones, TF-Slim and TF-contrib.learn that all ship with TensorFlow, that are all the right for Tensor Flow. That's the way we're going to use it for the time being, says Google's John Defterios. all kind of doing a slightly different version of this higher level wrapper thing. There's another framework also from Google, but not shipping with TensorFlow called Pretty Tensor that does the same sort of thing. And I guess none of these were good enough for DeepMind, because they went ahead with their own version of the same thing, which is called Tensorflow. It's not yet clear if it will be available to the general public, or if it'll be available in the near future. a couple weeks ago and wrote and released their very own high level TensorFlow wrapper called Sonnet. So I wouldn't begrudge you if you were kind of confused by all these things. There's a lot of different choices. They don't always play nicely with each other. But you have a choice and I'm happy to show you how to use it. I hope you'll use it as a tool to help you with your data analysis. I'm looking forward to hearing from you. TensorFlow has pretrained models. There's some examples in TF-Slim, and in Keras. 'Cause remember retrained models are super important when you're training your own things. There is also this idea of Tensorboard where you can load up your, I don't want to get into details, TensorFlow.lot of options, so that's good. Tensorflow has pret trained models, so it has some good options. It has some examples of pretrained model, and Keras has examples of models. but Tensorboard you can add sort of instrumentation to your code and then plot losses and things as you go through the training process. TensorFlow also let's you run distributed where you can break up a computational graph run on different machines. That's super cool but I think probably not anyone outside of Google is really using that to great success these days, but if you do want to run distributed stuff probably Tensorflow is the main game in town for that. A side note is that a lot of the design of Tensor Flow is kind of spiritually inspired by TensorBoard. this earlier framework called Theano from Montreal. I don't want to go through the details here, just if you go through these slides on your own, you can see that the code for Theano ends up looking very similar to TensorFlow. Where we define some variables, we do some forward work. We do some front-end work to make it easier for people to use the code. We also do some back-end code to make the code easier to read. We use the same code as Tensorflow to make our code more efficient. pass, we compute some gradients, and we compile some function, then we run the function over and over to train the network. So it kind of looks a lot like TensorFlow. So we still have a lot to get through, so I'm going to move on to PyTorch and maybe maybe maybe PyCognito as well. I'll be back in a week or so with an update on how we got to this point. I'm looking forward to hearing from you. PyTorch from Facebook is kind of different from TensorFlow. PyTorch has this tensor object which is just like a Numpy array. It's just an imperative array, it doesn't know what it's doing. We have sort of three explicit different layers of abstraction inside Py Torch. Take questions at the end of the talk.take questions atThe end. Share your thoughts on this article in the comments below or email it to jennifer.smith@mailonline.co.uk. anything about deep learning, but it can run with GPU. We have this variable object which is a node in a computational graph which builds up computational graphs, lets you compute gradients, that sort of thing. And we have a module object which are a neural network layer that you can use to build up a graph. And finally, we have an object that is a network layer which can be used to build a graph, and that can be done on the GPU. It's a very powerful tool. can compose together these modules to build big networks. The PyTorch variable is similar to the TensorFlow tensor or variable or placeholder, which are all sort of nodes in a computational graph. And now thePyTorch module is kind of equivalent to these higher level things from tf.slim or tf.layers or sonnet or these other higher level frameworks. So right away one thing to notice about Py Torch is that it's very simple to use. It's very easy to get started with. is that because it ships with this high level abstraction and like one really nice higher level abstraction called modules on its own, there's sort of less choice involved. Just stick with nn modules and you'll be good to go. You don't need to worry about which higher level wrapper to use to build your app. You just need to use nnmodules to build the app and you're done. It's a great way to get the most out of your app without having to worry too much about how to build it. PyTorch tensors are just like Numpy arrays so here on the right we've done an entire two layer network using entirely PyTorCh tensors. One thing to note is that we're not importing Numpy here at all anymore. We're just doing all these operations using PyTorch. Use.use.use to create your own web applications and web content. Use the.use tool to create web applications, web content, and Web content for your website and mobile devices. tensors. And this code looks exactly like the two layer net code that you wrote in Numpy on the first homework. So you set up some random data, you use some operations to compute the forward pass. And then we're explicitly viewing the backward pass ourself. Just sort of backhopping back and forth between the two layers of the net. It's very similar to what we did in the previous homework. It looks like the same code. It works the same way. through the network, through the operations, just as you did on homework one. And now we're doing a manual update of the weights using a learning rate and using our computed gradients. But the major difference between the PyTorch tensor and Numpy arrays is that they run on GPU so they run in the same way as the Numpy array. The results are the same, but the weights are updated using a new learning rate, and the gradients are updated with a new set of values. all you have to do to make this code run on GPU is use a different data type. You should think of PyTorch tensors as just Numpy. Rather than using torch. FloatTensor, you do torch.cuda.FloatTensor. This will cast all of your tensors to this new datatype and everything runs magically on the GPU. You can see the full code on Pytorch's README.com. For more information, visit the Py Torch website. PyTorch is the next layer of abstraction in PyTorch. So this is, once we moved from tensors to variables now we're building computational graphs and we're able to take gradients automatically and everything like that. That's exactly what it is, nothing specific to deep learning. So the next level of abstraction is the variable.plus GPU. So that's what we're working on right now. We're also working on the next stage of the project, which is the graph layer. here, if X is a variable, then x.data is a tensor and x.grad is another variable containing the gradients of the loss with respect to that tensor. So x. grad is an actual tensor containing those gradients. And PyTorch tensors and variables have the exact same API. So any code that uses tensors or variables has the same API as the tensors used in the code. For example, if a variable is called X, then X. data is x. data, and X. Grad is the loss of that variable. worked on PyTorch tensors you can just make them variables instead and run the same code, except now you're building up a computational graph rather than just doing these imperative operations. So here when we create these variables each call to the variable constructor wraps a PyTorCh tensor and then we use them to build up a graph. We then use the graph to build a graph of the variables we created. We can then use these variables to build the graph in the same way as before. also gives a flag whether or not we want to compute gradients with respect to this variable. And now in the forward pass it looks exactly like it did before in the variable in the case with tensors because they have the same API. So now we're computing our predictions, and it looks the same as it did in the previous pass. It's the same for tensors as well, so we don't need to change anything in the code. We just change the type of the variable. we're computing our loss in kind of this imperative kind of way. And then we call loss.backwards and now all these gradients come out for us. Then we can make a gradient update step on our weights using the gradients that are now present in the w1.grad.data. So this is the way that we get our loss. Back to Mail Online home. Back into the page you came from. Back To the page You came from, back to the page that you came From. ends up looking quite like the Numpy case, except all the gradients come for free. In a TensorFlow case we were building up this explicit graph, then running the graph many times. Here in PyTorch, instead the graph is an explicit graph that can be run many times in a row or a sequence of words. The result is a graph that looks something like this. The graph can then be used as a function of a function to create a new function. For example, the function could look like the following: we're building up a new graph every time we do a forward pass. And this makes the code look a bit cleaner. And it has some other implications that we'll get to in a bit. So in PyTorch you can define your own new autograd functions by defining the forward function in the code above. And you can also define the forward functions in your own code by defining them in the forward() function. And we'll talk about some of the implications of this later on. and backward in terms of tensors. This ends up looking kind of like the module layers code that you write for homework two. Where you can implement forward and backward using tensor operations and then stick these things inside computational graph. But most of the time you will probably not need to define your own autograd operations. Most of the times the operations you need are the ones that are already in the computational graph and can be used by other parts of the program. So here we're defining our own relu and then we can actually go in and use our own Relu operation and now stick it inside our computational graph to define our own operations. will mostly be already implemented for you. So in TensorFlow we saw, if we can move to something like Keras or TF.Learn and this gives us a higher level API to work with, rather than this raw computational graphs. The equivalent in PyTorch is the nn package. Where it provides an API that is similar to the one provided by Keras and TFLearn. This gives us the ability to use Keras in a way that is comparable to that provided by TFLearn and nn. these high level wrappers for working with these things. But unlike TensorFlow there's only one of them. And it works pretty well, so just use that if you're using PyTorch. So here, this ends up kind of looking like Keras where we define our model as some sequence of layers. It's a little bit different than Keras, but it's the same thing. It just has a different name, and you can use it to get around it. Our linear and relu operations. And we use some loss function defined in the nn package that's our mean squared error loss. And now inside each iteration of our loop we can run data forward through the model to get our predictions. We can run the predictions forward through a loop to get the data we need to make a prediction. We then use that data to make our predictions for the next iteration of the loop. And then we use the data forward to make the predictions for that iteration. loss function to get our scale or loss, then we can call loss.backward, get all our gradients for free. Then loop over the parameters of the models and do our explicit gradient descent step to update the models. And again we see that we're sort of building up this this huge data set that we want to use in our models. We're going to be using this data set for a long time to come, I think, in a number of different ways. I'll be sharing some of those with you in the future. New computational graph every time we do a forward pass. And just like we saw in TensorFlow, PyTorch provides these optimizer operations that kind of abstract away this updating logic and implement fancier update rules like Adam and whatnot. So here we're constructing an optimizer object telling it that we want to create a new computational graph. And we're doing this by creating an object called Adam and telling it to update Adam every time it does a forward-pass. And so on and so on. want it to optimize over the parameters of the model. Giving it some learning rate under the hyper parameters. And now after we compute our gradients we can just call optimizer.step and it updates all the parameters for us right here. So another common thing you'll do is give the model a learning rate of some sort. For example, we could give it a 50% learning rate or a 60%learning rate. We can do this by giving the model some parameters such as the learning rate and the learning level. And then we can use the optimizer step to update these parameters. in PyTorch a lot is define your own nn modules. So typically you'll write your own class which defines you entire model as a single new nn module class. And a module is just kind of a neural network layer that can contain either other other modules or trainable weights. In this case, the module class is called a nn model class. It can contain any number of modules, such as a network layer or a training layer. The module class can then be used to define other modules in the network. or other other kinds of state. So in this case we can redo the two layer net example by defining our own nn module class. So now here in the initializer of the class we're assigning this linear1 and linear2. We're constructing these new module objects and then store them in the nn class. We can then use these objects to build the two layers of the net. For example, we can use this to build a two-layer net with two layers. inside of our own class. And now in the forward pass we can use both our own internal modules as well as arbitrary autograd operations on variables to compute the output of our network. So here we receive the, inside this forward method here, the input acts as a variable, and the output acts as an internal variable. We can then use this output to compute our network output in a forward method. We receive the output inside of the forward method, and then use it in aforward method to calculate the network output. then we pass the variable to our self.linear1 for the first layer. We use an autograd op clamp to complete the relu. We pass the output of that to the second linear and then that gives us our output. And now the rest of this code for training this thing for the next layer. The code for the second layer is a bit more complex. It uses a second linear, a third linear and a fourth linear. The output of the first linear is then passed to the next linear. looks pretty much the same. Where we build an optimizer and loop over and on ever iteration feed data to the model, compute the gradients with loss.backwards, call optimizer.step. So this is like relatively characteristic of what you might see in a lot of PyTorch type training scenarios. Where you build an Optimizer and loops over and over and feed data. Back to Mail Online home. back to the page you came from. Back To the pageyou came from define your own class, defining your own model that contains other modules and whatnot and then you have some explicit training loop like this that runs it and updates it. One kind of nice quality of life thing that you have in PyTorch is a dataloader. So a dat aloader can can be used to define your own classes and models and so on. It can also be used as a training loop to make sure that the model is working properly. For example, a class can be defined and then it can be updated with a new model that has been defined. handle building minibatches for you. It can handle some of the multi-threading that we talked about for you, where it can actually use multiple threads in the background to build many batches for you and stream off disk. So here a dataloader wraps a dataset and provides some of these. So in this example, we have a dataset that is being used to build a batch of data. We can then use this data to stream it off disk in a different way. abstractions for you. And in practice when you want to run your own data, you typically will write your own dataset class which knows how to read your particular type of data off whatever source you want and then wrap it in a data loader and train with that. So, for example, you could write a dataset class to read data from a particular source and then train with it from that source. That's what we do. We write our own dataset classes for you, and then you train with them. here we can see that now we're iterating over the dataloader object and at every iteration this is yielding minibatches of data. And it's internally handling the shuffling of the data and multithreaded dataloadeding and all this sort of stuff for you. So this is kind of a completely PyTorch-based project. It's internally handled all of the multithreading and data shuffling and all of that sort of thing. So it's a completely different kind of project. PyTorch provides pretrained models. And this is probably the slickest pretrained model experience I've ever seen. You just say torchvision.models.alexnet pretained=true. That'll go down in the background, download the pretrained weights for you if you don't use PyTorch. And a lot of PyTorCh training code ends up looking something like this. The training code for this post is available on GitHub. For more information, visit the pytorch.org website. already have them, and then it's right there, you're good to go. So this is super easy to use. PyTorch also has, there's also a package called Visdom that lets you visualize some of these loss statistics somewhat similar to Tensorboard. So that's kind of nice, I haven't actually gotten around to using it yet, but I'll get around to it one of these days. I'm sure I'll be using it a lot more in the future. a chance to play around with this myself so I can't really speak to how useful it is. Tensorboard actually lets you visualize the structure of the computational graph. Which is really cool, a really useful debugging strategy. The app is available now on the Google Play store for Android and iOS. It's not available on iOS yet, but it's expected to be available in the near future. It is available for free on Google Play for now, with plans to roll out to iOS and Android later. And Visdom does not have that functionality yet. But I've never really used this myself so I can't really speak to its utility. PyTorch is kind of an evolution of, kind of a newer updated version of an older framework called Torch which I don't really use. But it's kind of fun to play with and it's a good way to test things out. It's a fun way to try new things out and see if you can do something with it that you wouldn't normally be able to. worked with a lot in the last couple of years. PyTorch is pretty much better in a lot of ways than the old Lua Torch, but they actually share a lot. of the same back end C code for Lua Torch. It's not a perfect solution, but it's a lot better than Lua Torch in many ways. For more information, visit thePyTorch website or the Pytorch mailing list. It is free and open source, with no paid features, and it's open source. Some of it ends up looking kind of similar to PyTorch, some of it's a bit different. Maybe you can step through this offline. But kind of the high level differences between.computing with tensors and GPU operations on tensors. and whatnot. So if you look through this Torch example, some. of it end up looking like this. Some of it looks kind of like this, but it's actually a lot more complex than that. It's a lot of different. Torch and PyTorch are that Torch is actually in Lua, not Python. So learning Lua is a bit of a turn off for some people. Torch doesn't have autograd. Torch is also older, so it's more stable, less susceptible to bugs, there's maybe more example code for Torch. But at least for me, I kind of prefer, I don't really see much reason for myself to use Torch overPyTorch. It's a little bit more of an adventure. anymore at this time. So I'm pretty much using PyTorch exclusively for all my work these days. We talked about this a little bit about this idea of static versus dynamic graphs. And this is one of the main distinguishing features between Py Torch and TensorFlow. So we saw in Tensor Flow that it can be used to create dynamic and static graphs at the same time. And that's what we're trying to do here in Pytorch. We want to be able to use dynamic and dynamic graphs at once. you have these two stages of operation where first you build up this computational graph, then you run the computational graph over and over again many many times reusing that same graph. That's called a static computational graph 'cause there's only one of them. And we saw PyTorch is quite different where we're actually building up this new computational. graph, this new fresh thing on every forward pass. That is called a dynamic computational graph. For kind of simple cases, with kind of feed forward neural networks, it doesn't really make a huge difference, the code ends up kind of similarly. and they work kind of similarly, but I do want to talk a bit about some of the implications of static versus dynamic. And what are the tradeoffs of those two. So one kind of nice idea with static graphs is that because we're kind of building up one graph, we're also building up another. And that's kind of a nice way of thinking about it. But with dynamic graphs, it's a little bit more complicated. It's like we're trying to build up a whole new graph. graph once, and then reusing it many times, the framework might have the opportunity to go in and do optimizations on that graph. And because we're going to reuse that graph many. times, maybe that optimization process is expensive up front, but we can amortize that cost. with the speedups that we've gotten when we run the graph many many times. So as kind of a concrete example, maybe if you write some graph which is very complex and you want to make it really efficient, you can kind of fuse some operations and reorder some operations, figure out the most efficient way to operate that graph so it can be really efficient. has convolution and relu operations kind of one after another, you might imagine that some fancy graph optimizer could go in and actually output, like emit custom code which has fused operations. So now it's computing the same thing as the code you wrote, says the programmer. The code has fused the convolution. and the relu. operations, fusing the Convolution and the. relu so now its computing the. same thing. As a result, the code now has the same result as the original code. but now might be able to be executed more efficiently. So I'm not too sure on exactly what the state in practice of TensorFlow graph optimization is right now, but at least in principle, this is one place where static graph really, you can have the potential for doing this. It's not a perfect solution yet, but it's a step in the right direction, and it could lead to a lot of useful new features in the future. It could also lead to some interesting new ways of working with Tensorflow. optimization in static graphs where maybe it would be not so tractable for dynamic graphs. Another kind of subtle point about static versus dynamic is this idea of serialization. So with a static graph you can imagine that you write this code that builds up the graph and then once once once you write the code that serialize the graph to the next stage in the process. It's kind of like that with dynamic graphs as well, where you can write code to serialize a dynamic graph to a static one. you've built the graph, you have this data structure in memory that represents the entire structure of your network. And now you could take that data structure and just serialize it to disk. And then you've got the whole structure ofYour network saved in some file. Andthen you can use it to build your own network. It's a very powerful tool, and it can be used to build any network you want. It could be used in a variety of ways, from web-based to cloud-based. could later rear load that thing and then run that computational graph without access to the original code that built it. So that's kind of a nice advantage of static graphs. Whereas with a dynamic graph, because we're interleaving these processes of graph building and graph execution, you kind of need theOriginal code at all times if you want to reuse that model in the future. On the other hand, some advantages for dynamic graphs are that it kind of makes, it just makes your code. a lot cleaner and a lot easier in a lot of scenarios. So for example, suppose that we want to do some conditional operation where depending on the value of some variable Z, we want. to do different operations to compute Y. Where if Z is positive, we wants to. compute Y if it is negative. If Z is negative, we. want to compute Z if it's negative, and so on. We can do this in the same way if we have a variable Z and a variable Y. use one weight matrix, if Z is negative we want to use a different weight matrix. And we just want to switch off between these two alternatives. In PyTorch because we're using dynamic graphs, it's super simple. Your code kind of looks exactly like you would expect, exactly what you would want to do. It's a very simple way to make a dynamic graph that can be used in a variety of ways. The code for the dynamic graph is in the source code for this article, which can be downloaded from the Pytorch website. would do in Numpy. You can just use normal Python control flow to handle this thing. And now because we're building up the graph each time, each time we perform this operation will take one of the two paths and build up maybe a different graph on each forward pass. Each time we do this, we build up a new graph, which we can then use to see how much data we have left to work with. The result is a graph that looks like the following: but for any graph that we do end up building up, we can back propagate through it just fine. And the code is very clean, easy to work with. Now in TensorFlow the situations is a little bit more complicated because we build the graph once, this control flow operator. But it's very clean and easy to use, and it's a lot of fun to use. It's very simple to understand and use, so it's easy to get to grips with and get used to. kind of needs to be an explicit operator in the TensorFlow graph. And now, so them you can see that we have this tf.cond call which is kind of like a Tensor Flow version of an if statement, but now it's baked into the computational graph rather than using sort of a sort of if statement. It's like a new type of Tensorflow statement. We call it a "cond statement" and it's a type of a conditional statement. And it's kind of a way of saying that you can't use an if-statement-as-a-statement. Python control flow. All the potential paths of control flow that our program might flow through need to be baked into the graph at the time we construct it before we ever run it. And the problem is that because we only build the graph once, we only need to build it once. So that means that we only get to see the possible paths of our program. So we only have one chance to see all the possible control flow paths that we might run our program through. We need to make sure that all of these paths are included in our graph. any kind of control flow operators that you want to have need to be not Python control flow operator, you need to use some kind of magic, special tensor flow operations to do control flow. In this case this tf.cond. Another kind of similar situation happens if you want. to want to use a different type of tensor operator. For example, you can use a tensor-flow operator to control flow in a different way. For more information on how to use this type of operator in Python, visit python.org/controlflow. have loops. So suppose that we want to compute some kind of recurrent relationships where maybe Y T is equal to Y T minus one plus X T times some weight matrix W and depending on each time we do this, every time we compute this, we might have a repeatable relationship. We might have to compute this again and again, and so on. This is called a recurrent relationship. It is a type of relationship that can be written as a series of terms. For example, we can write a recurrent relation like this: different sized sequence of data. We just want to compute this same recurrence relation no matter the size of the input sequence. So in PyTorch this is super easy. We can just kind of use a normal for loop in the for loop. And no matter how long the data is, we just want the same recursion relation. So this is just a very simple example of how to do this in the Python language. It's very simple to do. It just takes a few lines of code. Python to just loop over the number of times that we want to unroll and now depending on the size of the input data, our computational graph will end up as different sizes, but that's fine, we can just back propagate through each one, one at a time. Now in. Python to just loops over theNumber of times we want the data to be unrolled. We can then unroll the data a different size depending on how big the data is. We then back propagate the data through each of the different sizes. TensorFlow this becomes a little bit uglier. And again, because we need to construct the graph all at once up front, this control flow looping construct again needs to be an explicit node in the TensorFlow graph. So I hope you remember your functional programming because you'll have to use it as well. It's a bit of a pain, but it's a lot of fun. And it's all part of the same process. So let's try it out. those kinds of operators to implement looping constructs in TensorFlow. So in this case, for this particular recurrence relationship you can use a foldl operation and pass in, sort of implement this particular loop in terms of a Foldl. But what this basically means is that you have this sense of this looping construct. You can use this sense to implement this loop in a number of different ways, such as by using the foldl operator or by using a recurrence operator, for example. that TensorFlow is almost building its own entire programming language, using the language of computational graphs. Any kind of control flow operator, or any kind of data structure needs to be rolled into the computational graph so you can't really utilize all your favorite paradigms for working imperatively in Python. You kind of need to relearn a whole separate set of controlFlow operators. So at least for me, I find that kind of confusing, a little bit hard to wrap my head around sometimes. awkward to work with than the sort of native dynamic graphs you have in PyTorch. So then, I thought it might be nice to motivate like why would we care about dynamic graphs in general? So one option is recurrent networks. So you can see that for something like image, you can use recurrent networks instead of dynamic graphs. For example, recurrent networks can be used to make a dynamic graph look more like a real-world image. For more information on recurrent networks, see recurrent networks in the Pytorch source code. captioning we use a recurrent network which operates over sequences of different lengths. In this case, the sentence that we want to generate as a caption is a sequence and that sequence can vary depending on our input data. So now you can see that we have this dynamism in our captioning system. We can use this to generate different sentences depending on the data we are looking at, such as the length of a word or a line of text. We also use it to create different captions for different types of data. the thing where depending on the size of the sentence, our computational graph might need to have more or fewer elements. So that's one kind of common application of dynamic graphs. For those of you who took CS224N last quarter, you saw this idea of recursive networks where sometimes in recursive networks there are more or less elements in a graph. That's an example of a dynamic graph that can be used in this kind of way. And that's a common example of how dynamic graphs are used in computing. natural language processing you might, for example, compute a parsed tree of a sentence and then you want to have a neural network kind of operate recursively up this parse tree. So having a neuralNetwork that kind of works, it's not just a sequential sequence of layers, but instead a instead of a layer-by-layer sequence. That's what we're trying to do with this neural network. We want to be able to do that in a way that doesn't have to be sequential in nature. it's kind of working over some graph or tree structure instead where now each data point might have a different graph orTree structure. And it could vary from data point to data. So the structure of the computational graph then kind of mirrors the structure. of the input data. And so the data points could have different graphs or trees depending on what data they were looking at. It's a very interesting way of looking at data. It could be used in a lot of different ways in the future. point. So this type of thing seems kind of complicated and hairy to implement using TensorFlow, but in PyTorch you can just kind of use like normal Python control flow and it'll work out just fine. Another bit of more researchy application is this really cool idea that I like. That I like is a really cool concept that I've been working on for a while. It's called the "point.point" concept and it's based on the idea that point.point can be used to represent a point of interest. called neuromodule networks for visual question answering. So here the idea is that we want to ask some questions about images where we maybe input this image of cats and dogs, there's some question, what color is the cat, and then internally the system can read the question and that has these different specialized neural network modules for performing operations like asking for colors and finding cats. And then depending on the text of the question, it can compile this custom architecture for answering the question. And now if we asked a different question, like are there more cats than there are dogs? dogs? Now we have maybe the same basic set of modules for doing things like finding cats and dogs and counting, but they're arranged in a different order. So we get this dynamism again where different data points might give rise to different computational graphs. But this is a bit of a bit different, I'm not sure if this is the best way to look at it, but it's a good starting point for us to try and figure out what's going on. We'll have to see. more of a researchy thing and maybe not so main stream right now. But as kind of a bigger point, I think that there's a lot of cool, creative applications that people could do with dynamic computational graphs. And maybe if you come up with cool ideas, we'll feature it in lecture next year. So I wanted to talk very briefly about Caffe which is a type of graph that's used in the cloud. It's a very powerful tool and it can be used to do all kinds of interesting things. is this framework from Berkeley. Which Caffe is somewhat different from the other deep learning frameworks where you in many cases you can actually train networks without writing any code yourself. You kind of just call into these pre-existing binaries, set up some configuration files and in many case you can train networks with no code at all. It's kind of a different way of thinking about deep learning than some of the other tools that are out there today, like TensorFlow, for example, which is a very popular tool. can train on data without writing any of your own code. So, you may be first, you convert your data into some format like HDF5 or LMDB and there exists some scripts inside Caffe that can just convert like folders of images and text files into these formats for you. There are also some scripts that can convert files into other formats like CSV and EPUB. For more information, visit Caffe's official site or go to the Caffe website. For confidential support, call the Samaritans in the UK on 08457 90 90 90, visit a local Samaritans branch or click here. You need to define, now instead of writing code to define the structure of your computational graph, instead you edit some text file called a prototxt which sets up the structure. Here the structure is that we read from some input HDF5 file, we perform some computations on that file, and then we use that data to create a graph. The structure is the same as the one in the code above, except that it is written to a text file instead of a code file. inner product, we compute some loss and the whole structure of the graph is set up in this text file. One kind of downside here is that these files can get really ugly for very large networks. So for something like the 152 layer ResNet model, which by the way is a very large network, this can be a very ugly thing to work with. For something like that, this is a really good way to get a sense of how large a network is. It's a very powerful tool. was trained in Caffe originally, then this prototxt file ends up almost 7000 lines long. So people are not writing these by hand. People will sometimes will like write python scripts to generate these prototx files. Then you're kind in the realm of rolling your own computational graph abstraction. [laughter] Then youre kind of in the territory of Im going to make a graph that looks like this. Thats the kind of thing you do. That's probably not a good idea, but I've seen that before. Then, rather than having some optimizer object, instead there's some solver. This defines your learning rate, your optimization algorithm and whatnot. And then once you do all these things, you can go on to the next stage of the algorithm. That's what I've been doing for the past few days. It's a bit of a pain, but it's a good start. I'm not sure if it's worth it. just run the Caffe binary with the train command and it all happens magically. Cafee has a model zoo with a bunch of pretrained models, that's pretty useful. Caffe has a Python interface but it's not super well documented. You kind of need to read the source code of the python interface to see what it can do. So, kind of my general thing about Caffe is that it's maybe good for feed forward models, because it doesn't depend on Python. But probably for research. these days, I've seen Caffe being used maybe a little bit less. Although I think it is still pretty commonly used in industry again for production. So Caffe 2 is the successor to Caffe which is from Facebook. It's still pretty common in industry. I promise one slide, one or two slides on Caffe 1 and 2. I'll be back in a week or so to talk about Caffe 3 and Caffe 4. I'm looking forward to it. super new, it was only released a week ago. So I really haven't had the time to form a super educated opinion about Caffe 2 yet, but it uses static graphs kind of similar to TensorFlow. Kind of like Caffe one the core is written in C++ and they are both written in the same language, C++. They are both based on the Caffe programming language, but they are written in different versions of the language, so they are different. have some Python interface. The difference is that now you no longer need to write your own Python scripts to generate prototxt files. You can kind of define your computational graph structure all in Python. And then once your model is trained and whatnot, then we get this benefit that we talked about of static graphs where you can, you don't need the original training code now in order to deploy. It's kind of looking with an API that looks kind of like TensorFlow. a trained model. So one interesting thing is that you've seen Google maybe has one major deep running framework, which is TensorFlow, where Facebook has these two, PyTorch and Caffe 2. So these are kind of different philosophies. Google's kind of trying to build one framework to rule them all, and that's what we're trying to do with Tensor Flow. We're going to try to build a framework that can run all these different types of deep learning models. that maybe works for every possible scenario for deep learning. This is kind of nice because it consolidates all efforts onto one framework. It means you only need to learn one thing and it'll work across many different scenarios including like distributed systems, production, deployment, mobile, research, everything. Only need to know one thing to get the most out of the system and it will work in many different ways. It's a great tool to have in your toolbox for all types of systems. to learn one framework to do all these things. Whereas Facebook is taking a bit of a different approach. Where PyTorch is really more specialized, more geared towards research so in terms of writing research code and quickly iterating on your ideas, that's super easy in Py Torch, but for things like Facebook it's not so easy. It's not as easy in Facebook as it is in Pytorch. It takes a lot of work to get to the point where you're able to do that. like running in production, running on mobile devices, PyTorch doesn't have a lot of great support. Instead, Caffe 2 is kind of geared toward those more production oriented use cases. So my kind of general study, my general, overall advice about like which framework to use for which problems is Caffe 1 or Caffe2. That's what I'm going to be focusing on for the next few months, at least, until I get my hands on a version of Caffe 3. kind of that both, I think TensorFlow is a pretty safe bet for just about any project that you want to start new, right? Because it is sort of one framework to rule them all, it can be used forjust about any circumstance. However, you probably need to pair it with something else to make it work in the real world, so don't just use it as a tool for your own project. For example, you might want to use Google's Tensorflow to build your own neural networks. For more information on how to use it in your project, visit the Tensor Flow website. Some of the code ends up looking a little bit uglier in my opinion, but maybe that's kind of a cosmetic detail and it doesn't really matter that much. I personally think PyTorch.it with a higher level wrapper and if you want dynamic graphs, you're maybe out of luck. If you're looking for dynamic graphs you'll have to use a different tool, such as Graphite. It's a great tool, but you'll need to know how to use it to get the most from it. is really great for research. If you're focused on just writing research code, I think PyTorch is a great choice. But it's a bit newer, has less community support, less code out there, so it could be a bit of an adventure. IfYou want more of a well trodden path, try PyPy. It's a great place to start, but it's not as well-known as PyPy or PyPyTorch. You'll need to know how to use it to get the most out of it. If you're interested in production deployment, you should probably look at Caffe, Caffe 2 or TensorFlow. And if you're really focused on mobile deployment, I think Tensor Flow and Caffe2 both have some built in support for that. So it's kind of unfortunately, unfortunately, for path.path, Tensorflow might be a better choice. But if you want to get the most out of your Caffe app, you're going to have to go with it. there's not just like one global best framework, it kind of depends on what you're actually trying to do, what applications you anticipate but theses are kind of my general advice on those things. So next time we'll talk about some case studies about various CNN architectures. We'll also talk about how to get the most out of a CNN architecture and how to use it in a variety of different ways. Back to the page you came from. Follow us on Twitter @CNNOpinion.