Today we're gonna talk about learning in the setting of games. So what does learning mean? How do we learn those evaluation functions that we talked about? And then, er, towards the end of the lecture, we wanna talk a little [NOISE] bit about variations of the game- the games we have talked about. So, uh, how about if you have- how about the cases where we have simultaneous games or non-zero-sum games? So that's the, that's the plan for today. So I'm gonna start with a question that you're actually going to talk about it towards the end of the lecture, but it's a good motivation. So, uh, think [NOISE] about a setting where we have a simultaneous two-player zero-sum game. So can you still be optimal if you reveal your strategy? So lets say you're playing with someone. If you tell them what your strategy is, can youstill be optimal? That's the question. small. So, so the question is more of a motivating thing. We'll talk about this in a lot of details towards the end of the class. But, like, the reason that we have put this I guess at, at the beginning of the lecture is intuitively when you think about this, you might say, "No. I'm not gonna tell you what my strategy is, right? Because if I say, I'm gonna play it, like,. scissors, you'll know what to play." We had this minimax tree and based on that, the utilities that are gonna pop up are minus 50, 1 and minus 5. So if your goal is to maximize your utility, you're gonna pick bucket B, the second bucket, because that's the best thing you can do, assuming your opponent is a minimizer. So, so that was kind of the setup that we started looking at. And the way we thought about, uh, solving this game by- was by writing a recurrence. This is V which was the value of a minimax at state S. And if you're.going to pick bucket A, bucket B or bucket C, then the opponent is going to pick a number from these buckets. at the utility, er, so if you're an- at an end state, we are gonna get utility of S. And if the agent is playing, we- the recurrence is maximize V of the successor states. If the opponent is playing,. you wanna minimize the value of the successors states. And so that was the Recurrence we started with, and, and we looked at games that were kind of large like the game of chess, the branching factor is huge. The depth is really large. So we, we started talking about ways to- for speeding things up. One way to speed things up was this idea of using an evaluation function. of this weak estimate of your value is going to work well and give you an idea of what to do next. So, so instead of the usual recurrence, what we decided to add this D here, um, this D right here which is the depth that un- until which we are exploring. And then we decrease the value of depth, uh, after an agent and opponent plays. When depth is equal to 0, we just call an evaluation function. So intuitively if you're playing chess, for example, you might think a few steps ahead, and when you think about how the board looks like. In chess, you have this evaluation function that can depend on the number of pieces you have, the mobility of your pieces. Maybe the safety of your king, central control, all these various things that you might care about. So, so the hand- like you can actually hand-design these things and, and write down these weights about how much you care about these features. Okay. Well, one other thing we can do is instead of handcrafting it, we could actually try to learn this evaluationfunction. "I care about the number of kings and queens and these sort of things that I have, but I don't know how much I care about them. And I actually wanna learn that evaluation function. Like what the weights should be." "So to do that, I can write my evaluation function, eval of S, as, as this V as a function of state parameterized by, by weights Ws" "And, and my goal is to figure out what these Ws, what these weights are. And ideally I wanna learning that from some data" In the first part of the lecture, we're going to look at backgammon. And then towards the end of the class, we will talk about simultaneous games and non-zero-sum games. And, and that kind of introduces to this, this, um, temporal difference learning which we're gonna discuss in a second. It's very similar to Q-learning. Okay. So let's think about an example and I'm going to focus on the linear classifier way of looking at this just for simplicity. can actually, like, roll two dice and based on the outcome of your dice, you move your pieces various, various amounts to, to various columns. So your goal is to get all your pieces off the board. But if you have only one piece and your opponent gets on top of you, they can push you to the bar and you have to start again. Um, there are a bunch of rules about it. Read it, read about it on Wikipedia if you're interested. But you are going to look at a simplified version of it. maybe like the location of the X's and O's. The number of them. Yeah. So similar type of way that we- we've come up with features in the first few lectures. So for this particular board, here are what those features would look like. So if you look at number of O's in column zero 0 to 1, that's equal to 1. Remember we were using these indicator functions to be more general. So, so like here, again, we are using. these indicator functions. You might ask number of O's on the bar that's equal to 1, fraction of Os that are removed. So, so we have a bunch of features. These features, kind of, explain what the sport looks like or how good this board is. And what we wanna do is we wanna figure out what, what are the weights that we should put for each one of these features and how much we should care about, uh, each one. So that is the goal of learning here. somewhere. So, so one idea that we can use here is we can try to generate data based on our current policy pi agent or pi opponent, which is based onOur current estimate of what V is. Right. So currently, I might have some idea of what this V function is. It might be a very bad idea ofwhat V is, but that's okay. I can just start with that and starting with, with that V function that I currently have, what I can do is I can, I can call arg max of V over successors of s and a to get a policy for my agent. of episodes. We go over them to make things better and better. So, so that's, kind of, the key idea. Um, one question you might have at this point is, um, is this deterministic or not, like, do I need to do something like Epsilon-Greedy? But in this particular case, you don't really need toDo that because we have to get- we have this die that, that you're actually rolling the dice. And by rolling the Dice, you are getting random different- different random path that we might take- so that might take us to different states. us explore a little bit more. So then we generate episodes and then from these episodes, we want to learn. These episodes look like state action reward states and then they keep going until we get a full episode. One thing to notice here is, is the reward is going to be 0 throughout the episode until the very end of- end of the game. Right. Until we end the episode and we might get some reward at that point or we might not. Uh, but, but the reward throughout isgoing to be equal to 0 because we are playing a game. So s, take an action, you get a reward. You go to some s prime from that and you have some prediction. Right. Your prediction is your current, like, your current V function. So your prediction is going to be this V function and add state s parameterized with W. And this is your prediction. I'm writing the prediction as a function of w. Because it depends on w. And then we had a target that you're trying to get to. And my target, which is kind- kind of acts as a label. So it's kind of, the reward. playing games, in games Gamma is usually 1. And then one other thing to notice here is, I'm not writing target as a function of w because target acts kind of like my label, right? If I'm, if I'm trying to do regression here, target is my label. So I'm gonna treat my target as just like a value. So with respect to w, okay? How do I do that? I can take the gradient. What is the gradient equal to? This. is simple, right? 2 reduced, 2 gets canceled. Gradient is just this guy, prediction of w, minus target, times the gradient of this inner expression. So the objective function is prediction minus target squared. And then the update is this, this particular update where we move in the negative direction of the gradient. This is, this is what you guys have seen already, okay. All right. So so far so good. Um, so this is the algorithm we're going to use. is the TD learning algorithm. This is all it does. So temporal difference learning, what it does is it picks like these pieces of experience; s, a, r, s prime, and then based on that, it just updates w based on this gradient descent update, difference between prediction and target times the gradient of V, okay? So what if my V of sw is just equal to w dot phi of s, yeah phiof s. So what happens to my update? Minus Eta. between the two? Yeah, so this is very similar to Q learning. There are very minor differences that you'll talk about actually at the end of this section. All right. So, so I wanna go over an example, it's kind of like a tedious example but I think it helps going over that and kind of seeing why it works. Especially in the case that the reward is just equal to 0 like throughout an episode. So I want to just go over like one example of this. to write it in a simple for- not a simpler form but just another form. So w the way we're updating it is, the previous w minus Eta times prediction minus target, I'm gonna use p and t for prediction minus the target, times phi of s. So at this point, w hasn't changed, w is equal to 0. What is target equal to? So I'm predicting 0 but my target is 1, so I need to push my w's a little bit up to actually address the fact that this is. If you use like, uh, initialize rates do not be zeros which you update throughout instead of just to the end. Yeah. Okay and section two, so S4 and S9 are the same future of activities but you said S4 is S9 [OVERLAPPING]. Uh, this is a made up example, [LAUGHTER] so don't think about this example too much though. Well, is it that possible to have, an end state and not end state have the same feature vector, or no? As one, uh, entry that's always isn't [inaudible] like instead of 1, 2, we have 1, 0 leading to the, the final weight then the weight corresponding to that. Is going to- [OVERLAPPING] Yeah. It will never converge. And that kind of tells you that that entry in your feature vector, you don't care about that. If it is always 0, it doesn't matter what the weight of that entry is. So in general, you wanna have features that are differentiating and, and you're using it in some way. on the relationship between the features and the weights. Uh, they always have to be the same dimension, and what should we be thinking about that would make a good feature for updating the weights specifically, like- So, uh, okay so first off, yes, they need to be always in the same- in dimension cause you are doing this, um, dot-product between them. And then it's usually like hand designed, right. So, so i- i- it, it's not necessarily- you shouldn't think of it as how is it helping my weights. of 10, uh, if you're using the same feature extraction for both, how does that affect the generalized ability of the model, the agent? Yeah, so, so you might choose two, two different features and one of them might be more like so. So there is kind of a trade-off, right? You might get a feature that actually differentiates between different states very well, but then that makes learning longer, that makes it not as generalizable, and then at the end- on the other hand, you might get one that's pretty generalizable but then it might not do these specific things. to 0.25 and 0.75 then it kind of stays there, and you are happy. All right so, so this was just an example of TD learning but this is the update that you have kind of already seen. And then a lot of you have pointed out that this is, this is similar to Q-learning already, right? This is actually pretty similar to update, um, it's very similar, like we have these gradients, and, and the same weight that we have in Q- learning. of this idea of, I have this evaluation function, I wanna learn it from data, I'm going to generate data from that generated data. So, so that's what we've been talking about so far. And the idea of learning- using learning to play games is, is not a new idea actually. In '50s, Samuel looked at a checkers game program. So where he wa- he was using ideas from self-play and ideas from like similar type of things we have talked about. This idea of learning in games is old. People have been using it. In the case of Backgammon, um, this was around '90s when Tesauro came up with, with an algorithm to solve the game. And he was able to reach human expert play. And then more recently we have been looking at the game of Go. So in 2016, we had AlphaGo, uh, which was using a lot of expert knowledge in addition to ideas from a Monte Carlo tree search and then, in 2017, weHad AlphaGo Zero, which wasn't using even expert knowledge. all, like, based on self-play. Uh, it was using dumb features, neural networks, um, and then, basically the main idea was using Monte Carlo tree search to try to solve this really challenging difficult problem. So, in this section we're gonna talk a little bit about AlphaGo Zero too. So if you're attending section I think that will be part of that story. All right so that was learning and, and games. Now, so, so the setting where we take our games to simultaneous games from turn-based. Minimax sca- strategy seemed to be pretty okay when it comes to solving these turn-based games. But not all games are turn- based, right? Like an example of it is rock-paper-scissors. You're all playing at the same time, everyone is playing simultaneously. The question is, how do we go about solving simultaneously, okay? So let's start with, um, a game that is a simplified version of rock- Paper-Scissors. This is called a two-finger Morra game. Play the game and think about what would be a good strategy to use when you are solving this, this simultaneous game. We have these possible actions of showing 1 or 2. And then, we're gonna use this payoff matrix which, which represents A's utility. If A chooses action A and B chooses action B, this is the outcome. Make sense? So can you guys talk to your neighbors and play this game real quick? [LAUGHTER] How many of you are in the case where A chose 1, then- and B chose 1? Oh, yeah one. this value function, uh, over, um, over our state here. Now, we have this value function that is- do we- we shall use here, I'll just use here. That is again from the perspective of agent A. So, so I'm trying to like get good things for A. In this case it's not at the end [inaudible] ? Uh, yeah. And then this is like a one-step game too, right? So, like you're just playing and then you see what you get. write that pay-off matrix here. I'm gonna write A here, B here. agent A can show 1 or can show 2, right? If both of us show 1 at the. same time, agent A gets $2. If we show 2 at the same time,. agent A get $4. Otherwise agent A has to pay, so agent A. gets minus $3. So, so what is a policy in the setting? And, and then the way we refer to them in this case are as strategies. policies. So a pure strategy is just a single action that you decide to take. We have also this other thing that's called mixed strategy which is equivalent to, to stochastic policies. And what a mixed strategy is is a probability distribution that tells you what's the probability of you choosing A. So, so pure strategies are just actions a's. And then you can have things that are called mixed strategies and they are probabilities of, of choosing action a, okay? All right. game. So, so for this particular case of Two-finger Morra game, let's say someone comes in and says I'm gonna tell you what Pi A is. Policy of agent A is just to always show one. And policy of agent B is this, this mixed strategy which is half the strategy. Pi A chooses action A, Pi B chooses action B times value of choice A and B, summing over all possible a and bs. Okay, so let's look at an actual example for this. time show one, half the time show, show two. And then the question is, what is the value of, of these two policies? How do we compute that? [NOISE] Well, I'm gonna use my payoff matrix, right? So, so 1 times 1 over 2 times the value that we get at 1, 1, which is equal to 2. And, well, what's that equal to? There are two 0s here, that's minus 1 over2. 2 based on- based on this strategy, okay? Okay. So I guess this doesn't seem like we only have this one statement, so it's, we only take one action. Ah, so you might be interested in looking at what happens in repeated games. So that opens up a whole set of new questions that you're not discussing in this class. So, so the value is equal to minus 1 over 2. Okay? All right. so that was a game value. If we had more than one state, Would we have that for every single one. someone tells me it's pi A and pi B, I can evaluate it. I can know how goodpi A andpi B is, from the perspective of agent A. But with the challenge here is we are playing simultaneously, so we can't really use the minimax tree. Like, like think minimax. So agent B should be min- minimizing this. agent A should be maximizing this. That's, that's what we wanna do. So I'm going to go ahead and do that. assume we can play sequentially. So that's what I wanna do for now. So, so I'm going to limit myself to pure strategies. So maybe I'll, um, I'll come over here. I will just consider a setting- very limited setting and see what happens. And then each of them have actions of either or showing 1 or showing 2. They can show 1, show 1 or 2, right? If we do one- if we show one, 1, 1. player A gets what? $2? Is that right? It's 2, right? I can't see the board. Um, otherwise player A gets minus $3 if you have 2, 2, player A get $4. Right? So okay. So, so now if, if we have this sequential setting, if you're playing minimax, then player B is going second. Player B is gonna be like this one and in this case player A is going to be like. this one. What should player A do? Well in both cases Player A is gettingminus $3. If we have pure strategies, all right, going second is better. What's going to happen if we have mixed strategies? Are we gonna get the same thing? So, so that's the question we're trying to answer. So the value of the game, uh, would be, maybe I'll write it here.of the board, maybe up here. Okay. So what did we just learn? We learned, if we had pure strategies. That sounds intuitive and right. So far so good. the value of Pi A and Pi B. Pi A is already this mixed strategy of one-half, one- half, right? It's going to be equal to Pi- is this- yeah, actually. All right. So well, what is this equal to? Uh, that's equal to minus 1 over 2 Pi B of 1, plus 1 over Pi A of 1. We're gonna get $4, plus Pi B choosing 2 times Pi A choosing 1, and that's minus $3. So, if someone tells me, "Well, this is a thing I wanna do," I should try to minimize value of Agent A, right? So, so what I'm really trying to do as Agent B is to minimize this, right, because I don't want Agent A to get anything. So the best thing that I can do as a Agent 2 is to follow a pure strategy that always shows 1 and never shows 2. So that tells me that never show 2 and always show 1. deciding to go first. Player A is going to follow a mix- a mixed strategy. The way I'm writing that mixed strategy is more generally saying Player A. is gonna show 1 with probability p and is. going to show 2 with probability 1 minus p. And then after that it's Player B's turn. We have just seen that Player B, the best thing Player B can do is, is to do a pure strategy. So Player B could really like [inaudible] terms with the same then like Player B following a mixed Strategy. negative term, okay. So, uh, what's gonna happen is if you have 1, 1 and then, then that is going to give me 2, value 2, right? So it's 2 times p. Am I writing it right? 1 minus p times 3. Right. So with probability 1minus p, this guy is gonna pick 2. If this guy picks 1, you're gonna get minus 3, minus 3. So that is minus 3p. All right. So what are these equal to? In this more general case, Player A is playing first, uh, and is following a mixed strategy but doesn't know what p they should choose. Player B has to follow a pure strategy. And then under that case, we either get 5p minus 3 and minus 7p plus 4, okay? What should Player B do here? This is Player B and this min node. All right? So Player B is going to take the minimum of 5pminus3 and plus 7pplus4. here. So player is going to maximize that and also, I'm saying Player A needs to decide what P they're picking. So they're going to pick a P that maximizes that. Is this clear? [inaudible] Like these computations? Yeah, so these are the four different, uh, things in my payoff matrix. With probability 1 minus p, A isgoing to show me 2. I'm going to show 1, that's minus $3, times probability 1minus p. probability of 1 minus p. With probability P, A is going to show me 1. So that's why I'll lose $3, that's minus 3 times probability p. So what, what p should Player A decide? Uh, Player A should decide the p that maximizes this. Okay? All right, so the interesting thing here, is some line, right? With positive slope. This is 5p minus 3, let's say. And this minus 7p, plus 4 is another line. It's another line with negative slope. minimum of this? Where is going to be the minimum of this happening? Minimum of these two lines? Where they meet each other, right? Okay? So, so the p that I'm s- going to pick, isGoing to be actually the p, where, th- th- the value of p is where these two are equal to each other. And that turns out to be at, I don't know what it is, 7 over 12 or something. Yeah, so it's going to happen at 7 over12. And the value is minus 1 over 12. Right? So okay, so let's recap. Probabilities. So the thing A is deciding is, "Should I pick 1 with probability p and should I pick 2 with probability 1 minus p and what should that p be?" So, so what is the probability I should be picking 1? So that's what A is trying to decide here. So whatever A decides with p and 1 plus p ends up in two different results. Based on them, B is Trying to minimize that. So even in this case, where A was trying to come up with the best mixed strategy he could do, the. best mixed strategy A is doing is show, show a 1 with probability 7 over 12 and show 2 with probability 5 over 12. Even under that scenario, A is losing. A is always going to be either showing 1 or 2 and A is deciding which one. So A has to play a pure strategy because of that, right? Like the best thing A can do, is going to been a pure Strategy. So what if B plays first? So I'm going to swap this. If you are playing a mixed strategy, even if you reveal your best mixed strategy at the beginning, it doesn't matter if you're going first or second. So this is called the von Neumann's theorem. For every simultaneous two-player zero-sum game, with a finite number of actions, the order of players don't matter. If you, if you play mixed strategy,. your opponent is going to follow a pure strategy. Either 1 or 2 with probability 1. But with probability p, like, if we're doing like ordering, one of the two answers might- will come out. to 7 over 12 here, like these two values end up being equal. Equal, right? [inaudible]. [OVERLAPPING] Uh, none of them are actually equal. The reason that they end up be equal is you are trying to minimize the thing that this guy is trying to maximize. So no matter what your opponent does, like you're gonna get the best thing that you can do. So, so that's kind of the idea. All right. So let let's say I would pick a p that doesn't make these things equal. The key idea here is revealing your optimal mixed strategy does not hurt you which is kind of a cool idea. The proof of that is interesting. If you're interested in looking at the notes, you can use linear programming here. So, so let's summarize what we have talked about so far. Next 10 minutes, I want to spend a little bit of time talking about non-zero-sum games. In real life, you're kind of somewhere in between that, and, and I Want to motivate that by an example. dilemma? Okay. So, uh, so you have two players A or B. Each one of you have an option. You can either testify or you can refuse to testify. So you can- B can testify and A can refusal to testify, and I am going to create this payoff matrix. This payoff matrix is going to have two entries now in each one of these, these cells. And, and why is that? Because we have a non-zero-sum game. Because this was for player A, player B would just get negative of that. different players. So the von Neumann's minimax theorem doesn't really apply here because we don't have the zero-sum game. But do you actually get something a little bit weaker, and that's the idea of Nash equilibrium. So a Nash equilibrium is setup policies Pi star A and Pi star B so that no player has an incentive to change their strategy. So, so what that, that means is if you look at the, the value function from perspective of player A. from here. I start from A equal to minus 10, B equal to 0. Can I get this better? Can I make this better, or did I flip them I all? [NOISE] Okay. So let's say I start. from here. What if we start here? A has 1 year of jail, B has 1 years of jail. A has an incentive to change this now and get 0 years jail. B has an incentives to get 5 years jail instead of 10 years. Similar thing here. In general, there is at least one Nash equilibrium if you have a game of this form. In a collaborative Two-finger Morra game, it's not a zero-sum game anymore and, and you have two Nash equilibria. And then Prisoner's dilemma is the case where both of them testify. Of us are testifying and both of us are getting 5 years jail. Just kind of interesting because there is like a socially better choice to have here, right? Like both of me would refuse, like we would each get 1 year jail. There's a huge literature around different types of games, uh, in game theory and economics. Uh, but we have multi- we also have multiple game values from- depending on whose perspective you are looking at. If you're interested in that, take classes. And yeah, there are other type of games still like Security Games and or resource allocation games that have some characteristics that are similar to things we've talked about. And with that, I'll see you guys next time.equilibria.com.