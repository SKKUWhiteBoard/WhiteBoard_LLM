so in the next portion of today's lecture we're going to talk about how we can modify the policy gradient uh calculation to reduce its variance. In this way actually obtain a version of thepolicy gradient that can be used as a practical reinforcement learning algorithm. The first trick that we'll start with is going to exploit a property that is always true in our universe which is causality causality says that the policy at time t prime can't affect the reward at another time step t if t is less than t prime. that is it's the rewards from now until the end of time which means that it refers to the rewards that you have yet to collect basically all the rewards except for the ones in the past or the reward to go. The causality trick that i described before you can always use it you'll use it in homework two it reduces your variance. There's another slightly more involved trick that we can use that also turns out to be very important to make policy gradients practical and it's something called a baseline. grad log p by r of tau we multiply by r  where b is the average reward this would cause policy gradients to align with our intuition this would make policyGradients increase the probability of trajectories that are better than average and decrease the probabilities of those that are worse than average. We can show that subtracting a constant b from your rewards in policy gradient will not actually change the gradient in expectation although it will change its variance meaning that for any b doing this trick will keep your grading estimator unbiased. The average reward turns out to not actually be the best baseline but it's actually pretty good. different policy parameters you'll have one value of the baseline for parameter one a different value for parameter two. In practice we often don't use the optimal variance we just uh sorry we typically just use the expected reward but if you want the optimal baseline this is how you would get it all right so to review what we've covered so far we talked about the high variance of policy gradients algorithms. We talked about how we can lower that variance by exploiting the fact that present actions don't affect past rewards and we can use baselines which are also unbiased.