So, last time we were starting to talk about the sort of the general overview of what reinforcement learning involves. We introduced the notion [NOISE] of a model, a value, and a policy. Can anybody remember off the top of their head what a model was in the context of reinforcement learning? So what we're gonna do today is, sort of, um, build up for Markov Processes, up to Markov Decision Processes. And this build, I think, is sort of a nice one because it allows one to think about what happens in the cases where you might not have control over the world. Markov Decision Processes are the same as the Markov Reward Process except for now we have actions. So, the agent is in a state they take an action, they get immediate reward, and then they transition to the next state. The advantage of this is that each of the iteration updates are cheaper and they'd also will be some benefits later when we start to think about actions. Now, if we think about our Mars Rover MDP, let's just define there being two actions being A1 and A2. expected discounted sum of rewards. Now you might ask, okay well they- are they ever guaranteed to stop changing? And we'll get to that part later. We're going to get to the fact that this whole process is guaranteed to be a contraction so it's not going to go on forever. So the distance between the value functions is going to be shrinking. And that's one of the benefits of the discount factor. So if people don't have any more immediate questions, I suggest we all take a minute and then just compare with your neighbor of what number you get when you do this computation. There exists a unique optimal value function. The optimal policy for an MDP and an infinite horizon finite state MDP is deterministic. There are two choices for every state and there are seven states. The [NOISE] number of policies is |A| to the |S|. It's a mapping from states to actions so it's gonna be 2 to the 7th. We'll talk about- probably a little bit clearer to when we talk about contraction properties later. Um, any one want to take a guess of whether or not the optimal policy is always unique? I think there might be cases where it's not. on sort of the contraction operator. So, if an operator is a contraction it means that if you apply it to two different things, you can think of these as value functions. The distance between them shrinks after, um, or at least is no bigger after you apply the operator compared to their distance before. So how do we prove this? Um, we prove it- for interest of time I'll show you the proof. Again, I'm happy to go through it,. um, I- or we can go throughIt in office hours et cetera.