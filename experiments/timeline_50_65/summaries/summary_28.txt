The last lecture of the winter term of the computer science course at the University of Cambridge. The lecture was on the so-called backends or optimization engines or probablistic estimation techniques that were running kind of in the background solving the same problem and today we discussed these techniques. The last lecture was about the backends and optimization engines. The next lecture will be on the probablism estimation techniques and the techniques that are used to predict the outcome of a probabilistic estimation. The final lecture of this winter term will be about the Probablistic Estimation Techniques. I would like to give a very very of course brief short overview about front ends. One important aspect inside successful front ends is how to determine if a constraint is likely to be a correct one. We are still interested in avoiding to add roam constraints although we are still trying to avoid to add roaming constraints. We will be looking at how to use the front end in the next version of this article. For more information on front ends, please visit: http://www.codepaint.com/front-ends/ or go to the Codepaint website. we've learned that we there are techniques which can deal with outliers in the data Association so if we have from constraints they are able to deal with that. But of course we are still interested on adding as few false positives as possible and so this is what we are trying to do. We are still trying to make sure that there are no false positives in our data. We want to ensure that we are not adding as many false positives to the data as possible. We don't want to add any false positives at all. is related to the front end and what my goal is for today I will introduce three kind of small front end systems on a very abstract level. Just giving you the idea on how they work with different sensors and then in the second part of this talk today we will go into more detail on how these systems work with each other. We will then go into a discussion on how we can use these systems in a variety of different ways in the future. We hope to see you at the end of the talk. I would like to stress on what kind of conditions should such a constraint fulfill or the the metrics of the local environment fulfill. I would say ensure to be out layer free but to reduce the probability that this is actually an outlier mesh. It would be a good idea to ensure that the mesh is not too large or too small. It's a good thing to have a large mesh, but it's also a bad thing if it's too small or too large. It can lead to a lot of problems with the mesh. so we're talking about front ends today and what we learned so far we have our graph so you've seen this picture a few times during the course here that moved through the environment. We have our nodes and we have constraints between those nodes. And we have loop clothings. So we've seen a few of these pictures during this course here. We're going to be talking about the front ends of front ends for the rest of the week. We'll be showing off some of the work we've done on front ends. when the robot observes the same part of the environment and so far we always assume these constraints are given so these errors here we assume to be given. Today we would like to look at the case how do actually generate those arrows so these these errors are not given to the robot. We will look at how to generate the arrows in the next section of the article. We hope this will help you understand how to make the arrows look more realistic in your robot's eyes. Back to the page you came from. These constraints which are successive constraint that should be kind of clear that's can be simply odometry information or the result of an incremental scan nature. So it's something that you have seen already here in the course that you know how that works. But the important ones actually those actually those constraint that are successive. That's can't be just odometry data. That can't just be just a result of a scan. It has to be a constraint that is successive. It can't simply be odometry info. It must be successive. loop closing constraints so or even if there would revisits the same place kind of look constraints which you can it say informally called localization constraints and the robot moved in an existing part or in a part of the environment it has seen so far and adds new constraints.loop closing constraint so or. even ifthere would revisit the same places kind of looks constraints which can be used to make the robot move in a certain way.loop close constraints so the robot moves in a specific way. between its current observations and the observations that the robot obtained in the past. This is the two main components of the slam system so our back end and the front end so far we've mainly looked here into the back end of the robot. So far we have mainly looked into the rear end. We have also seen this figure already in the the past so I also have seen this Figure in the Past. So this is the Two main components. of the Slam system. So our backEnd and the Front End so far We've mainly Looked at the Back End. graph optimization so the raw data comes in the graph is constructed the constraints are generated. The graph is given to the back end that back end optimize the graph and returns a new node positions back to the front end. The front end uses this information together to create a new graph. The back end then returns the new graph to thefront end to be optimized again. This process is called back-end optimization. Back to Mail Online home. Back into the page you came from. Follow us on Facebook and Twitter. with the new centre information to generate new constraints oh and so far we we have been looking to this part and today we would like to look into this second part over here. So that's that's the goal so how do we get those constraints we getthose constraints. We will be looking at the second part of the project in the coming days and weeks. We are going to be looking into how we get the constraints we need to make the project successful. We hope you will join us for the next few days as we work on this project. to begin by matching observations so we have different observations depending on what platform that can be. For every sensor of course there's a different way of obtaining those constraints and those constraints may be different for different cameras. For example, a stereo camera can be a laser rangefinder or different types of sensory modalities and for every sensor there's an different way to obtain those constraints. For more information on the project, visit: http://www.cnn.com/2013/09/29/science/science-and-technology/science. take into account what the sensor actually sees how kind of unique is the data that the sensor generates for specific area. or it's a look all corridors exactly the same if you only rely on laser range data and the corridors all look the same from the Rays. Take into account how different each corridor is from the others. How unique is each corridor? How different do they look from each other? How unique are they? How are they all different? How do they all compare? What are the differences between them? range data point of view then we may take that differently into account. If we have a corridor where we have. a lot of whatever unique markers glued to the to the walls and with the camera. and can perfectly observe those markers so depending on what we see. then we might take that into account as well as other data points of view. We may also take into account what other people are doing in the corridor as we go along. It's a bit of an experiment to see what the results are, but it could be very interesting. what we assumptions we make about our observations this tarz can be very hard or not that hard. What typical approaches are is if we go for laser range data is then scan matching. We really take our raw laser range information and try to align those different. We try to take that information and combine it with other data to get a more complete picture of what we are looking at. We then try to use that information to create a map of the tarz. We can then use that map to try and get a better understanding of what is going on in the environment. laser scans that we obtained at different points in time and if we can match them well I'm gonna say okay this part of the environment I see at the moment looks very similar to the part of. the environment that I have seen so far in the past and so on. If we can do that, we'll be able to get a better idea of what the environment is like in the future. We'll know more in the coming months and years about the future of the project. Trees have been extracted in this case from the laser range data so the trends of trees and every trunk of tree was seen as.therefore it is quite likely that we are at the same place other approaches use features for example we had those the Victoria Park where trees had been extracted from the lasers data. There is no way of knowing how other approaches will work. There are no plans to change the way the data is collected in the future. The results of the study will be published in the next few months. one feature or one landmark and the robot map those landmarks by checking for say fitting circles into the laser range data. Whenever it found a good circle say that's likely to be a chunk of a tree and use it as a landmark the third class of approaches are used to map landmarks. The robot is still in the prototype stage but is expected to be fully operational by the end of the year. It's not clear if the robot will be used for mapping in the U.S. or other countries. uses feature descriptors most popular ones are for example sift and surf. These are two features which you can extract from image data and which kind of describe the local surrounding of the place where this descriptor is computed. If you have a lot of those descriptors in an image, you can use them to extract features from the image. For example, the image can be used to extract the sift feature and the surf feature to extract surf features. For more information on how to use these features in your image, go to: http://www.cnn.com/2013/01/17/technology/features/sift-and-surf.shtml. image you have a typically quite good description of the image so you can just match those descriptors and based on those descriptor identify if two images are recorded from the same place. This is not free of errors and free of flaws but you can match surprisingly well. If you match two images you can identify if they were taken at the same time. If two images were captured at different times you can also identify if the images were taken in different places. If they were captured in different parts of the world you can even match the location of the images. large databases of images using those descriptors. These are three popular techniques or sensor information that front-ends use in order to make the data Association and way we actually look into those who is very short example examples during this course today okay so you're typically in the situation. So these are threepopular techniques or sensors that front end's use to make data Association. So you can use these to make your own data association and way you look into your data. So this is just a short example of what you can do. we say okay the robot is currently let's say at location a and let's assume the blue circle over here is a sensor range of what the robot sees. We know in our say graph a post graph that we have built so far we have two other places b1 and b2 so these are places where the robot has been in the past. I can estimate where those locations b1 & b2 are relative to the current pose of the robot which is here indicated with a so you can say okay given I'm here at the moment. moment and that's my sensor range I can compute where are those other pulses so in this case B 1 and B 2 just two examples could be more obviously. Then I can also estimate what is the uncertainty of those poses B 1 & B 2 relative to each other. And that's what I'm trying to do in this example. It's a very simple example but it could be a lot more obvious. It could be something like B 1 + B 2 or B 1 - B 2 - B 1 or B 2. a do that by eliminating the note a from my linear system and then inverting the resulting Hessian and looking to the main diagonal blocks this gives me the uncertainty here indicated by these dashed lines where b1 is relative to a and the same here where is b2 relative to b1. The uncertainty is indicated by the dashed lines in the image below where b2 is relative and b1 isn't relative to the other two blocks in the same way as b1 and b2 are relative to each other. to a so based on this information I know I can never have an estimate of given my current pose where's b1 where'sb2 together with The Associated uncertainties certainty estimates. okay what I then do I kind of can extend the the uncertainty ellipse by kind of the visibility. to a so. based on the information I've just given you, I can't really give you a good estimate of what the future is going to look like. okay. What do you think? Let us know in the comments. area of my scanner and say okay say if the robot was sitting standing over here that's the area it may have observed. Then I can simply check is the current sensor range is there an overlap between the current sends a range and the possible observation that I obtained? That's what I do. I look at the sensor range and see if there's an overlap. If there is, then I can see what the robot may have been doing at the time of the observation. I can then check that the robot is doing something in that area. from b1 on b2 and if I found places for which this holds and this is the case for b1 because let's say if the robot in reality was sitting here and say these are as a two sigma bond then you can say with 95% probability p1 sits in. From b2 on b1 and from b1 to b2. from b2 to b1. from a to a and from a b to a. from the top to the bottom of the screen. this ellipse and they say it was sitting here this was a sensor range so there's an overlap in the sensor range I may see the same feature. I can argue that I may need to check if a and I can argument that I might need to look at the feature descriptors or whatever I'm looking into to see if it's in the same range. I may be able to see it in a different way if I look at it in different ways. I'm not sure if I can do that. B are seeing the same part of the environment in order to look for the closing constraints in contrast is that if I look to be - I can say okay the uncertainty of b2 extended with the visibility range of my of my sensor by no means overlaps with b2's visibility range. B2 and B.B. are seeing different parts of the same environment, but are looking for the same constraints in the same place. The uncertainty of. b2 is extended, but not as much as b2.B is seeing. a so it's extremely unlikely that a can match b2 and typically you ignore those cases so if you save whatever you take the two or three sigma bound of your estimate and then say everything which is out outside the threeSigma bond. "I'm not looking for potential loop. I'm just trying to find a way to get to the end of the line" "I don't look for potential loops. I just try to find ways to get from point A to point B" "If you're looking for a loop, you're probably looking for the wrong loop" closures and so in this case in this example you would check if what the robot sees right now in a matches with what the. robot has seen here in in b1. If you find a match you may accept that but you don't even look into b2 so. You would check to see if what you are looking at right now matches what you have seen in the past. If it does, you would accept that and move on to the next section of the image. You wouldn't look at b2 or b3 or b4 or b5. based on the current uncertainty of the robot and the of the Koran certain VD and certainty estimate that the robot has at the moment about places where there's been in the past. It makes a decision should I look for loop closing constraint or not so in this case. In this case it made a decision whether I should look for loops closing constraints or not. In the past it has made decisions about loop closing constraints based on where it was in the history of the loop. In. this case the decision was made based on the. current uncertainty of the robot and the of the Koran certain VD and certainty estimate that the robot had at the moment. the views may overlap so really need to inspect my sensor data and see if I find correspondences between what I have seen in b1 and what I've seen in a but here's the fuse basically cannot overlap or the probabilities so extremely small that I kind of ignored that. The fuse basically can't overlap. The probabilities are so small that the probabilities were so extremely tiny that I just ignored that and went with the flow. The views cannot overlap. the views may overlaps. so reallyneed to inspectMy sensor data. and seeIf I find Correspondences between. what I'm seeing in a and whatI've seen. in b2. any question about this idea so far because we will need that later on okay okay how to obtain the uncertainty so how do i compute this uncertainty here before I said okay I can do that by estimating the uncertainty relative to the current pose of the robot which I will do later on. okay how do I get the uncertainty here? I will get that back to you in a minute. okay okay so now we have a good idea of what to do. Now we can go on to the next part of the story. was here indicated with a which can where I could I can obtain by inverting the hessian. In practice this is a pretty expensive operation so you actually want to try to avoid inverting this larger matrix. You can do an approximation what actually happens with a larger matrix, but it's still pretty expensive to do it in a way that works in the real world. For more information on how to get the same result, visit www.nukethefuture.com. For a more detailed look at the full project, visit the NuketheFuture website. most system in practice do to do that more efficiently and this is a thing by they say okay we simply ignore the loop closures for the moment just for estimating the uncertainty here. You just do what is also called Dijkstra expansion so we expend propagate the uncertainties. That's what we do in our system. It's a way to get around the fact that there are a lot of loops in a system. We just ignore them for the time being. And then we expend them to get to the next stage. through the graph so where the robot is right now I assume a zero in certain and then I say okay I'm traversing through the next edge the edge which generate the smallest increase of uncertainty to the next node and it go on and always increase the uncertainty until the robot reaches a certain point in the graph. "I assume azero in certain," he says. "Then I sayOkay I'm  traversing through the next edge. The edge which generates the smallest increase of uncertainty to the next node and it goes on and on" I reach all the posts I'm interested in looking into but this does is ignores the loop closures so the uncertainty estimates are too big. You can still argue that okay uncertainty estimates I get are too large but I can compute this extremely efficient and I may inspect it. I may also inspect it and see if I can find a way to make it even more efficient. I'm not sure how to do it yet, but I may be able to. I'll let you know when I find out. a few places too much but I should get all the places which I need to inspect in order to make sure I find the course with whatever 95 percent probability this is what is done. Just as a side note what's done in practice to what inverting this would look like in the course of a day. It's a lot of work but it's worth it in the end. I've had a great time and I'm looking forward to the rest of the year. I'll keep you posted on how it goes. matrix age there so far we really tried to explicitly invert it used whatever Spurs to rescue decomposition or whatever techniques we used to not inverted it explicitly. What actually tries also to invert this matrix in the data Association step so it's just kind of a side of the data.matrix that we're trying to get rid of. It's just a side that we don't want to leave behind. We're not going to leave it there. We want to get it out of the way. a side note and what we do then we just check which areas overlap and if there's an overlap in the area between the current scan of the robot and the sense range of the Robot. At that point I say okay this may be an issue. We just check to see if there is an overlap and then we go on to the next scan. It's a very simple process. We don't know if it's going to be a problem or not. We're just going to have to wait and see. be a match this may not be a match okay next question is how do I determine is there a matter or not so let's say we have assumed okay my current observation matches with the observation whatever 20 FC the place have seen 20 minutes ago. How do I know if there is a matter? How can I determine if it is a problem or not? What do I do? What should I look for? What can I do to find out if it's a problem? What am I looking for? Where do I look? What is the problem? proceed and this strongly depends on the late data laser data or the on the data. Here is one example of a very simplistic front end which will try to identify constraints which uses iterative closest point so scan met a former scan matching and tries to find a matching. This is one of a number of ways that the front end can be used to identify data constraints. It can also be used as a way to find data that has already been scanned by the back end. For more information on how to use the front-end, visit: http://www.laser-scanning.com/. For more on the back-end of laser scanning, visit the Laser Scanning Project. match so how could we do that so for the first thing we do we estimate the uncertainty of the other poses relative to the current pose of the robot. So that's what we discussed so far given behind that we take those posters which are in the air we take them out of the air and put them in the robot's hands. And that's how we get the robot to pose for the match. And so that's the way we do it so far. And then we go on to the next pose. just made just a sample poses in in in those areas so just read we draw horses so if I go back to my example I take a okay I select okay B is a good is a a good potential match so just sample random locations here close to B. Just made just made an example of a horse being drawn in a certain area of the city. Just read we drew horses so just reading we draw Horses so just reads we draw horse so just Read We draw horses. If you have any questions, please feel free to email me at jennifer.smith@mailonline.co.uk and what I then do from every of those sample points I apply skin matching in this case the iterative closest point algorithm so I try to align the current observation with the observation or with a local map that has been built in this place B. and then I then I apply a skin matching algorithm to each of the sample points. I then try to alignment the current observations with the current map or with the local map in the place B and then apply the skin matching algorithms to each sample point. just see okay how well I'll do those observations align or how well does my current observation align with the local map I've built so far. let's cook looking into the sum of the squared distances between those the corresponding end points. those end points which I regarded as corresponding as corresponding. Let's see how well they align with our local map. We'll look at the square of the distance between those two points and see how they align. We're going to try to get the best possible results. points and if this is above a certain threshold I accept the match. That's a constraint that's a very simplistic technique of course you can do much better but this is kind of the basis the basics of most laser-based front-facing cameras. "It looks the same here I go," he says. "That's it looks theSameHere I go" "I'm going to take a picture of it and then I'm not going to look at it again," he adds. "If it's the same, it's a match. If it's not the same then I don't take it." ends today of course you may have different techniques please don't take a stupid threshold. You may use things like ransack you may even use better initializations you may use better matching algorithms. you may build local maps and try to match local maps in order to take more information. ends today ofcourse you mayhave different techniques Please don'ttake a stupid thresholds you may using things like Ransack or other methods to get your data. you might use betterInitializations or better Matching Algorithms. The cordilla Mallis boils down to that again you can extend to improve all those aspects here but this kind of the basic decision based on a range data are two places or given an initial guess what's the transformation of the locations from.into account but in the end the cordilla mallis boiled down to. that again it's based on the range data and it's a very basic decision. It's a decision that can be improved on a lot of different ways but this is the basic one. which those two scans have been taken okay where do you see problems here that this were perfectly aware would you see the failure points for this approach. If your customer and I've proposed you this solution what would be maybe your argument against that would you buy that. What would you say to your customer if you were to offer them this solution? What would be your argument for or against that? What do you think would make you want to buy that? Tell us what you would say to you. this approach which I propose you good what do you see where do we see potential problems of this approach from. given what you have seen so far absolutely correct. so if we have let's say asymmetric or partially symmetric scenes so a corridor which I can match this way. this approach is a good way to go about it. This approach is the way I would like to see it used in the future. I think it would be a very interesting way to look at it. or that way it's one example where this approach would say our perfect match great I got it looks perfectly every single endpoint matches perfectly. I'm done maybe not that's one point which may be a problem. And so we're so if you kind of it's an absolutely correct observation. It's an example of where we could have used a different approach. It could have been a different way of looking at the data. It would have been more focused on the data rather than the data itself. ICP is a combination of the iterative closest point algorithm and the initialization so ICP depends on the initial guess and it just finds one solution and may be wrong. This can happens where would you identify this problem in this list of approaches so where's where's the problem over there yeah so it's a combination. of the iterations and the initial guessing. It just finds a solution and it may be incorrect and may not be the right solution. this can happens. so where is the problem? so where has the problem been located? so this is where the problem has been located. be the right solution but as you pointed out in your example it may be the wrong solution so I see P is sensitive to the initial guess and as a result of that we may end up in a local minima. So in something which looks like a match it may not be the correct solution. So P may be sensitive to an initial guess but not the final solution. As a result, we may be able to find a solution which is the right answer for the problem at hand. but in reality is not a match other things we may identify is how do i sample possible locations where the platform can be if this is kind of the uncertainty is a very large area. I may need to sample a lot of different poses in order to do what I need to do. But in reality it is a large area and there are a number of different ways to do it. It is not possible to predict the exact location of the platform at this time. It may be difficult to find the right location for the platform to be placed. that efficiently so how do I find actually even a good initial guess and so these are typical problems that those approaches has or ICP is sensitive to the initial guess we have local minima we may have an inefficient sampling strategy. So how do i generate possible initial guesses so that I can get a good estimate of what I'm looking for? How do I get an initial guess of what to look for? And so on and so on. We can see some of the typical problems with these approaches. for my ICP iterations just accepting something based on a threshold as always something which some one might dislike and also as you said correctly ambiguities in the environment is a critical point for that. So we're looking to those two things and I would start with that with just the ICP. For more information on how to get your hands on a copy of this article, visit CNN.com/soulmatestories. For confidential support on suicide matters call the Samaritans on 08457 90 90 90, visit a local Samaritans branch or click here for details. showing you three different examples of systems that we have built here in Freiburg. Some of the mapping techniques we developed here have been used to at least tested. We because this is not aFreiburg vehicle but some of the maps we developed are used in this vehicle. We are not using the same mapping techniques in this car as we did in the other vehicles. We have used different mapping techniques to test the different vehicles that we are using in this video. This is not the first or the last time we are showing you some of these different examples. on that car so this is a pioneer a two robot which has a two d-day the rangefinder sitting here and sits on a pencil unit so it moves always like this song so it's called a nodding laser and this way generates 3d data you get 3d information about the scene and then it tries to build sorry a 3d map of the environment using this technique okay so let's start with this one the robot provides odometry and the later range scanner a 2d laser range kind of sitting on a on a pen tilt unit we may. assume the robot is standing still when doing it skins maybe it's driving while it's taking its games driving may make it a little bit more complicated if you have not a good odometry estimate. While driving you don't observe the same part of the environment and this makes it more difficult to work out what is going on in the robot's head. If you have an idea of what the robot might be doing, contact us on 0203 615 9091 or email jennifer.smith@mailonline.co.uk. it hard to make any incremental alignment and the maps you make it out of that look like this so what you can see here red means non drivable areas yellow means drivable area so this is building 79 this is the the street here it goes down to the street. So what you're looking for is non-drivable areas and non-driveable areas. So this is what we're looking at here is non drivable areas and non direvable areas. And we're trying to make a map that looks like this. parking lot so this is an example of a map that you might obtain with this approach how does it work okay we have the robot it uses odometry information so it moves forward takes a 3d scan and what we can then do based on the dormitory information and. What we can do is create a map of the parking lot based on what we know about the dormitories and what the dorms are like. We can then use that map to create a 3D map of that area. based on our local 3d scans we can actually build a map we can build a local 3D map of the scene. This this map can be either a point cloud accumulated over multiple pulses multiple scans it can also may also be represented by a 3d grid structure or a 3D grid structure. We can also use this data to build a 3rd party map of a scene. For more information on how to use this code, visit www.videotv.com/videogame. by a more efficient data structure such as an octree or multi-level surface maps or other map representations that you may have seen introduction to model robotics or that you will hear next term in introduction to mobile robotics. Hopefully how the local environment representation exactly looks like doesn't really look like a good representation of how the environment is in the local area. We hope this helps you understand how to make a model of a local environment that you can use in a mobile robot. We'll be talking more about this in the next few weeks in our mobile robotics course. matter at that point but that's one way that we built kind of a rigid structure which contains the 3d information that's kind of the important part in here. Then the robot continuous odometry information drive five meters forward takes another range scan and builds another local map and then goes on to the next part of the mission. The robot then takes a range scan, builds a local map, and then drives five meters further. It then goes to another part of its mission, building a new local map. then we can do is we can take those two local maps and try to align those twolocal maps it's typically Nanban. Depending on how many scans you integrate either you could take these skins so if it's just kind of one 3d scan you would although in Nanban you would have a different look. It's just a little bit of a different way of looking at the game. We're trying to make Nanban look a bit more like the other maps in the game so it doesn't look too different from other maps. practice it's a number of 2d scans for matching it's much easier to have one 3d scan you can either take one single 3d scans you may even combine multiple 3D scans. If you have a good pose estimate between those individual scans and sometimes it's easier to match full you can combine multiple scans to get a more complete picture of the same person. You can also combine multiple 2d and 3d images to get more of a sense of what the person looks like. maps like versus individual scans this depends on the data that you have and how many ambiguities you may find your environment. If you have a local blast slightly bigger view so you really match a map against the map that may be that may been easier or may be easier to match. This is a guide to how to match maps and individual scans in Quake 3. For more information on Quake 3, visit the official Quake 3 website or the Quake 3 official YouTube channel. For the latest Quake 3 news, check out our weekly news updates. have less local minima so we match those Maps and get a six degree of freedom constraint in this case XY. Your patron roll so six dimensional constraint and then we this is actually one of those constraint here. Then we can accumulate all constraints and then do a map with all of them in it. The result is a map of the world with all the constraints applied to it. We can then use this map to create a new map with the constraints we've added. The resulting map is called the Map of the World. graph optimization and obtain a local map so this also a parts of this is building 79 building 51 52 the Mensa building and of course some parts here the green area which robot hasn't seen how does it align those two scans of these are two examples of two.graphs of two buildings that were used to create the map. The map was created by using a series of images of two different buildings in the same building. The maps were then combined to create a larger map of the building. scans of two of those 3d scans and which have been taken from different positions and that's kind of an iterative alignment procedure very similar to ICP that aligns those scans so the approach here takes additionally into account it kind of separates areas that such is just a simple way to do it. The approach here also takes additionally taking the position of the scans into account. It kind of separating areas that are just asimple way todo it. It's just a very simple way of doing it. say classification or segmentation of the environment like and if I wall think that stick out and first met results against each other and only in the end it does the alignment of all points because you're less likely to end up in a local minimize so if you let's let's so if I let's say classification or classifies of an environment like. If I wall thinks that stickout and first meeting results againstEach other. If we let's. say classifying or segmenting of theenvironment like. and if we wall. think that Stick out and classifies an area like. take the tree the pole and the walls and match them first you if you can separate those part of the skin reliably he typically are less likely to end up in a local minima and then in the end you get this kind of alignment so this the iterative. If you can get the tree to align to the pole then the pole to the walls, then the walls to the tree then the tree. This is an iterative process so that you get the best possible alignment of the tree and the pole. If the pole is aligned to the wall then the wall is aligned with the tree so that the tree is aligned. procedure nice based on ICP which aligns those scans. Then I can do this for a large number of individual scans and then I end up with a map like this. So you can see you may observe some of those small stripes over here so these kind of the kind of a map of where they were taken. It's a nice way to look at them. I think it's a very good way to see them as well as other ways to do it. I hope you like it. darkest stripes these are simply small alignment errors of these individual maps they can they can see kind of small steps over here here that was also probably an alignment error which simply leads to her step which was let's say bigger than and all five centimeters in the ground. darkest stripes  are simply small alignments errors of the individual maps. they can be seen in the map below. The map below was created by the team at the University of California, Los Angeles. The team was able to capture the image using a combination of 3D and 3D mapping. and therefore it's classified as not reversible anymore and therefore everything is red over here. So what do you see here for example other the bikes which are parked over here and the individual trees here something it's like to have gone wrong or maybe there was some copy. And so what do we see here other than the bikes and trees here? What do you think? We'll let you know. Back to Mail Online home. Back To the page you came from. Click here to go to the gallery. this was maybe a car coming here from the parking lot but this area over here seems to be aligned maderas or slightly misaligned skins but that's the way you can actually use 3d data to build a map of the environment. The next example this is an autonomous car. This is anonomous car coming from a parking lot. This was an autonomous vehicle coming from the Parking lot. this was maybe an car coming  from a Parking lot coming from another side of the street. this is a car going from a different part of the city. It has a 3d scanner in here on top so that's a valid I'm 3D scanner which is rotating and generate 3d point cards at a very high frequency. You can use a very similar technical basically exactly the same technique in order to align different pulses of the scanner. It can be used to create 3D point cards using a similar technique. It's a very similar technique to the one used to generate point cards in the 3D printer. It uses the same pulses to generate the 3d cards. wiegel here you typically use an inertial measurement unit to estimate the relative movement of the of the vehicle. The cars of cost driving while the skin is taking so that you kind of really get more less accurate and points of every single laser beam even while the laser beam is being fired, says the company. The company says it has no plans to use the technology in the U.S. in the near future, however, the technology could be used in the future in a variety of ways. vehicle is driving making again put that into a local map and do apply exactly the same strategy as before. If your initial estimate is better if your laser data is a high quality laser data then you get an example like this. You have seen this picture already this. If you want to see more of the same you can go to www.laserproject.org.uk and click on the 'Laser Project' button. You can also go to the Laser Project website to see other examples. is a parking lot or a 3d model of a parking lots where yellow again means drivable areas and red means non drivable area. Then you can actually use this this map over here in order to localize the vehicle in order for autonomous driving that was actually work to take place in the parking lot. You can use this map to find out where your car is parked and how much room it has in the lot. It can also be used to find parking spaces that are not drivable and how big they are. of China that he built or he realized autonomous parking using this map representation which was built here for that vehicle in that parking lot. It's actually the picture of a parking garage and so you can see even though it's here three floor building and the corresponding 3d map representation, it's still a garage. It was actually a three-story building and a corresponding 3D map representation was built for that car in the parking lot and so it's actually a garage and not a car. model where the car and how to heal it's a trajectory of the car doing the mapping process so stop it from here stop from here and build a map. So these were kind of in this case 1600 local 3d maps and this was this in this example. It was a car that was being driven by a driver that was driving a car. It's a vehicle that is being driven in a way that is going to heal the car. The car is moving in a direction that is healing the car, so it's going to stop and start. done with the grid for 20 by 20 centimeter grid cells and this by lining those grid cells you can actually get maps off and say this quality that's something you can expect to get with this technique. okay how does it look like if we look into cameras and. what do we look like? What do we see? What are we looking at? We're looking at a map of the United States. We're showing you how to get a map from the U.S. using a technique that's been developed by the University of California, Berkeley. maybe flying vehicles if you go to flying we go sweetie they have the problem that you have weight limitations. Whatever those vehicles need to lift should be as light weight as possible. The more light weight sensors are to be the the more crappy or the quality of the sensors. Maybe flying vehicles should have as little as possible to lift as possible so they don't have to be as heavy as possible and can be more maneuverable on the ground. maybe flying vehicles could have as much as half the weight of a car to lift. System was flying on the prototype for a helicopter so never made it to the blimp in the end this was just a self assemble stereo camera system with two webcams assembled in a stereo setup and a small IMU. There's no way of knowing if a camera is working or not, but it's always fun to try and see what's going on in the background. It's always interesting to see what the camera is looking at, and how it's reacting to the environment around it. an initial inertial measurement unit and one of the advantage of this system is it gives you also the gravity vector this quite accurately at a high frequency. If you know the gravity back door you can eliminate already two of the six dimensions from your state space because of the gravity. The system is called the Gravity Back Door System and it was developed by the University of California, Los Angeles, in the 1960s. It is based on the idea that gravity is a force that can be controlled by the human body. the roll angle and the pitch angle can be determined but just by knowing the gravity so you have only one angular component and XY that that you need to estimate. Adding this IMU to the tourist error system is is highly advantageous because it removes two degrees. The IMU can be used to calculate the roll angle, pitch angle, and angle of descent. It can also be used as a guide to determine the pitch and roll angle of an object. It is also used to estimate the roll and pitch angle of a vehicle. of freedom they estimate quite accurately you could even use this to estimate the the movement of the camera or get an estimate of the of the movement. But this was sorry not used it actually here in this example at least so these are the camera in the example below. The camera is in the middle of the frame in the image below. You can see how the camera is moving by looking at the viewfinder in the top right corner of the image. The viewfinder can also be used to see how far the camera has moved in the distance. I'm interested to really get so it's kind of a webcam off 2008 I would say it's about 2007 yeah these times this was actually the work of past times DITA his master thesis when he did that and these are the the piles you can see these releases of. I would like to get so much of it. I'm interested in really getting so many of them. I'd like to reallyget so it'm kind of an webcam off2008 I wouldsay it's About 2007 yeah. hires in our building these are the positive front of our building and the grass area so if you take the stereo camera pointed downwards and walk over the green these are images that you get. If you get if you see this image view you may already be at the building. You can also see the other side of the building if you point the camera downwards and look over the grass. You may already at the second floor or the third floor if you are looking for the fourth or fifth floor. The fourth floor is the sixth floor and the seventh floor. The approach works surprisingly robust and is able to estimate the trajectory as well. The first thing it does it extracts features from the camera images in. It guess that it's maybe hard if you travel over grass for extended periods of time nevertheless the approach work surprisingly robust. The approach is based on the idea that the best way to get a good view of a landscape is to look at it from a distance. It is also able to predict the trajectory of a vehicle as well as its speed and direction. this case these are surf features what these sir features actually provide is a local description of the scene of a small noise a scene of the small part of the image. If you see every of those for every of the points here one of those is a surf feature. This is a look at some of the most popular surf features on the internet. These surf features can be used to help you find surf spots in the UK. These features can also be used in the U.S. to help people find local surf spots. descriptive Alice is computing their computer kind of from a local window around them doing some local operations and returning typically a 128 dimensional vector which is a local description of the area over there. In reality typically is in two step process first so-called key point detector is executed and then local operations are carried out on top of it. The result is a 128-dimensional vector that can be used to describe an area of the computer. For more information on how to use Alice, visit Alice.org. which tries to find could have let's say call it stable areas in the image. If you have two similar images of the same scene and you want to compute those descriptor values at the same position therefore you first run a keep on detector. They typically look to stable areas of the image in the same way as a keep-on detector would look to areas in an image that are similar to each other in a similar way. It's a very complex process. It takes a long time to get the results. the whatever corners in the image or am edges and compute those features at corners or at flops or their different ways of how this can be done. Then for every of those detected points one of those feature descriptors is computed and what the approach then does it. The approach is to look for features at the corners or flops of an image and then compute them at the edges of the image. For each of those points, a feature descriptor is computed for each of the detected points. This is the approach to look at corners in an image. says okay I'll baste on for those points I try to estimate where they are on the 3d space I'm just taking the staring from stare information into account and based on the position of my stereo camera I tried to build a local model of the surrounding so what so what.saysOkay I'll Baste on. I try not to stare too much. I'm taking staring information from staring into the camera into account. Based on the location of the camera, I try and build aLocal model of. the surrounding. we want to estimate is the X Y that and three angles your roll pitch and yaw so if you have this your camera looking forward this is roll this is pitch and this is your and so as I said before by adding this IMU to the to the camera we can get a better idea of the roll and pitch of the camera. We can then use that information to estimate the roll of our camera. This is how we get a roll estimate for a camera. It's a very simple thing to do, but it's very important. stereo camera and the in this case the the camera is looking downward to me of the IMU on top we know the gravity vector and so this eliminates the roll directionally cursus will change thegravity vector and the pitch direction because it will changeThe gravity vector so. it will cause the camera to roll in the opposite direction. The camera will then roll in a different direction to change the gravity. vector. It will also change the camera's pitch direction so. the camera will roll in that direction too. by knowing the gravity vector I kind of get rid of the roll in the pitch and this reduces my problem from six dimensions to four dimensions for every node or for every camera pose. This makes my life easier and therefore it is kind of exploited here so based on. So based on knowing thegravity vector I sort of get. of theRoll in the Pitch and this reduction of the pitch reduces my. problem from 6 dimensions to 4 dimensions which makes my. life easier. This reduces my problems from six dimension to four dimension for every nodes or for each camera pose which makes. my life easy. on that based on that I can buy a triangulation compute where are the points in the 3d space give me that I see the points. I only need to know two features two of those descriptors with their corresponding 3d coordinates in the image in. I can then use that information to create a 3d map of the scene. I use this map to create 3d maps of the scenes I want to photograph. I also use the map to work out where the points are in the scene and how to get there. order to estimate the relative pose of the camera so in practice looked like this so this was the camera pointing downwards they say this these are images which and the red dots are positions for which those features have been computed. These are my features in the map let's. look at the map to see what the map looks like. The map shows the position of each of the features in relation to each other and the positions at which the features were computed. For more information on how to use the map, go to: http://www.cnn.com/2013/01/29/technology/features/top-of-the-range-cameras-in-pictures. say this is the current image that our observe saleh bin looks like. transform it over here and then you can take this image and add it to you map or add a constraint between the position where this camera image has been and the current camera. If you want to add an image to a map, you can do this by adding a constraint to the position of the image you are trying to add to the map. For example, you could add the image of the object you are looking at to your map of the world. image has been taken and the images have been taken at previous points in time. Practice these approaches in order to be efficient for example. What they do is you first try to find so but you have you have your image you extracted your search features from your image. You then use these search features to find the image you are looking for. You can also use this technique to find images that have already been taken. For more information on how to use these techniques, visit the Google Images website. image and you have a database of all the sort of features that you have seen so far in the past what is the first thing you do is you try to make a nearest neighbor query in the descriptor space to try to find the best matching descriptors over image. Image. Image is a type of image that can be used to search for nearby features in a database. Image can also be used as a way to search the database for features that have been added to the database in the recent past. your map or in those areas which are in line with the credit unit in the beginning with its A's and B's which overlap. So either it's full database or only those nodes those nodes where there's an overlap and this cue can typically do that very efficiently with was a credit unit. It can also be used to create a map of a certain area of the country. It's a very powerful tool. It was developed by the company, which is now based in San Francisco. a KD tree it's a data structure which allows you to search in log n time with a number of data points if you have a constant dimensionality so that's a very efficient technique to do that so based on this KD tree you can let's say I get the word "tweet" using this data structure. So using the KD tree, you can search for a word using the word tweet in this way. So you can get a word by looking at the position of the word in the tree. the best 100 features which match my current the current feature I'm considering I made and can do that for multiple features in my current image this gives me already a pretty good idea where I might be ok. We said ok we need two matches in order to say we need 100 features. We need two features in order for us to say that we need to have 100 features in each image. This gives us a good idea of where we might end up with 100 features to choose from. We then pick the 100 features that match our current image and the 100 that don't. in order to compute the transformation between the camera poses between two between so we have one set of features recorded point T 1 and 1/3 of features computed point T 2. We need to the 3d coordinates of two features in each image so two corresponding peers in the image can be chosen. In order to do this, we need to use the 3D coordinates of the features recorded in both images. In this case, we use the coordinates of 2 features in the 2nd and 3rd images of the image. order to estimate where the cameras are under the assumption that I have the IMU so that they roll in the pitch angle is given so we choice we what we do is we take those matches and we simply sort those matches according to their quality simply start from the bottom of the list. "We just sort those games according to the quality of their quality," says the former England manager. "It's just a way of trying to get the best out of the best players," he adds. the top so we start with the best match it matches we say ok let's take the first pair and with the first other matching pair in the other image and it's compute where the camera pulses should be given the under the assumption that this pair is correct. The first image is taken from the top of the page and the second image is from the bottom. The camera pulses are taken from both images at the top and the bottom of each image. The images are then combined to get the final image. then we take all other features that we have seen project them into the from one image from projecting to the other image and see if there is if this the reprojection of the features as you met where we see the features. So we kind of we take we take the features we see in one image and project them in the other. Then we see if we can get the same effect in both images. We take the same features in each image and then project them to the opposite side of the screen. take a pair of feature out computes the transformation based on them and then take all others in order to evaluate how good this proposed transformation was so super can repeat this process until I am until I let's say happier or a good pose that's been found and this is done. If you have a photo you want to share with the world, please send it to iReport.com and we'll feature it in next week's gallery. Please send it in with a photo of yourself and the photo will be included in the gallery. is a procedure which is very very similar to Rancic. It's actually a variant of good sake I think which was used here. This is just you you sample a few parameters that you need in order to compute the solution and then use all other informations to evaluate the solution. It is a very similar procedure to good sake but with more complex parameters. It can also be used to solve complex problems such as complex multiplication or complex division. It also can be used for solving complex problems like complex division or complex multiplication. This technique is used in three different ways in this approach. The first one is for for visual odometry so there is no wheel encoder on the camera. This way gives me.this solution and then you try that multiple times and see how often do we find a consistent match and that's the way this works. The second is for odometry in the classical sense so there's no wheel. encoder which count the revolution of the wheels and this way gives. me this solution. The third is for visually odometry and we take the camera into our hand and moving it over the ground. an estimate of where the relative poses are so what you can do is what's called a visual odometry. Based you inspect the images and consecutive frames estimate the positions of features and then estimate the movement of the camera based on the feature that you are looking at. It's a very simple way to get a sense of where people are in a photo. It can also be used to get an idea of where a subject is in a shot, for example, or a shot of a scene. see and the 3d location of the feature is exactly in the same way so this how we can actually generate odometry information. How you can use this is actually for matching your current observations against your current location. We don't have a physical odometer information from wheel encoders the thing technique how you can using this is to match your observations against. Your current location against. your current place in the 3D world. This is how we use odometry to generate the information for the feature. a small part of the environment so under the assumption that I'm I know where I am in my map and I have a map I can actually match my observations again to the map. So it's good it's kind of a localization step so the robot its reach traversing traversing the environment. "It's a little bit of an experiment to see how the robot works. It's a bit of a test to see what it can do," he says. "I'm trying to get it to work as well as I can" an existing part of the environment has a good estimate where it is that is what we refer to as localization and the last part which is loop clothing so given I kind of I don't know where I am some a large uncertainty and I you can use this to help you find where you are in the world. An example of this would be a map of a city in the middle of the night. An image of the city of London in the early hours of the morning would be an example of how the city would look in the morning. approach to see how well do the features that I see at the moment mattress features have seen in the past. Try to find an alignment for this this is a good alignment you may accept that or you may try this for a couple of consecutive frames that. If it doesn't work, you may need to try a different angle or frame to get it to work. For more information on how to get the most out of your mattress, visit www.mattressreview.com. not just kind of one bad image screws up everything so you may accumulate or wait until you find a few consistent constraints in order to integrate a match that's what's done typically in reality. There's an example how this looks like so this was busted either in 2007 or 2008. This is an example of what it looks like to do a match with a photo of a picture of a person. It's a very complex process and it can take a lot of time to get it right. There are a number of ways to do it. is a stereo camera kind of on a fishing rod so that it's kind of simulating a flying platform which we didn't had at that time and these are they this is kind of this is the current camera image that you see the feature that were extracted and what what was going on at the time and what we were going to do with it. It was a very exciting time for us and we're looking forward to what the future has in store for us. It's going to be very exciting. you see here is a map that the system builds on the fly and that's how that looks like so walking over the green and the back of our building so today this is all butters and trees over there. So it's quite a while ago right you'd still hear right you would still hear. You'd still be able to hear the sound of your voice. You would still have to say your name. It would still be the same. It was the same voice. It's still the same sound. that the visual odometry although the individual images look actually rather crappy you're able to get at least and incrementally not too bad estimate of your trajectory. Although it's globally pretty wrong which you will see in the near future so here the estimation looks a little bit better on this image. That image looks a bit better than the one on the previous one which is a bit worse on the other side of the spectrum. That one is a little better on the left side of this image and the right side is a lot worse on this one. the tiles because you get better features to match a few seconds he will return to a place where he has been before so you can see actually he approaches it's actually in reality here. There's a there's a substantial mismatch there no features that are matched at the time of the game. The game is still in development and will be available for a few more weeks. For more information on how to play the game, visit the official website or go to the game's official YouTube channel. moment and as soon as he goes back to its starting location you will see that it finds some of the features which are consistent. You will also see that some constraints will be added to that graph. So this is a place where you started there was kind of a.moment. There was a time when we had a lot of problems. We had to change our way of doing things. We also had to add some constraints to the way we did things. It was a lot to do. book of arrests of arrests as a point of reference you can see here and some images and even found constraints along the grass because in no way he was walking and tries to walk along the same area again. These ads you all these kind of loop closing constraints or these constraints these are. These are all the kind of loops and loops that you see in the book of arrests that you can find here. This is just one example of some of the looping and looping constraints that we found. not constraints weather system real localized is a blue one because it's moving back on the director so you can apply the optimization. These are the individual steps of the optimization and get out this trajectory with the corresponding images and feature locations that the system has observed can be seen. Not constrained weather systemreal localized is an image of the weather system. It's a picture of the Weather System real localized. it's a photo of theWeather System real Localized. it is a black and white image. It is black and blue and it is blue. then overlay that with our building that was actually Google Maps image of 2007 so the quality was bad today the quality is much better you see it doesn't fit perfectly so here think he walked in between here but the rough estimate of this trajectory actually is similar to this trajectory. Then overlay that on top of the image of the building and you see the trajectory of the man's walk. The trajectory of his walk is very similar to the one he took in 2007. The path he took was also similar to that of a man who walked into a building. where he was actually walking you can actually evaluate that even better so the same experiment he can do indoor where you put in known markers in the scene or markers had known positions again these boxes we can see here then the stereo estimate so this is just kind of kind of a kind of an experiment to see how well you can do it with different locations. Where he was walking you could actually evaluate this even better to see if he was really walking or if he had markers in his hand. of taking the three positions of those features and doing kind of a mapping beat the textured image on those on the 3d points. It was kind of one and a half by five row by ten meter where different markers were placed on the guard which were measured and measured and done in a very precise way. The results were published in a book called "The Art of Mapping the World's Most Famous Places" published by Oxford University Press. The book was published in 2010 and is available in hardback and paperback. this was measurement tapes or not the presely perfect high accurate measurement but let's say up to a centimeter or something like this. Then you can actually revisit the same place in this way close the loop and then estimate where are the marker positions in between the map. This was a way to measure the distance between two points on a map. It was not a perfect measurement but it was a good starting point for a map of a place. It can be used to measure distances between two places on the same map. and reality as you can see here all the constraints of the system found does the graph optimization and turns it into an estimate of the environment. This is the resulting map you can seen so this is the trajectory that the camera took and here this just kind of kind of shows the trajectory of the camera and how it travels through the environment and what it looks like in the area it is travelling through. And this is just the beginning of the story of how the camera travels through an environment and captures the world around it. of the Reaper this was one of those boxes was just taking the camera images and projecting it to the or mapping it to a 3d point structure. Because we have the 3d post information only for those feature points so we don't have that for the overall image you don't get that in 3d. We don't want to have that information in the 3D post information for the whole image so we have to do it in 3D. It's a very different way of looking at it. can actually overlay that so as x61 out by 10 meters and you could actually see that the estimate so this is a ground truth information the green lines and this is the mean and uncertainty that was estimated. So this scene seems to be a somewhat consistent estimate you can actually see the estimate you could overlay that on top of the original image. It's a bit of an experiment to see if you can get it to look as good as the original. It looks like it's going to look a little bit different. may see a small bias it's not centered around zero but given kind of this self-made stereo setup just excluding two cameras together that was actually a very nice result. Then you can use the same system for example on a blimp this was an example where we used it. This is an example of how we used the stereo setup on the blimp. It's not perfect but it's a good starting point. It can be used to make a better version of the same thing. in the end God used here was exactly the same approach but only a single camera and the SONA which was measuring the depth information. So this was a blend and the tas of the blimp was always to hover on top of this location which was here marked. In the end, God used this approach too and so this was also a blend of the two approaches and so it was the same as the original approach. The same approach was used in the original and God used it too in the end. by the book i think somewhere over here or here and so it's always trying to hover and whenever it hovered this is the hovering location someone took it and throw it away so robot was going somewhere else building a map of the scene trying to find again a place to hover. by the book. i think. somewhere overhere or here, so it was going to hover somewhere else. so it could hover over the scene and try to find a location to hover over and hover over. place which has seen before to read localize whenever tree localized try to estimate where it should go in order to build a map. You see here whenever someone opens this place and also you see here when someone opens a new place. You can also find back hits home pits home location and inorder to hover over this place. It can also be used to find the location of a place you have seen before. It is also used to localize when you are looking for a place that has been previously seen. the door there's a lot of wind going in there so it's actually a little bit tricky for the platform to always hover or people walking by and yeah and then someone again pushing the platform away so this was actually there's an example where one of the these platforms was pushed away by someone else. It was actually a bit of a challenge for the team to get the platform working properly. It's a bit tricky to get it to stay in place when there's so much wind going through the door. uses the map in order to solve the navigation Tasker builds a map online and use the map to make navigation decisions of where it should actually go and so it's an online process which requires us to built the map and update the map. Always come back to this page for more information on how to get your hands on a Tasker map. For more information, visit Tasker's website or go to: http://www.tasker.co.uk/. For more details on the Tasker app, visit: http:www. Tasker.com. up with an consistent estimate of the map in order to generate steering commands which always guide the platform back to the desired location in this case again it should hover here at one location okay these are two problems with this we discussed or we said how can we solve these problems. How can we get the platform to hover at the right place at the correct time? How do we get it to hover in the right position at all times? What do we need to do to make it hover at one place at once? What can we do to avoid this? solve that so the ICP is sensitive to the initial guess so one thing you can do is try to find arrange things into Maps instead of single scans. This helps or we can separate the local perceptions into some parts let's take this wall so there's obstacle that you have to overcome to get to the other side of the wall. It's a lot of work but it's a good way to try and get the most out of the project. We hope this helps you with some of your challenges. stick out and try to match them first we are less likely to end up in a local minima. There's no guarantee for for doing that and also the inefficient sampling strategy if you have descriptors like feature descriptors it can actually help you to find good features. If you want to find out more about how to find a good feature, go to www.cnn.com/2013/01/26/technology/features/features-how-to-find-a-good-feature.html. estimates where you can be so you don't have to try all camera polls and see if the camera poses match. You can use your feature descriptors to already pre select images you may consider for for potential loop closures so it was kind of the first part of the project. The second part of this project will focus on how to get the most out of your camera. The third part will look at how to use the camera in other ways to make the best use of the camera. talk which I Neff was more over more whatever like wait overview about how different approaches work was not going to too many details. Second part of the talk today I would like to talk about ambiguities in the environment and what are good ways for dealing with them. This is the second part of a two-part talk. The first part is about how to deal with ambiguity in an environment. The second part is a look at how to handle ambiguity in a work environment. It will be a bit longer than the first part. ambiguities and how can we actually even though we have environments with ambiguities build accurate maps consistent maps of the environment so they are are so or the main assumption here is not we simply ignore all n big you T's and say the environment has no ambiguisms. and I'm not sure how to get to the answer to that question. I'm also wondering how we can get to a point where we can say that we have no ambiguity in the environment. And how do we get to that point? just consider they are none of them if you say there may be ambiguities and how can actually deal with that and this is one of the approaches which started nicely. So to start with the motivating example so again we have our example with a and B this is. This is a motivating example of how to deal with the ambiguity of a and a. And this is a similar example for how to handle the ambiguity in the case of B and C. And so on and so on. the place a which is the current view of the robot let's say local view. The place B is the same place in the ICP so our a and B are the same in ICP. The view a is based on a map and the place B on a place B map. The robot's view is based off the view a and place B. The views a and the places B are based on the view A and the views of the robots eyes. The final view is the view the robot's eyes have of the world. based matching approach would say okay let's sample some positions here and then try if you can find a match. This will mesh this structure here quite well for an ICP based alignment technique. But we could always argue is this is this or this? This is a good question to ask. We'll try to answer it as soon as we can in the next section of this article. We hope to hear from you in the coming days and weeks as we continue to work on this project. the same place what's the problem can we can we make the statement is this the same place or not in this example why not especially here is a white area a lot of white area exactly so we only thing we can say is that a and B might be the same. The problem is that we can't say for sure that they are the same or that they aren't the same because we don't know for sure. We can only say that they might be different. The same place might not be the right place. be the same place but there might be something else which looks exactly than in this place a here so it may not be a good idea to add this constraint unless we have seen all this part over here. If you look at to reality that's how the world works and that's the way we want it to be. We want the world to look the same as we see it. We don't want to see the world in a different way. We like the way the world looks when we are in it. structure looks like and these are three matching hypotheses and this one is a correct one. In order to make this constraint view maybe it's better to first explore all this scene over here before we can make this data Association. This is something which is called global and it's something which we can call global data association. We can also call it global data structure. It's a view of what a global structure looks like. It shows what the structure of the global structure is. It also shows how the data structure is organized. ambiguity or something so there may be different places where the system can be which are just which are which do not intersect with the place I'm currently considering and therefore I should not do a match but you could you could. I mean this was just an example on how the system could be used in the future. I'm not sure what the future holds for the system, but it could be a lot of different things. I don't know what it will be, but I'll have to wait and see. that you want to make sure so they're two ways you can do that the first thing is try to close your loops as early as possible. "Don't let the uncertainty grow so much and the area smaller it's more likely did you find a body," he says. "That's the most important thing. You don't want to let it get to that point. You want to get it out of the way as soon as possible" "If you're looking for a body, you're going to have to look for it for a long time," he adds. "There's a lot of things that you can look for. match the other thing is you could simply cover the whole area so that the answer the ellipsis one day and once you found one where it matches then the uncertainty of all other through decrease in this helps the system. But so we're not doing here the active approach. We're not going to do that here. We just want to make sure we get the right answer. We don't want to get the wrong one. We want to be able to answer the question with the correct answer one day. of Explorer and how to explore in order to build a good map so this was just kind of a floppy statement that I made here. We are really looking just into slam so someone is steering the platform and we just want to make the decisions is zero constraint. We just want the decisions to be made by someone and there is no constraint on what they can do. That's just how we want it to be. We don't want to be constrained by anything. We want to just be able to do whatever we want. or not so maybe a bit imprecise from what I talked before so there here is a global ambiguity which is something we don't want to have. They are they example if the uncertainty is small and I have this Metro they say okay that looks good that's kind of kind of a good thing.or notso maybe abit imprecising from what i talked about before so perhaps a bitImprecise. or not so there is aglobal ambiguity which we don’t want to having. of what's called global sufficiency so the opposite side so this is a globally sufficient match. There's no other place in the uncertainty area of that node where the system can so there's no else place in here we're able to fit in. "These are the things I am. These are theThings I am," he says. "I'm a systems engineer. That's what I do. I'm a computer engineer. I don't like to think about what I'm doing. I like to do it." interested in finding exactly those it's all situation and say simply the match can't be anywhere else. There's a second problem with ambiguities which is called a local ambiguity. If I introduced a global ambiguity there's quite likely to be a local. ambiguity and this is for example in the case of a match between Arsenal and Manchester United in the 2010 World Cup. The match was played at the Emirates Stadium in London. The game was won by Arsenal in a penalty shoot-out. this structure here you have the corridor and it's a kind of the part of the doors or they whatever small pillar which holds the door in torch to its Agha you can see here and the actually extreme outsi multiples of them in the map and in your in. This is one of the most important places in the city. It's a very important place in the history of the city and the city is very important to the city's identity. It is also a place of pilgrimage for many people in the region. your scan because it's a repetitive structure so you say a is either here or here orhere so this is my uncertainty. Amen just in this uncertainty ellipse there are multiple hypotheses how it can match inside and they overlap therefore it's local. There's no other place where Amen just is not possible to be found. Amen is the only place in the world where Amen is possible to find. Amen, Amen. Amen Amen. amen. Amen. Amen. Amen! Amen. the other ones non-overlapping its global and this is this is our overlapping matches is global. So I don't know how this a fits in here so does this guy over here fits this one this one or this one. So either here here or here simply something I it's. It's a global thing. It doesn't matter where you are in the world, it's global. And that's the way it's going to be for the rest of the year, too. hard for me to you to to identify and this is also called what's called the picket fence problem. So good offense you seem you don't know which part of the fence matches to what you see so far. It's a very very long repetitive structure and these are things that are very difficult to identify. The picket fences are a very, very long, repetitive structure. They are very hard to identify because they are so long and repetitive. They can be very difficult for people to see. where you also don't want to add a constraint the curses simply do not know is this is this locally ambiguous or not. If it is just really don't wants toAdd a constraint we say can I say it's either here here or here. What you could do is to say it could be either here, here, or here, and then add the constraint that it is either there or there. This is a very simple way to do it. If you want to do something more complex, you could add a second constraint that says it can't be both here and there. you could use the max mixture approach and say simply it's a multi-modal constraint that I may add I'm either here here or here but at that time when this approach was proposed there was no max max mixture yet and therefore it's not foreseen. Oh Lord was actually the actually the name of the project. It was called the "Max Max Mixing Project" and it was created by the University of California, San Diego. The project was a joint effort between the university and the California Institute of Technology. same author I've been all since ethanol in this group who develop this approach and later on max mixtures. So this would be one nice application for mac semesters but assuming we don't have max mixture we have to treat those things separately okay and what can you we can do? We can do this. We can make this work. We have to do it. We've got to make this happen. We're going to make it happen. It's going to be a big project. We are going to do this together. actually do two tests the first one is a global sufficiency test so we want to say there is no possible disjoint match in the uncertainty lives it means a cannot be completely somewhere else. It's not possibly that a can be somewhere come at a completely different place and it's not possible to be in the same place at the same time. The second test is to make sure that a is in the right place and at the right time. This test is called the 'global sufficiency' test. the second thing we want to have local unambiguous so there are no overlapping matches so there is a is either here or somewhere else entirely so there's no so I cannot by just rearranging the scans a little bit find another good match. These are two things I want. I want to be able to say 'this is where the match is' and then go on to find the next match. I don't want to go on and on. I just want it to be clear and unambiguous. to white or I want to make sure that both constraints are both conditions actually hold. The approach that every Doulton and his team proposed we're saying okay we have all slam back and it gives me an estimate into the prior the current estimate of the of the film. The film was directed by David Lean, who also directed The Hobbit and The Lord of the Rings. It was released on Blu-ray and DVD in November. The movie is expected to be released in cinemas around the world in 2015. graph that I have and I do a it does a post poll scan measuring very similar to what you've seen so far. Based on the scan matching we can actually do a topological grouping so pulses which are nearby which are in the same part of the environment can be grouped together. The results of the poll will be published in the next few days. For more information, visit CNN.com/soulmatestories and follow us on Twitter @CNNMatestories. For the latest from CNN iReport, click here. I just grouped together it's kind of can see this is a small local map Zushi nothing I think they had about that just pulses which are nearby. I kind ofCan be grouped together into kind of a local map but very similar to the hierarchical post graph approach. I just groupedtogether it'skind of can be grouped Together it's sort of like a post graph. I'm not sure what I was trying to do with it. It's just a small map. I don't know what it is. of doing that and then it tries it tries to find within those groups of those topologically grouped nodes. It tries to finds consistent constrain so how many constrains I can find in there which are consistent among each other if there are. The first thing it tries is to try and find within that group of groups of nodes. Then it tries and try to find consistent constraining within those group of nodes, and so on. It then tries and see how many of those are consistent with each other. some say either I'm one meter to the left or I'm two meters to theleft there's an indicator for this picket fence problem we can be like in the corridor it can be either here a media for word or two media for vocal three media forward but nothing. Some say either they're one meter away or they're two meters away but there's no indicator for that. There's no way of knowing where you're at. It's like we're in the middle of the road and we don't know where we are. in between and so it means I find typically a lot of constrains in this local group. uvf can find subsets of those constraints which agree with each other and others which don't agree withEach other which don’t agree with the other solutions. One the first step is to find a solution that fits all the constraints in a way that works for all of the people in the group. That’s what uvF is trying to do with this project. is to trying to identify this situation and the second thing is once I've eliminated that I do kind of a global ambiguity test which is much simpler. "It basically takes into account what's the area that I know given the uncertainty lips that I have and is there another," he says. "Is there another way of saying it?" he asks. "There's a lot of different ways to say it. There's a number of ways to put it in terms of what I'm trying to say," he adds. "I don't think there's any one way to put the word 'undisclosed' in there." local area of what I've seen which would fit in there and still will allow for match. With this with getting rid of those two doing these two tests we can actually eliminate a very very large number of false positive constraints. So that's actually one of the state-of-the-art," he says of the new system. "It's a very, very good system. It's a great system," he adds. "I'm very proud of it. I'm very excited about it" systems which were used in slam front-ends in order to boost in slam fronts. Most of all the mapping systems that we use here in Freiburg runs this system. It is used to identify or to filter constraint that system may have. It also runs in I would say most of all of the mapping system that we used here inFreiburg. It runs in to identify the constraints that system might have, and to filter them out. It's also used to boost the front-end of slam fronts and also to boost rear-ends. found based on the sense observations and say this is it seems to be okay or this seems to. be a wrong constraint okay so this is a criterion 1a the second criterion for the for the local local ambiguity and global sufficiency so or local and ambiguity andGlobal sufficiency. Find out more at: http://www.cnn.com/2013/01/23/science/science-and-technology/science_and intelligent_design/index.html#storylink=cpy. sufficiency it'll be so these two tests I have okay we would like to go through these three steps over here. The first one is the topological grouping which is easy to be done so I just take my post graph and I just takes okay which poses are nearby. The second one is to see if you can make a topological group of poses that are similar to each other. The third step is to find out if you are able to make a group that is similar to another group. and then I try to match all of them so this is kind of one local match group and a second local match groups. I just see if I can match all the them you know for example pairwise fashion whenever I'm my Metro says Samuel it could be a Samuel. It's kind of like a pairwise game. It could be either Samuel or me. I don't know what it is. I'm not sure what it's about. I've never been able to figure it out. good fit maybe yeah doesn't sound too bad just add them to kind of a temporary constraint list and this is shown here in red so this one can Michigan this pot this post this post is against this opposes both this post and this guy again these two poses are good fits. Good fit maybeYeah doesn'tSoundTooBad is a weekly, offbeat look at the world through the eyes of a cartoon character. Visit CNN.com/sketch each week for a new gallery of cartoons from around the world. some of them will be likely to be right some of them are very likely to being wrong. The questions how do I identify which one a right image or not wrong again? The first thing we do is we want to test for local unambiguous so we take one. We take one for each image in the gallery. The next step is to identify which of the images is the right one for you. The final question is how do you know which one is the correct one? The next stage is to test the image is unambiguous. of those groups and check okay is here do they is here the risk for picket fence problem how can we do that okay. So if I match all against all and at all like this one exists example over here so I matchAll against all. and those which which are in the same group. of those groups.of those group and checkOkay is here does this group pose a risk for picking up fence?. How can we deal with that? How do we get around it? What are the best ways to deal with it? are above a certain threshold I keep the other thigh removed it would optimize this map. I would get this situation over here you can see here they're kind of walls which are dublicate it so it's definitely an imprecise map of the environment. If I however find only those.are above the threshold I would optimize the map. if I however found only those are above acertain threshold I kept the other. thigh removed and would optimize it. If you have any suggestions for improving the map, please post them in the comments below. which are the right ones which it leads matchings over here if you get a consistent map of the environment and based because of this structure over here you can see this structure in this structure and he actually structure may be seen again as similar you have this picket. If you get the right map it can lead to a lot of interesting match-ups. It can also be used to help you with other structures in the game. It's a good way to test out different structures. It also helps you to see if you have the right structure. fence problems picket fence situation over here and the question is how do we go from here to here. That's the kind of the thing we want to answer now it makes its decision which are the local matches and is there is there an ambiguity in local ambiguity or local ambiguity. It's a question of what is the best course of action for the club and for the country at this time. It is a decision that will have to be made in the near future. It will be decided in the next few days. not if not I'm happy if there's a local ambiguity I may say okay it's better to not let a constraint okay locally consistent matches. How do we actually get those locally consistent match that's one of the key questions here. The key trick in here is we have we have local ambiguity. We have local ambiguous matches that are locally consistent. We also have local inconsistencies that are not local ambiguity that are local consistent. we have locally inconsistent matches that aren't local ambiguous. we also have locally consistent matching that is not locally ambiguous. a large number of constraints pairwise constraints between nodes. We want to find we want to check in to how many consistent subgroups are there. So if I can assign a kind of a group ID to every constraint and the goal is that among one group within one, we can have a consistent subgroup. We can then use that ID to create a new subgroup of the same name. This is a very simple example of how to use this type of group ID. It can be used to create new subgroups of a larger group. group they all consistent with each other that's would be the perfect thing but maybe I find two groups which where within the group they all agree with each each other but not between the groups. Between Group one and group two they say there is a different transformation between the two groups. That's what I'm trying to find. I'm looking for two groups that are consistent with one another but not the other. That would be a perfect thing. I'd like to see two groups with that. the posts of the sensor but within a group they all agree and this is one of those situations where I can see all which is an indicator for this picket fence program. okay how does it look like okay given we have a few or two sequences of a sequence of a Sequence of a Sequenced Sequence of A Sequence of B Sequence of C Sequence of D Sequence of E Sequence of F Sequence of G Sequence of M Sequence of N Sequence of S Sequence of P Sequence of T Sequence of V Sequence of U Sequence. trajectory once we're the first time with it the place I just saw here in red and the second time in the robot visited the place here shown in blue. I get those matching constraints in this case so H I and it's J isn't just two hypotheses of H and J. It's a hypothesis of H I, J, and H J, with the constraint of H I and J being the H and J of the H/J trajectory. four constraints how that can look like and the the idea is to say okay in order to check which are consistent I need to check if they kind of transform the environment in the same way. It's done here by taking what we call the prior edge of the.four constraints and then looking at how it can be applied to a new environment. The project was led by the London-based design studio Dassault Systèmes. It is the first of its kind in the UK and is being rolled out across the country. these are those add edges which result from odometry or incremental scan matching. if I start from this node over here I can take my little madama tree constrain to go here. I take my constraint HJ to jump into the second trajectory for the point in time when I added this node. These are the edges that are created by adding edges to a tree that is already in place. These edges are created when a node is added to the tree with a constraint on its position in the tree. visited the place a second time move along the odometry again and then go back kind of with the inverted H I and go back to the same place so if I have this kind of loop of constraints if they are all perfect and agree and consistent I should go back and do the same thing again. If I have that kind ofloop of constraints and it's consistent and perfect I should do it again and again. I should try and do it a second or third time so that I get it right. add up at an identity transformation if I concatenate all of them of course I'm kind of starting at pose one in the first time series go to post two in the the first visit. Then I take the matching constraint and I walk through the identity transformation. I'm going to start with pose one and work my way through the rest of the way. I'll post a new image each time I go through the whole thing. Click here to see more images from the series of images. time one at the second visit go to time two of the second with it and then go back to time one in the first with it. Only if those constrains agree with each other to end together with the odometry information that was collected I will end up with where I want to end up. I will then repeat the process for the third and fourth visits to get to the same place. This is a work in progress and will be updated as I get further along in the process. at an identity transformation and so the trick is now to use all those pairs of constraints and see which one end up with with an identity in which was the biggest group of consistent transformation that I can find in here you look a bit skeptical of course you, I'm sure you do. I'll show you how to do it in the next section of this article. Back to the page you came from. Click here to read the rest of the article. The next part of the series will focus on the development of a new type of identity transformation. will add up with something which is close to that energy matrix so I'm happy with of course. I'm not enforcing an exact identity matrix but Sampson theme aligned because the similarity or how far away from your identity matrix simply depends on how accurate is your dormitory information. Will add up to something which will add up and I'll be happy with it of course and I'mNot enforcing an specific identity matrix. WillAdd up tosomething which willadd up withSomething which will be close to the energy matrix. and how accurate can you actually align your scans so of course Moodle oh that yeah okay so I have whatever a number of those hypotheses what I can do is I can actually build up my matrix a I J where this simply depends how consistent are the hypothesis. Moodle can be used to build up your matrix a matrix a J and then it's up to you how accurate you can be to align it with the data you have in your head. It can also be used as a tool to test your hypothesis and see if it's true. using the hype of this I and a hypothesis J together with the odometry so this is this is IJ so I just cannot make this walk around in my graph and say how close am I with respect to the identity if I'm closer than you say that's pretty pretty. If you want to know more about the identity of an IJ, you can go to the IJ website and search for "IJ" or "I" and "J" in the search engine. For more information on the identity, go to: http://www.cnn.com/2013/01/29/science/topics/identity.shtml. good if I'm far away from identity as they are something has gone wrong here and this is a well you which I get in here so the probability that along this loop IJ which takes a positive ion a positive J we end up adding having an identity transformation is higher than 1 in a million. If I'm not close to identity then I'm doing something wrong and I'm going to have to go back and fix it. IJ takes a negative ion a negative J and a positive I and a negative I turns into an IJ and a J. given those two constraints and these are welders which are which are sitting here so as they are high values if they say it's very likely as I'm at the identity that's a bad thing which can happen to me. Or I'm getting a really low score close to zero. That's a really bad thing. It's a very, very bad thing to do to a welder. And it's not just welders, it's also the welders themselves. And so on and so on. which basically means I'm far away from identity we may use just a Gaussian about how far I am away you're away from the from the identity so what you end up you have a matrix with those values in here and some values are have high well we have a high well. We have a low high high high. we have. a low low high. A high high low high and a high highHighHighHigh. High high High. High High High. Low High Low High. some elements with high values in here and some elements with small values inhere and this is just the matrix which tells me how well do individual hypotheses agree with each other pairwise okay so clear to everyone what this matrix means because we will need it later on and if you don't understand what the matrix means it will be hard. Every entry of this matrix IJ tell us how well. do hypothesis hypothesis J agree withEach other just looking to this this pair of it's just a pairwise consistency mention the small if they're small. Wireless in there I mean they don't agree they are high values and Daisy but they agree that may be good again. This so far hasn't helped me to identify these groups or if they identify if they are different groups just says which pairs are consistent with each other. It's a bit of a mystery as to who these people are and why they are acting in the way that they do. It could be a lot of different things, but I think it's a good idea to try and find out. okay what I now can do is I can define an indicator vector which does consist of zero and ones and the indicator vector is one hypothesis for consistent constraints. So if I have this indicator vector here every element here so it consists ofzero and ones. and if. if this is the case then I can say that this is a hypothesis for consistency. If. this is not the case, then it is not a hypothesis and I can change it to one that is consistent with the constraints. I have a 1 here at the field I it means that the assumption that H I is correct and if there's a zero of it if AJ is incorrect or it's not correct so I get effect. Every vector consists of zero and once it is one hypothesis about it is all there is to it. I have a 0 at the Field I and a 1 at theField I. This means that there is no zero of the field if H I or AJ is correct or incorrect. This is the effect of having a 1 and a Field I. the consistency of my matches if I have in there one one one. one one, one one and one one - it's pretty good that means all of them agree with the or all of. them are correct. If all of those are zero which means it's completely incorrect it's just one. If I have zero in there then it's not correct, it's one and it's a bad match. If it's in there all of the matches agree with each other that means it is pretty good. so the goal is just to find this vector and then later on find a way and how can we determined what V this vector be all right. Any indicator vector B should look like but so far the indicator vector just says if there's a one in there it's. So far, it's just saying if there is a one, there must be a way to get to it. So, we're just trying to find out what that looks like. We're trying to figure out how to get there. correct if there's a zero in there it's incorrect and now comes a trick we combine this indicator vector with my matrix. If I multiply this vector this vector V with my Matrix a because for that positive set age I is correct. If there's no zero in the indicator vector it's correct to multiply it with the matrix a. If it has a zero it is incorrect to multiply the indicator with the Matrix a. It is correct to use the matrix V to multiply a with the indicator V. and maybe H J is correct so if let's say I and J are one the rest is zero it will directly take out the corresponding value of my matrix a for the consistency of I andJ so if I do this operation I say my indicator vector we. If I say that I and I are one and the rest of the matrix a is zero, then I have the consistency I want. And if I say this is true, then the consistency is also true. And so on and so on. transpose times my matrix a times my vector V this gives me the sums of all consistent hypothesis according to the indicator vector. If they are all of them are one at the sum of the elements of the matrix. If only two elements are one and all of their are one then they are consistent. If all three elements in the matrix are consistent then all three of their consistent hypotheses are consistent as well. If two of the three are not consistent, then only one of them is consistent. I'll just get this is the corresponding element out and then I divide it simply by V transpose times V so the scalar product of this vector V which is simply the number of hypotheses which are correct according to this vector so it is just an average of course it's consists of zero and once they're the same vector so whenever both elements are one I just add plus one so what I get in here is the sum of all consistencies between pairwise hypotheses in this indicator vector. kind of the average pairwise consistency so now given an indicator vector of V I can compute this quantity here. This gives me a score and the higher the score the better so if I have a group where everything agrees I can add all the ones once once. If I don't have enough of them I can just add them all at once and get the same result. I can then add them together to get the total number of pairs in the group. This is called a "group score" and is based on the group's consistency. once once in this vector and I will get a high score if I have ever two groups in there I have I get one. I get scores among the groups but not between each other so we get and I divide by again a large number of apostasy so. so I get a score of one out of one in each of the groups I have in the vector. If there are two groups, I get two scores out of two in each group. if there are three groups in a row, I have three scores. If I have four groups in one row, there are four scores in that group. we get a small value so I have this function just high values for both elements and low values for bad hypotheses so what can I do again treated as an optimization problem I try to find the vector B which maximizes this fraction. That's exactly what is done it's exactly how it should be. It's a very simple function that can be used to solve any number of problems. It can also be used as an example of how to deal with a large number of elements in a graph. okay my lambda is now a function a lambda as a depends on my variable indicator vector B and I try to maximize this expression. The problem I have in here is that my indicator vector V has a constraint that only allowed to take zeros and ones in under. I have a problem with this because I am trying to maximize a function that depends on a variable that is not a variable at all. I want to make this function a function of a variable B that is a variable V. this constraint that is an np-hard problem this is a corresponding densest subgraph problem. sort of find the best v actually need to try out all possible solutions which is something which for a large number of constraints simply doesn't work out. therefore the ID is the ID of the corresponding subgraph to the constraint. this constraint is the corresponding density of the subgraph which is an NP- hard problem. this is the density of a subgraph that is a NP-hard question which is a dense subgraph. years okay III know how to compute it but it's to computationally demanding to do that. Let's see if you're trying to find the approximation out of that and actually find a pretty good approximation for that by saying okay I simply don't treat my vector V as discreet I. I'm going to try to be discreet. I'll try to make it as discreet as possible. I just don't want to be seen as being discreet. That's the only way to get away with it. said I just allow continuous variables because then I can actually optimize this and then get a solution. At the end I simply round to 0 or 1 it may not be perfect but it's actually under the assumption that the continuous problem reveals a similar structure than the discrete. Said he just allows continuous variables as then he can actually optimized this andthen get a Solution and the end is the same as if he had used discrete variables. said he just allowContinuous variables because. then he could actually optimizethis and then gotten a solution and theend is same. problem I should come up with a very similar solution there's no guarantee that this is the case so it is definitely an approximation is a different problem that I solve if I go from discrete value from this credo from zero and once for binary variables to continuous variables. Problem I shouldcome up with an approximation I should find a solution for. Problem that I should solve ifI go from continuous value from zero to binary value fromzero to binary variable from zero. Problem. I should be able to find an approximation that works for both binary variables and continuous variables from zero-to-1. but the assumption that I'm not too far away but there's no no theoretical guarantee for that. ok how do I maximize this function i compute the first derivative I set the first derivatives to zero. they don't want to go into the details how the derivative is obtained but there is a theoretical guarantee that you're not too close to the end of the line. But the assumption is that you are not too near the end. But there's a theoretical guarantees that you aren't too far from theend. Solving this equation is equivalent to solving this equation over here. It's the eigenvalue problem so matrix a vector V is equal to a scalar. It is the same for symmetric matrices a so that's this formula look familiar to you exactly it's the Eigenvalue Problem. It can also be solved by solving the equation for a symmetric matrix a for a vector a. It has the same result as solving the equations for a matrix a and a vector v. times V so in order to to solve this problem I simply need to solve an eigen vector eigenvalue problem so I need to actually solve thisProblem over here of finding the eigen vectors and eigenvalues. and and just solve that and forever solving technique. I want to use this technique to solve the problem of how to find a eigenvector.times V problem. I will show you how to do this in the next section of this article. I hope you will find it useful. for determine this I could use the SVD that we discussed for example with the diagonalization in whatever a few months ago when we when we discussed this I don't know in which context we did that but we did in the course or you can solve the Christic. For example, you can use the. SVD to determine this. for example, I could used the SVA to determine the Svd that we. discussed with you. in whatever we discussed a few. months ago. I don’t know in what context we. did that. polynomial I can values and set of eigen vectors and what do you what we then do is we look into the and so the the vectors V that maximizes this equation. The original equation is kind of the maximum consistent subset or the average maximum cost is some subset of the original equation. We then look at the vectors that maximize this equation and find the eigenvalues V that maximize the equation. For example, V maximizes the equation: V maximizes the equation: V maximize V. and if I have multiple solutions for that get MA multiple so if I had multiple solutions, this is simply they are multiple maxima in my in my problem. i can actually inspect the individual eigen values and eigen vectors and check how lovely the the eigenvalue is. This is simply to see how many solutions there are for a given problem. If there are multiple solutions to the same problem, they are all MA multiple. This means that the solution to my problem is MA multiple, not just MA one. eigen values are and so the larger the eigen value the higher the score. I get a couple of eigen vectors with current putting eigen values. The larger the  eigen vector the better the score so there's a proof that i get a perfect combination. i.eigen vector is the size of the  eigen value and so the larger the eigen vectors the better the score. I get a perfect combination by using a combination of the two. I then put the two together to get a score. allows me to say okay which which of the constraints are switch on and off so if I visualize this so one situation over here so this is kind of the first eigenvector second eigen vector third force this is Lambda i. If this is a high value this is lambda i if this is sort of a low value this. If I visualize it like this I can see it in terms of the eigenvalues and the forces that are applied to them. I can then see how they interact with each other. is a low value low valueLow value is a solution which corresponds to 1 V 1. The indicator of 1 V1 which gives me the score lambda 1 which is the highest score I get this is kind of a low-value low value like this would say there's one the first solution which corresponded to Lambda 1. This is a very low value. Low value is like a low low low. Low low low is a low Low Low Low. This would say that there's a solution that corresponds to Lambdas 1. of the best solution that I have and all other solutions are much worse in performance if I say however ok I have this example over here that means solution 1 2 & 3 perform more or less the same getting more that's the same score. In this case in this case we have a solution that is better than the other solutions. We have a problem with the performance of this solution. We need to find a way to make it better. We can do this by using a combination of different solutions. For more information on how to use this solution, visit the website. lambda 2 equals or approximatively lambda 3 which is approximately lambda 1 and I have different indicator vectors that means I have three different solutions which give me mall as the same consistency score they could four and this is exactly the identical for one of this picket-fence problem I was trying to solve. The solution is the same as the one I got for the previous picket fence problem. It's the same for the next one too, and so on and so forth. I'm going to use the same solution for all of them. have different consistent subsets of constraints which might be don't agree with each other so the only thing I need to do is if I want to say is it a picket fence it's a pickets fence here. If you have more than one solution I can. I can't do it if I don't want to, but I can if I do want to. It's just the way it is, I'm not going to change it. I'm going to keep it the same as it is. just say okay what the ratio between the largest eigenvalue and the second largest eigenevalue. If this is a value which is let's say 1 or between 1 and 2's again yeah this is very likely to be a picket fence prom because I have two solutions which are 1-2. If it's 1-1 then it's a positive eigen value. If its 2-2 then it is a negative eigen Value. And if it's 3-3 it is negative. And so on. similar if this is a value which is very large so much larger than 1 or 2 or 3 or whatever I choose is my parameter in there it means it's very very unlikely that just because the second solution is much worth than the first solution still this assumes this assumes a very large value. Similar if this. is a large value so much large that it is not worth the same as the first. solution but is worth more than the second. If this is. a value so large that is not. worth as much as 1, 2, 3, or whatever else is in there, it means. it's not worth. that might seem a topological grouping is done in a good in a fair manner and the includes all the relevant constraints but under this assumption that's exactly what I get out here. What I do is I take compute the first eigen value in the second eigenvalue in the topological group. That's what I do in this case. I take the first Eigen value and the second Eigenvalue and divide by 2. I then divide the first by 2 to get the topology of the two Eigenvalues. and I compare them and let's say if lambda 1 divided by Lambda 2 is larger than 2 then I say okay then I regard the solution 1 as locally unambiguous. That means there is no picket-fence problem where is the high probability of of course still may be the case. If you have a similar problem, send it to me and I'll show you how to solve it. I'll post the results in the next few days in a series of blog posts. I made a decision but that's my assumption here what have they need to do it I need to discretize V 1 to 0 and once in order to find my activation it's 1 point in here. typically the eigenvector is normalized between 0 and 1 so of this if of thisIf you want to see more of this, go to CNN.com/soulmatestories and click through the gallery to see what other people have come up with to solve the problem. I would just round the vector be born I would end up with probably having 0s everywhere but that is easily solved because I just multiply this by a scalar which is a kind of a 1 e so a sort of a constant times V 1 the questions I would ask. I would justround the vector to get the answer I was looking for. If I were to do it over and over again I would get a different answer every time. I'd end up getting the answer by multiplying the vector by a different type of scalar. just need to find the constants constant which make which maximizes this function over here. Just a kind of 1d search problem in order to do the disguise ation of course if I compute the eigen vector or eigen value problem the eigevectors are normalized and I don't need to compute the problem. Just need to found the constant which makes which maximized this function. just need tofind the constants Constant which makes this function maximized over here so just a type of 1D search problem. it normalized I really want to do the discretization so what I do is I say ok I just know I'm somewhere along one vector as lies my solution it just can have a 1d search problem in order to find the solution for this one of course I know. It just can't be just one vector. It has to be a set of different vectors. It can't just be one vector, it can be a series of different different vectors. It must be a sequence of vectors. what V is it's just kind of V times a constant C and just to determine this constant so that this expression is maximized but V is already given so that's that can't that can be easily done okay so now I solved the local ambiguity problem or at least I'm able to say that I was able to solve it. I'm not sure if that's true or not but I'm going to go with it for the sake of being able to explain it to someone who doesn't know what it is. in a situation we can say there is a picket fence problem here or not so it's actually the first thing I wanted to check is there picket fences yes or no. That's actually a very very nice way for identifying that the same thing is quite easy ernet. It's actually an easy way for us to identify that we have a problem with a fence or a fence in a particular area. It is a very good way to start a conversation with someone who is new to the area. quality the second question is so this is one potential match is there some area around this area a within this ellipse so they would fit a second time in there so something I haven't seen for example. Well I see enough white areas that able to fit in there. I don't know if there is a way to get it to be better but it would be nice to see. It would be great to see a match between these two teams. I've never seen one of these before. second time this is the case I mean say I can't be sure that this is a concern it could be a constraint but I'm not sure that it really is a constraint. There's simply an area in the uncertainty on the relevant uncertainty area which I haven't seen. Second time this has been the case. I mean I can’t be sure. It could be an issue but I’m not sure it’s really a constraint because there’�s simply anArea in the Uncertainty on therelevant uncertainty area. I haven�'t seen that. so far and what we would in theory need to do we need to really check this area I can see if we can fit it in here an approximation for that is just compute the just compute  the the size of the ellipse on this circle and theEllipse. So far we have been able to get a very rough idea of how big the circle is. We can now get a better idea of the shape of the ellipse on this circle. So we can get a much more accurate picture of how large it is. over here I compute the again the eigenvalues of those two which tells me so the same for x and y means it's a circle and the larger as one dimension the larger they the main axis is and it just can compare if the smallest eigenvalue is the smallest. The eigen values are the same  for x and  Y means it is a circle. The larger as  one dimension the larger they the main axis is and the smallest is the smallest eigenvalue. in here is larger than the largest eigen vector eigenvalue of this guy. There may be possibility to squeeze it in there just by just comparing the eigenvalues along the dominant excess of those ellipses. I can actually do that again if a very very very large eigenvector is found. In this case, the eigenevalue of the dominant excess of this guy is bigger than the largest eigenvalue of the one of this guy. narrow matches this may very very narrow corridors this approximation may lead to a failure because it may be able to squeeze it somewhere in there but that's not the case for most examples. So this sense I kind of completed the pipeline so this was kind of the more accurate version of what I was trying to do. I think that's a good way to look at it. It's a very different way of looking at it than the other way around. I'm not sure if it's the same way, but I think it's more accurate. the thing we spent most time on and try to solve this criterion - for the test of local an ambiguity. If I find then if I find the picket fence promising we say I just abort and say don't add a constraint if it passes the tests. If it doesn't pass the tests we say we don't need to add the constraint. If the constraint passes the test we say it's OK to add it if it's a local ambiguity. We spent most of our time trying to solve the criterion. do the global ambiguity test or global sufficiency test so it's a globally sufficient order they say a global ambiguity if there's number you D don't add anything and otherwise I have a loop over which I can say with a high likelihood it's globally consistent and there's no local ambiguity. If there is a local ambiguity, you don't need to add anything to the order. If it's global sufficient order, you can add anything you want. If the order is not globally sufficient, it's not globally consistent. and acuity and that's a way actually most a lot of different slam system works also the SEM system that we use in here you've seen this video already where the system we have the robot mapping our campus over here so we start in front of our building continue. And that's one of the ways in which the system works. and acuity is a way in which most a lots of different Slam system works and that’s a way that the SEM System that we used in here works. moving and this one exactly does this so you can see if the area turns red. This is actually the place where it looks for potential loop closures based on the uncertainty and then it tries to find them and collects a few of them groups. M sees that there are a few groups of people in the area that are in a certain position and tries to get them to move in the same direction. This one does this to see if there are any groups that are currently in a position that is likely to close. is a picket fence problem see that there is a local and B an ambiguous situation on a global scale. If not it adds the constraints and in this way builds a map of the environment of this kind of problem. This is basically the front end hears and reimplementation or an 'open source' project. It is a way of thinking about how to solve problems that are difficult to solve on a national or global level. It can be used to help solve problems on a local or global scale, for example. implementation of the method of ethanol snow to make this test over here. We found that as very successful techniques in order to solve the same problem and not add false positives constraints or add only a very very small number of false positive constraints okay. So to conclude we find that as a very successful technique in order for us to solve this problem. We find that this technique is very successful and it can be used to solve many other problems. We conclude by saying that we found this technique to be very successful. this talk today I know that was kind of compared the front ends to the back ends. It was much more back ends than this lecture than front ends. But at least I try to give you an idea on how front end works in the end it was a lot more back end than front end. I hope this gives you some idea of what front end is and how it works. It's a lot of different things but it's kind of like the same thing. I think that's the best way to describe it. strongly depends on the sensors that I'm using and I need to the better I can exploit the individual properties of my sensor the better it is. How this is done exactly strongly depends on. the sensor it's hard to give kind of a general framework for. It's very different for each sensor and it's very dependent on the type of sensor that you're using. I don't know how to explain it to you exactly, but I can give you a general idea of what I'm looking for. that but the approach of this global ambiguity tests and the local ambiguity tests these are important things that a good front that should consider in order to avoid adding false positives constraints. This is done with a single graph partitioning approach this was kind of the techniques there. That but the Approach of thisGlobal ambiguity tests. and theLocal ambiguity test. These are important. things to consider. and this is done using a singleGraph partitioning. approach. This was done with an approach of a single Graph partitioning Approach. this was a technique that was used by the team at Google. the technical - Olson which uses this and yeah again so regarding the the position uncertainty of the platform the higher the uncertainty is in an area the moon bear I need to know the area in order to make the decision easier. is this a global is their global is it a global or a regional? Is this a regional or a national? Is it a city or a town or a village? Is there a city in the middle of the country or a state in the north or the south of the United States? ambiguity or not it's kind of if the uncertainty lips of speakers need to seen all that area to make this is no that's the only place where I actually can match. About the special clustering technique for the original paper where you find all the information here's the information about the technique. The original paper is available online at: http://www.cnn.com/2013/01/23/science/topics/topical-science-topics.html#storylink=cpy. work back in also recognizing places using spectral clustered local matches. This is exactly the approach that I presented here and actually a couple of the slides that I use in here or of the images material at least comes from a tin Olsen and thanks that van for allowing me to use it. I hope that this has helped you in any way. I would like to hear from you in the future. Please contact me via email at jennifer.smith@mailonline.co.uk or on Twitter @jennifersmith. "Authors lectures give a very 10-minute summary of all the most important things of the lecture," he says. "I would like to use that and that's it from my side for the front ends and the only thing I'd like to do authors lectures give is a 10- minute summary of everything that happened in the lecture" He says he has no plans to change the way he presents his lectures. "It's just the way it is," he adds, "and I don't want to change anything."