Machine learning is about how to acquire a model and acquire the parameters of a model, from data and experience. In the next few lectures, we're going to work through a sequence of different takes on machine learning that are going to highlight different subsets of the big ideas on this topic. We'll see today exactly how data kind of goes through the mill and gets turned into a model. And we'll see more in-depth examples and more structured examples of these kinds of problems later on when we talk about applications. In model-based classification, rather than directly learning from errors that you make in the world from experience, think like reinforcement learning model-free. The Naive Bayes assumption is an extremely radical assumption as far as probabilistic models go, but for classification, it turns out to be really effective and it goes something like this. You build a model where the output label and the input features are random variables. There's going to be some connections between them, and maybe some other variables too that might help you build a better model. word depends on the class and also the previous word. Will it be more accurate for classification? It really depends. In general, it will be a little more accurate, but at a cost of having significantly more complicated conditional probabilities to estimate. The principle of machine learning is something called empirical minimization, and we'll get into this in a great amount of detail in the next few slides. All right, we're going to get started again. We are now into the machine learning section which means we are done with the spooky Halloween time ghost-busting section. In a real classification problem, you have to smooth if you're going to use Naive Bayes. Every method is going to have a different incarnation of this. If I crank k to zero, I'm not going to smooth at all, and I'm going to fit my data very well. But the k that's going to be most accurate on my training data is zero, because that's what actually fits the training data best. We're talking a bit about this starting next lecture, but you're eventually going to make your errors.