This week in week three, we're actually going to have some human language, and so this lecture has no partial derivative signs in it. So, I just sort of want to explain something about human language sentence structure, and how people think about that structure. And then talk about how you can make can kind of come for any reason you want, but it might be especially good to come to me if you want to talk about, um, final projects. Okay. Let's get started again. neural, um, dependency parsers. Um, going on just, you know, a couple of announcements. So, assignment two was due one minute ago, so I hope everyone's succeeded in getting assignment two out of the way. Coming out just today is assignment three. In assignment three, what you're doing is building a neural dependency parser, and so we hope that you can put together what you learned about neural networks last week and the content of today, and jump straight right in to building a Neural dependency parser. um, writing a few lines of code at a time. Hopefully that works out for people. Um, if you have any issues with, with that, um, well, obviously, you can send Piazza messages, come to office hours. I mean, the one other thing you could think of doing is that there's sort of a one hour introduction to PyTorch on thePyTorch site, where you down- where you're directed for installing PyTorCh. Um,. now the final mentions, yes. Linguist: I can see patterns, like the cat, a dog, the dog, a cat, et cetera. So, I can also see other examples in my language of the large cat, or a barking dog, or the cuddly cat. And so I want to put those into my grammar. But at that point, I noticed something special, and these things look a lot like the things I started off with. So I could write something like, "The cat by the large crate on the, um, large table, by the door." Right. what you find is that prepositional phrases following the verb in English. But if you go to a different language like Chinese, what you find are the prepositions come before the verb. And so, we could say okay, there are different rules for Chinese, um, and I could start writing a context-free grammar for them. Um,so that's the idea of context- free grammars, and actually, you know, this is the dominant approached linguistic structure that you'll see in linguistics. dependence on other words. So, we have a sentence, ''Look in the large crate in the kitchen by the door''. And if we want to we can give these word, words word classes. But to represent the structure, what we're going to say is, "Well, look here is the the root of this whole sentence" And so, overall, you know, then- let me just so say here, you might want to why do we need sentence structure? You know, the way, um, language seems to work when you're talking. To be able to have machines that interpret language correctly, we sort of need to understand the structure of these sentences. Unless we know what words are arguments and modifiers of other words, we can't actually work out what sentences mean. And so, we need to know what is connected to what in order to do that. And one of the ways of saying, um, that's important is saying, ''What can go wrong?'' Okay. So, this is prepositional phrase attachment. It's sort of seems maybe not that hard there, but you know, it, it gets worse, I mean, it's a real example of a sentence from The Wall Street Journal. cops that are killing. So, this is what we'll say is the subject of kill, is the cops, and I'll just call them the San Jose cops here. And well, there's what they kill which say that, the man is an object of killing. And then while one person is the, the cop using knife to kill the person. And so that's then that this is, um, modifier and here if we complex we call it an instrumental modifier to say that the cops are killing people with a knife. ambiguities in the parsing of English, right? So, here's our prepositional phrase from space. And so, again, we can start to indicate the structure of that using our dependency. So, we have again two possibilities that either we have issues and the dep- and the dependencies of issues is that there are no issues. Okay, that's, um, one. Um, That one is not very funny again. So- so, here is a funnier example that illustrates the same ambiguity effectively. acquisition by Royal Trustco Limited of Toronto for $0.27, $27 a share at its monthly meeting. Boring sentence, but, um, what is the structure of this sentence? Well, you know, we've got a verb here, and we'veGot exactly the same subject, and for this noun,Um, object coming after it. But then what happens after that? well, here, we have a prepositional phrase. And so, well, what we wanna do is say for each of these prepositions what they modify, and starting off there only two choices. pattern of how things are modifying. Once you start having a lot of things that have choices like this, you stop having- if I wanna put an analysis ac- on to this sentence I've to work out the, the right structure. And so, if you get into this sort of combinatorics stuff the number of analyses you get when you get multiple prepositional phrases is the sequence called the Catalan numbers. Ah, but that's still an exponential series. And it's sort of one that turns up in a lot. of places when they're tree-like contexts. So, if any of you are doing or have done CS228, where you see, um, triangular- triangulation of, ah, probabilistic graphical models and you ask how many triangulations there are, that's sort of like making a tree over your variables. And that's, again, gives you the number of them as the Catalan series. But- so the point is, we ha- end up with a lot of ambiguities. So, that's actually a determiner, ah, no issues. Um, and then it's sort of like no heart or cognitive issues. So, heart is another dependent. It'ssort of a non-compound heart issues. And, um, at that point, we ha- have the "Cognitive" as an adjective modifier of the "Issues" and the "No heart", the determiner is just a modifier of "Heart", and then these being conjoined together. modifier of "Experience" and the "Job" is also a modifier of " experience" And then we have the same kind of subject, object, um, reading on that one. Um, but unfortunately, this sentence has a different reading, where you change the modification relationships. [NOISE] We've got this big phrase that I want to try and put a structure of to be used for Olympic beach volleyball, um. And then, you know, this is sort of like a prepositional phrase attachment ambiguity but this time instead of it's a prePOSitional phrase that's being attached, we've now got a verb phrase. to get out of language most of the time. The results demonstrated that KaiC interacts rhythmically with SasA Ka- KaiA and KaiB. We can sort of see this repeated pattern where you have, um, the noun subject here interacts with a noun modifier, and then it's going to be these things that are beneath that of the SasA and its conjoin things KaiA or KaiB are the things that interacts with. [NOISE] I actually mis-edited this. This should also be nmod:with. sentence at the top, submitted and then you say the dependence of submitted, uh, bills were in Brownback. We have a system of dependency labels. So, if you go to the Universal Dependencies website, it's not only about English. You can find Universal Dependency analyses of you know, French, or German, or Finish, or Carsac, or Indonesian, lots of languages. If you have a- a big calling to say I'm gonna build a Swahili Universaldependencies um, treebank, um, you can get in touch. In the later parts of the first millennium, there was a ton of work by Arabic grammarians and essentially what they used is also kind of basically a Dependency Grammar. There was this guy Wells in 1947 who first proposed this idea of having these constituents and phrase structure grammars, and where it then became really famous is through the work of Chomsky. So, in modern work, uh, there's this guy Lucie Tesniere. He sort of formalized the kind of version of dependency grammar that I've been showing you. that wasn't such a good idea, and it turned out to be much better to have these kind of treebank supporting structures over sentences. Once you have a treebank, it's reusable for all sorts of purposes that lots of people build parsers format. But beyond that, this sort of just became necessary once we wanted to do machine learning. So that if is it could parse at 469 sentences a second. I mean, strictly we won over here and we are a fraction behind on UAS. final issue, um, which is we don't want things that, um,. is whether we want to allow dependencies to cross or not, and this is an example of this. So, most of the time,Um, dependencies don't cross each other. Uh, but sometimes they do. And that's sort of rare, that doesn't happen a ton in English, but it happens sometimes in some structures like that. And so, what we say is that the positive sentence is projective if there no crossing dependencies and it's non-projective ifthere are crossing dependencies. And when it's not is when you kind of have these constituents that are delayed to the end of the sentence. to look at in terms of understanding what, um, a shift-reduce parser does. And here's a formal description of a transition-based shift- reduce parser and which also doesn't help you at all. Um, so, instead we kinda look at this example, uh, [LAUGHTER] because that will hopefully help you. So, what I wanna to do is parse the sentence "I ate fish". And yet formally what I have is I have a why I start, there are three actions I can take. And if I would've explored exponential size tree of different parsers, I wouldn't be able to parse efficiently. There's a set of actions and so you gonna build a classifier with machine learning which will predict the right action. Joakim Nivre showed the sort of slightly surprising fact that actually you could predict the correct action to take with high accuracy. Now if you wanna do some searching around, you can do a bit better, but it's not necessary. But so if you're doing this just sort of run classify, predict action, run classify and predict action. We then get this wonderful result. as we chug through a sentence, where we're only doing a linear amount of work for each word and that was sort of an enormous breakthrough. So the conventional way to do this is to say well, we want to have features. And well, the kind of features you wanted was so the usually some kind of conjunction or multiple things so that if the top word of the stack is good, um, and something else is true, right, that the second top word is verb, more to do with each other than others. To build a dependency parser, you're meant to build it with a certain accuracy. And so we can just count up the number of dependencies and how many we get correct. So here in my example, my dependency paths, I've got most of the arcs right but it got this one wrong. So I say my unlabeled attachment score is 80 percent or we can also look at the labels and then my parser wasn't very good at getting the labels rights, so I'm only getting 40 percent. that's sort of similar to when we were making those window classifiers and then we can concatenate a bunch of stuff together. So that gives us in our input layer. Um, so from there, we put things through a hidden layer just like last week. We do Wx plus b and then put it through a ReLU or a non-linearity. And then on top of that, we're simply gonna stick a softmax output layer. So multiplying by another matrix, adding another, um, bias term, and then that goes into the softmax which is gonna give a probability over our actions. bigger, deeper and spend more time choosing the hyper-parameters. Um, Beam search can really help. So in Beam search, you know, rather than just saying, "Let's work out what's the best next action, do that one and repeat over", you allow yourself to do a little bit of search. You sort of say, "Well, let's consider two actions and explore what happens." Um, quick question. Do humans always agree on how to build this trees and if they don't, what will be the agreement of humans relative to [inaudible] [OVERLAPPING] [NOISE]