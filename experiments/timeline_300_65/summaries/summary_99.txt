Third lecture on Foundation mulative AI. We're going to cover chat GPT. Next time we'll talk about stable diffusion image generation. We'll end with the lecture on AI ethics and regulation as well as a panel. It was a really daring bad not obvious at all at the time that this would actually work out. So should be be a lot of fun and just to quickly go through our course schedule as well a little bit right so today is January 16 uh and next time we will talk about emerging Foundation models basically Foundation models generative AI. GPT relies on a lot of tricks and Engineering insights and breakthroughs that we're not going to cover. We apply this self-supervised learning where we learn without label data so we can get as much data as we want because there's no human being in the loop. What we get from this you know by learning from observation and learning from the data directly is a very contextual and relational understanding of meaning. We'll talk about something that's extremely engineering heavy in you know chat GPT. be useful for you without getting into all the engineering details but of course in real life those engineering details really really matters and are very very hard to get right. I think that again like we talked about a little bit of a theme here right is that the why this new AI is so powerful is because it doesn't Force things to comply to Simple Rules right it kind of abandons our ability to understand and compress what we're seeing and deals with that chaos directly. I took this quote from a general from the 18 and 1700s and he says that P Theory which sets itself in opposition to the mind. We're going to create scores or predictions for all words in the L like in the human you know vocabulary in the English vocabulary that sounds extremely expensive and it is quite expensive and so I have different tricks to make this work. Only one word would be correct but it gives a lot of feedback as well because there's a lot. of information knowing which ones are not correct and and how we're going. to do this is that we'regoing to create these U probability scores meaning that these are just non- negative numbers and they all sum to one. The CTP is a very simple approach but it's it's a certain scale that's that's never been seen before and really that's what a big part of open eyes is. Just training the final model cost around $5 million just in in Compu electricity bills right that's how huge and much compute they spent on this. The first version it was using 175 billion parameters and just training the the the final models was using around $4 million to $6 million a year and that's not including all the iteration. bet and and the research there is like well you know we've been doing this language modeling for quite some time trying to understand Language by predicting the next word based on previous words and we're using it for certain things but I mean know very few thought and were convinced that if you just scale this up big enough it will become a multitask Sol and show humanlike Intelligence and that this actually really will work. People talk a little bit of this emerging abilities because also it's not linear right like you start adding and putting putting more and more compute and parameters and like oh it's still not very useful but then at some point just start being like extremely useful so it was it was a huge kind of leap of faith as well for open eye to say like well we're just going to go all in. Transformer part is extremely extremely important so there's a debate a little bit what was the most influential part of making uh CHP and large language model possible uh Transformer is definitely a significant part of it and and I'll let you judge for yourself but uh I think it's less important than the actual modeling perspective that we've we've come up with. In order to understand the Transformer we're going to start to thinking about how we can process sequences so text is just sequence of words. is that for every step here that's label with the same uh digit you know they can all be done in parallel. This is key because in in deep learning we use this uh um computer is called gpus. If we can make multiple steps into single step in parallel this is a single cost. We want to run things in parallel as much as possible so here basically you know this be a cost of four because all these different numbers can be run in parallel uh so this would just be acost of four and then of course processing this whole sequence will be a costs of nine. these are extremely extremely popular and a version of them called uh lstm long short-term memory networks um it performs really really well and some people say it performs you know almost better than Transformers a lot of times but they just take them longer to train because we're going to realize why it one a point but they work really really good and also notice here somehow that uh this was very very intuitive for researchers to say like well text we read text from left to right we process words one at a time and therefore our models should to to to tble to learn effectively. when to the Target so we kind of we're we're not going to enforce this quential structure we're just going to directly let the information flow from the previous word to the current word Etc right what we've seen so far and then if in the third step as well we also just go to let information flow directly and have a direct connection to the previous words and not force things to happen sequentially um and the important thing to notice here right before when we were processing things sequentially you had to wait for the previous step to finish to do the next step right but here you don't because here everything is processed independently. in parallel it's the same step and this is true for all of these steps yes this sense when we compute a output distribution over all the words later like as a prediction we talked about. In a recuit network things are processed one at a time so the the model can figure out that you know Financial comes after the because it sees the first and then Financial but in a Transformer you know in the below here like if the only thing you see is the word and they're all F to you know. be efficient and work well so now we're just going to look at the F last part which is the chat part um so you know you you you train this model now that you call DPT 3.5 or something and now you want to turn turn into chat GPT right so we have a uh a really uh good model basically we've done 99% of all the work that's that's required and a lot of people still kind of debate how important this last step is but open is it does a difference. Just goes off a little bit like here for example when you say you know I went to the financial and then just you know some small error happens and it goes off the road to restaurant like somehow you know they started seeing that okay now it's basically go Haywire because it went off and it's in a different space than it's been trained on. So somehow we want to be able to say like well if you find yourself you know alittle bit off the the path you should be ableto find your way back to be as as robust as possible to any kind of use case. is about like how do you figure out what actually helps you reach your goal and optimizing your score function even if it's delayed um okay so another thing in doing this that's very very important it's exploration versus exploitation so let's say now basically that our model has seen these two different cases and have received two feedbacks right. In one of these you will got a pretty good score and in one uh you know in the lower here you got apretty bad score so then basically that means that we just exploit the information that we've seen so far and do the best. gratification actually leads to very non- GRE and and independent robustness so these are the consequence of applying reinforcement learning where you only get feedback at the very end so there's less you know supervision right you're more on your own and you have to deal with an uncertainty of not having constant feedback. Okay so we've solved our problems uh we used human I mean we actually paid human beings for label data which is not maybe that goes against our principles here but but we still did it because open AI is Rich so they paid people to label things. so to summarize what's the big fuss well just predict the next word based on previous words that's basically it. Transformers allows us to leverage more data and train quickly because we can paralyze paralyze all these steps in during training and uh then uh when we've done this we have a really really sophisticated model. We spent 99% of our time in computer on on this preaching this Transformer to predict thenext word basedon previous words then we can adjust things to make it a little bit more nice to interact with if you're a human being. and self Suess learning care more about both aspects somehow. Next time we will talk about uh we'll do a similar Deep dive into stable diffusion there will be self supervised learning and Foundation mod an AI but I think it's going to be slightly more conceptually interesting um so should be a lot of fun and yes please go to the website for more information Etc and if you have any questions feel free yeah can I something can I assume that probability on that after there changes based on the subject of the totally yes yes yes great question okay. it's going to be able to collapse and create distribution that's much more targeted uh so is if it doesn't have data run a context or about you and your interest as a person it it won't be can to tailor to you right they cannot create Magic out of thin air it can only do the best of The Prompt and knowledge has so far. Al that's what's so fascinating with self suus learning because now it's the building block that makes all other Technologies in AI actually fruitful. The Transformer inspired by research about how we our kids learn right how the brain works. word mask it and try to predict it based on the surrounding words um yeah the answer to that is we used to do that way and predict that and it works better uh but due to engineering you can Bas basically kind Transformer you can maybe this can be like a homework for you but if you look at Transformer Works uh if you mask a word then you will like then you can only um okay I if you do this you know if only predict the last word based on previous words. each Target by itself but if you mask then you maybe mask like 15% of the words and then it's only like that's only 50% then in terms of getting that feedback uh so it just in end of the day it means that uh when you try this empirically doing it aut agressively and this trick to be able to use the each you know each word is a target itself just leads to better performance at this task of generating export based on previous words uh so yeah so it's I guess it's engineering empirical okay. you like ask a factual question and they give us something that's wrong and they're confident about it maybe that's bad so like I think what we're starting to see is that they um are very humanik even it's in its mistake they're like well they're biased and they have stereotypes around things uh they also like we do I mean they suffer from wishful thinking and some type of imagination where they rather be you know make you happy than being completely truthful uh so these are things that you then somehow have to balance uh yeah and then so that's like some problems immediately arrived. Video is a next level in terms of compute what it needs because they have tons of videos a frame is very expensive because it's a high dimensional picture or image but clearly you can learn a lot about the World by looking at videos right. You can even sort to understand how human beings work even better because you can see people being upset or sad or happy whatever right in in a video and start picking these cues up and you can connect the vision part to the text part and get a multimodality model that's able to do both.