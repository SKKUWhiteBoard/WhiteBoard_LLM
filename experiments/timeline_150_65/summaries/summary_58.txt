Micheal FEE: Today, we're going to continue with our plan for developing a powerful set of tools for analyzing the temporal structure of signals. Last time, we covered Fourier series and the Fourier transform. Today we'll talk about the convolution theorem, noise and filtering Shannon-Nyquist sampling theorem, among other things. The series of three lectures will continue on Friday and Saturday. The lectures will be webcast at 10:30 a.m. and 11:00 a. m. ET. and spectral estimation. And next time, we're going to move on to spectrograms and an important idea of windowing and tapering, time bandwidth product, and some more advanced filtering methods. So last time, I gave you this little piece of code that allows you to compute the discrete Fourier transform using this Matlab function FFT. And we talked about how in order to do this properly, you should first circularly shift. And then circular shift again to get the negative frequencies in the first half of the vector. This shows an example where we took a cosine. as a function of time. At some frequency, here, 20 hertz. We compute the Fourier transform of that and plot that. So that's what this looks like. Here is a cosine at 20Hertz. And you can see that what you see is the real part as a. function of frequency. It has two peaks, one at plus 20Herz. And the imaginary part is 0. So any questions about that? Feel like I didn't say that quite as clearly as I could have? OK. we plot power in log base 10. A difference of an order of magnitude in two peaks corresponds to a unit called a bel, b-e-l. More commonly used unit is called decibels, which are 10 decibel per bel. So decibles are given by 10 times the logbase 10 of the power of the square magnitude of the Fourier transform. Does that make sense? Good question. All right, any questions about this and what the meaning of decibela is? which is 2 bels, which is 20 decibels. All right, now I just want to show you one important thing about Fourier transforms. There's an interesting property about scaling in time and frequency. So if you have a signal like this that's periodic at about-- I don't know, it looks like-- OK, there it is-- about 5 hertz. If you look at the Fourier transform of that, you can see a series of peaks, because it's a periodic signal. Fourier transform pairs are functions where you have a function. You take the Fourier transform of it. You get a different function. If you take the transform of that, you go back to the original function. OK? All right, so that was just a brief review of what we covered last time. And here's what we're going to cover today in a little more detail. We'll talk some more about the idea of Fourier transforms pairs. And we'll also talk about Fourier transformations in general. Convolution in the time domain looks like multiplication in the frequency domain. It's a very powerful theorem. We're going to talk about the Fourier transform a Gaussian noise and this power spectrum ofGaussian noise. We'll talk about how to use the convolution theorem to understand Fourier transforms of different types of functions. And then we'll look at how to apply it to different kinds of data sets, such as data from a computer or a computer network, for example. do spectral estimation. And we'll end up on the Shannon-Nyquist theorem and zero padding. And there may be, if there's time at the end, I'll talk about a little trick for removing the line noise from signals. OK, so let's start with Fourier transform pairs. So one of the most important functions to know the Fourier transforms of is a square pulse like this. The Fouriertransform of asquare pulse is a function called the sinc function. It's basically a sine wave that is weighted so that it's big. The Fourier transform of a square wave, of this square wave of with 100 milliseconds, is a sinc function. If we take that pulse and we make it narrower, 25 milliseconds, then you can see that the sincfunction, it's the same sinc. function, but it's just stretched out in the frequency domain. As you make the pulse in time longer, the bandwidth gets smaller. And it turns out that the product of the width in time and thewidth in frequency is just a constant. make the pulse in time wider, then the Gaussian in frequency space gets narrower. This is where the Heisenberg uncertainty principle comes from, because wave functions are just-- you can think of wave functions as just functions in time. So if the particle is more lo-- [AUDIO OUT] in space, then if you compute the Fourier transform of that wave function, it's more dispersed in momentum. OK, so the uncertainty in momentum is larger. It's very cool. spacing in the time domain is just equal to 1 over the spacing in the frequency domain. So that's another Fourier transform pair that you should remember. OK, convolution theorem. Imagine that we have three functions of time, y of t, like this one. We could calculate the Fourier transforms of that. And that's capital Y of omega. And then we have some other function, x of t,. And its Fouriertransform, X of omega, and another function g of tau and its Fouriers transform, capital G of Omega. So in this case, we're defining y as the convolution of g with x. a convolution. The convolution theorem tells us that the Fourier transform of y is just the product of the Fouriers transform of g and x. So that, you should remember. All right? I'm going to walk you through how you derive that. I don't expect you to be able to derive it. But the derivation is kind of cute, and I enjoyed it. So I thought I'd show you how it goes. So here's the definition of the convolved. What we're going to do is we'regoing to just take the. Fouriertransform of y. Capital Y of omega is just. the integral over all time dt y of t e to the minus i omega t. actually reverse the order of integration. We're going to integrate over t first rather than tau. Then we can move the g outside the integral over t, because it's just a function of t Tau. So now, we have an integral dt x of t minus tau e to the minus i omega t. And what do you think that is? What would it be if there were no tau there? If you just cross that out and that, what would that be? Anybody know? What that's? The convolution theorem relates convolution in the time domain to multiplication in the frequency domain. So if you take a Gaussian, some window centered around 0 in time-- this is a function of time. So what we're going to do is we'regoing to calculate transform of aGaussian times a sine wave. So I just showed you the Fourier transform of the Gaussian. Now I'm going to show you how to do the same thing with the sine waves. now right? So there's a little Gaussian pulse in time. We're going to multiply that by this sine wave. And when you multiply those together, you get this little pulse of sine. OK? [WHISTLES] Sorry, constant frequency. Boy, that's harder to do than I thought. [WHistles] OK, just a little pulse Of Sine. So what's the forehead transform of that? Well, we don't know, right? We didn't calculate it. But you can actually just figure it out. of that and can involve it with the Fourier transform of that. So let's do that. If this is 200 milliseconds wide, then how wide is this? AUDIENCE: [INAUDIBLE] MICHALE FEE: It's 1 over 200 milliseconds, which is what? 5 hertz, right? 1 over 0.2 is 5. The Fouriertransform of this sine wave-- and I think I made it a cosine instead of a sine. You can just know in your head that that's the product of a Gaussian and a sine wave, or cosine. And therefore, the Fourier transform of that is the convolution of aGaussian with these two peaks. There are many, many examples of interesting and useful functions in the time domain that you can intuitively understand what their Fouriertransform is just by having this idea. It's very powerful. Here's another example. We're going to calculate the Fouriers transform of this square windowed cosine function. So it's a product of the square pulse with this cosine to give this. Gaussian noise is a signal in which the value at each time is randomly sampled from a Gaussian distribution. It's that kind of wiggly, peaky thing. The Fourier transform of that is just two peaks. And we're going to eventually bring all these things back together. OK? All right, so what is Gaussian noise? So you can do that in Matlab. That's a very simple function. This returns a vector of length N, sampled froma normal distribution, with variance 1. wanted to show you what the autocorrelation function of this looks like, which I think we saw before. So if you look at the distribution of all the samples, it just gives you a distribution that it has the shape of a Gaussian. And the standard deviation of that Gaussian is 1. Now, what if you plot the correlation between the value of value of this function at time t and time t plus 1? Is there any relation? So they're completely uncorrelated with each other. correlation of this function with itself at different time lags. If we do that, you get a 1 at 0 lag and 0 at any other lag. So that's the autocorrelation function of Gaussian noise. All right, now what is the power spectrum? So we can take this thing-- and, remember, when we plot the power [AUDIO OUT] just plot the square magnitude of just the positive frequencies. Why is that again? Why do we only have to plot thesquare magnitude? of the positive frequencies? AUDIENCE: [INAUDIBLE] Gaussian, so they're all [INAudIBLE] MICHALE FEE: Yep. We're going to come back, and I'm going to show you that on average, if you take many different signals, many copies of this, and calculate the power spectrum and average them all altogether, it's going to be flat. But for any given piece of noisy signal, the powerSpectrum is very noisy. so now let's turn to spectral estimation. How do we estimate the spectrum of a signal? So let's say you have a signal, S of t. And you've got a bunch of short measurements of that signal. What you can do is calculate the power spectrum, just like [AUDIO OUT] for each of those signals. So literally, we just do what we did here. We have a little bit of signal. We Fourier transform it, take the square magnitude. And now, you average together all of your different samples. into short pieces. extracting that little piece of signal from this longer signal is essentially the same as multiplying that long signal by a square pulse. So that process of taking a long signal and extracting out one piece of it has a name. It's called windowing. Sort of like you're looking at a scene through this window, and that's all you can see. OK, so one way to estimate the spectrum of this signal is to take the signal in this window. And then apply this window to the next piece. problem with that? Why might that be a bad idea? Yeah. So instead of multiplying this signal by square pulses, we sample the signal by applying it by little things that look like little smooth functions, like maybe a Gaussian, or other functions that we'll talk about do an even better job. OK? All right. So that process is called tapering, multiplying your data by a little [AUDIO OUT] paper that's smooth, unlike a square window. Computing spectral estimates from each one of those windowed and tapered pieces of data gives you a very good estimate of the spectra. a noisy signal that has a little bit of underlying sine wave in it, if you take the autocorrelation of that function, you get a delta function and then some little wiggles. So there are ways of pulling periodic signals, periodic structure out of noisy signals. But it turns out that this method of spectral estimation [AUDIO OUT] did the most powerful way to do it. I'm just going to show you one example. This blue function here is noise plus the red function. And it's buried in the noise, so that you can't see it. we're learning about, you can see that that signal buried in that noise is now very easily visible. So using these methods,you can pull tiny signals out of noise at a very bad signal to noise ratio, where the signal is really buried in the noise. So it's a very powerful method. And we're going to spend more time talking about how to do that properly. All right, so let me spend a little bit more time talk about the power spectrum of noise, so that we have a better sense of what that looks like. So remember, I told you if you take a sample of noise like this and you estimate the spectrum of it, you compute the powerSpectrum of one sample. of noise. In order to estimate what the spectrum of noise looks like, you have to take many examples of that and average them together. And when you do that, what you find is that the power spectrum of Gaussian noise is a constant. It's flat. The power spectrum, really, you should think about it properly as a power spectral density. So there is a certain amount of power at different frequencies in this signal. And forGaussian noise, that power spectraldensity is flat. It’s constant as a function of frequency. low pass, by convolving a signal with a kernel. So when you convolve, that's the kernel for a low pass. And for a high pass, anybody remember what that looks like? AUDIENCE: [INAUDIBLE] MICHALE FEE: Yep. OK, so this was how you filter a signal. We're going to talk now about how you do filtering in the frequency domain. And then you subtract off a low-pass filtered version of the signal. if filtering in the time domain is convolving your [AUDIO OUT] with a function, what is filtered in the frequency domain going to be? MICHALE FEE: It's going to. be multiplying the Fourier transform of your signal times what? The Fourier transforms of things like that. All right, so here's an example. So in blue is the original Gaussian noise. In green is the kernel that I'm smoothing it by, filtering it by. Convolving the blue with the green gives you the red signal. Noise.noise.com: How does a Gaussian filter work? The power spectrum of that signal is going to just be aGaussian. Filtering in the frequency domain means multiplying the spectrum of your signal by a function that's low at high frequencies and big at low frequencies. Does that makes sense? So convolving our original blue signal with this green Gaussian kernel smooths the signal. It gets rid of high frequencies. It's that simple. Any questions about that? Well, yes-- multiply in the frequency, could you theoretically multiply by anything and that would correspond to some other type of filter? So why don't we just like throw away high frequencies? Or something like multiply by a square in thefrequency domain and correspond tosome different filter we don't know. MICHALE FEE: Yeah. You can do that. It would be convulsing your function with a sinc function. It turns out that's-- the reason you wouldn't normally do that is that it mixes the signal across all time. Gaussian, you're not adding some of the signal here that were over here. Convolving with a sinc function kind of mixes things in time. So normally you would smooth by functions that are kind of local in time, local in frequency, but not having sharp edges. So we're going to talk about how to smooth things in frequency with signals with kernels that are optimal for that job. That's Thursday. What would a high- pass filter look like in the frequency domain? So high-pass filter would pass high frequencies and suppress low frequencies. The Wiener-Khinchin theorem relates the power spectrum of a signal with the autocorrelation. If we plot this on a log plot in decibels, a Gaussian, which is e to [AUDIO OUT] like f squared, that's minus f squared. That's why on alog plot this would look like an inverted parabola. Any questions about that? I want to tell you about a cool little theorem called the Wiener/Kinchen theorem. The power spectrum of a signal is just the Fourier transform of the autocorrelation. What's the width of this in time? How would I get that from here? How are the width in time and frequency related to each other for-- AUDIENCE: [INAUDIBLE] MICHALE FEE: Right. The power spectrum is the magnitude squared of the Fouriers transform of a delta function. It's a constant. And how about our smoothed? Our smooth signal has a power spectrum that's a Gaussian in this case. width of this in time is just 1 over the width of [AUDIO OUT] So you have to take the full width. Does that makes sense? OK. Wiener-Khinchin theorem, very cool. All right, let's talk about the Shannon-Nyquist theorem. Any signal that has discrete components and frequencies is periodic in time. Anyone who's acquiring signals in the lab needs to know this. It's very important. And we talked about how if you have a signal that's periodic in. time, that you can write it down as a set of sets. frequencies that are integer multiples of each other. In these signals, time is sampled discretely at regular time intervals. Discretely sampled in time means that the Fourier transform is periodic. In fact, really be thinking that those discreetly sampled signals have a Fourier transforms that's actually periodic. There's another copy of that spectrum sitting up here at 1. It's also periodic. And I've been showing you the Fouriers transforms of those signals. But I've only been showingYou this little part of it. over the sampling rate and another copy sitting up here. So there are copies of this spectrum spaced every 1 over delta t. The sampling rate needs to be greater than twice the bandwidth of the signal. That means delta t is too big. These copies of the spectrum are too close to [AUDIO OUT] and they overlap. That overlap is called aliasing-- a-l- i-a-s-i-n-g. It's a little strange, but we'll push on because it's going to be more clear. signal has some bandwidth B that in order to sample that signal properly, your sampling rate needs to be greater than twice that bandwidth, 1, 2. Actually, there was actually recently a paper where somebody claimed-- I think I told you about this last time-- there was a paperWhere somebody claimed to be able to get around this limit. And they were mercilessly treated in the responses to that paper. So don't make that mistake. Now that's an amazing claim. Right? You have a [AUDIO OUT] time. All right, it's wiggling around. of what's happening between those samples. And I can perfectly reconstruct the signal I'm sampling at every time point, even though I didn't look there. So how do you do that? Basically, your sampled signal, you're regularly sampled signal,. has this spectrum-- has this Fourier transform with repeated copies of the signal, repeated copies. of the spectrum. The spectrum of the original signal is just this piece right here. So all I do is in the frequency domain I take that part. I keep this, and I throw away all those. square wave in the frequency domain equivalent to in the time domain. Multiplying this spectrum by this square wave, throwing away all those other copies of the spectrum and keeping that one is multiplying by a square wave. MICHALE FEE: Convolving the timeDomain sinc-- that regular train of samples, convolving that with a sinc function. If we take that function, which is a bunch of delta functions here, here, and here, it's like doing what? The Nyquist-Shannon theorem says that we can perfectly reconstruct the signal we've sampled as long as we sample it at a sampling rate that's greater than twice the bandwidth of the signal. So there's this cute trick called zero-padding, where you don't perfectly reconstruction the original signal, but basically you can interpolate. So you can extract the values of theOriginal signal times between where you actually sampled it. OK? And basically the trick is as follows. We take our sampled signal. We Fourier transform it. And what we do is we just add zeros. When you inverse transform, inverse Fourier transform, what you're going to have is your original samples back, plus a bunch of samples in between that interpolate. So you can essentially increase the sampling rate of your signal after the fact. Pretty cool, right? Again, it requires that you've sampled at twice the bandwidth of the original signal. Yes. From nearly all applications, you have a pretty good idea of what's going on. And then when we inverse Fouriers this, you can see that you have an array of zeros between and make it a longer vector. good sense of what the frequencies are that you're interested in a signal. And then what you do is you have to put a filter between your experiment and your computer that's doing the sampling that guarantees that it's suppressed all the frequencies above some point. And that kind of filter is called an anti-aliasing filter. So in that case, even if your signal had higher frequency components, the anti-Aliasing filter cuts it off so that there's nothing at higher frequencies. Does that makes sense? see something at the wrong frequency. That's an example of aliasing. OK? OK, so here's anexample. We have a 20 hertz cosine wave. I've sampled it at 100 hertz. So I'm, you know, 5-- so what frequency would I have to sample this in order to reconstruct the cosine? I'd have to samples at least 40 hertz, so those are the blue points. And now, if I do this zero-padding trick, I Fourier transform. I do zero- padding by a factor of 4. That means if I take the Fourier. transform signal and I'm now making that vector 4 times as long by filling in zeros, then I inverse. sample the signal in the time domain and then add a bunch of zeros to it before you Fourier transform. And that gives you finer samples in the frequency domain. And I'll show you in more detail how to do this after we talk about tapering. And it's very simple code actually. Matlab has built into it the ability to do zero-padding right in the FFT function. OK, let's actually just stop there. I feel like we covered a lot of stuff today.