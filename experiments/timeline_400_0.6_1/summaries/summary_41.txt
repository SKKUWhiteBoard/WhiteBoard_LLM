homework two is out now. This weekend sessions will be having some more background on deep learning. We're also gonna be reaching, uh, releasing by the end of tomorrow, what the default projects will be for this class. Um, and those proposals will be due, um, very soon, er, in a little over a week. The assignments, [inaudible] are they limited to TensorFlow? asked the question if, if the assignment is limited toTensorFlow. I'm pretty sure that everything relies that you're using Tensor Flow. Last week, we were discussing value function approximation. Today we're gonna start to talk about other forms of value function approximations using deep neural networks. We're mostly not gonna talk so much about enormous action spaces, but we are gonna think a lot about really large state spaces. And so, this could be things like a laser range finder for our robot, which told us how far away the walls were in all 180 degree directions. And the key thing was that we don't know what the true value of a policy is. So, the two ways we talked about last time was inspired by the same. The number of data points you need tends to scale with the dimension. So, one alternative that we didn't talk so much about last time is to use sort of a really, really rich function approximator class. Um, in general we're going to have almost no theoretical guarantees the rest of the time, but in practice they often work really well. And so, what we're gonna talk about today is thinking about deep neural networks which also have very flexible representations but we hope they're gonna scale a lot better. Deep neural networks are artificial connections with artificial neural networks inside our brain. They are used in unsupervised learning like predicting whether or not something is a cat or not or, you know, an image, uh, of a particular object, um, or for regression. They combine both linear and non-linear transformations, and they need to be differentiable if gradient descent is to be used to fit them. In the last 5 to 8 years, there's auto differentiation. So, you don't have to derive all of these gradients by hand instead. In 1994, we had TD backgammon which used Deep Neural Networks. And then we had the results that were kind of happening around like 1995 to maybe like 1998, which said that, "Function approximation plus offline off policy control, plus bootstrapping can be bad, can fail to converge" So I think it sort of really changed the story of how people are perceiving using, um, this sort of complicated function approximation, meters, and RL. And so, perhaps it was natural that, like, around in like 2014, DeepMind and DeepMind combined them and had some really amazing successes with Atari. DQN, deep Q-learning addresses these is by experienced replay and fixed Q-targets. Experienced replay, prime number if you guys have heard about this, if you learned about DQN before is we're just gonna stroll data. So, even though we're treating the target as a scalar, the weights will get updated the next round which means our target value changes. So this is nice because basically it means that you reuse your data instead of just using each data point once, you can reuse it and that can be helpful. Of what the agent is doing. So, remember the agent's just learning from pixels here how to do this. Um, and the beginning of its learning sort of this policy. You can see it's not making- doing the right thing very much, um, and that over time as it gets more episodes it starting to learn to make better decisions. So this is really cool that sort of it could discover things that maybe are strategies that people take a little while to learn when they're first learning the game as well. Replay is hugely important and it just gives us a much better way to use the data. Double DQN is kind of like double Q learning, which we covered very briefly at the end of a couple of classes ago. Greedy Policy is where we average networks, average reward networks, and then use one of the Qs as the target for the other network. Then we update Q2 with 50 percent probability, we pick the next action from the next network, this is a pretty small change. CNN's John Defterios asks students to vote on whether they think it matters which of four possible replay backups to do. If you pick any of the four, you're going to get the same value function no matter what you do. It matters the order in which you do it, so ordering can make a big difference, he says. Defterio: I think to say like what should we be putting in our replay buffer, not only do we wanna think about what- what should be in a replay buffer but also what order do we sampled them? It could be super tempting to try to start, by like implementing Q learning directly on the ATARI. Highly encourage you to first go through, sort of the order of the assignment and like, do the linear case, make sure your Q learning is totally working. Even with the smaller games, like Pong which we're working on, um, it is enormously time consuming. It's way better to make sure that you know your Q Learning method is working, before you wait, 12 hours to see whether or not, oh it didn't learn anything on Pogge.