then we are going to continue with the second part of the lecture today which focuses on the problem what actually happens if the gaussian assumption that i have about my constraints doesn't hold. You will get so called outlier observation which is far away from what the real observation would be and as you will see in some small examples having this outliers in your optimization problem is something which hurts dramatically which actually screw up your solution. Already a few outliers can lead to a environment model which is completely unusable for doing any navigation task. a gaussian the problem is if you have outliers so let's say i measure always. one meter one meter 10 99 centimeters one meters one meter now an outlier 100 meters that's actually something which screws up my solution dramatically. they get a completely wrong estimate on what the environment looks like. If we would have the possibility to integrate a multi-modal distribution here that would actually be a nice beneficiary and what i want to talk about here today is how to integrate first multimodal distributions and second having the ability to deal with outliers. If you have structures in the environment and there's a lot of clutter in the scene the clutter even if it has a repetitive pattern may lead to a multimodal belief about what the relative transformation between two poses let's say. Other things is gps can even be problematic if you have this called gps multi-pass problems you have reflections of the gps signal based on larger buildings. You may get beliefs or you may get outlier measurements and the question is how can we actually take that into account. a corridor through what can be actually sit here or here and his local perception will actually match quite well so you may get one of those multimodal beliefs where you say i mean the mode is roughly where the mean of that gaussian would be if i would approximate this function by a gaussian but maybe i'm ending up here in another mode. Having the ability to take into account multi-modal beliefs is actually helpful there's another real world example so this is the intel research lab data set that you have also experienced. over here so this is a single constraint you can already see that you don't have a straight wall over here anymore so it's kind of bended a little bit like this due to the single constraint which obviously has a really really large error so the least square error minimization at the error is squared error term tries actually to minimize that if we add i think whatever and two three four five i think there were 10 constraints 10 wrong constraints the map actually gets so distorted that is unusable for navigation. and in this case screwed up the measurement how can we incorporate that into the graph based slam approach? So the problem that we actually have is if we look to our um an individual constraint so the lack of an observation given the given the current configuration of the nodes was a gaussian distribution. So that's what we had before if we now say okay we would like to have simply a sum of gaussians in order to have a multimodal constraint. So we assume we have normalized this the sum is a pretty good starting point for the for when searching for problems. don't have a single constraint we have a number of constraints right hundred thousand millions of constraints how do we combine those constraints if we minimize the squared error what are we actually doing if you're minimizing the squarederror we are minimizing this expression over here. If we go to the log like negative log likelihood we're going to optimize here this term here minus a constant and here we can't go further than that that's a problem there's where it fails do you see what is the dirtiest way for you to fix this? The key trick is to simply ignore all of them except the most prominent one. If the if the means of these gaussians are far apart from each other the approximation error isn't that big if they are near each other disaster or disaster but may have big errors. If you move the max operation in here if you're maximizing a function or you're minimizing a lock of that function is equivalent so we can move the lock into in there and then have our problem solved that's kind of a nice thing. a really nice way to handle this problem quite practical because if i need to integrate that into a current optimization system i can actually do that very very easily. The only thing i need. to do whenever i experience one of those multimodal constraints i don't compute the error i have to compute the. error for every mode and simply select the one which has the best performance. The nice thing is that actually between iterations you can the system can swap between different modes and therefore the optimization in one iteration takes into account only one mode of the gaussian. kind of in the system swaps to this other one if it is an outlier there's a very high likelihood this will swap to that unless it's an outliest which you have a bad initial guess. If you do this you can actually add 1 10 100 constraints and those are constraints which simply swap to the other mode and don't harm the optimization much. Even if you look to the to the runtime error if you do the red one are the multimodal ones and the other one the regular solution here with this telescopy composition. is a bimodal distribution for the inline and the other one for the outliers the red is in leia blue's outlier you can also handle those cases say i'm either there or there or somewhere else. In most cases actually the vehicle executes what you tell the vehicle to do but in some cases simply doesn't move so this max mixture idea is actually a pretty easy idea pretty simple idea just reply funny no one has done that in robotics until recently a few years ago the first one was edwin olsen and pratik. that's actually a nice thing so um another thing is it can handle both things at the same time data station errors as well as multimodal constraints. So the combination of outlier rejection and dealing with wrong data associations is actually kind of nice we also can do this obviously in 3d. So this is again this data set with the sphere that we have seen before robot moving in a virtual sphere with constraints so this is gauss newton and this is the max mixture gaussNewton and um so you can see here there's a non-perfect alignment in here. Just replace this information matrix here with a variant which has a scaling factor so a constraint dependent scaling factor added to that. This leads to the case that constraints which are far away from what we expect have a smaller influence on the optimization. There's actually closed form you can derive that under certain properties where you end up with this operation. The key idea the intuition behind that is if i have a constraint which have a large error so where the um the current configuration is far from what the constraint tells me just reduce the uh or increase the uncertainty that is associated to that so decrease the information matrix. in this area kind of the the core center of attraction both both perform equally well because there's no scaling involved but the further you move out the more the red curve gets scaled. So the the error is weighted down the further i'm away however we still have a linearization point. So if i compute the jacobian over here i still has a jacoobian which drags the system into the right direction so even if i initialize that quite far away i still get kind of pushed into theright direction if you have a small video. Max mixture as well as for dcs is that kind of the tails of this gaussian distributions contain too few probability mass they're too close to zero. Outlier which is really far away from the current estimate the whole mole is tracked in this direction. If you have constraint which introduce large errors these are these outliers. This can actually screw up the optimization when computing the minimal error configuration so one way you can do is or fits into the framework of its so-called robust m estimators which intuitively say we don't assume a gaussian distribution. now you get different properties in the optimization so if you use um if r is just the quadratic function then we exactly have the original problem that's what we minimized x of minus uh squared error so if we have this one we have we examine exactly in the gaussian world and now there are different techniques how we can actually address that one thing is we could take simply the absolute value so we don't square it just take theabsolute value of the error that's not the parabola that we have but the absolute function. There are different ways of uh kind of row function that we can actually plug in there and um that we're then trying to minimize. The choice of this this function raw kind of encodes the noise properties that you expect if you take a quadratic function you live in the gaussian world. The system optimized according to a different cost function but this allows you to take into account for example these these heavier tails so that outliers still are not weighted that dramatically and impact your solution so much. optimize the log likelihood so this row function goes into the log likely but you still can do compute the logarithm which is something you need to do in order to come up with an effective minimization procedure. In most realistic data sets and situations when you deal with robotics there's a non-zero probability that there will be a datasization error in there. The more likely this data association error the the worse it gets if you optimize without taking these outliers into account and that's actually an easy way to integrate that. here that by changing this function you can't get much better behaviors kind of deciding which function to use for the underlying optimization problem is not on it's not always an easy and easy choice so this requires some expert knowledge some good intuition on coming up with the way with one of those functions. Next week which is the last week of the term i will briefly talk about front ends and give kind of a short summary on what typical front ends exist obviously we're not going to all the details as we did that here.