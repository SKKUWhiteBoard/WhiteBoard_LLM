Last time we talked about some of the kind of the bigger questions in deep learning theory. And today, we are going to start talking about the optimization perspective in deeplearning for two lectures. The main focus is to analyze what the functions you are optimizing look like so that you can use some trivial or some standard optimization algorithm for it. The bigger question we are trying to address here is that why many optimization algorithm are designed for convex functions. But why they can still work for nonconvex functions? In calculus, x is a local min of the function f if there exists an open neighborhood-- let's call it n-- around x such that, in this neighborhood n, the function value is at least fx. If x is the local min, it means that the gradient of fx is 0. The gradient square of the Hessian of f is PSD. So these are necessary conditions for being a local minimum, but not vice versa. So why? I guess a simple example here, I cannot-- it's easy to come up is that you just say maybe-- actually, I'm not taking the simplest one. execute our prime. So this is what will happen. So here is a condition called strict set of conditions. And if you certify the condition, then you remove those pathological cases which requires high order derivatives. So-- sorry. Strict-saddle condition-- so in some sense, I guess I'm not sure whether this makes sense before I define it. But generally, you are basically saying that you want to rule out this kind of somewhat subtle possible candidate of local minimum. So every point, whether it's a local minimum or not, can be told from examine only the first order gradient. is the definition. So we say f is alpha, beta, gamma strict-saddle if, for every x in RD, it satisfies one of the following. So if you can satisfy number one here, you cannot be a local minimum. You can have a point with gradient 0 and Hessian is PSD, right, it doesn't satisfy one, two. That's the pathological case. You have to be close to a real local minimum, right? So that's what this strict-Saddle condition is saying. exactly the thing that we did for the strict-saddle. But if you think about it, it's basically the same statement. Anyway-- so cool. So we are basically done with the first part, so about identifying the subset of functions that are easy to optimize. And next, we are going to show some examples where these kind of properties can be proved rigorously for machine learning situations. But these examples are pretty simple. They are not deep learning. So these are still roughly the best that people can do in some sense. The best rank one approximation is basically the Eigendecomposition or the singular value decomposition of the matrix here. Just for simplicity, let's also assume this matrix M is symmetric. And this becomes a nonconvex objective function because you have a quadratic term here. And our goal is to show that, even though it's non Convex, all local minimum of this g are global minimum under the assumptions that we have mentioned, so like rank one, PSD. So how do we prove this? So as you can imagine, the proof is pretty simple. The plan is very simple. You first find out all stationary point, the first order stationary points. And then you prove that they are all global minimum. So basically, it's just more or less like we solve all of these equations and see what are the possible local minimum you can have, right? So let's firstly use the stationary points, a gradient condition. So gradient of g of x, I'm not going to give a detailed calculation here. This is equal to minus this times x. The Hessian is in dimension d by d because you have d parameters. Sometimes your parameters is a matrix, and the Hessian becomes a fourth order tensor. And it's kind of very complex to be even just written down to just write down the Hessians. So here is a kind of a very useful trick and which actually also has some fundamental reasons that this is useful. So the useful thing is that, if you look at the quadratic form regarding the. Hessian and you transpose Hessian v or v in the part that was Hessian. times v, this is the quadRatic form related to Hessian, and this is much easier to compute. point and is x is not global min then moving in v1 direction-- so moving inv1 direction wouldn't change our function very much. Because you have stationary point, that means your point is flat. And that's why it's not a local minimum. So that's basically the gist of the analysis. OK. so now, let's talk about matrix completion, which is kind of like an upgraded version of PCA. And as I said, this is actually a pretty important question in machine learning. So let me define the question first, and then I can briefly talk about why people care about it. In a matrix, the columns are indexed by the users. Amazon wants to understand what each user's preference is, want to know that each user likes which item, right? So the Amazon has an incentive to just fill in the entire table. So that's why you have to recover all the rest of the entries to serve the users better in the future. And the most used method to solve this is basically nonconvex optimization to find this ground truth matrix M using the fact that you have a low rank structure. it's unlikely it can work. So basically, that is saying that p is bigger than roughly 1 over d. And speaking of the objective functions, this is actually a pretty commonly used method in practice. So you just say I'm going to minimize this function that's called fx, which is defined to be that basically you have a parameterization called xx transpose. And you want to say this matrix actually faced all my observations, right? So you are taking a sum over all possible observed entries because these are the only cases you know what the entries are. convex transition methods and so and so forth. However, those methods often have stronger guarantees. For example, they have tighter sample complexity bounds. In practice, just because the convex transition takes too long time, people actually are using objective functions or methods like this. And that's why it's also practically relevant to analyze these kind of objective functions because they are, indeed, used in practice. All right, so our main goal is to prove that this objective function has no local minimum, all local minimum are global. also have strict-saddle conditions. So you can also prove that. It's just that I didn't include it just for the sake of simplicity. All right, so I guess so the proof is obviously too long to cover in 1 minute. So I guess I'll leave it to the next lecture. I can take some questions if anybody has any questions. Otherwise, I think we are good today. OK, there's a question. So are there any network models where these are known to be hold? The answer is no especially if you look for a global property, like globally, all local minimum are global. assume that the input are linearly separable, then there is a proof for this. And there are a bunch of other cases where you can have some partial results. Next week, in the next lecture, maybe the second half of next lecture,. I'm also going to give another result, which is somewhat more general. It applies to many different architectures, but it has other kind of constraints. First of all, it doesn't really show exactly these kind of landscape properties. It shows that these kinds of properties holds for a region, for a special region in the parameter space.