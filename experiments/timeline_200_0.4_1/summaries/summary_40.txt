Danqi Chen is one of the foremost researchers in question answering. She's particularly well known in recent work for her work on using dense passage retrieval methods for open domain question answering, and is the professor at the Princeton University. Danqi once upon a time was the head TA of CS224N. So she's quite familiar with the context of this class. So I'm very happy to introduce to you some of the fundamentals in this field, as well as on cutting edge and state of the art topics. just not with us using the both directions. In the bottom right, we sum over i, so why does the i remain in bi? Is that correct or is that a typo there? This is another typo. So the output of gi will actually range from the 1 to N, which is the number of the context words. There are lots of questions about this. What is the rationale for the expression of the gi? How does one come up with such an expression? OK, OK, I'm sorry. The attention layer is basically modeling the interactions between the query and context. The query-to-context attention is trying to measure the importance of these context words with respect to some question words. The final output layers are basically just two classifiers just trying to predict the start and end positions. And the whole BiDAF model can be just trained in an online training set, in terms of the log likelihood of the gold answer and the end position of the answer. It's basically taking the product of these two probabilities, but it is the sum of the two negative probabilities. BERT is a deep bidirectional transformer encoder pre-trained on large amounts of text. It is trained on the two training objectives, including masked language modeling and the next sentence prediction. The BERTbase has 110 million parameters and the BERT-large model has 330 million parameters. If you just take this BERT model, and by just optimizing all the parameters together, it can give you a very high performance. And even if you use a stronger pre-training models, they can even lead to better performance on SQuAD. Today, because she doesn't have a Stanford login, we're going to do questions inside Zoom. So if you'd like to ask a question, if you use the raise hand button, we can promote you. And if you hang around and don't leave the Zoom for more than a few minutes, maybe we'll just promote everybody who's still there into people in the regular Zoom for some bits of discussion. There are now four people who've been promoted. OK. Should I read those questions? Should I look at the chat or? No. Most existing question answering datasets or reading comprehension datasets have been collected from Mechanical Turk. So it is very difficult to avoid some kind of artifact though, like a simple clues or superficial clues-- let's say not superficial but some simple clues that are for the machines to pick up. So that's the reason that more specialized models that have been trained very well in one data set, it's very hard to generalize this kind of thing to another data set. So if you do the remnant questions in your train, dev, and test set, there's a debate of just debate of it's inevitable that it's going to overlap. The goal of this project is trying to ingest all the phrases in the Wikipedia. So these conversations are built using the training set of the question answering datasets. By using end to end, we apply this encoder to all the phrase, like 60 billion phrases in this region. So the model is definitely able to generalize from theTraining set to all of the Wikipedia phrases. And then we can use text as the representation, can actually generalize well for the unseen questions. It doesn't have to have seen the phrase then. Is this what you were asking? I see, OK. Next question is about the future of NLP. Do you think that in order to solve NLP in the sense that you can perform on par with humans on all NLP tasks it's sufficient to only interact with text data? Or do you think we'll eventually need the sort of experiences and common sense that. you get only from seeing and viewing the world and having a set of interactions that we as humans have? Yeah. I mean, common sense is a very difficult-- even in the context question answering. So all these things need to be resolved. encountered this paper called Learnable Quantizers, which essentially learns baseless representation for the quantizers jointly with the leads of the network. And while this would be extremely effective if you were to just like, say, train from scratch, I was just sort of curious, do you think there is some way to do this say a pre-trained BERT model or something like that? I had a few ideas with like beam search for instance, but I don't see a very clear way of doing that. Next question is from Danqi, who was one of the co-organizers of the EfficientQA task. Danqi: How concerned should we be about potential biases into these record labels or how we evaluate them, or is that just more of a concern for more open ended questions? Yeah, this is definitely very important. I mean, we will have more discussion of toxicity and bias coming up very soon, including actually Thursday's lecture as well as a later lecture, not specifically about QA though.