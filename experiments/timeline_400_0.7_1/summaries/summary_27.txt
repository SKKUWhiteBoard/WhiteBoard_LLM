James Swan: I hope everybody saw the correction to a typo in homework 1 that was posted on Stellar last night and sent out to you. We have four course staff that review all the problems. We try to look through it for any issues or ambiguities. Today, we'll look at another one, the eigenvalue decomposition. And on Monday, we will look at a different decomposition called the singular value decomposition, which is a different type of transformation than the one we looked at today. "We ran out of time a little bit at the end of lecture on Wednesday," he says. "There were a lot of good questions that came up during class" "One topic that we didn't get to discuss is formal systems for doing reordering in systems of equations" "We saw that reordering is important. In fact, it's essential for solving certain problems via Gaussian elimination" "It's the difference between getting a solution and writing a publication about the research problem you're interested in and not" Permutation matrices are one class, maybe the simplest class, of unitary matrices. They're just doing row or column swaps, right? That's their job. If I swap the rows and then I swap them back, I get back what I had before. So this product here does nothing to the system of equations. It just swaps the unknown. There are a couple other slides that are in your notes from last time that you can look at. We'll see some today. James W. Swan: We discussed sparse matrices and a little bit about reordering and now permutation. He says permutation is a form of preconditioning a system of equations. He shows a simulation that tells us how probable it is to find the Plinko chip in a particular column. Swan: We could construct an alternative model that didn't have that part of the picture that we wanted to have. He's happy to answer questions on.at and he'll be back in a few minutes. Eigenvectors of a matrix are special vectors that are stretched on multiplication by the matrix. So they're transformed. But they're only transformed into a stretched form of whatever they were before. For a real N-by-N matrix, there will be eigenvector and eigenvalues, which are the amount of stretch, which is complex numbers. And finding these values is the subject that we focus on today at the end of the talk. The talk will be on the subject of sparse matrices, which have a sparse structure. eigenvectors and eigenvalues are non-linear because they depend on both the value and the vector, the product of the two, for N plus 1 unknowns. They seem like special sorts of solutions associated with a matrix. And if we understood them, then we can do a transformation. But how do you actually find these things, these eigen values? Well, I've got to solve an equation A times w equals Lambda times w, which can be transformed into A minus Lambda identity times w. Determinant of a matrix like A minus Lambda I is a polynomial in terms of Lambda. The N roots of this characteristic polynomic are called the eigenvalues of the matrix. There are N possible lambdas for which A minus lambda I becomes singular. Here's another matrix. Can you work out the eigevalues of this matrix? Let's take 90 seconds. You can work with your neighbors. I'm going to do it myself. Anyone want to guess what are the eigervalues? Eigenvalues have certain properties that can be inferred from the properties of polynomials. The determinant of a matrix is the product of the eigenvalues. Diagonal systems of equations are easy to solve, and it's easy to find their eigen values. If there is a complex eigenvalue, then necessarily its complex conjugate is also an eigen Value. The trace of aMatrix is the sum of its diagonal elements, and the elements of a triangular matrix are eigen Values, too. a matrix is also the sum of the eigenvalues. These can sometimes come in handy-- not often, but sometimes. For example, a rate matrix can tell us something about how different rate processes evolve in time. The rate matrix has units of rate, or 1 over time. And they tell us the rate at which different transformations between these materials occur. The characteristic polynomial of that is. The eigen values of that matrix are going to tell us. something about the rate processes. Eigenvalues can be interpreted in terms of physical processes. This quadratic solution here has some eigenvalue. It's not unique, right? It's got some constant out in front of it. So add the first row or subtract the second row. And then we'll compare. This will just be a quick test of understanding. Are you guys able to do this? Sort, maybe, maybe an answer, or an answer for the eigen value. Is that too fast? Are you OK? No. James Swan: Can you find the eigenvalues and some linearly independent eigenvectors of a matrix? He says if an eigenvalue is distinct, then it has algebraic multiplicity 1. He says geometric multiplicity is the dimension of the null space of this matrix. Swan: When we don't have a complete set, we're going to have a hang-up associated with a problem in your homework, he says. But he says it's useful for solving systems of equations or for transforming systems. in a lot of cases. You can prove-- I might ask you to show this some time-- that the eigenvectors of a symmetric matrix are orthogonal. They're also useful when analyzing systems of ordinary differential equations. So here, I've got a differential equation, a vector x dot. So the time derivative of x is equal to A times x. And if I substitute my eigendecomposition-- so W lambda W inverse-- and I define a new unknown y instead of x, then I can diagonalize that system of equations. triangular form for this matrix. We'll talk next time about the singular value decomposition, which is another sort of transformation one can do when we don't have these complete sets of eigenvectors. You'll get a chance to practice these things on your next two homework assignments, actually. So it'll come up in a couple of different circumstances. I would really encourage you to try to solve some of these example problems that were in here. Solving by hand can be useful.