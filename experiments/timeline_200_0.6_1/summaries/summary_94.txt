then we are going to continue with the second part of the lecture today which focuses on the problem what actually happens if the gaussian assumption that i have about my constraints doesn't hold you can have multiple reasons why this doesn't holding. As you will see in some small examples having this outliers in your optimization problem is something which hurts dramatically which actually screw up your solution so already a few outliers can lead to a environment model which is completely unusable for doing any navigation task. Where the geometry of what you computed doesn't fit to the real world geometry anymore and one of the questions actually how to handle that. a corner so either my my laser beam hits one of those walls over there or goes out through the glass pane to the next building so i'm either measuring here two meters or measuring 200 meters it would be nice if i could say to the system as either 1 meter or 2 meter or 200 meter i simply don't know take into account a distribution which is not a gaussian with a single mode but why not taking account a multi-modal distribution. If we would have the possibility to integrate aMulti-Modal distribution here that would actually be a nice beneficiary and what i want to talk about here today is um ways for oops okay ways for doing that. they look very very similar it may be very hard for a robot to distinguish that we are here whatever in room 18 and all in room 16. Other things is if you have structures in the environment and there's a lot of clutter in the scene the clutter even if it has a repetitive pattern may lead to a multimodal belief about what the relative transformation between two poses let's say or you walk along whatever a corridor with very little feature every few features you only have say some pillars. can we actually take that into account it's not always easy to get this information out of your gps because typically it runs the kalman filter internally or most of the gps devices do that so they get a gaussian belief is screwed up. If you get the raw measurements you may be able to do better by allowing for multimodal distributions in here. There's one small example so there's a small robot which moved through the 3d board and this is what was similar to this repetitive structure that i was talking before. have also experienced and if you look to those poses over here in the poses down here how those individual structures match um if you just apply let's say scan alignment you may say this may match so maybe someone has opened the door which was closed before or here is a door now closed which was open all the other scans map actually quite well the same holds here. Even as a humanism you say okay there is definitely a misalignment between the skin so they don't fit perfectly but that's something which actually can result from small changes in the environment. over here so this is a single constraint you can already see that you don't have a straight wall over here anymore so it's kind of bended a little bit like this due to the single constraint which obviously has a really really large error so the least square error minimization at the error is squared error term tries actually to minimize that. If we add two three four five i think there were 10 constraints 10 wrong constraints the map actually gets so distorted that is unusable for navigation at least down here here you still may be able to navigate within. will actually end up in dramatic mapping errors so the system is unusable so having good data cessations is really important so already screwing up a small number of places is something which can hurt your optimization if you don't take that into account. How can we deal with the problem that we have places which look identical we have cluttered scenes uh we have may have gps multi-pass problems so the signal gets reflected for example at tall buildings and in this case screwed up the measurement how can we incorporate that into the graph based slam approach? the first attempt to to solve this problem this would be our probability distribution what is the problem with this probability distribution so i say okay some of my constraints are these multiple multimodal constraints i will simply go ahead and implement that. What's the problem that you're going to experience if you make this is some so this is we know how to solve that right this is what you know howto solve that that's what we did so far. If you do it exactly that way you start coding your stuff at some point i say hmm something doesn't work here what's what is that. don't have a single constraint we have a number of constraints right hundred thousand millions of constraints how do we combine those constraints if we minimize the squared error what are we actually doing? If we go to the log like negative log likelihood we're going to optimize here this term here minus a constant and here we can't go further than that that's a problem. There's where it fails do you see what is the dirtiest way for you to fix this let's say you started implementing that you have your implementation is. done you know he said oh damn i can't do that what would be the ugliest trick that you can do in order to make that work even worse no that's not quite what you're going to do i mean the sum is kind of the the the bad thing what can i do with the sum instead of the sum i can i can get rid of thesum in some nice way sorry oh the integral of some to the integral actually makes our life typically worth um so that's that's going to fly. The key trick is to simply say uh i just say where am right now is this actually the approximation error is actually kind of small if the gaussians are quite separated from each other. If one mode would be 2 meters and the other one 2 meter 5 then this may not be a good approximation but what we're going to do is we say okay just select the cave mode which is currently the best one so it's a maximization operation again. The idea of this max mixture approach to actually change the optimization from a sum of gaussian to a max of gaussedians. sum anymore we don't have we only have the mux in here and so then the the log if you go for for the log term i can move the log inside so when you see i just have this constant factor which i had before as well or negative log like i'm minimizing negative log likelihood this is exactly my expression so i stick with exactly the same operation here. The only thing i need to do whenever i compute the error function ineed to pick the mode of the gaussian which gives me the best performance. exactly the same in my code the nice thing is that actually between iterations you can the system can swap between different modes and therefore although the optimization in one iteration takes into account only one mode of the gaussian as you can switch the modes it is you still have the ability to deal with multimodal constraints if i do that that's what the result looks like so this was the original stuff you have seen before one wrong constraint 10 and 100 if i go for max mixtures let's say either it's a perfect fit or it'sa very very very flat gaussian this can be in layer or outlier. kind of in the system swaps to this other one if it is an outlier there's a very high likelihood this will swap to that. If you have a bad initial guess and the bad initial guessing is in line with the outlier then you may run into problems. Those are constraints which simply swap to the other mode and don't harm the optimization much. The solution may be still a little bit different than the one which is done completely without constraints because you still have this very very tiny error but it's actually kind of within the noise. individual constraints so when you evaluate the error the evaluation of the error is somewhat more expensive because every constraint can have can be multimodal or bimodal in this case. There's not no big difference in the operation of those systems when you um if you use the red or the. or the or the blue plot this is exactly the trick that is used um so if you just want to deal with outliers so the the red one is kind of the inlier and the blue function is the one which is the outlier. robot and the ground is muddy the wheels may slip before you get grip and the and the robot starts starts driving if this is the case although you're executing command you're standing and then you start moving so you may get this kind of distribution. In most cases actually the vehicle executes what you tell the vehicle to do but in some cases simply doesn't move. This max mixture idea is actually a pretty easy idea pretty simple idea just reply funny no one has done that in robotics until recently a few years ago. that's actually a nice thing so um another thing is it can handle both things at the same time data station errors as well as multimodal constraints. So the combination of outlier rejection and dealing with wrong data associations is actually kind of nice we also can do this obviously in 3d so this is again this data set with the sphere that we have seen before robot moving in a virtual sphere with constraints. This is gauss newton and this is the max mixture gaussnewton and um so you can see here there's a non-perfect alignment in here because you don't see the regular structure. if he increases to 100 outliers this is just whatever a big mess whereas this one still is able to solve those things quite nicely. The key idea the intuition behind that is if i have a constraint which has a large error so where the um the current configuration is far away from what the constraint tells me just reduce the uh or increase the uncertainty that is associated to that so decrease um the the information matrix so scale down the information Matrix. The question is still how do we get to that point. actually compute these so the main changes we go to this formulation we have the scaling factor over here and we need a good way to compute the scaling Factor so how can we actually do that and there's actually closed form you can derive that under certain properties. So what you do is you compute the original error then you compute this sij and just multiply your information matrix with this this leads to the case that constraints which are far away from what we expect have a smaller influence on the optimization so we can actually visualize this. in this area kind of the the core center of attraction both both perform equally well because there's no scaling involved. The further you move out the more the red curve gets scaled so it gets kind of fatter andfatter and better. If the error increases if you're further away from the mode this is what your error function looks like so the further away the more you down weight the influence of this constraint because you increase the uncertainty of the constraint through the gaussian distribution you can generate a flatter error function. and flatter and flatter gaussian distribution the further the point is actually away there's also one technique which you can find which is also quite easy to implement because you just need to compute the scaling factor and multiply that with your information matrix for every constraint. The tails of this gaussian distributions contain too few probability mass they're too close to zero that's very unlikely that you are really far away from the mode and therefore if you have one outlier which is really far from the current estimate the whole mole is tracked in this direction. now you get different properties in the optimization so if you use um if r is just the quadratic function then we exactly have the original problem that's what we minimized x of minus uh squared error so if we have this one we have we examine exactly in the gaussian world and now there are different techniques how we can actually address that one thing is we could take simply the absolute value so we don't square it just take theabsolute value of the error that's not the parabola that we have but the absolute function. Max mixture is kind of the max mixture for a bimodal distribution for dealing with outliers. The system optimized according to a different cost function but this allows you to take into account for example these these heavier tails so that outliers still are not weighted that dramatically and impact your solution so much because they completely disagree with the solution that you actually have before the solution. For dynamic covariance scaling just as a node this is similar to a robot stem estimator of these families that we have seen it's actually an equivalent. here that by changing this function you can't get much better behaviors kind of deciding which function to use for the underlying optimization problem is not on it's not always an easy and easy choice so this requires some expert knowledge some good intuition on coming up with the way with one of those functions. Next week which is the last week of the term i will briefly talk about front ends and give kind of a short summary on what typical front ends exist obviously we're not going to all the details as we did that here. setup to another sensor setup can be quite tricky on but the back end itself which sits here doesn't really change that much therefore the focus in this course which was much more on the backEnd. At least i would tell you a little bit about what typical front desk exists and how you could realize a front end if you want to build a slam system. Well that's something we are going to do next week that's it from my side thank you very much and hope to see all of you next week.