In this module, I'm going to briefly introduce the idea of differentiable programming. Differentiable programming is closely related to deep learning. I've adopted the former term as an attempt to be more precise in terms of highlighting the mechanics of writing models as you would code. So let's begin with our familiar example, a simple neural network. And this is the programming part of differentable programming which allows you to build up an increasingly more sophisticated model without losing track of what's going on. In a three layer neural network, we start with our feature vector. In this case, it's a six dimensional vector. And we left multiply by a matrix. I've drawn some lines here to help us interpret this matrix as a set of rows where each row corresponds to a hidden unit. And I'm going to take the dot product of each row with the input vector to produce a hidden vector of dimension 4. I'm Going to add a bias term and then I'mGoing to apply an activation function. Now I have a vector and now I can do the same thing again. we're going to see a lot of these box diagrams which are going to represent functions that we can reuse and have a nice interpretation. So the FeedForward function takes in an input vector x and produces an output vector which could be of a different dimensionality. And the way to interpret what people are doing is performing one step of processing. In particular what that processing is, is taking this input vector, multiplying it by a matrix, adding a bias term and applying an activation function. So this is a very compact way of writing something that would otherwise be quite complicated. The FeedForward function that we just introduced, takes a vector as input and we can represent an image as a long vector by, for example, adding all the rows. But then we would have this huge matrix that we would need to be able to transform this vector resulting in a lot of parameters which may make life difficult. To fix this problem, we introduce convolutional neural networks which is a refinement of a fully connected neural network. So here is an example of ConvNet in action. It goes through a number of layers and over time it computes increasingly abstract representations of the image. ConvNets have two basic building blocks. You can take CS231 if you want to learn all about ConvNets. But instead I'm going to focus on the interface and show how these modules compare. And so Conv takes an image and the image is going to be represented as a volume which is a collection of matrices, one for each channel, red, green, blue. Each matrix has the same dimensionality as the image, height by width. And what the Conv is. going to do is it's going to compute another volume of a slightly different size, usually the height and width of this volume is going. to be equal. Conv is going to compute this volume is via a sequence of filters, and intuitively what it's going to do is try to detect local patterns with [AUDIO OUT] So here is one filter and how it works is I'm going to slide this filter across the image. And then for the second filter, I am going to use to fill up the second output channel. The second operation is MaxPool which again takes an input volume and then it produces a smaller output volume. going to slide a little max operation over every 2x2 or 3x3 region. So the max over these four numbers is going to be used to build this [INAUDIBLE] and so on. That's all I'm going to say about MaxPool. If you want to go into the details, you can check out this demo or you can learn more in 231. But again, I want to highlight that there's these two modules. One for detecting patterns and one for aggregating, to kind of reduce the dimensionality. be learned. The second thing is I also haven't specified the hyperparameters which is the number of channels, the filter sizes, and so on, which are actually pretty important for getting a good performance. But I just wanted to highlight the overarching structure and the idea that you can compose in a fairly effortless way. So now let's turn our attention to natural language processing. So here is a motivating example. Suppose we want to build a question answering system. We have a paragraph. It's from Wikipedia and we have a question. We want to select the answer from that passage, from the paragraph. somehow related to product. And also the fact that some words are ambiguous, like product can be-- multiplication or output. So there's a lot of processing that needs to happen and it's hard to kind of specify in advance. So we're going to define an EmbedToken function that takes a word or a token x and maps it into a vector. And all this function is going to do is it's going to look up vector in a dictionary that has a static set of vectors associated with particular tokens. is something that has an interface but not an implementation. A SequenceModel is going to be something that takes a sequence of input vectors and produces a corresponding sequence of output vectors. So in other words, I want to contextualize these vectors using the sequence models. I'm going to talk about two implementations of the Sequence models. One is recurrent neural networks and one is transformers. The SequenceModel can be thought of as reading a sentence left to right. So we have a word which gets mapped into a vector that produces some hidden state. And then we're going to read a second input vector, and I'm Going to update this hidden state along with this hiddenState. A simple RNN works by taking a hidden state, multiply by a matrix, take the input and multiply by the matrix. And then I add these two and I apply an activation function. So at the end of the day, I have the sequence model because that maps input sequence into an output sequence. And I notice that each vector here now depends on not just the input vector but [INAUDIBLE] So if you look at h3, h3 depends on x3, x2, and x1 following this computation map. Collapse takes a sequence of vectors and returns a single vector. There's three common things you can do. If you're doing text classification, you probably want to pick the average to not privilege any individual word. But as we'll see later if you're trying to do language modeling, you want to take the last. The score for, let's say, binary classification is going to be equal to taking the input sequence of tokens. You embed all the tokens into a sequence and now you can apply a sequence model, for example, a sequence RNN. The attention mechanism takes in a collection of input vectors and a query vector and it outputs a single vector. So mathematically what this is doing is you start with the query vector. I'm going to multiply a matrix to reduce its dimensionality, in this case from 6 to 3. And the attention is going to process y by comparing it to each of these x's. OK. So these types of functions where the input and output have the same type signature are really handy because then you can compose them with each other and get multiple steps of computation. Here is one of the input vectors. x1, x2, x3, x4. I'm going to reduce its dimensionality to also 3 dimensions. And now I can take the dot product between these x's and y's. So that's going to give me a four-dimensional vector of dot products intuitively measuring the similarity between the x and the y. So now I have a distribution over the input Vector x1. I can use those probabilities, those weights, when I multiply by x to take away the combination of the columns of x. multifaceted thing. So one thing that the transformer does is it allows us to use multiple attention heads. The transformer uses something called self attention, which means that the query vector is actually going to be the input vectors. So I'm selecting out the input vector and I multiply it by a matrix to reduce the dimensionality. I've done this twice, but in general you can do this any, 4 or 16. So now I concatenate these vectors. I have a four-dimensional vector from this computation. themselves. So if self attention takes a sequence of input vectors and then it's going to output the same sequence of output vectors where the first vector is, I'm going to stick x1 into the query vector for y and compute the attention, and then x2 and x3 and x4. So each of these vectors is comparing a particular input vector with the rest of the input vector and doing some processing. So in contrast with the RNN, you have representations that have to kind of proceed step by step. And the number of steps is the length of a sequence which causes these long chains. be done once the parameters are learned from data. You can think about this as a sequence model that just takes input sequence and contextualizes the input vectors into output vectors. There's two other pieces I need to talk about before I can fully define the transformer. Layer normalization and residual connections. These are really kind of technical devices to make the final neural network easier to train. I'm going to package them up into something called AddNorm and it also has a type signature of a sequences model. processing each xi in context. Now we have enough that we can actually build up to BERT which was this complicated thing that I mentioned at the beginning. So BERT is this large unsupervised pretrained model which came out in 2018 which has really kind of transformed NLP. And the basic building block for generation is, I'm going to call it GenerateToken. And you take a vector x and you generate token y. And this is kind of the reverse of EmbedToken which takes a token and produces a vector. Translated sentence, given the input sentence or a document summarization or semantic parsing. Each of these sequence can be framed as sequence-to-sequence tasks based on, usually these days, basically BERT and Transformers. So we started with-- Now in hindsight, it seems kind of very simple, FeedForward networks. And we looked at images and looked at convolutional neural networks which were built on Conv layers and MaxPool layers and also FeedForward. So the nice thing about packaging this in a module is that now this is used in transformers and different places as well. encourage you to consult the original source if you want kind of the actual, the full gory details. Another thing I haven't talked about is learning any of these models. It's going to be using some variant of stochastic gradient descent, but there's often various tricks that are needed to get it to work. But maybe the final thing I'll leave you with is the idea that all of differentiable programming is built out of modules. Even if you kind of don't understand or I didn't explain the details, I think it's really important to pay attention to the type signature of these functions.