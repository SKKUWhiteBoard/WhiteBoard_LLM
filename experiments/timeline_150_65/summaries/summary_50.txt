The second half of the MIT OpenCourseWare lecture series will focus on multisequence alignment. In the second half, George Church will talk about how to get an empirical substitution matrix from distantly related protein sequences. The second half will be available on Thursday, February 14, at 10:30 a.m. and 11:30 p.m., respectively. For more information about MIT Open CourseWare, visit ocw.mit.edu and follow us on Twitter @MITOpenCourseWare. let's think about it in three dimensions for just a moment here. And when you have a multiple alignment, you can think of it as dynamic programming on this hyperlattice and that the indels for any pairwise combination may not be optimal for the triple. And let's go beyond triple, but to a very simple dinucleotide alignment. And we will say that this is the optimal multiple alignment. You can see here that the multiple examples of AT anchor the A and T as being separate positions, even though normally, if you just did a pairwise alignment with a high gap penalty. You would not have these canceling indels. will be a recursive algorithm where the score of a two-character string is defined in terms of the maximum of various shorter strings. At the very top is the case where we have no insertions-- the simplest case. Now, the number of different cases we have here-- before, it was 3 for a global alignment, which was 2. Now k is three for a three-way comparison. And all possible subsets is 2 to the k minus 1, in this case, so it's 7. and you can just walk through them. Now, as k grows, then both the space complexity-- the amount of lattice points that you have to store somewhere, either in RAM, or disk, or somewhere-- grows by n to the k-th power. Now to compute each of those nodes-- well, I mean, what will be on the order of 2 to thek power, because remember I said that the number of subsets in a lattice is k. So this is the seven cases for a three-way comparison. general is going to be 2 to the k minus 1. And so the time complexity is have to do 2 to k comparisons per node. And the larger k is, the more you can explore. It's like doing a huge mutagenesis experiment and exploring viable mutants. And it's not like we only want to do k equals 2. There are very good reasons for inferring structure or function without experiments, just from sequence. So we use all the power that we developed for the pairwise comparison and we're just generalizing it. want to do multisequence alignments, so how do we deal with this? This is the way we dealing with most non-polynomial calculations, that is to say, in this case, exponential, which is we approximate. Now, you can get something that's very close to the true optimum if how to prune this hyperlattice. Remember, one of the examples I showed was you could take this band. If you know where the band should start and how wide it should be,. you can essentially prune off many of the nodes without really losing any optimality. in the next couple of slides is a tree alignment, as illustrated by ClustalW. By the way, pruning is illustrated by a program called MSA, which is short for multisequence alignment. And we'll show a star alignment. When we get, later on into the transcriptome part of the course, we will talk about the Gibbs algorithm. And I think most of you, if I had given you the luxury of just thinking this through during the break, how you would do the multialignment, this might be the algorithm you would come up with. The best score is S1 with S3, which has a score of 9. And so you can construct a tree. The distance of each from the common ancestor is indicated by the length of these lines. And you get this 4 by 4 matrix. It's going to be symmetric, so you only have to do the diagonal and the off diagonals on one-half of it and.pairwise alignments. And this is-- basically, we're starting to describe the method by which we construct a Tree. The final branching closest to the trunk of the tree or the roots of tree is called the dendrogram. The longer branches indicate greater divergence. And they're in their own cluster. The common ancestor for all the sequences, which would be the common ancestor of the common ancestors of the first two clusters, is represented by this final branching. And here, distance is this horizontal axis. And then, once you have this dendprogram, the next step, or the full steps, are aligning each of the sequences. S2, S4. And you could imagine keeping doing this hierarchical process. If there were additional sequences which are even more distant related, let's say S5. So you can see how you can align not only sequences, but also pseudosequences. So that's one method. This is a different method. And here, the premise is that you've got one sequence which is sufficiently close to all the other sequences that you can use it as an anchor sequence. Whatever indels you put in individually pairwise, for that sequence, can be propagated throughout the entire multisequence alignment. give a score. These scores are the scores that would have come out at the end of that traceback in the pairwise alignments. So this is not a pairwise matrix. This is the results of 5 times 4 over 2 pairwisealignments. Each of these boxes itself is the outcome of a full matrix on S1 versus S2, for example. And you can see from these set of scores that the best score, or the best set of Scores for any sequence, is S1. best score for S1 with each of the others, and have S1 in red in each case, and use that as the anchor. And so then in the multialignment, you take all the indels relative to the red one, and introduce them so that it's the anchor, and so on. So those are two radically different ways. And we'll get to the Gibbs sampling later. The Gibbs sampling, just in a nutshell, is in general when you have a hard problem, where you can't comprehensively go through the entire space, what you do is sample it. by having this storing, this pairwise or multi sequence in a matrix-- so that's actually you've done a trade-off where you've taken up computer memory in order to save time. And then if you're willing to sacrifice a little accuracy or a little comprehensiveness, then you can save even more time or memory. Now we want to use motifs, which is the sort of thing that you get out of local alignments, to find genes. And we're going to use the motifs and the finding genes as a way of introducing a particular motif. or RNA-coding region, you'll have regulatory elements such as promoters and so-called CG islands. The CG islands are basically an abundance of the CG dinucleotide. Of the 16 different dinucleotides, CG happens to be underrepresented in general invertebrate genomes, and over-represented in promoter regions upstream from genes. And the reason is probably that they bind to transcription factors, and the transcription factors protect them from methylation, and thereby protection them from a mutagenic process that would otherwise cause them to become a TG. in the cell, as well as other constraints on the sequence. Promoters and CG islands are sort of degenerate. They're weak sequence signatures. There's a high variety, and they're used in combinations. Conservation requires that you have the right species, that at least some of the species in your multisequence alignment are just the right distance. If you have very rare [? trends, ?] you need to have the cDNAs, if you have them, to look for conserved positions and interspecies conservation. cell type and the rare [INAUDIBLE],, rare messenger RNA within a cell type. So let's talk about the sizes of proteins. If you go to humans, this would go out to 10s of thousands of amino acids long for the largest proteins. But let's focus attention on the smallest proteins. How is it that it precipitously drops off at 100 amino acids? Why are there so few proteins that are short? And there are slightly more short proteins in Mycoplasma? Any guesses why they're so few? Stop codons tend to be made up of As and Ts. The stop codons are TAG, TGA, and TAA. So if you have an AT-rich genome, you're going to tend to have a lot. You tend to run into a stop codon at random quite frequently. And you can see that there's this general trend. You need to have more codons in a row to convince yourself that a genome is CG-rich. So it's usually somewhere in between. convince yourself as the GC content on the horizontal axis goes higher. And basically, the place where you start getting too many false positives is around 100 amino acids. And so that's why the community just decided to cut off there. When we get to proteomics, we'll talk about ways that you can empirically, by mass spectrometry and so forth, find those small proteins. And genetically, of course, you can find them. Let's talk about the most extremely small ones, and ask whether these extremely small open reading frames are interesting. presumably a separate molecule, maybe possibly a degraded version of it. But in some way or another, the 23 sRNA encodes this pentapeptide, which is not just some junk. But this one actually confers erythromycin resistance at low levels in wild type. It is not a mutant kind of peptide. It's the normal pentapepticide. Now, here's three examples that are related to one another. They have this very strange amino acid composition when you do the translation conceptually in the computer. row happen to be-- the next gene down is a histidine biosynthetic gene. And not only that, but about eight histidine genes in a row come after that. And the same thing with phenylalanines. This weird excess of tryptophan is upstream of tryPTophan biosynthesis genes. So what does this all mean? What it means, probably-- and there's actually quite a bit of experiments on this-- is that this is an excellent feedback loop, where you want to do feedback in the most relevant way. you're not, then you'll pause here. That ribosome will hesitate, waiting for the right transfer RNA. And as it hesitates, this RNA changes. It's folding. And a series of events results in-- if it's hesitating, then it wants to make the biosynthetic genes downstream. So the tRNA has to be charged up. So you get this nice, little feedback loop that the hesitation causes a change in RNA, which causes change of transcription. ask, how do we deal with them more rigorously? And the way we deal With these profiles, we're going to take a multisequence alignment. You acknowledge that you don't have a generic substitution matrix for all positions in all proteins or all nucleic acids. Because one position might be, say, an alpha helix. We have one substitution matrix. And another one might be in a coil. So here, this is all about what motifs are all about. Each position has a different set of rules. be A, C, G, or T. These are four different sequences, real start sites, that we've aligned, either manually or by computer. This is dead easy to do the alignment, but the interpretation here is the position upstream of the start codon doesn't matter. So your matrix down below is-- A,. C,. G and T each get a 1, which is a count. We're doing it in terms of counts here because that's just a restatement of the data. perfectly good start codon in, say, 1 sequence in 10 or 1 in 4 in this case. And so you get 3 and a 1. So this is the weight matrix or position-sensitive substitution matrix. This is more precise than a consensus sequence or a single sequence from the sample. But it's not the most precise way of representing this. It's position sensitive, but we've lost the higher order correlations between positions. But let's see how this plays out, this position sensitive. the same motif, ATG. The T and G were invariant in this larger sample, or nearly invariant. And it turns out that-- again, experimentally, you find this little blip of Gs and As, mostly, at minus 9 relative to the A of ATG at 0. And then the base just upstream from the ATG is almost completely random. And so its information content is close to nil, and so it's 0 bits. Now, this is easy enough that you can just do a big search aligning on theATG. verified-- this motif-- so the ATG motif binds to transfer RNA, and the GA-rich motif actually binds to a ribosomal RNA sequence. And so then basically, the messenger RNA is coaxed into the right position, to be in the right location of the ribosomes where the tRNA can bind the initiator. So here's an example where you can do a multisequence alignment. Here are 1,000 of sequences. k equals 1055. And you can find these motifs that have great biological significance. do that is now take this weight matrix, and ask for each-- we're scanning the genome, and we run into the sequence [? AAT ?] AATG. Now you want to know, how good a match is that to this weight Matrix, which was taken from either 4 sequences or 1,000 sequences? And the way you do it is for each position, you ask what was the score in the whole learning set? And now this should be a now independent test set you're trying this out on. for this particular tetranucleotide instance of this motif represented by this weight matrix. And then you can see that the top three sequences, which all have ATG, have the best scores. And the bottom one, GTG, even though it's a valid member of the learning set, it was something which was underrepresented statistically. GTG tended to be less frequently encountered than ATG and so it gets a lower score when you search the genome for it. So if you prioritize these, they would be prioritized in this order by 12, 12,12, 10. So now the final topic, which talks about a very simple and short motif, which is the CG motif. of them is recognition-- for example, is a particular sequence of protein start? In other words, does it have a score which is statistically significant? That's basically what we were doing, very anecdotally, in the previous slides. Or another task is discrimination. We ask questions like, is this protein more like a hemoglobin or like a myoglobin? The first question is about one sequence relative to, say, a weight matrix. The Other one is about two sequences, asking how-- or three sequences-- whether a particular protein is more like one than another. P of s/m, s bar m-- is the probability that you would get that sequence given a model. And as with any good probability, as we mentioned in the first class, they should sum to 1. We can also have the probability of a sequence in your population of sequences irrespective of model. These are probabilities which are not conditional. They do not depend on something else. And here's a very useful theorem, called Bayes' theorem, which is completely general. It doesn't depend on models and sequences. the middle, it means that you have the probability of the model given the sequence. It's called a posterior probability. Now let's see what all this Bayesian stuff is useful for. We're going to be doing-- of the various applications, we had recognition discrimination and database search. We'll have two models, a model that we actually have a hydrolase and the model that We have randomness. We want to report all the sequences where the probability that that sequence, given the model, is better than that sequence given a null model or random amino acids, that that is significant, and it's significant by the delta. between just the null versus the probability of the model in general. So if we look, if we, let's say, do a database search where we have scoring metrics just as the ones we developed earlier in the talk, we'll get one distribution in orange. And if we score for fide hydrolases, we might get this distribution in blue. And we're asking whether the probability. of getting a particular sequence given the model this is a hydrolase is better than. probability of getting that sequence at random, the orange. the dinucleotide level, and so on, and rather than just dump this on you as a mathematical fact, I want to give you some biological rationale for why you can have nonrandomness at every order of a Markov chain. And you might have a bias where C would be rare because the Cs mutate into Us. In organisms that lack a uracil glycosylase, which would then return it back to a C, Cs will change into Us because it's a very common chemical reaction. biases. And I just elaborated on one of them here, which is the triplet bias, documented here that this 10 times lower frequency of ATG than of some of the other arginine codons. So now let's talk about a Markov model. This is not a hidden Markovmodel yet. In just a moment, it will be. It's a MarkOV model because we're asking what is-- the columns that we had kept independent when we were making profiles or weight matrices. We've said that CGs are underrepresented in the genome as a whole, and they're over-represented in promoters. This particular transition of what's the probability of getting a G given a C in the 5-prime position-- this is one of those conditional probabilities. And so this particular arrow going from a C to a G is represented by this probability. And you can see going the other way is a different probability. That would be p of C given G. And these little arrows will refer to itself, is example of a p of an A given an A. There's 16 possible transitions, including four homopolymers, AA, TT, CC, GG, and 12 transitions of the other dinucleotides. We've got CG islands where the CGs have been protected from methylation, and hence, protected from mutations. And then outside are the ocean, which are not protected. And you want to know where the island begins and ends because that helps you know where regulatory factors are. And so when you look at a new sequence, you won't know whether you're in an island or not. are 0s. 0s are a problem, both for the CG dinucleotide in the ocean and for the transitions between oceans and islands. The way you handle it is called pseudocounts. You basically say, what if we just missed finding that thing? We're going to add 1 to it because however big the counts are, you can always add 1. And so you can see. You can actually calculate these conditional probabilities by hand in the privacy of your home, not while the hordes are waiting to get into the room. known islands, again annotated by some person. And you can see those that this A matrix, focusing on those things that were 43 and 0 before, now more realistic numbers are 27% and 8% for an island and an ocean. Now we're going to plug these numbers-- basically, I've cut off the transition tables, which are off to the right. Now let's use them to actually do an HMM. And the sequence we're testing is, is CGCG in an ocean or an island? What's your guess? extreme case. But this is actually using the numbers from the previous slide, which were taken from real oceans and islands. And so there are two possible places it can be, and they're equally probable. It's in an ocean or island, just given the C, 1/8. Now you make a transition where you multiply this times the A matrix, A sub k l, so you're going from state 1 to state 1, from an island to an island. And if you carry this all the way out to all four tetranucleotides, you get a much higher probability of being in the island.