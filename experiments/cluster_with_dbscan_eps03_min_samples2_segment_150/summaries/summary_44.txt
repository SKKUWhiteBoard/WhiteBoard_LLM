need to know what gradient descent is. The bigger question we are trying to address here is that why many optimization algorithm. are designed for convex functions. But why they can still work for nonconvex functions? So why they still work and actually, pretty well in practice forNonconveX functions in deep learning. Of course, this is not a 100% statement because it depends on which function you are talking about, what you have, and so forth. But I'm just saying, for most of the cases in deep. learning, stochastic gradient descent or gradient descent seems to work pretty well. Strict-saddle condition is basically saying that you assume your function doesn't have this kind of somewhat subtle possible candidate of local minimum, right? So every point, whether it's a local minimum or not, can be told from examine only the first order gradient and the second order gradient, second order directives. So if you certify the condition, then you remove those pathological cases which requires high order derivatives. But there are also-- my bad. One, two doesn't tell you exactly whether a point is local minimum per se if you don't have that assumption. that makes it tricky because then the higher order gradients start to matter. And when you look at this-- and once it becomes about the third order derivative or fourth order derivative, things becomes much more complicated. So that's why local minimum is not only always a property of the first and second order derivative. And there's a theorem, which is the following. So the theorem is that verifying if x is a local minimum without any assumption of the local minimum of f is actually NP-hard. is the definition. So we say f is alpha, beta, gamma strict-saddle if, for every x in RD, it satisfies one of the following. So the first one is that, for some of the x, it just satisfies that fx, the true norm of x is larger than alpha. So if you satisfy this, you cannot be a local minimum because your Hessian is not positive semidefinite. And the second thing is that the Lambda min of the Hessian at x is less than minus beta. So these are not stationary points. They are not local minimum. The eigenvalues are distinct even though we don't have to assume this. Then, basically, all the stationary points are of the form that x is equal to plus minus square root lambda i times the eigenvectors. And now, let's look at which of these is a local minimum. So ideally, we just want to say that only vi, the v1 thing, is the local. So 2 means that the first term goes away. So you get just x2 norm square is bigger than v1 transpose in Mvi. methodology also applies here when you talk about the Hessian. You just iteratively expand it, Taylor expand it into something like g of x plus some epsilon times some vector. And then if you have this, then this basically corresponds to v dot g square gxv. So if you apply these kind of techniques, you can get the. Hessian like this. So the quadratic form of the Hessia is equals to something like this, right? So we have to have this. And we know that the Hessians that are larger than 0 is equivalent to that. In many cases, you only care about a few special v's because some of v's are much more informative than the others. So you want to choose some informative v's to evaluate this formula so that you get some important information about what x can be. So it turns out that the v's that are informative here is the top eigenvector. How do you know this? It requires some intuition, it requires some trials and errors, so and so forth. But I guess it also probably makes sense because theTop eigen vector direction is the global minimum, right? with a recommendation system. So suppose you think of we have a matrix. And in one side, the columns are indexed by the users. And every user probably have an opinion about every item, right? Either they like it or not, so and so forth. But it's not like every user buys every item. So every user only buys a very small subset of the item. And that's why you have to recover all the rest of the entries to serve the users better in the future. assume that the input are linearly separable, then there is a proof for this. And there are a bunch of other cases where you can have some partial results. Next week, in the next lecture, maybe the second half of next lecture,. I'm also going to give another result, which is somewhat more general. It applies to many different architectures, but it has other kind of constraints. First of all, it doesn't really show exactly these kind of landscape properties. It shows that these kinds of properties holds for a region, for a special region in the parameter space. We are going to start talking about the optimization perspective in deep learning for two lectures. In most of the cases, people observe that nonconvex functions in machine learning can be optimized pretty well by gradient descent or stochastic gradient descent. We're going to show some very, actually, simple cases where we can prove this. But I guess, as I mentioned, there is some caveat about whether you can even converge to a local minimum. So I'm not going to prove any of the theorems here.