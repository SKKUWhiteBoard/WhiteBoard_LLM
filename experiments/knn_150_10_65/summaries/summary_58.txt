The Fourier transform is the product of a Gaussian and a sine wave, or cosine. Convolution in the time domain looks like multiplication in the frequency domain. We're going to talk about the convolution theorem, noise and filtering Shannon-Nyquist sampling theorem the function into the imaginary part of the Fourier transforms. We'll talk about how to now right? So there's a little Gaussian pulse in time, and when you multiply those together, you get this little pulse of sine. and spectral estimation. And next time, we're going to move on to spectrograms and an important idea of windowing and tapering, time bandwidth product, and some more advanced filtering methods. And we'll end up on the Shannon-Nyquist theorem and zero padding. And there may be, if there's time at the end, I'll talk about a little trick for removing the line noise from signals. And I'll show you how to do this in more detail after we talk about tapering. Fourier transforms have an interesting property about scaling in time and frequency. When you take the Fourier transform of a function, symmetric functions, even functions, are always real. The faster something moves in time, the more stretched out the frequencies are. If you take this pulse and stretch it out for 500 milliseconds, then you can see the same sinc function, but it's just stretched out in the frequency domain. If we take that pulse and make it narrower, then we can make it 4 times narrower. If this is 4 times wider, then this will be 4 times larger. we plot power in log base 10. A difference of an order of magnitude in two peaks corresponds to a unit called a bel, b-e-l. If we plot this on a log plot in decibels, you can see that on a Gaussian, which is e to f squared, that's minus f squared. We're going to spend a lot of time in the next lecture addressing how you solve that problem in a principled way and make a good estimate of the signal. called the time bandwidth product. You can see that as you make the width in time narrower, the bandwidth in frequency gets bigger. Any signal that has discrete components and frequencies is periodic in time. Heisenberg uncertainty principle comes from the wave functions are just-- you can think of wave functions as just functions in time, he says. The Shannon-Nyquist theorem is a very important theorem for acquiring signals in the lab. The Wiener-Khinchin theorem, very cool. that particle can be computed as the Fourier transform of the wave function. So if the particle is more lo-- [AUDIO OUT] in space, then if you compute the Fouriers transform of that wave function, it's more dispersed in momentum. So this concept of time bandwidth product in the physical world is what gives us the Heisenberg uncertainty principle. The sampling rate needs to be greater than twice the bandwidth of the signal. The higher the sampling rate is, the further these spectra are in time. Micheal Fee: Imagine that we have three functions of time, y of t, x of t and X of omega. He says the Fourier transform of y is just the product of g and x. Fee: So shifting the inside of it by a small amount tau isn't going to do anything. He shows you how to derive the transform by reverse the order of integration, which is a function of tau, rather than the other way around. The derivation is kind of cute, and I enjoyed it. Fourier transform of Gaussian noise is just two peaks, with wiggly stuff around them. Power spectrum of filtered signal is just the power spectrum of your original signal times the power Spectrum of the kernel. High pass or low pass? What kind of filter is that called again? High pass. Low pass. All right, so let me play you what those sound like. [LOWER STATIC] It got rid of about the high frequency parts of the with each other? wanted to show you what the autocorrelation function of this looks like, which I think we saw before. So if you look at the distribution of all the samples, it just gives you a distribution that it has the shape of a Gaussian. And the standard deviation of that Gaussian is 1. Now, what if you plot the correlation between the value of value of this function at time t and time t plus 1? Is there any relation? So they're completely uncorrelated with each other. Using these methods, you can pull tiny signals out of noise at a very bad signal to noise ratio, where the signal is really buried in the noise. So it's a very powerful method. And we're going to spend more time talking about how to do that properly. So now let's turn to spectral estimation. How do we estimate the spectrum of a signal? So let's say you have a signal, S of t. And you've got a bunch of short measurements of that signal. What you can do is calculate the power spectrum for each of those signals. Filtering in the frequency domain means multiplying the power spectrum of your signal by a function that's low at high frequencies and big at low frequencies. So convolving our original blue signal with this green Gaussian kernel smooths the signal. It gets rid of high frequencies. Any questions about that? Well, yes-- AUDIENCE: So why is it that like-- you need to like-- of I guess when you filter a signal, either high pass or of that Gaussian? It's just another Gaussian. low pass, by convolving a signal with a kernel. The kernel for a high-pass filter is a delta function that reproduces the function. You can take a signal like this, Fourier transform it, multiply it by a square window to suppress high frequencies. What would be the corresponding temporal kernel that that would correspond to? It would be convulsing your function with a sinc function. So we're going to talk about how to smooth things in frequency with signals with kernels that are optimal for that job. signal has some bandwidth B that in order to sample that signal properly, your sampling rate needs to be greater than twice that bandwidth, 1, 2. There was recently a paper where somebody claimed to be able to get around this limit. And they were mercilessly treated in the responses to that paper. So don't make that mistake. Now, what's really cool is that if the sampling rate is greater than two, something amazing happens. You can perfectly reconstruct the signal. Now that's an amazing claim. see something at the wrong frequency. That's an example of aliasing. OK? OK, so here's anexample. We have a 20 hertz cosine wave. I've sampled it at 100 hertz. So I'm, you know, 5-- so what frequency would I have to sample this in order to reconstruct the cosine? I'd have to samples at least 40 hertz, so those are the blue points. And now, if I do this zero-padding trick, I Fourier transform. I do zero- padding by a factor of 4. That means if I take the Fourier. transform signal and I'm now making that vector 4 times as long by filling in zeros, then I inverse.