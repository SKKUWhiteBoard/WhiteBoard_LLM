well welcome back everybody to uh the last lecture 162. this is kind of a a special lecture um i did get some requests for more information about distributed storage and quantum computing and so i think we're going to do that. i want to make sure that we talk through the chord algorithm since that's a i think relatively simple thing to understand and is very cool and applied pretty much everywhere so if you remember one of the things we talked about last week was basically this cap theorem which was really a conjecture that eric brewer put forth back in the early 2000s. is a good way to understand global storage systems as a result now um at the very end of uh last lecture we were talking about key value stores. Key value stores are very simple in interface excuse me so basically uh you can have an arbitrary key although that's usually a hash over some value. You can have a value associated with it and if you do put a key comma value that goes somewhere into the ether and then when you do get of the key you get back the value that you started with.  node except that in reality what happens is this gets distributed over a whole bunch of nodes and so the question is really many parts to this question one is how do we actually do that distributing another is when some client does a get how does it figure out which node to go through. So far we haven't really talked about how to even make that work okay. So today i want to tell you about the cord uh algorithm which has been turned into storage systems of many sorts including those used by amazon et cetera okay facebook. here's an example of a recursive lookup which is like routing so basically if i say i want to get uh whatever key 14 has got it goes to the master directory and then that directory forwards it on it routes it to the particular node that's got the results and then the the node returns to the directory which returns back to the original client that's recursively routing its way through. An iterative approach is one in which the client basically talks to the directories then they talk to the individual nodes and um we're not routing queries through anywhere every individual client is.  consistent hashing is a way to take your keys and figure out a clean way to distribute them throughout the system without having to know pretty much all of the nodes that are participating. The chord algorithm lets you get by with only knowing essentially a logarithmic number of nodes in the total system and you can still do this well so we're going to associate each one of those storage nodes is going to get a unique id okay and that unique id will be in the hash space. This is basically going to be a mechanism to divide our space up and we'll talk about that in the next slide. So imagine you take their i don't know their ip address and their owner and whatever you concatenate all those things together and you hash them and you get a single 256-bit id out of that now we're going to talk more about secure hashes a little bit later in the lecture. Every node has an id and it's going to be in this ring space this unit dimensional space from 0 to 2 to the m minus 1 where m is big okay and so let me just show you the picture here. said in practice m is really something more like 256 and so uh this ring is really big and um these nodes are much more sparsely distributed around the ring okay questions we only have a very small class today so you guys are likely to get your questions answered anybody okay no questions all right should we move on now is a system that was developed uh with a group of researchers at mit and at berkeley um and you can think of it as a distributed lookup service uh and it's in my view i like to teach about it because it's the simplest and cleanest algorithm for distributed storage that i have seen. product there's cademlia there's a lot of interesting ones several designs here at berkeley and so this problem of how to look up a key value pair got a lots of study in the early 2000s okay and let's look about the way to think about chords lookup mechanism is once again routing so it's going to be we're going to describe this in a recursive fashion to start with and then of course you can do this in an iterative way as well i think the recursive version is a lot easier to thinkabout so every node in the system is going to know who its successor node is. correct so how did we find the first one again that's a great question so the answer is that any clients that talk to the storage server need to know at least one node in the system. The client which i haven't shown separately out here happened to know about node four and node four serves as a gateway into the ring all right did that answer that question now you can see that this doesn't seem very ideal because if i've got a thousand nodes that are storage nodes i may have to take many hops to find out what i want here. Just its successor then we can always find the server we're looking for okay now what does this really mean okay so here's this ring and here's you know key 14 stored on node 15 let's say what it really means is something more like this right so these nodes since we we're doing hashes over their ip addresses and some metadata it means that they could be anywhere in the world and then we're connecting them together based on their hash name so fort talks to eight eight talks to 15 and so on so that for instance key 14 happens to be stored here on the east coast node 4 is up in alaska. means that no particular part in the in the world here might be a hot spot it means unfortunately though that we don't have the most uh local of look up because if we start at node four it'd be nice if we could just go down to 15 and back okay now this is a really good question here about redundancy how do we get redundancy out of this for the moment uh suspend that question for just a second certainly we could put raid servers or what you know raid storage on each of these nodes and that would be great if the disks fail but uh we would like something even more powerful. beachfront property in nevada and then has a plan to basically cause uh california to fall into the ocean and therefore have really expensive properties fortunately superman uh saves the day and it doesn't happen so um okay so if we move um forward with this by the way i'm showing you these clients now to make this a little more clear the clients need to know one gateway into the system in order to talk to the system okay so that's going to be part of the initial lookup. wrong with who's connected to whom and then if it finds a problem it can run notify to help reconnect the ring okay so let's uh these are the kind of things that are a lot easier to see with um animations and so what i want to show you here for instance is here's a new a new node or it's a node that crashes coming back then suppose that what iwant is to join the ring so what do i do well just like we've been talking with clients presumably what i know is i know one of the nodes in the system. about this algorithm is all that the ring is going to do is it's going to figure out who is responsible for storing key 50 as if this was just a regular key value lookup okay. 50 starts by updating its successor 58 so now it's technically connected somewhat to node 58 but you know 44 is also connected to 58 so we now have a kind of a weird partially connected ring okay and let's look through what happened. At that point 44 will change its successor to 50 and then finally 50 will notify 50 about itself at which point 50 knows its predecessor and when all said and done we have the the node 50 has joined. If you lose two nodes in a row then what i've just described to you is no longer going to work so there is a way to completely break the ring such that the stabilized procedure won't reconnect it can anybody think about what the right thing to do there is in that scenario how do we make sure that two failed nodes can't prevent the ring from re reattaching itself? So far we still have this pretty expensive lookup process which is order n now we have figured out how to make this stable so first of all as long as we have a fully connected ring we can always find the storage uh for this for the data. now um and what you should do is you should take a look uh take aLook at that paper because they describe this in more detail but basically what you want is a stabilization procedure that can work even when nodes uh several nodes in a row are failing. What you'll see is that that there's a way to do that as long as you have multiple links and what we're going to do right now for performance is going to make that even harder to destroy the connectivity okay so if you look here the question is sort of how do we make sure that we have better than order n? be 96 what node would store 82 well that would also be 96 what nodes would store 84 that'd be 96 at some point we get to what node will store 80 plus 32 so 112. okay and we're going to keep track of a logarithmic number of these pointers. The power the powerful thing about this is once i've got all these nodes now i can do a really fast routing process to figure out how to find which node is going to store the key i'm interested in. first routing hop is going to say well if i'm at 80 and i want to get somewhere over on the ring i'm going to connect i'mGoing to correct the bit i've got in this case it would be a 1 in the high point. Then i'll turn it to 0 by taking a long hop, then i'll take a less long hop and so on. i end up with a logarithmic number of hops to get me to my destination and you can view that like i'm correcting the bits from my starting point. first of all we're going to have um more than one forward and backward link called the leaf set. In the predecessor reply message node a can send its k minus one successors and so on and so you can see what's going to happen. During these heartbeat process of looking things up you know asking well hey my successor who's your predecessor during that process the stabilized process we'll get back multiple nodes. This is going to help us get a forest of connection connectivity forward and backwards and that's gonna allow us to keep our leaf set as as correct as possible. The finger table look uh lookup process that just keeps renewing these pointers over and over again. As new nodes come in the finger table adjusts as new nodes leave or as old nodes leave. So that we end up with really high probability even if half of the nodes fail so you can find data with the right number of leaf nodes. That's kind of what's proved in that chord paper and that's not that many because it's a logarithmic number okay so uh before i.me okay and so that that's a very good way to start. key 14s stored on node 15. If node 15 weren't there key 14 would be stored on 20. If 15 dies or goes away we don't have the data. So it's fine that the consistent hashing tells us where it should be stored. But we can't store it there because we've lost our data so we got to do something else here okay. We're going to take the forward leaf set or you can do both forward and backwards up up.go on to uh storage fault tolerance for the data does anybody have any questions on this we good okay. depends on the algorithm. We're going to store 14 on the successive nodes that we know about because of the leaf set so we'll store it on 20 and 32 and now what's good about this is if node 15 fails which is the point that's supposed to store it we've already got a copy on node 20 and node 20 can notice oh 15 went down therefore node 20 will start the process of making sure that 35 gets a replica. okay questions and the ring is going to stay connected because of our connectivity. algorithm and so what's good about this is like i said you store the data in the cord ring and it it's very hard to destroy okay why are they called leaf sets that's a good question the reason they're called Leaf sets is because in some sense you can view the uh if you take any given starting node like 58 and you view the set of fingers that’s a tree and so eventually you get to the leaf set and so it's like a tree with leaves so that's where the leaf is coming from. i guess you could call them branches if we don't want to mix metaphors too much and the leaves and none of those quite have what i want so i'll follow one of these branches it's a little shorter and then eventually i'll get to a node that's this one where that node knows because of the leaf set which node is the one i'm looking for which is going to be the one that's just bigger counterclockwise or excuse me clockwise from the id i's looking at okay. So this leaf set not only serves to help us with our replication but it also serves as uh as part of the last couple of hops we can use it to basically find who's supposed to have our data. randomness is helping us to avoid correlated failures where yeah we have a bunch of copies but they're all in the same machine room and the building got struck by lightning so that doesn't happen in a chord algorithm. The downside of course is performance might hurt if you happen to be too far away from a copy um and so i will tell you that there are subsequent versions of cord which uh when you're doing this routing and you have a lot of options here see how we have many places we could go. Think of this like a dns built out of cord and so what the client does is the client doesn't know where the data they're interested in is they ask the cord ring. The cord ring tells them who to talk to and then they can talk directly to them and exchange data over the shortest path possible using tcpip or whatever. So you can now get the best of both worlds and that you have very hard to destroy lookup process and then you can choose to replicate that data on close to the client. what we did with the the tapestry lookup process back for ocean store okay so i did want to point out that what i've just described to you this chord ring is actually used in lots of uh cloud services these days the idea at least. For instance dynamodb and i have a paper for that up on the reading from last time uses the chord rings and you can look down here but it uses them rather than spreading them around the planet. It uses them within their machine rooms as a way to distribute load. have a service guarantee that says we'll get a response within 300 milliseconds uh for say 99.9 percent of the requests okay. This is very in contrast essentially to what we've been talking about a lot of the rest of the term uh which is focusing on mean response time. Instead we want to have guaranteed performance okay and this is again thinking i want you to think back to when we were talking about real time scheduling and what was important there was keeping the predictability of the scheduling time low. it adapts automatically which is pretty good okay so what i wanted to do next uh i'm going to leave that there a little bit i want to talk a littlebit about security and then um talk through a couple of things and then i wants to uh try to get to quantum computing as well so we can i know there was some of you asked some questions about that so i'mgoing to leave this topic unless there's more questions okay so i's going to talk through  things that i'm pretty sure i'm assuming everybody kind of knows but i want  to make sure we all have the same terminology so um you know security is an interesting thing it's basically computing in the presence of an adversary. Security is basically using those mechanisms to prevent misuse of resources so for instance virtual memory is a mechanism that can be used for protection security policy would be making sure that when we use virtual memory we don't let malicious processes or different processes owned by different people use the same memory and have a potential for screwing each other up. "It's a it's a constant arms race uh preventing people from breaking into things you care about by using new techniques and the distinction between protection and security i think is an important one" security policy built with our protection mechanisms okay so i wanted to point out something interesting i don't know if you've ever seen this before but here is a car in the ditch. Back in july of 2015 there's a team of researchers that took complete control of a cheap suv remotely exploited a firmware attack over the sprint cellular network. They basically caused the car to speed up and slow down and and veer off the road and uh totally wirelessly so this is a little scary uh to think about now fortunately no humans were harmed. Firmware in this car was accepted as authentic even though it came from a malicious third party. One of the questions that's important is do you know where your data came from that's a provenance question. Do you know whether it's been ordered or changed or altered in any way that's an integrity question. This is a question of the rise of fake data which is kind of much worse than fake news which is about corrupting the data and making the system behave very badly.  confidentiality is making sure that the data is read only by authorized users. Non-repudiation is a surprisingly important thing that people don't often talk about. cryptography is one of the central points of many of these mechanisms. You just have to use it correctly and this is communication that's in the presence of adversaries. It's been studied for thousands of years there's actually something called the code book which you should look up which talks about you know Thousands of years of cryptography and cryptography. The central goal has always been confidentiality about encoding information so an adversary can't extract it the general premise is that there you know there's a key and if you have the key you can decode things if you don't have thekey it's impossible. Public key cryptography where there's really two associated keys and you encode with one and decode with the other really leads to all sorts of really interesting authentication problems okay so basic cryptography which you've probably heard about is you have a secret key and you take the plain text and you encrypt it with the secret key. single symmetric key encryption work which symmetric because the same secrets used at both sides is to prevent a adversary from holding on to an old message and sending it later is you have to start adding what are called nonces which are things like timestamps and so on so that every time you send this it's unique and if somebody sends an old version you can detect it. The idea of a secure hash function is one where you take data and you run it through a hash function and get a bunch of bits out of it. we can take a plain text something like a contract and we can run it through a hash function where we take that key and an append m and that's called a digest now we can send that across and the data and at the other side we can verify by re uh computing that hmac okay and if they match the one that was sent across versus the one you computed yourself then you can know that the message is not corrupted otherwise it's corrupted. So we can use hashes to prove later that you know after the transmission has happened that the data is authentic okay so hashing is pretty powerful. it wasn't uh you know if the integrity wasn't high you know it was basically didn't match then we could know that that firmware is probably bogus and we shouldn't be using it okay now the downside of everything we've talked about is both sides share the same key and so if you leak the key then you got problems okay and furthermore you have to somehow share the key. So that requires you to go in a dark alley and you know hand the key over and so this seems like only part of the solution. it's private key and that private key is something that i hold uh secret but the public key i broadcast and so this is basically uh this is the the basis for all sorts of modern algorithms okay among other things if i were to encrypt uh the hash over data and then encrypt it with a public key then i can know for a fact that that data has made it through and only could have come from somebody okay so that's part of uh how we actually sign things all right so for instance here's alice and bob let me show you a fun algorithm here bob sends his public key out in to the wild to alice. Now alice can encrypt messages and send to bob and only bob can decrypt him. Alice can send her public key and now bob can send things to her. is a valid public key requires public key infrastructure but that's another story so now i'm i'm going i went through this very quickly how many people have never seen anybody uh never seen this kind of thing before or is this all pretty familiar okay good oh great this is in cs70 great so let's talk about a project that i've been working on so again you could view security as trying to protect things with a firewall or you could View security as it's all about the data and if you can protect the data then you can protection everything okay. A virtual machine that's in modern hardware that allows you to set up a secure channel and do some secure encryption in a way that not even the local operating system can see the data. If we have these secure enclaves stored everywhere and secure encrypted data then perhaps we can do some interesting things okay and we canDo them securely and so um let me see i'm running low on time here i wanted to say something a little bit interesting here about why data breaches which we've heard a lot about in the last four years are so prevalent okay. and now we suddenly have this issue that physical devices that are trusting on this security suddenly start performing things they're not supposed to okay so the real reason we get these data breaches everywhere is because people think that they can put these boundaries up in a way that don't um can't be breached and of course we know that's not true. The problem really is not only are things breached but the integrity and the provenance of that data is not known so what do we do the data centric vision which is one that i've adopted in uh my research group is one in which we think about shipping containers full of data. One person said well why don't we just make things that are all the same size and shape and then all of a sudden we've got ships trains cranes all of the the the infrastructure for handling these things are the same across the planet. Now i can ship something from my house in lafayette to beijing the outskirts of beijing just by calling the right trucks to come pick up a shipping container which gets taken to the port of oakland put on a ship and then it goes across the ocean and it's unloaded and and so on. a data capsule and inside the data capsule is a bunch of transactions that are hashed so remember those hashes we talked about and signed where we uh we use a private key to sign a hash over something and as a result we trust that this really came from the person who said it did because only they could have the private key. As a result of these data capsules this gives us a cryptographically secure way of moving data around to the edge to the cloud and back again in a way that nobody can fake out okay another way to look at this is this is like almost a blockchain in a box okay. handle this standardized metadata and what is the standardized metadata well it's a hash over an owner key and some other metadata about who created this and that forms a unique address that you can route to in our system unlike um not unlike an ip address. The data becomes a first-class entity so your data basically can float pretty much anywhere so you could put a data a data capsule server in your house and all of a sudden your local data capsules could be stored there or if you're doing some communication with somebody else you could get a copy of their data. data capsules and again because it's like a blockchain in a box it's not possible for somebody to fake data that doesn't belong in there okay and so think back to that firmware issue with the car in the ditch okay and the other thing is that metadata we're looking at actually has details about what the network should and should not enforce. So this vision if this uh is ultimately complete is one where you instead of paying for ip service you'd pay for data capsule service and um you'd be able to store your data in a way that was secure okay and could be used anywhere you want and you'd own your data. how we want to be dealing with data all right sorry if that's a lot of information but i wanted to see if there's any questions there before i switch over to some quantum computing all righty give me a second i'll be right back. So first question is how do we know the data is secured so um just like with a blockchain let me just back up to the picture here which i think is a is a good one to be talking about um what we know is the following. bundles of data and if somebody tries to put garbage in there a legitimate person who's trying to look at this can just throw the garbage out because there's no way that that garbage could have been put in there uh in a way that meets the integrity constraints of the data okay. So it's not forgeable um it's uh it maintains its integrity the the transactions can't be swapped or whatever and so it's a unique umly uh high integrity kind of bundle of data. The vision here really is of pretty much everybody using data capsules everywhere okay and if you can get that to happen then you could potentially have a very interesting scenario here. Part of what we're doing is we're working with roboticists and machine learning folks to put their data and their models for grasping and so on inside of data capsules and as a result they can reside securely in the edge in say your robots or whatever in a way that can't be breached okay. This is really targeted at secure edge infrastructure in addition to the cloud so these data capsules can move back and forth. unforgeable all right good so let me say a little bit about using quantum mechanics to compute and since there's only a few of you tonight if you're uh willing to hear me out i can talk for a little longer just to get through a couple of other things on quantum computing. It's basically using weird but kind of useful properties of quantum mechanics two of them quantization and superposition and that quantization really gives us the ability to talk about something like a one or a zero. Superposition is having a bit which is both a zero and a one in certain fraction of uh between the two and that's where things get interesting okay it's like it's fifty percent zero fifty percent one or something in between that's called superposition. Most digital abstractions that you might learn about in 151 or pick your 141 some of those uh various vlsi classes is you're spend a lot of time trying to get rid of the quantum effects. However if you're willing to allow things to not be always a one or a zero that's a different story. always a zero what you can do is you can just start doing quantum computing and that's basically using quantization and superposition to compute. Some interesting results just to tell you uh quickly here is for instance shore's factoring algorithm factors large numbers in polynomial time even though the best known classical ones are sub-exponential in the number of bits. If you could get a shores algorithm running on a quantum computer pretty much all rsa cryptography would be broken because you could factor okay. a time that's a square root in n rather than half of n okay that's pretty interesting right um the other uh so uh 191 is mentioned in the chat. That's a good class to take if you're interested in quantum computing. Another one that's my favorite i think best application of quantum computing is what i like to call material simulation. This was kind of the original uh the original application ofquantum computing that was uh thought of and basically the idea there is if i want to design a brand new element or brand new material to build things out of. Google and ibm are building quantum computers. The goal is to prove that quantum computers could be faster than classical ones. The machines are currently running at four degrees kelvin or something really cold. There are other types of technologies including ion traps that potentially are pretty interesting that there have been some thoughts over the years might be able to run at something closer to room temperature not there yet the current goal of google and ibM is to do something which they call quantum supremacy which is basically to prove quantum supremacy. Quantum supremacy is the ability of a quantum computer to do something better than a classical machine. This is a demonstration of quantization which is a way of looking at the spin of certain particles like protons and electrons and neutrons. These are particles that have this intrinsic spin and so now i got one and zero or up and down okay and a representation called the heisenberg representation looks at this messy physical situation like this which is either a zero or a one in these brackets and that represents spin up and spin down. something people were looking at okay but the temperature here was less than one kelvin which is really cool okay but let's suppose now here's where the quantum computiness gets pretty tricky okay and and uh bear with me just a little bit i know i'm going a tiny bit over here but um if you think of the zero and the one thing okay this is actually a wave function if you take quantum mechanics representing spin up and spin down and what's interesting is the wave function in quantum mechanics is a complex function. here is actually sort of in one state andsort of in another okay and those are those are two options and it turned out that there was there's a set of famous bell uh inequality experiments that were done that showed that reality is actually the second choice so in fact as weird as it is uh that proton is is a combination of zero and one at the beginning. It's only when we look carefully and force it to be one or the other when we actually try to measure it then it gets forced into a state okay. codes believe it or not which can protect this quantum information from being measured by accident by the environment. As a result really we can we can hold these quantum states for a long enough period of time to actually do something interesting with them. Let me show you this simple two-bit state okay this is called an epr pair for einstein pradonsky rosen it was produced by einstein and pedonsky and rosen as a thought experiment and the idea is i've got two bits but i don't have all four options i only have a zero zero or a one one. you do a bunch of computing on it such that the probabilities are kept and you measure okay and the way it looks is that you take uh let's say you put an input with all possible combinations of the input input of the inputs being equal values all possible probabilities it looks like you're doing computation on all possible values at once but then when you measure you pick up exactly one and that's the answer you get okay. If you don't do anything very interesting here this is going to look like you randomly picked some input and computed on it so basically what we're talking about here looks like a random computation. high probability some answer that was hard to find that's what we would like okay and so if you look here um you know if the two n inputs are equally probable there could be two to the n outputs that are equally likely. What we'd like is the probability of the outputs to be piled up high on the answer we want and it turns out that something like fourier transform does the trick okay so if we can do a fourier transforms on some input we can actually get an interesting output. quantum computer can do it polynomial time and let me show you how here's how it is in a nutshell you pick a random x between 0 and n that's easy you say if the gcd the greatest common divisor between x and n is not 1 you just found a factor you win. We find the smallest integer r such that x raised to the r is is congruent to one mod n okay so that uh you know basically is doing modulo multiply that's really hard to do well. With a quantum computer what we can do and unfortunately i guess i don't have time to do this because we're running out of time but i can set up a situation where my input to my algorithm is all the possible k's uh if i take a bunch of values and i compute uh the the value x to that value and i add them all together as a superposition and i do a fourier transform what i'll find is that x to the r congruent to one as i have r go through all its possible values. actually investigated if you were to build uh that factoring algorithm and you could do it as quantum circuits that could run on a quantum computer what would that look like. We actually investigated ways of optimizing that and we could actually look at performance of different options for the shortest factoring algorithms. So we built a cad tool to do that so um i i don't know i think it's a pretty interesting area right now and there's a lot of interest in it all right so um sorry i kept you guys way over but this is the last lecture i figured if anybody was interested. i think it's it's pretty exciting project we got working on it if anybody's interested in that and then we told you a little bit about quantum computing and uh feel free to come ask me or also look at 151 or 191 excuse me um which is an interesting class on quantum computing all right well thank you everybody sorry for going way over today thank you for those of you that stuck around and uh i hope you have a good uh finalizing of project three andThose of you listening in cyberspace later as well you are all great.