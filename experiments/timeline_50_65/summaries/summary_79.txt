This week, we focus on machine translation related topics. In the second half of the week we take a break from learning more about machine translation. We'll be back next week with a look back at week four of the training program. Back to the page you came from. Follow us on Twitter @CNNOpinion and @cnnopinion. Follow CNN Living on Facebook and Twitter. For the latest from CNN Living, follow us on Instagram and @CnnLLive. and more on neural network topics, and talk about final projects, but also some practical tips for building neural network systems. So for today's lecture, this is an important content full lecture. So first of all, I'm going to introduce a new task, machine translation. And it turns out that machine translation can be used to improve the quality of speech. So that's what we're going to focus on in this lecture. And we're also going to talk about how to use neural networks to improve speech. our task is a major use case of a new architectural technique to teach you about deep learning, which is sequence to sequence models. And so we'll spend a lot of time on those. And then there's a crucial way that's been developed to improve sequence tosequence models, which we'll talk about in the next few days. We hope to see you at the end of the month. Back to the page you came from. Back To the pageyou came from, back to the site you were from. is the idea of attention. And so that's what I'll talk about in the final part of the class. I'm just checking everyone's keeping up with what's happening. So first of all, assignment 3 is due today. So hopefully you've all got in your neural dependency parses parsing text well. I'll be back in a few minutes to talk about the next assignment, which is due on Monday. Back to Mail Online home. back to the page you came from.  assignment 4 is out today. You get nine days for it. It's bigger and harder than the previous assignments. So do make sure you get started on it early. And then as I mentioned Thursday I'll turn to final projects. OK. So let's let's get to it. We're going to start with assignment 4, then we'll move on to the next two assignments. We'll get to the final project on Thursday, and then the next assignment is out on Friday. get straight into this with machine translation. So machine translation is the task of translating. So very quickly, I wanted to tell you a little bit about where we were and what we did before we get to neural machine Translation. And so let's do the prehistory of machine translation, which is a bit like a history of the English language. It's a bit of a back story, but it's a lot of fun. And it's all about the language. So let's get straight into it. a sentence x from one language which is called the source language to another language, the target language forming a sentence y. So we start off with a source language sentence x. L'homme, and then we translate it and we get out the translation. Man is born free, but everywhere, but not in one place, but in many places, all over the world, and in many languages. The world is not one country, but many, many countries, and all of them have their own language. In the early 1950s, there started to be work on machine translation. And so it's actually a thing about computer science if you find things that have machine in the name, most of them are old things. He is in chains. OK. So there's our machinetranslation. OK, so there'sOur machine translation, and so on. He's in chains, andso on. So, there's his machine translation,. and so upon. So on. And on. and so forth. this really kind of came about in the US context in the context of the Cold War. So there was this desire to keep tabs on what the Russians were doing. And people had the idea that because some of the earliest computers had been so successful at doing code that they might be able to do the same thing with code. And so that's what they were trying to do in the early days of the computer age. And that's why they were able to get so close to the code. breaking during the Second World War. Then maybe we could set early computers to work during the Cold War to do translation. And hopefully this will play and you'll be able to hear it. Here's a little video clip showing some of the earliest work in machine translation from 1954. It's from 1954, and you can watch it here: http://www.dailymail.co.uk/news/features/2013/01/30/machine-translators-from- 1954.html#storylink=cpy. $500,000 simple calculator translates Russian into English. One of the first non-numerical applications of computers, it was hyped as 'math wizardry' Instead of mathematical wizardry, a sentence in Russian, it could be 'a sentence in English' It could be used to teach people how to read and write. It could also be used as a teaching tool for people in need of English-speaking skills. It has been dubbed 'the most versatile electronic brain known' The computer will be able to do about with a modern commercial computer about one to two million words an hour. And this will be quite an adequate speed to cope with the whole output of the Soviet Union in just a few hours of computer time a week. When do you plan to go in for full scale production? We'll let you know in the next few days if you want to talk to me about the project or if you're just interested in hearing about how the project is going. hope to be able to achieve this speed? I our experiments go well, then perhaps within five years or so. And finally, Mr. McDaniel, does this mean the end of human translators? I say yes for translators of scientific and technical material. As regards to poetry and novels, no, I don't think we'll ever replace the translators. But despite the hype it ran into deep trouble. Yeah. So the experiments did not go well. And so in retrospect, it's not very surprising that the early work did not. work out very well. I mean, this was in the sort of really beginning of the computer age in the 1950s. That it was also the beginning of people starting to understand the science of human languages, the field of linguistics. So really people had not much understanding of either of these two fields at the time. It was a very exciting time for linguistics and for computer science, I think, because it was the first time that people had a real understanding of both. side of what was happening. So what you had was people were trying to write systems on really incredibly primitive computers, right? It's probably the case that now if you have a USB C power brick, that it has more computational capacity inside it, than the computers that they were, says Apple's Steve Wozniacki. "I think we're going to see a lot more of this in the future," he says. "It's going to be a really exciting time" using to translate. And so effectively, what you were getting were very simple rule based systems and word lookup. So it was sort of like, dictionary look up a word and get its translation. But that just didn't work well. Because human languages are much more complex than that. Often words are not words at all, they are words that have different meanings. So that's what we were trying to do. But it just wasn't working very well, so we had to change the way we translated. have many meanings and different senses as we've sort of discussed about a bit. Often there are idioms. You need to understand the grammar to rewrite the sentences. So for all sorts of reasons, it didn't work well. And this idea was largely canned. In particular, there was a famous famous quote that said, 'You can't rewrite a sentence if you don't know how to write it' And that was a bit of a mistake. It's a bit like that. US government report in the mid 1960s, the ALPAC report, which basically concluded this wasn't working. Work then did revive in AI at doing rule based methods of machine translation in the 90s. But when things really became alive was once you got into the mid 90s, and then again in the early 2000s. OK. That's a long way off, but it's a start, and it could be a step in the right direction. But it's not the future. when they were in the period of statistical NLP that we've seen in other places in the course. And then the idea began, can we start with just data about translation i.e. sentences and their translations, and learn a probabilistic model that can predict the translations of fresh sentences? So suppose we're translating French into English. We can say, what's the probability of different English translations? And then we'll choose the most likely translation. It was found felicitous to break it. this down into two components by just reversing this with Bayes' rule. So if instead we had a probability over English sentences p of y, and then a probability of a French sentence given an English sentence, that people were able to make more progress. And it's not immediately obvious, but it could be the reason for the increase in progress in the first place. It could also be that people are just more willing to try new things if they know they're going to get a better result. as to why this should be because this is sort of just a trivial rewrite with Bayes' rule. That allowed the problem to be separated into two parts which proved to be more tractable. So on the left hand side, you effectively had a translation model where you could just say 'this is a translation problem' on the right hand side of the screen. That's how we got to the answer we were looking for in the first place, which was that it was a translation error. give a probability of words or phrases being translated between the two languages without having to bother about the structural word order of the languages. And then on the right hand, you saw precisely what we spent a long time with last week, which is this is just a probabilistic. This is exactly what we've been trying to show all along, that there's a chance of words being translated from one language to the other. It's a very, very small chance, but it's there. And that's what we try to show. language model. So if we have a very good model of what good fluent English sentences sound like, which we can build just from monolingual data, we can then get it to make sure we're producing sentences that sound good. The translation model hopefully puts the right words into the right places. We can then use that model to build a translation model to produce the right sentences in the right place in the English language. The model can then be used to translate these sentences into other languages. them. So how do we learn the translation model since we haven't covered that? So the starting point was to get a large amount of parallel data, which is human translated sentences. At this point, it's mandatory that I show a picture of the Rosetta Stone which is the famous RosettaStone. So the first thing we do is to get the data from a large number of different sources. We then use that data to build a model for the human translation of sentences. The model is then used to learn how to translate human sentences. original piece of parallel data that allowed the decoding of Egyptian hieroglyphs because it had the same piece of text in different languages. In the modern world, there are fortunately for people who build natural language processing systems quite a few places, where parallel data is produced in large quantities. For more on this story, visit CNN.com/soulmatestories and follow us on Twitter at @Soulmatters and @jennifer_cnn. for more on the SoulMatters blog, visit http://www.dailymail.co.uk/news/features/2013/01/29/soulsmatters-blog-soul-matters.html. The European Union produces a huge amount of parallel text across European languages. The Canadian Parliament conveniently produces parallel text between French and English, and even a limited amount in Inuktitut, Canadian Eskimo. And then the Hong Kong parliament produces English and Chinese. The French. Not the French. The British. The Americans. The Australians. The Canadians. The English. The Spanish. The Germans. The Italians. The Belgians. The Bulgarians. And the Italians. And even the Spanish. So there's a fair availability from different sources. And we can use that to build models. So how do we do it though? All we have is these sentences. And it's not quite obvious how to build a probabilistic model out of those. Well, as before, what we want to do is build a model of the world. And that's what we're going to do with the data we have right now. We're just going to use it as a starting point. The alignment variable is going to give a word level or sometimes phrase level correspondence between parts of the source sentence. So in this case, what we're going to do is introduce an extra variable, which is an alignment variable. So a is the alignment variable, and the word level is the word or phrase level. The word level can be anything from a word to a phrase to a word-to-phrase correspondence. The alignment variable can also be a word or a phrase level or a word/phrase level. and the target sentence. So this is an example of an alignment. And so if we could induce this alignment between the two sentences, then we can have probabilities of pieces of how likely a word or a short phrase is translated in a particular way. And in general, alignment is a good way to predict how a word is going to be translated in the future. It can be used to predict the likelihood of a word being translated into a different language or a different word being used in a different way. is working out the correspondence between words that is capturing the grammatical differences between languages. So words will occur in different orders in different languages depending on whether it's a language that puts on the subject before the verb, or the subject after the verb. Or the verb before both, or both, depending on the order of the words. The results will be published in the next few months in the form of a book called The Grammar Book of Words. The book is published by Oxford University Press and is available in paperback. the subject and the object. And the alignments will also capture something about differences about the ways that work languages do things. So what we find is that we get every possibility of how words can align between languages. So you can have words that don't get translated at all. And you can also get words that aren't even translated at the end of the day. That's what we're trying to get at here. We're not trying to predict the future, we're just trying to capture the past. in the other language. So in French, you put a definite article "the" before country names like Japon. So when that gets translated to English, you just get Japan. So there's no translation of the "the". So it just goes away. On the other hand, you can get many to many to be Japon in other languages, such as Spanish, French, German, Italian, Spanish, Portuguese, Italian and many others. So it's not as bad as it sounds. One French word gets translated as several English words. You can get the reverse, where you can have several French words that get translated as one English word. So mis en application is getting translated as implemented. And you can get even more complicated ones. So here we sort of have four English words being translated as two French words. But they don't really break down and translate each other well. I mean, these things don't only happen across languages. They also happen in other countries. happen within the language when you have different ways of saying the same thing. So another way you might have expressed the poor don't have any money is to say the poor are moneyless. And that's much more similar to how the French is being rendered here. And so even even even the word 'poor' can be used in different ways to express different things, he says. He adds: 'Even the word poor can mean different things to different people, depending on the context' English to English, you have the same kind of alignment problem. So probabilistic or statistical machine translation is more commonly known. What we wanted to do is learn these alignments. And there's a bunch of sources of information you could use. If you start with parallel sentences, you can see how often words and phrases co-occur in parallel sentences. You can look at their positions in the sentence. And figure out what are good alignments for English to English. But alignments are a categorical thing. And so you need to use special learning algorithms like The expectation maximization algorithm for learning about latent variables. In the olden days of CS224N before we start doing it all with deep learning, we spent tons of time dealing with latent variable algorithms. But these days, we don't cover that at all. And you're going to have to go to the next page to learn about the other algorithms in the CS 224N program. The next step is to learn how to use these other algorithms to train your own deep-learning systems. "We're not really expecting you to understand the details here. But I did then want to say a bit more about how decoding was done in a statistical machine translation system" "And so what we wanted to say was that we wanted you to go to CS228.off and see CS228 if you want to know more about that. And we're not actually expecting you, but we did want to give you a bit of an insight into how it was done." "We wanted to talk about the way that the data was decoded. And so, that's what we did." The naive thing to do is to say, well, let's just say we had a translation model and a language model. And we want to pick out the most likely why there's the translation of the sentence. And what kind of process could we use to do that? Well, the naive thing is to saying, well,. well, lets just say, 'Let's just pick out what's most likely to happen in the translation' And that's what we did in this case. enumerate every possible y and calculate its probability. But we can't possibly do that because there's a number of translation sentences in the target language. That's exponential in the length of the sentence. So that's way too expensive. So we need to have some way to break it down more. And that's what we're trying to do with this project. We want to be able to do it in a way that doesn't make it too expensive for us to do the translation. And while we had a simple way for language models, we just generated words one at a time and laid out the sentence. And so that seems a reasonable thing to do. But here we need to deal with the fact that things occur in different orders in source languages. So we need a way of dealing with the different order in which things occur. And that's what we're going to try to do in the next section of this article. It's a bit of a challenge, but we'll get there. and in translations. And so we do want to break it into pieces with an independence assumption like the language model. But then we want a way of breaking things apart and exploring it in what's called a decoding process. So this is the way it was done. So we want to explore it in a different way, and that's what the decoding process is about. So that's the way we did it. And that's how we got to where we are now. We're working on a new version of the book. start with a source sentence. So this is a German sentence. And as is standard in German. You're getting this second position verb. So that's probably not in the right position for where the English translation is going to be. So we might need to rearrange the words. So what do you think? Share your thoughts in the comments below or tweet us @CNNOpinion. We'll feature the best in our weekly Newsquiz. Back to Mail Online home. back to the page you came from. we have is based on the translation model. We have words or phrases that are reasonably likely translations of each German word, or sometimes a German phrase. And so then inside that, making use of this data, we're going to generate the translation piece by piece kind of like we did with our neural language models. So we going to start with an empty translation. And then we's going to say, well, we want to use one of these LEGO pieces. And these are effectively the LEGO pieces out of which we'regoing to want to create the translation. so we could explore different possible ones. So there's a search process. But one of the possible pieces is we could translate "er" with "he", or we could start the sentence with "are" translating the second word. So we could Explore various likely possibilities. And if we're guided by our own language, we could find a way to translate the word 'er' with 'he' or 'he-er' or even 'er-he' and 'er'-er. language model, it's probably much more likely to start the sentence with he than it is to start it with "are" though " are" is not impossible. OK. And then the other thing we're doing with these little blotches of black up at the top, we're sort of recording which which is the most likely to be the first word in a sentence. OK, so that's what we're going to do with the first few sentences. We're probably going to start them with "he" and then "are". OK. German words we've translated. And so we explore forward in the translation process. And we could decide that we could translate next the second word goes, or we could translating the negation here, and translate that as does not. When we explore various continuations. And in the process, I'll go and go and talk to you about what I've learned. I hope you'll join me for the next few days. I'll be in New York City on Thursday night and Friday. through in more detail later when we do the neural equivalent. We sort of do this search where we explore likely translations and prune. And eventually, we've translated the whole of the input sentence. And I've worked out a fairly likely translation. He does not go home. And that's what we'll do in the next stage of the project. We'll try to do the same thing in the second stage and see if we can get the same result in the third stage. we use as the translation. OK. So in the period from about 1997 to around 2013, statistical machine translation was a huge research field. The best systems were extremely complex. And they had hundreds of details that I certainly haven't mentioned here. The systems have lots of separately designed and separately designed. We use lots of separate designed and individually designed translation systems. We used lots of different types of software to do it. We also used a number of different languages to do the translation, such as English, French, Spanish, German and Italian. built components. So I mentioned language model and the translation model. But they had lots of other components for reordering models, and inflection models. There was lots of feature engineering. Typically, the models also made use of lots of extra resources. And they were lots of human components, as well as lots of machine components. It was a huge project, and a lot of work went into it, but it was all worth it in the end. It's a very exciting time for us. effort to maintain. But nevertheless, they were already fairly successful. So Google Translate launched in the mid 2000s. And people thought wow, this is amazing. You could start to get sort of semi-decent automatic translations for different web pages. But that was chugging along well enough. And then we got Google Maps, which is a very different beast. It's a much more complex system. It takes a lot of work to get it up and running. But it's a very, very powerful tool. to 2014. And really with enormous suddenness, people then worked out ways of doing machine translation using a large neural network. And these large neural networks proved to be just extremely successful, and largely blew away everything that preceded it. So for the next big part of the lecture, what we'll be talking about is how to use these neural networks in the real world. We'll be looking at how they could be used in the future to help people around the world in a variety of ways. I'd like to tell you something about neural machine translation. In practice, it's meant slightly more than that. It has meant that we're going to build one very large neural network, which is called a 'neural network' It means you're using a neural network to do machine translation, but in practice it's used to do more than one thing at a time. It can be used for a variety of different things, including speech recognition, for example. It's a very powerful tool. completely does translation end to end. So we're going to have a large neural network, we're Going to feed in the source sentence into the input. And what's going to come out of the output of the neural network is the translation of the sentence. We'reGoing to train that. That's how we'll be able to do it in the future. We'll be doing it in a number of different ways, from speech recognition to speech recognition. We will be doing that in a variety of ways in the coming years. model end to end on parallel sentences. And it's the entire system rather than being lots of separate components as in an old fashioned machine translation system. So these neural network architectures are called sequence to sequence models or commonly abbreviated seq2seq. And we'll see that in a bit. We'll talk about the neural network architecture in more detail in the next few minutes and then go on to the next stage of the project, which is a new type of neural network. Back to Mail Online home. back to the page you came from. they involve two neural networks. There's one neural network that is going to encode the source center. So if we have a source sentence here, we are going to use two RNNs to encode it. The version I'm presenting now has two Rnns. But more generally, they involve two Neural Network Network (Neural Network) Networks that are used to encode source sentences into source sentences. These are called source sentences and source sentences are written using source sentences as source words. encode that sentence. And what we know about a way that we can do that. So using the kind of LSTMs that we saw last class, we can start at the beginning and go through a sentence and update the hidden state each time. And that will give us a result that is the same as the previous version of the code. And we can then use that result to get the full meaning of a sentence. We can then go through the sentence again and get the same result. representation of the content of the source sentence. So that's the first sequence model, which encodes the source sentences. And we're going to feed it in directly as the initial hidden state for the decoder, or RNN. So then on the other side of the picture, we have our decoder RNN, and it's a language model that's going to generate a target sentence conditioned on the final hidden state of the encoder Rnn. And now this second green RNN has completely separate parameters. But we do the same kind of LSTM computations and generate a first word. "he." And so then doing LSTM generation just like last class, we copy that down as the next input. We run the next step of the LSTm, generate another word here, copy it down, and chug along. And we've translated the sentence, right? So this is showing the test time, right?" "He." "Yes." "He" "Yes" "No" "I don't think so." "No, I don't believe so." behavior when we're generating the next sentence. For the training time behavior, when we have parallel sentences, we're still using the same kind of sequence to sequence model. But we're doing it with the decoder part just like training a language model, where we're wanting to do teacher forcing and teacher forcing. We're still doing the same thing with the sequence part of the model, but we're using a different kind of decoder for the training part of it. It's called a sequence-to-sequence model. predict each word that's actually found in the source language sentence. Sequence to sequence models have been an incredibly powerful, widely used work force in neural networks for NLP. Although historically, machine translation was the first big use of them, and it's sort of the canonical use, they're used now as part of NLP research and development in the U.S. and around the world. For more information on how to use Sequence to Sequence models in NLP, visit the NLP Institute's website. everywhere else as well. So you can do many other NLP tasks for them. You can think of text summarization as translating a long text into a short text. But you can use them for other things that are in no way a translation whatsoever. They can be used to do summarization, for example, or for other types of data analysis, such as data analysis of a database or a database of data. They are very versatile and can do a lot of different types of work. So they're commonly used for neural dialogue systems. So the encoder will encode the previous two utterances, say. And then you will use the decoder to generate a next utterance. Some other uses are even freakier but have proven to be quite successful. So if you have any way of using them, let us know in the comments below. We would love to hear from you about your use of them. Back to Mail Online home. Back To the page you came from. representing the parse of a sentence as a string. If you sort of think a little it's fairly obvious how you can turn the parse into a string by just making use of extra syntax like parentheses, or putting in explicit words that are saying left. You can turn a string into a sentence by using extra syntax such as parentheses or putting out explicit words such as "left" or "left". You can also turn a sentence into a word by using the word "word" instead of "sentence" for example. arc, right arc, shifts like the transition system that you used for assignment 3. Feed the input sentence to the encoder and let it output the transition sequence of our dependency parser. And somewhat surprisingly that actually works well as another dependency parsers. Well, then we could say let's use the encoders to feed the input sentences to. The encoder will then output a sequence of events that the dependency parser can read. And that's what we use to build our dependency Parser. way to build a dependency parser or other kinds of parser. These models have also been applied not just to natural languages, but to other kind of languages, including music, and also programming language code. So you can train a seq2seq system, where it reads in pseudocode in natural language, to be able to read code in a natural language system. For example, you could train a system to read pseudocodes in natural languages using a system that reads in pseudo-code in the natural language. and it generates out Python code. And if you have a good enough one, it can do the assignment for you. So this central new idea here with our sequence to sequence models is we have an example of conditional language models. So previously, the main thing we were doing was doing the same thing over and over again. Now we have a way to do it in a different way. We have a new way of doing it. We can do it with a series of models. And we can do this with a sequence of models as well. was just to start at the beginning of the sentence and generate a sentence based on nothing. But here we have something that is going to determine or partially determine that isGoing to condition what we should produce. So we have a source sentence. And that's going to strongly influence what we're going to produce. We have to make a decision about what we want to do with the source. We can't just start with nothing and go from there. We need to decide what the source sentence is and what it means. determine what is a good translation. And so to achieve that, what we're going to do is have some way of transferring information about the source sentence from the encoder to trigger what the decoder should do. And the two standard ways of doing that are you either feed in or feed in information from the source. But we want to be able to transfer that information from source to decoder in a way that makes sense to both of us, so that we can understand it. a hidden state as the initial hidden state to the decoder, or sometimes you will feed something in as theInitial input to the Decoder. And so in neural machine translation we are directly calculating this conditional model probability of target language sentence given source language sentence. AndSo at the start of this article, we were able to show that we can use a conditional model to predict the probability of a given sentence. We are happy to clarify that this is not the case in all cases, and that the conditional model is not used in every case. For more information, visit the project's website. each step, as we break down the word by word generation, that we're conditioning not only on previous words of the target language, but also each time on our source language sentence x. Because of this, we actually know a ton more about what our sentence that we generate should look like. We're conditioning on the previous words and the source language sentences. We know a lot more about how our sentence should look in the first place, and how it should be written. We also know more about our target language than we did before. The perplexities of conditional language models are similar to the numbers I showed last time. They usually have almost freakily low perplexities, that you will have models with perplexities that are something like 4 or even less, sometimes 2.5. So if you look at the perplexity of these kind of language models, you will find them like the numbers you saw last time, with numbers like 4, 2, or 1.5, for example. For more information, visit http://www.be.co.uk/language/conditional-language-models. because you get a lot of information about what words you should be generating. OK. So then we have the same questions as we had for language models in general. How to train a neural machine translation system and then how to use it at runtime? So let's go through that. So how do we use it? How do we get it to work? How can we get the most out of it? What do we do with the data that we get? What are the implications for us? both of those in a bit more detail. So the first step is we get a large parallel corpus. So we run off to the European Union, for example. And we grab a lot of parallel English French data from the European parliament proceedings. So then once we have our parallel corpus, we can start looking at the data in more detail and try to understand it. And that's what we've done with this project. We've been looking at it for a year and a half now. parallel sentences, what we're going to do is take batches of source sentences and target sentences. We'll encode the source sentence with our encoder LSTM. We will feed its final hidden state into a target L STM. And this one, we are now then going to train word by word by comparing the source and the target sentences, word-by-word. We are now going to be able to train the target sentence word- by-word by comparing it to the source. what it predicts is the most likely word to be produced, versus what the actual first word, and then the actual second word is. And to the extent that we get it wrong, we're going to suffer some loss. So this is going to be the negative log probability of the word that we're looking for, and we're not going to get it right all the time. We're not looking for the perfect word, we just want to know what it's going to look like. generating the correct next word "he" and so on along the sentence. And so in the same way that we saw last time for language models, we can work out our overall loss for the sentence doing this teacher forcing style. We generate one word at a time, calculate a loss, and then work out the overall loss of the whole sentence. We can do this in a similar way to the language model example we used last time. We start with the first word and work our way up. relative to the word that you should have produced. And so that loss then gives us information that we can backpropagate through the entire network. And the crucial thing about these sequence to sequence models that has made them extremely successful in practice is that the entire thing is optimized, he says. "It's a very, very complex system," he says, "but it's very simple to understand. It's very easy to understand and it's extremely easy to use. It just takes a lot of work to get it to work." as a single system end to end. So starting with our final loss, we backpropagate it right through the system. So we not only update all the parameters of the decoder model, but we also update all of theparameters of the encoder model. In turn, this will influence what in turn will influence how the system is used in the future. It's a very complex system, but it's all done in a single go-around. We're not going to reveal all the details at this point. We'll let you know when we do. conditioning gets passed over from the encoder to the decoder. So this moment is a good moment for me to return to the three slides that I skipped. I'm running out of time at the end of last time, which is to mention multilayer RNNs. So the Rnns that we've been talking about are the ones that we're going to use in this talk. We'll be talking about them in the second half of the talk, when we'll talk about how they're used in the cloud. looked at so far are already deep on one dimension then unroll horizontally over many time steps. But they've been shallow in that there's just been a single layer of recurrent structure about our sentences. We can also make them deep in the other dimension by applying multiple RNNs on the same sentence. We've already shown that this can be done by using multiple layers of recurrent structures on a single sentence. But we've also shown that we can do it with multiple layers on different parts of a sentence. top of each other. And this gives us some multilayer RNN. Often also called a step Rnn. And having a multilayers RNN allows us the network to compute more complex representations. So simply put the lower RNNs tend to compute lower level features, and the higher Rnns should compute higher level features. And just like in other neural networks, whether it's feed forward networks, or the kind of networks you see in vision systems, you get much greater power and success by having a stack on multiple layers of recurrent neural networks. are two things I could do. And it shouldn't make any difference because I've got the same number of parameters roughly. I could have a single LSTM with a hidden state of dimension 2000, or I couldhave four layers of LSTMs with ahidden state of 500 each. But it wouldn't make a difference because they're all the same size. It's just a matter of how you want to use them. And I'm not going to tell you how to do it. that's not true. In practice, it does make a big difference. And multilayer or stacked RNNs are more powerful. Can I ask you, there's a good student question here? What would lower level versus higher level features mean in this context? Sure. So I mean, in some sense, these are lower level features in a sense, but in practice, they're more powerful than lower level ones. Yeah. That's not a bad thing. It's just a different way of looking at it. are somewhat flimsy terms. The meaning isn't precise. But typically, what that's meaning is that lower level features and knowing sort of more basic things about words and phrases. So that commonly might be things like what part of speech is this word, or are these words the name of the word? It's a bit of a loose term, but it can be used to refer to a lot of things. It can also be a way of referring to a person's basic knowledge of a language. a person, or the name of a company? Whereas higher level features refer to things that are at a higher semantic level. So knowing more about the overall structure of a sentence, knowing something about what it means, whether a phrase has positive or negative connotations. What its semantics are. What it means to be a person or a company, or a person. A company. A person, a company or a name. A word. A phrase. A noun. A verb. A name. when you put together several words into an idiomatic phrase, roughly the higher level kinds of things. OK. Jump ahead. So when we build one of these end to end neural machine translation systems, if we want them to work well, single layer LSTM encoder-decoder in neurals machine translation. If we want to make them work well we need to have a good single layer encoder and decoder. We need to be able to do that in a single layer. systems just don't work well. But you can build something that is no more complex than the model that I've just explained now. That does work pretty well by making it a multi-layer stacked LSTM neural machine translation system. So therefore, the picture looks like this. So we've got this. And we've also got this, which is a more complex version of the same model. And this is a version of that model that is a bit more complex. And that's the one that we've been using in the video. multilayer LSTM that's going through the source sentence. And so now, at each point in time, we calculate a new hidden representation that rather than stopping there, we sort of feed it as the input into another layer. And the output of it, we feed into a third layer of L STM. So our representation from our encoder is then this stack of three hidden layers, whoops. And then that we use to then feed in as the initial, as theinitial hidden layer into then sort of generating translations. A 2017 paper by Denny Britz and others, that what they found was that for the encoder RNN, it worked best if it had two to four layers. And four layers was best for the decoder Rnn. And the details here like for a lot of neural nets depend on how many layers are in the RNN. The research was published in the open-access journal, Theoretical and Computational Numerical Theory. It was published by the Proceedings of the National Academy of Sciences, New York. so much on what you're doing, and how much data you have, and things like that. But as rules of thumb to have in your head, it's almost invariably the case that having a two layer LSTM works a lot better than having a one layer L STM. After that, things are a little more complicated, but it's still a good rule of thumb for people to use in the future. If you have a lot of data to work with, you might want to use a two-layered LASTM instead of just one. become much less clear. It's not so infrequent that if you try three layers, it's a fraction better than two. But not really. If you try four layers,. it's actually getting worse again. It depends on how much data, et cetera, you have. At any rate, it is normally very very clear. But it's normally very difficult to get to the bottom of it at the start of the process. It can take a long time to get there. hard with the model architecture that I just showed back here to get better results with more than four layers of LSTM. You have to be adding extra skip connections of the kind that I talked about at the beginning of this article. Normally to do deeper LSTm models and get even better results. you have to add extra skip connections of the sort that I talk about. at the end of the article. To see the rest of the video, click here: http://www.cnn.com/2013/01/29/technology/lstm-models-and-the-world/index.html. very end of the last class. Next week, John is going to talk about transformer based networks. In contrast, for fairly fundamental reasons. They're typically much deeper. But we'll leave discussing them until we get on further. So that was how we train the model. So let's just go a step further and talk about how we trained the model and how it works. We'll be back next week to talk more about our next project. Back to the page you came from. bit more through what the possibilities are for decoding and explore a more complex form of decoding than we've looked at. But the simplest way to decode is the one that we presented so far. So that we have our LSTM, we start, generate a hidden state. It has a secret state, and we can then decode it. We can decode the secret state to get the state of the hidden state we are looking for. This is called a 'hidden state' and it can be used to decode hidden states. probability distribution over words. You choose the most probable one the argmax. And you say "he", and you copy it down and you repeat over. So doing this is referred to as greedy decoding. Taking the most likely word on each step. And it's sort of the obvious thing to do. It's the most common way to learn a new word. But it's not always the most obvious. It can take a long time to learn new words. So it's a bit of a process. to do, and doesn't seem like it could be a bad thing to do. But it turns out that it actually can be a fairly problematic thing toDo. And the idea of that is that with greedy decoding, you're taking locally what seems the best choice. And then you're doing it again and again, and again. And again, until you get it right, and then it's not as good as you thought it was going to be. And that's a problem. stuck with it. And you have no way to undo decisions. So if these examples have been using this sentence about, he hit me with a pie going from translating from French to English. So, if you start off, then you say, OK, "il" the first word in the translation, you're in trouble. And if you say "il," then you're stuck with it, and you're not going to be able to get out of it. You're going to have to keep going. should be he. That looks good. But then you-- and then you say, well, hit, I'll generate hit. Then somehow the model thinks that the most likely next word after hit is "a". And there are lots of reasons it could think so. Because after hit most commonly, there's a "a" There's a lot of reasons why a model might think that's the next word. It could be wrong, but it could also be right, too. direct object now and then he hit a car, he hits a roadblock, right? So that sounds pretty likely. But once you've generated it, there's no way to go backwards. And so you just have to keep on going from there and you may not be able to generate the same thing again. So that may be the end of the story, but it's not the end. It's the beginning of a new chapter in the history of the human race, and it's going to be a long one. translation you want. At best you can generate, he hit a pie, or something. So we'd like to be able to explore a bit more in generating our translations. And well, what could we do? Well, I sort of mentioned this before looking at the statistical empty models. Overall, what overall, what do you think? Share your thoughts in the comments below or post a video on our Facebook page, or tweet us at @CNNOpinion and @CNNLive. we'd like to do is find translations that maximize the probability of y given x. We can do that as a product of generating a word at a time. We also have to have a probability distribution over how long the translation length would be. So we could say this is the model. And let's generate and score all possible sequences y using this model. That's where that then requires generating an exponential number of translations. And it's far, far,far,far away from a full model. Far too expensive. So beyond greedy decoding, the most important method that is used. And you'll see lots of places is something called beam search decoding. And so this isn't what neural, well, any kind of machine translation has one place where it's commonly used. That this isn’t a method that’s commonly used in machine translation. It's a very, very different method that's used in other languages. And it's very different to what neural is used for. specific to machine translation. You find lots of other places, including all other kinds of sequence to sequence models. It's not the only other decoding method. Once when we got on to the language generation class, we'll see a couple more. But this is sort of the next one that we're going to look at. We'll talk about it in a little bit more detail in the next few days, but this is the first of a series of posts on the new machine translation technology. you should know about. So beam search's idea is that you're going to keep some hypotheses to make it more likely that you'll find a good generation while keeping the search tractable. So what we do is choose a beam size. And for neural MT, the beam size is normally around 1,000 times the size of a human brain. So that's about 1,100 times the human brain's size. That's a lot of brain power. So, that's what we're doing. We're doing beam search. fairly small, something like 5 to 10. And at each step of the decoder, we're going to keep track of the k most probable partial translation. So initial sub- sequences of what we're generating, which we call hypotheses. So a hypothesis, which is then sort of the prefix of a theory. And then a theory, which then becomes a hypothesis again. And so on and so on, until we get to the right place in the code that we think is the most likely. translation has a score which is this log probability up to what's been generated so far. So we can generate that in the typical way using our conditional language model. So as written all of the scores are negative. And so the least negative one, i.e., the highest probability one, is the one that is most likely to be true. So that's what we're trying to do here. We're looking for a language that can be used in a variety of ways to express a message. is the best one. So what we want to do is search for high probability hypotheses. So this is a heuristic method. It's not guaranteed to find the highest probability decoding. But at least, it gives you more of a shot than simply doing greedy decoding. So let's go through the steps to find out what we're looking for. We'll show you how to do it in the next episode of The Daily Discussion, on Monday, November 14 at 10am ET. Back to the page you came from. an example to see how it works. So in this case, so I can fit it on a slide. The size of our beam is just 2. Though normally, it would actually be a bit bigger than that. And the blue numbers are the scores of the prefixes. So these are the numbers that are used to calculate the beam size and the number of beams that can be used to make a slide, for example. For more information on how this works, or to try it yourself, go to www.cnn.com. are these log probabilities of a prefix. So we start off with our start symbol. And we're going to say, OK. What are the two most likely words, to generate first according to our language model? And so maybe the first two mostlikely words are he and I. And so we start with the start symbol, and we say, "What is the most likely word to generate?" And we say "He and I" and we generate the word "he and I." there are the log probabilities. Then what we do next is for each of these k hypotheses, we find what are likely words to follow them? So we might generate he hit, he struck, and then we find the most likely word to follow each of those. For example, we might say he struck and then follow that by saying he hit the ball. And so on and so on until we get to the end of the story, which is the story of a baseball player hitting a ball. I was, I got. OK. So at this point, it sort of looks like we're heading down what will turn into an exponential size tree structure again. But what we do now is we work out the scores of each of these partial hypotheses. So we have four partial hypotheses, and we're going to try to figure out what the most likely one is. We'll see if we can figure it out in the next few hours or so. I'll let you know how it turns out. He hit, he struck. I was, I got. And we can do that by taking the previous score that we have the partial hypothesis and adding on the log probability of generating the next word here, he, hit. So this gives the scores for each hypothesis. And then we can compare them to each other to see how close they are to being the same person. And that's what we do in the game of "He Hit, He struck" The game is on ABC at 8 p.m. ET. say, which of those two partial hypotheses? Because our beam size, k equals 2, have the highest score? And so they are, I was, and he hit. So we keep those two and ignore the rest. And so then for those two, we are going to generate k hypotheses for the other two. And we are then going to create a hypothesis for each of the other hypotheses, and so on and so forth. And that's what we're going to do. the most likely following word. And again, now, we want to find the k most likely hypotheses out of this full set. And so that's going to be he struck me and I was, I don't know, he. He hit a, he hit me,. I was hit, I was struck. He struck me. I. was hit. I was. struck. And he hit a. He. hit me, he was hit by me. He was struck by me, and he was hitting me. struck me. And he hit a. So we keep just those ones. And then for each of those, we generate the k most likely next words tart, pie, with, on. Then again, we filter back down to size k by saying, OK, the two most likely things here are tart and pie. And so we generate a size k of tart, pies and on, which is the size of the word tart. That's how we get the word 'tart' "He struck me with a pie. And we could continue working on those, generate things, find the two most likely," he says. "And at this point, we would generate end of string. And say, OK, we've got a complete hypothesis. and we could go from there" he adds. "We could do that with.pie or with.end of string," he adds, "or we could do it with. end of a string or with the pie" then trace back through the tree to obtain the full hypothesis for this sentence. So that's most of the algorithm. There's one more detail, which is the stopping criterion. So in greedy decoding, we usually decode until the model produces an end token. And when it produces the end token, that's when the algorithm is called greedy decoding. And that's what we do in this case. We decode until we get the end of the model. And then we trace back to the beginning of the tree. we say we are done. In beam search decoding, different hypotheses may produce end tokens on different time steps. And so we don't want to stop as soon as one path through the search tree has generated end. Because it could turn out there's a different path throughthe search tree. We want to make sure we're not doing the same thing over and over again. We don't know if we've found the answer yet. We need to keep searching. We're not done yet. tree, which will still prove to be better. So what we do is sort of put it aside as a complete hypothesis and continue exploring other hypotheses via our beam search. And so usually, we will then either stop when we've hit a cut off length, or when we"ve completed a hypothesis. But we will still explore other hypotheses in the future, as long as we have a good idea of what we're looking for. And that's what we'll continue to do in the next few weeks. n complete hypotheses. And then we'll look through the hypotheses that we've completed and say which is the best one of those. And that's the one we'll use. OK. So at that point, we have our list of completed hypotheses and we want to select the top one with the most data. And we want it to be the one that has the highest level of certainty. And so that's what we're going to do. We want to get to the bottom of it. highest score. Well, that's exactly what we've been computing. Each one has a probability that we've worked out. But it turns out that we might not want to use that just so naively. Because that turns out to be a kind of a systematic problem, which is not as a result of any one person's work. It's a problem that can be solved in a number of ways, not just one or two, as some people think. It can be done in a lot of different ways. theorem. But in general, longer hypotheses have lower scores. So if you think about this as probabilities of successively generating each word, that basically at each step, you're multiplying by another chance of generating the next word probability, and commonly those might be 10 to the minus 3, 10 to. 3, or 10 to 10. Theorem is based on the fact that the probability of generating a word is equal to the number of words it takes to generate that word, so 10 is more likely than 10. the minus 2. So just from the length of the sentence, your probabilities are getting much lower the longer that they go on. In a way that appears to be unfair since although in some sense extremely long sentences aren't as likely as short ones. They're not less likely by the way, they're just more likely to have a shorter sentence than a short one. It's just a different way of looking at it, I think. I don't think it's fair to say that short sentences are less likely than long ones. that much. A lot of the time we produce long sentences. So for example, in a newspaper, the median length of sentences is over 20. So you wouldn't want to be having a decoding model when translating news articles that says, huh, just generate two word sentences. They're just way way way too long. That's why we're doing this. We're trying to reduce the number of words that are used in a news article, so that we can understand it better. high probability according to my language model. So the commonest way of dealing with that is that we normalize by length. So if we're working in log probabilities, that means taking dividing through by the length of the sentence. And then you have a per word log probability score. And that's what I use to work out the probabilities of a word in a sentence. That's the way I work with the language model, and that's how I use it to work with words. you can argue that this isn't quite right. In some theoretical sense, but in practice it works pretty well and it's very commonly used. neural translation has proven to be much, much better. It has many advantages. It gives better performance. The translations are better. In particular, they're more fluent because neural language models produce much more fluent sentences. But also, they much better use context because neural Language models give us a very good way of conditioning on a lot of things. contexts. In particular, we can just run a long encoder and condition on the previous sentence, or we can translate words well in context by making use of neural context. Neural models better understand phrase similarities and phrases that mean approximately the same thing. And then the technique of optimizing can be used to translate words into other words or phrases that are similar or similar to each other in a way that makes it easier to understand. For more information on how to use neural context, visit neuralcontext.org. all parameters of the model end to end in a single large neural network has just proved to be a really powerful idea. So previously, a lot of the time, people were building separate components and tuning them individually, which just meant that they weren't actually optimal when put into a much bigger system. So really a hugely powerful guiding idea in neural network land is if you can sort of build one huge network, and just optimize the entire thing end toend, that will give you much better performance than component-wise systems. of that later in the course. The models are also actually great in other ways. They actually require much less human effort to build. There's no feature engineering. In general, no language specific components. You're using the same method for all language pairs. Of course, it's rare for things like this to happen, but it's not unheard of in the world of computer science. It's just a different way of looking at the problem, and a different approach to solving it. to be perfect in every way. So neural machine translation systems also have some disadvantages compared to the older statistical machine translation system. They're less interpretable. It's harder to see why they're doing what they are doing, where before you could actually look at phrase tables and they were useful. So it's not a perfect system, but it's a better one than what we have now, and it's getting better and better, and we're getting closer to the point where we can use it in real life. they're hard to debug. They also tend to be sort of difficult to control. So compared to anything like writing rules, you can't really give much specification as if you like to say I'd like my translations to be more casual or something like that. It's hard to know what to do with them, so it's difficult to know where to go with them. They can be hard to control, they can be difficult to debug, and they can't be controlled very well. they'll generate. So there are various safety concerns. The best way to evaluate machine translation is to show a human being who's fluent in the source. I'll show a few examples of that in just a minute. But first, before doing that, quickly how do we evaluation machine translation? The best Way to evaluate Machine Translation is to Show A Human Being who’s fluent in The Source. The Best Way to Evaluate Machine translation is To Show A human Being who's Fluent in The Sources. target languages the sentences, and get them to give judgment on how good a translation it is. But that's expensive to do, and might not even be possible if you don't have the right human beings around. So a lot of work was put into finding automatic methods of scoring translations that were good enough. The most famous method of doing that is what's called BLEU. And the way you do it is you have a human translation or several human translations of the source sentence, and you're comparing a machine generated translation to those pre-given human written translations. And you score them for similarity by calculating n-gram precisions, i.e., words that overlap between the computer and human reason translation. There's a penalty for too short system translations. So BLEU has proven to be a very useful tool for computer translation. It's been used in the U.S. for more than a decade, and is now being trialled in other countries. It was developed by the University of California, Los Angeles, and the California Institute of Technology. be a really useful measure. But it's an imperfect measure. That commonly there are many valid ways to translate a sentence. And so there's some luck as to whether the human written translations you have happened to correspond to which what might be a good translation from the system. There's a lot of work to be done on how to get the most out of the system, but I think it's a good start. I hope this has helped you out a little, and I hope it's given you a bit of an insight into how the system works. more to say about the details of BLEU and how it's implemented. That you're going to see all of that during assignment 4, because you will be building neural machine translation systems, and evaluating them with a BLEu algorithm. And there are full details about BLE U in the assignment handout. And you'll be able to use them in your own projects. And that's what's going to be happening in the next few weeks, when you're working on assignment 4. But at the end of the day, BLEU gives a score between 0 and 100 where your score is 100. If you are exactly producing one of the human written translations, and 0 if there's not even a single unigram that overlaps between the two, you're on the right track. With that rather brief intro, here are the rules for making a good translation. The rules are a bit vague, but the basic idea is that you should be able to produce a human written translation. I wanted to show you sort of what happened in machine translation. So machine translation with statistical models, phrase-based statistical machine translation that I showed at the beginning of the class had been going on since the mid 2000s decade. And it had produced sort of semi-good results of the sort of the early 2000s. I wanted to see if we could get some of the same results from a machine translation system that we had in the 1990s and 2000s, and we could. kind that are in Google Translate in those days. But by the time you entered the 2010s, basically progress in statistical machine translation had stalled. And most of the increase you were getting over time was simply because you're training your machine to be more accurate, he says. "You were getting barely any increase over time," he says of Google's machine translation efforts in the 1990s and 2000s. "Most of the time, it was just because you were training your Machine to be better at it," he adds. models on more data. In those years, around the early 2010s, the big hope that most people had. was that they would be able to get a better deal on their mortgage. That was a big hope for many people in the U.S. and around the world. That hope was dashed when the housing market crashed in 2008. That's when many people realized that they couldn't get a good deal on a mortgage, even if they had a better rate of return on their home. in the machine translation field had was, well, if we built a more complex kind of machine translation model that knows about the syntactic structure of languages, we'll be able to build much better translations. And so those are the purple systems that we're working on now. We'll be using tools like dependency parsers in the future to help us build more complex machine translation models. We hope to use these tools in the next few years to improve machine translation in a number of different ways. here, which I haven't described at all. But it's as the years went by it was pretty obvious that barely seemed to help. And so then in the mid 2010s, so in 2014 was the first modern attempt to build a neural network from machine translations and encoded-decoder model. And that was the start of a new era in machine translation. And it's been a huge success for the field of machine translation since then. It's also the beginning of the next phase in the field, which is the creation of new neural networks. by the time it was sort of evaluated in bake offs in 2015, it wasn't as good as what had been built up over the preceding decade. But it was already getting pretty good. But what was found was that these new models just really opened up a whole new world for people to explore and play with. It was a big step forward for the company, which had been in decline for a decade before the new models came out. It's now a much more exciting time for car enthusiasts. pathway to start building much, much better machine translation systems. And since then, things have just sort of taken off. And year by year, neural machine Translation systems are getting much better and far better than anything we had preceding that. So for at least the early part of the year, that's what we're going to be working on. And it's going to get better and better, I'm sure, over the next few years. It's a long way off, but it's a good start. application of deep learning and natural language processing, neural machine translation was the huge big success story. In the last few years, when we've had models like GPT2 and GPT3, and other huge neural models like BERT improving web search. It's a bit more complex. But this was the first. But it's the first in a long line of attempts to do this kind of thing. And it's a big step forward for the field of machine translation, which is still in its early stages. area where there was a neural network, which was hugely better than what had preceded, and was actually solving a practical problem. And it was stunning with the speed at which success was achieved. So 2014 were the first what I call the first 'Neural Network Years' 2014 was the year of the neural network. It was the first time the world had had a network that could solve practical problems that lots of people in the world need. That was a big step forward for the field of artificial intelligence. here fringe research attempts to build a neural machine translation system. Three or four people who are working on neural network models thought, oh, why don't we see if we can use one of these to translate, learn to translate sentences, where there weren't really people with a neural network to do it. There are now a few people working on the project, but it's still in its early stages, and it's not clear if it will be able to be used in the next few years. Within two years' time, Google had switched to using neural machine translation for most languages. By a couple of years later, after that, essentially anybody who does machine translation is now deploying live neural machine Translation systems. So that was sort of just an amazing technological transition that for the preceding decade, the big statistical machine translation systems like the previous generation of Google Translate had literally been built up by hundreds of engineers over the years. But a success was achieved so quickly that within two years of that switch, Google was able to get much, much better results. comparatively small group of deep learning people in a few months with a small amount of code. And hopefully, you'll even get a sense of this doing assignment 4, were able to build neural machine translation systems that proved to work much better. Does that mean that machine translation is becoming more and more popular? We'll have to wait and see, but it's a step in the right direction, say the researchers at the University of California, San Diego, who have been working on the project for a few years. solved? No. There are still lots of difficulties which people continue to work on very actively. But there are lots of problems with out of vocabulary words. And there are domain mismatches between the domain and the language. And you can see more about it in the Skynet Today article that's linked at the bottom of the page. The article is based on an earlier version of this article that was published on November 14, 2013. We are happy to clarify that this article was amended on November 16, 2013 to make clear that we are still working on solving the problem of out of vocab words. training and test data. So it might be trained mainly on news wire data but you want to translate people's Facebook messages. There are still problems of maintaining context over longer text. We'd like to translate languages for which we don't have much data. And so these methods work by using data from the news wire and testing it on Facebook messages to see if it can translate the text. It's not a perfect system, but it's a good starting point for a project that needs more data. Far the best when we have huge amounts of parallel data. Even our best multilayer LSTMs aren't that great of capturing sentence meaning. There are particular problems such as interpreting what pronouns refer to, or in languages like Chinese or Japanese, where there's often no pronoun present. But there is still a lot of work to be done to improve our understanding of language and language meaning. Back to Mail Online home. back to the page you came from. Back To the pageYou were from. an implied reference to some person working out how to translate that. For languages that have lots of inflectional forms of nouns, verbs, and adjectives. These systems often get them wrong. So here's just sort of quick funny examples of the kind of things that can happen in languages that don't have this kind of system. So there's still tons of stuff to do. We'll be back in a few days with some more examples of how to use this system in other languages. of things that go wrong, right? So if you asked to translate paper jam. Google Translate is deciding that this is a kind of jam just like this. Raspberry jam and strawberry jam. And so this becomes a jam of paper. There are problems of agreement and choice. So if we asked for paper jam, we would get a jam that was made out of paper, which is a different type of jam than the one we were asking for. We would get the jam that we got out of raspberry and strawberry. you have many languages don't distinguish gender. And so the sentences are neutral between things masculine or feminine. Malay, or Turkish are two well known languages of that sort. But what happens when that gets translated into English by Google Translate is that the English language model just kicks. It's a bit of a shock to the system, but it's not that bad. It just means that the language model kicks in and that's the way it's always going to be. We'll have to wait and see what happens next. Use singular they in all contexts when you're putting material online. So these gender neutral sentences get translated into, she works as a nurse. He works as an programmer. So if you want to help solve this problem, all of you can help by using singular they. In the meantime, please share this article with your friends and family on Facebook and Twitter. Back to Mail Online home.back to the page you came from. Click here to share this story with your family and friends. People noticed a couple of years ago that if you choose one of the rarer languages that Google will translate. And that could then change the distribution of what's generated. And people also work on modeling improvements to try and avoid this. Here's one more example that's kind of funny. People noticed a two-year-old that Google would translate in a rarer language that it doesn't usually translate in. That's called the "Google Translate" tool, and it works in English, Spanish and French. such as Somali, and you just write in some rubbish like ag ag ag. Freakily, it had produced out of nowhere prophetic and biblical texts, as the name of the Lord was written in the Hebrew language. It was written  in the language of the Hebrew nation, which makes it very strange. It's like the Bible is written in a different language every time, and it's very strange to see it written in such a different way every time. It makes no sense, it's just a waste of time. no sense at all. Well, we're about to see a bit more about why this happens. But that was a bit worrying. As far as I can see, this problem is now fixed in 2021. I couldn't actually get Google Translate to generate examples like this anymore. So there are reasons for this, and we're going to find out what they are. We'll have to wait until 2021 to see if the problem has been fixed, though. We're looking forward to finding out more about it. lots of ways to keep on doing research. NMT certainly is a flagship task for NLP and deep learning. And it was a place where many of the innovations of deep learning NLP were pioneered. People continue to work hard on it, people have found many, many improvements. And there's lots of work to be done in the area of NMT and NLP. It's a great place to start and there's a long way to go in the field of NLP research. actually for the last bit of the class and the minute I'm going to present one huge improvement, which is so important that it's really come to dominate the whole of the recent field of neural networks for NLP. And that's the idea of attention. But before I get onto that, I want to give you a little bit of an overview of what I've been working on in the last few months. I've just finished a paper that was published in the Journal of NLP, and I want you to take a look at it. For assignment 4 this year, we've decided to do Cherokee English. We hope it will be interesting, but it's also a real challenge. It's a new version of the assignment, which we hope will beinteresting. But it's a challenge, too, because it's in Cherokee English, which is a very different language. So we hope it'll be interesting. It'll be a challenge. And it's going to be fun. We're looking forward to it. We've got a lot of work to do. Cherokee is an endangered Native American language that has about 2000 fluent speakers. It's an extremely low resource language. So it's just there isn't much written Cherokee data available period. And particularly, there's not a lot of parallel sentences between Cherokee and English. And here's the answer to the question, "What is the difference between English and Cherokee?" The answer is that Cherokee is a lot more complex than English. The answer to that question is, "There is no difference between the two." For languages for which there isn't much parallel data available, commonly the biggest place where you can get parallel data is from Bible translations. So you can have your own personal choice wherever it is over the map as to where you stand with respect to where the Bible stands. Google's freaky prophetic translations. For more information on how to use Google Translate, go to Google translate.com. for more information about how the service works, or to get more information, visit Google translation.com for more. religion. But the fact of the matter is if you work on indigenous languages, what you very, very quickly find is that a. lot of the work that's done on collecting data on indigenous. languages, and a much of the material that is available in written form for many indigenous. communities, has been lost. The work of the U.S. government to improve indigenous language and culture has been largely lost, he says. The U.N. has been working with the governments of Australia and New Zealand to help improve indigenous languages. languages is Bible translations. So this is a piece of parallel data that we can learn from. So the Cherokee writing system has 85 letters. And the reason why it has so many letters is that each of these letters is a Cherokee letter. And so here's the initial bit of a story long ago where seven boys who used to spend all their time down by the townhouse. OK. So you can see that theWriting system has a mixture of things that look like English letters and then all sorts of letters that don't. letters actually represents a syllable. So many languages of the world have strict consonant vowel syllable structure. So you have words like right up here or something like that for Cherokee, right? And another language like that's Hawaiian. And so each of the letters represents a combination of a consonant and a vowel. It's a very complex system, but it's very common in many languages around the world, including English, Spanish, French, German, Japanese, and many others. and a vowel. And that's the set of those. You then get 17 by 5 gives you 85 letters. Big thanks to people from University of North Carolina, Chapel Hill who've provided the resources were using for this assignment. Although you can't do that, you can do this assignment with the help of some of the resources we've been using for the past few weeks. We hope you've enjoyed it as much as we have. We'll see you in the next week or so. do quite a lot of languages on Google Translate. Cherokee is not a language that Google offers on Googletranslate. So we can see how far we can get. But we have to be modest in our expectations because it's hard to build a very good MT system with only a few languages on it, he says. He adds: "It's hard for us to be very good at it because we don't have enough languages on the system" He adds that Google is working on adding more languages to its Translate service. a fairly limited amount of data. So we'll see how far we can get. There is a flipside, which is for you students doing the assignment. The advantage of having not too much data is that your models will train relatively quickly. so we'll actually have less trouble than we think we'll have with this assignment. We'll see what we can come up with in the next few days. We're looking forward to seeing what you can do with your data. We hope you'll join us. did last year with people's models taking hours to train as the assignment deadline closed in. So we have some idea what we're talking about. So the Cherokee originally lived in Western North Carolina and Eastern Tennessee. They then sort of got shunted and moved to other parts of the U.S. and became known as Cherokees. They are now considered one of the most endangered tribes in the United States, along with the Blackfeet and the Creek. They live in parts of North and South Carolina, Georgia, and Tennessee. Southwest from that. And then in particular, for those of you who went to American high schools and paid attention, you might remember discussion of the Trail of Tears. A lot of the Native Americans from the Southeast of the US got forcibly shoved a long way further West. And that's when they were forcibly pushed out of the Southeast and into what is now called the West Coast of the United States. That's where they were forced to leave their homes and go to the West. The writing system that I showed on this previous slide, it was invented by a Cherokee man, Sequoyah. That's a drawing of him there. And that was actually kind of an incredible thing. So most Cherokee now live in Oklahoma. There are some that are in North Carolina. And so most Cherokee are from Oklahoma, but some are from North Carolina, too. So that's a little bit of a change in the mix, but it's not a big one, I think. he started off illiterate and worked out how to produce a writing system that would be good for Cherokee. And given that it has this consonant-vowel structure, he chose a syllabary which turned out to be a good choice. So here's a neat historical fact. So in the 1830s and 1840s, the Cherokee were illiterate. And so they had to learn how to write using a system that was good for them. And that was the syllabaries. 1840s, the percentage of Cherokee that were literate in Cherokee written like this was actually higher than thepercent of white people in the southeastern United States at that point in time. OK. Before time disappears, oops, time has almost disappeared. I was just starting to say, oh no, I'll. Oh no. I'll say it again. Oh, my God, I'm sorry. I'm so sorry, I didn't mean to say that. I did not mean to. have to do a bit more of this. Next time. That'll be OK, right? So the final idea that's really important for sequence to sequence models is the idea of attention. And so we had this model of doing sequence to. We'll have to do more of that next time, I'm sure. That's the way it is. We're not going to do it all at once, but we're going to try and do as much as we can in the next few days. sequence models such as for neural machine translation. And the problem with this architecture is that we have this one hidden state, which has to encode all the information about the source sentence. So it acts as a kind of information bottleneck. And that's all the info that the generation of the next generation needs to get. It's a very complex architecture, but we're getting close to the end of it. We're getting closer to the point where we can use it for machine translation, but it's not there yet. is conditioned on.is conditioning on. Is conditioned on is conditioned on, and so on. But that method turns out to be better for things. Well, I did already mention one idea last time of how to get more information where I said look maybe you could kind of average all of the vectors of the source to get a sentence representation. But this time, I'm going to use a different method. This time, we're going to look at the data as a whole, not just the vectors. like sentiment analysis. And not so good for machine translation, where the order of words is very important to preserve. So it seems like we would do better, if somehow, we could get more information from the source sentence while we're generating the translation. And in some sense, this just just makes sense, in that it's a better way to get the information we need to make a better translation. But it's not a perfect solution, and we'll have to keep trying to improve it. corresponds to what a human translator does, right? If you're a human translate, you read the sentence that you're meant to translate. And you maybe start translating a few words. But then you look back at the source sentence to see what else was in it and translate some more. That's what a translator does. It's the same thing a translator would do if they were trying to read a book. It would be like reading a book and trying to understand it. words. So very quickly after the first neural machine translation systems, people came up with the idea of maybe we could build a better neural empty MT that did that. So the core idea is on each step of the decoder, we're going to be able to focus attention on a specific part of the code. And that's the concept of attention. It's the idea that we're trying to get more out of our brains. We want to make our brains focus on different parts of a code. use a direct link between the encoder and the decoder that will allow us to focus on a particular word or words in the source sequence and use it to help us generate what words come next. I'll just go through now showing you the pictures of what attention does. Use these pictures to help you understand how attention is used in the game. Use the code below to play the game and learn more about how attention works in a game. The code is written in C, C++, C#, C5, C6, C7 and C8. and then at the start of next time we'll go through the equations in more detail. So we use our encoder just as before and generate our representations, feed in our conditioning as before, and say we're starting our translation. But at this point, we take this hidden representation, and then we take the hidden representation and translate it to the real world. And then we go on to the next part of the series. The next part will be about the code that is used to translate the code to the human brain. say, I'm going to use this hidden representation to look back at the source to get information directly from it. So what I will do is I will compare the hidden state of the decoder with the hiddenState of the encoder at each position. This will generate an attention score, which I will use to calculate the source's attention score. I will then use this attention score to determine how much attention the source is paying to the source, and how much it is paying the source. which is a kind of similarity score like a product. And then based on those attention scores, I'm going to calculate a probability distribution as to by using a softmax as usual to say which of these encoder states is most like my decoder state. And so we'll be training to get a better understanding of what is going on in the brain. We'll also be training the brain to learn more about what is happening in other parts of the brain, such as how the brain works. the model here to be saying, well, probably you should translate the first word of the sentence first, so that's where the attention should be placed. So then based on this attention distribution, which is a probability distribution coming out of the softmax, we're going to generate a new attention. The new attention is based on the new attention distribution coming from thesoftmax. The softmax is a type of probability distribution that is used to generate attention. It's based on a number of factors, such as the number of words in a sentence. output. And so this attention output is going to be an average of the hidden states of the encoder model. That is going be a weighted average based on our attention distribution. So we then kind of take that attention output, combine it with the hidden state of the encoding model and get the output of the encode model. The output is then the average of that output and thehidden state of that encoding model, which is called the output state. That's the output output. And then we take that and combine it to get the attention output. "We hope to generate he.the decoder RNN and together, the two of them are then going to be used to predict via a softmax what word to generate first," he says. "And then at that point, we sort of chug along and keep doing the same kind of computations at the same time," he adds. "It's a very exciting time for us. We're looking forward to seeing what the future holds for this kind of research" he says, adding that it could lead to a new generation of computers. each position. There's a little side note here that says sometimes we take the attention output from the previous step, and also feed into the decoder along with the usual decoder input. So we're taking this attention output and actually feeding it back in to the hidden state calculation. And we're also taking the attention input and feeding it into the normal decoder output. And so we're actually feeding this output back into thehidden state calculation as well as the usual input to the decoders. that can sometimes improve performance. And we actually have that trick in the assignment 4 system. And you can try it out. OK. So we generate along and generate our whole sentence in this manner. And that's proven to be a very effective way of getting more information from the audience. And it's a very good way to get more information about what's going on in the room. So that's what we're going to try and do with this article. We'll let you know how it goes. source sentence more flexibly to allow us to generate a good translation. I'll finish this off by going through the actual equations for how attention works. Next time I'll go through how attention is generated in the language of the brain and how it works in the brain of the human brain. Click here to read the rest of the article and follow me on Twitter @jenniferjames1 or on Facebook @JennyJames1. Back to the page you came from.