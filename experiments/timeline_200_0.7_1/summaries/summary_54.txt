foreign I'm really excited especially for this lecture which is a very special lecture on robust and trustworthy deep learning by one of our sponsors of this amazing course themus AI. Theyus AI is a startup actually locally based here in Cambridge our mission is to design advance and deploy the future of AI and trustworthy AI specifically. I'm especially excited about today's lecture because I co-founded Themis right here at MIT right here in this very building in fact this all stemmed from really the incredible scientific innovation and advances that we created right here. Sadhana sadhana is a machine learning scientist at Themis Ai. She's also the lead TA of this course intro to deep learning at MIT. Her research focuses specifically on how we can build very modular and flexible methods for AI and building what we call a safe and trustworthy Ai. Sadhana will be teaching us more about specifically the bias and the uncertainty of AI algorithms which are really two key or critical components towards achieving this Mission. She'll be talking to you all about robust and trustworthy deep learning on. behalf of Themis so over the past decade we've seen some tremendous growth in artificial intelligence across safety critical domains in the Spheres of autonomy and Robotics. We now have models that can make critical decisions about things like self-driving at a second's notice and these are Paving the way for fully autonomous vehicles and robots. That's not where this stops in theSpheres of medicine and Healthcare robots are now equipped to conduct life-saving surgery. We have algorithms that generate predictions for critical drugs that may cure diseases that we previously thought were incurable. rooms is this these are some headlines about the failures of AI from the last few years alone in addition to these incredible advances we've also seen catastrophic failures. Every single one of the safety critical domains I just mentioned these problems range from crashing autonomous vehicles to healthcare algorithms that don't actually work for everyone even though they're deployed out in the real world. At a first glance this seems really demoralizing if these are all of the things wrong with artificial intelligence how are we ever going to achieve that vision of having our AI integrated into the fabric of our daily lives. A lot of the problems in modern day AI are the result of a combination of unmitigated bias and uncertainty. In this lecture we're going to focus on investigating the root causes of all of these problems these two big challenges to robust AI are bias bias and uncommunicated uncertainty. This lecture will look at how to overcome these two challenges and how to make AI more robust and reliable. The lecture will also focus on how to improve the quality of the data that is being used to train AI models. Bias bias is a word that we've all heard outside the context of deep learning but in the context. of machine learning it can be Quantified and mathematically defined. We'll talk about how to do this and methods for mitigation of this bias algorithmically and how Themis is innovating in these areas in order to bring new algorithms in this space to Industries around the world. After we talk about uncertainty which is can we teach a model when it does or doesn't know the answer to us to its given task and we will talk about the ramifications for this for real world AI. a lot of clinical data sets where they often contain fewer examples of diseased patients than healthy patients because it's much easier to acquire data for healthy patients than their disease counterparts. We also have selection bias at the data portion of the AI lifecycle think about Apple's series voice recognition algorithm this model is trained largely on Flawless American English but it's deployed across the real world to be able to recognize voices with accents from all over the world. These biases can be propagated towards models training Cycles themselves which is what we'll focus on in the second half of this lecture. I have a model that I trained on the past 20 years of data and then I deploy it into the real world in 2023 this model will probably do fine because the data input distribution is quite similar to data in the training distribution. What would happen to this model in 2033 it's it probably would not work as well because the distribution that the data is coming from would shift significantly across this decade. If we don't continue to update our models with this input stream of data we're going to have Obsolete and incorrect predictions. Commercial facial detection systems are everywhere you actually played around with some of them in lab two when you trained your vae on a facial detection data set. Facial detection systems also present in the automatic filters that your phone cameras apply whenever you try to take a picture and they're also used in criminal investigations. We'll analyze the biases that might have been present in all of them for in the in the next few minutes. We're going to face evaluation bias so now let's talk about another example in the real world of how bias can perpetuate throughout the course of this artificial intelligence life cycle was also comprised mostly of lighter skin faces these accuracy metrics would be incredibly inflated and therefore would cause unnecessary confidence and we could deploy them into the real world in fact the biases in these models were only uncovered once an independent study actually constructed a data set that is specifically designed to uncover these sorts of biases by balancing across race and gender. There are other ways that data sets can be biased that we haven't yet talked about so so far we've assumed a pretty key assumption in our data set. This is a really big problem and it's very common across a lot of different types of machine learning tasks and data sets. The first way that we can try to mitigate class imbalance is using sample re-weighting which is when instead of uniformly sampling from our data set we instead sample at a rate that is inversely proportional to the incidence of a class in our dataSet. The second way we can mitigate class and balance is through loss re- weightsing which means that samples from underrepresented classes contribute more to the loss function. input face to a as a negative it'll be highly penalized if it does so because the loss of the faces would contribute more to the total loss function. The final way that we can mitigate class imbalance is through batch selection which is when we choose randomly from classes so that every single batch has an equal number of data points per class so is everything solved? There are other forms of bias that exist even when the classes are completely balanced because the thing that we haven't thought about yet is latent features. latent features are the actual represent is the actual representation of this image according to the model. set of features can we still apply the techniques that we just learned about the answer is that we cannot do this. The problem is that the bias present right now is in our latent features all of these images are labeled with the exact same label so according to the as the model all we know is that they're all faces. We can't apply any of the previous approaches that we used to mitigate class imbalance because our classes are balanced but we have feature imbalance now however we can adapt the previous methods to account for bias in latent features which we'll do in just a few slides. our biased features and then apply resampling so let's say in reality that this data set was biased on hair color most of the data set is made up of people with blonde hair with faces with black hair and red hair underrepresented. If we knew this information we could label the hair color of every single person in this dataSet and we could apply either sample re-weading or loss relating just as we did previously. The second thing is exactly what you said which is once we have our bias features going through and annotating every image with this feature is an extremely labor-intensive task. to dbias a model so what we want is a way to learn the features of this data set and then automatically determine that samples with the highest feature bias and the samples with lowest feature bias we've already learned a method of doing this in the generative modeling lecture you all learned about variational autoencoders which are models that learn the latent features of a data set. We want samples that are similar to each other in the input to decode to latent vectors that are very close to eachOther in this latent space. walk through step by step a de-biasing algorithm that automatically uses the latent features learned by a variational autoencoder to under sample and oversample from regions in our data set. This debiasing model is actually the foundation of themis's work this work comes out of a paper that we published a few years ago that has been demonstrated to debias commercial facial detection algorithms and um it was so impactful that we decided to make it available and work with companies and industries. good representation of what a face actually is so now that we have our latent structure we can use it to calculate a distribution of the inputs across every latent variable. We can estimate a probability distribution depending on that's based on the features of every item in this data set essentially what this means is that we can calculate the probability that a certain combination of features appears in our data set. Then we can over sample denser or sparser areas of the data set and under sample from denser areas. The approach basically approximates the latent space via a joint histogram over the individual latent variables so we have a histogram for every latent variable Z sub I and what the histogram essentially does is it discretizes the continuous distribution so that we can calculate probabilities more easily. This allows us to train in a fair and unbiased manner to dig in a little bit more into the math behind how this resampling works. We can Define the adjusted probability for sampling for a particular data point as follows the probability of selecting a sample. data point x will be based on the latent space of X such that it is the inverse of the joint approximated distribution. As Alpha increases this probability will tend to the uniform distribution and if Alpha decreases we tend to de-bias more strongly. This gives us the final weight of the sample in our data set that we can calculate on the Fly and use it to adaptively resample while training. Once we apply these this debiasing we have pretty remarkable results. Keep this algorithm in mind because you're going to need it for the lab 3 competition which I'll talk more about towards the end of this. lecture so so far we've been focusing mainly on facial recognition systems and a couple of other systems as canonical examples of bias however bias is actually far more widespread in machine learning consider the example of autonomous driving many data sets are comprised mainly of cars driving down straight and sunny roads in really good weather conditions with very high visibility. In some specific cases you're going to face adverse weather bad um bad visibility near Collision scenarios and these are actually the samples that are the most important for the model to learn. an extremely famous paper a couple years ago showed that if you put terms that imply female or women into a large language model powered job search engine you're going to get roles such as artists or things in the humanities. If you help input similar things but of the male counterpart you'll end up with roles for scientists and engineers so this type of bias also occurs regardless of the task at hand for a specific model. Finally let's talk about Healthcare recommendation algorithms these recommendation algorithms tend to amplify racial biases. uses that UL will also be developing today and for the next part of the lecture we'll focus on uncertainty or when a model does not know the answer. We'll talk about why uncertainty is important and how we can estimate it and also the applications of uncertainty estimation. To start with what is uncertainty and why is it necessary to compute let's look at the following example this is a binary classifier that is trained on images of cats and dogs for every single input it will output a probability distribution over these two classes. Uncertainty estimation is useful for scenarios like this this is an example of a Tesla car driving behind a horse-drawn buggy which are very common in some parts of the United States. The exact same problem that resulted in that video has also resulted in numerous autonomous car crashes so let's go through why something like this might have happened. There are multiple different types of uncertainty in neural networks which may cause incidents like the ones that we just saw we'll go through a simple example that illustrates the two main type of uncertainty that we'll focus on. of a regression task the input here x is some real number and we want it to Output f of x which is should be ideally X cubed so right away you might notice that there are some issues in this data set assume the red points in this image are your training samples so the boxed area of this image shows data points in our data set where we have really high noise. If we queried the model for a prediction in this part of in this region of the data set we should not really expect to see an accurate result. names to the types of uncertainty that we just talked about the blue area or the area of high data uncertainty is known as aliatoric uncertainty. The green areas of this just the green boxes that we talked about which were Model uncertainty are known as epistemic uncertainty. This cannot be learned directly from the data however it can be reduced by adding more data into our systems into these regions okay so first let's go through alliatoric uncertainty so the goal of out estimating alliatorIC uncertainty is to learn a set of variances that correspond to the input. model we have the same output size that predicts a variance for every output so the reason why we do this is that we expect that areas in our data set with high data and certainty are going to have higher variance. The crucial thing to remember here is that this variance is not constant it depends on the value of x. Now that we have this model we have an extra layer attached to it in addition to predicting y hat we also predict a sigma squared how do we train this model? that we're estimating is accurate so in addition to adding another layer to estimate alliatoric uncertainty correctly we also have to change our loss function so the mean squared error actually learns a multivariate gaussian with a mean y i and constant variance and we want to generalize this loss function to when we don't have constant variance. We can think about this for now as a generalization of the mean square error loss to non-constant variances. Now that we know how to estimate aliatoric uncertainty let's look at a real world example for this task we'll focus on semantic segmentation. a data set called cityscapes and the inputs are RGB images of scenes the labels are pixel wise annotations of this entire image of which label every pixel belongs to and the outputs try to mimic the labels they're also predicted pixel wise masks. Why would we expect that this data set has high natural alliatoric uncertainty and which parts of this dataSet do you think would have aliatoric uncertainty. Even if your pixels are like one row off or one column off that introduces noise into the model the model can still learn in the face of this noise but it does exist and it can't be reduced. uncertainty can best be described as uncertainty in the model itself and it is reducible by adding data to the model so with epistemic uncertainty essentially what we're trying to ask is is the model unconfident about a prediction. A really simple and very smart way to do this is let's say I train the same network multiple times with random initializations and I ask it to predict the exact I call it on the same input. We should see that with familiar inputs in our Network our networks should all converge to around the same answer. very little variance in the um the logits or the outputs that we're predicting. If a model has never seen a specific input before or that input is very hard to learn all of these models should predict slightly different answers and the variance of them should be higher than if they were predicting a similar input. training an ensemble of networks is really compute expensive even if your model is not very large training five copies of it tends to it takes up compute and time. However the key insight for ensembles is that by introducing some method of Randomness or stochasticity into our networks we're able to estimate epistemic uncertainty. can keep these Dropout layers enabled at test time usually we don't keep Dropout layer enabled attest time. We don't want to lose any information about the the Network's process or any weights when we're at inference time. When we're estimating epistemic uncertainty we do want to keep Drop out enabled at Test time because that's how we can introduce Randomness at inference times. So we have that that measure of Randomness and stochasticity so again in order to implement this what we have is a model. epistemic uncertainty so both of the methods we talked about just now involve sampling and sampling is expensive ensembling is very expensive but even if you have a pretty large model um having or introducing Dropout layers and calling 24 word passes might also be something that's pretty infeasible. At Themis we're dedicated to developing Innovative methods of estimating epistemic uncertainty that don't rely on things like sampling so that they're more generalizable and they're usable by more Industries and people. training The Ensemble and calling multiple ensembles on the same input we received multiple predictions and we calculated that variance. Now that we have many methods in our toolbox for estimating epistemic uncertainty let's go back to our real world example let's say the again the input is the same as before it's a RGB image of some scene in a city and the output again is a pixel level mask of what every pixel in this image belongs to which class it belongs to. Would you expect to have high epistemic uncertainty in this example? the sidewalk to the road and other parts of the sidewalk are labeled incorrectly and we can using epistemic uncertainty we can see why this is the areas of the sidewalks that are discolored have high levels of uncertainty. maybe this is because the model has never seen an example of a sidewalk with multiple different colors in it before or maybe it hasn't been trained on examples with sidewalks generally either way epistemic Uncertainty has isolated this specific area of the image as an area of high uncertainty. and uncertainty to mitigate risk in every part of the AI life cycle. We have analyzing the data before a model is even trained on any data we can analyze the bias that is present in this data set and tell the creators whether or not they should add more samples. Once we're actually training a model if it's already been trained on a bias data set we can de-bias it adaptively during training. Afterwards we can also verify or certify deployed machine learning models making sure that models that are actually out there are as safe and unbiased as they claim they are. is by leveraging epistemic uncertainty or bias in order to calculate the samples or data points that the model will do the worst on the model has the high the most trouble learning or data set samples that are the most underrepresented in a model's data set. Lastly we can think about we're developing a product at Themis called AI guardian and that's essentially a layer between the artificial intelligence algorithm and the user and the way this works is this is the type of algorithm that if you're driving an autonomous vehicle would say hey the model doesn't actually know what is happening in the world around it right now. i's product called capsa which is a model agnostic framework for risk estimation so capsa is an open source Library you all will actually use it in your lab today. It transforms models so that they're risk aware so this is a typical training pipeline you've seen this many times in the course by now we have our data we have the model and it's fed into the training algorithm and we get a trained model at the end that outputs a prediction for every input. With capsa what we can do is by adding a single line into any training workflow we can turn this model into a risk-aware variant that essentially calculates biases uncertainty and label noise. then further analyze and so this is the one line that I've been talking about um after you build your model you can just create a wrapper or you can call a wrapper that capsa has a an extensive library of. Capsa wraps models for every uncertainty metric that we want to estimate we can apply and create the minimal model modifications as necessary while preserving the initial architecture and predictive capabilities. This could be adding a new layer in the case of a variational autoencoder this could be creating and training the decoder and calculating the Reconstruction loss on the Fly. Themis is unlocking the key to deploy deep learning models safely across fields. We can now answer a lot of the questions that the headlines were raising earlier which is when should a human take control of an autonomous vehicle. What types of data are underrepresented in commercial autonomous driving pipelines. We now have educated answers to these questions due to products that Themis is developing and in spheres such as medicine and health care we can now answers questions such as when is a model uncertain about a life-threatening diagnosis. compete in the competition which the details are described in the lab but basically it's about analyzing this data set creating risk-aware models that mitigate bias and uncertainty in the specific training pipeline. At Themis our goal is to design advance and deploy a trustworthy AI across Industries and around the world. We're hiring for the upcoming summer and for full-time roles so if you're interested please send an email to careers themesai.io or apply by submitting your resume to the Deep learning resume drop and we'll see those resumes and get back to you.