This week in week three, we're actually going to have some human language, and so this lecture has no partial derivative signs in it. The idea of phrase structure is to say that sentences are built out of units that progressively nest. So, we start off with words that, cat, cuddly, et cetera, and then we're gonna put them into bigger units that we call phrases. And then you can keep on combining those up into even bigger phrases, like, "The cuddlely cat by the door" what you learned about neural networks last week and the content of today, and jump straight right in to building a neural dependency parser. Um, the other thing that happens in assignment three is that, we start using a deep learning framework PyTorch. So, if you have any issues with, with that, um, well, obviously, you can send Piazza messages, come to office hours. We have under the sort of office hours page on the website, a listing of the expertise of some of the different TAs. Grammar is made of a, um, a determiner, followed by a noun. We have a system of dependency labels. All we're doing for this class is making the arrows for the arrows. And you should be able to interpret things like prepositional phrases as to what they're modifying. Dependencies are connected and whether that's right or not, you can get in touch with the tree bank to get a sense of what's going on in the tree. And if you think, "Man, this stuff is fascinating. I wanna learn all about these linguist structures," you don't need to be a linguist. In this system of dependencies I'm going to show you, we've got in as kind of, um, a modifier of crate in the large crate. The second meaning the sentence can have is, that's the man has a knife. And so, the interpretations of these sentences that you can get depend on putting different structures over the sentences in terms of who is- what is modifying what? Um, here is another one that's just like that one. Um, scientists count whales from space. to be able to do that. And one of the ways of saying, um, that's important is saying, ''What can go wrong?'' Okay. So here, is a newspaper article. Uh, ''San Jose cop kills man with knife''. Um, now, this has two meanings and the two meanings depend on, well, what you decide depends on what, you know, what modifies what? Okay. Meaning one. The cop stabs the guy. [LAUGHTER] In programming languages, an else is always construed with the closest if. But that's not how human languages are. Human languages are, um, this prepositional phrase can go with anything proceeding, and the hearer is assumed to be smart enough to work out the right one. That's where if you want to have artificial intelligence and smart computers, we then start to need to build language understanding devices who can also work on that basis. Doctor: No heart, cognitive issues.um, that are starting to turn up as in the bottom example." to potentially consider an exponential number of possible structures because, I've got this situation where for the first prepositional phrase, there were two places that could have modified. And so, if you get into this sort of combinatorics stuff the number of analyses you get when you get multiple prepositions is the sequence called the Catalan numbers. Ah, but that's still an exponential series. And it's sort of one that turns up in a lot of studies of the human brain. "There's a question of how far apart words are. Most dependencies are fairly short distance. They not all of them are. There are two ways we can do that. We're treating each arc individually, treating each word individually. And there are the correct arcs and to evaluate dependency parsers, we're simply gonna say which arcs are correct. So there's a gold arc from two to one from zero to one. She saw a subject, and there's the root of the sentence, these are the gold arcs. Um, if we're gonna generate a parse, as to what the head of each word is, what we're going to propose as the unlabeled attachment. being attached, we've now got this big verb phrase we call it, right, so that when you've sort of got most of a sentence but without any subject to it, that's sort of a verb phrase to be used for Olympic beach volleyball which might be then infinitive form. Sometimes it's in part of CPO form like being used for beach volleyball. And really, those kind of verb phrases they sort of just like, um, prepositional phrases. Whenever they appear towards the right end of sentences, they can modify various things like verbs or nouns. is, um, the results demonstrated that KaiC interacts rhythmically with SasA Ka- KaiA and KaiB. Most NLP work uses fine-grained parts of speech. So maybe we could have distributed representations, a part of speech that represent their similarity. Um, well if we're gonna do that, why not just keep on going and say the dependency labels. They also have a distributed representation. And so, we built a representation that did that. So the idea is that we have in our stack, to [inaudible] [OVERLAPPING] sort of put the words in a line and that makes it. He see, let's see the whole sentence. You draw this sort of loopy arrows above them and the other way is you sort of more represent it as a tree. So, the dependence of bills and were submitted words, the dependent of submitted and you're giving this kind of tree structure. And that's not very good if you want to parse the whole web, whereas if you have something that's linear time, that's really getting you places. Most attempts to understand the structure of human languages are essentially Dependency Grammars. Lucie Tesniere formalized the kind of version of dependency grammar that I've been showing you. Universal Dependencies is project I'm strongly involved with. It's not only about English. You can find Universal Dependency analyses of French, or German, or Finish, or Carsac, or Indonesian, um, lots of languages. Of course, there are even more languages which there aren't universal Dependencies analyses of. So, if you have a big calling to say I'm gonna build a Swahili Universal it is. In the 60s, 70s and 80s, people used to hand-engineer language features. These features were very sparse. Each of these features matches very few things. Um, they match some configurations but not others so the features tend to be incomplete. And so it turned out that actually computing these features was just expensive so that you had some configuration on your stack and the buffer. And then you wanted to know which of these feature were active for that stack and buffer configuration. So that worked well but, you know, I had different choices of when to pa- when to shift and when to reduce. And I just miraculously made the right choice at each point. Using a neural network to make the decisions of Joakim Nivre Style shift-reduce parser, we could produce something that was almost as accurate as the very best parsers available at that time. Um, they put in Beam search as I sort of mentioned. Beam search can really help. Do humans always agree on how to build this trees and if they don't, what will be the [inaudible] or agreement of humans relative to each other? Um, sad but true. Sometimes the answer to making the results better is to make it bigger, deeper and spend more time choosing the hyper-parameters. There's still room to do better. I mean, at the unlabeled attachment score, it's actually starting to get pretty good. But there certainly are cases and that includes some of the prepositional phrase attachment ambiguities. Sometimes there are multiple attachments that sort of same clause although it's not really clear which one is right even though there are lots of other circumstances where one of them is very clearly wrong. But, you know, so this actually, um, led to ah sort of a new era of sort of better parsers.