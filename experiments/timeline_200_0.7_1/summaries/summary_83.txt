RAMESH RASKAR: So this is a position, and this is superposition. And that concept of a position or superposition applies to all three types, shadows- or refraction- or reflection-based techniques. So we saw this last time, and we'll see how-- we already have some projects that are inspired by biological vision. You know, Matt is trying the chicken. And I think it's going to be-- [LAUGHTER] It is going to. Coded imaging is a co-design between how you capture the image and how you process the image. In a typical film camera, or even it is digital camera, you take the picture, and that's basically the end of the story. Here, you're trying to do something clever about how the picture is taken. You can either take a really short exposure photo. But that's going to be very dark. If you take a high ISO, you can recover some information. Or you can just take a long-exposure photo by keeping the shutter open. Photographer RamESH RASKAR: When you try to recover this information, you start getting this banding artifacts. And we'll see it in the next slide, why that happens. If you keep the shutter open for even longer, it'll blur correspondingly longer. So you can basically represent that as a sharp photo, where there is a convolution of the sharp photo with some kind of a Convolution filter. But then you will get a blurry photo, which is well exposed, but a lot of high-frequency details are lost. have basically a 1D convolution that's converting this image into this image. And the way to think about that is in the Fourier domain because convolution in the image domain, our primary domain, is multiplication. So let's say we take this photo. Find the Fouriers transform here. Multiply that by the Foueline transform of a box function, which is a sync. So what that means is that I'm going to take the lowest frequency, multiply by that value, and so on. the frequencies in the image by the amplitude of the Fourier transform of this. And you can already see that lower frequencies will be preserved. But there's also something strange happening. Even some of the lower frequencies are actually being set to 0, which means that in this photo, these frequencies are missing altogether. They have been suppressed. So the moment you take the photo, the damage is done. And there's nothing you can do to recover those frequencies because in the Fouriers domain, you cannot divide those frequencies by 0 and recover an image. box function, which is equivalent to-- when you release the shutter, opening the-- release your shutter button-- opening the shutter and keeping it open for exposure duration and closing it. Instead of keeping the shutter open for the entire duration, you open and close it in a carefully chosen binary sequence. So at the end, you still get just one photo. But now something magical has happened because first of all, if you look at this number one, you'll see that it's not the same as before. It has-- it seems to have these replicas. frequencies-- they're all preserved. Of course, they're attenuated. It's not as high as-- it's not 1.0.0, it's reduced. Maybe it's 0.1 or so. There is still some hope to recover this photo back from this because, in the denominator, we will not have seen. this because of the shutter. So of course, if you try to implement this mechanically, where you open the shutter and then mechanically try to close the shutter, that will be problematic. The idea is very simple. Instead of keeping the shutter open for the entire duration and getting a well-exposed photo, the shutter is open for only half of the time. The support for the representation of the Fourier domain of that function that you describe there is infinite, right? So you actually truncate this in order to-- RAMESH RASKAR: It's not infinite because you still have some width. AUDIENCE: Right, but you have infinite high frequencies there by the sharp conditions. I mean, you can think of this one goes to infinity. But there's hardly any energy left. So although it went to infinity, there is not much energy left in the process. When you get to invert the process then, that's why you're still not getting the perfect images to-- RAMESH RASKAR: In this case as well. You still lost some high frequency, right? You haven't seen the results yet for this. But it's a very controlled experiment in a laboratory. but not 0. If you invert the process-- RAMESH RASKAR: From here, this is what you get. It cuts off and corresponding to the width. The width of this post was very short than yesterday were very far away. And that's this loss of these frequencies also shows up as these artifacts at regular frequencies, at regular intervals. So, again, this one-- this doesn't go to infinity all the way. It's open for two milliseconds, open for four milliseconds. filter from space? RAMESH RASKAR: It corresponds automatically to filter in space. So your 52 vector is going to stretch or shrink based on how fast the object is moving. And you mostly have to think about image space motion because the speed in the real world and-- the distance are they get-- you divide to normalize by the distance. If you're in a dark room, you can just strobe the light, rather than opening and closing the shutter. So you only have to worry about the image space distance. mobile demo of that scene. This women try to figure out the car make and the license plate number. RAMESH RASKAR: [LAUGHS] Well, I don't know how fast you can-- AUDIENCE: Well, the problem is you can't [INAUDIBLE].. RAMESHRASKar: Yeah. So what are some-- let's look at some pictures, actually. So you get a reasonable result. But going back, what are the limitations of this method? Yes. If we did 100 milliseconds, it picks up speed, then your assumption that the 52-length vector will map to some stretched or shrunk version of 52 is not valid. Some parts will go faster and slower. It's acceleration in the measurement, but in the real world, it's still constant speed. You can either go to object space or you can come back to image space to make sure there is no acceleration. it's all linear.this one. What else? AUDIENCE: I guess there should be a little less of an acceleration of-- all of them should be moving the same-- RAMESH RASKAR: Exactly. have multiple cars, for example, and they're all independent, then it's fine. As long as it's moving in a straight line at a constant speed, you're OK. But if the two cars overlap, what happens? Our model fails again. If two cars are partially overlapping during the exposure, it's possible, but it's more challenging because you don't know exactly how fast the two car are moving. So it's just like-- AUDIENCE: OK, but that's just so you can get more light. When I take a picture, the camera automatically decides what the exposure time should be. Similarly, you should look at the speed of how things are moving maybe with an ultrasound Doppler or whatever. You need to know how much the blur is. Another major disadvantage is let's say I want to take this bottle. And if I just rotate it and motion blur that, it will not work. For any point in the front that you're looking at it, it'll work. But the point that was in the back, that all of the 52 sequence-- maybe for the first 10, it was occluded. And the remaining 42, it's seen. during that 52 window. In general, the technique works well when things are moving naturally. But if somebody wants to do this kind of an experiment, or move things behind an occluder and move out, those are very challenging scenarios. Can you combine both horizontal and vertical [INAUDIBLE] masks? RAMESH RASKAR: Vertical, horizontal is fine. You can-- it doesn't matter. It could be moving vertically. Basically, your point spread function-- the blur function will be vertical rather than horizontal. of that object moves in a straight line, OK. It doesn't matter which direction and what speed. So the problem here really is the point spread function or the blurred function is very critical. And this is what we want to study about half of the class. And the concept is very, very,Very interesting because light is linear. So eventually, it's very linear. What happens to a point happens to the rest of the object. So if I have a car that's moving, and I tell you how exactly one point of the car is behaving in the image, I can tell you automatically how the other points are behaving. All of it is going to have the same behavior. same spread image. So you can either-- for experiments, you can just put an LED on the car and see how that LED moves. And that tells you everything. There's also an impulse response. And when you're trying to find a speed of a car, [INAUDIBLE],, a very small impulse. And it answers and comes back. It does. The point spread function for your time of flight. So that's the same concept here. You just want to call leading the world, take a picture, andsee how it works. to engineer activity of the camera. So in this particular case, a point that was moving created a blur like this. And by engineering the time point spread function, it stops looking a bit like that. And then it just turns out that this one is easier to deal with than this one. So that's the basic concept, engineering or actively changing the point spreadfunction. So this is very counterintuitive because you would say, let me just build the best lens and the best exposure time. And so that kind of mimics the human eye. hope, there is some computational technique, that will allow you to go from here to here. As far as I know, animals don't have deconvolution circuitry or deep-learning circuitry. I can look at a blurry image and kind of figure out. I mean, this was a challenge for you, right? Right. So we have pretty sophisticated eyes, but we're still not able to deep learn what this is. If you have some prior knowledge of how the Volkswagen logo looks like, maybe you can say, OK, maybe that was this. But on the other hand, you're immediately willing to believe that this photo is a blurred version of this photo. to recover some information. The goal of coded imaging is to come up with clever mechanisms so that we can capture light. The circuit is very, very simple. You just take the hot shoe of the flash, and it triggers. When you lose the shutter, it triggers the circuit. And then you just cycle through the code that you care about. What can we do for defocus blur that is for motion blur? What can you do fordefocus blur? We, again, want to engineer the point spread function. would you apply spatial coding? AUDIENCE: Coded aperture? RAMESH RASKAR: C coded aperture. So this is coded exposure, coded aperture-- very easy. And all you're going to do is put some kind of a code in the aperture of the lens. And this is how, actually, it started in the days of-- in scientific imaging, especially in astronomy, coded apertures are very well known. So I've been following this for a long, long time. And I thought, it must be useful for something in photography. system. It took almost two years to realize that to put this coded aperture in a camera, there are only a few places where you can put it to get good results. So out of that came this particular experiment. And that was just a graph paper. And then we said, OK, let's come back and think about this. What's going on? Why don't we get good Results? So it took nearly two years. To see the full interview, go to CNN iReport.com. This is a standard film lens, which, of course, can also be used with a digital camera. It's 100-millimeter focal length lens. When you focus with this, it works in very interesting ways. It has to deal with chromatic aberration, geometric aberrations, such as radial distortion. So it has to move all these lenses with corresponding ratios, OK? So I'll pass this around, and you'll see that there are these notches on this lens that are in a parabolic fashion. When you think about a visual camera, you make this very simplistic assumption. That is a pinhole, and there's a sensor. And when you put a lens, we assume that the center of the lens is the central projection, that this always can be assumed to go to that point. So we said, let's look at this aperture. And back then, it was still a reasonable-looking lens. And we went in our lab, and we cut open all the way. And you can start putting new apertures in this plane. and create one single center of projection for normal cameras. For professional lenses, that's not true. For normal cameras, you have the central projection. But again, conceptually assume that all the rays are going through that point because you can replace this whole thing by one single lens in a [INAUDIBLE]. So finding that plane is actually a tricky problem. In retrospect, it's very easy. If the lens makers are putting everything there, we should put a recorded aperture also in the same plane. But placing it over there, it turns out you get the same blur. a out-of-focus picture? What will happen to the LED? AUDIENCE: It's going to look like a code. RAMESH RASKAR: Why is that-- why is that happening? So let's think about [INAUDIBLE] focus. So we have our lens, right? And we have a point light. And we will put some code here. When it's in sharp focus, it doesn't really matter what the code is. Basically, you're talking about half the light, so the photo will be half a square. Ramesh Raskar: If you put the sensor all the way here, you'll see the whole code. If you start moving away, the code will shrink. When it's auto focus, we just see the code, all right? By the way, this is the same idea behind another project, which is [INAUDIBLE]. So the idea came around at the same time of how to make this happen. And eventually, when you put it here, we get another code. That's exactly what's happening here. I'm in a different mode. If I look at this picture, you will see that-- so this is a sharp photo. It's blurred with disc. And it's blurred With that function. You can already see that it seems to preserve slightly more information. But it's still-- you won't be able to with your naked eye. You'll not be able. to figure out what underlying patterns are. After the blurring, you can. do these simple tricks, where the person you're interested in is out of focus. But then you can refocus digitally. So this is the input photo and the stock photos, all right? its Fourier transform is 52 long. So there are 52 entries here, and almost all of them are the same. If I just take a square aperture, a traditional one, and take asquare transform, it will look something like this. So a Fourier transforms of 7 by 7 will have a peak in the middle. But the rest of the time, it's more distributed instead of just all being near the center. It's more like a crossword-puzzle-shaped item, should be easy. In communication theory, everything is [INAUDIBLE].. We think about carrier frequencies of radio stations in frequencies. And convolution, deconvolution-- much easier to think in frequency domain. Although all the analysis in the frequency domain, at the end, the solution is very easy-- just flutter the shutter or just put. the values will be constant. And that's the magic of a broadband code. So if we're placing a broadband. code, certainly we have an opportunity to recover all the information. a coded aperture. Extremely simple solution to achieve that. It's very similar to the [INAUDIBLE].. AUDIENCE: Half the light. Very good. RAMESH RASKAR: Are there disadvantages? Or challenges? Not really disadvantage. Remember, in the. in the photo, at a distance, take our false photo. They will all look like this. At a distance,. At adistance, takeOur false photo, they will allLook like this, or you could put hearts in it, or, like-- motion case, we had to know how much the motion is. But the size of the blur is dependent on what? AUDIENCE: Belt. RAMESH RASKAR: The belt. But not just depth-- depth from the plane of focus, right? So that's an extra parameter you would estimate somehow. Maybe you can use a rangefinder or something like that, or just a software. There are methods you can employ. That's what you would do, like, in a light field, when we did the refocusing. the depth. When it comes into sharp focus, my edges, that must be the right depth. Unfortunately, it doesn't work out in this case. The main reason is that, because it's coded aperture, no matter where you refocus, it still looks like it has very high frequencies. So that makes it challenging. Yes. And we won't go into the detail, but the main reason for that is that it's a coded aperture. So you need to find this 7-by-7 pattern or even the previous case, the 52 pattern. And you take a random sequence. I said, by the time I come tomorrow morning, I'll find a really good code. And I came back next morning. Nothing had happened. I waited all day. It was still running. And it never came out of that. So 2 the 52 is pretty challenging. But even if you use a cluster, it's still a pretty big number. And there are all these theories about how to create different codes for different applications. So you can start with some code and do a gradient descent and so on. good solutions for 2D. But for 1D, there are some really good solutions to come up with that. For 2D, for certain dimensions, they call it one more 4 or three more 4. Basically, when you divide by 4, the remainder can be 1 or 3. And there are certain sequences that are beautiful mathematical properties, of which sequences could have broadband properties and which may not. So it turns out you cannot-- there's a little bit of cheating going on here. filter to the beginning of the signal. This particular filter is actually not circular, but it's linear. So when you apply the filter here, when you start applying the filter at the end of the image, you don't go back to the front. For circular convolution, the match is very clean and beautiful and smoother course work. Or for linear convolution,. there is no good mechanism. So we came up with our own code called RAT code, R-A-T, which is after three quarters. convolution-- I mean the linear convolution is basically circular convolution with a lot of padding of 0s. Finding a code that's 1,000 long is nearly impossible. So, yeah, so it seems like can just choose a random sequence and get a similar property. But actually, it doesn't work. The chances of arandom sequence doing the right thing for you is very, very low. Instead of [INAUDIBLE].. [LAUGHTER] AUDIENCE: Are astronomy people are already using-- In astronomy, you have circular convolution because they use either two mirror tiles and one sensor or one mirror tile and two sensors. If you tile aperture, you'll get really horrible frequency response, unfortunately, because if you put two tiles, that means certain frequencies are lost. It's saying that, if I understand this right, basically, by taking the DC coefficient, you're reconstructing almost everything. But there is a non-zero value at other frequencies. But-- RAMESH RASKAR: No, no, that's very important. you could get a very good approximation. But if-- to a naive consumer, this photo-- so look at this part, OK? This photo and this photo looks almost the same, right? And remember, in this photo, many of those frequencies are lost. And in this picture, those frequencies aren't lost because all the frequencies are preserved. But that's because our eyes are not very good at thinking about what the original image could be, given either this one or the previous one. yet. There is no medium filtering or smoothing or anything. It's just pure x equals b, x equals a backslash b. What's amazing about coded imaging is that the math is elegant and beautiful and sometimes complicated, but the implementation is very easy. At the end, all I had to do is put this code or shutter it, and very easy to explain. All right, so let's move on. OK, let me finish this one. We only saw two ways of engineering the point spread function. question. RAMESH RASKAR: Yes, go ahead. AUDIENCE: What if the mask was not quite? If you have some information by the board so that you could set up approximate [INAUDIBLE].. RAMESHRASKar: So what would you have? RASkar: In case of aperture, yes. It doesn't have to be opaque or transparent. It could be a continuous value. It turns out that, for any continuous code, there is a corresponding binary code that will do an equally good job. so far. And that's because in a binary code, you get to play with the phase function. Mike? AUDIENCE: Has anyone tried to combine the coded aperture and the coded [INAUDIBLE]? RAMESH RASKAR: That's a great idea. People talk about it, but nobody has done it. It's like we are sick of it, so we don't want to do it. But I think it's worth trying. And because those are orthogonal motion blur. Can you use both at the same time and record? There are orthogonal technologies, basically. RAMESH RASKAR: Exactly. So it's amazing because motion is time, and the focus is space. They're completely Orthogonal. So you can play with it. It's very interesting. Eventually, it's going to have a 2D projection. Yeah, eventually, it'll be able to do that. It'll be very interesting to see how it'll work in 2D. It will be very different from what we've seen before. at the top-- I'm sorry, at the bottom. The bottom one goes at the top. And when you think about the cross-section of all the straws, it's kind of cylindrical, when they all come together. So no matter where you are, the image is out of focus but by the same amount. It turns out that from that, you can recover images. Like, so this is open aperture. We discussed it in the class, so I hope you remember that. saw this right in the very first class, by the way. And the benefit of that, it turns out, is that it preserves the spatial frequencies, and it has the benefit that, no matter which steps you are at, you have the same defocus blur. So the disadvantage of coded aperture was that you need to know what the depth was to be able to deblur. But now, because it's independent of depth, you can just apply the same deconvolution and get back a sharper image. smart people who invented this. It's very sad Because that part is done. So they just wanted the technology. And it's in a lot of cameras. There's another company called Tessera, which has a very similar solution. But what they do is-- this one, basically what it does is they are simply placing an addition here so that this part of the lens will focus on an image here. This one focuses here. The top of the Lens has a short focus lens. adding small matchsticks on top of the main lens-- or the way they do it is they actually put one single sheet that looks like that, an additional layer of support, a face mask. A face mask basically means you are changing the face of incoming light. That's why, as we learned about at the beginning, if you have something very far away, this slows down a little bit. This is the Syrian optic solution or the [INAUDIBLE],, which is actually bought as another company. the name. The solution is very similar. I'm sure they're fighting out in court right now. Same solution. Instead of putting this particular guy, that's just going to add some extra glass, but mostly in a minor form. It's just [INAUDIBLE] on that one. So basically the same solution but creating different focal length for different [? partners. ?] AUDIENCE: Yeah. Although you said, I mean, there's this portion there, where if you have another blur [INAudIBLE],, right? RAMESH RASKAR: Right. blur is only about 10 pixels, no matter where you [INAUDIBLE]. So maybe that was the matter. If you have a point of access, it's still going to create an image that's blurred 10 pixels. So this is, again, very counterintuitive, where you go to make the image intentionally blurred. It's just that it's blurred everywhere. And then we also saw this one very early on, where the point spread function-- typically when something goes in and out of focus, it looks like a point. of this? AUDIENCE: Does [INAUDIBLE]? [LAUGHTER] RAMESH RASKAR: She would have used it by now. If you go slightly out of focus, you get a very different point spread function. So when you're looking with a microscope, depending on what the depth of your tagged particle is, the point spreadfunction will look very different. So you can estimate the depth by looking at the orientation of those two dots. So that's very interesting. some point, they'll stay the same. RAMESH RASKAR: The xy still remains traditional microscope 1 micron, 1/2 micron. But the z-dimension is 10 nanometers. They are still working on a lot of these concepts. OK? So let's very briefly look at compressed sensing because it's something you should be familiar with. It's a very cool idea, by the way. As a scientist, I really like it. But when somebody like Technology Review or Wired Magazine says, Top 50, Top 10, of course, I wish I'm listed among them. Single-pixel camera was listed as one of the big things in 2005 by Technology Review. The idea is, instead of taking one single photo, what you're going to do is turn on a single photodetector and aim it at a set of micrometers. If I just turn on this one micrometer-- by the way, this is what's in your DLP projectors, the Texas Instruments Digital Light Processing Micrometer Displays. | just receive light from the scene for that one pixel. Group at Rice University claims it can take a million readings to get a million-pixel image. If you're on 2 megapixels, then you need to take 2 million [? pics. ?] All right? So the claim is that imagine if you go through this million pixels, you willget a million megapixel image, right? But of course, the light will be very little if you just turn on one pixel. And, again, if you take million such readings, you can recover this picture. That's the concept. you had this photo as a JPEG. In a composite, it might take up only about tens of thousands of bytes. So I can take this picture effectively with just 10,000 pixels but recreate a million-pixel image. And that's where the concept of compressive sensing or compressed imaging comes up. You want to take something that is much higher resolution but recover it in a compressed way, where it's taking the picture with a hardware and compressing the software. So how does it look mathematically? of these projections. This is the [INAUDIBLE] matrix, for those of you familiar. And these are our measurements. So we're going to say, given these measurements, I'm going to recover my original image. Now, when you think about a natural image, the claim is that if you just use DCT, some photo coefficients, then you can compress the image and represent them with very few bytes, only 10,000 bytes for a megapixel. That means that if I just put a Fourier transform here, then I can convert the coefficients into the image. can transform the image and measure in [? your ?] measurements. And there are certain cases where it is really true. You have signals that can be compressed very easily. A very classic example is in communication, where you have a huge band of frequencies, and software radio. Instead of tuning it with electromagnetics, you just capture the whole signal. And the necklace theory says, if your band is, I don't know, 100 megahertz, then you must capture it with a signal that has a bandwidth of 100 milliamps. When you do JPEG, it does frequency transform. It says, perceptually, the higher frequencies are not as important, so I'm going to represent them with fewer quantization grids. Or certain values are too small. So all this operation-- changing quantization bands, truncating, or thresholding, are all nonlinear operations. They are not linear operations. So in general, this scheme doesn't work. But you will continue to see people who come to you and say, you know, I have this magical thing I just heard or compressive image. dangers of [? compositions. So what this is achieving is basically allowing you to build a camera with a single sensor. But do we really want it just to do compressed sensing? RAMESH RASKAR: From a scientific point of view, if somebody can build this and show that you can take fewer measurements and recover the image, that's a breakthrough. How do you use it? I agree with you that, in terms of practical implementation, maybe this is the best application, maybe it's not. a very, very active field. If you think about a B1 and B2 and visual processing, there's a lot of work that has been done over the last 30 years. There's good work at CSAIL as well. But that's purely software. And maybe you're asking, can we use sensing mechanisms that are similar to our brain so that we don't need to do any software? That's the secret of success for film, of film photography, is that if somebody had given you this problem before the invention of film, that there is a scene-- and I want to give you a sensation of the same scene. Photography is a record of visual experience, which is great for humans, but not so great for computers. What computers care about are all these high-level features. That's why we're going back to the drawing board and saying, let's build cameras that are not mimicking human eye but actually extracting more information, like [? apertures ?] that we remove the flash camera, or additional information with light-field cameras or multi-spectral cameras and so on. of-- so Brett was asking, why would you want to do [? precisely? ?] When do you have to reduce the number of measurements? And I think one of the problem [INAUDIBLE].. I don't know. The debate about whether it's really better or not is photography. RAMESH RASKAR: Tomography, yeah. Tomography is a very high-dimensional signal. There are only a few places. If you think about taking a CAT scan of your body, there are only four or five types of materials. looks just like a cartoon does-- some whites clothes, some black clothes. And that's why compressive sensing works very well there. Compressive sensing allows you to take less measurements. But the problem is you need to actually have more information about the scene before you take the measurement. The measurements are done in a non-adaptive manner. So you don't have to know anything about the scenes to do this measurements. That's actually one power of compressive. But if you want it to actually succeed, you have to be able to reconstruct the scene. In software and reconstruction, I don't worry about some prior information about the scene, which is great. But in the case where you're just taking a set style of captures, that you're limiting yourself in what kind of scenes will be compatible with that capture. So for example, if I just had a scene that's all white, then just one captured would be enough. RAMESH RASKAR: No, the claim is that even if you don't, then you lose the benefit of taking less pictures. you don't know anything about the scene, you take very few measurements. Once you take its transform, some transform, it's very sparse. It can be represented in a complex place. The code for it's inside the dual photography thing. So tomography is the same. It's 4D capture for 3D representation. OK, so I'm sorry we're not taking a break. Should we take a 30-second break before we move on to two very small topics, such as quantum computing? which is how to write a paper and wishlist for photography. Which isHow to Write a Paper and Wishlist for Photography: How to Write A Paper and Write A Wishlist For The Camera. For more information on writing a paper or wishlist, go to: http://www.cnn.com/2013/01/30/photography/how-to-write-a-paper-and-wishlist-for-photography-how- to- Write-A-Paper-And-Wishlist.html.