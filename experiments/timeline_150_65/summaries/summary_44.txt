We are going to start talking about the optimization perspective in deep learning for two lectures. And here, I guess I'm going to explain what optimization landscape means. It really means the surface of the loss function, but I guess you will see. So we're going to introduce some very basic things about optimization, but the main focus is to analyze what the functions you are optimizing look like so that you can use some trivial or some standard optimization algorithm for it. So you don't need any background about optimization. need to know what gradient descent is. The bigger question we are trying to address here is that why many optimization algorithm. are designed for convex functions. But why they can still work for nonconvex functions? So why they could still work and actually, pretty well in practice for non-conveX functions in deep learning? Note that it's not like these algorithms, like gradient descent or stochastic gradient descent, can work for all functions. It's just a way to get around the problem. In machine learning, there are atypical case or outliers, whatever you call it, especially if your parameterization of your model is very complex or kind of somewhat weird. For example, if you have a very deep network, like a feedforward standard deep networks, then it's actually pretty hard to optimize. However, some of these are solved by changing the architecture, which changes the optimization landscape. Anyway, Anyway, Some of These Are Solved by Changing the Architecture, which Changes the Optimization landscape. In most of the cases, people observe that nonconvex functions in machine learning can be optimized pretty well by gradient descent. And we are trying to understand why we can optimize reasonably well. So that's the question. And maybe just before talking about more details, let's first quickly review kind of like what gradient descent is just in case. This is very quick. So suppose g theta is the loss function. And the algorithm is just something like sets 0 is some initialization. And you have something like theta t plus t. 1 is equals to theta t minus eta times the gradient of g of theta. This is gradient decent. And you can have stochastic versions of it. Many of you probably know them. And I'm going to list a few facts just to kind of motivate the discussions here. So maybe let's call it observation. The first observation is that so GD cannot always find local mean or global minimum, right? This is for continuous functions,. This is kind of obvious. Because depending on where you initialize, depending on how the function look like. And gradient descent will go rightward. finding global minimum of general functions, general nonconvex functions, is NP-hard. This is just really saying that it's computationally intractable to find global minimum. So clearly, you cannot hope that gradient descent work, you know, in the worst case for all possible nonconcex functions. So observation two, actually, this is a theorem. But just to clarify what does that mean, that means that there there is no such thing as a "global minimum" Exists a function that you cannot solve. But it doesn't really mean that there is no subset of functions that you can easily solve, right? So for example, a convex subset of function can be solved in polynomial time. So gradient descent can solve convex functions, as I said. And I guess the observation four is that objectives in deep learning is nonconvex. This is probably not entirely trivial. It's almost trivial, but not altogether trivial. But I'm just saying, for most of the cases inDeep Learning, for example. stochastic descent or gradient descent seems to work pretty well. And why they are finding the global minimum? So you know that because you know the loss function is nonactive, right? So suppose you run ImageNet or you do some kind of vision experiments. And you can see how small the loss SGD or GD can get you. And often, the lossfunction is pretty small, something like y minus 2 or something like that. Or depending on whether you use regularization sometimes it could be y minus 4, y minus 5 depending on the situation. or the intractability of optimizing nonconvex functions. So the way that can reconcile this is really just that the lower bound, the impossibility results, is about worst case functions. But actually, we didn't identify all the functions that we can solve. There are actually more functions than convex functions that gradient descent or some other algorithms can solve, and that's a slightly larger family in between. And today, we are going to talk about these kind of functions. And two, we're going to prove that, as for some special-- some of the loss function, in machine learning problems belongs to this set we just identified. The results, they're actually, in some sense, kind of intuitive. But they do require a lot of backgrounds to talk about details, right? So that's why we don't focus on that. We mostly focus on the second part, which is more about the statistical properties of those functions used in machine learning. The first bullet is to show why I SGD can solve this set of functions. But I guess I would tell you what people can show in the first bullet, along the line of first bullet. simple.simple.com: So basically, you say that-- so you know that gradient descent can find local minimum. This is somewhat easy to believe, but actually, there are some caveats about it. We need to characterize when you show that the functions we are actually using in machine learning are solvable by GD and SGD. We are going to identify the set of functions that we aregoing to identify to besolvable byGD andSGD is just a set of Functions with the property that all local minimum are also global minimum. have this property, right? Of course, not all problems have this property. We're going to show some very, actually, simple cases where we can prove this. But I guess, as I mentioned, there is some caveat about whether you can even converge to a local minimum. So this is actually somewhat nuanced. So I'm going to formalize this converging to aLocal Minimum. But, of course, I'm not going to prove any of the theorems here. So the next part is the convergence to local Minimum. this in calculus class, right? So x is a local min of the function f if there exists an open neighborhood-- let's call it n-- around x such that, in this neighborhood n, the function value is at least fx. So that's the definition of local min. If x is the local min, it means that the gradient of fx is 0. The gradient square of the Hessian of f is PSD. So these are necessary conditions for being a local minimum, but not vice versa. that makes it tricky because then the higher order gradients start to matter. And when you look at this-- and once it becomes about the third order derivative or fourth order derivative, things becomes much more complicated. So that's why local minimum is not only always a property of the first and second order derivative. And there's a theorem, which is the following. So the theorem is that verifying if x is a local minimum without any assumption of the local minimum of f is actually NP-hard. NP-hard. So I've told you that finding a global minimum is NP-hard, but actually, finding a local minimum is also NP- hard. So we have to consider these kind of pathological cases, which makes things harder, right? So how do we proceed? So the way to go beyond it is that there is a way to also remove some of the pathological cases as well so that you can find a local Minimum in polynomial time. And then we can. execute our prime. So this is what will happen. So here is a condition called strict set of conditions. And if you certify the condition, then you remove those pathological cases which requires high order derivatives. So-- sorry. Strict-saddle condition-- so in some sense, I guess I'm not sure whether this makes sense before I define it. But generally, you are basically saying that you want to rule out this kind of somewhat subtle possible candidate of local minimum. So there's no set of cases in your function. I think I wrote a book chapter about this kind of optimization thing for our book. So I can send that to the person who take the Scribe notes. And that probably help you to have some references. But the materials are not exactly the same as the book, so you still have to do the Scribes kind of from scratch in some sense. OK, cool. So the definition of strict-saddle, I'm citing this paper. The very, very original paper that introduced this term and this notion is by Rong Ge, et al. is the definition. So we say f is alpha, beta, gamma strict-saddle if, for every x in RD satisfies one of the following. So the first one is that, for some of the x, it just satisfies that fx, the true norm of x is larger than alpha. And the second thing is that the Lambda min of the Hessian at x is less than minus beta. So if you satisfy this, you cannot be a local minimum because your Hessian is not positive semidefinite. Theory: If you are just given an arbitrary function, differentiable functions, you should then be able to check whether it satisfies strict-saddle, right? It should be as hard as finding a local minimum in some sense. Theorem is somewhat kind of like informal just because I'm not-- it's pretty formal in the sense that all the bounds are correct. Then many optimizers, for example, GD, SGD, if you use the theorem, can be used to solve the problem. written word correctly, and many other articles, like cubic regularization, I guess many algorithms can do this. So far as can converge to a local min with epsilon error in Euclidean distance in time poly d-- d is dimension-- 1 over alpha, 1 over beta, 1over gamma, and 1 over epsilon. So this theorem is very coarse-grained. Of course, different optimizers have different convergence rate. But at least for the purpose of this course and this lecture, we are not interested in which one is faster. We are mostly just interested in whether it's polynomial time versus exponential time. Strict-saddle is just because the pathological case is a saddle point, right? So when you have these kind of cases where the gradient is 0 and the Hessian is PSD, but not strictly positive semidefinite-- so you have some direction where you can potentially have a negative curvature. This is called cubic regularization. Cubic regularization is a type of strict-s saddle. It's a special kind of strict saddle point. So in some sense, this explains the name for this. one of the early work in 2006 by Nesterov. But there are many other optimizers. Many other people published papers on this. I think I can add more references in the Scribe notes, in the final Scribes notes, to cite some of the recent works. So basically, the next theorem is trying to say that-- are global and you have the strict-saddle set of condition, then this means that optimizers can converge to global min. I guess I'm writing it a slightly different way. Just in some sense, I unpack it a little bit. thought that this either provides a slightly different way of thinking about this, or it's just more explicit. So basically, you say that you assume the strict-saddle condition, but let's rephrase all local minimum global strict-Saddle condition like this. So you say there exist epsilon 0, and tau 0, such that, if x in RD satisfies, the gradient is small at the epsilon and the Hessian is larger than minus tau0. somewhat big, almost kind of larger than 0. So then, actually, it's close to a global minimum of the function f, right? So this condition is just a slight different way to say that you have all local minimum global and strict-saddle together, all right? And then I know this condition. Then optimizers-- again, the same set of optimizers which can converge to local minimum. All right, so it's not a big deal. Many optimizers can convergence to aglobal min of f up to, say, delta-error and Euclidean distance in time poly 1 over delta, 1 over tau 0, and d. exactly the thing that we did for the strict-saddle. But if you think about it, it's basically the same statement. OK. Anyway-- so cool. So we are basically done with the first part, so about identifying the subset of functions that are easy to optimize. But these are all local minimum, global minimum functions. And next, we are going to show some examples where these kind of properties can be proved rigorously for machine learning situations, but these examples are pretty simple. They are not deep learning. do linearized network, there is a little bit more things to do beyond that. And the second example I'm going to give is matrix completion. This is an important machine learning question by itself as well, right? So before deep learning, this was one of the most important topic maybe in machine learning. And now, still I think it's used in the recommendation system. So we're going to talk about that. OK, cool. So any questions so far? I guess let's talk about PCA first. The best rank one approximation is basically the Eigendecomposition or the singular value decomposition of the matrix here. Just for simplicity, let's also assume this matrix M is symmetric. And this becomes a nonconvex objective function because you have a quadratic term here. And our goal is to show that, even though it's non Convex, all local minimum of this g are global minimum under the assumptions that we have mentioned, so like rank one, PSD. so one dimension. So d is 1. Then you just have a scalar, m minus x squared squared. This is our function, g of x. And you plot this function. This function looks like this. And there are two local minimum. And they are both global minimum because there is some symmetry here. And if you have a higher dimension, it becomes a little bit more complicated. But generally, you have some kind of rotational kind of symmetry here to make this happen. You first find out all stationary point, the first order stationary points. And then you find all local minimum, and you prove that they are all global minimum. So basically, it's just more or less like we solve all of these equations and see what are the possible local minimum you can have, right? So gradient of x is 0. And what is the gradient of g of x? I'm not going to give a detailed calculation here. But believe me, this is equal to minus this times x. So this means m times x is equal to 2 norm fx squared times x, right? Because the three things together, the last two things, becomes the 2 norm of x squared. And that's a scalar. And this is a matrix vector application. So basically, this is saying that x is an eigenvector. So x is eigen vector, and x squared corresponds to eigenvalue. So the eigen Vector doesn't have a scale. So you first find out the unit eigenVector. me just specify all this. So this part just follows some intuition. So suppose eigenvalues are distinct even though we don't have to assume this. Then, basically, all the stationary points are of the form that x is equal to plus minus square root Lambda i times the eigenvectors. And now, let's look at which of these is a local minimum. And then we say OK, all of these are global, right? So ideally, we just want to say that only vi, the v1 thing, is the local. minimum because that one is also a global minimum. So how do we do this? And also, we don't necessarily want to assume all the eigenvalues are distinct. So there's the small thing to be done regarding that as well. So let's compute Hessian, right? So we need to use the Hessian. So here, it's actually not that hard. The Hessian is in dimension d by d because you have d parameters. Sometimes your parameters is a matrix, and the Hessians becomes a fourth. order tensor. And it's kind of very complex to be even just written down to just write down the Hessian. So here is a kind of a very useful trick and which actually also has some fundamental reasons that this is useful. So the useful thing is that, if you look at the quadratic form regarding the. Hessian and youLook at v transpose Hessian v or v in the part that was Hessian times. v, this is the quadRatic form related to Hessian, and this is much easier to compute. methodology also applies here when you talk about the Hessian. You just iteratively expand it, Taylor expand it into something like g of x plus some epsilon times some vector. And then if you have this, then this basically corresponds to v dot g square gxv. So if you apply these kind of techniques, you can get the. Hessian like this. So the quadratic form of the Hessia is equals to something like this, right? So we have to have this. And we know that the Hessians that are larger than 0 is equivalent to that. In many cases, you only care about a few special v's because some of v's are much more informative than the others. So you want to choose some informative v's to evaluate this formula so that you get some important information about what x can be. So it turns out that the v's that are informative here is the top eigen vector. By the standard results in PCA, you know that the best one-to-one approximation is the best approximation to the global minimum. top one eigenvalue-- eigenvector with the right scaling. So the second case is that x has eigen value, let's say, lambda, which is strictly less than lambda 1. And then because x is an eigen vector, also the eigen Value of x is orthogonal to the eigenevalue. So you know that x is Orthogonal because different eigenvectors with same eigenvalues will be orthogonally. There is no guarantee that two eigenivectors are always orthogona because they could have the same Eigenvalue. So 2 means that the first term goes away. So you get just x2 norm square is bigger than v1 transpose in Mvi. And we have a contradiction because this is contradictory with the assumption that lambda is less than lambda 1. So write that. OK, any questions about this? So, guys, maybe just a very quick summary-- so basically, this is saying that, if x is stationary-- by stationary point, it always means first order stationary point. So I'm not going to clarify that in the future. point and is x is not global min then moving in v1 direction. Because you have stationary point, that means your point is flat. So changing in v 1 direction wouldn't change it by a lot. It would lead to a second order improvement. And that's why it's not a local minimum. Because if you are local minimum, moving inv1 direction shouldn't give you any second order improvements either. So that's basically the gist of the analysis. All right-- so cool. so now, let's talk about matrix completion, which is kind of like an upgraded version of PCA. The setup is the following. We are given random entries of M. So we pick some random indices of M, and you review the corresponding entries. That's the only thing you know about M. And then the goal is to recover the rest of the entries, right? More formally, so you say that there is omega, which is a subset of the. entries, subset of indices of d times. And so in other words, you can assume M equals something like zz transpose. And z is in dimension d. P omega of A is the matrix obtained by zeroing out every entry outside omega. And everything that is not in omega, you'll make those entries 0. So we observe P omega of M. And our goal is to recover M.d. And why people care about this question a lot in the past, one reason is that it has this relationship. D.C. scientist: We need to find a way to solve the problem of how do we find the answer to the question "How do we solve the mystery of M?" with a recommendation system. So suppose you think of we have a matrix. And in one side, the columns are indexed by the users. And every user probably have an opinion about every item, right? Either they like it or not, so and so forth. But it's not like every user buys every item. So every user only buys a very small subset of the item. And that's why you have to recover all the rest of the entries to serve the users better in the future. matrix M, there is no way you can recover the other entries because they can be arbitrary. So that's why you have to assume that the matrix M has some low rank structure or some other structures. So maybe just to give you a quick kind of sense about how does this structure matters here-- so if you count the number of parameters, we have d parameters, right, to describe a rank one matrix of dimension d by d because you can just write it as xx transpose. it's unlikely it can work. So basically, that is saying that p is bigger than roughly 1 over d. And speaking of the objective functions, this is actually a pretty commonly used method in practice. So you just say I'm going to minimize this function that's called fx, which is defined to be that basically you have a parameterization called xx transpose. This is your target. And you want to say this matrix actually faced all my observations, right? So you are taking a sum over all possible observations. the only cases you know what the entries are. You know this Mij, and you minus this with xi times xj. So this is our prediction. This is our observation. And you take the square and take the sum over all the observed entries. And just to follow future notational easiness, actually you can write this as P omega of M minus xx transpose, right? Because this is the matrix. You're looking at error matrix, right, and then you zero out all of those that you don't know. convex transition methods and so and so forth. However, those methods actually often have stronger guarantees. For example, they have tighter sample complexity bounds. In practice, just because the convex transition takes too long time, people actually are using objective functions or methods like this. And that's why it's also practically relevant to analyze these kind of objective functions because they are, indeed, used in practice. All right, so our main goal is to prove that this objective function has no local minimum, all local minimum are global. this assumption. It may not sound very intuitive. I wouldn't spend too much time on it, but just let me mention it. So this is called incoherence assumption. And this assumption is necessary. People know it. I guess we assume, for example, we assume the ground truth has norm 1. This is with all this generality, just which is for convenience fix of scale. And then after you fix the scale, you assume that the groundtruth vector z-- so we call that [INAUDIBLE] zz transpose. the reason why you don't want that is because, for example, a counterexample is that, if z is just e1, then your M is just  e1e1 transpose. So basically, all bets are off. You have to see enough entries. So this incoherence condition is, in some sense, trying to rule out these kind of pathological cases. But I'm not going to talk too much about it. It's just for the formality. the theorem is that suppose p is something like poly mu and log d over d epsilon. Recall that we are in a regime that p is roughly 1 over d. And this is a poly factor in mu and also poly log in d, OK? And then we assume the incoherence. And then our local of f are when we are-- so actually, you can prove that they are all exactly global minimum. But for the moment, we only prove that the error will be exactly 0. also have strict-saddle conditions. You can also prove that. It's just that I didn't include it just for the sake of simplicity. And you do have to prove that to have the rigorous result. And if you don't prove it, you just prove that all local minimum are global. Sometimes you may get somewhat misleading results. So I think there is a paper that shows that actually, in somewhat weird cases, you can show very strong looking results. The reasons why they are so strong is because somehow, in that setting, you ignore the strict-Saddle, which is problematic. The answer is no especially if you look for a global property, like globally, all local minimum are global. I don't think we have any proofs for any real neural network models. I guess there is a proof for linearized network models, like all the activations are linear. If you have more than two layers, you don't have strict-saddle conditions. You have a lot of [INAUDIBLE] points. Otherwise, I think we are good today.take some questions if anybody has any questions. assume that the input are linearly separable, then there is a proof for this. And there are a bunch of other cases where you can have some partial results. Next week, in the next lecture, maybe the second half of next lecture,. I'm also going to give another result, which is somewhat more general. It applies to many different architectures, but it has other kind of constraints. First of all, it doesn't really show exactly these kind of landscape properties. It shows that these kinds of properties holds for a region, for a special region in the parameter space.