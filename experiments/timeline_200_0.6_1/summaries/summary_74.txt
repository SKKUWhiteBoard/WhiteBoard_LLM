This lesson will first dive into some signal Theory and then move on into things that we're more familiar with things like deconvolutions and using Transformers for next note prediction. The first thing we want to talk about is how can we sample and quantize a continuous time signal. We'll then go into some geometric signal theory and with Transformers and finally how how we can kind of generate sounds using these. The lesson will end with a soft introduction to digital signals and then we'll move on to the next lesson. not very easy for a computer to do given that every operation needs to be in on a continuous time signal. The way that we we can fix that is through the process of discretization or to to make a an analog signal a digital signal for us to be able to process. The two kind of main ways that we can make our signal easier to process is one by taking samples at certain time periods and two by quantizing our level so instead of dealing with A continuous scale we can quantize at certain levels for example a frequency of like 2 4 6 8 Hertz. signal to an analog output we can kind of start by talking about the ADC circuit. This uses something called the SARS ADC algorithm essentially what this is is a binary search to figure out what is my best digital approximation of my analog signal right so I have a continuous signal pass it through my sample and hold circuit. From here we take in our our input and our output is based on the amount of bits of precision that we want to have so depending on whether we want a two-bit approximation a three bit approximation. here we use a low pass filter which is also covered in courses like 16b um the motivation behind this is that we we maintain a signal that's the pass band. We allow every signal to pass when we hit certain cutoff frequency we Wane our signal by a certain factor and that factor dictates the slope of this line. Our signal will continuously Wane after that cutoff frequency is hit on the right is kind of the picture of the approximation where we can approximate a different quantization levels different bit parodies based on the Stars ADC algorithm. voice and Pitch it up very fast right what quantization level do we do we want there. Can we do a lossy pitch up with a uh with by filling in the the blanks in some intelligent way through prediction or kind of note fitting which is an interesting consideration I think given the fact that audio is a continuous time signal um the digitization process and the choices you make matter a lot and because of that this field is so interesting and there's a lot of really didactic work around how we can take these continuous signals. with a whole bunch of things in digital signal processing. If the sampling frequency is less than double of the highest frequency present aliasing will happen. This asserts that you need at least two samples per period. The aliasing phenomenon is incredibly interesting this happens both visually and um auditorily um and aliasing is an entire um topic just based on itself. As you you can see here your approximation gets very different as your your sampling rate increases and decreases if your assembling rate is 0.3 Hertz. also increases we're at one Hertz you're almost perfectly fitting the polynomial on the the points um something interesting to consider is that at a sampling rate of of 0.3 you are fitting a polynictional right because you're you're creating apolynomial through the points that you have. However this polynomal is very not indicative of the actual signal that you're trying to process right while we we can see that this this does kind of capture the trends of our data and that it falls when our data has fallen.  aliasing is the byproduct of poor sampling. A lower wave resolution will result in a modified output signal as compared to the original input that we're trying to process. frequencies that are higher than one half of the sampling rate will be automatically transformed to lower frequency sees that's where information loss stems from. There's a lot of literature about um aliasing effects um including spatial illnessing right um you see that uh there's there's a lots of different ways that aliasing takes place. we we take certain points along this you can see that the image changes we get kind of the gist of this um but this is a a compressed image it contains similar information it allows us to kind of see what's happening but it contains this in a very compressed format super sampling is where um you know this this image is uh take in this this 4x4 uh image and we're kind of running a kind of stride over top where uh we're losing some information in the background as you cansee it really Blends together but in the foreground. um this is is kind of uh and and add add-on um to this presentation it uh doesn't uh really contribute exactly to what we're talking about but I thought this was incredibly cool. With one line in C you're able to to generate Melodies um which is incredibly cool um yeah so please check it out if you if you guys have a chance. The next thing we're going to talk about briefly is geometric signal Theory and how that can tie in to reconstructing signals. A vector can be reconstructed with a linear combination from its projections onto another set of vectors if and only if the set used is a basis. A basis is a Subspace that's covered by all linear combinations of vectors. Given that a basis is required the vectors used must be orthogonal to one another and this is very important as it ensures that that we're maximizing the amount of information gained right by having two vectors that are orthogona to one other. We can say s 0 to SN is called the span which is a representation of all the vectors that can contribute to a a certain Vector combination. The idea behind the the last two sections here was to give you motivation for for how signals work and how kind of classical reconstruction can occur using math that we're all familiar with. This really covers any Vector in R2 um e0 uh we can Define as one zero so a horizontal Vector E1 is zero one a vertical Vector if we look at where we we're trying to project some Vector X onto uh the the e0 space right we can kind of see how the math works out here. familiar with perhaps in terms of how we can use those to reconstruct signals and ultimately how we could use them to generate audio right predict the best uh kind of next node um so looking at the next the next step here we want to use deep learning for reconstruction right where we are are reconstructing a low quality audio to high resolution audio right um and this is this is the kind of uh model framework um that we can used for this um you might notice it really closely resembles a unit which is something that we talked about during image segmentation. to uh kind of increase uh the uh the resolution of a certain low quality image form so as you can see if this is our initial wave our our final wave is is much more populated right we're able to gain information through this unit process. At each layer the number of filter Banks is doubled so that while the the mention along the waveform is is halved the filter Bank Dimension is increased by two. We're reducing the size of our uh of our image but we're increasing the dimension the dimensionality of it. we're developing a representation of these these audio signals so as we pass through the bottleneck layer which is constructed identically to a down sampling block right these connect eight up sampling blocks which have residual connections to the down sampling blocks. What this allows you to do is it allows you. to preserve features and share features that are learned from the low resolution representation of the image into our higher resolution output. The final convolutional layer um has a restacking operation and it does the reordering operation following our our sub pixel deconvolution. after this restacking step um and the the loss function used throughout this process specifically was was kind of a mean squared error loss function so by playing around with different kinds of of loss functions you might be able to yield better performance. These kinds of techniques are used in a variety of ways to reproduce music from bands um in the 1900s um the late 1900s mid 1900s whose recordings may not have been preserved in full quality um so by you know through the remastering process they're able to to kind of do things like this. waveform and the reconstructed waveform. We want to convert our data into usable tokens which in the music realm does provide a different issue than it did for our other two representations. The first step takes the form of converting data which is music files into a token sequence which is individual notes. We can tokenize this into a series of tokens that all correspond to our vocabulary which is very very easy to map by a dictionary to certain Keys. We're adding color to our our downsampled waveform to to ultimately get something that hopefully resembles our true waveform better. intuitive what we can do for uh for music is We can approximate this which is a series of notes in a piano roll right or a graph that has our offsets and our pitches right so our pitches span you know a certain uh pitch set. We want our our information to be captured in multiple Dimensions right we want the the pitch of the note as well as the length of thenote for example these two notes at the bottom right they correspond to to e and A2. We're trying to form multiple notes can be played at a single point in time. n62 which is this actual note representation on piano. A single song can be transformed into 12 songs of different Keys which can help increase our sample of training data and generalize key scales and beats throughout a data set. Data augmentation provides an amazing data sub multiplier to to Simply get more data. The more data you have the more data that can be used to improve training data. We're able to capture information in a a very sequential and very structured way so putting this all together you can get your initial translation right where you have a um our our initial translation. the better your model will be and the more generalizability you have in your Transformer the better it'll perform right we talked about Occam's razor. A generalized Transformer a generalized solution can fit the Goldilocks of what we want in a model. It's easier for machines to predict keys without flats and Sharps um which has you know similar to what humans do it's easier to focus on the regular keys on piano um so this specific example was trained on on those um however the glass andSharps with specific augmentations and specific training processes can also be added to our vocabulary. example um this specifically I believe this uses like a music 21 framework but it's able to you know tokenize a certain item and then you're able to transpose this to a certain amount of notes or a certain key. Just by transposing you'reable to increase your your training sample size which is very cool and can improve model performance by a ton. The next thing to consider is positional beat encoding right we want to include some metadata to feed into our model to give it a better sense of musical timing because the position of the token in our our tokenized representation it doesn't correspond to its actual position in time. The attention mask is a way to keep the model from peaking and essentially leaking information at the next token it's supposed to predict. By applying an another mask a window of size 2 you're essentially enforcing a window size of two where you're only updating the information you see every two time steps right at the very end you can't see the final two bits of information and that lack of information is very important to a Transformer as it allows you to predict several steps ahead and will ideally produce a more generalized model. hidden state memory Transformer memory specifically to this model enables very fast inference for music prediction right we've done a lot of things to optimize for for our prediction we're including a beat embedding so that's not something it has to learn. We're able to get a sense of of relative position with Transformer XL whereas vanilla Transformers will use Absolution absolute position only. It's important for music models to know the position of each token relative to one another because positionality matters right the order that you're playing the notes really is is what matters the most. our positional beat encoding which we're including for the the model to have um so I'd kind of like to end with a little demo generated by by somebody who who use this model to kind of predict the end of Canon in in D Major by by Pachelbel so here'sPachelbel's Canon I'm kind of in the spirit of Christmas coming up as well um yeah so as you can see there um this is the original pocket balls Canon and this is what's predicted. So yeah there's a a lot to do in this field um a lot of really cool things happening um. things a try yourself uh yeah thank you guys for tuning in have a good one. Things you might want to try yourself. Things that you may want to give a try. uh yeah. things you might be interested in trying yourself. things that you might like to give yourself. uhYeah. things a try yourselves. uh Yeah. Things a try themselves. Things your might like yourself.things you may like to do yourself. thanks you guys. for tuning into this week's episode of The Daily Discussion.