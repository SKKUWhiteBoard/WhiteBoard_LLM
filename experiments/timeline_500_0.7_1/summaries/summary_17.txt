Machine learning is about how to acquire a model from data and experience. In the next few lectures, we're going to work through a sequence of different takes on machine learning that are going to highlight different subsets of the big ideas on this topic. We'll start with model-based classification, and, as an example of that, we'll work through some details of how the Naive Bayes models work. And we'll have a couple running examples, such as that spam classifier that pulls out all the emails you don't want from your email. In the real world, getting the right kind of data is often one of the hardest parts of building and deploying a machine learning system. We'll see today exactly how data kind of goes through the mill and gets turned into a model. Also see how something like spam classification starts to give you a little bit of a window into how other natural language tasks work. And we'll see more in-depth examples and more structured examples of these kinds of problems later on when we talk about applications. is going to be unique. It's going to have to be at least one pixel off of something else you've seen. There's a lot of inputs that are really noisy, and you're training set, they might be hard, expensive to label, because they're noisy. And then at test time, you're going to make mistakes because machine learning is not perfect. We can look at other kinds of patterns. We could look at how many connected components of ink there are. What's the aspect ratio? How many loops are there? machine learning methods get better at doing that. We'll talk about that in a couple weeks when we talk about narrow nets. There's tons of classification tasks. It's probably the most widely-used application of machine learning. In model-based classification, rather than directly learning from errors that you make in the world from experience, instead we're going to learn by building a model from our data, and then doing inference in that model to make predictions. After today we'll look at the model-free methods. The Naive Bayes assumption is an extremely radical assumption as far as probabilistic models go, but for classification, it turns out to be really effective. The model itself might look something like this, where the class is the cause, and it independently causes each of those features. And that means when you go to make a prediction, it decomposes into a product of a bunch of different feature conditional probabilities, and we'll see examples of that and unpack the inference for that. In a Naive Bayes model, each class 1 to 0 is equally likely. In a real collection of data, 10% of the examples are 1 and 10% are 2. In addition to the prior probability of each label, we can compute things like, what is the probability that pixel 3 comma 1 is on, given each class? This isn't a distribution over on or off. These are just the probabilities-- what I'm showing here-- just the probability of that pixel for each class. And it's going to be some number. The standard model for text is to say that the features are the words and that the random variables are the word at each position. This is called a bag of words model. In the standard case, each feature gets its own distribution. Here, we assume the features. are all identically distributed, but there are multiple copies of that feature. for the different positions. And this is nice, because when the document is longer or shorter, you don't really need to change your model. Or, it is the first time. In this example, we're going to compute the probability of spam along with all of the words. And we're also going to multiply that by the number of words in the corpus. And in spam, the most likely word is the. Somewhere far down that list is the word free. What do you think is the most frequent word in spam? I heard free. I heard the. What else? Any other guesses? You laugh, but here's the answer. What's going on here? I thought free meant spam. All right, so where do these tables come from? be weighed, and that's what's going on here in the conditional model. If you started, if you did prediction in this, and you started with the class and you cranked out a pretend document, it would look significantly more like a real email than if you just did a bag of words. However, will it be more accurate for classification? It really depends. In general, it will be a little more accurate, but at a cost of having significantly more complicated conditional probabilities to estimate. We are now into the machine learning section which means we are done with the spooky Halloween time ghost-busting section. But I have candy. During the break, everybody get up and come grab candy if you would like. This gets applause? All right. Come up and grab some, please. I'm not allowed to take it home. [NO SPEECH] All right, we're going to get started again. Let's talk about training and testing. The principle of machine learning is something called empirical risk minimization. But really, the reason we're doing this is not just for Naive Bayes, or probabilistic model estimation in general. The main worry is that you over-fit. You can have plenty of training data, but you can sort of learn in a way that fails to generalize. And then, you can have tons of data drawn from the wrong distribution. How do we limit that? Mechanically, we limit the complexity of your hypothesis. We penalize sort of overly-specific models in various kinds of ways, and we'll see some examples of that even today. How can you do badly on the final exam? There's just no way to have sort of seen the whole space. In practice, there's usually other little shards of the data that you're going to want to have. So, for example, one common one is held-out data. We'll see today and in future lectures what that's for. You don't want to test your classifiers on theData that was used to train them. It's like if you go back through, and you try those same practice exams, you're like, wow I really know this stuff now. But it's like, no. This was your training data. you're training data. The question is, do you generalize? This can happen to your classifier too, so you always want to test your performance on data that was not used to train it. You need to have some metric, and there's a lot of possible metrics. An easy one is accuracy. For how many of these emails did I make the correct decision? Fraction of instances predicted correctly, but actually, that's actually not a great metric for spam detection. Any ideas why? What's wrong with accuracy? Spam detection is, in some ways, a very poor example of a canonical classification problem. The problem here is not that you're test accuracy is low, but your training accuracy was also low because you didn't learn anything. Spam is being generated by people who are trying to defeat spam filters. And so in that sense, over time, spam classification doesn't actually look like a standard classification problem because it's adversarial. OK. Any questions before we talk about generalization and over-fitting. OK, so in these images, you want to fit the hat right. the thing we're trying to do is to fit a curve to this data. So you say, what is the fit? Is the fit getting as close as possible to the last dot? So what is my constant approximation to this? Does anybody want to hazard a guess? Let's call it five. OK, did I fit something about this data? Yeah, I felt something about the data. I fit basically it's mean. Did I capture the major trends? No. All right, let's try again. Let's fit a linear function. It's close, right? It's a better fit than the constant function. Notice that when I went to linear function, the space of hypotheses grew. Instead of just lines, now it's like lines with slopes and intercepts. In Naive Bayes probabilistic models, over-fitting usually shows up as zeros in your probability table. In other methods, it's actually going to show up in totally other ways. So let's figure out some ways to do that, to just illustrate what it would like to look like. We could take that polynomial and limit the degree of the polynomials. Using it under a hypothesis, you can also shrink the hypothesis space, so you can fit less under it. We already know one kind of over-fit to limit that. The maximum likelihood estimate, or relative frequency estimate, is used in machine learning. It says, OK, the probabilities are just the counts in the training data. For each probability I assign to red, and one minus that goes to blue, I can compute the probability of D. This is something you could try writing out for yourself. Of all of those probabilities, the one that matches the frequency of the data is the one. that maximizes the probability. of theData. But in practice, you need some smoothing. CS281A is an open-source computer program. It can be used to test computer models. CS281A uses a Bayes rule to find the parameters which maximize the product of this, which is what we were doing before. But there's this extra term, p of theta, which says, if I want to know what parameter or what probability is most likely, I need to weigh the likelihood of the data against how likely I think that parameter is in the first place. This is actually, due to Laplace, hundreds of years ago now, who's a philosopher who kind of worried about things. In a real classification problem, you have to smooth if you're going to use Naive Bayes. Instead of computing odds ratios on the maximum likelihood, I can instead do some smoothing and see after that smoothing, what has the biggest odds ratio? And suddenly things that only occurred once, they don't percolate to the top, because they haven't occurred enough to overwhelm that flat prior that I'm associating them with. So this is the top of the odds ratios for ham on the left, and favoring spam on the right. Some of these maybe make sense. Like, there it is. probably in there somewhere. If you see money, that's a good sign that it's spam. Or capital order, or credit, presumably credit card, I don't know. There are some things that indicate ham. This looks like general English text. What is going on there? Helvetica vs. Verdana. This reflects the default fonts that were in use at this time across different platforms. And so one of the things you find in machine learning is, you know what you think the features are going to be. But you might be wrong. In general, you're going to need more features. In spam classification, we found out that it wasn't enough to just look at words, you've got to look at other sort of metadata from the ecosystem. For digit recognition, you do sort of more advanced things than just looking at pixels, where you look at things like edges and loops and things like that. You can add these as sources of information by just adding variables into your Naive Bayes model, but we'll also talk in the next few classes about ways to add these more flexibly.