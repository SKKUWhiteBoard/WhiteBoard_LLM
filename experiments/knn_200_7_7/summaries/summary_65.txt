Peter Solovits was invited to speak at a meeting of the National Academy of Science, Engineering, and Medicine. He was on a panel that discussed AI and decision making, privacy and informed consent in an era of big data, science curricula for law schools, emerging issues, and science, technology, and law. SolovITS: One of the things that I was very surprised by is somebody raised the question of shouldn't Tatel hire people like you guys to be clerks in his court? interesting to me. He said, no, he wouldn't want people like that, which kind of shocked me. And so we quizzed him a little bit on why, and he said, well, because he views the role of the judge not to be an expert but to be a judge. So by formalizing it, you might win. However, conversely, the use of technology to determine whose liberty is deprived and on what terms raises significant concerns about transparency and interpretability. example, if you are two identical people but one of you happens to be white, the chances of you getting bail are much lower if you're black. If historically, judges have been less likely to grant bail to an African-American than to a Caucasian-American, then the algorithm will learn that that's the right thing to do. And then the second problem, which I consider to be really horrendous, is that the algorithms are developed privately by private companies which will not tell you what their algorithm is. Peter Zolovich: What do you mean by fairness? What characteristics would you like to have an algorithm have that judges you for some particular purpose? He says it's impossible to pin down one specific definition, but for the pre-trial success rate for example, I think having the error rates be similar across populations is a good start. Zolovic: There are three criteria that appear in the literature. One of them is the notion of independence of the scoring function from sensitive attributes. In the ICU population, men have more substance abuse problems, women have more depression. In the psychiatric population, white patients have more topics enriched for anxiety and chronic pain, whereas black, Hispanic, and Asian patients have higher topic enrichment for psychosis. In psychiatry, when you look at the models that we're building, there is a racial bias in the data that we have in building the models. These models are particularly simple in psychiatry, particularly when youLook at the huge gap in the public insurance patients with a huge difference in anxiety and depression. is that if you count O as the optimal possible model over all possible model families, and you count L as the best model that's learnable by a particular learning mechanism, then the bias is essentially O minus L. The variance is like L minus A, it's the error that's due to the particular way in which you learned things. You can estimate the significance of differences between different models by just permuting the data, randomizing, essentially, the relationships in the data. If yours lies outside the 95% confidence interval, then you have a P equal 0.05 result that this model is not random. Until 1967, it was illegal for an African-American and a white to marry each other in Virginia. If you went to get a marriage license, you were denied, and if you got married out of state and came back, you could be arrested. This happened much later. Trevor Noah, if you know him from The Daily Show, wrote a book called Born a Crime, I think, and his father is white Swiss guy and his mother is a South African black. He had to pretend to be-- his mother was his caretaker rather than his mother in order to be able to go out in public. off the 50th floor of a building that's under construction, and that's probably a reasonable defense. Now, how do you demonstrate disparate impact? Well, the court has decided that you need to be able to show about a 20% difference in order to call something disparate impact. So the question, of course, is can we change our hiring policies or whatever policies we're using to achieve the same goals, but with less of a disparity in the impact? That's the challenge. attribute given the outcome. So that says, can we build a fair scoring function that separates the outcome from the protected attribute? And usually, there are knobs in these learning algorithms, and depending on how you turn the knob, you can affect whether you're going to get a better classifier that's more discriminatory or a worse classifier. So you can do that in pre-processing. And so in this story, the optimal score is basically going to depend on whether you have a computer science degree or not. of scoring them than we do of scoring group B. So you might wind up with a situation where you wind up hiring the same number of people, the same ratio of people in both groups. Or alternatively, it could be caused by malice also. There's also a technical problem, which is it's possible that the category, the group is a perfect predictor of the outcome, in which case, of course, it's a bad idea. The outcomes are likely to be better for a group A than for group B, which means that you're developing more data for the future. they can't be separated. Now, how do you achieve independence? Well, there are a number of different techniques. One of them is to do a Gann-like method where you say, I want to train my classifier, let's say, not only to work well on getting the right answer, but to work as poorly as possible on identifying which data set my example came from. So this is the same sort of idea. It's a representation learning idea. And then you build your predictor, R, based on this representation, which is perhaps not perfectly independent of but it's exactly the right drug for you. that's another popular way of looking at this. So for example, if the scoring function is a probability, or the set of all instances assigned the score R has an R fraction of positive instances among them. If it turns out that R is not well-calibrated, you can hack it and you can make it well-Calibrated by putting it through a logistic function that will then approximate the appropriately calibrated score. These guys in the tutorial also point out that some data sets actually lead to good calibration without even trying very hard. A new study suggests that gender influences whether you're a programmer or not. It turns out that visiting Pinterest is slightly more common among women than men. And then visiting GitHub is much more commonamong programmers than among non-programmers. So what they say is, if you want an optimal predictor of whether somebody's going to get hired, it should actually take both Pinterest visits and GitHub visits into account, but because those go back to gender, which is an unusable attribute, they don't like this model. the database, but the type of insurance you have correlates pretty well with whether you're rich or poor. So we did that, and then we looked at the notes. We wanted to see not the coded data, but whether the things that nurses and doctors said about you as you were in the hospital were predictive of readmission, of 30-day readmission. So these are some of the topics. And so we said, what happens when you look at the different topics, how often does it happen? The eICU data set we've mentioned, it's a larger, but less detailed data set, also of intensive care patients. And there, we see, again, a separation of mechanical ventilation duration roughly comparable to what we saw in the MIMIC data set. On the other hand, if you look at the use of vasopressors, blacks versus whites, at the P equal 0.12 level, you say, well, there's a little bit of evidence, but not strong enough to reach any conclusions. standard or remote supervision notion of a larger population that has a tendency to be mistrustful according to our model. And so if you look at chart events in MIMIC, for example, you discover that associated with those cases of obvious mistrust are features like the person was in restraints. If a person is in pain, that correlated with these mistrust measures as well. And conversely, if you saw that somebody had their hair washed or that there was a discussion of their status and comfort, then they were probably less likely to betrustful of the system.