All right welcome to the third lecture on Foundation mulative AI. So today we're going to cover chat GPT um and um right. I mean I think for a lot of people chat GP was the the tool or the the AI that really made people understand this is different. All rightWelcome to theThird lecture on foundation mulativeAI. Today we're Going to coverChat GPT and chat GP. We'll also be looking at chat GP as a tool to help people understand how to use AI. now we're able to do things we weren't able to doing before and and definitely uh created some kind of hype uh. So hopefully after this lecture you'll you understand kind of the basic idea and also somehow understand the BET right the bet that open Ai and Ilia the open the BET and open up the world to each other. The BET is a weekly, live, online chat show hosted by CNN's Jake Tapper and CNN.com's Adriana Navarrette. Follow the show on Facebook and Twitter. head researcher did in terms of what actually would lead to CHP and how in hindsight it might be quite I mean easy but it was a really daring bad not obvious at all at the time that this would actually work out um so should be be a lot. "I think it was the most daring thing a researcher has ever done," he says. "It was the best thing a person could ever have done" "There was no way of knowing that it was going to work out," he adds. "There were a lot of surprises" of fun and just to quickly go through our course schedule as well as a little bit right so today is January 16 uh and next time we'll talk about stable diffusion image generation and then we will talk about emerging Foundation models basically Foundation models generative AI in the commercial space. Of course we'll also be talking about the latest developments in the field of artificial intelligence. We'll be back on January 16 to talk about our next course, which will be on the subject of Foundation models Generative AI. H we'll have two guest speakers and then we'll end with the lecture on AI ethics and regulation as well as a panel. okay so what have we talked about before we started off H with an introduction a short high level intuitive answer to what is foundation M generative. We'll end the show with a lecture on the ethics of AI and regulation and then a panel on the future of AI in the world of M-generative systems. We hope you will join us for the rest of the show. AI we went a little bit on a philosophical digression and asked about how's the world structured because that allows us to think about how we should learn in the world then we on the second lecture went through all the different algorithms um and yeah today we'll we'll dive into some of the algorithms that we'll be looking at in the next few weeks and we'll share some of our findings with you. We hope to hear from you in the coming weeks and months. Back to the page you came from. Foundation models geni.in more specifically into chpt and kind of uh pull everything together um and to reiterate right so what do we do in uh Foundation modelsgeni well we apply this self-supervised learning where we learn without uh label data so we can we can get you know as much as possible out of the data. In this case, we use the Foundation model to learn without label data. So we can learn without Uh label data in this case. In other words, we learn with no label data at all. "There's no human being in the loop so there's no limit how much we can scale this up," he says. "What we get from this you know by learning from observation and learning from the data directly is a very contextual and relational understanding of data," he adds. "We can learn as much of the data as we want because there's not a human being involved in the process at all. We can learn from observation as well as learning from data directly," he concludes. meaning and we gave this example before about you know from a supervised learning perspective you learn what a dog is from seeing you know labeled examples of dogs. In reinforcement learning you focus on optimizing certain goals and you understand a dog in relation to how it makes you happy or fulfilled in some sense or optimizing your goals. In self supervised learning right it's the foundational technology behind uh Foundation models you learn from observing dogs in different context and you get a very relational definition of a dog so it's something that's walk by. an owner with a leash it has an anistic R with cats it chases fris with oone right this is your definition of what a dog is and today we'll you talk about something that's extremely engineering heavy in you know chat GPT. GPT relies on a lot of tricks and relies on the help of a team of engineers. We'll talk about how to get the most out of your dog in a variety of ways. Back to the page you came from. Follow us on Facebook and Twitter. and Engineering insights and breakthroughs that we're not going to cover. I think still though you know like it's like talking about a car you can understand the high level perspective of a car and get some insights how to work how it works and how it's going to be going to go. We'll be back next week with a look at the next generation of the F-1 and F-2 cars in the U.S. and around the world. Back to Mail Online home. back to the page you came from. be useful for you without getting into all the engineering details but of course in real life those engineering details really really matters and are very very hard to get right. That's something that we won't really dive into in this lecture because that's just when you bring something to the table. We'll be talking more about how to get the most out of your phone in the next few weeks. We hope to see you at the end of the lecture on October 31. For more information, visit CNN.com/sport. up certain scale and you have to paralyze a lot of machines Etc and think about high parameters it's a whole science so it's not trival at all but it's kind of hard uh to teach in a course like this. You have to learn by just actually actually doing it on a certain scale. It's not a trival thing at all, it's just a different kind of way of looking at the world. And it's actually a very interesting thing to learn. building this stuff um okay so um also a little bit of philosophizing in this uh class as well um I think that again like we talked about a little. bit of a theme here right is that the why this new AI is so powerful is because it doesn't. need to be programmed. It doesn't need to know how to do everything. It just needs to be able to do it. It needs to have the freedom to do what it wants to do. It can't be told what to do, it can only do it by itself. Force things to comply to Simple Rules right it kind of abandons our ability to understand and compress what we're seeing and deals with that chaos directly. That's why AI is so powerful and so humanlike um so also like when I talk about this in CHP we try to try to understand what we are seeing and how we are perceiving it. We try to compress it and make it easier for us to understand. We're trying to make it so that we are able to make sense of it. make very high level um statement but of course the nuances matters and I think it's quite interesting. I took this quote from a general from the 18 and 1700s and he says this uh quote that P Theory which sets itself in opposition to the mind and what what he means by P Theory. It's a very interesting quote and I find it to be quite interesting as well as a very good point of view. I think the nuances of P Theory are quite interesting and it's very interesting to think about. he meant was that he's a general so he fights in battles and War. At the time people loved to come up and theorize around War like we should have certain rules and how soldiers should behave in fighting and stuff like that. But he's like well I've been. He's like, 'I've been in War' and he says, 'That's what it's like to be a general and to fight in War. That's what war is about' and so on. in War uh and Wars don't comply to rules first off so you know everybody has a plan before they get hit in face basically. "You know as people start shooting at you and you have this fog of War of you don't know what's going on there's no," he says. "There's no rules in War and Wars. There's no regulations in War. There is no rule in War," he adds. "there's no way of knowing what is going on in War" simple rules to help you there and also what he says this in terms of the mind he says like well actually he's realized by working with soldiers that soldiers and human beings our mind we're not good at acting according to rules that we try to memorize we're very. very. simple rules. He says that he has realized that by working. with soldiers he has learned that soldiers. and human being our mind our mind is not good. at acting in a way that we're supposed to act in. intuitive and very kind of quick to react to things by our intuition that's what really really matters. "If you force a soldier's well Al try to memorize a lot of rules and that's how it should act in a battle you're kind of kind of making a mistake," he says. "We're strong at intuition. That's what we're good at. We're not good at memorizing rules. We don't know how to memorise rules. You can't memorize rules." of screwed and very limited in what you can do uh which also is something that I think AI uh in a new type of AI leverages okay so chat GPT um right this is a really amazing breakthrough that uh has some very humanlike Mastery of language that we have never seen before. I think it's going to be a really exciting time for the future of AI. I'm excited to see what the future has in store for us in terms of how we interact with it. I hope to see a lot more of this in the future. can communicate that can basically solve a really wide array of tasks for us. Anything that can be phrased in terms of text language it can it can basically solving and now as well when with gp4 ET becomes uh it's able to handle multi modalities. But it's it's extremely. extremely powerful and can do a lot of things that we don't know how to do before. It's a really exciting time for us and we're looking forward to seeing what the future has in store. powerful so let's try to break this apart well first off what does this name actually stand for well the chat part is obvious it stands for chat and then GPT stands for generative pre-trained Transformer and this is a I mean a good description of what this uh actually actually stands for. So let's start with the chat and the GPT part of the name and see what the rest of the story has to do with it. We'll be back in a week or so to talk about the next episode of The Walking Dead. is um and I think also if you look at the the two different three different concepts here they're also almost corresponding length in terms of how important and influential they are in making chat GPT work. So chat part we we'll cover last it's the kind of the least. It's the sort of the most important part of GPT. We'll cover it next week in a Q&A session with the GPT team at the conference in Las Vegas. We're also going to talk about how to use chat in the future. important one in some sense H the Genty pre-trained is the self supervised step of how you train this and arrive at this uh model and then the Transformer is the basically the engine behind it in some way. So let's start with this generative pre-train what does it do. What does it mean to train a Genty model? How do you train it? What do you do with it after you've trained it? How are you going to train it after it's been trained? mean how do we pre-train this model and that's basically where openi spent 99% of the compute was to do this pre-training step so it's it's very very important okay so what we are going to do is that we're going to uh just take some random text from the internet, and we'll use it to train our model. We'll use that text as the basis for our training. And we'll then use that data to train the model. And so on and so on. so we have a sequence of words and and then we're just going to try to predict uh the next word based on previous words so let's say we have uh we start with i here as input. Then we want to someh predict the Target right so we have that as an input. And then we are going to be able to predict what the Target is based on the previous words. So we have the Target as the input and we are trying to predict the next words based on those words. know we know or the computer knows somehow by just downloading the text that what this whole sequence is. When it trains this AI model it hides part of it right so it just inputs I to the AI model and then the model is supposed to do what it's supposed to be doing. The sequence is called ‘I’, ‘II’ and ‘III’. It’s a sequence of letters and numbers. It starts with the letter I and ends with the word ‘i.’ something with it right so it's supposed now to make a guess or what's the next word so you you basically would allow the model to uh guess and then maybe it's off right and then you can give some negative feedback uh and then when it gets it right. It's supposed to guess what the word is and then give a negative feedback if it doesn't get it right, so you give it negative feedback and then it tries to guess it again. It tries to get the word right again. you can give some positive feedback right so this is the high level uh what we want to accomplish so the first thing actually that you start thinking about is well there's multiple you know giving a sequence there's just one ground truth. There's still in this example there's still the correct prediction. You can still give the correct feedback right. You're still in the example. You've still got to give the right feedback. You have to give a correct prediction right so that you can give a positive feedback. just one single word that actually will follow but there's tons of words that are not the correct guesses so maybe you want you know you want to allow the model to make the best use of this example as possible. So you can basically make a lot of guesses. Just one word that will follow. But there's lots of words. That actually will followed. But it will not be the correct word. That will be the wrong word. It will be a different word. And it will be different for each word. and you can you can give them information a lot of aot about a lots of different uh guesses that are actually wrong right. So you're able to give more information to the model here like hey actually go and gone here are wrong umbrella is also wrong right and and so on. And you can also give them more information about what you think they're doing and what they're looking at. And so you can actually give them a lot more information than what they think you're doing. then it kind of gets it right and then you give some positive feedback back. We're going to create scores or predictions for all words in the L like in the human you know. Then we'll kind of do this um we're going on to maximize this so uh we're gonna do this. We'll do this and we'll do that and then we'll see what happens. And we'll just kind of see how it goes from there. It's kind of like a game, you know? vocabulary in the English vocabulary that sounds extremely expensive and it is quite expensive and so I have different tricks to make this work. You're going to make a guess and a score like a prediction for every word in English in theEnglish language and only one would be right. "Only one would" is the answer to the question, "What is the most expensive word in the vocabulary of the English language?" "There is no word that is more expensive than the word 'expensive'" U probability scores are just non- negative numbers that can't be wrong. "It gives a lot of feedback as well because there's a lot. of information knowing which ones are not correct," he says. "We're going to create these U probability scores meaning that these are justNon-negative numbers and can't.be correct but it gives a lots of feedback," he adds. The U probability score is a non-negative number that cannot be correct but can be given as a guide to which numbers should be considered correct. they all sum to one so they actually corresponds to the the the models guess at the likelihood of like the likely of a word coming next so you make a distribution of all possible English words and then the score corresponsive likel of this word coming after the what is. They all sum up to one to make a total of 1, which is the number of words in the English word distribution. The number 1 is the most common word in English, followed by 2, 3, 4, 5, 6 and 7. seen so far uh okay so here you know at this point the model is in i12 it creates a distribution you know here's this four words but all words in English language then um you know you Rel you you reveal which one is the correct one so the model can be used in the next generation of iPhones and iPads. The iPhone 6 is expected to be released later this year. It will be the first iPhone with a built-in touch screen. The iWatch will be released next year. is the correct thing to come after and then H you give this feedback to to the model it's called back propagation. So you give some feedback through model it should push the uh the score or the probability distribution for the correct one to be bigger or larger and so on. It's a kind of feedback loop that works like a feedback loop for a computer game. It can be used to get feedback on how to improve a player's performance or to improve the odds of winning a game. then reduce all other ones so you know the next time it sees uh the same example or a similar example it actually does better. You know this is just one single example but you accumulate all of these directions and information across a batch of examples that you use. Then reduce all of the other examples. Then you have a set of examples you can use to test your knowledge of something. You can use these examples to teach your students how to better understand the world around them. It's a very powerful way to learn. see at the same time so it takes small small steps to getting a a better and better distribution of what word will come next given previous words. You do this in a batch on tons of examples and of course you know we. know we know we, so it's a lot of work but it pays off in the long run. It's a good way to get a better idea of what the next word is going to be, and it's also a great way to learn more about the language. have unlimited amount of data uh because we can just get text Data from online of course we do this on the whole um uh sequence so we can make the most use of this sequence. We predict the uh the next word for every possible uh combination here. We do this so that we can use the most of the data we have to make the best use of it. We use text data from online to get the most data from the most possible combinations of words. We can use this data to make sure that we get the best possible results. all right so now that we've trained it we have a model that's able to uh predict the next word given previous words so again we uh have some starting point I for example it's not a very interesting prompt to model but it's a starting point it gives us. All right so  we have a model that's able to predict the next word given previous words. So again we have some starting point to predict the next word. a distribution now over all possible words in English language. Then we just take the argmax and we put it into a sequence and we get a distribution. We can sample where you take theargmax which which the most likely word to come after this we we take argmax  and we get  a distribution over all words in the English language and then we just take  the argmax we can sample  where you take the argmax and  we get  a distribution over all words in the English language. longer sequence and then we can run the model on this sequence. Then we do the same thing and we just keep going H we can make our sequence longer and longer and we can continue this uh you know for example till we reach a period or something. It's a long sequence but it's not a very long one. It just goes on and on. And we can do it for a long time. We can do this for years and years. And so on and so on. like a a specific token that says we're happy until we complete a complete sentence for example um and this is kind of expensive uh to do because you have to generate one thing at a time. But of course training is is much faster because then you can just training and not have to do it all at once. So that's what we're doing. We're training to be able to do all of these things at once, which is a lot of work but it's much faster than having to do them all at one time. you don't need to generate and run on your own input you just run look at the input that you get yet but here you actually have to look at. the prompt generate the next word add it and run it again so it's kind of expensive and it's sequential. You don't want to run the same thing over and over again. You want to be able to run different words at the same time and get different results from the same source. You can do this by looking at the prompt and then running it again. in that sense but you don't have to do that during training only in evaluation and training is what's most expensive so that's fine somehow and okay so you know if I went home it's not a very interesting prompt uh what type of prompts would be more interesting well what kind of prompt would you like to see? You know what I'm saying? I'm just saying that I would like to know what prompts you would like me to write about. If you want to see more of my work, please send it to iReport.com. If it's really good at predicting the next word based on previous words we can give it interesting prompts. It can start solving interesting tasks for us by just being able to predict the next words based on the previous words. We can also use it to help us with our reading comprehension and vocabulary. We've used this technique in the past to help people with reading comprehension. We'll be using it in the future to help with vocabulary and other tasks. We hope this helps you with your reading comprehension as well. on previous words um so here we see that we we uh basically have this different language task we just give this to uh the model and if it's really good it should be able to generate the the sensible things that we're looking for. If you try this you'll see that it works. If it doesn't, you'll have to go back to the previous words and try it again. It's the same with other words. You can try different words and see if they work. If they don't, try a different word. for CHP right it does so it basically has killed a lot of different research Labs that focus on a specific task because now it does all of this really really well and I mean this is basically from a modeling perspective this is chdp in a nutshell okay. For CHP, the key is to be able to do all of these things at the same time. To do this, you need to have a very good understanding of how the computer works. For more information on how CHP works, visit the CHP website. so what's the I mean it sounds maybe sensible and reasonable but of course the what set CTP apart was a tremendous scale. This was trained at a scale with an amount of data and parameters that we never seen before. So this is a a year old now but this is still very interesting to me. It's a very different way of looking at the world. And it's very exciting to be a part of it. It is a very exciting time for me. I'm excited to see what the future holds. this is I think this was 3.5 or something the first version it was using 175 billion parameters and just training the the final model like I not including all the iteration that you have to do to try things out. Just training the final models cost around $5. This is the final version of the model that we used. It is based on a set of parameters that we have been using since the beginning of the project. The final model was created using a combination of these parameters. It was then trained on a series of other parameters. "It's a very very simple approach but it's it's a certain scale that's that's never been seen before and really that's what a big part of open eyes," he says. "That's how huge and much compute they spent on this and uh again so it was a veryvery simple approach," he adds. "It's it’s a veryVery simple approach. But it's an approach that's ... that's not seen before. And that's really what's what we're trying to do." "We've been doing this language modeling for quite some time trying to understand Language by predicting the next word based on previous words," he says. "We're doing you know we're using it for certain things but I mean know very very well," he adds. "There's a lot of research that's being done on how to predict words based on what people have already said" "It's a very powerful tool," says the professor. "It can be used to predict what people are going to say in the future" few thought and were convinced that if you just scale this up big enough it will become a multitask Sol and show humanlike Intelligence and that this actually really will work. "People talk a little bit of this emerging abilities because also it's not linear right like you start," he says. "It's not a linear process. It's not something that happens in a linear way. You have to go through a series of steps to get it to work. And then you have to do it again and again." adding and putting putting more and more uh compute and parameters and like oh it's still not very useful but then at some point just start being like extremely useful. "It was it was a huge kind of leap of faith as well for open eye to say like like like this is what we need" he said. "I was like, 'Oh my God, this is really cool' " he said of the project. "And then it just started to go from there," he said, "and it just went from there" well we're just going to go all in and just make this bigger and bigger andbigger and and and then like in hindsight like maybe it makes sense but it could have been a case like it wouldn't work and then people like oh that's a stupid bet like. That's what we were trying to do. We were like, we're going to do this. We're like, 'We're just gonna do this,' and then it just kind of fell apart. It was just like a lot of things that just didn't work. why would you think such a simple idea and approach would lead to to such sophisticate intelligence but it did okay so we covered thetive pre-train part right so uh you know we've now said how we basically are going to uh train our model but how does this model do? And so on and so on. And so we're basically going to basically train our models and see how they do. And then we're going to see if they can do what we want them to do. look like like how does this kind of engine look like so if if if this whole thing is a car then basically g pre-trained is how you teach a driver to drive and then the Transformer is the engine that that it leverages. Some people definitely claim that this. this is the kind of car that this engine leverages and some people definitely say that this is what this car is all about. This is the car that it's all about and this is how it's going to work. Transformer part is extremely extremely important so there's a debate a little bit what was the most influential part of making uh CHP and large language model possible. Transformer is definitely a significant part of it and and I'll let you judge for yourself but uh I think it's less important than the actual modeling perspective that we've we've come up with okay so in order to understand the Transformer we're going to start to thinking about how we can process sequences so text is just sequence of words. process sequences okay so let's say we have this uh uh sentence that we downloaded from online and we want to process one word at a time. We want to predict the next word so we'll have our model and it basically uh looks at the first words create some intermediate embedding or feature here one and and then it uses the that in a second step to uh predict thenext word. To be able to do a good job you also wants to able to incorporate the previous word and the features from there so you kind of also processes and includes into the second Vector both the previousword and the current word to create a new representation of the whole sentence so far. then it uses that to to kind of predict the the next Target okay we go on and we do the same thing um and uh of course we do this for whole sent and I think what's this sounds maybe trival but the thing that's important to notice here is that it's not just one thing. It's a whole set of things that can be done. It can be used to predict what's going to happen in the future. And it can do it for a long time. is that for every step here that's label with the same uh digit you know they can all be done in parallel so everything at step two here can be done  in parallel they don't need to wait for anything right step two needs to wait on step one to do step one. If step one is done first and step two is done second, then step one can do step two and so on. This way you don't have to do any of the steps at the same time. You just do one step at a time. finish right but all the two can be run in parallel they don't rely on each other and step three can be. run after step two is been run. This is key because in in deep because in the deep you need to be able to multitask and not rely on the same steps for each step. It's important to make sure the steps are parallel so that you don't need to rely on them for the next step in the same way as the previous one. It is also important to ensure that the steps don't have to be done in exactly the same order. learning we use this uh um computer is called gpus and basically the cost is you know if something can be done in parallel it's a single cost. We don't care how much they do in par if it's it's done in Parallel it's A single cost so if we. do something in parallel we don't have to do it in parallel. So if we do a thing in parallel, it's not a cost to us. We just have to make sure we do it at the same time. can make multiple steps into single step in parallel this is a single cost we want to run things in parallel as much as possible. So here basically you know this be a cost of four because all these different numbers can be run in parallel uh so this would be a total of four. This would be the total cost of the whole system. This is a total cost for the entire system. It would be four times the cost of one of the other ways to do the same thing. And it would be done at the same time as the other way. just be a cost of four and then of course uh processing this whole sequence will be a costs of nine. "This might be you know seem like a pretty good job and reasonable because this is a sequence we have to process process it somehow and we're paralyzing," he says. "We have to ... process it some way," he adds. "It's going to cost a lot of money to do this." "We are paralyzing. We are going to have to do a lot more work on this one," the doctor says. most of the steps so maybe that's the best we can do and this is called a recurrent new network when we process things this in a sequential way H we try to Pary as much as possible but your current process depends on the previous the previous step and so on. Most of the time we don't Pary but we do try to try and Pary in a way that makes it easier for us to see what's going on in the background. We try to make it so that we can see what is going on at any given time. these are extremely extremely popular and a version of them called uh lstm long short-term memory networks um it performs really really well. Some people say it performs you know almost better than Transformers a lot of times but they just take them longer to train because we're going to have to teach them how to do it. We'll be back in a week or so with a new set of videos to show you what we've been up to. We're going back to the U.S. for a week. to realize why it one a point but they work really really well and and also notice here somehow that uh this was very very intuitive for researchers to say like well text we read text from left to right we process words one at a time and therefore our brains process words in this way. To realize why we read texts from right to left we read them in this order. To process words we read each word in a different order and therefore process them in different ways. To understand how this works we read a word from left-to-right to right. models should to to to tble to learn effectively from them okay so we're going to simplifies a little bit and just think about how information Flows In in these uh uh networks or models. So again for rec Network we basically have this very simple information flow where uh where uh information flows in and out of the network. So that's what we're trying to do here. We're basically trying to make it easier for people to understand what's going on in the network so that they can make better decisions. things flow forward this in kind of this sequential way right so to get from uh you know for the information from I to go to the uh information prob being processed step number nine basically right when you want to predict the period has to travel eight or nine. So to get to that point the information has to go through a series of steps. So it's kind of like a sequence of events that go from one thing to the next. So that's how we get to the point we're at. steps here to to to uh be used so let's think about this a little bit start we start off now in a Transformer which basically starts off the same way. So we we let the first we know we just process the first word uh and we prict the first words. We then process the second word and the third word and we process the fourth word and so on. We end up with a list of steps that we can take to get to the next step in the game. Next Target based on that uh but the the difference happens when we look at the second word so instead of doing sequentially and saying that we're going to use the process that we've been used that we used before we're just going to directly incorporate information from I and when to the Target so we kind of we're we're not going to enforce this quential structure we're Just going to let the information flow from the previous word to the current word Etc right what we've seen so far. Every Target somehow has a a node or sorry an edge to the previous word so they can all be run in parallel they don't need.finish to do the next step right. Here everything is processed independently so you don't have to wait because every Target has a node. Every Target somehow. has a nodes or edges to it so it can all. be ran in parallel. Here you don’t need to wait to do. the next part right but here you do. because here everything is processing independently. to wait for anything they basically kind of re redoing all the work somehow for every step because they all have this added to the previous words so all of these steps now like none of the steps have to wait for each other right. There's no independency they can. can't wait for. anything they can't do it on their own. "There's no independence they can do it themselves," he says. "They can't. wait for something to happen to them. They have to do it together." just go Direct to the the the source and use that information and of course you know we do this for the whole sequence and um again to reiterate right so for the last Target all of these computations can be done in parallel right they they're somehow aggregated at the end of the sequence. Just go direct to the Source and use the information that you get from the source to do your computations in parallel. That's what we do. We do that for the entire sequence. We don't do it for just one Target. the Target and they can all be done in parallel because they don't rely on each other but also right this can also be done at the same time in parallel. Eight doesn't depend on Nine and Nine doesn't depends on eight so so they can also Be done. The Target and the Target can be done by eight and Nine in parallel as well as by the eight and the nine. The eight and nine don't depend each other as much as the nine and the eight. The nine and eight don't have to be in the same place at all. in parallel it's the same step and this is true for all of these steps yes this sense when we compute a output distribution over all the words later like as a prediction we talked about the first thing know oh yeah yeah sorry sorry sorry. So exactly this this this works this is how it works. Yes this sense is true when we computed a output Distribution over all of the words. Later on we'll use that distribution to make a prediction about what the future will look like. We'll show you how that works in a second video. only during training now right okay and I'll come to that actually later so this is only during training where we can optimize this way okay. But that's also yeah that's a great question but that is also like something that's uh in uh in deep learning we basically almost almost I don't know how to answer it but that's what we're trying to do. We're doing a lot of work on this. We've been working on it for a few months now. It's a big project for us. mean the training is the most expensive part uh because then you op you optimize and and you do back propagation to update update your parameters which is very very expensive when You' when you kind of uh when it's done you freeze it and you don't update things more. When you're done with the training, you're not going to be able to update things as much as you would like. You're going to have to do a lot of work on the training part of the process. and it's going to be much faster to run so it's much less uh uh well that's a modification but uh it's a little bit less sensitive in a sense. "We care about about I mean we care about both being fast uh and yeah I mean but somehow but somehow," he says. "It's a bit of a change but it's not a big one" "We don't care about being fast. We just want to be fast," he adds. "That's all that matters" H this is going to be much much faster to train than a recurr network so you're going to get much much better performance. Then the difference in deployment is going be less uh significant okay but during training we can do this because we have a faster network. We can do it during training because we've got a much faster network to train with. We've got to do this during training. We have to do it in training. It's going to make a huge difference in the way we train. not upend the words we just see them in the sequence we can do this um yeah actually this is my not this is of course only training um all right so now you know again we look at this next to each other uh like what's I mean maybe maybe. Not upending the words is a way of showing that we're not going to upend them. We're just going to see if we can get them to do what we want to do. We don't have to change the words. I mean what's the biggest difference somehow right one of them the top Rec Network looks very structured right this has kind of a strong bias of of processing things sequentially. The bottom looks very chaotic and it looks like Ah that's just a lot of connections it probably is. It looks like it's going to be a very busy day. It probably will be. It's a busy day for us. It will be very busy for us for a long time. We're going to have to get used to it. pretty hard to make sense of the sequence given that it's all FedEd you at the same time uh so it's may be surprising that one works better another one that definitely kind of needs more data to start learning useful things. Transformer needs typically needs to learn more data before it can learn useful things so Transformer usually needs to be fed a lot of data to be able to make a decision about what it wants to learn about the world. It's not surprising that the Transformer that works better is the one that needs the most data. more data to start doing a good job but there's also another thing that we're uh you know really forgetting here right so in a recuit network things are processed one at a time so the the model can figure out that you know Financial comes after the because it's the most important part of the network. So that's what we're trying to do here. We're going to try to do a better job of making sure that we get the most out of the data that we have. sees the first and then Financial but in a Transformer you know in the below here like if the only thing you see is the word and they're all F to you know for for if you look at the prediction we going to do at at step number nine. Sees the. first and first and second and third and fourth and fifth and sixth and seventh and eighth and ninth and tenth.sees. the first, second, third, fourth, fifth, sixth, seventh, eighth and Ninth. if you see all these words the same time right there's some kind of comp like you can permute all the words and you basically see the same thing so there's no sequ. There's no sequential structure and force and Transformer whatsoever. Right everything is being fed at the time. There is no sequancy and there is no Transformer at all. There are no words and there are no letters and no words at all but there are words that can be permuted to make a Transformer. same time so there's no sequence anymore right you're seeing everything at the same time. So how do you solve this well you do the simplest thing you can H again just we remove the numbers here and make make it more obvious that there's. No sequence anymore. So you're. seeing everything. at the. same time sothere's no sequences anymore. so there're no sequence. um so how do we solve this? You do the simple thing you do. H again. Just we remove. the numbersHere and make. it more. obvious thatthere's. just you know words there's no sequence anymore because everything is connected in Transformer but recur not there is still the sequence by how by virtue of how things are processed so how we solve this that is that for for the Transformer we're just going to add to each word a positional encoding so we just add the position again. The Transformer has to figure out if the sequence matter it should use that information even but it now has that information at its disposal because we're going to encode a sequential structure. things are processed but by just appending a positional encoding and this is actually you know quite almost contra inuitive it's you know it's it's like seeing all the words in a book at the same time like it's fast but it's very confusing and then you have to like like like go back and read it again. It's like that, you know, it's quite almost almost contra-inuitive and it's a bit of a shock. But it's actually quite interesting. figure out by a small you know number how things are actually oriented so like if you go to the movies and you think in terms of of frames you can sit down and digest the whole movie in one second. It's like super efficient but you're seeing all the same things at the same time. That's how you get a sense of what's going on in a scene. It can be a little bit confusing at first, but once you get the hang of it, it gets easier. frames you know flash at the same time and then after like in your own head you have to put them in a sequential order if it's useful as you know understand the plot of the movie which typically is right but there not like the transformer has to learn.frame you knowflash at the. same time, then after you know in your. own headYou have to. put them together in a. sequential order. If you know the. plot of a movie which is right, you typically is. right. But there notlike the transformer. has to learning. that implicitly because it's not happening directly okay so why is this good as well well it's good because another like it's fast but also it'sGood because for new networks memory is very hard so it's hard for networks to remember things so let's say you know that that that. That implicitly because that's what's going on. That's not what's happening directly. It's not going to happen directly. That doesn't mean it's bad. It just means that it's going to be a different way. if you read a book or you watch a movie if you want to understand the end part it might be good to kind of go back and look at the the start starting part of the book or something you know or or it's good if you remember that. It's good to remember that when you're trying to understand a movie or a book. If you're not sure what you're going to do it's a good idea to go back to the beginning and start at the beginning. information but probably you know if you don't remember you have to go back and look it up so in a Rec Network Rec Network because we're processing things uh sequentially here so to for something to be used like to for for information about the first uh word in the Rec Network. "Rec Network" is the name of a network of computers used to share information. The Rec Network was created in the 1970s by the television network NBC. It was the first network of its kind. the sequence to be used the last step here so you know to have information about I at the step point you digest money. You have to somehow remember that information through throughout this whole processing step right. That's very very hard for networks to do and a lot of people find it hard to do. It's very hard to get the sequence right and get the information right. It can be very difficult to get that sequence right in the first place, and it's very, very, hard for people to get. of work was done to make that work better but the nice thing with Transformer is that it has a direct connection. If it's a very strong kind of recurring uh thing that the first word and last word are the same, that's the way to go. "Transformer" is out now on Blu-ray and DVD. For more information on Transformer, visit www.transformer.co.uk or go to the official Transformer website. The official website for the movie is Transformer.com. word correlate somehow it can pick up on that very quickly and make that edge very very strong and kill out other edges. So you can incorporate information very quickly or very efficiently so you can basically incorporate long distance information in this sequence very efficiently because there is no long distance. There are no long-distance words in the sequence because there are no long distance words. There is no short-distance words in the sequence because there is no long distance information in this sequence. real sequential structure but when we force the sequential structure and Curr Network it's much harder because then we need to remember that as we process things okay. So to summarize Transformers um we do everything I mean everything we care about in when it comes to deep learning that we can do. We do everything we can to make sure we get the best out of our data. We try to do as much as we can in the shortest amount of time possible. We don't try to be perfect, we just try to get the most out of it. we want to do things in parallel uh as much as possible because if everything is done in parallel it's a single cost and and the Transformer is doing that optimally because it just process everything in parallel and then it removes sequential structures. We have to you know give it a try, you know what I mean? We want to get it right. We want it to be perfect. We don't want to mess it up. We just want to make sure we get it perfect. that information to the model by appending positional encodings um and also it's good that it's you know it turns out that this long uh distance information in data is typically very useful to be able to incorporate efficiently and memory is hard so Transformer are good at incorporating this information. That information is usually very useful, and it's hard to store in memory, and Transformer can incorporate this information efficiently. It's a good example of how Transformer is able to work with data that's very long. information and and is able to do better by it and they replace these Rec networks and again right this is we talk especially during training but this is uh uh the difference is less severe when it comes to uh inference or when we deploy them and uh again. It's a different way of looking at it, but it's the same thing, it's just a new way of seeing it, and it's a good way to look at it. And we're going to continue to use it in the future. like since now we have St super plus learning we can train by just downloading text from the internet and there's no human being to help. Loop the scale of data is so much you know so big that we can learn a lot by you know we can learning. We can learn we can, we can. We have a lot of data to learn from, and we can use it to improve our knowledge of the world. We're going to use it for a long time, I'm sure. learn and afford to use a lot of data to learn basic things so Transformer has much less structure and has to relearn a lot. Since we have so much data and we don't need to have labeled data we have we can afford that right. Transformer is the latest version of the open-source Transformer operating system. It is the first version of Transformer to be released to the general public. It will be available in the U.S. and Europe in the next few months. we can afford a train on a scale that we haven't seen before so that's also why this works so well okay so uh now we have a language model right we know how to train it uh we know what kind of engine that it can use to uh. We know what type of language to use and how to use it. So that's why we can afford to build a train that's so much bigger than any other train we've ever seen. That's why it's so easy to build. be efficient and work well so now we're just going to look at the F last part which is the chat part um so you know you you you train this model now that you call DPT 3.5 or something and now you want to turn turn into chat GPT. That's what we're going to do here. We're just looking at the last part of the training. We want to see if we can get it to work well. We don't want it to fail. We just want to make sure that it works well. right so we have a uh a really uh good model basically we've done 99% of all the the work that's that's required. A lot of people still kind of debate how important this last step is but open is it does a difference uh but you know when it's done it's a really good model. It's a good step. It makes a big difference. But it's still a long way to go. We're not there yet. We've got a lot of work to do but we're getting there. we have this model we see we we see that kind of oh it it works fairly well but H we want to be able to improve it. There has some stupid failure cases and we just want to make it a little bit more sophisticated so the first thing is to make the model more sophisticated. We want it to be a little more sophisticated than what it is now. It's a bit of a work in progress, but we're working on it. We're going to try and make it better and better. we we we're going to do is that the model now has been trained on a vast amount of data from you know any Source on internet you can imagine right so novels Wikipedia Facebook posts uh you know anything basically but how you how users are going to use it. We are using a model to predict how users will use the social network. We will be releasing the results of the study in the next few weeks. We hope you will share your thoughts on the project with us on Twitter and Facebook. this is through some chat bot right so it's dialogue like human dialogue is what they call it and of course it's been trained on a lot of text that's not human dialogue. Now we're just going to say we want to be able to hone in and focus and focus. We want to have the ability to focus on what we're doing and not what's going on around us. We don't want to get distracted. We just want to do what we need to do and focus on it. and adjust itself a little bit by training only on human dialogue. So we going we going to go and collect the best data we have of human dialogue from whatever source that we have. We're going to train for alittle bit longer on only that data so that we can adjust ourselves a bit more to the human dialogue that we're training on. That's what we're doing right now. It's going to take some time to get used to, but it's a good first step. we're going to find T the parameters only on human dialogue data so it can hone in its parameters and focus on a specific use case. okay so we do that and we're even one step closer and so now we are even working we work even better but then opening up the data. We're even working even better and then opening the data up to the public. It's a very exciting time for us. We've been working on this for a long time and it's been a great learning experience. I wants take it one step further and say like well there are some observed problems in this model we're going to address um and uh one one thing is that right now when we've trained on on this text data right each each uh Target is worth the same. I want to go one more step further. I wants to go two and a half and say that we're not going to stop training on text data. We're just going to change the way it's trained on. I don't want to stop it. somehow and we don't. We don't really uh separate good or bad dialogue so the model doesn't really know what's good and bad dialogue. It just knows what's plausible dialogue from the internet but somehow we just want to say like well yes you know what're plausible dialogue but it doesn't. It doesn't know what is good and what is bad. It's like we just don't know how to separate it. We want to separate good dialogue from bad dialogue but we can't. would really good if you understood what's not helpful dialogue and what's helpful dialogue so you can just give us helpful dialogue. Another another problem is that we're somehow too greedy so when we train things to predict the next word based on previous words all we care all we want is the right answer. We're all too greedy. We don't want the wrong answer, we just want the right response. We want to know what the answer is. We need to know the difference between helpful and not helpful. about is to give the most likely next word but if you want to generate a sentence right you don't really care about optimizing the likelihood of the next word. You care about optimize the accumulated likelihood of your whole sentence and a lot of times you can sacrifice that to get a good sentence. The most important part of a sentence is to make sure that it doesn't sound like you are trying to make a point about something that isn't important to you in the first place. For more information on how to write a sentence, visit CNN.com/writing. Short-term profit for optimal long-term profits. What we really care about is the the score of the sentence at the very end. When we're done with it we don't care about picking the best. We just want to get the score at the end. We don't want to pick the best sentence. We want to make the most out of the time we have. It's the same thing when it come to generating these these sequences so what we really cared about was the score. We wanted to get to the end of it. you know the best step in at every you know every step of the way for example here right if you go down a little bit at bank you can you can reach out reach a much higher optimal score at the end of the sentence and that's of course. That's of of course the best way to get the most from a sentence. You can get the best out of a sentence if you know how to get it right at the beginning of it and the end. It's the same with every sentence. what we care about so somehow we're too greedy we should be a little bit more long-term optimizing. If you give a whole sentence we care. about the the quality of that sentence that's what we want to optimize for okay and then the third Nuance or or kind of kind of thing is what we're trying to get out of a sentence. That's the kind of quality that we're looking for. We want to make sure we're getting the quality we want out of the sentence. of difficult with the current model is that somehow we would like this model to be a little bit bit more robust let's say. So it turns out that this model now has been trained on uh text online and of course it it works really really well but people still get frustrated with it. And so we're trying to make it a little more robust so that people don't get frustrated by it as much. And we're also trying to improve the quality of the data that we get. are going to use this and interact with it in ways that maybe it doesn't really correspond perfectly to its training data so it's going to uh see things I haven't seen before. There might be a kind of a distributional shift between how people use it and what the training data says. "I think there's a lot of potential for this to be used in the future," he said. "It's a really exciting time for the future of computer science. It's a very exciting time" it's been trained on and also right as we said when we Deo this model they're going to you know generate a word add it to its its own. Add it to like its current sequence and then rerun itself on its sequence right so it iteratively create a word. It's going to go through a series of words and add them to each other. It will then add them together to create the word it's looking for. And then it will repeat the process again and again. longer and longer sequence by running it it itself on its own output so it adds a word after word. No AI model is is perfect so maybe it accumulate some error as it start adding words and it's just going to go off a little bit and it will add more and more words to the sequence. "It's like a little voice in the background saying, 'I'm going to add some more words here' and it'll add more words and more sentences," he said. "And it's like, 'Oh my God, I'm adding more words. It's going to keep adding words." just goes off a little bit like here for example when you say you know I went to the financial and then just you know some small error happens and it goes off the road to restaurant like somehow you know they started seeing that okay now it's basically go. Just go to the bank. Just goes off to the restaurant. just goes off. to go to a bank. It's basically gone off to a restaurant.just goes to a different part of the building. It goes off in a different direction. It doesn't stop. Haywire because it went off and it's in a different space than it's been trained on. It's just going to generate nonsense so somehow we want to be able to say like well if you find yourself you know a little bit off the the the path you should, that's what we're trying to do. We want to make sure that people know what to do if they find themselves in that situation. We're not trying to make people feel bad about themselves, we're just trying to help them out. be able to find your way back to be as as robust as possible to any kind of use case and also when you generate things you don't want to you know go off the road. You want to be able tofind your wayback as as much as you can. You don't know where you are, you want to know how to get back to where you came from. That's a big part of being able to work with a mobile phone. It's also a part of how you get around the world. possible to be as useful as possible okay so uh these are the the the three different things we want to address right what's good and bad dialogue. We want to be more robust and learn to solve correct and this is one of the things we're trying to address. We don't want to. be that greedy and we don't wants to be. that greedy. and this and this are the things that we're working on right now to make sure we're not that greedy or greedy. is where we're going to do reinforcement learning from Human feedback uh and that's what open AI does on chtp. "This was very very hyped for a long time but now people talk less about it uh okay so what do we do well we have a great model," he says. "We have a good model and we're trying to get it to learn from human feedback" "It's a very exciting time for open AI," he adds. "I think it's going to change the world" that's been fine tune on dialogue and it's able to generate really good answers still to different prompts so uh we're going to run this model now on a collection of prompts and we're Going to generate four we'reGoing to sample four prompts for any as four answers to any prompt that we have so let's say we have you know a million prompts that we found online now. We run our model four times on each prompt with different random seeds we we sample uh four. That's how we get the answers. different answers so now we have 1 million prompts with four uh candidate answers okay and then we're going to say that we're pretty rich so we're Going to pay people to actual human beings to label these they're Going To rank this this uh prompt or the answers that were given to us. So now we've got 1 million questions with four candidate answers and we have four candidates to choose from. We're going To rank these and we are going to pay them to label them. So we have 4 candidates to pick from and we're gonna rank them. these models produce to these prompts so we're going to pay pay actual human beings to score them and say are they good or not. "Human beings are very very expensive H so we are going to paid pay actual humans to score these models," he says. "They're going. to rank they'reGoing to rank the the quality of these outputs okay but again human beings arevery very expensive" "We are paying people to score the models and to rate them," he adds. "And we are paying them to rate these models" we don't want to use them too much so here again we're going to go to reinforcement learning and say to deep learning in Ai and say well we now have 1 million prompts with four ranked answers but why don't we train now a AI model a new AI. We're now going to use reinforcement learning to train a new Ai model. We are now using reinforcement learning in a new way to train an AI model. The new AI model is called deep learning. It is based on deep learning, a type of reinforcement learning. We will be using it to train new AI models. model basically to simulate a human being assessing assessing the quality and ranking these prompts um so um we're going to now take this uh model this robot basically right to look at a prompt and generate an answer and then and then it tries to uh predict the score. The robot is called 'RobotBot' and it was developed by the University of California, San Diego. It is based on a computer program called RobotBot that was developed in the 1990s. It was designed to be able to answer questions and answer them in the style of a human. that a human being would give to this uh answer this prompt right so just learn to imitate human beings ranking these answers okay why is this good well it's good because now we can basically uh Rank and score answers as much as as much of we want because we can now rank answers and score them as well as we want. That's the way to do it. That is how we rank answers. We can basically Rank and Score answers. That’s how we score answers. the computer is very very cheap so we can scale this to as many settings as we want um right so this is this is much much cheaper okay so what do we have well we have a robot or AI model here that's able to take a prompt and take a command. The robot is then able to respond to a prompt by taking a command and responding to the prompt in a similar way to a human. It then takes a command to respond in the same way to the next prompt. an answer and then give it a score like let's say between one and five and say how good is this so what we solve now is that we we know what's good or bad dialogue because a robot or a computer has learned to imitate human beings that clearly know what is good and byad dialogue. So now we can run this robot on any prompt and answer and get a score of how good this answer is so now we suddenly have at least some insights around what's going on. good or bad dialogue so we've solved that okay and the last two problems we are going to solve by using reinforcement learning. What is reinforcement learning well we talked about this a little bit before but uh something is very important and characteristics of reinforcement learning is this. Good or bad Dialogue is a problem that can be solved by positive reinforcement learning, which is the use of negative reinforcement to teach people to be more positive. This is a way of teaching people that positive reinforcement is a good way to deal with bad dialogue. delayed feedback so uh in reinforcement learning we're going to have our our starting point of a really good model. But we're also going to allow it to start generating things right. It generates a word puts it in its own input and it reruns itself so it becomes a model. It's going to generate a word and put it into its own output and it will then rerun itself. It will then be able to generate other words. It is going to be a model that can generate other things. longer longer sequence one over at a time so we start off with this I and we now have a probability distribution. We decide what to go for next and we go with went and then we have a few options again and we we take a next step. We then go on to the next step in the sequence. We now have the probability distribution and we decide what we want to do next. We go on and on until we reach the end of the sequence and then the next stage. two and again there's no at this point there is no feedback we don't know if we're doing a good job or not before we had instant feedback because we had a Target and we could learn to do better here here. There is no instant feedback so we're.doing a better job here here and we're learning to do it better. We don't have instant feedback and that's why we're trying to get better at it. We're doing better at doing it better at Target. Two and a half. on our own and only when we you know we reach some uh predefined token like a pier for example then we stop. Then we give uh our sequence that we produced to this robot and then it tell robot like hey is this good or bad so only so we know what we're doing and how we're going to do it. We don't want to do anything that we don't have to do. We just want to be able to do what we want with the data we have at the time. at the very end when we're like Hey we're done we give it to the robot and he scores it then we get the feedback okay and why is thisWhy is this difficult well it's difficult because let's say we do this again so I mean when we produced it was a lot of work. But we're happy with the results and we're looking forward to seeing what the future holds for us. We're going to continue to work on this project. We've got a lot more to do. I went to a walk period I mean uh at least it's a pretty good sentence it's like medium score at least. But let's say we now generate I went to lip I row row period. I mean that's not not a very good sentence. It doesn't make any sense. It's not a good sentence at all. It makes no sense. That's a very bad sentence. I'm not sure what to do with it. I don't know what to say. I just want to go home. basically it's is a very very bad score uh but you know a big part of reinforcement learning now is how do you make sense of this information. You have two signals you start off doing the the same Step at the first step and then they diverge what would happen if you did the same thing at the second step. It's a bad score but it's a good score. It means you've learned how to deal with the situation. It doesn't mean you're going to do it all the time. It just means you're learning how to cope. actually caus one sentence to be better than other one right how do you in incorporate this this this delayed feedback to actually learn to generate good sentences um because we I mean we don't really know right what what what did CA us to make better that's what reinforcement is is. "Reinforcement is is a way for us to learn how to make our sentences better" "Reaction is the way we learn to learn to make good sentences" "We don't know what what we did to make a sentence better. We don't actually know what it is that made it better" is about like how do you figure out what actually helps you reach your goal and optimizing your score function even if it's delayed um okay so another thing in doing this that's very very important it's exploration versus exploitation so let's say now basically that our model has seen that our goal has been reached. We want to make sure that we don't repeat the mistakes that we've made in the past. We don't want to repeat those mistakes in the future. We would like to show that we have learned from our mistakes and that we can achieve our goals. these two different cases and have received two feedbacks right and in one of these you will got a pretty good score. In one uh uh you know in the lower here you got apretty bad score. So let's say we rerun this model again and it it was a good score in one case. It was a bad score in the other case. And so on and so on until we got to the end of the test. And then we did it again. And it was the same. went from I to went and then it let's say it kind of it remembers a little bit what we've seen so far. Then it can say either here we can uh say we can just try to be kind of greedy right and exploit what we have seen. So it can do either one of those things. It can do both of them at the same time. And then it can decide which one it wants to do. And it can then do either of those two things at once. far so if we go down this path to I went to a walk then at least we know that going to do a decent job and better than this alternative that we've seen. So then basically that means that we just exploit the information that we have seen so far. That's what we're trying to do. We're going to try to do the best we can with the information we have at our disposal. We'll see how it goes over the course of the next few days. and do the best based on the current knowledge that we've received right but the problem with this is that if we do this you know we're not going to see anything new. We're just going to explore and explore the the things that already have received feedback on they. "We're just trying to do our best," he says. "It's not about doing something new, it's about doing the best we can do at the time we're given" "We have a lot of work to do," he adds, "but we're going to try our best" we already know St pretty well but we're not we're never going to do really good to do better. We're just only going to explore the sequence that's we already have seen H and this of course is not good. If you start exploring a different rout for. St. then you're not going to be able to do a good job of getting to the end of the story. That's what we're trying to do here. We want to finish the story in the best way we can. example you might again find a much better solution that's much more optimal. You know you want to explore your your space to be able to do better than you've done before and see and see parts of the data you you might have missed in the past. If you have a problem with a solution, try to find a way to make it even better by adding more data to the solution. For more information on how to get started on a project like this, visit www.cnn.com/getinvolved. haven't seen before to get more you know useful feedback back and and and scores from the robots um and also what I think is quite important to to emphasize here is that and this exploration uh cannot be completely random right let's assume that you would just uh generate a set of results that you'd like to get back from the robot. It's important to make sure that the results are not completely random, and that they're not just random random results that are generated by the robot at random. a sequence you know of 50 random words I mean that would be completely nonsense. You would you wouldn't be able to get good feedback on this at all right you just be complete nonsense and random and you would not be able. to get any useful feedback and. I mean you would just be. completely nonsense andRandom and you just would be. complete nonsense. and random. You just would. be.completely nonsense andrandom and you'd not be. able to. get any good feedback at all. it would be very very hard to improve so in order to generate you know this exploration you you know you want to be a do a very very targeted exploration around language is still kind of make sense. So the robot gives you good feedback and you actually can. You actually can do it. It's a very fun way to try and learn more about the language, I think it's a great way to start. I'm very excited to see what the future has in store for us. start you know making progress so that's also why you uh open is able to use reinforcement learnings because they have a really good model and St language already. "They're only really exploring the fringes of the knowledge this model model already has so like they basically only explore," he says. "That's why they're able to do what they're doing. It's like they're trying to do something that's already been done before," he adds. "It's just a way for them to do it." good prompt or good answers but they still do some exploration there but they're not doing a random exploration they're using the current knowledge to do an effective exploration of the space. okay um so this now the reinforcement is is forced to balance exploration versus exploitation to optimize delay. Good answers are better than bad answers. Good questions are better answers than bad questions. Good prompts are better questions than bad ones. good answers are more likely to get a good answer. Good prompt is better than good questions. gratification actually leads to very non- GRE and and independent robustness. These are the consequence of applying reinforcement learning where you only get feedback at the very end. There's less you know supervision right you're more on your own and you have to H deal with an uncertainty. It's a very different way of looking at the world, and it's very different to the way we think about the world in the U.S. and around the world. It is very different from what we think of as 'the West' or 'the East' of not having constant feedback you have to figure out things by yourself which leads to you being more robust. Also again in reinforcement learn here the only thing we care about is the signal at the end so we don't care about making the best next step we take. We just want to make sure we get the right signal. We don't want to get the wrong signal, we just want the best signal. That's what we're trying to achieve here. We want to be able to make the best signals we can. care about optimizing the whole output so we're now addressing these things and somehow this corresponds. You know if you raise a child for example h a lot of children will do better if you give them some space on their own to figure out things as well and not not.care about optimize the entire output. It's a good thing we're addressing these issues because it's going to make a big difference in the future of the company. It'll be great to see how this all works out in the long-term. just constantly doting on them H it leads to kind of more robust uh people Okay so we've solved our problems uh we used human I mean we actually paid human beings for label data which is not maybe that like goes against our principles here but but but uh we uh we paid human being for labelData.com data. We used human people for label Data. We actually paidhuman beings forlabel data. It leads to a more robust system. We use human beings to label data. still did it because open AI is Rich so they paid people to label things but uh then they want you know they didn't want to spend too much money so they created an AI to replicate the the job of the human beings H but then at least they did it. Still did it so they can say they did because openAI is Rich. still did it as a joke. Still do it becauseopen AI isRich so they Paid people to labels things but then they created a AI to do the job for them. got a you know robot or computer model and now is able to say what's good or bad dialogue okay we still want you know we still wants to be uh optimize you we don't want to be too greedy we want to optimize the the complete output and we. want to make sure we're getting the best out of it. We want to get the best possible experience for our customers. We don't just want to give them the best experience for their money. We also want to provide them with the best experiences for their lives. want to be more robust and and and be able to self correct so we use reinforcement learning to optimize and make this robot happy so we've we've now addressed all these things and and we got an even better model okay but now we have an evenbetter model. Now we have a better model and we're going to use it to make more robots. We've got a lot of work to do but we're looking forward to it. We're working on a new version of the robot that's going to be even more robust. but we already had a good model to start with so why stop here right why don't we just use this model now to uh generate uh you know for each prompt generate uh new answers that are even uh better answers and then you give these answers now to the audience. You know, for example, if you were to say, 'I want to go to the beach' you would say 'Yes, I would like to visit the beach.' And you would go there and you would have a great time. And so on. human beings to score them right and train a new robot to imitate this scoring and then you use reinforcement learning to train on this to get an even better model right. It's not nothing stopping you for just rerunning this whole cycle again and someh this makes sense as it's not something you do very often in the real world. I'm not sure why this is the case but it seems to make sense to me. I don't know why it's being used in this way but I think it's a good idea. well because if you now have a better model you kind of want to you want to go to the human beings and get more feedback that's more relevant to this model. "This model now is is doing better than the previous one so you wants to have feedback," he says. "You want to have more feedback from the people that you're trying to reach out to," he adds. "It's a good way to get feedback from people who are trying to help you improve your model" that's maximally useful for your current ability is right if you're a kid learning to write or whatever you know you want to get more sophisticated feedback as you become better at it. That's exactly the same thing we can do this models we can we can just run this. We can run this models and get the same feedback as if we were doing it in a real-life situation. That is exactly what we do. We just run the models and we get the feedback as we go. step uh and do it all again and you know you can done you can do this as much as you want of course uh maybe with some uh you know decreasing returns I don't know exactly. I think open a runs this two or three times um okay cool. OK cool, cool, I think I'm going to take a break. I'll be back in a minute or two. I'm sorry, I've got to go to the bathroom. I've been up for a few minutes. so to summarize what's the big fuss well just predict the next word based on previous words that's basically it uh who knew that this is going to work at the scale that uh you know and reach reach this kind of intelligence that we're seeing uh was quite hard. So to summarizeWhat's theBigFuss? Well just Predict the Next Word based on Previous Words. That's Basically It. So To Summarize What's the BigFuss Well Just predict the Next word based upon Previous Words That's basically It. uh but it did uh Transformers allows us to leverage more data and train quickly because we can paralyze paralyze all these steps in during training and uh then uh when we've done this we have a really really sophisticated model. We spent 99% of our time in computer and we spent 99 per cent of ourtime in computer. We can't do this with other people. We have to be able to do it with ourselves. It's a very, very complex problem. We don't know how to solve it. But we can try. on on this preaching this Transformer to predict the next word based on previous words. Then we can adjust things to make it a little bit more nice to interact with if you're a human being. So you fine- tune on some data set around human dialogue and then you then you can use it for other things. On this preaching on this preach this Transforming to predictions the next words based on prior words. then we can use this to use other things for other projects. on on this preacher on this Trans transformer to predict. the nextword based on past words. incorporate human feedback and reinforcement learning to get even uh a little bit more of performance out of this okay um again uh obviously self super learning in Foundation models are at the core of of CHP um uh also maybe I mean uh generative a versus self-supervised learning U. S. A. U.S. C.E.O. John Defterios is the founder and chief executive officer of Foundation Models. Foundation models are at the core of CHP. maybe it's useful to kind of um I mean the difference between entive Ai and self super learning is not clear uh and people use it uh typically I mean there is more the difference is more clear when it comes to uh the research space but people say that. Maybe it's Useful to Kind of Um I mean  I don't know if that's a good thing to say. I mean I don't think it's a very good thing at all. I'm not sure what to say about it. CH P generative AI or ex Strang uses s super learning to generate output right to create something. "Generative AI somehow puts more emphasis on the ability to be able to generate some output right" "Catp knows both how to read and to write they are very related skills" "Ex Strang" uses super learning in a similar way to catp to learn how to write and to read. "ExStrang" is the name of a series of books about the life of a man called Strang. and self Suess learning care more about both aspects somehow somehow. Next time we will talk about uh we'll do a similar Deep dive into stable diffusion there will be self supervised learning and Foundation mod an AI but uh I mean this is terminology uh okay awesome so next time we'll talk about  deep dive into stable diffusion and self supervised learning and foundation mod an AI but we will be talking about both. think it's going to be slightly more conceptually interesting um so should be a lot of fun and yes please go to the website for more information Etc and if you have any questions feel free yeah can I something can I assume that probability on that after there changes? I'm not sure what the answer is. I'm just going to say that I'm going to try to make it a little bit more interesting. I don't know if it will be as exciting as the first one. based on the subject of the totally yes yes yes great question okay so the question is uh does the distribution or the probability of over the next words given the previous words change given the prompt yes. Al like yes the more the longer sequence and the more context or the longer prompt you have the more specific prompt the more Peak your distribution will be because the more you have context, the more your distribution is likely to be higher. So that's the whole point right because you train to generate the distribution. information the model have around your specific context in use case the more it knows how to collapse into a space that you want to know about right. So if you just say you know if you start sampling the model with no prompt it's going to generate so the more information it has about you the better the model can work for you. The more information the model has around you the more likely it is to be able to do what you want it to do for you in a given situation. most uh common starting points on the Internet or something just like random text but if you start saying like hey I'm interested in history this this this then it's going to say like well okay I know history I'm going to generate things from Wikipedia history blah blah and so on. If you start with something like "I want to learn more about the history of the U.S. and I want to know what happened in World War II" then it will generate a list of things to learn about. it's going to be able to collapse and create distribution that's much more targeted uh so is if it doesn't have data run a context or about you and your interest as a person it it won't be able. to tailor to you right they cannot create Magic out of. right they can't create Magic. out of it. It can't be created out of a set of rules. it can only be made up of rules that it can be applied to people in a certain way. It has to be based on data. thin air it can only do the best of The Prompt and knowledge has so far. Data is so important to have around you right and good prompt Etc and it's also why this prompt engineering to be able to like how do you create the best. It can only be done with the knowledge you have so far and the data that you have. It's why data is so crucial and why prompt engineering is important. It is also why it is important to know how to use the best data to create a prompt. prompts to get what you want um yes so um just to like double check so for the reinforcement learn with even feed back um the the part that the robot that learns to rate the responses that's supervised learning because yes it is supervised learning and then the actual. learning and the responses are based on the responses given by the robot. The robot is then able to learn how to respond to different situations. It is called a 'robotic robot' and was developed by the University of California at Berkeley. model generating is reinforc learning because itates the response yeah that's a great Point actually okay so yes in this what we talk about right now basically we had a few models involved. We talk about reinforcement learning supervised learning and self-supervised learning right so exactly the robot that you're talking about is a robot that is learning to respond to a question. It's a very complex process. It takes a long time to train a robot to learn how to answer questions. It also takes a lot of time to teach a robot how to respond. that just H try to replicate the the human beings putting scores on the um uh answers that we generated prompts yes. That's train using self uh supervised learning right because you have these labels now that you want to replicate so it'sTrain using supervis learning but you know you know what I'm talking about. That just H tries to replicate. the the Human beings putting Scores on theUm uh answers. that we generate prompts yes that's trainUsingSelfUhSupervisedLearning.com. how it actually works that it that that model also leverages the pre-train model that we have so it's as a starting point so again like you can see now as well that like supervised learning where you have limited amount of labels ET only becomes really useful when you have a lot of labels to work with. It's a very powerful tool. It can be used in a variety of ways. It is not just for teaching people how to read books. It also can be very useful in other areas. have a starting point from superv learning right so you already have a world model that you can leverage. Then you you can use your your labels more efficiently right so the first step of preaching the next word based on previous words that's selfs supervised learning right that's a self-supervised learning right. That's self- supervised learning. Right that's an example of self- superv learning. It's a way of teaching yourself how to use your world model. It can be a way to teach yourself about the world. "We already we have a starting point already so it's easier and then we do reinforcement learning right so we try to generate so we has a model do reinforcementlearning but we already do 99% of the work," he says. "Then we have supervised learning what we learn from the human beings but we have already learned that from human beings so that's easier," he adds. "We have a model that does reinforcement learning but we also do supervised learning as well so we have to generate that too" again the starting point of the reinforcement learning is this self-supervised learning you know model that's trained already using self Su learning so Al that's what's so fascinating with self suus learning because now it's the building block that makes all other Technologies in AI actually fruitful. yes wondering language yes wondering about language. Yes wondering about the meaning of the word "wonderful" yes wondering why we use the word 'wonder' when we use it. Yes, I'm thinking about how to say "fascinating" models are they actually models based on how a child learn is it any is it related to any sort of like cognitive science type of like learning because I saw like Transformer that's kind of how a normal person learns right when you're learning a new content you to be able to learn a new way. I don't know if that's true but that's how I think it is. I think that that's the way a normal child learns. I'm not sure if it's true. Rel it with everything else you know yeah so I'm just I mean this yeah okay uh so the question is basically is the Transformer inspired by research about how we our kids learn right how the brain works. I mean you're going to find a lot of work yeah I mean I'm not going to lie to you. I'm going to be honest with you. It's going to take a long time to get to the bottom of this. I don't know how long it will take but I'm sure we'll get there. around you know making those connections and uh then there's a huge debate in in like the Deep Learning Community is that is that actually true is it kind of wishful thinking and in hindsight we make this connection right and I mean and a lot of people throw around that it could be true. I mean that's a big debate in the deep learning community. It's like, is that really true or is that kind of Wishful thinking? I don't know. I'm not sure if it's true or not. I do too right you for around similarities to how our own brain works and some people are like oh that's not you know we have to be very very careful because we kind of compare. I think that there's strong connections. We're anthropomorphizing AI in some sense in some way, he says. "I think that we're very close to having an artificial intelligence that is like us" he adds. "We have a very long way to go before we have a fully functioning AI" right but like what came first I actually think that uh people have some intuition they tinker and they try things and then suddenly something work and they work on intuition and actually theory is very much hindsight so some engineer played around with things actually theer was from Google. Right but like like what comes first I think thatuh people haveSome intuition theyTinker and try things. And then suddenlySomething work. And that's what they do. And they're like, "Oh my God, this is amazing" right so we build a Transformer they play around things they want to accomplish things and then like like oh this makes sense this is probably how I would do in just have some intuition and then it works. When it works the theorists come in and say like like when it works and then when it worked the theorists came in and said like like this is how it would have worked in this situation. It's like a game of 'Where are we going to go next? What are we doing next?' well this reminds me of this and this uh so there was definitely no strong like this is how kids learn and then we replicate that it the more I would say the more likely description of how the Transformer came about was just Engineers tinkering out and trying things. "There was definitelyno strong likethis is how children learn" "There were definitely noStrong like this was how kids learned" "We were just trying things out," he said. "I was just trying out different things. We were just tinkering" and then it it just like oh this works now and they are not prbly completely conscious themselves about what they were inspired by you know so does that mean that we not have collaborations neuroscientist yeah it means that the collaboration with neuroscientists in deep learning is is quite. quite. And then it is just like Oh this works and they're like Oh that's what I want to do. And it's like oh that's the way to do it. It's just like that. quite rare actually yeah that's a good question yeah it's it's I would say it's quite rare yeah next based on I want to yeah um yeah like the question is why do we only PRI the last word based on previous words why don't we just take a random. Why don't you take the question and go with the answer you get? That's what I'm going to do. I'm just going to go with it and see what happens. That's the way I like it. word mask it and try to predict it based on the surrounding words um yeah the answer to that is we used to do that way and it works better uh but due to engineering you can Bas basically kind Transformer you can maybe this can be like a homework for you but if you look at Transformer Works uh if you mask a word uh then you will like then you'll only um okay I if you do this you know if only predict the last word based on previous words you can make this okay attention. so you can okay this maybe any like you have this tensor product you can Define this m Matrix High dimensional matrix product in a way that's efficient so you can basically uh in a single goal predict the target like you can you know you have a sentence you can. So you can essentially in asingle goal predict. the target. You can basically in aSingle Goal predict the targets. You know you can have a single Goal. You've got a sentence. You're going to have a goal. You have a target. can you can run the whole sentence and predict the next word based on previous word for all the words in a single goal if you do um Mass language modeling you cannot do that. Then you basically have a full attention you can attend forward and backwards but you can't do it backwards. If you can do it you have the attention you need to be able to focus on the task at hand. You can do that by focusing on the goal and paying attention to the words you are trying to achieve. can only run the targets that you you masked uh so this means that uh in terms of in like in Terms of signal to the model if you have a a sequence of uh length 2,000 you have2,000 signals for auto regressive because you can use auto regression. You can only use auto Regressive to run targets that are masked in a certain way. You have to be able to do this in a way that doesn't cause the model to go into auto Regression mode. each Target by itself but if you mask then you maybe mask like 15% of the words and then it's it's only like that's only 50% then in terms of getting that feedback uh so it just in end of the day it means that uh when you try this, when you're trying to get feedback, you're only getting 50% of what you need to get the feedback you're looking for. So it just means that when you trying this, it doesn't work as well as you think. empirically doing it aut agressively and this trick to be able to use the each you know each word is a target itself just leads to better performance at this task of generating export based on previous words. uh so yeah so it's I guess it's engineering empirical okay it's. it's a trick. It's a little bit of a trick to get it to do what it's supposed to do, but it's not a big deal, it's just a way to get better performance. not as in an Ideal World if you have unless compute you would have you know you would rather do it in a clean modeling way uh. But now because of these restrictions and Computing stuff you just try something like hey this a speed up here and they see it and it's not as good as it used to be. It's a lot of work to do it that way. But it's better than doing it the other way around. I think that's the way to go. I don't know if that's true. oh actually works better given the same amount of compute yeah does that give you some sense of answer we can talk more about it offline as well. What are like the main challenges of these models nowadays and are there other like language models that are like better but actually work better? We'll be talking more about this offline as we go on to the next stage of the conference in New York City in the next few days. We'll also be talking about how to use these models in the future. they do like more expensive than or like this is like the best um yeah I think. One challenge is to uh make them behave like we want them to meaning like okay we want. I think that's one of the challenges of these large Lang models well uh one challenge of theselarge Lang models is to make them behaving like we wanted them to mean like okay they want to. I believe that's the challenge of this generation of Lang models. I'm looking forward to seeing what the future holds for us. them to be you know a little bit politically correct at least right and and and kind to us we don't want to make up things right we want them to be. I mean we want to able to rely on it as much as possible and if they if they don't, we'll try to find a way to get them to do it for us. We want to be able to depend on it. We don't just want it to be the way it is. We just want to have it. you like ask a factual question and they give us something that's wrong and they're confident about it maybe that's bad so like I think what we're starting to see is that they um are very humanik even it's in its mistake they're like well they're somehow they're biased are are they right? Are they biased are they biased? Are we biased are we biased? Is that what they think we think of them? Do they think that we are biased? Do we think that they are biased or are we? we going to talk about this like they're biased and they have stereotypes around things uh they also like we do. I mean they suffer from wishful thinking and some type of imagination where they rather be you know make you happy than being completely truthful. So like these like these, we're going to be honest with each other. We're not going to lie to you. We don't lie to each other, we just don't tell you the truth. We just say what we think. are things that you then somehow have to balance uh yeah and then so that's like some problems immediately arrived and then what else people definitely working on and open is working on autonomous agents. How can you get planning into this so if you if you now you. If you're working on an autonomous agent and you want to do something like that, how do you do it? How do you plan for it? What do you need to do to make it happen? What are the steps you take to get started? know let the model be able to uh generate some input generate some output and then you know digest that input generate more output and stuff like in this kind of planning step uh it becomes has much much better abilities. So you can basically instead instead of using a model to do all the work, you can use it to do some of the planning. So that's what we're doing here. We're trying to make a model that can do a lot of different things. So we're going to try to do that in the future. of having a single go at your prompt if you can you know have a few tries and improve on itself and only get give you the the response after it's you know done this internal uh iterative uh um processing it will be to do much better and then do it again. If you can't do that, then you're going to have to do a lot of work to get it right the first time. That's the way to do it. It's a lot harder than it looks. like again if you throw in some tools there it can search internet to get more information you can retrieve information and be able to do do much much more. "This kind of planning is uh definitely something that people know that these models you know very very helpful," he says. "It's very helpful. It's very, very helpful" he adds. "I mean, it's definitely a tool that people should have in their toolbox," he adds, "because it can do a lot more than just search the internet" of course it's more expensive again because it has to run for longer and stuff but it's very very useful so then what you have is like open a talk about a star and stuff. So these are reinforcement learning techniques of how to do planning well so how to. how to plan well so that you can get the best out of your time. It's a lot of work but it pays off in the long run. It can be very, very useful. I think it's a very useful tool. incorporate planning is something that people talk about a lot and then of course multimodalities so it's not hard to see that this idea of predicting next word based on previous words corresponds really well to videos. Just to kind of predict the next frame based on previously frames uh. It's not easy to see, but it's a really good idea. It works really well for videos just to predict thenext frame based upon previous frames uh, it's really good for videos to do that. and why aren't people doing it well because it's videos are suddenly a next level in terms of compute what it needs because they have tons of videos a frame is very expensive because it is a high dimensional picture or image uh but clearly you can learn a lot about how to do it well. It's videos that are the next level because they are so easy to compute. It can be a very expensive process to do well but it can be done very well if you're good at it. the World by looking at videos right you can even sort of understand how human beings work even better. You can see people being upset or sad or happy whatever right in in a video and start picking these cues up and you can connect the vision part to the reality part of the human experience. The World is a series of short stories by British author David Walliams. The first, The World, is published by Simon & Schuster at £12.99. The second, The Second, is released at £14.99 and is published at £16.99 by Simon and Schuster. the text part and get a multimodality model that's able to do both in a really really sophisticated way uh also something that I think these these people are working on all right thank you thank you for your time. The next step is to get a model that can do both the text and the video part in a way that's really sophisticated, I think that's something that we're working on right now. Thank you for all your time and I'll see you next week.