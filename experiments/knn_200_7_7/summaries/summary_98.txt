presenting okay share your screen that's what I'm doing oh you are okay hopefully it is yeah stop sharing yeah you should be sharing my screen under your camera until I can decide if I click on slideshow this is still show my camera uh it does I guess I can minimize it do screen sharing are you recording too yeah great baseball back yeah I mean it's my first time giving my lecture so I'm as good as I can be do you want that cheers I mean I have to write something to hear me okay okay you know that it works. Deep learning is a type of machine learning that combines feature extraction and output prediction. It allows you to automate the entire process from feature extraction to training. Post-processing is a way to make the process of learning more efficient. Deep learning has been used in the field of Vision and Vision Vision is not as used as machine learning is in other areas. It gives you some context as to why deep learning is being used and why it's different from classical machine learning. It's a way of taking the entire machine learning process off of one step and combining it with another. Getting these features is pretty easy because you can just take each row directly right or you can also maybe take a column depending on however the data is arranged in this tabular format. What if your input is something complex like it can be text audio images right how do you extract all the relevant features from such a complex input? Since this is a CV class I'm going to go over the CV example and turns out that there are special feature extractors for images so this is sort of what classical machine classical CV look like. age right over here we have a neural network that whose goal is to sort of predict something about a car. As you go deeper down the network is trying to become more and more abstract. depth refines representations you start with course information like edges and you go all the way down to like find information like this mental model of a car so I know that this was a lot of information does anyone have any questions about any of the any of this uh in case nudge I'm gonna pass the my country Verona who will talk about transfer learning. Pre-trained networks are trained on very very large data sets and oftentimes again more data means better representations so if we're only given a little data using some sort of pre-trained network can be a great idea. There are also a lot of free trained networks that we can use immediately so you can use models that haven't trained on largeData sets. Pre-compute networks can also be used to fine-tune the whole pre- trained network rather dimensional 28 by 28 dimensional image. Transforming learning is especially huge in NLP. Without transfer learning we're basically trying to learn two separate tasks and train our model separately. Transfer learning allows us to apply the knowledge that we've learned from one pre-trained network and apply that to the second task. We've talked about two main techniques for doing that freezing some layers and fine-tuning our our Patriot Network. We'll go into some details and examples of this in action but especially in natural language where there's some semantic meaning so you already know the meaning. have more confidence that we won't overfit if we fine-tune so we've briefly talked about overfitting in the past and this is a topic that will continue to come up when we train um uh an hour and so um on the other hand if we have a smaller data set even though it's similar to the original model it's not a good idea sometimes to fine tune because you can definitely overfit. In practice it's mostly beneficial toinitial your weights from the pre-trained patient model okay so um before we sort of get into embeddings um just some practical advice. of your pre-trained models um think about what sort of tasks it was sort of trying to accomplish in the beginning um the data that it was trained on and then also consider your learning rates so we'll talk a little bit more about learning rates and greater depth again next week when we talk about some of these more particular models. No network embeddings are useful because they can reduce the dimensionality of your categorical variables for example and meaningfully represent them in the transform space. In this photo example they've decided to use teasney for thedimensionality reduction so taking the embedding Vector dimensions and mapping them to a 2d space in this case. part of our neural network for our Target task so this sort of allows us to get in a bedding that's nicely customized for our particular task but it may take longer than training the embedding separately. The idea here is that we can basically reuse our embeddings for other tasks again embeddins they're sort of just like what we talked about earlier the broad ideas that we're trying to represent our data in meaningful ways. So here's sort of an example of how you can sort of see how there's different ways like we have um one hot Target probabilities. of words once you've pre-trained you can understand syntax so you can imagine in tasks for natural language processing it takes a lot of time if you want to retrain an entire network so using pre- trained networks and using transfer learning is a really really good idea. Not only does that apply to NLP it applies to almost other domains every other domain xcp yeah okay so we will transition to the next part of the lecture which is going to be on self-supervised free training. Even without any labels we can still learn something meaningful about the structure of the raw data. We can do unsupervised representation learning and indeed we can. The valency provides representation learning is done is to play to something called self-supervised learning. If you want to label a billion images you would have to hire manual labor. The process is called supervised learning like the name suggests and some examples of supervised learning can be your typical classification problem like the one that we just showed where you classify digits. hope is that if the model can learn to predict the original order like what patch goes in the let's say the top left corner versus the top right corner it is trying to learn something meaningful about the image. It's going to learn that an image can be made up of different parts and those parts are going to be related to each other. There are some more technical details on the slides um I won't go into those but something that the authors actually did I actually go mention this is when they sample the patches and they divide it into a grid instead of taking the grid directly they actually generate each patch a bit. this can be done is you could predict a word from its surrounding context so say if you have the sentence the dog with the man you could try to predict the word bed from dog and dog because a dog should kind of imply that the word bit is associated with it so this approach is called a continuous bag of words model. There are many other ways to train virtual back models one example is um skip Ram so instead of predicting aword from a context you instead predict the context from a word so you like kind of like flip the model upside down.