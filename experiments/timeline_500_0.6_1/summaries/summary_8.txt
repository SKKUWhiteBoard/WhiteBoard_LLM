The 24th lecture of 188, before last one. The idea behind these two lectures is to look at advanced applications, where we have covered a good amount of the material in the ideas behind those applications. We will not quiz you on these application lectures, on the final exam, or anything like that. It's more meant to give you more perspective rather than extra study materials for the final. So far, I've looked at foundational methods for search, for acting adversarial environments, for learning to do new things, and for dealing with uncertainty, noisy information. DeepMind's AlphaGo can predict who will win from a certain situation in Go. The game is much harder to solve than chess or Tic-Tac-Toe. The branching factor in Go is much larger than in chess. A neural network can be trained to evaluate the value of a position. This gives you a lot of data that can be used to decide who is likely to win in a given situation. The first thing being learned is a policy network, which is deciding which moves to play against. even longer. It could be that by using human knowledge, you're in some kind of based enough attraction. I don't know if that would be the case or not, but that's a possibility. It also depends on how much randomness you have in your exploration. If you have enough randomness, then initialization will have much less effect than if you have limited randomness. How good can this system get? Is it even possible to learn good Go players by just playing against yourself? That's something people did not have an answer to until this experiment was run. We have four control channels, two in each joystick. A collective is the action for the main rotor collective page. It's the average angle of attack as the blade goes through the air. The tail rotor has a variable pitch also, and that pitch allows you to modulate how much thrust you get from the tail rotor. If you want to fly forward, you've got to rotate nose-down and then you can accelerate vertically in a helicopter frame. But vertically, the helicopter frame will now be partially forward and partially up, and up compensates for gravity. able to look ahead only two seconds, rather than needing to look beyond that. A value function tells us, OK, how good is it to end up here? We also have a reward at each time tick. The fastest we flew this helicopter was close to 55 miles per hour, so almost highway speeds. The algorithm's only this big, so it's pretty fast for something of this size. And then once you've recovered, let it learn on its own and let it switch to recovery mode. autonomous car drive a desert race. It's a time trial type race. There's no other cars that you have to overtake, but you're kind of on your own and try to do it as fast as possible, 150 mils off road. Well, it's on a road, but it's this kind of road that is not like a regular road. So it's pretty hard to distinguish road from non-road, and if you steer off the road, you might lose your car if you go down some kind of ravine. Four cars finished the 150-mile Berkeley autonomous car race in 2005. What goes onto the cars? There is IMU, like right on a helicopter, a lot of computers. Lasers, where you shoot out laser beams. Cameras, radar, control screen, steering motor. How do you decide with path to follow? Often, your sensor readings will tell you if there might be obstacles or not.. A camera will be better at that than a LIDAR. Somebody needs to tell you what is road, what is not road. The devil is really in the details, in the long tail of special events that can happen when you're driving. In urban environments, there's even more need to recognize, not just road versus not road. A lot of progress has been made this is video from 2013. This was before deep neural networks were heavily used for this kind of thing. It's only getting better to recognize what's in scenes, thanks toDeep neural networks. Instead of classifying into which categories in the image, you would classify each pixel, as to what is in each pixel. That way, you get a semantic segmentation. is between 10 and negative 2 and 10 negative 3 per 1,000 miles of human driving. In green is the Google slash [? wave ?] mode disengagement. It's when the driver decides they want to take control because they don't trust the autonomous system right now to avoid an accident. And we see that it's going down how often that needs to happen, but still a bit removed from where humans are at. Where does this data come from? If you test in California, you have to report this data to the DMV.