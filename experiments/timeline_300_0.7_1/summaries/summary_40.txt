Week 6 of CS224N is now past the halfway point. Today is the day that you have to have done the mid-quarter survey by. If you haven't, this is your last chance to get the half-point for that. Final project proposals are due. We really encourage you to try and hand them in on-time. And then today, delighted to have our first invited speaker. Danqi Chen, one of the foremost researchers in question answering, and she's particularly well known in recent work. here to give this lecture on question answering. The goal of question answering is to build systems that can automatically answer questions posed by humans in our natural language. Question answering, or, let's say QA in short, is one of the earliest NLP tasks, and the early systems can even date back to the 1960s. The question and answer has enabled a lot of really useful real world applications. For example, today today we can answer questions like, "What do worms eat?" and they'll finally retrieve the answer, that's the grass. if you just put your question in a search engine like Google. And those kind of systems are also able to handle more complex questions like how-to questions. So the question is, how can I protect myself from COVID-19? So there isn't really a simple and short answer to this question. So you can see that the system actually returns a very long paragraph, including the best way to prevent illness is to avoid being exposed to this virus. So actually this paragraph is actually a summary from CDC article. So this is also one type of question answering problems. In 2011, this IBM Watson QA system has been shown to beat two national Jeopardy champions. So this is a part of a historical event, at least in the NLP history. The system is built on both the unstructured text and also the structured data. And it's actually, there are really lots of NLP modules that have included.timer. So we know that this class is about deep learning. So today different has completely really transformed the landscape of the question answering systems. Question answering is probably one of those fields that we have seen the most remarkable progress in the last couple of years driven by deep learning. So in this lecture, I will be mostly focusing on the text based, or textual question answering problems. And another class, bigger class of the question answering problem is called visual question answering. So if you have interest in these type of problems, I encourage you to check out those problems, but I'm not going to go into them. Back to the page you came from. dig into these problems today. So next, I'm going to start with a part 2, reading comprehension. So reading comprehension is a basic problem that we want to comprehend a passage of text and answer questions about the content. So basically to answer this question, so you need to find this sentence, like, in 1861, Tesla attended this school where he studied German, arithmetic, and religion, and only German is a language. So this is really just similar to how we humans actually test the reading comprehension test to evaluate how well we actually understand one language. personal subject, Barack Obama. So we want to fill in what is-- fill in this question mark and figure out, OK, where Barack Obama was educated at. So one way to solve this problem is basically trying to convert this relation into a question. So where did Barack Obama graduate from? And taking all of that relevant piece of text and then by applying a reading comprehension problem. Then basically, we can find out-- the correct answer should be Columbia University. That is also the output of this information extraction system. for the final projects, you will need to use this dataset. Stanford Question Answering Dataset is actually a supervised reading comprehension dataset, which consists of 100K annotated passage question and answer triples. So for these datasets-- so the passages is like a single paragraph selected from the English Wikipedia. And the questions are crowd-sourced, basically like from Mechanical Turking. And this kind of a large scale supervised dataset are also very key ingredient for training the effective neural models for reading comprehension. Stanford, so it's called Stanford Question Answering Dataset. Today, after four or five years now, so SQuAD still remains the most popular reading comprehension data set. So the state-of-the-art AER systems still have time to just train or sequence tagger on top of the word. Danqi, one question you might answer is, so if you can do other tasks like named entity recognition or relation extraction by sticking something ontop of BERT and fine tuning for it or do it as a question answering, does one or the other method work better? answer to that. So next, I'm going to talk about how to build neural models for reading comprehension, and in particular, how we can build a model to solve the Stanford Question Answering Dataset. So let's first formulate this problem. So the input of this problem is let's take a context or paragraph. And also we take our question, Q. And the question consists of n tokens q1 to qN. And because the answer has these constraints, the answer must be a section of text in the passage. during that period between 2016 and 2018. So this was a family of models, basically LSTM-based models with attention. And then we need to fine-tunel this model for the reading comprehension task. So I'm going to spend a little bit more time on this part because I know that for the default final project, you will need to implement this model from the scratch. And so I'm just going to briefly tell about how to build this BER-- use the BERT models for theReading comprehension. comprehension problem because they really share a lot of similarities. In the reading comprehension problem, we need to model which words in the passage are most relevant to the question. And if they are relevant to question, so these are also relevant to which set of the question words. So this is a very key important thing that we actually need to models. And that is actually very similar to the machine translation model. But in this reading comprehension Problem, we don't need to really generate anything. So we just take the path into question. BiDAF stands for Bidirectional Attention Flow for Machine Comprehension. It was proposed by Minjoon Seo and other folks in 2017. It remains one of the most popular reading comprehension models and achieved a very good performance at that time, at least on the SQuAD data set. So next, I'm going to just dissect this model layer by layer and talk about what this layer is actually doing and how we can really build this model from the bottom layer to the top layer. care about the generation, so we can just use two bidirectional LSTMs to represent the representations. The next component is our next layer. It's called the attention flow layer. So the idea of attention is trying to capture the interactions between the context and the query. So for each context word, we need to find our alignment because we find which words in the question can be actually aligned with this context word. So this is actually just an intuition of these two types of attention.  query-to-context attention is trying to measure the importance of these context words with respect to some question words. So by taking the max for each row in this matrix, so it's basically trying to see, OK, which question word is actually most relevant to this context word? And then you can just apply your softmax. And then this will give you a probability that OK, what is the probability of this condition i, would be based on the start position of the final answer string. the dot product between w end and this vector, and this can produce all the probability over all the conditions which predict how likely this position will be the end the position of the answer. So by passing the mi to another bedirectional LSTM, their reasoning is that they're trying to capture some kind of dependence between the choice of the start and end. OK, so this model is actually achieved-- like on SQuAD data set, it achieved a 77.3 F1 score. If you remove the context-to-query attention, the performance will drop to 67.7 F1 score. And then if you remove this part, it will drop a 4-point F 1 score. So basically this theory tells us that these kind of attention scores can actually capture the similarity between the question words and the context words. And now here is our attention visualization to show that how this smorgasbord of attention actually can capture the similarities between the questions and the contexts. those negative scores pretty well, yeah. OK, so next, I'm going to talk about BERT, how to use the BERT model to solve this problem. So BERT is basically a deep bidirectional transformer encoder pre-trained on large amounts of text. And it is trained on the two training objectives, including masked language modeling and the next sentence prediction. So, OK, we can actually use BERT for our reading comprehension. So it's actually very easy and very straightforward. BERT models are pre-trained while BiDAF models only builtd on top of the GloVe vectors. Pre-training basically can just change everything and it also gives you a very, very large boost in terms of the performance. And even if you use a stronger pre-training models or modern, like a-- stronger models than the BERT models, they can even lead to better performance on SQuAD. And then finally, if you see even the latest pre- trained language models, including the XLNet or RoBERTa or Albert. These models are either bigger or these models are trained on bigger corpus. BiDAF models essentially are trying to model the interactions between the question and the passage. BERT model essentially, they're trying to use a self-attention on top of the concatenation of question and passage. And actually after the BiDAF cannot. This is also before the BERT can add. So we can see that these two models, you can really just trying to. model the attention between the passage and question. And this is actually was exactly the Bert model is doing. proposed in SpanBERT. Instead of using only the masking of individual words, we propose that we want to mask a contiguous spans of words in the passage. So we are trying to mask out all these possible answer spans from the passage as our training objective. And the second idea proposed in SpanberT is-- because at the end of it, it goes around to predict an answer span. So essentially, we're actually essentially try to predict two endpoints as well as the answer. SpanBERT greatly outperformed Google BERT and other BERT basically across all of the datasets. This number has already exceeded even the human performance on SQuAD. So that really tells us that OK, even if we are not going to increase the model size or increase the data, by designing better pre-training objectives can also go a long way and do a much better job at least in the question answering and the reading comprehension datasets. OK. So so far, we've demonstrated that by using BiDAF model and by using BERT models, we can get a very good performance on the SQuad dataset. The performance of a BiDAF model drops from 75.5 to even like 30%. So for you, like this kind of attack, the performance level just dropped to very low, like 4.8%. So here is another paper that actually just came out in 2020. So there has to be a lot of evidence showing the similar things. So today we compute a very good reading comprehension data set on the individual data sets. But these systems trained on one dataset basically cannot really generalize to other datasets. So it basically really cannot generalize from one dataset to another dataset. Open-domain question answering is a problem that-- so it's different from reading comprehension that we don't assume a given passage. So this problem, there isn't any single passage, so we have to answer questions against a very large collection of documents or even the whole web documents. So if you look at the example of Google example I showed at the beginning, so these techniques will be used. Chris, is there any question I should answer at this point here? I think you can go on. very useful in the practical applications. Open domains is just in contrast to closed domains that deal with questions under a specific domain. So the idea is that let's take a question-- OK, so here, the article is trying to answer questions using a very large collection of documents such as the Wikipedia. So we can just decompose this problem into, as I just mentioned, in our retrieval and the reader component. And finally the reader basically takes the question and takes this type of the passages and finally returns the answer. are 20 million passages in Wikipedia, so it's actually very hard to model this part. The idea is that we can also really just train the retrieval part by using two BERT models using only the question and answer pairs. And this model kind of works really well, and it can largely outperform the traditional IR retrieval models. So here is actually a really nice demo. So again, the database ferries the whole Wikipedia. And you can see that if you ask a question of who tells Harry Potter that he is a wizard in the Harry Potter series, the system had really found out the correct article. answer, which is exactly what you have seen here from the Google example here. So the answer would be the Rubeus Hagrid, who is actually the person who told tells Harry Potter that he's a wizard. OK, I'm going to skip this slide. And then finally, very quick. Some researchers have demonstrated that maybe you don't even need this retrieval stage. If you just use a very large language model, you can also just use all open-domain questions answering. DensePhrases.dense vectors. So what you just need to do is just to do this kind of nearest neighbor search in the answer space. You just encode all the phrases in Wikipedia, encode them using vectors. And by taking a question, you can just encode this question in a single vector. And then we can just do the nearest neighbors search and then it can directly give you the answer. So this could be very fast and they can even run on the CPUs without needing to run a very expensive deep neural network. Danqi can stay for a bit to answer questions, but not forever. Because she doesn't have a Stanford login, we're going to do questions inside Zoom. So if you'd like to ask a question, if you use the raise hand button, we can promote you so that you appear in the regular Zoom window and can just ask questions and see each other. If you hang around and don't leave the Zoom for more than a few minutes, maybe we'll just promote everybody who's still there into people in theregular Zoom for some bits of discussion. showed that if you only use a very few examples, you can also do open-domain question answering pretty well. But this kind of model is huge, like what number? How many parameters I forgot in the GPT Stream model, yeah. So this is my answer, is that if we can leverage a very large and very powerful pre-trained language model, there is a possibility that we can actually do the question answering well with only a small number of examples. And also there are some other promising directions, including unsupervised question answering. Most existing question answering datasets or reading comprehension datasets have been collected from Mechanical Turk. So it is very difficult to avoid some kind of artifact though, like a simple clues or superficial clues. So that's the reason that more specialized models that have been trained very well in one data set, it's very easy to pick up these kind of clues, and is very hard to generalize this kind of thing to another data set. What about the natural questions data set? Doesn't that avoid that objection? Yeah. Natural questions will be much better, but there are some other issues. Real questions that are asked by users. So it kind of avoids this kind of superficial artifact between the question and the passage. But there's some other issues that people like to ask some common questions. So this model basically performs on par with the dense passage retrieval model. But it is actually-- so I skipped one slide. So using a T5 model has a better data retrieval. This actually performed really well. So I will just say dense phrases work basically similar in this block. But compared to this generative model, we're still a few points behind. The goal of this project is trying to ingest all the phrases in the Wikipedia. So these conversations are built using the training set of the question answering datasets. So the model is definitely able to generalize from theTraining set to all the Wikipedia phrases. So it doesn't have to have seen the phrase then. Is Iskind of the intuition behind the dense phrases apart from the answers will probably be in close proximity? And what if the datasets has answers to a specific question like very far from the actual information? Say, the answers to your question may not resided inclose proximity to the words in the question. This is similar to the retrieval or the dense passage retrieval. So you still try to train on passage representation, here is the phrase representation. But the representation is only using the training set of the question answering datasets. But by taking the encoder and then we are going to encode all the representation, all the passages of phrases in Wikipedia. And then we can use text as the representation and can actually generalize well for the unseen questions. Yeah. So the question is what if the nearest neighbor search doesn't return the answer? while asking a question if you want. Next person is [AUDIO OUT]. All right. Thank you for taking the time to teach this. My question is kind of quick. So you mentioned work, they brought up a set of relatively simple questions that show how brittle or poor the current models can be. Did that kind of change the community to improve how to evaluate the models? Because they're actually doing pretty poorly on some of those, right? Yes. OK. Next is-- Hey, how is it going? Thanks so much for the lecture. solve the easy problems. Do you think that the current sort of benchmark data sets are maybe a little bit too easy for- [INTERPOSING VOICES] Or just like that. Just try to break the current system, come up with some harder questions. So we actually really need some kind of dynamic evaluation and also introduce more just kind of adverse examples, harder questions also, things like that, yeah. Are you still game for a couple more questions? Sure. I did want to mention it is 9:10 PM on the East Coast. sure if I really have the answer, but I also want to quickly mention that quantization has been definitely a very useful technique to make the model smaller. So we have been also exploring the quantization in the DensePhrases project recently because the storage has been still very large. Yeah, I'm not sure about the question about the connection between quantization and also pre-training. I am not sure. Thank you. Next question is-- Hi Danqi. Danqi is too modest to mention that she was one of the co-organizers of the EfficientQA task. OK. yeah, I guess I'm just a little worried about who comes up with the test cases? Who determines what the right answer is? I mean, we will have more discussion of toxicity and bias coming up very soon, including actually Thursday's lecture as well as a later lecture, not specifically about QA though. OK. Next person is-- Thank you for the lecture. Yeah, my question is also related to the open domain question answering. So I was just wondering how much of the learning side of domain sort of generalization or domain alignment techniques can be combined with language level, like question answering? on the same page. So I have seem some work sort of trying to learn some kind of disentangled representations that can better generalize to the different domains on adversarial examples. And the question is whether this technique can be generally applied to question answering or? Yeah, I'm just wondering to what extent will they work. Because I think language has a lot of specific things like dependency and other sorts like these techniques does not actually take care of. But it's definitely an interesting point. The key difference between the generative model and the extractive model is that for generative models, you can actually leverage more input passage together and do the generation. So the RAG model actually doesn't perform as well as this model. By the way, I also want to mention R AG model is actually not doing better than DPR because it's base model, this is large model. So these numbers are a little bit confusing. So it's actually basically really on par. Their base perform similarity. can you use generative models to generate the span? You can. You can definitely do that. The model is very large, like 11 billion parameters. So the parameters are basically trying to memorize a lot of information that has been. Because the model has been pre-trained on the text and also has been fine-tuned. All right, thanks. Do you want to call it a night or do you want one more question? Either way, yeah. It's up to you. There is a lot of interest in extending these question answering techniques or just encoding techniques, embedding techniques to recommender systems. The first question is whether these techniques can be generalized to other languages. If we have, actually, I think that the techniques could be generally applied to other language. There has been a lot. of work trying to do cross-lingual question answering and stuff like that. But there has been some constraints that a. lot of models or systems that I described here actually require very strong pre-trained language model. and also requires lots of training examples for the Pure-DSS.