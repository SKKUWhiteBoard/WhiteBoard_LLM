Machine learning is about how to acquire a model and acquire the parameters of a model, from data and experience. In the next few lectures, we're going to work through a sequence of different takes on machine learning that are going to highlight different subsets of the big ideas on this topic. We'll see today exactly how data kind of goes through the mill and gets turned into a model. And we'll see more in-depth examples and more structured examples of these kinds of problems later on when we talk about applications. Machine learning can be used to make predictions about spam or ham. The training set for machine learning is very noisy, so it can be hard to label. Machine learning is not perfect, and some inputs are just really, really hard, and they're going to look like this and we're just not all even going to agree on what that's supposed to be. The goal is to be able to predict labels of new images that are not the ones we've already seen, OK? So that's actually subtle, but it's super-important. representations that if the thing gets tilted or it's a little bit lighter. We could look at how many connected components of ink there are. What's the aspect ratio? How many loops are there? It's increasingly the case, especially for problems like this, that we feed in low-level features like pixels, and higher level features like edges, tend to get induced increasingly more as our machine learning methods get better at doing that. We'll talk about that in a couple weeks when we talk about narrow nets. We're going to talk about model-based and model-free learning. Model-based learning involves building a model from our data, and then doing inference in that model to make predictions in the world. After today we'll look at the model- free methods. We'll also talk about how to learn the parameters of a models from data once we've decided it's structure. And then, the thing we'll mostly talk about today is how to teach a model to recognize a person. word depends on the class and also the previous word. This Is a better model of language. If you started, if you did prediction in this, and you cranked out a pretend document, it would look significantly more like a real email than if you just did a bag of words. However, will it be more accurate for classification? It really depends. In general, it will be a little more accurate, but at a cost of having significantly more complicated conditional probabilities to estimate. If I take a spam document, and I permute all the words randomly, Ive definitely, like it is no longer syntactically-valid English. class which is not kind of strongly connected to the actual ordering, Naive Bayes is really good. Otherwise, you add other correlations like this to fix it. OK? Other questions? Good questions. All right. A couple more general slides, and then, we'll take a break. We are now into the machine learning section which means we are done with the spooky Halloween time ghost-busting section. But I have candy. So during the break, everybody get up and come grab candy if you would like. Machine learning theory is based on trying to say something precise about the connection between what's going on in your data and this future used to which you're going to put the classifier to. The main worry is that in picking the parameters of your model, you do a really good job of capturing that training data, but it doesn't generalize. This is like you download all the exams from past years, and you optimize. You learn all those answers, and then you go to the final exam and it's totally, totally different questions that look nothing like those. Spam detection is, in some ways, a very poor example of a canonical classification problem. Over-fitting means fitting the training data very closely, but not generalizing well. Under-fitting, where you're just like, I don't know what's going on, I'm just guessing randomly everywhere, is not going to work very well. Spam is being generated by people who are trying to defeat spam filters. And now you have spammers who are primarily looking at information or contact information. the thing we're trying to do is to fit a curve to this data. You can always fit your data more. It's about fitting your data to the point where the patterns that you are capturing are ones which generalize to test. This is definitely over-fitting. Over-fitting shows up not just on these continuous functions. It also shows up, for example, in a hypothetical digit classification, we might say, here is an image I've never seen before. Let's use Naive Bayes to classify it. So what would we do? We'd do our running total. over-fitting usually shows up as sampling variance. To do better, we need to smooth, or regularize, our estimates. We can use elicitation, right? You can ask a human. You can go to a doctor and say, hey, I'm building a classifier. What fraction of people with meningitis will present with a fever? And a doctor can give you a guess. It may be qualitative. You could also do that empirically. You Could go collect a bunch of records. The maximum likelihood estimate, or relative frequency estimate, is a way to estimate the likelihood of an outcome. The more samples you draw, the more accurate your estimate will be. But in practice, you need some smoothing to prevent things like zeros in these estimates. This is actually due to a philosopher who kind of worried about things like things like how do I estimate the probability the sun will rise in the morning? Every morning it's risen, so far so far. Every morning the sun's risen. So I need some way of incorporating that into my estimate. there's 100 blues. Now how many reds do I have? Well, I do my computations as if I had 102 reds and 101 blues. And suddenly, even though there are still more reds than blues, in my posterior estimate here, it's pretty close to 50-50. So as I crank up k, I have a stronger prior, and I fit less. If I crank down k,. I fit more, and so I now I have. a dial which can trade off the amount of fitting against generalization. In machine learning, you know what you think the features are going to be. But you might be wrong. So we learn our parameters from the training data. We tune them on some different data, like some held-out data. And then eventually, you're going to take the best value, do some final tests, test run. It's important for when we start to get to neural nets, where the story here is going to change. We're talking a bit more about this starting next lecture. In spam classification, we found out that it wasn't enough to just look at words, you've got to look at other sort of metadata from the ecosystem. For digit recognition, you do sort of more advanced things than just looking at pixels. Try to do things that are invariant to rotation and scale and all of that the vision folks think about. You can add these as sources of information by just adding variables into your Naive Bayes model, but we'll also talk in the next few classes about ways to add these more flexibly, and also ways to induce these.