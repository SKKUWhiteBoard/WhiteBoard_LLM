foreign I'm really excited especially for this lecture which is a very special lecture on robust and trustworthy deep learning by one of our sponsors of this amazing course themus AI. Themis AI is a startup actually locally based here in Cambridge our mission is to design advance and deploy the future of AI and trustworthy AI specifically. I'm especially excited about today's lecture because I co-founded Themis right here at MIT right here in this very building in fact this all stemmed from really the incredible scientific innovation and advances that we created right here. an extremely famous paper a couple years ago showed that if you put terms that imply female or women into a large language model powered job search engine you're going to get roles such as artists or things in the humanities. If you help input similar things but of the male counterpart you'll end up with roles for scientists and engineers so this type of bias also occurs regardless of the task at hand for a specific model. Finally let's talk about Healthcare recommendation algorithms these recommendation algorithms tend to amplify racial biases. Uncertainty estimation is useful for scenarios like this this is an example of a Tesla car driving behind a horse-drawn buggy which are very common in some parts of the United States. The model does not know what this image is. The exact same problem that resulted in that video has also resulted in numerous autonomous car crashes so let's go through why something like this might have happened. There are multiple different types of uncertainty in neural networks which may cause incidents like the ones that we just saw we'll go through a simple example. a data set called cityscapes and the inputs are RGB images of scenes the labels are pixel wise annotations of which label every pixel belongs to and the outputs try to mimic the labels they're also predicted pixel wise masks. Why would we expect that this data set has high natural alliatoric uncertainty and which parts of thisData set do you think would have aliatoric uncertainty. If a model has never seen a specific input before or that input is very hard to learn all of these models should predict slightly different answers and the variance of them should be higher. At Themis we believe that uncertainty and bias mitigation unlock a host of new solutions to solving these problems with safe and responsible AI. We can use bias and uncertainty to mitigate risk in every part of the AI life cycle. We're developing a product at Themis called AI guardian and that's essentially a layer between the artificial intelligence algorithm and the user and the way this works is this is the type of algorithm that if you're driving an autonomous vehicle would say hey the model doesn't actually know what is happening in the world around it right now as the user. compete in the competition which the details are described in the lab but basically it's about analyzing this data set creating risk-aware models that mitigate bias and uncertainty in the specific training pipeline. At Themis our goal is to design advance and deploy a trustworthy AI across Industries and around the world. We're hiring for the upcoming summer and for full-time roles so if you're interested please send an email to careers themesai.io or apply by submitting your resume to the Deep learning resume drop and we'll see those resumes and get back to you.