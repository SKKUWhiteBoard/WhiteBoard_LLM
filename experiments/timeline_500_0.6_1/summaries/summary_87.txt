Andrej Karpathy: In this module, I'm going to briefly introduce the idea of differentiable programming. Differentiable programming is closely related to deep learning. It allows you to build up an increasingly more sophisticated model without losing track of what's going on. So let's suppose you want to do image classification. We need some way of representing images. To fix this problem, we introduce convolutional neural networks which is a refinement of fully connected neural networks. So here is an example of ConvNet in action. In NLP, words are discrete objects and neural networks speak vectors. So whenever you're doing NLP with neural nets, you first have to embed words, or more generally, tokens. So we're going to define an EmbedToken function that takes a word or a token x and maps it into a vector. And all this function is going to do is it's going to look up vector in a dictionary that has a static set of vectors associated with particular tokens. But the meaning of the words and tokens depends on context. So this representation of the sentence is not going to be a particularly sophisticated one. A simple RNN works by taking an old hidden state, an input, and a new hidden state of the same dimensionality. LSTMs, or long short term memory, were developed to solve this problem. So now we have our sequenced model on RNN which produces a sequence of vectors, and the number of vectors depends on how long the input sequence is. So suppose we want to do classification, we need to somehow collapse that into a single vector. So you can intuitively think about this as summarizing the collection of vectors as one. There's three common things you can do. Transformers are a way of combining different types of networks into one. They can be used to solve problems in language modeling. They use something called self attention, which means that the query is actually going to output the input vectors. So if self attention takes a sequence of input vectors, then it's going to stick the first vector into the query vector for y and then compute the attention, x2 and x4. So in other words, I've basically generated a sequence where all n squared of all the objects, all squared of the vectors, where I've allowed them to communicate with each other.