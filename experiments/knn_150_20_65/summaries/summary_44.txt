Last time, we talked about some of the kind of the bigger questions in deep learning theory. Today, we are going to start talking about the optimization perspective in deeplearning for two lectures. The main focus is to analyze what the functions you are optimizing look like so that you can use some trivial or some standard optimization algorithm for it. The bigger question we are trying to address here is that why many optimization algorithm are designed for convex functions. But why they can still work for nonconvex functions? So GD cannot always find local mean or global minimum, right? This is for continuous functions. So why? I guess a simple example here, I cannot-- it's easy to come up is that you just say maybe-- actually, I'm not taking the simplest one just for some reason because I'm going to use this again. So let's say suppose you have f of x1, x2, which is something like x1 squared plus x2 cubed, OK? The origin satisfies the gradient 0 and satisfies the Hessian is PSD. So these are necessary conditions for being a local minimum, but not vice versa. There is some caveat about whether you can even converge to a local minimum. We're going to show some very, actually, simple cases where we can prove this. The problem here is that, when the gradient of fx is 0 and also the Hessian is not strictly positive semidefinite, it's just a positive semidefinite. If your second order derivative is literally 0 in some direction, then a third order derivative starts to matter. So that's why local minimum is not only always a property of the first and second order derivatives. NP-hard. But actually, finding a local minimum is also NP-hard, right? So we have to consider these kind of pathological cases, which makes things harder. So the way to go beyond it is that there is a way to also remove some of the pathological cases as well so that you can find a local Minimum in polynomial time. And then we can talk about matrix completion, which is kind of like an upgraded version of PCA. And as I said, this is actually a pretty important question in machine learning. I think I wrote a book chapter about this kind of optimization thing for our book. So I can send that to the person who take the Scribe notes. And that probably help you to have some references. But the materials are not exactly the same as the book, so you still have to do the Scribes kind of from scratch in some sense. OK, cool. The third strict-saddle condition sounds hard to check. So you cannot check. There is no way you can check whether empirically your function satisfies this condition. So now, what you can do is this, right? do linearized network, there is a little bit more things to do beyond that. And the second example I'm going to give is matrix completion. This is an important machine learning question by itself as well, right? So before deep learning, this was one of the most important topic maybe in machine learning. And now, still I think it's used in the recommendation system. OK, cool. I guess let's talk about PCA first. So I guess I'll maybe more precisely say matrix factorization. so one dimension. So d is 1. Then you just have a scalar, m minus x squared squared. This is our function, g of x. And you plot this function. And there are two local minimum. And they are both global minimum because there is some symmetry here. OK, so let's talk about the proof. So how do we prove this? So as you can imagine, the proof is pretty simple. The plan is very simple. So if you apply these kind of techniques, you can get the Hessian like this. The quadratic form related to the Hessian is much easier to compute. The methodology is the following. You just iteratively expand it, Taylor expand it. And then if you have this, then this basically corresponds-- if you replace epsilon to v, and so forth. So in this case, from this quadratics form you can figure out what the corresponding matrix is. But for many other cases, actually, it's very hard to write out that matrix of the Hessians. It requires some intuition. But it also probably makes sense because the top eigenvector direction is the global minimum. top one eigenvalue-- eigen vector with the right scaling. So the second case is that x has eigen value, let's say, Lambda, which is strictly less than Lambda 1. There is no guarantee that two eigenvectors are always orthogonal because they could have the same eigen Value and they are just in the same subspace. But if they have different eigenvalues, then they have to be Orthogonal. So that's why x1 is orthogona to b1. Let M be a rank one matrix, and symmetric, and PSD just for simplicity. You can assume M equals something like zz transpose, right? And z is kind of the ground truth. And the setup is the following. So we are given random entries of M. We pick some random indices of M, and you review the corresponding entries. That's the only thing you know about M. And then the goal is to recover the rest of the entries,. More formally, so you say that there is omega, which is a subset of. the entries, subset of the indices of d. assume that the input are linearly separable, then there is a proof for this. And there are a bunch of other cases where you can have some partial results. Next week, in the next lecture, maybe the second half of next lecture,. I'm also going to give another result, which is somewhat more general. It applies to many different architectures, but it has other kind of constraints. First of all, it doesn't really show exactly these kind of landscape properties. It shows that these kinds of properties holds for a region, for a special region in the parameter space.