Machine learning is about how to acquire a model and acquire the parameters of a model, from data and experience. In the next few lectures, we're going to work through a sequence of different takes on machine learning that are going to highlight different subsets of the big ideas on this topic. We'll see today exactly how data kind of goes through the mill and gets turned into a model. And we'll see more in-depth examples and more structured examples of these kinds of problems later on when we talk about applications. is going to be at least one pixel off of something else you've seen. So you can't just collect all the data. You can get data that is similar, but then, in the end, you're going to have to generalize. And so, we'll talk about how that's going to work. What features might you use to detect digits? Well, somebody puts a grid of numbers. Your eyes and your visual processing system is already doing all kinds of processing. People will think about computer vision, replicate some of that processing. the thing we're trying to do is to fit a curve to this data. So you say, what is the fit? Is the fit getting as close as possible to the last dot? So what is my constant approximation to this? Does anybody want to hazard a guess? Let's call it five. OK, did I fit something about this data? Yeah, I felt something about the data. I fit basically it's mean. Did I capture the major trends? No. All right, let's try again. Let's fit a linear function. It's close, right? It's a better fit than the constant function. Notice that when I went to linear function, the space of hypotheses grew. Instead of just lines, now it's like lines with slopes and intercepts. In Naive Bayes probabilistic models, over-fitting usually shows up as zeros in your probability table. In other methods, it's actually going to show up in totally other ways. So let's figure out some ways to do that, to just illustrate what it would like to look like. We could take that polynomial and limit the degree of the polynomials. Using it under a hypothesis, you can also shrink the hypothesis space, so you can fit less under it. We already know one kind of over-fit to limit that. In general, your model is going to make errors. In spam classification, we found out that it wasn't enough to just look at words. For digit recognition, you do sort of more advanced things than just looking at pixels. You can add variables into your Naive Bayes model, but we'll also talk in the next few classes about ways to add these more flexibly and also induce these right, right? All right, I'm going to stop there for today and as you go, please come and grab some more candy.