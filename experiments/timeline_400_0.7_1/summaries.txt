==================== [1/100] ====================
Summary:
John ESSIGMANN: I measure my blood sugar at different times during the day. Gluconeogenesis technically means new synthesis of glucose from non-carbohydrate precursors. The medicine I take is called Metformin. It has a number of targets, but one of them is one of the enzymes, called PEPCK, Pyruvate Carboxykinase, that's in the gluconeogenic pathway. The liver provides a constant stream of glucose to these organs that absolutely require it, like our brain. the night, is to take this drug that will prevent the switch to produce more and more sugar by gluconeogenesis. The drug will stop the switch from producing more sugar to producing less sugar. It will also prevent the production of more sugar from the body's own production of sugar. This is called the 'gluconeogenesis inhibitor' and is used in the treatment of diabetes and obesity. It is a drug that stops the switch between the two processes from occurring. It also prevents the body from producing too much sugar.

ROUGE-1: 45.30, ROUGE-2: 30.98, ROUGE-L: 31.54
BERTScore: 67.45

==============================================
==================== [2/100] ====================
Summary:
In this lecture, we introduce and develop the concept of independence between events. If I tell you that a certain event A has occurred, this will generally change the probability of some other event B. In such a case, we say that events A and B are independent. We will then proceed to define the independence of a collection of more than two events. Finally, we will close with an application in reliability analysis and with a nice puzzle that will serve as a word of caution about putting together probabilistic models.

ROUGE-1: 62.59, ROUGE-2: 60.96, ROUGE-L: 62.59
BERTScore: 82.29

==============================================
==================== [3/100] ====================
Summary:
In today's video i'll show you the importance of de-gassing your bread dough as it's fermenting. No matter how gentle you try to handle your dough you will always de-gas it if only a little bit in today's comparison video we'll make 4 breads they will be made from the same dough but they will all be treated differently. The first one of the four breads will be left alone from the beginning of fermentation until it's baked. The final one will be folded shaped and degassed three times and we won't be fermenting them for the same amount of time. worst and you should not be making bread like that if you want to use this method it should be fermented for much longer and perhaps go in a higher tin at the end of the day it's all up to you make your bread the way you like it make it fit your style and taste experiment try different methods don't just follow recipes ask questions and if you ever get stuck check out more videos in the principles of baking playlists you might find some answers there so what do you think of degas thing do you degas your bread though let me know down in the comments and don't forget to read the blog post linked in the video description.

ROUGE-1: 22.87, ROUGE-2: 22.50, ROUGE-L: 22.87
BERTScore: 67.58

==============================================
==================== [4/100] ====================
Summary:
Of the nearly 11,000 amendments proposed in the centuries since, only 27 have succeeded as of 2016. To actually change the Constitution, the amendment must be ratified by three-quarters of all states. Most other democracies pass amendments every couple of years, but the U.S. hasn't passed one since 1992. Ratifying amendments has also become harder as the country has grown larger and more diverse. Americans today are the most politically polarized since the Civil War, making it harder to pass amendments. nearly impossible to reach a broad consensus. The late Supreme Court Justice Antonin Scalia once calculated that due to America's representative system of government, it could take as little as 2% of the total population to block an amendment. Interestingly, the founders themselves may have foreseen this problem early on. In a letter to James Madison, Thomas Jefferson wrote that laws should expire every 19 years rather than having to be changed or repealed. Although he believed that the basic principles of the Constitution would endure, he stressed that the Earth belongs to the living, and not to the dead.

ROUGE-1: 46.23, ROUGE-2: 43.07, ROUGE-L: 45.48
BERTScore: 66.04

==============================================
==================== [5/100] ====================
Summary:
50 years ago, John McCarthy and Marvin Minsky coined the term artificial intelligence. Progress has been made, especially in the last 20 years. But we need different expertises. Not all in computer science, but in other ones. And so, this was the people that we put together from different labs, from neuroscience, from computer science,. from cognitive science, and from a number of institutions in the US. Especially MIT and Harvard. Let me tell you a bit more about the background here. This idea of merging brain research and computer science. Artificial intelligence and machine learning is the focus of a new Nature supplement. In the first five years of the center, that's the main focus. We want to understand how the answers are produced by our brain at the computational, psychophysical, and neural level. We are not yet at the point in which we can answer all those kind of questions at all these different levels. But some, we are. One example is, who is there? It's essentially face recognition. And this is an interesting problem, because we know from work, originally in the monkeys, and then with fMRI in humans.

ROUGE-1: 27.69, ROUGE-2: 25.72, ROUGE-L: 27.40
BERTScore: 66.67

==============================================
==================== [6/100] ====================
Summary:
Chef Todd Moore shares with you the seven basic skills that I think everyone should have to cook food consistently in the kitchen and be proud of the results. If you already have all seven of these skills and cooking techniques great you can work for me on the other hand if you only have one or two of these Skills that's still fantastic why because I know you'll want to add other skills and learn to cook basic methods know the methods behind all written recipes then you'll be making your cooking a winner every single time. like to cut things up you'll be using fresh vegetables uh anyway uh second in my chef test to anticipate when oil is about to smoke now the skill here is understanding the convective cooking process. When the chef notices the oil starting to change from being perfectly smooth to beginning a convection process then adds the protein product to the pan just before there's visible smoke this is where you get the splattering reaction in the pan. If you give me a chicken breast with a beautiful brown plate appeal that shows your ability to control heat so that the item develops color but doesn't lose moisture or burn you fail this test. one that's lost moisture one that's burned one that shows the lack of involvement in the preliminary steps of the sauté process Chef test number four here's the answer thicken a liquid to make a sauce. Chef should have an understanding of the difference between boil simmer and poach a common mistake of Home cooks and chefs alike for that matter is always boiling items boiling is not a cooking method once you understand how to control the reaction of liquid in a pan you'll be able to perform a poach of a very delicate item like eggs. a chicken most people can put a piece of meat in for a while and I don't have 3 hours to wait on the chef test anyway but roasting something delicate like fish now this shows the ability to show to control dry convective heat dry versus Mo moist in controlling dry heat. There's a very fine line between the coagulation of proteins the stiffening and shrinking of what you're cooking at 165° F and the 212° fah when moisture starts evaporating the key to cooking with dry heat is being able to live in this temperature zone between 165 and 212 where the food Cooks for drying out. a cook that uses a thermometer and this is a cook that passes this test not one that is obviously more cooked than the other to be well done they all look similar on the outside and they're different internal temperatures on the inside if you can Master this you are a master griller most people just burn stuff you fail this test. If you can't tell which steak is rare medium well done if you're taking them off the grill guest atam the chef that can that can't control direct to Source conductive heat would create waste in my restaurant.

ROUGE-1: 37.06, ROUGE-2: 36.46, ROUGE-L: 37.06
BERTScore: 64.14

==============================================
==================== [7/100] ====================
Summary:
Third degree heart blocks also known as a complete heart block this type of heart block is the worst of all blocks. Electrical signal from the atria isn't making it to the ventricles. The person could be born with it so it could be congenital or the person has severe heart disease or they have a myocardial infarction or they're taking some type of medication that they become toxic on like digoxin. What is the treatment for a heart valve problem now what is the treating for a third degree heart block? third degree heart block well with this your patient's usually going to have some signs and symptoms because whenever the heart is beating like this those ventricles and atria they're really being independent of each other it's not going to perfuse your body so you're going tohave a low cardiac output. Some treatment that can be given to that patient is that atropine can be administered to help that heart pump more efficiently or the patient could be connected to a temporary pacemaker which will again get that heart beating correctly. Eventually the patient will need a permanent pacemaker implanted.

ROUGE-1: 46.34, ROUGE-2: 43.60, ROUGE-L: 44.44
BERTScore: 68.87

==============================================
==================== [8/100] ====================
Summary:
The final contest, which is due tonight, is to design an agent that plays together with another agent to try to collect food pellets while not getting eaten by ghosts. So submissions for that, your last chance to submit are tonight at midnight. And on Thursday in lecture, we'll discuss the results. So today's lecture, as well as Thursday's lecture,. will be mostly on advanced applications. We will not quiz you on these application lectures, on the final exam, or anything like that. It's more meant to give you more perspective. Today's state-of-the-art in Go is that there are computer players better than the best human players. But actually, if you went back to March 2016, that was not the case yet. So how do you make an AI for Go? Let's go back to what we were looking at in lecture on games, MiniMax. MiniMax is about solving games in adversarial environments. And in what it did, we had to update this graph that you see at the very beginning of the course. DeepMind's AlphaGo is a computer program that can play the game of Go. It uses a deep neural network to learn how to search for the best moves. It can also learn to predict who is likely to win from a given situation. DeepMind is now working on a system that can learn to play Go without human help. It's called AlphaGo Zero, and it's being developed by DeepMind's Sergey Brin and Yann Leibovitz. It will be unveiled at the World Economic Forum in Davos this week. AlphaGo Zero has no prior knowledge. It starts at a pretty, actually, negative elo rating. Loses all the time initially. It's playing itself, but then gets tested against a set of players. After 21 days, it goes past where AlphaGo Lee Sedol was. And then it was still creeping up after 40 days. Go, even though it's a master game, in principle, has a solution. Once you reach that level, essentially, there's no further to go, because you solved the game. With reasonable compute power, it traverses the whole tree. Even with alpha beta pruning, I don't think that'll happen anytime soon. It could be that by using human knowledge, you're in some kind of based enough attraction. For a local [? optim, ?] that maybe not as good as another one that might be out there. It also depends on how much randomness you have in your exploration. If you have enough randomness, then initialization will have much less effect than if you have limited randomness. Researchers have created an RC helicopter that can fly autonomously. The system uses a hidden Markov model to learn from scratch. The researchers hope to use the system to teach people how to fly helicopters in the future. They hope the system will also be used to train people to be better Go players, among other things, by teaching them how to play the game without knowing any prior knowledge of the game or how to do it well.. The project is funded by the National Institute of Standards and Technology (NIST) In the RL lecture, we saw an example of a helicopter reliably hovering. Upside down is harder. How do you keep yourself upside down? Well, that main rotor can have a negative angle of attack. If you're flying upside down, it keeps you up in the air. And it's actually more efficient because when you pull in air, you're accelerated. In the video we watched, the helicopter had already been flipped. The pilot flipped the helicopter, toggled autonomous control for inverted flight, and that's what we saw. saw it making these wild motions, overcompensating. It pushed the controls so hard that the engine died. The engine just couldn't push it, died. You lose control over your helicopter, more or less, at that point. Then what happened is our human pilot took back control to try to save the helicopter. And believe it or not, they actually saved this. It landed a little harder than you want to land, but it landed on its feet and it could be recovered from that. the helicopter to follow a path that's not flyable. So what if we collect paths from a human pilot and then ask the helicopter to fly those paths? Well, we could learn the trajectory from these as noisy observations. Hidden Markov models. If we have something we don't know that evolves over time, but we have some noisy measurements of it, we can run an HMM to recover what we actually want. We see here is something that's better than any single one of the demonstrations. We want to take a closer look. able to look ahead only two seconds, rather than needing to look further. A value function tells us, OK, how good is it to end up here? We also have a reward at each time tick. And our search over those two seconds is what results in the control we apply. The fastest we flew this helicopter was close to 55 miles per hour, so almost highway speeds. The algorithm's only this big, so it's pretty fast for something of this size. It was possible to fly this helicopter at the level of the best human pilots. Berkeley student Woody Hoburg took charge in seeing how far we can get without human experts for some simpler things, not for all the maneuvers here. He was able to have it learn to hover reliably with the only human input being shut it off when it looks like it might start doing something dangerous. We did not push that further to flying those maneuvers. There is some work. If you look at Woody, Woody was shutting things off.to this behavior. Then recently at OpenEye, there's been some work on robots learning to do back flips. it has more time, and if it already has a recovery controller, then you can imagine that. And Claire Tomlin's group here at Berkeley has done some work in that direction, where they have a safe controller and a learned controller. And the learning controller is learning on its own while the safe controller keeps things in check so the helicopter doesn't crash. So what we used there is a model-based reinforcement learning method. So we learned a dynamics models for simulator from data that was collected. To learn a good controller in the simulator, we used something called iterative LQR. is a separate linear feedback controller for each time slice. If there is no wind, you can actually just run the linear feedback control. It will be fine. But if there's some wind gusts that could throw you off, you want to use the value functions and the two second look ahead against those value functions to do the controls. Training a unified policy across the entire space might work. It might take some work, exactly, figuring out how to do it. It could be interesting to revisit that now and see what the current understanding of how to train these networks. In 2015, there was the Doppler Robotics Challenge, which was held in Pomona, just east of Los Angeles. The robot had to, essentially, drive a car or walk, but driving the car was recommended. It turned it's very complex to get a robot to do that. The thing is modeling these situations proved even harder than modeling helicopters, because your sensing needs to understand whether or not you're already making contact, and making contact or not. You can be very close, but not have contact. It's a very subtle thing. Stanford robot becomes first vehicle in history to drive 132 miles by itself. Five robots remain on the course. To finish, they must wind through a treacherous mountain pass. After months of tireless effort, there's a lot at stake. The first time it's ever been done, autonomous vehicles. A vision they all share will now be put to the test. The Dartmouth Grand Challenge is held every year at Dartmouth University in Dartmouth, Dartmouth, New Hampshire, and Dartmouth College in Hanover, New York. Four cars finished the 150-mile Berkeley autonomous car race in 2005. What goes onto the cars? There is IMU, like right on a helicopter, a lot of computers. Lasers, where you shoot out laser beams. And based on how long it takes them to get back, you know how far away the nearest obstacle is in that direction. Cameras, radar, control screen, steering motor, usually, you would have a high level planner choosing a path and then a low level controller following that path. A camera will be better at that than a LIDAR. Self-supervision is a trick that's very widely used to reduce labeling efforts. In urban environments, there's even more need to recognize, not just road versus not road. A lot of progress has been made this is video from 2013. This was before deep neural networks were heavily used for this kind of thing. It's only getting better to recognize what's in scenes, thanks toDeep neural networks. The devil is really in the details, in the long term. tail of special events that can happen when you're driving. You can measure progress by just demo videos, which is one way, and it gives you some kind of feel for what's going on. Another way to measure progress is to see how are these cars doing relative to human drivers. If you test in California, you have to report this data to the DMV. It's a number of events per 1,000 miles driven. Red there is human fatalities. Then yellow is human injuries. In green is the Google slash [? wave ?] mode disengagement. so many decisions. If they're gigantic, use a lot of power. That's a problem. Let's see what we can do to build smaller networks to make decisions. What else did we not cover yet? Personal robotics. I want to spend a little more than two minutes on that, so let's keep that for Thursday. that's it for today. Bye. [SIDE CONVERSATIONS] [Side CONversation] [sideconversation.com: Do you know more about this topic? Email us at jennifer.smith@cnn.com].

ROUGE-1: 25.63, ROUGE-2: 23.08, ROUGE-L: 23.21
BERTScore: 63.56

==============================================
==================== [9/100] ====================
Summary:
So let me say here just the repetition, normal, inferior. Let us say we are talking about price of good 1 has gone up substitution, income these are the effects and overall. When P 1 goes up subs because of substitution effect x 1 will. Come down. And for income also because of income effect it will come down. So, overall it comes down. While inferior goods substitution effect quantity demanded would come down while income effect is Up. Up. Can you give me an example its very very difficult to find Giffen good in real life why? Sir, like if Gucci is a brand and it is. Prices comes down by significant amount. No one could buy a Gucci, it is at that level. Only because of it is prices. I am not certain about this statement then when price of these goods would come down then people would not buy. That is not true. So, these luxury goods are not Giffen goods fine, it is clear? Let me give you just an example typically it is very very difficult to find Giffan goods. You know if you take an example of potato that I gave you earlier and it was it was talked about for a very long time in economic discipline that during Irish famine the potato was Giffon good. for meat and other food products. To get enough calorie; they had to increase the consumption of potato. So, in that case if this story is true then potato is Giffen good ok, but the problem is with this the second requirement that income effect is larger than the substitution effect. Typically this is not fulfilled that is why we have very-very hard time getting finding out, figuring out Giffsen good in an economy. It's very hard to find out what's good.

ROUGE-1: 50.80, ROUGE-2: 48.69, ROUGE-L: 49.79
BERTScore: 77.13

==============================================
==================== [10/100] ====================
Summary:
which we are to for next week and here and I'll talk about that actually week from today because it's callay on Monday so it will get behind but if we get any further behind I'll have to make it up sometime the but if you don't have that you be copies available upstairs is it c um come to my office okay to dany's office he doesn't have copies of it there that's six six upstairs in the first floor immediately after CL and then on the next Friday I will talk about the parts in the in the inquiry that were assigned and also a that essay of of the oral concept. exclusion crisis of 1679 and 81 um the original part of the second CH was written it believe about 79 and 880 and then other chapters that were were added in 81 83 and two chapters added after. Now by definition a mixed Constitution is a constitution in which two or more constitutionally defined agents share in and each have a part in What L calls the legislative power that is the supreme power in the constitution is the Legislative power. In the English case of course it's the crown and Parliament so we're only actually dealing with two agents in this case and neither is supreme so we want to say that they are coordinate powers. changed his mind he thought perhaps it was not all together safe never knew in England the English were regarded in those days as very unstable people. Mark says nothing about the details of of this process that is how does he Enis that this constition power is actually to take place he doesn't give any institutional account of how he supposes actually might be done laon is also vague on this he says something has the idea that it would begin in the county courts and they organized there and would then they presented Des sent to a parliament. L's View might have been a politically unappealing Doctrine so far as in might estimate also the other way wanted to emphasize the continuity between Charles II James II and of Orange and Mary Mary was James's eldest daughter. They maintained in effect that James had just advocated or meod from bacon and the Parliament finding a bacon Throne had simply found an occupant for it and that act did not imply the parliament was Supreme. Now why did L refuse to alter his view I think we have to assume that he didn't change his mind and he persisted in the doctrine. The distinction between constituent and and ordinary power and the idea of power is is connected with the motion of aary trust. How this constition power gets expressed in institutions and may effective then it's always some extent this problematical. These basic rights ways of politically expressing this power have a kind of priority over over other rights and reason I mention this is I want to think of the basic structure what I earlier called that as of course including these institutions or whatever they are that provide ways through which this conuent power can be both expressed and protected. and who does not have the right to vote and what the conditions on it are maybe we want to free it from that of course what I'm thinking of eventually um is that that that would be one motivation for introducing an idea like the origal position that say it's a way of conceiving how con power might be exercised. No point in criticizing someone for something they didn't intend to do okay well I think it's stop so remember there's no class here Monday so the next class here will be next Friday.

ROUGE-1: 18.63, ROUGE-2: 18.18, ROUGE-L: 18.57
BERTScore: 66.48

==============================================
==================== [11/100] ====================
Summary:
David Kaiser: particle cosmology is a new subfield within physics. He says it studies the smallest units of matter, the fundamental forces and elementary constituents of matter. Kaiser: The field is doing pretty well these days by other measures. Its annual budget just within the U.S. is on the order of $1 billion a year, roughly, he says. It is really a booming, booming subject of study, Kaiser says. The field literally didn't even exist 45 years ago, he adds. acceleration throughout the 1960s, a number of high-energy theorists were trying to put together these highly symmetric models to account for things like the nuclear forces. These nuclear forces are self-evidently of short range, unlike gravitation or electromagnetism, which, in principle, can extend arbitrarily long distances. The nuclear forces really exert themselves across nuclear dimensions, very tiny fraction of the size even of a single atom, let alone macroscopic scales. That will make it very unlikely for that force to be felt across a very large distance. The question of mass turns out to have been on many specialists minds in the 1950s and '60s, but as embedded in quite different-sounding conversations. The Brans-Dicke theory of gravity was put forward in 1961 by, at the time, a very young Carl Brans. Their idea was actually to try to go back to this notion of Mach's principle and more thoroughly account for mass. So even though the ideas were bubbling up around the same time and often published in the same journals, they still were embedded inquite different research traditions and conversations. for that within a quantitative theory of gravity. So they wanted to modify Einstein's general theory of relativity in a very specific way to try to address this question of mass as it had been articulated around Mach's principle. So their idea was to introduce a whole new kind of matter, a newkind of particle in nature. They labeled it with the Greek letter phi. And now the idea was that instead of having a single constant unit strength of gravity labeled by Newton's constant capital G-- that's the G that's in like Newton's force law. Geometer's tool of quantifying the warping of space and time is called the Ricci curvature scalar. multiplying that is this constant, this unit strength of gravity in Einstein's theory. So what Brans and Dicke do is say, well, let's replace G by 1 over phi. So now 1 over G becomes phi in the Lagrangian or in the action for gravitation, where in principle phi, the local strength ofgravity could change across time and space. field is much less likely to vary either over space or time. In the limit that omega becomes arbitrarily large, then phi, in a sense, can't afford to vary at all. The kinetic energy cost is too high. If the field doesn't change at all, it acts like a constant. So they had this very clever fudge factor, a coupling constant, so that, in principle, the local strength of gravity could be changing all the time. But the amount of that variation could be controlled by this one new parameter in the theory. Jeffrey Goldstone was among the first to show that a certain kind of characteristic shape for a potential energy function could be used to break the symmetry of the nuclear forces. Goldstone introduced a new hypothetical form of matter, the Higgs field, which acts like it has a molasses-like mass. He says this new field stretches through all of space and time, and all the rest of matter interacts with it. If these other force-carrying particles get stuck at some non-zero value of their field, it actually is stuck at a non- zero value of its field. Brans-Dicke field phi was the first quantitative alternative to Einstein's general theory of relativity in nearly half a century. Unlike the photon, these newer force-carrying particles interact with this new scalar field. When they interact with phi, then, all of a sudden, they have drag this field around. They start acting as if they have a large mass. It's an induced mass coming from this spontaneous symmetry breaking. So now, again, you can have your cake and eat it too. The Higgs and Brans-Dicke fields were published to wide acclaim in their own separate fields as early as 1961. It took more than 15 years -- nearly 20 years-- until people began to say, hey, these two scalar fields are meant to pervade all of nature. These were very highly prominent contributions, but there's almost no overlap between the two fields. The earliest paper that co-cited the two papers was 11 years later in 1972 after their original publications. It was really in this late period when a small handful-- still less than 1%-- started citing the papers together. In 1979, two separate theorists working independently of each other suggested that the two fields might be literally the same, not just comparable or worth considering side by side. So it's not that they're somehow intrinsically totally separate or different from each other. Instead, their status, really, is historical. How people assess them or what they thought they were good for was changing over time. And so what had changed? That's what we'll pick up in the next part of this three-part series. more than just the letter that they chose. There was a lot of what we might have considered similarities. And yet, the two sets of ideas really were treated so separately. By the end of the '70s, things did not look very good for Brans-Dicke gravity experimentally. In fact, Einstein's theory was not highly favored, and that's why everyone else started paying attention on the gravity side of the field. So we might wonder, well, was it changes in data? Did experiments force a new evaluation? Evidence for the Higgs-Boson wasn't found until July of 2012, or maybe December of 2011, the first hints experimentally-- well past the period we're talking about. The main story that's mostly given-- I alluded to this in the very beginning of today's class-- is actually hearkens to changes in ideas. Two of these sets of ideas, in particular, are usually pointed to-- and they came in rapid fire in 1973 and 1974. The first of them is called asymptotic freedom. That suggests the behavior of quarks and gluons might be really different at very high energies. In the late '60s and early '70s, physicists saw a really dramatic change in the kind of infrastructure for the discipline. The US budget for that subfield fell in half in just four years. The field that had, so to speak, the most to lose or lose during this reversal of fortune was high-energy particle physics. We can ask a bit more that's very coarse-grained, but we can dig in more and look at subfield by subfield. vast majority of those came really in the later '70s, in the wake of these pedagogical reforms. So remember that big report comes out in 1972. You start seeing curricular changes as early as '73, '74. By '75, '76, '77, you start seeing, in some sense, the market respond with many textbooks being really rushed into print. Some of these textbooks were basically mimeographed lecture notes. Now there's very, very fancy books published in a more typical way. You see really a rush to get more books on that subfield in particular. a continuous unitary symmetry, which is like saying you could rotate the electron field by any continuum amount, and the equations remain unchanged. The photon only has to mop up a relatively simple symmetry, the U1 gauge symmetry. Whereas SU2 was what I was pointing to when I was referring to the weak nuclear force. That's a discrete symmetry. It's a more complicated symmetry structure. And that's the symmetry group that these force-carrying particles-- the W and the Z particles-- are invented to enforce. broken at lower energies, they take on different features. It's another example of spontaneous symmetry breaks. So gravity gets stuck being weak because its local strength is arising through the Brans-Dicke field getting in a symmetry-breaking phase. And only in the broken-symmetry phase do we experience a phenomena that we are used to. That's right. And that's super cool and fun and lots more to be said about that. But that is, indeed, where that nomenclature came from. physicist with whom he accidentally wound up swapping apartments happens to have been immersed in gravitation and cosmology. By the mid to late '70s, he's now asking questions at this interface between his formal training in high-energy particle theory and this new hobby interest. He writes his version of his broken-symmetric gravity to suggest very creatively but tentatively that the Brans-Dicke field and the Higgs field might be identical. It was actually that second version that becomes more and more common. Few physicists today think Brans-Dicke theory of gravity best describes our Universe. But the theory is still very much alive today. In fact, interest in the field grew even as it was getting experimentally less and less favored. The idea that we're picking single theories and that they replace each other, I think, just misses this fine structure, says David Weinberger of the University of California, Los Angeles. He and Weinberger will look at inflation next time on inflation. does exactly the kinds of things that Smolin and Zee had been doing, trying to unify the Brans-Dicke and Higgs field. And in turn, these new folks, especially people like Mike Turner and Rocky Kolb, went on to become real institution builders in their own right. So in fact, they were accelerates. Not only had they been trained to think carefully at this new interface, they helped really accelerate the trend. So I'm going to pause there. We'll look more squarely at inflation in cosmology. new hybrid area, really of what becomes the poster example of particle cosmology. And as I say, including my own students and many people now around the world, it's now just totally bizarre not to consider the Brans-Dicke field and the Higgs field is somehow relatable or maybe even identical. So it goes from who even thought of it to who wouldn't even try that? What counts as natural can shift in a pretty short time scale. Those shifts can be driven as much by things well outside of the physicist's control, geopolitics, and national scale budgets. soon, everyone. Soon, everyone will be able to play together again. Soon. Everyone will be playing together.soon. Everyone. Will be. Playing together again soon.soon,everyone. Everyone, playing together againsoon.soon,. everyone. soon, everyone, will be. playing together once again. soon.everyone. will be Playing Together again.soon! everyone. will. be.playing together soon. soon! Everyone. will play together soon! everyone will.be. playing again soon! soon.

ROUGE-1: 23.60, ROUGE-2: 21.53, ROUGE-L: 20.40
BERTScore: 58.09

==============================================
==================== [12/100] ====================
Summary:
Frida Ghitis: Ferdinand Magellan may have been the first person to actually circumnavigate the globe. She says Spain and Portugal had their eyes on the same prize: trade routes to the Spice Islands. When a Portuguese defector claimed that a westward route existed, King Charles made him captain of a Spanish armada, she says. Ghitis says Magellan's legacy lingers: galaxies and space programs named after him, and he was celebrated in Spain. of the "Victoria" sailed into harbor in southern Spain in September 1522.

ROUGE-1: 17.48, ROUGE-2: 13.88, ROUGE-L: 12.94
BERTScore: 60.28

==============================================
==================== [13/100] ====================
Summary:
Ani was a real person, a scribe from the Egyptian city of Thebes who lived in the 13th century BCE. His Book of the Dead, a 78-foot papyrus scroll, was designed to help him attain immortality. Ani's epic journey begins with his death. His body is mummified by a team of priests who remove every organ except the heart. It's then stuffed with a salt called natron and wrapped in resin-soaked linen. In addition, the wrappings are woven with charms for protection and topped with a heart scarab amulet.

ROUGE-1: 25.56, ROUGE-2: 24.23, ROUGE-L: 25.56
BERTScore: 68.27

==============================================
==================== [14/100] ====================
Summary:
JUDY HOYT: We're going to begin this lecture on handout number 14. We'll be moving now to chapter 7. This will be our first lecture on chapter 7 on the topic of dopant diffusion and profile measurement. So far, we've discussed a number of major topics, including the fabrication of wafers themselves and cleaning, point defects in silicon. During the next few lectures, including this one, we'll discuss the accurate control and placement of active dopant regions. In semiconductors or in silicon, we typically don't have a cube or a chunk of material. We're usually measuring the resistance of a thin sheet in the near surface region. The resistivity of the cube is given by, essentially, the electric field divided by the current density. The sheet resistance is just given by rho over the resistivity over xj. So that's a simple way and a convenient way of calculating resistance of various structures in semiconductor devices. The resistance of the regions that are extrinsic to the device, such as the contact resistance, the source and drain resistance, should be no more than about 10%. refer to different lateral source-drain gradients. For lateral gradients larger than about 4 nanometers per decade, the Vt roll-off is just too large. You wouldn't be able to make a 25 nanometer MOSFET-- so, again, illustrates the importance of controlling the lateral doping profile and of controlling diffusion processes themselves. So given that brief introduction to the electrical effects, let me go on now on slide number 15 and talk about dopant diffusion fundamentals. In silicon IC processing, there are two different steps that we refer to in diffusion historically. Dopant diffusion is described by Fick's first law, which describes how the flux or the flow of dopant depends upon the doping gradient. When the concentration gradient goes to 0, essentially, the dopant or the atoms are uniformly distributed, say, in the solid, and the flow would stop. Later on, we'll talk about the more atomistic diffusion mechanisms and effects of dopants in the silicon lattice. We're going to consider macroscopic first-- macroscopy models for diffusion. The placement of dope regions is critical because it determines many of the characteristics of short-channel MOSFETs. The time evolution of a doping profile, if the case is simple, is governed by a fixed loss-- the so-called diffusion equation. We talked about the diffusion of a Gaussian profile with a fixed dose with a complementary error function, which we apply for a constant surface concentration. And finally, we talked about some simple analytic solutions that the diffusion equation can be applied to.

ROUGE-1: 9.06, ROUGE-2: 8.66, ROUGE-L: 8.43
BERTScore: 71.64

==============================================
==================== [15/100] ====================
Summary:
Atas model shows what happens if aggregate demand increases and firms respond to this by saying we want to make more output. As people demand these higher wages shortening our supply curve decreases basically all the way back to essentially where it was before. The price of tuition to Missouri State goes down but my wage is fixed under contract right and so they're gonna have to pay me the Saints of tuition declines and my input prices stay the same that's bad for them and bad for all of the firm's too. it's gonna be trust me I've been doing this for 20 years questions on the AAAS wanna yes so that's what we're going to talk about when we talk about fiscal policy and monetary policy. Changes in taxes and changes in government spending are also pretty obvious right if your taxes go up and the government doesn't give you more goods and services for that you have less stuff similarly if taxes go down you have more money in your pocket right so you can change people's wealth by changing their taxes or by changing the services that they're getting in exchange. Keynesian model has problems too it doesn't work perfectly either so that's why we have these changes in aggregate demand. We'll get to that when we look at fiscal and monetary policy okay so I will see you guys on Wednesday we only have the exam. We only had the exam we only has the exam so we will be back on Wednesday to talk about fiscal and Monetary Policy. We will be talking about the impact of the stimulus package on the U.S. economy and the global economy.

ROUGE-1: 13.49, ROUGE-2: 11.69, ROUGE-L: 11.77
BERTScore: 59.20

==============================================
==================== [16/100] ====================
Summary:
As a nurse you play an important role in teaching the parents about car seat safety and this education actually starts at birth before the child even goes home from the hospital in their first car ride. In this lecture we're going to concentrate on the main concepts that you need to know as a nurse and for exams first let's talk about the four types of car safety restraints that you can use in a motor vehicle. The back seat of the car is actually the safest place for a child 12 and under. our child we're told is in a forward-facing seat so we want to look at our options. These findings demonstrate the child doesn't fit this restraint because that's what we're looking for and this restraint is not secured properly. The buckle chest clip should be at armpit level not one inch below it okay so if you would like more free quiz questions you can access the link below and don't forget to access the other videos in this pediatric nursing series and thank you so much for watching.

ROUGE-1: 13.52, ROUGE-2: 13.24, ROUGE-L: 13.52
BERTScore: 65.10

==============================================
==================== [17/100] ====================
Summary:
Machine learning is about how to acquire a model, from data and experience. In the end, we want to build good systems. Where do you get accurate systems? You get them from good models. Good models come from good data, and we're going to look at that last part now. What we are going to do today is we're Going to start with model-based classification, and, as an example of that, we'reGoing to work through some details of how the Naive Bayes models work. takes on machine learning that are going to highlight different subsets of the big ideas on this topic. We're going to have a couple running examples. One of them is that spam classifier that pulls out all the emails you don't want from your email. And something like digit recognition, we'll start to give you a window into how other vision tasks work. We'll see more in-depth examples and more structured examples of these kinds of problems later on when we talk about applications. The boundary between what is actually spam, unsolicited commercial email, and emails you just don't want, can be a fuzzy boundary. Some people just click on an email they wish people hadn't sent them, even if it's like from their mom. "To be removed from future mailings, simply reply to this message and put Remove in the subject. 99 million email addresses for only $99." "I know this is blatantly off-topic, but I'm beginning to go insane. Had an old Dell Dimension XPS sitting in the corner and decided to put it to use" is going to be unique. It'll be at least one pixel off of something else you've seen. So you can't just collect all the data. You can get data that is similar, but then, in the end, you're going to have to generalize. And so, we'll talk about how that's going to work. What features might you use to detect digits? Well, somebody puts a grid of numbers. Your eyes and your visual processing system is already doing all kinds of processing. There's tons of classification tasks. It's probably the most widely-used application of machine learning. Classification, you're given inputs, you predict labels, or classes, y. Medical diagnosis could be classifications. Fraud detection could be, think about your credit card company. Automatic essay grading, auto grading, this can be a machine learning problem.. Review sentiment. Here's a bunch of reviews of my product. Which ones are good? which ones are bad? Have they gotten better in the past 10 days since the new announcement? And so on. In model-based classification, rather than directly learning from errors that you make in the world from experience, think like reinforcement learning model-free. The Naive Bayes assumption is an extremely radical assumption as far as probabilistic models go, but for classification, it turns out to be really effective and it goes something like this. You build a model where the output label and the input features are random variables. There's going to be some connections between them, and maybe some other variables too that might help you build a better model. A general Naive Bayes model places a joint distribution over the following variables, y, which is your class, and some number of features, which you get to define. You're going to have to write code which extracts them from your input. So if your spam feature is, have more than 10 people received this email in the past hour? The machine learning will do the work to connect the probability of that taking on a certain value up to the class. And that means when you go to make a prediction, it decomposes into a product of a bunch of different feature conditional probabilities. is we're going to need to figure out the prior probability over labels, and for each feature-- which is each kind of evidence-- we'll need to compute a bunch of conditional probabilities for each class. These things collectively, all these probabilities that we use to plug and chug and get our numbers out, are called the parameters of the model. So it has to come from data, which parameters we want. All right, let's see some examples of what these conditional probabilities in a Naive Bayes model would look like. So in this particular vector, in these parameters, each class 1 to 0 is equally likely. The Naive Bayes classifier predicts the probability of each word in an email. The most likely word for spam is the word free, and the most likely for ham is too. The probabilities are calculated incrementally, as the words come in, position by position. The terms that are going to show up on the left here are the terms for each word as it comes in, which is the product of the prior and the evidence for each class. The total probability of our running running here would be 033 for spam, and 066 for ham. model, the way they're aggregated is multiplying their conditional probabilities. Gary, would you like to lose weight while you sleep? And then once you've seen the whole email, and you look at the end, you'll notice two things. One, the total probability is very small, because I multiplied a bunch of probabilities. That's always true, in Bayes net inference. I have to divide both of these very small numbers by their sum to get the conditional posterior that I want. word depends on the class and also the previous word. This Is a better model of language. If you started, if you did prediction in this, and you cranked out a pretend document, it would look significantly more like a real email than if you just did a bag of words. Will it be more accurate for classification? It really depends. In general, it will be a little more accurate, but at a cost of having significantly more complicated conditional probabilities to estimate. All right. Let's take a break now. We are now into the machine learning section which means we are done with the spooky Halloween time ghost-busting section. But I have candy. Machine learning theory is based on trying to say something precise about the connection between data and training. The main worry is that you over-fit. The principle of empirical risk minimization goes something like this. We would like to find the model, classifier, whatever, that does the best-- whatever the best means-- on our true test distribution. We don't actually know that true distribution. So how do you make progress? Well, what you do know is you have a training set. And you can't pick the parameters which are going to do best. Accuracy is not a great metric for spam detection. The actual loss-- or cost-- of different kinds of mistakes may not be the same. Over-fitting means fitting the training data very closely, but not generalizing well. And so there are a lot of different metrics people have for different kind of tasks. And again, we're going talk a lot today and next time about over-fitting and generalization. We want a classifier which does well on the test data. And then we try to come up with methods where the training accuracy is going to mean something about the test accuracy. opposite, which is under-fitting, where you're just like, I don't know what's going on. That's not over-fit. It's not going to work very well. The problem here is not that you're test accuracy is low, but your training accuracy was also low because you didn't learn anything. Spam is being generated by people who are trying to defeat spam filters. Others are going to make it through, and spammers aregoing to double down on what's working. Now there's sort of an arms race here. over time, spam classification doesn't actually look like a standard classification problem because it's adversarial. Here's an example of this tradeoff. In general, we're going to do discrete classification. But for this example, let's imagine the thing we're trying to do is to fit a curve to this data. So I can pick a model. You don't want it to be too small, because if you over-fit, you're not going to be able to generalize. than the quadratic. It's about fitting your data to the point where the patterns that you are capturing are ones which generalize to test, and that's a tricky balance. Over-fitting shows up not just on these continuous functions. It also shows up, for example, let's imagine in a hypothetical digit classification, we might say, here is an image I've never seen before. So what would we do? We'd do our running total. We'd say, all right. Well, before I look at any features, the numbers are the numbers two and three,let's say, are equally likely. in a corner where there's no number. This is an example of over-fitting, because this probability versus this probability, that is about the idiosyncrasies of the samples I have in my data. What do you think, in my training data for ham versus spam, things with the highest odds ratio for ham would be? These are things that are significantly more likely for ham than for spam. Words like Gary, except when I look at my data, it's actually a mess. It turns out, there are a bunch of words in this data which occur in spam once, and in ham zero. of over-fitting, where the exact details of which sample points you drew when you collected your data get captured in a way that doesn't generalize. To do better, we need to smooth, or regularize, our estimates. In Naive Bayes probabilistic models, over- fitting usually shows up as sampling variance. For other methods, it's going to show up in totally other ways. The exact mechanics of over-fits are going to vary from model to model. The maximum likelihood estimate, basically, you have to work this out, right? Maybe we can just go back and do it real quick. OK. So let's say r is my probability of red, and one minus r isMy probability of blue. What is the probability of D? So, that's a thing you can do, and it's totally reasonable. But in practice, you need some smoothing. So I guess we have some Halloween ghosts after all, even though we're now into the post-Halloween lectures. But we want no surprises to our model. of this data? Well, it's basically I got an r, and then I got another r. And then I've also got the other thing, which is one minus r. So as I change the probability of red, this term is going to go up and down. And the balance, the point where that's going to be maximized you can sort of, if you set it up carefully, take derivatives, find the extreme point, you'll get the relative frequency answer out. the sun will once not rise. So I know that this estimate is wrong, and I need some way of mechanically incorporating the fact that there are events which I haven't seen, but which I know to be possible, or at least that I'd like to model as being possible. Laplace said, well, basically, it's a pretty good idea to take into account the probabilities in your observation, but you should hold out an extra observation for everything you didn't see to reflect it potentially happening at some point in the future. In a real classification problem, you have to smooth if you're going to use Naive Bayes. This is the top of the odds ratios for ham on the left, and favoring spam on the right. If you see money, that's a good sign that it's spam. There are some things that indicate ham. This looks like general English text. What is going on there? Helvetica vs. Verdana. If I crank down k, I fit more, and so I now have a dial which can trade off the amount of fitting against generalization. In general, your model is going to make errors. The k that's going to be most accurate on my training data is zero. That's the maximum likelihood estimate. We learn our parameters from the training data. We tune them on some different data, like some held-out data, because otherwise, you'll get crazy results. And then eventually, you're going to take the best value, do some final tests, test run. We're talking a bit more about features, because it's important for when we start to get to neural nets. In general, you're going to need more features. In spam classification, we found out that it wasn't enough to just look at words, you've got to look at other sort of metadata from the ecosystem. For digit recognition, you do sort of more advanced things than just looking at pixels, where you look at things like edges and loops and things like that. You can add these as sources of information by just adding variables into your Naive Bayes model, but we'll also talk in the next few classes about ways to add these more flexibly.

ROUGE-1: 26.40, ROUGE-2: 25.44, ROUGE-L: 24.31
BERTScore: 64.11

==============================================
==================== [18/100] ====================
Summary:
in this video we're gonna talk about how a country can gain from exporting goods or services through international trade. We're gonna look at how consumer surplus producer surplus and total surplus are going to change when we introduced the idea of trade in allowing Chile's copper manufacturer producers to trade on the global market. The world price of copper is five thousand four hundred and forty dollars a ton. Because the world price is higher than the price in Chile Chile will export copper. There is a shift of some of the consumer surplus is going to go to the producer surplus. and sell that copper on the world market and so they end up exporting they endup exporting 1 million tons of copper to other countries. So we end up with this net benefit where we get this this whole new triangle which is producer surplus but it's increasing the total surplus. As a whole Chile is better off because it's exporting this copper and so we get a net benefit of 1.5 million tons. It's a new kind of triangle where we have a producer surplus and a total surplus, which is good for Chile.

ROUGE-1: 26.24, ROUGE-2: 23.04, ROUGE-L: 20.81
BERTScore: 68.65

==============================================
==================== [19/100] ====================
Summary:
Thiazide tells us that this medication works in the early part of the distal convoluted tubule that's found within this nephron. This transporter is called the sodium chloride co-transporter and it is considered a cyanide sensitive transporter so hence why this drug works so well. While loop diuretics are a lot more effective than a thiazide diuretic but the thiazid does provide a nice diuresis effect. They're less effective in patients who have a compromised GFR a go merrill ER filtration rate. the high uric acid level and hyperglycemia again teach your diabetic patients to monitor their blood glucose really closely while taking a thigh. orthostatic hypotension this is where the when the patients maybe they've been sitting or lying down they get up they can fall they become dizzy you want to teach them to change position slowly. how do their lungs sound is their blood pressure coming down maybe they're getting this drug for hypertension how is their weights are they gaining weight or losing weight so we play a huge role with that as well okay so that wraps up this review over thighs I diuretics.

ROUGE-1: 10.35, ROUGE-2: 9.93, ROUGE-L: 10.35
BERTScore: 62.22

==============================================
==================== [20/100] ====================
Summary:
Future John Green tells you that in a stunning turn of events the 2020 presidential election will be won by - Harry Styles. Because… that’s how much we love Harry Styles in 2020. The U.S. was facing what turned out to be the 2nd worst economic crises in the past 150 years. A mixture of public and private activities that tilted towards short-term economic thinking, speculation and irresponsible spending. In the early 2000s, many millions of Americans bought real estate assuming that its value would increase rapidly and forever. Traditionally, people in this situation can’t borrow hundreds of thousands of dollars, but in the early 2000’s, these loans were giving the benign sounding designation “subprime” So it’S important to understand that it wasn’tshows just like big Wall Street banks financing huge deals with debt, regular people were doing it - like me. All this created a classic housing bubble, which was doomed to burst. Also, with the interest on government Treasury Bills effectively zero, investors had to look elsewhere for better returns. In 2008 Barack Obama was faced with America’s biggest economic challenge since the Great Depression. The Bush Administration tried to stop the damage by getting Congress to pass the Troubled Assets Relief Program, or TARP. At the time Obama's election seemed a political watershed and not just because he was the first African American president. He appealed to young people and minorities, and he harnessed the power of social media to communicate with supporters, and get out the vote, and also raise TONS of money. Obama promised to change the culture of Washington. He would end partisan squabbling. He also wanted a foreign policy based on diplomacy. He wanted to reduce inequality and increase access to health care. And he wanted to curb “greed and irresponsibility” that had helped bring on the economic crisis. But he also wanted to end the wars in Iraq and Afghanistan and reverse global warming. That’s a tall order. So how has he done? Not bad. Well, some would say not great either. For instance he launched diplomatic outreach to the Muslim world, but a lot of this was more rhetoric than action, as in his verbal support for the revolution that overthrew Hosni Mubarak in Egypt. He also followed through on his promise to end the war in Iraq, although to be fair the Bush administration had really set him up for success there. And he increased the number of U.S. troops in Afghanistan as part of a longer term plan to ending the war there, which has sort of worked? He also authorized a successful military operation that killed Osama bin Laden. depends on who you ask. Among 9 large studies, 6 found that the stimulus did have a positive effect on growth and employment, 3 found that it had little or no effect, and economists are equally divided. The stimulus is estimated to have saved about 3 million jobs, but it also increased the deficit quite a bit. Liberal economists see America’s current 7% unemployment rate as evidence that the Keynesian policies should have gone further, while conservatives say that the Stimulus exploded the federal deficit and debt. The Tea Party is concerned that deficits are out of control and that rising government spending is going to ruin America. The 111th congress was one of the least productive in American history. Unwillingness to compromise precipitated a series of mini-fiscal crises over things like the budget and raising the debt ceiling. Meanwhile, the economy has slowly added jobs and looks halfway decent at the moment mostly because Europe looks so bad. The particular brands of ideological certainty that we see today may seem new but if you look at American history you realize that this has been going on for a long time. job to protect you not only by having a standing army but also making you wear your seat belt? Those are ultimately ideological questions, but we have to grapple with them in a real practical way. And the great story of American governance is compromise. But that is also often been the tragedy ofAmerican governance as when the Constitutional Convention compromised over whether African American people were people. So if you’ve learned anything this year, I hope its been that the American story that we find ourselves in now isn’t entirely novel. And I think we have much to learn from those who came before us.

ROUGE-1: 37.84, ROUGE-2: 36.34, ROUGE-L: 36.52
BERTScore: 61.62

==============================================
==================== [21/100] ====================
Summary:
Simple graphs don't have direction. They just correspond to a mutual connection, which is symmetric. An edge just has two endpoints that are in V, and we don't distinguish the endpoints. There's a thing called multi-graphs where there are multiple edges between vertices. And there could also be self loops, but weDon't need those. Let's not complicate matters. We're talking about simple graphs. OK, so the formal definition of a simple graph is that it's an object G that has a bunch of parts. A basic concept in graph theory is the idea of the degree of a vertex. The degree is simply the number of incident edges that touch it. The handshaking lemma says that the sum of the degrees summed over all the vertices is equal to twice thenumber of edges. There is no degree 3 graph with this spectrum of degrees 2, 2, 1. It's impossible. And there's a very elementary property of degrees that we're going to actually make something of in a minute. written as a formula, twice the number of edges. The proof is trivial, but let's make something of this. It is an application of graph theory to sex. We ask the question, are men more promiscuous than women? And there have been repeated studies that are cited in the notes that show again and again that when they survey collections of men and women and ask them how many sexual partners they have, it's consistently the case that the men are assessed to have 30% more. to model the relationships between men and women by having a graph that comes in two parts. It's going to be called a so-called bipartite graph. So looking back at this graph, this edge from that blue M to that orange F indicates that they had a sexual liaison. They were partners. OK, so this is a simple graph structure that we can use to represent who got together with whom in any given population of men and woman. The left hand side is the number of-- is the sum of the degrees of men divided by the size of the M population. And here I'm doing a little trick. Notice that the F's cancel out. In the US overall there are slightly more women than men. There is 1.035 women for each man in the US population. The men degree is the female population divided by the M population times the average degree of the females. But this has nothing to do with their behavior, or promiscuity, or lack of it. It's simply a reflection of the ratio of the populations. But we do get them consistently in one survey after another. You will no longer be fooled by such nonsense.

ROUGE-1: 38.30, ROUGE-2: 36.63, ROUGE-L: 35.14
BERTScore: 66.26

==============================================
==================== [22/100] ====================
Summary:
Robotics is a really cool and important direction for the future. I really believe that we are moving towards a world where so many routine tasks are taken off your plate. I like to think of robots as as the machines that put computing in motion and give give our our machines in the world the ability to navigate and to manipulate the world. We have artificial intelligence which enables machines to see to hear and to communicate and to make decisions like humans and then we have machine learning and to me machine learning is the key. is about learning from and making predictions on data and this kind of application of machine learning is Broad it it applies to cognitive tasks and physical tasks but regardless of the task we can characterize how machine learning works as using data to answer questions that are either descriptive predictive or prescriptive. In the context of robots we have three types of learning and you have seen different aspects of these methodologies throughout the course we have supervised learning and unsupervised learning and then we have reinforcement learning which is about learning. Robots have a cycle that most often consists of three steps we have perception a perception step we have a planning and reasoning step and we have an action step and so this is what we are going to talk about today. We use data this is manually labeled data that gets fed into a convolutional neural network and then the labels are used to classify what the data is so for instance for this image we may have classifications like car duck Road and we do this so that when the system when the car sees a new image the car could say oh this is a this is ducks on road now. to do given input reinforcement learning is causing a huge revolution in robotics. Reinforcement learning is concerned with how intelligent agents ought to take action in an environment in order to maximize the notion of a cumulative reward. In order to get the simulation to drive a real robot we actually need to think about the Dynamics of the robot and so in other words we have to take into account what the vehicle looks like and its kinematics what are its Dynamics and so it's really cool because really we are now able to train in simulation. In 1995 a Carnegie Mellon project called nav lab built a car that was driven by a machine learning engine called Alvin and Alvin drove this car all the way from Washington DC to Los Angeles. The car was in autonomous mode for a large part of the highway driving but there was always a student there right ready to um to take control and the car did not did not drive inonomous mode when there were when it was raining or when there was a lot of congestion or when the car had to had to take exits. Now 1995 is a long time ago right I mean it's before many of you were born. This work um computers needed about 10 minutes to analyze an image can you imagine okay so how do you go from that to enabling an autonomous vehicle to drive at 90 kilometers an hour well um what they did was they they developed some very fast solutions for paring down the image to only the the the aspects that they needed to look at. They assumed that there were no obstacles in the world which made the problem much easier because all the car had to do was to stay on on the road. This has been a game changer for autonomous cars and we're getting back to the connection between hardware and software. Alexander: We have very effective and Deployable solutions for robot cars that move safely in Easy environments where there aren't many static nor moving obstacles. Many companies and research teams are deploying and developing self-driving cars. Many of these preconditions revolve around certainty in perception planning learning reasoning and execution before we can get to Robo taxi but we can have many other robot solutions that are much that that can happen today. Alexander: We can use deep learning and reinforcement learning to take us from images ofroads onto steering and throttle and what you can do with this is really great. Alexander developed the Vista simulator. The Vista simulator can model multiple agents multiple types of sensors and multiple types. of agent to agent interaction. The solution also allows us to to localize the the vehicle so it's really super exciting okay so we can. We can get this human-like control but assuming light control requires a lot of data and I've told you at the beginning that we have to be careful with the data. From this from this data this data is processed and it's from thisData we can learn to maximize the likelihood of particular control signals for particular situations.  liquid networks are Dynamic causal models and I want to show some more examples that explain how these models are Dynamic ozone models. These are models that essentially test the data that we have used to train a human pilot to drive a drone. We're able to extract decision trees from these kinds of solutions and these decision trees provide understandable explanations from human understandable explanations. This is really important for safety systems all right so let's see some examples of the data we've used to test this data and we've had a pilot drive a Drone to test these models. that yields models that generalize to unseen scenarios essentially addressing a challenge with today's neural networks that do not generalize well to unseen test scenarios because the models are so fast and and compact you can train them online and on edge devices. We have one project that is looking at whether we can understand the lives of whales and so what do I mean by this so here is an example where we have used a robotic drone to find whales and look at what they do and track them and here is some some clips from what the system is able to do. your dimensions and can create a bespoke shoe just for you. All the all the clothing all the items in our environment can kind of awaken our clothing could become robots. The temperature could get adjusted automatically by monitoring people's comfort and gesture. Just-in-time Holograms could be used to make the virtual world much more much more realistic much more connected and so here they're discussing the the design of a new flying car and let's say we have these flying cars and then we can integrate these cars with the it infrastructure.

ROUGE-1: 22.83, ROUGE-2: 21.69, ROUGE-L: 21.44
BERTScore: 66.42

==============================================
==================== [23/100] ====================
Summary:
hair up grab an apron let's go now with the new chefs in place. I just can't wait to get to work and prove to my mom and my sister that I can do this I think the message is clear nobody scared to walk through that door and get their hands dirty in that kitchen no we're not tomorrow is a big day let me tell you I need everyone on their game good night guys get some sleep thanks thank you very much chefs oh my God this is amazing last night Tatiana and her team eagerly learned a few of the new recipes. spaghetti meatballs absolutely delicious and then finally pizzas four of them Margarita prito zucchini done with the shrimp and the meat lovers and again we have a massive asset there in that pizza Cen and we're going to take advantage of that yeah dig in let's go oh my gosh this is so beautiful m oh my God I am thrilled this is amazing this is like the menu of my dream Oh my God the pizza are incredible ohMy God I love that I know I'm excited for the new menu.

ROUGE-1: 52.19, ROUGE-2: 51.29, ROUGE-L: 52.19
BERTScore: 71.66

==============================================
==================== [24/100] ====================
Summary:
HONG LIU: This is a key relation between the bulk and the boundary theories. The more you go to the interior of the space time, then corresponding to the lower energy process when viewed form the field theory. So the duality is that once you realize there's such relation, since the two sides are completely different objects, so the game is that you really have to do lots of guess work. Essentially, you have two sides if you want to relate things on two sides. HONG LIU: We have N equals 4 super Yang-Mills theory. And then here, you have type IIB string in Ads5 times ds5. Here on this side, there is a conformal symmetry which we explained before because this is a four dimensional theory. But being the right hand side, we still have some SO 5, S5. So this is more like a space time symmetry. And the isometry of S5 precisely gives you SO6. And we will not be explicit. You can also map the supersymmetry between them. For each global symmetry in the field theory side, there's a corresponding local symmetry on the gravity side. The isometry is important for the following reason. Even though this is a subgroup, when we talk about quantum gravity, the AdS5 times S5 specifies the asymptotic geometry of the space time. So-called large transformations is that they don't go to the Identity at infinity. These large gauge transformations can be considered as the global part of the diffeomorphism. The gauge symmetry is just redundant freedom. You never see it on the other side. The classical gravity limit is the same as QFT, Quantum Field Theory in curved space time. So gravity does not fluctuate. But your matter field can fluctuate, h bar equal to 1. And then alpha prime goes to zero. So this is what we expect. If you still remember what we did before in the large N gauge theory, in theLarge N limit, the fluctuations become very small, et cetera. So that means that we can actually use classical gravity to, in principle, solve problems which are strongly coupled. In principle, the corrections beyond this limit can again be studied on the gravity side. So this is the classical limit. In the classical string limit, still you can see the N go to infinity, which corresponds to GN. But here, the alpha prime can be arbitrary. So let me just say alpha prime finite no longer zero. And then this is just corresponding to Lambda finite, which is no longer infinite. So any questions? Yes? AUDIENCE: Doesn't small alpha over R squared mean large GS? essentially the semi classical gravity limit because we still treat the matter fields essentially as quantum. HONG LIU: Yang-Mills theory lives on Minkowski space. And then you say you can imagine that this is the boundary, this relation is related to the bulk and the boundary. And this is a postulate based on that fact. Yes? AUDIENCE: I thought one of the motivations for thinking about the holographic duality was to try to escape [INAUDIBLE] And all of a sudden, it strikes me, so we're trying to get on the boundary of AdS. Then they will also exist in the massless particles. HONG LIU: Given this mapping, any operator is due to a bulk field. So for local operator on the field theory side, we can immediately ask questions related to operators on this side, and try to ask what's the counterpart on the other side. And ask the story about the field on the gravity side. We can start developing the relations. So do you have any questions regarding this? So now I'll try to answer this question. As I said before, in establishing such dictionary you often have to do lots of guesswork, and then you check. HONG LIU: Phi in principle can fluctuate. And its expectation may also be able to fluctuate in the space time. So expectation value essentially can be associated with the boundary value of the field. With this relation, we have established a connection. between the Yang-Mills theory coupling with the value of phi at the boundary of AdS. With the relation, the operator corresponding to the dilaton must be the N equal to 4 super Yang-mills Lagrangian. phi 0 O in the boundary theory must be related-- now I'm generalizing this story. The bulk field phi due to O has a boundary value phi 0. If you want to preserve the symmetry of the original Lagrangian, then you can choose a certain O. But in principle, you can choosing any O. You can break the symmetry if you want. Impose this kind of boundary conditions. Then you may break AdS symmetry too. So in the end, what we will describe is a self consistent story. I will not contradict myself in my later discussion. J mu. For simplicity, let me just take it to be u(1). Then I can deform the boundary theory by adding a source for this J mu. And a mu is the source. And then according to this identification, the A mu must be A mu-- we should be able to identify it as the boundary value of some bulk field. So this is a bulk vector field. But now we can argue why this should be a gauge field. And to see this is very easy because since J mu is conserved, then this coupling-- so let me call this star star star. T mu mu to the boundary Lagrangian. And this is the source to the stress tensor. This is when h is very small. When we consider the information, we always consider the source is small. But now, we can argue this thing, mass corresponding to boundary value of the metric in the gravity side. So this is a very general statement valid for any correspondence between the gravity and the AdS metric. And now, from your knowledge of quantum field theory, when I add such a term to the field theory,. it's the same. and the field theory. And that also tells you that if you have a theory which due into a higher dimensional theory, and then that theory has a stress tensor, then this bulk theory must have gravity. So you can say, if any field theory is due to a theory of one higher dimension, that theory must involve gravity-- nothing about quantum gravity. Let's stop here. We're not going to get into quantum gravity right now, we're just going to talk about the theory of gravity.

ROUGE-1: 22.74, ROUGE-2: 21.94, ROUGE-L: 21.39
BERTScore: 65.45

==============================================
==================== [25/100] ====================
Summary:
Bolek Wyslouch: How do you convert a given physical system with all the forces, et cetera, into some sort of fixed form, fixed type of notation? Bolek: Once we understand two, we will then generalize to infinite number of oscillators, which is actually-- so this model, which consists of weights hanging under the influence of gravity plus the springs will be then used for many applications of the concepts later in this course. Boleslaw: We can come back to that initial conditions and impose some initial conditions. BolesLAW WYSLOUCH: The coordinate system is this. When we start talking about the system in principle in the case of somewhat larger angles, you have to worry about vertical positions as well. So let's look at what are the forces acting, for example, on this mass, the mass, which is-- if it's displaced from a vertical position. Temporarily, let's introduce an angle here to characterize this displacement from vertical. And then we just plug in-- also the angle can be converted into position by realizing that the distance times the angle is equal to displacement, the usual geometry. me do everything. Let's write down everything in the matrix form, because it turns out that linear matrices are very useful for that. We will use them very, very-- in a very simple way. So instead of repeating writing, all the x1s, x2, et cetera, instead I just stick them into one or two element objects. I use matrices to multiply things, and if I want to know x1 and X2, I can always go, OK, the top component of vector x, lower component of vectors x gives me the solution. Right? BOLESLAW WYSLOUCH: The physics answer is to find fixed frequency modes us such that the system, the complete system, oscillates at one frequency. This is so-called normal mode. It turns out that every of the system will have a certain number of frequencies, normal modes, that would-- the whole system would oscillate at the same frequency. So I propose that-- so of course, we use the usual trick that anytime we have a solution in complex variables, we can always get back to real things by taking a real part. Mathematically, the way to find out the oscillating frequency is you take a determinant of m minus 1 K minus i omega squared must be equal to 0. So let's try to see how to calculate things. So this is the matrix that contains all the information about our system, the mass, the gravitational acceleration, the length, the spring strength, et cetera. And we assumed they oscillate with a fixed frequency. But if I find them, this would correspond to the normal frequencies. is basically equivalent to the following equation g over l plus k over m minus omega squared. So there is one solution, one oscillation, that does not depend on the spring constant. And there's a second solution which corresponds to minus, where omega squared is equal to g overl plus 2k over m. And this is interesting. This is a normal mode. And it can go forever at this particular frequency. The spring is completely irrelevant for this motion. If I cut it off, the motion will not change. It just happens that two identical pendula are going at their own natural frequency. so it's stretch from both sides. And the whole system oscillates at the same frequency, and because of this additional force of spring, the frequency is actually higher, it's larger. All right, so that's the first step in understanding the system. We now know that there are two oscillations and two normal frequencies. The next step to finish our understanding of the system in a mathematical way, to describe it fully, I have to know what is the shape of oscillations. So in principle, at the end of the day, I still want to know how much 1 moves, how much 2 moves. The shape, 1 and 1, and 1 minus 1 is fixed, because these are the shape of normal modes, which corresponds to those frequencies. And the superposition of x1 plus x2 gives you the most general combination of possible motion. So if I write this down now in terms of position of number 1 and number 2, so I have a position of X1 as a function of time. In general, it will look like this. It will be some sort of constant alpha, cosine omega 1 t plus phi. The magenta is normal mode number 2. And blue and the red are the actual pendula. And the motion of blue and red is simply a linear sum of the two. And this is exactly what-- this is the computer simulation that shows you that one of them is going up, the other one down, et cetera. This is for the certain combination of initial conditions. I could go change initial conditions in my program and have a different behavior. But whatever happens, I would be able to-- it will always be a combination of thetwo motions. BolesLAW WYSLOUCH: You can have all kinds of motions by simply adjusting initial conditions. So do you know-- so this is how we can have different shape of motion, depending on the initial condition. So let's say, in fact, do I have it in this one here? Yes. So now, let's take it to, for example, Jupiter. Jupiter, g, is much larger. So the frequency would be larger. Things will be faster, right? That's the higher frequency. But also the difference between two frequencies will be smaller. like this. OK? So there are in fact two-- when you look at this picture, you can see two frequencies. One which is clear the oscillation of the-- high-frequency oscillation. But there's also this kind of overarching frequency of much smaller frequency, and this is what corresponds to a difference of two things. So we see this here. We see it on the pendula. But now what we are going to do is we're going to try to hear it, right? So this is a demonstration which maybe it works, maybe not. sort of early on, to instead of, so far, when we talked about pendula, we describe their motion in terms of motion of number 1, motion of Number 2. It turns out we can rewrite the equation into some sort of new variables, where, so-called normal coordinates, where you'll simultaneously describe both of them and then kind of mix them together to have a new formula. So instead of keeping track of x1 and x2 independently, you define something which I called u1, which is simply x1 plus x2. The determinants needed no matrices, no nothing. We just added and subtracted the two equations, and things magically separated. So you can always have a linear combination of parameters for arbitrary size coupled oscillators system where you combine different coordinates, and you basically force the system to behave in a way in which it induces the single oscillation, single frequency. So this is, again, a very powerful trick, but usually for most cases, you can do that only after you have solved it, after you've found out normal modes, et cetera.

ROUGE-1: 21.63, ROUGE-2: 21.06, ROUGE-L: 21.33
BERTScore: 60.13

==============================================
==================== [26/100] ====================
Summary:
GILBERT STRANG: Take a bar, a material bar. The ends of the bar are kept at temperature zero, they're frozen. Heat is flowing around in the bar, and where is it going? It's flowing out the ends. So I insulate the bar and I put it in the freezer. And I have an ordinary bar and an ordinary temperature. And the heat is escaping out the sides, so the end x equals 0 and the end f equals 1. And that's the solution. differential equation. We have a whole function to match, so we need all of those. And Fourier series tells us how to do that matching, how to find these Bk's. So that's a separate and important question, Fourierseries. Thank you for your time. Back to Mail Online home. Back To the page you came from. Back into the article you came From. The story behind the story: Click here to read the full transcript of this article. Back onto the page of the story you come from.

ROUGE-1: 20.68, ROUGE-2: 16.05, ROUGE-L: 15.75
BERTScore: 54.96

==============================================
==================== [27/100] ====================
Summary:
James Swan: I hope everybody saw the correction to a typo in homework 1 that was posted on Stellar last night and sent out to you. We have four course staff that review all the problems. We try to look through it for any issues or ambiguities. Today, we'll look at another one, the eigenvalue decomposition. And on Monday, we will look at a different decomposition called the singular value decomposition, which is a different type of transformation than the one we looked at today. "We ran out of time a little bit at the end of lecture on Wednesday," he says. "There were a lot of good questions that came up during class" "One topic that we didn't get to discuss is formal systems for doing reordering in systems of equations" "We saw that reordering is important. In fact, it's essential for solving certain problems via Gaussian elimination" "It's the difference between getting a solution and writing a publication about the research problem you're interested in and not" Permutation matrices are one class, maybe the simplest class, of unitary matrices. They're just doing row or column swaps, right? That's their job. If I swap the rows and then I swap them back, I get back what I had before. So this product here does nothing to the system of equations. It just swaps the unknown. There are a couple other slides that are in your notes from last time that you can look at. We'll see some today. James W. Swan: We discussed sparse matrices and a little bit about reordering and now permutation. He says permutation is a form of preconditioning a system of equations. He shows a simulation that tells us how probable it is to find the Plinko chip in a particular column. Swan: We could construct an alternative model that didn't have that part of the picture that we wanted to have. He's happy to answer questions on.at and he'll be back in a few minutes. Eigenvectors of a matrix are special vectors that are stretched on multiplication by the matrix. So they're transformed. But they're only transformed into a stretched form of whatever they were before. For a real N-by-N matrix, there will be eigenvector and eigenvalues, which are the amount of stretch, which is complex numbers. And finding these values is the subject that we focus on today at the end of the talk. The talk will be on the subject of sparse matrices, which have a sparse structure. eigenvectors and eigenvalues are non-linear because they depend on both the value and the vector, the product of the two, for N plus 1 unknowns. They seem like special sorts of solutions associated with a matrix. And if we understood them, then we can do a transformation. But how do you actually find these things, these eigen values? Well, I've got to solve an equation A times w equals Lambda times w, which can be transformed into A minus Lambda identity times w. Determinant of a matrix like A minus Lambda I is a polynomial in terms of Lambda. The N roots of this characteristic polynomic are called the eigenvalues of the matrix. There are N possible lambdas for which A minus lambda I becomes singular. Here's another matrix. Can you work out the eigevalues of this matrix? Let's take 90 seconds. You can work with your neighbors. I'm going to do it myself. Anyone want to guess what are the eigervalues? Eigenvalues have certain properties that can be inferred from the properties of polynomials. The determinant of a matrix is the product of the eigenvalues. Diagonal systems of equations are easy to solve, and it's easy to find their eigen values. If there is a complex eigenvalue, then necessarily its complex conjugate is also an eigen Value. The trace of aMatrix is the sum of its diagonal elements, and the elements of a triangular matrix are eigen Values, too. a matrix is also the sum of the eigenvalues. These can sometimes come in handy-- not often, but sometimes. For example, a rate matrix can tell us something about how different rate processes evolve in time. The rate matrix has units of rate, or 1 over time. And they tell us the rate at which different transformations between these materials occur. The characteristic polynomial of that is. The eigen values of that matrix are going to tell us. something about the rate processes. Eigenvalues can be interpreted in terms of physical processes. This quadratic solution here has some eigenvalue. It's not unique, right? It's got some constant out in front of it. So add the first row or subtract the second row. And then we'll compare. This will just be a quick test of understanding. Are you guys able to do this? Sort, maybe, maybe an answer, or an answer for the eigen value. Is that too fast? Are you OK? No. James Swan: Can you find the eigenvalues and some linearly independent eigenvectors of a matrix? He says if an eigenvalue is distinct, then it has algebraic multiplicity 1. He says geometric multiplicity is the dimension of the null space of this matrix. Swan: When we don't have a complete set, we're going to have a hang-up associated with a problem in your homework, he says. But he says it's useful for solving systems of equations or for transforming systems. in a lot of cases. You can prove-- I might ask you to show this some time-- that the eigenvectors of a symmetric matrix are orthogonal. They're also useful when analyzing systems of ordinary differential equations. So here, I've got a differential equation, a vector x dot. So the time derivative of x is equal to A times x. And if I substitute my eigendecomposition-- so W lambda W inverse-- and I define a new unknown y instead of x, then I can diagonalize that system of equations. triangular form for this matrix. We'll talk next time about the singular value decomposition, which is another sort of transformation one can do when we don't have these complete sets of eigenvectors. You'll get a chance to practice these things on your next two homework assignments, actually. So it'll come up in a couple of different circumstances. I would really encourage you to try to solve some of these example problems that were in here. Solving by hand can be useful.

ROUGE-1: 24.65, ROUGE-2: 21.90, ROUGE-L: 21.73
BERTScore: 62.91

==============================================
==================== [28/100] ====================
Summary:
A random variable is a number that's produced by a random process. Number of faulty pixels in a monitor is also a random variable. The number of alpha particles detected by a Geiger counter in a second is believed to be a random number. And if I flip coins then the number of heads in a given number of flips-- let's say I flip a coin n times -- will be another rather standard random variable, the professor says. The professor concludes by saying, "We just saw some random variables come up in the bigger number game" heads is a number that comes out of this random process of flipping the three coins. Another one is simply a [? 0-1 ?] valued random variable where it signals 1 if all 3 coins match in what they come up with, and 0 if they don't match. One of the things that's a convenient use of random variables is to use them to define various kinds of events. The event that C equals 1, that's an event that-- it's a set of outcomes where the count is 1 and it has a certain probability. We think of the outcomes in the sample space as the results of a random experiment. Usually this would be a real valued random variable. Usually it's the real numbers. Occasionally we'll use complex valued random variables. Actually, that happens in physics a good deal in quantum mechanics, but not for our purposes. We're just going to mean real value from now on when we talk about random variables and random variables are a total function, by the way. They are an outcome and they have a probability. And when the outcome is translated into a real number, that's what the random variable does. could say explicitly where it comes from as an equation. It means that the probability that R1 is equal to a1 and R2 is equalto a1. And the definition then of mutual independence of the random variables R1 through n, Rn holds is that this equation it holds for all possible values, little a1 through little an. So let's just practice. Are the variables C, which is the count of the number of heads when you flip three coins, and M, [? the 0-1 ?] valued random variable that tells you whether there's a match, are they independent? Well certainly not. this can have value 0 and 1. If I have k random-- if I have a bunch of random variables, a large number much more than k, they're k-way independent if every set of k of them are mutually independent. And of course as with events we use the 2-way case to call them pairwise independent. There's k Hi's and there's O, which is the mod 2 sum of the indicator variables Hi from 1 to k. And what we saw when we were working with their event version is that any k of these events are independent. makes the k plus 1-- k plus first. And the reason why any k of them were independent was discussed in the previous slide when we were looking at the events of there being an odd number of heads and a head coming up on the i flip. For a bunch of major applications this pairwise independence is sufficient. It's harder to check mutual independence. You've got a lot more equations to check. We'll be making use of it in an application later when we look at sampling and the law of large numbers.

ROUGE-1: 37.35, ROUGE-2: 35.40, ROUGE-L: 34.88
BERTScore: 67.16

==============================================
==================== [29/100] ====================
Summary:
SAR Gowan as a ruthless bloodthirsty Warrior um we really don't see him in that regard. The Green Knight is the villain he's the antagonist versus the protagonist but does he truly represent that role of villain that we expect a villain to play. Throughout the story of seral when we again come back to what a knight truly is what they believe their practices their nobility you know there there's the PowerPoint on Lut where it talks about their their pentagram their four or five things. well must be five it's pentag the five things that you know that the way that they live their life. Being HonorBound having honor is huge you've seen that in movies you know Samurai and stuff people would rather die than be captured and so they fall on their sword right that's very you know Roman to some degree with regards to Brutus and and and uh some of those other Emperors um you know from from the past um so anyways follow along uh we're going to uh go through this it'll take a little bit of time and uh we'll talk about it upon completion. bath he didn't know that he was going to have to marry that woman did he no would he have still done it well how much does he value his life you know probably um but here gow G is trying to figure out exactly what's going to happen and so they decide to uh to have the fight there at the bottom 176 and by fight I just mean contest um the the Green Knight graciously stood on the ground with his head slightly slanted he even got down exposed his neck the naked neck for the business. The Knight tells GNE you have a year to find me you can find me I the Knight of the green Chapel green Chapel no kidding Everything Green line 232 233 come or be called a coward. The Knight came here specifically to challenge the best that Earth has ever bred is what he was saying and so this is the moment and now GNE has a year and a day to think about his impending doom is he going to go do we think he's going to take the axe shot is this all a test is he Going to trick something I mean these are all thoughts that should be going through your mind. you receive the the hunter the owner of the house the husband says whatever you receive you must give to me that seems kind of weird what possibly could he receive while he's there well we find out that the wife really kind of starts to come on to him over time okay real flirty she's she's a temptress okay very seductive uh she gives him a kiss what does he do when the husband comes home that day cuz the husband's going to share his his food share his everything with him as long as GNE you share with me. up my sleeve come time to get that shot in the head um and so he does leave and he keeps that GLE for himself um and doesn't give that away page 185. He roamed looking for this man he roamed up to the roof of that rough dwelling then from that height he heard from a hard rock on the bank beyond the brook a barbarous noise what it clattered amid the cliffs fit to cleave them apart as if a great sight were being ground on a grindstone. Justin: The Green Knight is a coward and you were just proved to be a coward. The Knight knows about the girdle and so he knows all about the wife giving that to the Green Knight. Justin: If he never violated his oath the most he was ever going to do was scare him and probably wasn't even going to hurt him. He says do it again I will not flinch go on game on 185 187 excuse me um I shall stand your stroke not starting at all till your axe has hit me. so he's the protagonist but yet I don't think uh you know the whole good vers bad protagonist you know hero villain protagonist antagonist it's kind of a a mdl you know very murky we don't understand exactly who it is. We do have the Damsel in Distress we have that woman the temptress um the sexual being that's trying to corrupt the hero um and take him down the path maybe uh satanic in some degree with regards to trying to lure the good person away with the forbidden fruit.

ROUGE-1: 29.41, ROUGE-2: 28.55, ROUGE-L: 28.03
BERTScore: 65.01

==============================================
==================== [30/100] ====================
Summary:
hey what's going on YouTube boy Robert and my mission is to teach you everything in the kitchen now earlier this week at work I learned and I was wondering what are the fruits and vegetables I can turn in the salt as well so today so these are the fruit and vegetables we have today we have blueberries strawberries Kiwis red beets yellow beets pineapples right in fruit and cucumber this is the equipment you'll need for today you don't need one blender sheet trays mason jars parchment paper aluminum foil a spice grinder a bowl with a strainer and a plastic spatula. I did that right because the fruit was already soft and I blue knitted with water it went right through the strainer so next time I'll call it just smear it on the actual cutting board itself another cutting board the sheet tray yeah I'll smear on the key trade first but the two prettiest ones I would have to say it was probably the red beets [Music] now movies be sweeter you can blend it with sugar and but I had sort of sweetness as for these a cucumber the Brendan beets and the red beans a blue noodles with salt so they could be added to savory dishes.

ROUGE-1: 53.94, ROUGE-2: 53.58, ROUGE-L: 53.94
BERTScore: 69.91

==============================================
==================== [31/100] ====================
Summary:
Sarah with register nurse rn.com goes over how to check Vital Signs. Vital Signs are pain oxygen saturation temperature heart rate respirations and blood pressure. Before you start what you want to do is perform hand hygiene. You want to provide privacy to the patient and tell them what you're going to be doing because you's going to have to touch them in order to do this so let's get started the very first thing we want toDo is ask them if they are in pain. If they do have pain ask them the quality what does it feel like. we're going to do turn your thumb Omer on make sure you're using the proper sleeves if you have any sleeves for it clean it everything like that follow your hospital protocols and have the patient lift up their tongue and put the probe underneath the tongue and have them close the mouth with the tongue over the probe and hold it there until it beeps a normal temperature is about 97° fit to 99° F okay and take the thermometer out and read it and his temperature is 98.2 and then clean it properly per Hospital protocol now I'm going to take his oxygen saturation every system has different ways of how they measure it. A normal pulse rate in an adult is 60 to 100 beats per minute okay the heart rate I got 60 and his respiration were 16 now we are going to get his blood pressure now whenever you're getting blood pressure you want to make sure that you get the right size cuff in most settings they have the automatic blood pressure cuffs where you don't have to blow it up yourself. A lot of times you may have to learn how to do a manual one now my previous video and a card should be popping up I go over the two-step method. blood pressure cuff you have these little arrows and it says left arm right arm and this is his left arm so we're going to make sure that we put this Arrow about 1 to two inches above that artery so let's slide it up and then make sure our cff it properly. Put our stethoscope in her ears and you're just going to place it over where you have heard that brachial artery and then you'reGoing to blow the cuff up to about 180 to 200 mm of mercury or until you don't hear that braak your artery anymore.

ROUGE-1: 41.36, ROUGE-2: 39.67, ROUGE-L: 41.26
BERTScore: 70.90

==============================================
==================== [32/100] ====================
Summary:
The grenade algorithm uses a minimal set of three-point correspondences to solve the camera pose estimation problem. The disadvantage of this particular formulation is that it ends up with a four degree polynomial which means that it could give up to a total of four possible solutions. The solution that gives the minimal reprojection error would be considered as the correct solution. The question becomes whether can we find the solution directly from any four point correspondences or more than four point Correspondences such that the solution is unique. rewritten into this form and uh interestingly we note that for every equation here it's only a function of our two unknowns so uh we'll get this system of uh polynomial equation written as f12 that is dependent on s1 and s2 and f13 depending on s 1 and s3. So as a result we'll first get six polynomials in terms of uh s i and s j so why is the reason why we have four points and each pair gives one polynomic so we have to do four choose two. any of the unknowns in the the depth where we simply write it as s i square over here so this can be derived in in this way so here i wrote out all the six combinations of the polynomial equations f i j over here. Each one of this equation it's a just a function of two unknowns  in the four uh unknown depths s1 s2 s3 and s4 and any three of them it's going to give rise to a fourth order polyn coefficients equation. three by one vector it consists of lambda squared, rho and rho squared which are the unknowns that we want to solve and now uh this is actually an over determinant system because b here is seven by three so this means that we have seven equations over here but only three unknowns. Taking svd of b we get the left singular factors multiplied by the singular values as well as the right singular vector matrix over here and the solution would be the one that corresponds the last column of v over here so once we have solved for the null space vector y using the svd what we can do is to proceed on to solve for Lambda and Rho. solve for lambda and rho uniquely here which means that after solving this and putting it back into the equation over here we'll be able to get a unique solution for t5. In the case where the ratio that's solved by y 0 over y 1 and y 1 over y 2 is not the same we can simply take the average of these values to get the final solution of x. Once we have gotten all the unknown depths we can do the same thing to to apply absolute orientation to recover the camera pose as in the grenade algorithm. correspondences and we can see that because rho and Lambda here can be determined in a unique way and x here can also be determined. We can simply take out the average of this because all the illusions are going to be quite similar or quite close to each other. As a result we'll get the unique solution provided that the four points are not degenerate so uh in this linear four point algorithm the the there will still be two degenerate cases where uh if theFour points are collinear. equation we can stack them all up so one two all the way to six equations and we end up with a coefficient matrix with an a matrix of of dimension 6 by 5. in order for non-trivial solution to exist then this guy here better be of a maximum of rank four so what we can do here is that we can take the svd of a and this will give us u sigma v transpose where we simply the vector that corresponds to the least singular value in sigma. The epmp algorithm mitigates the problem of the linear endpoint algorithm that was shown earlier on by quan and lun that was published in the year 1999 which has cubic complexity in the order of the number of points. The objective is actually to find out the relative transformation between the camera frame and the world frame and notice that in this case the way that it's they define a control a set of control points in this particular approach over here they actually avoid the expression of or expressing the constraints using the unknown depth altogether so there's no unknown depth here. We'll first see how to solve for alpha as well as cj in the world frame using a simple technique that was proposed in this particular paper. The first step would be to compute the centroid of the world points that means that i give uh we are given all the this points which we denote as p i c over here expressed in world frame. We can then make use of the distribution of the set of 3d points in the World frame to get the other three control points. The only set of unknowns that remain would be the control points in the camera frame denoted by cj uh to the superscript of c where j here equals to uh 1 2 3 and 4. And we saw earlier on that this can be expressed in terms of p-i-c which is the weighted sum of the control Points. And since we can make w here the subject we can substitute w back into the first two equations to eliminate w over here the projective scale over here hence we get these two independent equations. The points are actually the same 3d points that is defined in two different reference frames so let's say these are the 3d point p i w would be all expressed with respect to f w. We can solve r and t between these two frames using the absolute orientation algorithm. The efficient pmp algorithm is a on complexity algorithm that solves the camera post estimation problem using theabsolute orientation. It solves for the unknown depth and then make use of the known depth to solve for the rotation and translation using the Absolute post-estimation problem. for the camera pose using a set of control points using four control points in particular and we saw that this uh it's uh the complexity is a linear in terms of the number of points which is much easier to compute then we also saw the degeneracy cases for the camera post estimation problem in particular. If all the points plus the camera center forms a plane then this is also a degenerate case and that's the end of today's lecture thank you mx equals to zero. Mx equals zero.

ROUGE-1: 23.78, ROUGE-2: 22.94, ROUGE-L: 23.20
BERTScore: 71.88

==============================================
==================== [33/100] ====================
Summary:
Markus Klute: If there is a time dilation effect due to gravitational fields, then there's also a redshift which is of gravitational fields. He asks you to estimate the magnitude of this effect. Klute says the speed of light is pretty fast, 3 times 10 to the 9 meter per second. And this distance is only 22 and 1/2 meters, so we find that this is a tiny, tiny,tiny effect, he says. But nevertheless, experimentalists at Harvard tested this effect in the 1950s and '60s.

ROUGE-1: 39.72, ROUGE-2: 36.58, ROUGE-L: 39.72
BERTScore: 71.50

==============================================
==================== [34/100] ====================
Summary:
During the semester we have a few recitation instructors they help with the students during the recitation section. During those sections your the the instructor will solve a similar problem like what is actually covered during the same during the lecture and that give the students another chance to look at more example and to get for media will get used to the calculation which we carry how for the first time during the the lecture. We did not record the Recitation sections during the fall semester in 2016 on the other hand we included problem-solving videos from Professor with Busha.

ROUGE-1: 67.77, ROUGE-2: 66.22, ROUGE-L: 67.77
BERTScore: 85.45

==============================================
==================== [35/100] ====================
Summary:
In this video, we're going to compute some useful quantities for the exponential random variable. The CDF of x is the probability that X is less than or equal to little x. We use the standard formula, which is minus infinity to infinity t times fx of t dt. For this, if you evaluate the balance, 0 makes this 0, and 0 to get 1 over. And so the expectation is 1 over lambda, and so the variance is part c, so OK, so far so good.

ROUGE-1: 8.43, ROUGE-2: 7.36, ROUGE-L: 8.24
BERTScore: 68.58

==============================================
==================== [36/100] ====================
Summary:
Professor: Can you explain the physical significance of the crystal momentum? Professor: The information about the momentum can be encoded in these spatial variation of the phase of the wave function. Professor: A central property from the Schrodinger equation is at the time variation d dt of p is equal to the expectation value of minus d the potential of x d x. The momentum expectation value is time independent, so it can be written in the form of e cubed, equal to e to the minus i upon h bar p l. Glossing over in the entire story here. Which of the following. So, is u of x a real function? Well, so when we started out asking what are the eigenfunctions of the transit by l operator, all we showed was that, and I'm going to do this on a separate board just to make it clearer. Tell me if this turns off, because it kept bumping. OK. So we've determined is that if we take q l is equal to alpha, then Phi sub q if eigenvalue label by its eigen value, q, can be written in the form e to the i q x u sub q of x. In a new way of thinking about the wave function, we have a potential in terms of the potential of l. The eigenfunction of a wave function is defined by the terms of its potential. The potential is the potential in this statement that what we have here is just some translation by x. So we have this potential, and if we translate it by l, then I do a new construct called a new potential. We have a new wave function called Phi sub q, which is equal to e to the i q naught x. This diagram is telling you is which e's are allowed. If you put on a capacitor, played across your perfect lattice, you don't get any current. So the particle, the charged particle in your lattice just oscillates back and forth in a block oscillation. And that is manifestly what happens with copper. But the experimentalist comes back to you and says look dude. That is a ridiculous model because the copper isn't in fact perfect, it's messy. So how do you test the model? Well there are two ways to test the situation. One is you improve the model to incorporate properties that copper actually has. And see if you can actually get the same conductivity. the theory side, because I'm a theorist and you should not let me in a lab. But I collaborate with experimentalists, so they're nice people. They're very good physicists. So here's something you can do. You can build a system that has exactly a periodic potential. It turns out it's very difficult to do this with quantum systems. But what you canDo it with lattices not of atoms, but lattices of dielectric. That equation can be put in exactly the same form as the Schrodinger equation for the time evolution of a wave function. To handle an electric field, you need the potential to be constantly varying. In this experiment, so as the wave packet moves along, what's discovered is that the position-- if I draw the x as a function of t, so now the role of t is being played by the distance it's moved along the wave guide, what you find is that it does this. It exhibits beautiful block oscillations. And this has been proved in a very small number of real honest quantum mechanical systems. situation, it depends on the system. And exactly how it depends is something that is an active area of research. So don't throw away the model. Observe that you've modeled the wrong system. If you find a system that fits your-- that is-- that shares the assumptions of your model, that's when you ask did it work. And it worked like a champ. OK. Now let's talk about real materials. This is going to close up our discussion bands and solids. And this is actually what I wanted to get to at the beginning of the lecture. single electron, and let's put it in the system. What state will this single electron fall into? Yeah one event. But which state? AUDIENCE: [INAUDIBLE] PROFESSOR: Yeah, if you kick the system around, you let it relax a little bit. It's going to fall down to the ground state. You have to couple to something else like hydrogen has to be coupled with an electromagnetic field to decay. But couple it, kick it, and letting it decay. It'll settle down to its groundState. macroscopic amount of energy. Well, it's not macroscopic. it's large. It's not infinitesimally small. That means that there's a minimum amount of. energy that that incident light must have in order to excite the. electron in the first place. So very long wavelength light will never do that. Light along wavelength will not have enough energy to. excite an electron across this gap into the next band to allow. there to be a current, which could oppose the electric field. In one dimensional crystals, the only thing that can happen is, look if you have each band come from allowed energy state. If we include spin, and we have splitting, then it becomes a more subtle story. In 3D, this isn't such a big deal, because those splittings are tiny, and so the states can sort of overlap. But in 1D they can't. But spin in one dimension is little-- I'm lying about spin. But do you really want me to get in spin? In three dimensions, you guys did an interesting thing, when you studied, you didn't know this was about the structure of solids, but it really was. When you studied the rigid rotor, you found that you had energy eigenstates and they were degenerate with degeneracy 2 l plus 1. And what we found here is that these guys could cross. States from different multiplates, with different values of l, had energies that could cross as a function of the strength of the deformation of your system. So here we can have states crossing. There's no nodes here in three dimensions. we add in a lattice we get bands again. The structure's a little more intricate because it depends on the momentum. But these bands now can overlap. Everybody see that? Because there's nothing preventing states from different-- in different multiplates from having the same energy in three dimensions. So how much energy does it take to excite an electron and cause a current that opposes the induced electric field? Nothing. Any electric field that you send in will be opposed by an induced current. called a band insulator. Because there are other ways of being an insulators. So what determined the exact band structure in for a 1D periodic potential? Two properties. One was l, the periodicity. And that came in the q l and k l. And the second is the detailed shape of the potential. Now in three dimensions, the things that are going to determine the potential are not just the distance between atoms, but you have a three dimensional lattice. And so when you solve the problem for the energy eigenvalues is a function of now the three different components of the crystal momentum, you'll get a different set of equations. the atomic orbital structure of the individual atom, the crystal structure, and the resulting band structure. You will almost always find overlapping bands in three dimensions in sufficiently high energy. What we need is one of two things. We need either the band gap coincidentally is ridiculously small, or we need a free particle that has no band gaps at all. That's a good example of that? A free particle, these band gaps go to 0. And so that's a conductor. Just an electron. It conducts, right? OK. If you have a hot piece of copper, then the lattice is wiggling around. And every once in a while, an ion can hit one of the electrons and excite it, give it some momentum. The sea of electrons is constantly being buffeted by this thermal fluctuation. And as a result, you constantly have electrons being excited up, cruising around, falling back down. So you end up with some population of electrons. And they can ask-- and both when asked, although not quite in this language, how likely are you to get an electron up here? How likely is an electron to be excited up thermally? small number. And e to the minus of a small number is close to 1. So at high temperature, you're very likely to excite electrons up here. At low temperatures, it's basically an insulator. This is called a semiconductor. And there are notes on the Stellar. web page that discuss in a little more detail what I just went through and. show you how you build a transistor out of a semiconductors. And the important bit of physics is just this. and metallic without actually being shiny and metallic. And it's not a pigment, so it doesn't absorb light and decay over time. It's like the best thing you could ever do if you wanted to be a shiny, fluttery, flying thing. So there's an incredible amount of physics in this story of the band gaps. And consider this an introduction to the topic. OK. So that's it for band gaps, and I want to move on to the remainder, the last topic of our course. Which is entanglement and quantum computation. Einstein created a thought experiment which we're going to study in detail next week called the EPR experiment. He sent one particle off to a distant planet and the second particle to my sister in DC. And my sister measures this second particle and determines what state it's in and is immediately determined what state the first particle is in over in this distant planet Zorg, right? So that's deeply disconcerting. How can something here dramatically change the state, the configuration, the initial configuration, of a particle arbitrarily far away? Isn't that deeply concerning? who have taken courses in [INAUDIBLE]-- and I'm sure that's all of you because of the GIRs. So I'm very used to microphones, but not in this context. OK. Is this-- yeah, it's on. Can you hear me? All right. So there are lots of ways to slice the story of Einstein by the time he reaches the EPR experiment, which is Einstein, Podolsky, and Rosen for the three people who actually wrote the paper. Allan Bloom: Bell had a lovely way to describe this problem of entanglement. Bloom: He had a friend named Bartelstein who had two quirks. He used to say, if you could only see one leg and that sock was pink, you knew to a certainty that the other sock was not pink.Bloom: Same thing. If you have a coin and you cut it in half down the-- so you've got two coin shape disks. You know somebody is cheating by tossing in the half coin that only has a tail on it. Einstein, Podolsky, and Rosen argued that the EPR paradox suggested that quantum mechanics was incomplete. They said that if you can perform a measurement, you know that quantity absolutely. But you can't do the-- so on the one hand, quantum mechanics says you can’t know physical reality to this level of precision. And on the other hand, the fact that you can do that measurement violates the relativistic picture of reality. But the question is whether or not the framework of quantum mechanics is somehow unsatisfactory in any formal sense.

ROUGE-1: 24.46, ROUGE-2: 23.47, ROUGE-L: 23.38
BERTScore: 62.37

==============================================
==================== [37/100] ====================
Summary:
The secret to chopping fresh herbs is to chop them not bruised them. The tip is to put a banana in a paper bag then add your unripe fruit put it in a dark place and the banana will speed up the ripening process of the other fruit. Cut a mango the easy way homey you stalk end up cooking either side of the stone cut all the way into the flesh making squares without cutting through the skin then turn it inside out and carefully cut your pieces off. A great tip to prevent burning sensitive skin when working with chilies to get rid of that spice and that heat on your fingers. lemon squeeze a little bit of lemon juice and that instantly gets rid of the heat fresh lemon juice for perfect ball potatoes always start them off with cold water and never boarding water this way by the time the center's are the potatoes are cooked the outside won't be falling apart my tip to get the flesh out of a Kiwi this is simply cut the fruit in half and scoop out with a teaspoon try it it really works. A great tip to check if a pineapple is ripe is to pull a leaf out from the top if it comes away easily it's ripe and ready for slicing.

ROUGE-1: 44.19, ROUGE-2: 42.91, ROUGE-L: 43.99
BERTScore: 73.24

==============================================
==================== [38/100] ====================
Summary:
hey everyone it's sth register nurse rn.com and in this video we're going to be going over our weekly inlex practice question. Let's see what our question says a patient who has a health history of uncontrolled hypertension coronary artery disease and diabetes militis is prescribed to take propanolol. If you have provided the patient with education about this new medication which statement by the patient indicates your teaching was effective? We're looking at the correct statement so out of all these three are going toBe wrong and one will be correct. beta blocker you have to watch out for some certain things that can happen Okay so what do beta blockers do in a nutshell beta blockers block your sympathetic nervous nous system. Whenever you take this drug it acts on those neurotransmitters specifically norepinephrine and epinephrine so those will be blocked and in the heart. What will happen is that you will get a slower heart rate instead of that tacac cardia now think about diabetes malius whenever these patients have hypoglycemia their heart rate goes up. increase that blood sugar naturally with that process so you have this double-edged sword you have where they're not going to be able to notice their blood glucose. If a patient misses a dose of their beta blockers they don't need to double the dose they need to take it as soon as they remember unless that next dose is due. If the patient is already diabetic taking insulin probably that's what we assume and um so they have to monitor their blood sugar especially with these non-selective beta blockers so that is our answer now. look and see why D is wrong this patient says that they're going to stop their beta blocker immediately if they experience cold hands and feet well this is a normal side effect with these non-selective beta blockers. You would never just immediately stop taking a beta blocker they need to be tapered off of this because if they just all of a sudden quit taking that medication they can go have cardiac death or something worse can happen so that answer is wrong for that reason okay so that wraps up this inlex practice question.

ROUGE-1: 41.95, ROUGE-2: 40.13, ROUGE-L: 41.74
BERTScore: 69.80

==============================================
==================== [39/100] ====================
Summary:
This is part 2 of a guide to clinical reasoning or how to create an accurate differential diagnosis from a patient's presentation. The patient is a 75 year old woman presenting with epigastric pain for four hours. She had moderate nausea it has refused to attempt to eat or drink anything since the pains onset because she is concerned that it will concert a vomit which she has not yet done. She denies changes in her bowel habits shortness of breath chest pain changes her skin or eye color. She has had no surgeries her medications her medications. The patient is an elderly woman a piercer stated aged in moderate discomfort secondary to abdominal pain her temperature is 99.8 heart rate 110 blood pressure 132 over 80 respiratory rate 26 and oxygen saturation of 98% on room air h ee and t exam reveals normal oral pharynx but poor dentition her neck is supple without breweries and without lymph adenopathy her chest is clear to percussion and auscultation bilaterally her cardiac exam has a regular tachycardia hypertension her jbp is undetectable at either 45 degrees or when she's completely sipping on abdominal exam she has non distended and there are no scars. The problem representation is a one to two-sentence summary using precise medical terminology of the most highly relevant aspects of the patient's history exam and diagnostic tests. The patient is a 75 year old woman with abdominal pain for four hours it is epigastric well localized progressed over 45 minutes is constant with no exacerbating or alleviating factors and associated with nausea. She has hypertension diabetes moderate alcohol use smoking and is drinking water from a newly drilled well on exam. She is MA in moderate distress she has a borderline temperature tachycardia and tachypnea. primary symptom using semantic qualifiers and ending with the highly relevant diagnostic data using clinical syndromes when possible. For this patient we would start with mrs. Smith is a 75 year old woman with multiple cardiovascular risk factors and alcohol use presenting with acute constant epigastric pain. The next time you are in rounds or in a teaching conference and the senior physician asks you to summarize the case if a statement similar to this one effortlessly comes out of your mouth I guarantee you everyone in the room will be impressed. bowel that can cause acute epigastric pain include relatively common and minor things such as food poisoning and gastroenteritis and the uncommon but potentially lethal problem of bowel infarction. The major structures are of course the liver and gallbladder along with other components of the extra hepatic biliary system. Other organs in the chest to consider include the esophagus from which pain classically radiates to the epigstric area as in GERD the heart can also referred pain to theEpigastrium region as seen in acute coronary syndrome. pain from a pulmonary embolism can be referred to the abdomen although it more typically is to the right or left upper quadrants and not the epigastric now that we have a framework we move on to the final and hardest step applying the key features to that framework so here is our framework once again and here are our key features how does one start this process the brute force method would simply be to take each diagnosis listed one at a time and review each individual key feature to decide if it impacted the probability of the diagnosis and what that impact was. severe nausea at that so the presence of knowledge here definitely argues in favor of the diagnosis arguing just modestly against pancreatitis is the fact that the pain had no exacerbating or alleviating factors. The pain from pancreatitis classically improves with setting up or leaning forward which this patient does not describe. The sensitivity and specificity of this symptom feature to the best of my knowledge has never been studied therefore we don't know exactly how much to weigh this in our assessment of the probability of pancreatitis there. I would place relatively little weight on the lack of positional component to the symptom. The patient does have numerous risk factors and the combination of epigastric pain and nausea is not an uncommon way for acs to present particularly in a either a woman or a diabetic. The unremarkable lfts in and of themselves rule out acute hepatitis a paddock abscess usually has symptoms localized to the right upper quadrant and is also associated with lft abnormalities. A normal light bass cholecystitis also typically causes right upper Quadrant symptoms and signs along with elevated alkaline phosphatase although it's not enough to make the diagnosis likely enough to seriously consider. The patient has no major risk factors for RPE and has no shortness of breath well it's certainly possible to present with a PE with just pain this case is just not what a PE looks like. The combination of acute abdominal pain nausea and a possible contamination of well water is all consistent with heavy-metal poisoning specifically arsenic and lead. The history of the newly drilled well depending on where you are in your training is the most interesting it's the most important point if an RPE. element does not fit into the framework yet still seems to be a key feature the framework must be incomplete in this case I would add another category of diagnosis to our four existing categories of epigastric right upper quadrant left upper quadant and chest that fifth category is acute abdominal pain secondary to systemic toxic metabolic problems. Four major members of this group are heavy-metal poisoning a rare genetic disorder called acute intermittent porphyria another virgin etic disorder called familial Mediterranean fever and finally angioedema for any of these to be the final diagnosis this patient would need to have an atypical presentation of a rare disease. involves the provisional diagnosis and as you move further down the list ideas start to diverge a little bit I would certainly expect most experienced doctors to identify pancreatitis as the leading diagnosis in this case so that concludes part 2 of 3 of this video series on the clinical reasoning. I hope you found it interesting and useful while the approach I presented here is not the only one in use I guarantee medical trainees that if you consciously employ it while on the wards you impress your peers and evaluators and more importantly create a more accurate differential.

ROUGE-1: 38.39, ROUGE-2: 37.30, ROUGE-L: 36.12
BERTScore: 62.24

==============================================
==================== [40/100] ====================
Summary:
Danqi Chen is one of the foremost researchers in question answering. She is the professor at the Princeton University. Danqi once upon a time was the head TA of CS224N. She's quite familiar with the context of this class. So today I'm very happy to introduce to you some of the fundamentals in this field, as well as on cutting edge and state of the art topics. So here's my plan for this lecture. I would give a brief introduction of what question answering is. And then today, delighted to have our first invited speaker. is question answering, and what kind of problems that people are studying today. Then I'm going to spend the most of this lecture focused on one type of question answering problems called reading comprehension. And then at the end of the lecture, I'm hoping to spend hopefully like 10-ish minutes to talk about a more practical, and in my opinion, more exciting problem called open domain question answering. And so my point is to try to quickly go over some of those state of the art measures in this area. OK so let's just get started. Our open- domain questions or closed-domain questions are simple questions versus more complex or compositional questions. And for the answer type, it can also be a short segment in the text, or a paragraph, and a list or even the yes or no questions. All these problems may require very different techniques or different data, or even different evaluation metrics to evaluate all these different problems. And, the question and answer has enabled a lot of really useful real world applications. For example, today if you just put your question in a search engine like Google. Question answering is probably one of those fields that we have seen the most remarkable progress in the last couple of years driven by deep learning. Almost all of the states are question answering systems today built on top of the end to end training of the deep neural networks and the pre-trained language models such as BERT. In this lecture, I will be mostly focusing on the text based, or textual question answering problems. So basically, we are trying to answer questions based on text.timer. the unstructured text. There are many other really big question answering problems. And each of them can be really like a subfield in NLP and they actually have very different challenges and also model designs. So next, I'm going to start with a part 2, reading comprehension. I just want to quickly check if there are any quick questions I can answer before I start us on part 2. OK. So let's talk about the reading comprehension then. So basically we wanted you to question build answering systems to answer questions that can answer questions over a very large database. Reading comprehension has been viewed as a very important test bed for evaluating how well computer systems understand human language. This is really just similar to how we humans actually test the reading comprehension test to evaluate how well we actually understand one language. So this is also the way that we actually post questions to test the machine's language understanding ability. It actually has been formally stated back in 1977 by Wendy Lehnert in her dissertation. She says that, since questions can be devised to query any aspect of text comprehension, the ability to answer questions is the strongest possible demonstration of understanding. Personal subject, Barack Obama. So we want to fill in what is-- fill in this question mark and figure out, OK, where Barack Obama was educated at. So one way to solve this problem is basically trying to convert this relation into a question. So where did Barack Obama graduate from? And taking all of that relevant piece of text and then by applying a reading comprehension problem. Then basically, we can find out-- the correct answer should be Columbia University. That is also the output of this information extraction system. And another example is actually called a semantic role labeling. sets have been also collected, basically runs this size around 100K. So for these datasets-- so the passages is like a single paragraph selected from the English Wikipedia, which usually consists of 100 to 150 words. And the questions are crowd-sourced, basically like from Mechanical Turking. And this is a very important property of the dataset, is that each answer is a short segment of text, or we called it span in the passage. So we can see that there is an exact match score between the predicted answer and any of the gold answers. answer to that. So next, I'm going to talk about how to build neural models for reading comprehension, and in particular, how we can build a model to solve the Stanford Question Answering Dataset. So the input of this problem is let's take a context or paragraph. And also we take our question, Q. And the question consists of n tokens q1 to qN. And because the answer has these constraints, the answer must be a section of text in the passage. We are going to predict a start and then end. called context-to-query attention. The idea is for each context word, can we find the most relevant words from the question for the query words. And then the second type of attention is called the query to context attention. So this is actually just an intuition of these two types of attention. And this is also why this model is called a bidirectional attention flow because there's a context- to- query attention and there is also a query-to. context attention in this model. means this context word is not very relevant. So basically, that's why we take the softmax-- take softmax over on top of the max, yeah. OK. Do you want even more or do you want to go on? I probably should go on. We have a lot of slides, but I'm happy to answer questions after those ones. Yeah. So the last part of this model is actually the easiest, the most simplest one. Lastly, these are two layers, modeling layer and output layers. the dot product between w end and this vector, and this can produce all the probability over all the conditions which predict how likely this position will be the end the position of the answer. So by passing the mi to another bedirectional LSTM, their reasoning is that they're trying to capture some kind of dependence between the choice of the start and end. OK, so this model is actually achieved-- like on SQuAD data set, it achieved a 77.3 F1 score. They found that both attentions in two directions are actually important. the models are actually a very similar ballpark. So numbers range from the highest number here, 79.8 until after the ELMo was introduced, the numbers have actually improved quite a bit. And now here is our attention visualization to show that how this smorgasbord of attention actually can capture the similarity between the question words and the context words. So basically this theory tells us that these kind of attention scores can actually capture those negative scores pretty well, yeah. OK, so next, I'm going to talk about BERT, how to use the BERT model to solve this problem. BERT models are pre-trained while BiDAF models only builtd on top of the GloVe vectors. Pre-training basically can just change everything and it gives a very large boost in performance. Even if you use a stronger pre-training models or modern, like a-- stronger models than the BERT models, they can even lead to better performance on SQuAD. And then finally, if you see even the latest pre- trained language models, including the XLNet or RoBERTa or Albert, these models can give you another like 3, 4 F1 score. pre-training has been so important. Next I will quickly talk about-- OK, a question here is that can we actually even design better pre-training objectives for reading comprehension or question answering? And the answer is actually yes. So this is actually a work I did with Mandar Joshi and other folks one year ago called SpanBERT. So think about this. So for SQuAD and other a lot of extractable reading comprehension datasets, the goal is trying to predict the answer span from the passage. SpanBERT greatly outperformed Google BERT and other BERT basically across all of the datasets. This number has already exceeded even the human performance on SQuAD. So that really tells us that OK, even if we are not going to increase the model size or increase the data, by designing better pre-training objectives can also go a long way and do a much better job at least in the question answering and the reading comprehension datasets. OK. So so far, we've demonstrated that by using BiDAF model and by using BERT models, we can get a very good performance on the SQuad dataset. is another paper that actually just came out in 2020. So there has to be a lot of evidence showing the similar things. So today we compute a very good reading comprehension data set on the individual data sets. But these systems trained on one dataset basically cannot really generalize to other datasets. And all the other numbers in this table basically shows that if you train one system on one datasets and then evaluate on another dataset, the performance will drop quite a lot. So it basically really cannot generalize from one dataset to another dataset. a large collection of documents. So one example is just taking the whole Wikipedia, which has five million articles. And we're going to return the answer for any open-domain questions. So this problem, there isn't any single passage, so we have to answer questions against a very largeCollection of documents or even the whole web documents. This is actually a much more challenging and also more practical problem. So if you look at the example of Google example I showed at the beginning, so these techniques will be very useful in the practical applications. Danqi can stay for a bit to answer questions, but not forever. Because she doesn't have a Stanford login, we're going to do questions inside Zoom. So if you'd like to ask a question, if you use the raise hand button, we can promote you so that you appear in the regular Zoom window and can just ask questions and see each other. If you hang around and don't leave the Zoom for more than a few minutes, maybe we'll just promote everybody who's still there. unsupervised question answering so by using some kind of approach like some form of unsupervised machine translation, this kind of idea. That can be borrowed-- can borrow the idea from that and can also work pretty well, reasonably well in unsuper supervised question answering. So my question is I guess it's kind of interesting that there's not really that strong of a transfer effect between data sets that are kind of ostensibly similar. So have there been any research done on how close I guess the formatting and the semantic content of these question answering datasets actually adheres to the data that BERT is pre trained on? And if so, has there been sort of any effect found between those similarities or differences? The goal of this project is trying to ingest all the phrases in the Wikipedia. So these conversations are built using the training set of the question answering datasets. So if we use say a different data set that does not present the information using the structure presented in Wikipedias, this model may not work as well as.kind of the intuition behind the dense phrases apart from the answers will probably be in close proximity. And what if the datasets has answers to a specific question like very far from the actual information? Say, the answers to your question may not resided in close. proximity to the words in the question. the answer? So why do you think the nearest neighbor-- I mean, you always can find something, right? The question is that whether it's close enough or not. So the question is what if in the datasets that the answer is not close enough? Yeah, that's a good question. If you really come up with something that is really very far away from all the questions that we have been seeing in the training set, that could be possible. Basically it depend on how the text are formatted. thank you for bringing this one up. OK. Next is-- Hi, thanks for taking the time. In what extent can in context learning help models to be more robust with respect to different domains? Can you tell me what you mean by in context? So like basically you provide the template generated by BERT. And then instead of directly predicting the classes of text classifications, you just use some word to represent that class or predict the wordings there. OK, actually, I have been through this something related to that recently. Do you think that the current sort of benchmark data sets are maybe a little bit too easy for- [INTERPOSING VOICES] Or just like that.solve the easy problems. So all these things need to be resolved. One final thought is that having a lot of transversely trying to have a lot. of humans in the loop of the frameworks to evaluate these kind of systems. Just try to break the current system, come up with some harder questions. Are you still game for a couple more questions? Next question is from Danqi, who was one of the co-organizers of the EfficientQA task. Danqi: How concerned should we be about potential encoding of biases into these record labels or how we evaluate them, or is that just more of a concern for more open ended questions? Yeah, this is definitely very important. I mean, we will have more discussion of toxicity and bias coming up very soon, including actually Thursday's lecture as well as a later lecture, not specifically about QA though. people also leveraged similar things for question answering. So I was just wondering to what extent these kind of techniques can work on one group of tasks, not just limited to question answering, but mainly question Answer. Yeah, I don't know. At least for the work that I have seen so far, it all applied or operated at a very simple sentence classification task. OK then we've got-- and maybe we should call this the last question. Hi, what is the intrinsic difference between solving question answering with generative models like T5 versus encoders like BERT? The key difference between the generative model and the extractive model is that for generative models, you can actually leverage more input passage together and do the generation. So for this model, there isn't any retrieval. So you can always find the answer from the question, right? So this model really has you relying on all the parameters you memorized, all the encodings. And then Generative models are you're remembering the whole question and you try to retrieve the memory when you answer the question. The model is very large, like 11 billion parameters. So the parameters are basically trying to memorize a lot of information that has been.information. So by just taking this input, it has to just rely on the parameters to infer this answer. So it's actually very hard to-- yeah, it's a definite balance between memory and the generalization from it. All right, thanks. Do you want to call it a night or do you want one more question? Either way, yeah. collect so many examples for other languages. And there has been also a lot of work trying to do cross-lingual question answering and stuff like that. If we have, actually, I think that the techniques can be generally applied to other languages, he says. "I think that we have a lot to learn from each other," he adds. "We have a long way to go, but I think we're making progress." "We've got a lot more to learn," he says, "and we're getting better at it."

ROUGE-1: 28.18, ROUGE-2: 27.07, ROUGE-L: 26.09
BERTScore: 64.06

==============================================
==================== [41/100] ====================
Summary:
 homework two is out now. This weekend sessions will be having some more background on deep learning. We're also gonna be reaching, uh, releasing by the end of tomorrow, what the default projects will be for this class. Um, and those proposals will be due, um, very soon, er, in a little over a week. The assignments, [inaudible] are they limited to TensorFlow? asked the question if, if the assignment is limited toTensorFlow. I'm pretty sure that everything relies that you're using Tensor Flow. Last week, we were discussing value function approximation. Today we're gonna start to talk about other forms of value function approximations using deep neural networks. We're mostly not gonna talk so much about enormous action spaces, but we are gonna think a lot about really large state spaces. And so, this could be things like a laser range finder for our robot, which told us how far away the walls were in all 180 degree directions. And the key thing was that we don't know what the true value of a policy is. So, the two ways we talked about last time was inspired by the same. The number of data points you need tends to scale with the dimension. So, one alternative is to use sort of a really, really rich function approximator class. These actually have a lot stronger convergence results compared to linear value function approxIMators. Um, and everyone just [inaudible] name first please to stop me. Yeah. [LAUGHTER] So, can you repeat again why the exponential behavior happening? Yeah. A lot of these sort of kernel based approximators or non-parametric. Deep neural networks are very flexible representations. They can be guaranteed to be averagers which is a linear value function approximator. These approximators are guaranteed to converge compared to a lot of other ones. But they're not gonna scale very well and in practice you don't tend to see them, though there's some really cool work by my colleague, Finale Doshi-Velez, over at Harvard who's thinking about using these for things like health care applications and how do you sort of generalized from related patients. Going to push them into some function and then output something which is probably gonna be also a vector. Then you're gonna push that into another function, and throw in some more weights. I'm gonna do that a whole bunch of times, and then at the very end of that you can output some y which you could think of as being like our Q. Then, we can output that to some loss function j. These are- happen a lot in unsupervised learning like predicting whether or not something is a cat or not. your weights. When I first learned about deep neural networks, you have to do this by hand. But I think one of the major major innovations that's happened over there, you know, roughly what? Like last 5 to 8 years is that there's auto differentiation. So, that now, um, you don't have to derive all of these, uh, gradients by hand instead, you can just write down your network parameters and then you have software like TensorFlow to do all of the differentiation for you. and as usual we need a loss function at the end. Typically, we use mean squared error. You could also use log likelihood but we need something that- that we can differentiate how close we are achieving that target in order to update our weights. [NOISE] Yeah? Name first. So, this ReLU function is not differentiable, right? It's ended up being a lot more popular than sigmoid recently, though I feel like it [OVERLAPPING]. It's not differentable at one point? Yes. But I don't see how gradient [inaudible] is gonna work on the part where it's flat. The universal function approximator property is stating that that will not occur for, um, uh, deep neural network if it is sufficiently rich. Another benefit is that potentially you can use exponentially less nodes or parameters compared to using a shallow net which means not as many of those compositions to represent the same function. Then the final thing is that you can learn the parameters using stochastic gradient descent. All right. We're now gonna talk a little bit about convolutional neural networks. They're used very extensively in computer vision. In 1994, we had TD backgammon which used Deep Neural Networks. And then we had the results that were kind of happening around like 1995 to maybe like 1998, which said that, "Function approximation plus offline off policy control, plus bootstrapping can be bad, can fail to converge" So I think it sort of really changed the story of how people are perceiving using, um, this sort of complicated function approximation, meters, and RL. And so, perhaps it was natural that, like, around in like 2014, DeepMind and DeepMind combined them and had some really amazing successes with Atari. where they changed things to be more on policy or, or which we know can be much more stable. Um, ah, they are doing deep learning in, in this case, Deep-Q Learning. And so it can still be very unstable, but they're gonna do something about how they do with the frequency of updates to the networks. We'll see how it works here. Anyone else? Okay, cool. So, we'll- we'll see an example for breakout shortly, um, of what they did. Dennis and David, both had sort of a joint startup on, um, video games, a long time ago. So, their idea was, we'd really like to be able to use this type of function approximator to do Atari. They picked Atari in part.as well as in our derivative. And whether we're gonna see is, is uh, an alternative to that. Okay. Can anybody think of an example where maybe an Atari game, I don't know how many people played Atari. In this case, there's 80 joystick button positions, um, may or may not need to use all of them in particular game. And the reward can be the change in score. Now notice that that can be very helpful or may be it, depends on the game. So in some games it takes you a really, really long time to get to anywhere where your score can possibly change. In other cases, you're gonna win a reward a lot. And so that's gonna be much easier to learn what to do. approximators act. So they're, they're representing the Q function. They're going to minimize the mean squared lost by stochastic gradient descent. Uh, but we know that this can diverge with value function approximators. And what are the two of the problems for this? Well one is that, uh, there is this or the correlation between samples which means that if you have s, a, r, s prime, a prime, r prime, double prime. standard approach, just uses a data point. In the simplest way of TD Learning or Q-learning, you use that once and you throw it away. So, even though we're treating the target as a scalar, the weights will get updated the next round which means our target value changes. And so, you can sort of propagate this information and essentially the main idea, is just that we're gonna use data more than once, um, and that's often very helpful. And we'll look at that more in a minute. use in that value of S prime for several rounds. So, instead of always update- taking whatever the most recent one is, we're just gonna fix it for awhile. Because this, in general, is an approximation of the oracle of V star. What this is saying is, don't do that, keep the weights fixed that used to compute VS prime for a little while. And that just makes the target, the sort of the thing that you're trying to minimize your loss with respect to, more stable. In terms of stability, it helps because you're basically reducing the noise in your target. If you think of this as a supervised learning problem, we have an input x and output y. The challenge in RL is that our y is changing, if you make it that you're- so your y is not changing, it's much easier to fit. This is also sort of reducing how quickly you propagate information. So, you might misestimate the value of a state because you haven't updated it with new information. Do you every update the- Minus at all, or is that [inaudible] Great question. This is really just about stability and that's- that's true for the experience replay too. Experience replay is just kinda propagate information more- more effectively and, um, this is just gonna make it more stable. Uh, so these aren't sort of unique using deep neural network. I think they were just more worried about the stability with these really complicated function approximators. Of what the agent is doing. So, remember the agent's just learning from pixels here how to do this. Um, and the beginning of its learning sort of this policy. You can see it's not making- doing the right thing very much, um, and that over time as it gets more episodes it starting to learn to make better decisions. So this is really cool that sort of it could discover things that maybe are strategies that people take a little while to learn when they're first learning the game as well. In general, replay is hugely important and it just gives us a much better way to use the data. So, we've done some work, um, using a Bayesian last layer, using like Bayesian linear regression which is useful for uncertainty. Other people have just done linear regression where the idea is you- you sort of, um,. uh, deep neural network up to a certain point and then you do, uh, kind of direct linear regression to fit exactly what the weights are at the final layer. That can be much more efficient, but you still have a complicated representation. Doubled DQN is kind of like double Q learning, which we covered very briefly at the end of a couple of classes ago. The idea was that we are going to maintain two different Q networks. This is to try to separate how we pick our action versus our estimate of the value of that action to deal with this sort of maximization bias issue. The second thing is prioritized replay. So, in Mars Rover we had this really small domain, we are talking about tabular setting through just seven states. And we're talking about a policy that just always took action a1 which turned out to mostly go left. Vote if you think it matters which ones you pick, in terms of the value function you get out. Back-propagate from the information you're already [NOISE] have on step one to step two. So, if you pick backup three, so what's backup three? It is, S2, A1, 0, S1. So if you do the backup, that's, zero, plus gamma V of S prime. And this is one. So that means now you're gonna get to backup and so now your V of. S2 is gonna be equal to one. The order in which you did, do it. If you had done S3, a1, 0, S2, your S3 wouldn't have changed. Order can make a big difference. The number, of, um, updates you need to do until your value function converges to the right thing can be exponentially smaller, if you update carefully. It's very computationally, expensive or impossible in some cases to figure out exactly what that uh, that oracle ordering should be. But it does illustrate that we might wanna be careful about the order that we do it and- so, their, intuition, for this, was, let's try to prioritize a tuple for replay according to its DQN error. It could be super tempting to try to start, by like implementing Q learning directly on the ATARI. Highly encourage you to first go through, sort of the order of the assignment and like, do the linear case, make sure your Q learning is totally working. Even with the smaller games, like Pong which we're working on, um, it is enormously time consuming. It's way better to make sure that you know your Q Learning method is working, before you wait, 12 hours to see whether or not, oh it didn't learn anything on Pogge.

ROUGE-1: 23.55, ROUGE-2: 22.69, ROUGE-L: 22.86
BERTScore: 65.16

==============================================
==================== [42/100] ====================
Summary:
 salt makes up a tiny part of any bread though which has a huge effect on it and most bread is made with salt nowadays. salt has a tightening effect on the gluten it strengthens the dough and makes it more cohesive as yeast consumes the sugars in the dough. salt helps with controlling fermentation it draws moisture through the cell walls of yeast in a process called osmosis. salt can help with preserving the color and flavor of flour unbleached flour has carotenoid pigments which give the crumb of our bread the creamy color and a wheaty aroma. Ahsoka would be made with hot or boiling water leaving it to soak at room temperature and keeping it warm for a long time would run the risk of it going off and spoiling salt inhibits enzymatic activity it can prevent the soaker developing off flavors so adding just two percent of salt to your soaker will ensure that it doesn't get any funky flavors. You should account for the amount of seeds and grains and for the flour in the recipe when you're calculating the total amount of salt. If you don't add enough salt you'll end up with a bland tasting loaf. it's rising more slowly whilst the one on the left is already collapsing. the one in the right is still pushing on. what did you think of this experiment did you learn something new let me know down in the comments see more videos like this one click right here that's all i have for you today thank you for watching i'll see you in the next one. i'll be back with a new video in a week or so. I'll let you know what it's about.

ROUGE-1: 24.42, ROUGE-2: 22.63, ROUGE-L: 22.61
BERTScore: 59.85

==============================================
==================== [43/100] ====================
Summary:
In this section, we're going to talk about the relativistic Doppler effect. We make good use of our space-time diagrams, which we discussed earlier. The question is how is this being observed by an observer which is moving with a relative velocity v with respect to the source? So the question is not how this observed-- how this is seen by the source but how it is being viewed by the observer. We have to apply Lorentz transformation to the data. that the period now is given by 1 plus beta over 1 minus beta square root of that times tau. And the frequency is the inverse. So we just calculated relativistically how the period and the frequency of a wave is Lorentz transformed. We'll have 1 minus Beta over 1 plus Beta squareroot of that [? times ?] the frequency. That's how we get the period of the wave and the rate of its movement. The period is the same as the frequency, but the rate is the opposite.

ROUGE-1: 49.45, ROUGE-2: 42.07, ROUGE-L: 40.06
BERTScore: 71.88

==============================================
==================== [44/100] ====================
Summary:
Hollywood Legend Will Smith owns a team in the new E1 racing series that aims to prove the potential of electric power in the Marine industry. The nine teams have the same boat but they're working out how to push the tech and try to get ahead of the competition. The boats can reach 50 knots that's around 93 km per hour so how do they reach those speeds? The key bit here is getting up on the thin bits of the foil and staying above the water to have the speed that's right. hope it continues for many years to come. We are proud to be a part of the history of the city. We hope to see it continue for many more years. Thank you for all your support and good wishes. We will continue to support the city in any way we can over the next few years. Back to Mail Online home.Back to the page you came from. Back To the pageyou came from, click here for more information on this story and to read the rest of the article.

ROUGE-1: 17.51, ROUGE-2: 11.90, ROUGE-L: 12.41
BERTScore: 57.97

==============================================
==================== [45/100] ====================
Summary:
Marginal rate of substitution of good 1 with respect to good 2 is infinity. When we do not say when we say just MRS, we are not using any particular term, you can use both way. We will always use this, yes here is 0 and MRS here is infinity, but in both the examples MRS of cola is Infinity. So, just be careful what we are talking about. This is you know a point of confusion many people just change the axis when they talk about it. getting x you get 1 by x, and why? Let us see what do we mean by marginal rate of substitution mathematically. We want this person to have the same level of utility by consuming one of these two bundles. So, of course, when we are increasing the amount of 1 good to bring what will happen using this, if we use the monotonicity what will happened. More of 1Good and same of the other good what is happening. If one is bad, remember the definition right from the beginning if one is. bad then marginal rates of substitution would be positive, because to increase the amount. of bad you will have to compensate that person by giving more of the. other good. see, that MRS is given as a positive number, that only means that the author has introduced a negative sign here to convert the MRS into apositive number. So, it does not matter. Is it clear? Do you know the answer to this question? If so, please email us at jennifer.smith@mailonline.co.uk. If you don't, we would like to hear from you. Please send us a photo of your MRS and we will send it to you.

ROUGE-1: 35.59, ROUGE-2: 30.94, ROUGE-L: 31.94
BERTScore: 68.46

==============================================
==================== [46/100] ====================
Summary:
in this video we're going to discuss what externalities are in economics so an externality is when you do something that affects the well-being or the good of another person or a company but you're neither harmed or rewarded for what you did to that person. The externalities can be positive they can be negative a negative externability is whenyou've harmed someone you've done something to somehow impose a cost on someone or some some company or something and you haven't reimbursed that person you haven’t paid them any money or or done anything to compensate forwhat you did. at 3: a.m. and you said okay you know what I'll deal with this but I need you to give me an extra 50 bucks a month toward my rent and then you work out an agreement that's different but we're assuming here they haven't done anything to reimburse you they're not paying you. Now a positive externality is where you are doing something that doesn't harm someone it actually benefits that other person you're doing something good that just as a side test like it's as a tangent it's actually helping some other person or people and so but those people aren't turning around and compensating you for it right. your lawn and you mow your lawn and stuff but you don't really spend a l a lot of time making your house look pretty now if your neighbor is trying to sell their house they have a for sale sign up they might appreciate if you went out and really did a great job maintaining your home they would really just love that because then when people come to see their house which is for sale that would increase the value of their home right if the neighboring properties like yours look really really nice then that would help them sell their home.

ROUGE-1: 46.00, ROUGE-2: 45.09, ROUGE-L: 46.00
BERTScore: 70.97

==============================================
==================== [47/100] ====================
Summary:
Political philosophy is the oldest of the social sciences. It can boast a wealth of heavy hitters from Plato and Aristotle to Machiavelli, Hobbes, Hegel, Tocqueville, Nietzsche. The study of the great books or great thinkers of the past can easily degenerate into a kind of antiquarianism, into a sort of pedantry, says Professor Steven Smith. But these works provide us with the most basic questions that continue to guide our field, he says. "The ideas of John Maynard Keynes, both economists and political philosophers, when they are right, are more powerful than is commonly understood," says Smith. The term "regime" is a familiar one, but what is a regime? How many kinds are there? How are they defined? What holds them together, and what causes them to fall apart? Is there a single best regime? Those are the questions I want us to consider. The concept of the regime is perhaps the oldest and most fundamental of political ideas. The title of the book that you will be reading part of for this semester, Plato's Republic, is actually a translation of the Greek word politea that means constitution. The political world does not present itself as an infinite variety of different shapes. It is structured and ordered into a few basic regime types. Regimes are necessarily partisan, that is to say they instill certain loyalties and passions. The study of regime politics is in part a study of the distinctive national character types that constitutes a citizen body. You can't understand a regime unless you understand, so to speak, what a people stand for, what they look up to as well as its structure of institutions. David Frum: Political philosophy is always guided by the question of the best regime. He says it's difficult to make a judgment on what is the best. The best regime is superior in some ways to all actual regimes, he says, but it has no concrete existence anywhere. Frum says philosophy can never be truly loyal to anyone but what it thinks is best, or what is best for its own people. The good citizen is relative to the regime, you might say regime-specific, Frum writes. unnecessary or redundant. It would wither away. Political philosophy exists and only exists in that... call it "zone of indeterminacy" between the "is" and the "ought," between the actual and the ideal. This is why political philosophy is always and necessarily a potentially disturbing undertaking. Those who embark on the quest for knowledge of the best regime may not return the same people that they were before. But there is some compensation for this, I think. The study of political philosophy may be the highest tribute we pay to love.

ROUGE-1: 19.19, ROUGE-2: 17.25, ROUGE-L: 17.84
BERTScore: 61.75

==============================================
==================== [48/100] ====================
Summary:
Ahern: I have never had an exam where I had fewer questions. There were maybe 10 questions I got on the exam and that was for a class of this side, really unusual. I find most students are honest. I've only had a handful of situations where in this class, where I've had dishonesty as an issue. The only students I've ever had with serious dishonesty issues actually were in smaller classes interestingly enough. But it has happened here and I do have taped evidence when it does happen. Ahern: How many zwitterions, how many amino acids can form zwittersion? None. They all can. That's not trick. You guys should know that, right? Is the answer "None" a trick? I'm sorry, I don't think so, folks. There are no amino acids, everything, so think about it. Ahern: What did you have in mind for the last question? The one about the Kcat. How can you have two apparent Kcats? It depends on how you calculate the concentration of the enzyme. where time is sometimes a factor, and so I was going to have 3 of the longer answer questions and I decided not to do that and I made them shorter. Next term, the format of the exam changes in 451. Most people like 451 better because the maximum number of points on a question I think is 3. If you're going to spend a fair amount of time on something, it should be worth more points. Anybody hate the exam? You can say it. As I said, I don't...okay, there you go. same manner that the enzyme is binding a substrate. And so the parallels of those with respect to concentration and so forth, that's really the reason you see those two curves being essentially the same. So the first thing I say is "Okay folks, let's get started." I don't live consciously, I just blurt it out. [class laughing] Alright, well let's turn our attention. Thank you for your feedback. That's not a lot of feedback, but I do appreciate feedback and I'm always happy to listen to what you have to say about exam formats. a few minutes talking about today. Using genetic techniques today, it's very easy to alter the genetic code for any of these proteases and change which amino acid is presence at any given place. Researchers have changed, for example, a serine residue of 221, which is the serine, gives it its name, to an alanine. Or changing the histidine position 64 to anAlanine or changing all 3 to analine. And when they do that, and they compare the activity, so this is the log of Kcat. So Kcat of course is a good measure of velocity. Neil Ahern: Are there studies done on where you can replace the serine, histidine, or aspartic acid with something other than alanine and perhaps increase the function? Student: [inaudible] Would this one respond to DIPF? Neil: It's possible it will be better but my suspicion is it will probably be somewhere in here. Mutation and selection. We think of mutation as mostly being detrimental because most mutations give rise to things that aren't functional, kind of kind of. Some mutations actually give rise to more functional enzyme. That's how enzymes evolved evolutionarily in the first place. One of these classes of proteases is known as cysteine proteases. Ahern: The nitrogen on histidine withdraws the electron from the sulfhydryl group on cysteina, which is then nucleophilic and attacks the substrate. It's actually removing the proton. The histidine is removing theProton from the sulfur. That makes a sulfur left behind with extra electrons. Those extra electrons are nucleophobic, they attack the peptide bond. "I hate telling joke about sex or the other, but... Carbonic anhydrase" is a pretty amazing enzyme. It can catalyze the conversion of a million molecules of substrate into product per second, per enzyme. Most of our body tissues are at a pH of 7 to 7.4. We look at Blood, for example, is about 7.3 to 7,4. "I want every woman to love me" and poof, he turns into a box of chocolates. Ahern: The rate of formation of the nucleophile is the critical step in the catalytic action of this enzyme. At pH 9, the enzyme is still holding its shape enough that it's actually able to continue catalysis. When we get above that, we're going to see the ending drop off precipitously. Ahern: There is a lot of interest in manipulating enzymes to increase their activity and increase their efficiency. In the case of carbonic anhydrase, what's actually limiting them is actually defusing them. A restriction enzyme is a protein that grabs a hold of DNA. Most of the time it's going down that trip down the DNA molecule, it does not encounter the sequence GATATC. In fact that sequence will occur randomly, only about once every 4,000 residues. What happened physically inside those enzymes? Shape change, right? We saw a shape change that happened in those. Those very tiny shape changes caused the enzyme to have all of its properties. In the case of the serine proteases, that shape change resulted in the creation of the enzyme. alkoxide ion. In the case of the restriction endonucleases, that shape change is more dramatic. What it does is it actually causes a bend to occur in the DNA. So we think of the DNA molecules of being straight and linear, but when the enzyme is bound to that proper site, the enzyme goes "oh, whoa!" and it bends. The DNA molecule is physically bent at that point. It's physically bent. Now that bending turns out to be critical for the catalytic action.

ROUGE-1: 22.34, ROUGE-2: 21.10, ROUGE-L: 21.35
BERTScore: 62.91

==============================================
==================== [49/100] ====================
Summary:
AA and I are going to show you how to make vanilla extract so easy so yummy so good for all the things that you would use it for cakes whatever really really good isn't it so let's go [Music] simple now all you want for this is about 1 oz or roughly 30 G of vanilla beans so they'll be you know long Dobby whacker things and just cut them up into I don't know a couple of cenm you may maybe maybe qu an inch half an inch or so Chuck them in now you don't have to worry about scraping out the seeds. like this no no you don't want to try it oh it smells so good do I smell this no you sure no it's really good can I smell it no oh let's give this a little little taste hold on oh yummmy do you want totry no no I'll see you next time for my next [Music] meal. Like this noNo, you won't even try it. No, you're not going to even smell it. You're not even going to taste it.

ROUGE-1: 54.78, ROUGE-2: 50.58, ROUGE-L: 49.86
BERTScore: 74.59

==============================================
==================== [50/100] ====================
Summary:
RAFAEL JARAMILLO: Let's talk about intermediate phases and line compounds. He recalls intermediate phase in a three-phase system. He draws a free-energy composition diagram in such a case. Jaramillo: All possible common tangents are going to converge at the same point. He says this type of structure is a Laves compound, which is also known as Laves Laves, or Laves' compound. It's a broad structure, he says, and it can be used to develop new materials. you drop down to low temperature, both of these intermediate phases appear as line compounds. And one of the hallmarks of a line compound and phase diagrams with line compounds is that you have very different structure types. So you can kind of see how these are not-- you don't reach this hexagonal magnesium 2 nickel phase just by individually substituting our atoms from the HTP magnesium phase. And they're really fundamentally different structure. But this is the point I want to make here is that these are very distinct structures, and they're only occurring at very distinct compositions. of compound from the elements in their reference states. So, for example, I might have 2 moles of magnesium in. its alpha phase plus alpha-- let's say HTP-- plus a mole of nickel in its alpha FCC. phase. And these can react to form magnesium 2 nickel. And there's a normalization that you need to apply in order to use formation free energies in free-energy composition diagrams. For example, we need normalization to use delta formation free energy on a free energy composition plot. algebraically-simple point, but it's a conceptual point that it's easy to mess up. All right, I want to give you some examples of line compounds in nature and in technology. And then we'll come back to discuss the thermodynamics a little more. Before I move on to some examples, are there questions about this algebra, this arithmetic, the concept of formation energy, the mouse-face plot, anything, anything of that nature? So I have some examples here. agencies around the world have developed in the last century. There are certain silicon-based bronzes that have been developed for that application. The solid silicon phase appears to have no equilibrium solubility of copper. Silicon's not an intermediate phase, but it is a line compound. It's not three. that's why it's a trick question. Somebody, please, how many line compounds are there in this system? AUDIENCE: Is it three? RAFAEL JARAMILLO: Four. Doping semiconductors is why we're able to talk to each other over Zoom. Doping some metals into silicon is as important as it gets. gallium arsenide is the basis for all optoelectronics and photonic technology. The transmitters and receivers of all modern telecommunication devices, including your phones, are based on what's called III-V semiconductor, such as galliam arsenide. The two areas where you're going to find gallium. arsenide are places where silicon either isn't fast enough for electronics or you need to use. light in addition to electronics. Silicon carbide is a refractory-- some people will say it's a ceramic. Some people say no because it doesn't contain oxygen. It's of enormous industrial importance. It also is an emerging semiconductor material for high-power electronics. In 50 years from now, if the idea of a power substation is a thing of the past, it will be due to silicon carbide and similar high- power electronics that are being developed today. And this last one, this is the titanium-sulfur system. This is a neat system because, first of all, it has everything in it. Third of the Nobel Prize work that was recognized recently in 2019 with the chemistry Nobel Prize was for work on titanium-sulfide-based cathode. So these are-- for example, systems that have line compounds and other things. At low temperature, similarly to gallium arsenide, it's going to be a really boring free-energy composition diagram at low temperature. And it doesn't have to be that way. Here's a case of a line and an element that actually does have a pretty wide, solid solution. we have these composition variables. We're familiar with that. The equilibrium condition, the equilibrium condition dG equals 0 satisfied by common tangents. Chemical potentials are the same in both phases so that the coefficients are 0. That's ensured by the common tangent construction. So this is just recalling, right? Now let's imagine two line compounds. B3A2 and B4A3. How did I come up with that? Well, I sketched an imaginary phase diagram, and then I had to follow through on my sketch. 1/7 of the Gibbs free energy of the IV-III phase. And these phase factions are determined by lever law. This is really the point. When you have line compounds in coexistence in equilibrium, there are no internal composition variables. And that's what's led to everything we've been enjoying over the last month and a half, is the fact that solution phases are variable. And now we have these cases where the compositions are not there [INAUDIBLE] anymore. OK. I want to leave you with one final thought, which is leading into Wednesday's lecture,. which is the case of metal oxides. z is an integer or a rational fraction, and it's fixed-- SiO2, magnesium oxide, Al2O3, so forth. So when we return on Wednesday, we're going to talk about the thermodynamics of this reaction. We'll use this property of being line compounds, and we're also going to use a bunch of other things as well. Back to the page you came from. Follow us on Twitter @CNNOpinion and @cnnopin.

ROUGE-1: 31.35, ROUGE-2: 29.36, ROUGE-L: 28.40
BERTScore: 63.05

==============================================
==================== [51/100] ====================
Summary:
A random variable can take different numerical values depending on the outcome of the experiment. Some of the possible numerical values of a random variable will be more likely than others. We will describe these relative likelihoods in terms of the so-called probability mass function, or PMF. The PMF is also sometimes called the probability law or the probability distribution of a discrete random variable. It is a set consisting of two elements. It's a subset of the sample space. And it has a probability. And that probability we will be denoting with this notation. In order to do any probability calculations, we also need the probability law. Let us assume that every possible outcome, there's 16 of them, has the same probability which is therefore 1 over 16 for each one of the outcomes. What we want to do now is to calculate the PMF of this random variable. We need to find this value for all choices of z, that is for all possible values in the range of our random variable and add the probabilities of those values. For example, if X and Y both happen to be 1, then Z will take the value of 2. And similarly if we have this outcome, in those outcomes here, the random variable takes thevalue of 4.

ROUGE-1: 26.44, ROUGE-2: 25.39, ROUGE-L: 24.40
BERTScore: 74.31

==============================================
==================== [52/100] ====================
Summary:
Protein three-dimensional structure, how you get at it experimentally and computationally. Its implications for the binding of small molecules such as drugs. How can we use these to recognize other macromolecules, other proteins and nucleic acids? We have these motifs that we could find, weight matrices for them by aligning lots of sequences. Now instead of aligning sequences, let's see what we can do by mutating both the protein part and the nucleic acid part. The binding constant is measured in the molarity, roughly where you get half maximal or equilibrium binding. Instead of being single stranded, ready to bind fluorescent nucleic acid, it's double stranded. The protein complex in this case is a bacteriophage, which is displaying the three zinc fingers in red. The middle one is the one in the past slide was mutagenized. And similarly, the array is combinatoric-- every possible DNA sequence that you're interested in is present. groove of the DNA. And the reason the textbook is wrong, first of all, it emphasizes the non-helical part of the zinc finger. And also the way it loops through the DNA, if you look at this carefully in your textbook, it actually interdigitates with a phosphodiester bond. So we now have a semi-empirical way of computing-- in a certain sense, predicting new regulatory protein DNA interactions with double-stranded DNA. Can we extend this to RNA? This is a much more complicated situation with RNA because you don't have these long perfect double helices anymore. You have these very short RNA helices. Programming is just the beginning of interacting with input and output from various devices. The topic today is proteins, and this really is the main contact between the exquisite regulatory mechanisms. Here, we need sensors to sense the environment. We need actuators to then deliver back into the environment what the cell wants to do or to interact with other cells. You have to have feedback, synchrony, so on, and that's the topic of some of our network analysis in the last three lectures. that you can basically program the almost digital nucleic acid world inside the cell but via clearly analog inputs and outputs. We also-- I've listed some of the scariest proteins that I could think of. And we're going to talk about three of them. One of them in the slide is the proteins that are actually involved in causing the symptoms that come from when you're worried about anthrax. And then we'll talk about HIV yet again, this time, polymerase mutants that cause drug resistance. minus 9th seconds. That's atomic motion. The turnover of an enzyme that is the time it takes to for a small molecule, say, to find and bind the enzyme, to possibly go through a catalytic step, and to dissociate as a product. That could be used as a timer in a circuit of these longer time frames, like cell cycle, circadian rhythm, very long time frames in ecological systems with bamboo and various pests, and then development and aging, which can be on the order of hundreds of years. your accuracy, as you can get from NMR and X-ray crystallography, you now are in a position to study catalytic mechanism and design and improve ligands, such as drugs. This is really where we want to be. There may be a day where we can do this all from ab initio prediction or modeling at very great distances. But for now, modeling atvery short, say, 80% to 90% amino acid similarity, is important. And ways that you can discover the small molecules by a clever use of parts of it that you know bind. letter is the new amino acids. So for example, D67N means a [? sparcade ?] at position 67 and wild type changes to an asparagine. And that causes a drug resistance in the HIV, with unfortunate consequences for the patient. We can take-- now, making mutations in polymerases is not entirely of negative consequences. And I'm going to show you a really beautiful example where a DNA polymerase, you want to change it so that it can now handle what would normally be an inhibitor. The way we program in general proteins is either transgenics, where we might overproduce the protein, or homologous recombination. Point mutants are not the only way to generate conditional mutants. But there are ways that you can program, and conditional, meaning thatyou can regulate under what conditions the protein is expressed or not. Another way is by modulating the activity of the proteins from the outside with drugs or drug-like molecules and chemical kinetics. And under the subheadings for that, you can make these by combinatorial synthesis. that we didn't discuss before. But it's related to what we've been talking about. In the case of the zinc finger, we made an altered specificity. We made new zinc fingers with bind to completely new trinucleotides. With the DNA polymerase, by changing one amino acid, we could make it now accept almost four logs better. And here, many different-- many of these are enzymes, where you can not just knock out the enzyme, but actually make it recognize a new substrate, or change radically the binding constant and catalytic rate for new substrates. Three-dimensional structure could be used to predict deleterious human alleles. Multi-sequence profiles are a good way of looking at the conservation. That's a way of prioritizing single-nucleotide polymorphisms that might have impact on pharmacogenomics or disease in general. Now, as we integrate that with the chemical diversity that we can create-- that's that's what we're trying to do, says Dr. H.A. Hellyer, professor at the University of California, San Diego. The idea of chemical diversity, in a way I hope nicely connects to where we've been with RNA arrays. RNA arrays, and the double-stranded RNA array that we used earlier in class today, can be generated in a commentorial sense. Solid phase comes up again and again in arrays. It's very obvious why you have a solid phase. You want to be able to address it by its positions in x and y on the array. And it's a fantastic way of getting purification of your products simply by washing rather than doing complicated purification procedures. Protein synthesis is a completely synthetic way of getting short peptides. You can think of these as drug-like molecules. These are naturally related-- they can be analogs of nucleic acids and proteins. And we'll talk about opportunities for making these analogs. A large class of pharmaceuticals, including most of the antibiotics, are made by a fairly small class of organisms, streptomyces in certain plants. And this process by which it's made in more detail, is very akin to fatty acid synthesis. where the acyl carrier protein, ACP in the box, binds to the first monomer, and it starts transferring it from protein to protein along this multi-domain huge protein. And there's actually three proteins in a row here. And each of the steps are taken in order along the protein, and they involve things like a synthase step. And by changing the order of substrate specificities, you can build up a huge combinatorial collection here in microbial communities, and also in the laboratory. We have hundreds of proteins for which we have three-dimensional structures. From some of them, we have information on what ligands they bind. And typically, we want to select targets for binding drugs, or for solving the structures of proteins. And how can we do this? How can we decide which targets are high on our list to go for next? This is an example of a strategy where you use a little bit of prior knowledge, which can be empirical or it could be purely computational, about how to limit your library. If they are homologous to previous interesting targets, then that puts them high on the short list. If they are conserved and you knock it out, then you might expect that to be lethal. And that might make it a good target for an anti-bacterial. If you want to limit the action of your therapy to the surface in order to, say, reduce the cross-reaction with internal molecules, you can sometimes restrict yourself to surface-acceptable proteins. And in fact, a very large class of drugs is aimed at surface-accessible membrane proteins. Once you have a regulated gene, getting the protein sequence is easy. Getting from the sequence to secondary structure is easy in the context of some of these other things. But still, the accuracy is only around 77% for secondary structure and about 25% for ab initio three-dimensional structure. We'll pick up this thread right after a break and carry on to actually how we get the three- dimensional structure, whether it's predictive or experimental, and the computational tasks there. Take a break.

ROUGE-1: 28.84, ROUGE-2: 27.73, ROUGE-L: 27.40
BERTScore: 59.95

==============================================
==================== [53/100] ====================
Summary:
 AI is replacing human tasks faster than you might think according to a CNN business article more than half of large US firms plan to use AI within the next year. The New York Times reports that generative AI could automate activities equivalent to 300 million full-time jobs around the world. open ai's chief executive that's Sam mman says governments will need to assume the bulk of responsibility in supporting workers AI labor market disruptions and the question will employees just end up training AI systems only to them be replaced by them. last decade you know over a trillion customer insights are built on our platform every single week and now here we are in the generative AI Revolution. One quarter employees saved a combined 50,000 hours call me a cynic but are you just there putting those people to work harder or are you saying actually you can have a day off go home I mean how does that work how do we square that Circle well I think like you know any other uh benefits of becoming more efficient or more productive it is now spending more time on higher value impact work in your job. Cisco CEO: We have to train the Next Generation in both these Advanced AI skills but also some of the fundamental essential digital skills that we need. We've got to do both uh to help Society yeah and so that's a sort of skills Gap isn't it and Stephanie I wonder whether firms are ready because I'm looking at numbers here 97% say they are.get us that extra day off or a 3-day 4 day working week that we might all want um chintan talk to me about skills as well.  AI decoded looks at the impact of AI in the world workplace. Will AI be able to speed up and automate all sorts of monotonous or repetitive roles but there's also now a concern that intelligent chatbots could replace roles that have traditionally relied on a more human touch things like customer service or call center helplines. Are we destined to teach AI how to do our own jobs we're going to speak to one reporter who's been researching the threat posed to workers and how some are now fighting back around the world. Emma asked if we would actually see an increase in Union membership for private sector workers in the United States. She said that workers who feel that their jobs are at risk because of AI are going to rely more heavily on their unions to advocate for them. Emma said that the role that I'm seeing unions play is just reminding employers that they should be bringing workers to the table to help make and shape decisions around the way that AI is used in the workplace. EM: Could we actually replace CEOs or Executives and actually end up having AI run companies and just get rid of those High expensive salaries altogether? were most at risk because of automation. People are realizing you have to throw out the door any ideas you had about who is really at risk and say every job is going to be changed. We just hope that the workers whose jobs are changing have a voice in saying how yeah it's turkeyy is not going to vote for Christmas are they if those are the ones that could find their jobs being replaced but there's so much in there and Stephanie just a final thought from you in all of this about briefly if you will.

ROUGE-1: 26.06, ROUGE-2: 24.50, ROUGE-L: 23.72
BERTScore: 63.95

==============================================
==================== [54/100] ====================
Summary:
foreign I'm really excited especially for this lecture which is a very special lecture on robust and trustworthy deep learning by one of our sponsors of this amazing course themus AI. Themis AI is a startup actually locally based here in Cambridge our mission is to design advance and deploy the future of AI and trustworthy AI specifically. I'm especially excited about today's lecture because I co-founded Themis right here at MIT right here in this very building in fact this all stemmed from really the incredible scientific innovation and advances that we created right here. Sadhana is a machine learning scientist at Themis Ai. She's also the lead TA of this course intro to deep learning at MIT. Her research focuses specifically on how we can build very modular and flexible methods for AI and building what we call a safe and trustworthy Ai. We hope that really today's lecture inspires you to join us on this mission to build the future of AI and with that it's my great pleasure to introduce sadhana sad hana is an expert on the bias and uncertainty of AI algorithms. accurate class boundary between the two classes so how can we mitigate this this is a really big problem and it's very common across a lot of different types of machine learning tasks and data sets. The first way that we can try to mitigate class imbalance is using sample re-weighting which is when instead of uniformly sampling from our data set we instead sample at a rate that is inversely proportional to the incidence of a class in the data set. The final way thatWe can mitigate class balance is through batch selection which iswhen we choose randomly from classes so that every single batch has an equal number of data points per class. set of features can we still apply the techniques that we just learned about the answer is that we cannot do this. The problem is that the bias present right now is in our latent features all of these images are labeled with the exact same label so according to the as the model all we know is that they're all faces. If we knew this information we could label the hair color of every single person in this data set and we could apply either sample re-weading or loss relating just as we did previously. to dbias a model so what we want is a way to learn the features of this data set and then automatically determine that samples with the highest feature bias and the samples with lowest feature bias we've already learned a method of doing this in the generative modeling lecture. Now we'll walk through step by step a de-biasing algorithm that automatically uses the latent features learned by a variational autoencoder to under sample and oversample from regions in our data set um before I start I want to point out that this debiasing model is actually the foundation of themis's work this work comes out of a paper that we published a few years ago. good representation of what a face actually is so now that we have our latent structure we can use it to calculate a distribution of the inputs across every latent variable and we can estimate a probability distribution depending on that's based on the features of every item in this data set. This allows us to train in a fair and unbiased manner to dig in a little bit more into the math behind how this resampling works this approach basically approximates the latent space via a joint histogram over the individual latent variables. data point x will be based on the latent space of X such that it is the inverse of the joint approximated distribution we have a parameter Alpha here which is a divising parameter and as Alpha increases this probability will tend to the uniform distribution and if Alpha decreases we tend to de-bias more strongly. This gives us the final weight of the sample in our data set that we can calculate on the Fly and use it to adaptively resample while training. Once we apply these this debiasing we have pretty remarkable results this is the original graph that shows the accuracy gap between the darker Mills and the lighter Mills. an extremely famous paper a couple years ago showed that if you put terms that imply female or women into a large language model powered job search engine you're going to get roles such as artists or things in the humanities. If you help input similar things but of the male counterpart you'll end up with roles for scientists and engineers so this type of bias also occurs regardless of the task at hand for a specific model. Finally let's talk about Healthcare recommendation algorithms these recommendation algorithms tend to amplify racial biases. the core idea behind uncertainty estimation so in the real world uncertainty estimation is useful for scenarios like this this is an example of a Tesla car driving behind a horse-drawn buggy which are very common in some parts of the United States. The exact same problem that resulted in that video has also resulted in numerous autonomous car crashes so let's go through why something like this might have happened. There are multiple different types of uncertainty in neural networks which may cause incidents like the ones that we just saw we'll go through a simple example. model we have the same output size that predicts a variance for every output so the reason why we do this is that we expect that areas in our data set with high data and certainty are going to have higher variance. The crucial thing to remember here is that this variance is not constant it depends on the value of x so now that we have this model we have an extra layer attached to it in addition to predicting y hat we also predict a sigma squared how do we train this model our current loss function does not take into account variance at any point. a data set called cityscapes and the inputs are RGB images of scenes the labels are pixel wise annotations of this entire image of which label every pixel belongs to and the outputs try to mimic the labels they're also predicted pixel wise masks. Why would we expect that this data set has high natural alliatoric uncertainty and which parts of this dataSet do you think would have aliatoric uncertainty? And that's exactly what we see if you train a model to predict aliatoic uncertainty. Even if your pixels are like one row off or one column off that introduces noise into the model the model can still learn in the face of this noise. very little variance in the um the logits or the outputs that we're predicting. If a model has never seen a specific input before or that input is very hard to learn all of these models should predict slightly different answers and the variance of them should be higher than if they were predicting a similar input so creating an ensemble of networks is quite similar. Themis is dedicated to developing Innovative methods of estimating epistemic uncertainty that don't rely on things like sampling so that they're more generalizable and they're usable by more Industries. At Themis we believe that uncertainty and bias mitigation unlock a host of new solutions to solving these problems with safe and responsible AI. We can use bias and uncertainty to mitigate risk in every part of the AI life cycle. We're developing a product at Themis called AI guardian and that's essentially a layer between the artificial intelligence algorithm and the user and the way this works is this is the type of algorithm that if you're driving an autonomous vehicle would say hey the model doesn't actually know what is happening in the world around it right now as the user. compete in the competition which the details are described in the lab but basically it's about analyzing this data set creating risk-aware models that mitigate bias and uncertainty in the specific training pipeline. At Themis our goal is to design advance and deploy a trustworthy AI across Industries and around the world. We're hiring for the upcoming summer and for full-time roles so if you're interested please send an email to careers themesai.io or apply by submitting your resume to the Deep learning resume drop and we'll see those resumes and get back to you.

ROUGE-1: 27.42, ROUGE-2: 26.56, ROUGE-L: 27.03
BERTScore: 66.55

==============================================
==================== [55/100] ====================
Summary:
HONG LIU: So if you want to compute, say, some scattering amplitude from alpha to beta-- so alpha's some initial state and beta's some final state. Say alpha consists of momentum p1 and PN-- or pm, and beta, say momentum p m plus 1 and pn. And then you can get this scattering amplitude just by taking your momentum-space correlation function, OK, for the n points. So this is obtained by doing a Fourier transform. So any questions on this? Yes? AUDIENCE: So can you explain again why this diagram, like, you have one branch and then there's a loop? HONG LIU: To derive that theorem, the time-order matters. OK, you can do this for the full interacting theory. Yeah, just use the free scalar as the-- yeah, because when you go to plus or minus infinity,you can just reduce to the free particles. OK. So this is the first comment. The second comment, each side, we need to here-- from here,we need to truncate the external propagator. OK,. You can consider arbitrary, complicated diagram, OK, as far as that only happens-- yeah. HONG LIU: Z is the same for different processes, but it's different for different particles. Z also has an expansion, just 1 plus order lambda, et cetera, so the leading order, so Z does not contribute. OK, you can just set it to 1, and when you go to the higher order, then the Z can make a contribution. So you just need to separately take into account the Z, and there's no need to contribute-- to calculate those things separately. HONG LIU: This is the self-interact-- yeah, just when the-- when you have an interacting theory. When the particle propagates, it actually can interact with the virtual particle. And so that, this kind of interaction, will affect the property of the propagation. It can have the most effect by prefactor, but actually can change the mass, too. But for the-- oh, it can correct for the mass-- and also, that's the subject of the QFT2. Klein-Gordon equation was the first attempt to write down a wave equation for relativistic particles. Dirac equation was aimed to cure those problems. It solved the first problem, OK, but didn't really solve the second problem. But the level is, again, due to more fundamental reasons, you cannot really interpret the Dirac equations as a quantum mechanics equation. It should be interpreted as a field theory. So nowadays, we interpret this this is the-- gives the field theory for-- OK, of fermions. course, Dirac didn't know this, so essentially, he discovered this beautiful theory for the wrong motivation. And yeah, this happens over and over again in physics. But the key is that if you are good enough, you will find something new and that something new will be useful. OK, so this, first, I'll talk about, introduce the Dirac equation, OK, and its covariance. OK. So the goal is that you write down the equation, like a Schrodinger equation, which is first-order-in-time derivative. doesn't make any sense, OK? So alpha and beta, they have to be some kind of constant, OK, but if alpha andbeta are constant, then this is not even a rotational invariant, not to mention Lorentz environment, OK. So alpha is just some constant, and then, so let's say they are n-by-n matrices. Then in order for that equation to make sense, then psi has to be a n- component vector. OK. They're just some constant Hermitian matrices. And then, so then he reasoned that for this equation, if we want this equation to be Lorentz covariant, then at least it should have the relativistic plane wave as its solution. If it does not even have that type of solution, then you, yeah, of course cannot be covariant. OK, so the minimal requirement-- so before we really try to see how can we make this into a covariant equation, we say let's consider minimal requirement. So we'd like to call this equation star. The Klein-Gordon equation has the following form. We want the right-hand side to be equal to this equation. If that works, then we're guaranteed to have a plane-wave solution of such a dispersion relation because this equation has such a relation. So for this to be true, we just need to have-- so let's compare the second-order derivative term. And then we need alpha i, so we need-- first, when i not equal to j, the off-diagonal terms, they all should vanish, because here there's only diagonal terms. HONG LIU: We said the alpha and the beta, they must be Hermitian. So if we satisfy all these four conditions, and then we will guarantee that that equation star should have the plane-wave solution. And so now you just try to find matrices, satisfy those conditions. OK, you see that to satisfy them needs at least a 4-by-4 matrix, so n has to be 4. So for example, here is one solution. Say you take beta to be 1, so all matrices here should be understood as 2- by-2 matrices. alpha acts on a different component of psi. If you took the grad of psi and then multiplied by alpha, then you'd mix up the components of it. So you'd have a-- so alpha x because alpha x would just be the Pauli x matrices times the grad x. OK? Good? So now, good, you say we have an equation, OK, so psi-- soAlpha is a 4-by-4 matrix, so that means that psi should be a four- component vector. HONG LIU: This is a new object, so we call it spinor. OK, so one half, essentially, you can-- yeah, based on one half,. you can generalize it. So that's why I say this was really genius, because just nobody could have thought of this. OK? There was no clue of such a structure. Yeah. So this is a 4-by-4 matrix. It's just a convenient way to write these 4- by-4 matrices. OK. HONG LIU: So I divided these 4-by-4 matrices into four 2- by-2 blocks, and then I specify each block. AUDIENCE: Yeah, it's just blocks, not matrices in the matrix. HONG LIu: So that I don't have to write all four components. I just-- yeah. OK. So now I will denote-- introduce a new notation so that it looks nicer. I'll denote the gamma 0 equal to i beta and then the gamma i equal toi times beta alpha i. HONG LIU: All the gamma matrices, so you can check yourself, OK, all these are given by just this. When mu not equal to nu, of course, the right-hand side is 0, so that just is equal to that equation. From those solutions of beta and alpha, we can write down different solutions of gamma. So for example, for 1, you're corresponding to gamma 0 equal to minus i. There are infinite number of such solutions, and I will comment on all those different solutions. same story just here, you just forget about beta. OK, the same story, you forget the beta, and then you only need alpha i alpha j the commutator to be 0 for i not equal to j. And then also, you'd want the alpha i square equal to 1, OK, so for any i. So now these are the conditions for the alphas, and now you can actually satisfy by 2-by-2 matrices. So what matrix satisfies this kind of relation? AUDIENCE: Pauli. the meaning of all these different solutions for alpha and beta or for gammas, OK? So as I mentioned, you can have infinite number of solutions. What's the meaning of them? So first, let's imagine when we look at this equation-- so as I said, this is a matrix equation. In this matrix equation, then you have this psi which is a four- component vector. OK, so imagine, just say, consider making a basis change in psi. So now, so psi prime satisfies essentially the same equation but a different gamma matrix. because one is going to change the basis. So you can show any matrices which satisfy that equation, they're all related by similarity transformation. They're just corresponding to a change of basis. Different forms of the gamma, they may be useful for different purposes. OK, so it depends on which regime, sometimes you use different gamma matrices. So now having introduced the Dirac equation and then the structure of theDirac equation, but still we haven't showed that the DirAC equation is covariant. Consider such a Lorentz transformation, OK, so and then phi transforms as following, phi prime x prime phiprime. New phi evaluated at the new position should be the same as phi evaluation at the old position. So the phi-- yeah, just equal to phi lambda minus 1 x. So now if you look at the Klein-Gordon equation, let's see how this is covariant. OK, it's the same in any frame. Now we want to show that the Dirac equation has the same property.

ROUGE-1: 29.70, ROUGE-2: 28.73, ROUGE-L: 28.84
BERTScore: 68.38

==============================================
==================== [56/100] ====================
Summary:
The US Supreme Court is the highest federal court in the United States. The Constitution doesn't specify any qualifications, so a president can nominate any individual to serve. Most presidents nominate individuals who broadly share their ideological view. So far, six justices have been foreign-born, at least one never graduated from high school, and another was only 32 years old when he joined the bench. The Senate Judiciary Committee votes to send the nomination to the full Senate with a positive or negative recommendation, often reflective of political leanings. approve, it's by a simple majority vote, with ties broken by the vice president. With the Senate's consent, the president issues a written appointment, allowing the nominee to complete the final steps to take the constitutional and judicial oaths. In doing so, they solemnly swear to administer justice without respect to persons and do equal right to the poor and the rich. This job is for life, barring resignation, retirement, or removal from the court by impeachment. Of the 112 justices who have held the position, not one has yet been removed from office as a result of an impeachment.

ROUGE-1: 47.13, ROUGE-2: 44.80, ROUGE-L: 44.20
BERTScore: 65.55

==============================================
==================== [57/100] ====================
Summary:
This is actually the second time we are having this class. We had it last year in a smaller version. That was for six units of a credit, and we had it once a week. And so we got some good response last year. So, with the support of the math department, we decided to expand this class to be 12 units of credit. And we have every Tuesday and Thursday afternoon from 2:30 to 4:00, as you know, in this classroom. And the purpose of this course is really to give you a sampling menu to see how mathematics is applied in modern finance. my personal story. I want to tell you why I tell the story later. But the story actually was in the mid '90s. I just left Salomon Brothers -- that was my first financial industry job -- to go to Morgan Stanley in New York to join the options trading desk. So the first day, I sat down, I opened the trading book, I found something was missing. So, I turned around, I asked my desk quant. I said, where is the vega report? So, let me show you. called kappa at Morgan Stanley. Kappa is also called vega by some uneducated traders at the Salomon Brothers. They have mistaken vega as a Greek letter after gambling at Vegas. So anyway, so that was my first day. So obviously, I learned how to call kappa very quickly. And I called it kappa in the last 17 years, but you will hear people calling it vega. Obviously, I have probably more people call it the vega, but anyway. But why did I tell you the story? What point I try to make? everything are changing very rapidly. Even nowadays, they're still changing. So with that, I will give you some background on how the financial markets actually started, and that's really the history part of this industry. In early days people need to exchange goods, so there's exchanges. Then it becomes centralized. There are stock exchanges, futures exchanges all over the world where these products will be listed as securities. That's one way of trading, which is centralized. Obviously, in the last 10, 15 years, now we have ECNs, electronic platforms. Trade over-- you know, even larger volume of those trades. has money to lend out, someone needs to borrow money. Loan is really a private agreement between two counterparties or multiple counterparties. When you securitize them, they become bonds. Commodities, actually, you know. Metal, energy, agriculture products are traded, mostly in the futures format and some in physical format, meaning you take deliveries. And the real estate, you're buying and sell houses. And further of all of these, you heard probably a lot about the derivative products. So, that started with swaps, options. type of player is really bank. After 1933 Glass-Steagall legislation, there were two main types of banks. Commercial bank is supposedly, you're taking deposits and lending out the money. Investment bank supposed to focus on the capital markets, raising capital, trading, and asset management. Some people blame that, and probably for a very good reason, for the cause of 2008 financial crisis. But why do we need financial markets? There's a need for it. It's really the need to bridge between the lenders and the borrowers. Banks and so-called dealers play the role of market making. If you want to buy something or sell something, if I'm a broker, I don't make you a price. I actually put two people together, matchmaking, make that trade happen. So, that's a broker's role. There are individual investors, retail investors, same meaning. Mutual funds, who actually manage public investors' money. Hedge funds. They basically find opportunities to profit from inefficient market positioning or pricing, so they have different strategies. If you invest in the Australia Ozzie, Australian dollar. The Australian dollar may become weaker to the yen. You may lose all your profit, or even more. And further, if everybody plays the same game, then when you try to exit, you have the adverse impact of your trade. Even if you are not a finance guy, you work in a corporate, you just do you import, export, or building a factory. So, risk management, nowadays, becomes pretty widespread responsibility. stock, I think a lot of people know pretty much where it is. But if it's not transparent, so what do you do? So that market maker comes in to provide that liquidity, and then takes the risk. They manage the book by balancing those Greeks, which I mentioned earlier. And we talk about the volatility exposure was vega. And on top of that, what are the tail risks? What are the events can actually get you into big trouble? So people use value at risk. I think Peter will-- or Choongbum will-- probably Peter will teach. Trading is about finding relationships between prices, and trying to profit from those relationship mispricing. Not many people focus on arbitrage, because lot of people are gut traders. If you trade gold in the States, the gold price happen in Asia and in Europe matters, right, because you're trading the same thing. And that's just a simple example. But a spot price versus forward price, that's a deterministic relationship. It's a mathematical relationship. If that relationship breaks down, you can also profit. So there are many examples mathematical relationship which gives you the arbitrage opportunity. Math is very effective, because when you, your bank, your corporate, you want to buy some financial instruments, you have to know where is the price. So, pricing model, which Vasily and many of his colleagues can tell you more. Risk management itself is very challenging. It's not a purely mathematical question, but yet, math plays a very important role to quantify how much exposure you have. Then, I think a lot of people with math background, or in general, people are looking for the so-called holy grail trading strategies. You just turn it on. It makes money by itself. Professor Jake Xia: Trading is really all about how do you risk manage, have the discipline, and how to manage your losses. When you go to the market, you buy a stock. When the stock goes up, makes bit of money, the natural tendency is to let's take profit. But when the stock loses money, what's your natural tendency? Professor Jake XIA: I think natural tendency, lot of people will keep it. But if you think the fundamental story is very sound, you should think about as if you don't have a position. actually choose choice B, because that's low expectation, which makes sense. But I think if you ask a larger audience, I think a lot of people don't really want to choose choice A, because they don't want to lock in the loss. If your bank account balance is-- let's say you are a freshman student. Your choice will be very different from someone has $100,000 in his bank account. And also, your risk tolerance, how much you can tolerate. Learn the math, learn the finance first, but keep those questions along the way when you are learning during this class. Go to the course website, read what we have put up for the financial glossary. If you still have things you don't understand, compile your own list of financial concepts, which you can search on the web or even ask us. So, that's really-- and read other materials on the course work. We got maybe-- how about this? We still got about 15 minutes or 12 minutes left, so I'll pass it to Vasily, then maybe we can leave five minutes for some questions. every time. So, if you want to differentiate this functions and get a derivative, then this derivative will be quite noisy. And so, instead of getting the true derivative, you might obtain something quite different from true derivative just because there is a confidence interval around any point. So obviously, there is somewhere balance, and the question was, is there an optimal shift size to get the derivative? And that's what-- uh oh, the slide got corrupted. There was an answer. And we actually are implementing it in our systems and plan to use it in practice. We put syllabus there, a short list of literature. We will be posting a lot of materials there. Probably most lectures will be published there. Jake's slides are there already. So, any questions? We like to get your emails so we can put you on the website for further announcements. But it's probably easier if you put your email on the sign up sheet, so that we can [INAUDIBLE]. VASILY STRELA: Yeah, but please visit and sign up here, because there will be announcements to the class.

ROUGE-1: 31.77, ROUGE-2: 30.45, ROUGE-L: 31.37
BERTScore: 61.44

==============================================
==================== [58/100] ====================
Summary:
Climate change is changing us from the inside out in the UK one in four adults and one in 10 children experience mental illness. Half of the people they spoke to experienced negative impacts on their mental health due to the heat. People on Lower incomes and other disadvantaged groups groups were more likely to feel the negative effects of extreme heat because it was harder for them to keep their homes cool. You don't even need to be alive to experience some of these effects a study of expecting mothers who experienced Hurricane Sandy in 2012 the huge storm that hit New York showed that in unborn children who did experience that storm girls as early as preschool were 20 times as likely to experience anxiety. people in that storm showed that about half of these folks had some kind of post-traumatic stress and that's relative to 5% of the general population. Direct effects of climate change on people's mental health there's also knock on effects like loss of income and Rising prices which inevitably have implications for people's well-being. We are not separate from our environment we are connected not just to the world around us but of course to one another and it and it is only in working with one another that we're going to be able to move forward.

ROUGE-1: 34.78, ROUGE-2: 33.63, ROUGE-L: 31.18
BERTScore: 59.52

==============================================
==================== [59/100] ====================
Summary:
Adam Martin: I want to show you how you can go from being interested in some property of an organism, or even its behavior. How would you go from there to identifying genes and mechanisms that are responsible for that type of behavior or appearance? And on my title slide here, I have three fruit fly mutant phenotypes that you can see, and each of these mutants defined genes that were subsequently found to be present-- or homologous genes were present in humans and were shown to play important roles in human biology. "Genetic approaches are as fundamental to biology as math is to physics," says professor. "The two heroes of today's lecture will be the roundworm, Caenorhabditis elegans, and the fruit fly," he says. "These model organisms are fairly small, and they're easy to house large numbers of them in a lab," he adds. "They're often cheap to house in the lab and work with. Also, they develop fast," professor says of model organisms. The goal in genetics is to identify a mutation that alters a gene function that gives you a phenotype that you're interested in. So you try to induce mutations. And these mutations could be spontaneous mutations, meaning you didn't do anything to induce it, but they just appear as a variant in the population. In the way we can induce mutations is by using some type of mutagen. So for example, you could have some sort of chemical mutagen that increases the error rate in DNA replication, or you could use radiation to induce DNA damage. Miles Miles: If you're feeding an organism some chemical mutagen, you're inducing random mutations in different places. Miles: You don't know which are the ones that you want until you look at their phenotypes and try to find the needle in the haystack. He says in model organisms, we can actually find these types of mutations. Miles says the most likely mutation would be overshadowed by the direct gene, the female gene, that's in the male generation of fruit flies. the sonic-hedgehog gene is important in cancer. And actually, there are now a number of drugs that are being developed to target the hedgehog pathway. One was approved back in 2012 for use in treating basal-cell carcinoma. And there's currently another drug that's in phase-II clinical trials for treating some forms of leukemia. So this is a story that goes from identifying this weird fly mutant all the way to clinical trials, developing drugs, whose purpose is to inhibit this signaling pathway. if this is a bona-fide cell-death mutant, then you should have extra cells. And it turns out the ced-3 mutant blocks all 131 of these cell deaths. So there's an active mechanism involved in the cell death. Any questions about how these screens go in the crosses before I move on? OK, so I have one more story to tell you about, and this relates to behavior. We all behave, some of us better than others. So how is it we can go from something as abstract as behavior to specific genes and mechanisms that control this? Researchers identified mutants that didn't show this 24-hour cycle. Other mutants alter the period of this cycle so that it's either longer or shorter. This is a really elegant way to look for genes that are important for circadian rhythm. This was done by Konopka and Benzer. And they did a nice screen. It involves a genetic trick which I'll show you. And the fly-- the reason it was done, in this case, is they wanted to mutate the X chromosome and then have the fathers pass on their X chromosome. F1 mutated males, and crossed them, again, to attached X flies. So now, you have multiple males, all of which are mutant. The mutants that the Benzer Lab identified had altered period of the sleep-wake cycle, and therefore, the gene was named "period" So this screen identified a gene called "period." This is a gene. And the gene in humans is associated with familial advanced sleep-phase syndrome. So defects in the genes that were identified in Drosophila are relevant to human sleep disorders.

ROUGE-1: 21.07, ROUGE-2: 19.80, ROUGE-L: 20.56
BERTScore: 65.98

==============================================
==================== [60/100] ====================
Summary:
In this chapter we will discuss two applications, one price control and second taxation, so right. Sir, does this slope of this graph denote anything price demand upon, some price upon some quantity? So, wait little later we will talk about that that topic, right now we are just talking about movement and shifts, the direction of movement. We are not talking about the slope. So, what is price control what do we mean by price control? Price control is how we can regulate the prices of the goods in the market. Many people in this country would like to have gas connection, but the suppliers cannot afford to give those many gas connections at the government-determined price. So, what they say you go to, you go for gas connection if it is your turn if somehow you are getting gas connection. What the seller would say that you want gas connection you will also have to buy gas burner from me then well you will get the gas connection ok. You are not interested in buying gas burner at really at the price which is much higher than the price that you will you have to pay in the market for a gas burner. When we have a price floor, we get excess supply then you have non-market rationing mechanism. Like the payment is not done at the time of buy, it is done after year or after few months. And one more thing we have to pay a minimum wage to the laborers. So, all these things happen whenever we have price control. But I am not saying that price control does not have advantage that when we talk about the welfare economics towards the end we will talk about it. But what I am saying definitely that there are definitely better ways to cater to the welfare of public.

ROUGE-1: 18.06, ROUGE-2: 17.50, ROUGE-L: 17.89
BERTScore: 68.55

==============================================
==================== [61/100] ====================
Summary:
Barbara Imperiali: We are going here now from the smallest carbon atom. She says in order to make proteins like hemoglobin and antibodies, we need large entities. The things that will feature today are the transfer RNAs and the ribosome, she says. IMPERIALI: And what to me is intriguing is that the size of theribosome is pretty similar to theSize of the rhinovirus, a little smaller than the hepatitis virus, but quite a bit smaller than some of these other viruses. messenger is single-stranded RNA, obviously. It has a 5 prime cap, which has got this funky 5 prime, 5 prime bridge that's resistant to exonucleases. Somewhere in that sequence is a start codon. It's something that says this is the bit I want to translate. Often, there's a lot of stuff here that you don't translate. Part of what's known as the ribosome binding site. And there are many features in this part of the sequence that are very important for translation. The genetic code, which forms the basis of this entire concept, has some features to it where it does have some degeneracy. But we'll go through the degeneracy, and we'll take a look at the genetic code because it will tell us exactly how the three letter-- the words made of three bases encode everything we need for translation. The ribosomes were a point of work that was awarded a Nobel Prize for that work, and then on, things started to get to the point where you know, these are the things you want to get out to. decade later, the sort of details of the structure, but not the structure itself. And it was really exciting in the 2000s when Ramakrishnan, Steitz, and Yonath solved the structure of the prokaryotic ribosome. So each of these things has taken a decade to happen, but they are fundamental, major, important things that we can act on and move forward to understand more. All right, so let's move to the transfer RNAs. And I've flashed up this slide a couple of times, but I actually now have the movie of theructure of a transfer RNA. A lot goes on with the rest of the structure. It's a very important structure in the mechanisms of protein translation and synthesis. And, coming down to the anticodon loop, you're going to read the messenger 5 prime to 3 prime. And so, if you look up here at the acceptor stem, the amino acid is joined by an ester bond to the 3 prime end of the transfer RNA, but, hopefully, you can see in here. There's the carboxyl. And CHR designates the aminoacid where R would be the side chain of your amino acid. Pseudouridine is a base known as pseudouridine. It has an interesting structure where the bond to the ribose ring is not a carbon-nitrogen bond. The genetic code gives you the identities of what are known as the codons. The codon is the absolute-- the sort of Rosetta Stone for translating messenger RNA to amino acid sequence using codons, says Barbara IMPERIALI. The most common start is the codon for methionine, which is the terminus of the protein. When you're looking to read what your amino acids that get put in may be, you're going to look at the codon. And it will tell you exactly the amino acid. The genetic code in that box that I just showed you is written down for maximum clarity and ease of use. So, whenever you see a particular three-letter code on the messenger, you will then be able to know what amino acid it would code. So you really want to be reading 5 prime to 3 prime in the codeon. small and large subunits. In orange-- well, that's kind of a burnt orange-- is a sneaky little bit of the messenger. In yellow are the transfer RNAs. And there's one more unit on here that I won't describe too much. It's a protein factor that helps all the processes occur. Generally, it's thought to help the loaded tRNA come to the ribosome, get it in place, and then go away. So it's some of these extra helper proteins that are involved. In each step, you're bringing in a tRNA that's loaded with an amino acid where the anticodon of the amino acid is complementary to the codon that's within the messenger RNA. Methionine is the first amino acid. And it's always at the N-terminus. Then there's another amino acid comes in. And then the next thing that happens is there's a movement such that a new bond gets-- a new amide bond is formed between the methionine and the phenylalanine. That's how that comes together. There are errors that will introduce defects into the ultimate protein. The first type of error is a nonsense mutation, which might be leaving out a base pair, inserting one, substituting one. The next types of mistakes are what are called silent mutations. And the last ones are the ones where we start to encounter errors in DNA that result in errors in proteins that may cause genetically inherited diseases. But that's the story for another day. It's not always perfect, but what is known now is that, as proteins are emerging from the ribosome, they're starting to fold almost immediately. we had a missense mutation, and we incorporated a valine instead of a glutamic acid, just through one change in the DNA. The missense mutations are the more serious ones because you end up with a full length protein that might have a mistake in it. And then that would affect the function. Don't forget my office hours on Monday if you need them. I think this field is fascinating. Once you get used to the mechanics of it, it's really cool to think of how you go from DNA to RNA to folded proteins.

ROUGE-1: 25.38, ROUGE-2: 24.23, ROUGE-L: 23.87
BERTScore: 65.21

==============================================
==================== [62/100] ====================
Summary:
Professor Donald Kagan: We are living in the early years of a polis sometime in the eighth century B.C. The date that's sort of typical for the general phenomenon of colonization coming out of the mother cities of Greece. Every time we see a colony, learn anything at all about it, it appears to exist in the form of a Polis. That powerfully suggests that that was the typical characteristic style of life that had already been established for Greeks before they sent out the colonies. Greece is again that date we keep talking about, 750 roughly. But in fact, the earliest date according to Greek tradition, if my memory is correct, was something like 773. The Greeks were, even though they went to sea plenty, they were terrified of the sea for very good reason. The vast overwhelming majority of people needed to farm land, in order to stay alive. One answer, and it's the one that is most widely believed among Greek scholars, is that the growth of population that we have mentioned in connection with the rise of the polis is still working. place for trade. The desire for a good commercial opportunity might well have been one of the elements that these people who were leaving their home cities sought. In any group of people there is a small minority, I want to emphasize small, who just love to do risky things. They just love adventure; they're never happy if they're safe, and so off they go seeking adventure and seeking to make a fortune. And now that we do have something, namely, this wave of colonization, they join that as well. Colony is a Latin word ultimately for colonia and the Roman colonies were, first of all, garrisons that they planted in land they had conquered to keep the people quiet. The Greek word for this is, apoikia, and most literally it would mean a home away, an away home and that's what they're making. They are establishing for themselves a household, a home someplace away from where they started. They were the antecedents of the Serfs, which we will see later on in medieval history in Europe. of eminence, and yet unlikely to be part of the sort of dominant faction in that city, because otherwise why would they leave? Anyway, the Greeks had a name for this individual. He was called an oikistes; he is the found of the colony. So, now he has decided to do it and he's gained recognition from the town council, let's say, and he can go forward. Now, he has to have an idea. What is more typical, I think, is that he thinks, I would like to take and have found a colony on the southeastern coast of Sicily. Why? Because he knows something about it. John Hale of the Yale Class of 1973 is now an archaeologist at the University of Louisville. He discovered evidence that totally confirmed the Greek story. They tell you precisely what the gases were, what the characteristics of those gases were. It squared beautifully with all details that we heard about the Delphic Oracle. So, here's just one more case where Yale helped to straighten out the world, but you notice it wasn't done by a Yale faculty member, we engage in confusing the world but our alumni do a much better job. Greeks and barbarians came to Delphi to consult the Delphic Oracle. Most of the things they asked were questions that really had a yes or no answer. The oracle probably gained its fame for being very good precisely at answering this question. These people knew more than anybody else about these things, and so consulting that oracle was a very act of rational thought, says John MacIntosh, professor of ancient history at the University of California, Los Angeles, and author of the book, "The Lost City of Syracuse" will decide whether it's a good idea for him or not. Now, recruiting is tremendously important because you need to have a certain number of settlers to make the settlement viable. So however many that is, that is what you try to recruit and you recruit typically at a time when it's easy to get people together so you can tell them the story. The best time would be at some great festival. There are festivals held in each city just for its own citizens and my guess is that when you could do that, when you felt that you could recruit a full colony from your fellow citizens in Corinth, let us say, that's what you did. Syracuse is an independent polis, autonomous, self-governing, whatever regime it wants, etc. It is not a subject of anybody, not Corinth or anybody else. The most typical, the usual, everything else is an exception is that there are friendly relations between the mother city and the apoikia. In the Peloponnesian War, Syracuse finds itself besieged by the Athenians. They go to Corinth asking the Corinthians to please help us. The Corinthians send very little, send a couple of ships and a general. The city of Corinth sent out a lot of colonies, which is why we know something about their arrangements. When Potidaea got into trouble with Athens, and found itself besieged, Corinth sent a real army to go in there and fight, and I think that's because of this very special relationship that they had. At the other end of the spectrum it's again Corinth and they have a colony up in the northwest called Corcyra, it is the modern Island of Corfu. The first relationship between them is a navel battle, and thereafter we hear of them quarrelling and fighting with each other. more time, that the overwhelming normal situation is the first one I described, friendly relations. We do know, again, Thucydides is our source, that it was customary for colonies to send representatives back to the mother city for the religious observations that were common to them all. They feel like their relatives, and what could be more natural. You're out there in Sicily and you discover, of course, that you don't have all of the things that you used to have available to you. Kagan: Before the rise of the polis, the Greeks had already spread out from their original settlements. By the tenth century B.C., we see Greek cities lining the coast of Asia Minor on the west, and even around on the bottom and to some degree on the north, and on the islands in the Aegean. The way the Greeks did their immigration into Asia Minor actually had a pattern so that you can go from north to south and you will find some consistency. Kagan: It's a helpful story to remember, because we tend to think of Greece because of its modern geography. there, Palestine, there are no Greek cities there and that is because during the period we're talking about those regions were occupied by civilized powerful people. There is one exception. In the sixth century, I think it's around--imagine around 550 or something like that, the Greeks settle a single colony in the Delta of the Nile of Egypt at a place called Naucratis. So, that's a great exception to everything I've been saying. Going west, would you believe, when you get into what is now Libya, there was a very important Greek colony of Cyrene. powerful. It tried to control not only North Africa, but the waters of the Mediterranean in the west entirely. The Carthaginians, in fact, have a powerful pied à terre in the western part of Sicily. So, that's how far east they get and in time the Carthaginian also cross over into Spain and they control some portion of the Spanish coast closest to Africa. There are now Greek cities on the coast of Spain and there continue to be Greek cities, not everywhere, in a spotty way into France. Greeks are not interested. You will find very rare of the case of a Greek city, which is founded away from the sea. Lots of these towns sent colonists up north into the Dardanelles and so on and beyond. Sparta starts at an early point, sends out a colony to southern Italy at some time, probably early. Thebes, the greatest city in Boeotia, also does not colonize. So what can we speculate is the meaning of that? Greece's impact was greater in the west and the north than it was in the east and the south. The Greeks lived among people who were more civilized than they. They had very little to teach or to impose upon those people rather than vice versa. philosophy is going to be invented in Miletus probably in this sixth century B.C. Well, MileTus was on the main routes to all of the places where advanced knowledge could be found, Mesopotamia, Egypt. It is inconceivable the Greeks could have developed a civilization that they did without contact with these eastern civilizations. There is no evidence of a capitalist class growing up in Greece. But you do have people who are engaged in activities that are different from those of the rest of their people. The hoplite revolution is going on. More and more farmers are becoming independent, self-sustaining, hoplite farmers of the kind that I've described. You can't expect them to continue to live in the same way as they did before, deferential to their betters, that is to say, tugging their forelocks before the aristocrats. come to mean weights of silver or gold. So now you have a change in fundamental economic things. Well, all of this is tied up with this colonial story I've been telling you. Finally, I think, it works in both directions at the same time in terms of the impact all of these changes have on the political situation. On the one hand the changes, that is (A) the rise of the hoplite class; (B) the development of lots of commerce and industry and wealth in a new kind. that colonization provided something analogous to that for the Greek people. So now, here we are somewhere in the seventh century, most of these places I've been talking about have been settled, the currents that I have been describing are flowing and the kinds of problems they have give rise to what will be felt in most of the towns. That is the proper introduction to the next topic, which I'll discuss next year. No not--it seems like a year, but it's next Tuesday actually.

ROUGE-1: 30.49, ROUGE-2: 29.10, ROUGE-L: 29.30
BERTScore: 65.80

==============================================
==================== [63/100] ====================
Summary:
Instructor: We are asked to identify the areas of consumer surplus, producer surplus, tax revenue, and deadweight loss in this market after the tax. So pause this video, have a go at it. Even if you struggle with it it will make your brain more attuned to when we work through it together. All right, now let's work through this together. And I just want to sort of understand what's going on here before I even try to answer their questions. So first, let's think about the consumer surplus. The consumer surplus is going to be the region above our new horizontal price. And below the demand curve. unit quantity. And so this area is the government, is the revenue to the government. So, S plus U is equal to tax revenue. Tax revenue. And then last but not least, what about the deadweight loss? Well remember, the dead weight loss is the difference between the original the total surplus. When we just let things naturally go to equilibrium. The difference between that and now our new total surplus, which is now lower because we have not allowed the market to function in a very natural way because of this tax on it.

ROUGE-1: 34.57, ROUGE-2: 33.61, ROUGE-L: 34.23
BERTScore: 73.66

==============================================
==================== [64/100] ====================
Summary:
Sir Gawain, nephew of King Arthur, was invited to a party at Camelot. A towering knight riding an emerald steed burst into the room and proposed a game. The Green Knight declared he would allow the bravest warrior present to attack him with his own axe. If they could strike him down, they would win his powerful weapon. However, the knight would be allowed to return that blow in one year and one day. Gawain tried to forget this bizarre vision, but despite the strangeness of the knight’s game, he was determined to act honorably.

ROUGE-1: 24.43, ROUGE-2: 21.68, ROUGE-L: 22.14
BERTScore: 69.18

==============================================
==================== [65/100] ====================
Summary:
well welcome back everybody to uh the last lecture 162. this is kind of a a special lecture um i did get some requests for more information about distributed storage and quantum computing and so i think we're going to do that. i want to make sure that we talk through the chord algorithm since that's a i think relatively simple thing to understand and is uh very cool and applied pretty much everywhere so if you remember one of the things we talked about last week was basically this cap theorem which was really a conjecture that eric brewer put forth back in the early 2000s. node except that in reality what happens is this gets distributed over a whole bunch of nodes and so the question is really many parts to this question one is how do we actually do that distributing another is when some client does a get how does it figure out which node to go through. So today i want to tell you about the cord uh algorithm which has been turned into storage systems of many sorts including those used by amazon et cetera okay facebook so um before we get there i wanted to remind you of this notion of recursive versus iterative lookups.  consistent hashing is a way to take your keys and figure out a clean way to distribute them throughout the system without having to know pretty much all of the nodes that are participating. The chord algorithm lets you get by with only knowing essentially a logarithmic number of nodes in the total system and you can still do this well so we're going to associate each one of those storage nodes is going to get a unique id okay and that unique id will be in the hash space. This is basically going to be a mechanism to divide our space up and we'll talk about that in the next slide. So imagine you take their i don't know their ip address and their owner and whatever you concatenate all those things together and you hash them and you get a single 256-bit id out of that now we're going to talk more about secure hashes a little bit later in the lecture. Every node has an id and it's going to be in this ring space this unit dimensional space from 0 to 2 to the m minus 1 where m is big okay and so let's just say there's a lot more slots on here than 64. Cord is a system that was developed uh with a group of researchers at mit and at berkeley. It's the simplest and cleanest algorithm for distributed storage that i have seen and it's a comparison point for all sorts of other algorithms. The way to think about this is we put a bunch of storage nodes on this ring and then we're going to decide where to store our key value pairs based on where the key is on the ring. After four to eight and fifteen is gonna store everything from nine to fifteen and twenty is gonna storage everything from sixteen to 20. Nodes are connected based on their hash names. Because of the hash being a randomizing function uh we've scrambled the geography of this ring. No particular part in the in the world here might be a hot spot it means unfortunately though that we don't have the most local of look up because if we start at node four it'd be nice if we could just go down to 15 and back okay now this is a really good question here about redundancy how do we get redundancy out of this for the moment. beachfront property in nevada and then has a plan to basically cause uh california to fall into the ocean and therefore have really expensive properties fortunately superman uh saves the day and it doesn't happen so um okay so if we move um forward with this by the way i'm showing you these clients now to make this a little more clear the clients need to know one gateway into the system in order to talk to the system okay so that's going to be part of the initial lookup. about this algorithm is all that the ring is going to do is it's going to figure out who is responsible for storing key 50. So just by asking the ring where key 50 belongs it now has some information about nodes that it can talk to. If you lose two nodes in a row then what i've just described to you is no longer going to work. There is a way to completely break the ring such that the stabilized procedure won't reconnect it. We have figured out how to make this stable so first of all as long as we have a fully connected ring. The power of the powerful thing about this is once i've got all these nodes now i can do a really fast routing process to figure out how to find which node is going to store the key i'm interested in. If you have log m where m is the number of nodes of the system you can end up with a situation where you can find data even if half of your nodes fail. So that's kind of what's proved in that chord paper and that's not that many because it's a logarithmic number. algorithm and so what's good about this is like i said you store the data in the cord ring and it it's very hard to destroy okay why are they called leaf sets that's a good question the reason they're called Leaf sets is because in some sense you can view the uh if you take any given um starting node like 58 and you view the set of fingers that're a tree and so eventually you get to the leaf set and so it's like a tree with leaves so that's where the leaf is coming from. Chord algorithm is sort of geographically distributed by nature. The randomness is helping us to avoid correlated failures where yeah we have a bunch of copies but they're all in the same machine room and the building got struck by lightning so that doesn't happen in a chord algorithm. The downside of course is performance might hurt if you happen to be too far away from a copy um and so i will tell you that there are subsequent versions of cord which uh when you're doing this routing and you have a lot of options here. what we did with the the tapestry lookup process back for ocean store okay so i did want to point out that what i've just described to you this chord ring is actually used in lots of uh cloud services these days. For instance dynamodb and i have a paper for that up on the reading from last time uses the chord rings and you can look down here but it uses them within their machine rooms as a way to distribute load. So basically you have a service guarantee that says we'll get a response within 300 milliseconds uh for say 99.9 percent of the requests. it adapts automatically which is pretty good okay so what i wanted to do next uh i'm going to talk a little bit about security and then um talk through a couple of things and then i want to uh try to get to quantum computing as well so we can i know there was some of you asked some questions about that so i'mgoing to leave this topic unless there's more questions okay. Security is kind of dealing with actions of a knowledgeable attacker who's really trying to cause harm and we want to make sure that uh they can't really screw us up okay. security policy built with our protection mechanisms okay so i wanted to point out something interesting i don't know if you've ever seen this before but here is a car in the ditch. Back in july of 2015 there's a team of researchers that took complete control of a cheap suv remotely exploited a firmware attack over the sprint cellular network. They basically caused the car to speed up and slow down and and veer off the road and uh totally wirelessly so this is a little scary.  cryptography is one of the central points of many of these mechanisms you just have to use it correctly. This is communication that's in the presence of adversaries uh it's been studied for thousands of years. If you have a good algorithm like aes it's not possible for an adversary to send a message that the receiver will treat as real because you have to have the secret key now one thing you do need to do in order to make this work is to use the right algorithm. i'm hoping that if you haven't taken 161 it's on your list because there's a very interesting set of things that people can talk about. single symmetric key encryption work which symmetric because the same secrets used at both sides is to prevent a adversary from holding on to an old message and sending it later is you have to start adding what are called nonces which are things like timestamps and so on so that every time you send this it's unique and if somebody sends an old version you can detect it. The idea of a secure hash function is one where you take data and you run it through a hash function and get a bunch of bits out of it. we can take a plain text something like a contract and we can run it through a hash function where we take that key and an append m and that's called a digest now we can send that across and the data. At the other side we can verify by re uh computing that hmac okay and if they match the one that was sent across versus the one you computed yourself then you can know that the message is not corrupted otherwise it's corrupted. Hashing is pretty powerful and i'm not going to have a lot of time to go through this with you that's a 161 topic. Data centric vision is one that i've adopted in uh my research group is one in which we think about shipping containers full of data so if you think about uh down the port of oakland you've probably all seen these shipping containers. This was a a great invention back in 1956 so before 1956 what happened was we had uh longshoremen who would take a bunch of things and they would go to a ship and they'd play tetris with it. Now i can ship something from my house in lafayette to beijing the outskirts of beijing just by calling the right trucks. a data capsule and inside the data capsule is a bunch of transactions that are hashed so remember those hashes we talked about and signed where we uh we use a private key to sign a hash over something and as a result we trust that this really came from the person who said it did because only they could have the private key. As a result of these data capsules this gives us a cryptographically secure way of moving data around to the edge to the cloud and back again in a way that nobody can fake out. that we know and the second thing is of course we can put arbitrary encryption on top of this as well to make it uh private so really the signatures are about integrity and who put the data there and uh the encryption would be about privacy. If you have a signature only at the end of a chain of data you can essentially check the rest of the chain by checking the hash pointers so these are all of the things you get out of a blockchain by the way for those of you that are familiar with bitcoin or whatever. The vision here really is of pretty much everybody using data capsules everywhere okay and if you can get that to happen then you could potentially have a very interesting scenario here. Part of what we're doing is we're working with roboticists and machine learning folks to put their data and their models for grasping and so on inside of data capsules and as a result they can reside securely in the edge in say your robots or whatever in a way that can't be breached okay. This is really targeted at secure edge infrastructure in addition to the cloud so these data capsules can move back and forth. unforgeable all right good so let me say a little bit about using quantum mechanics to compute. It's basically using weird but kind of useful properties of quantum mechanics two of them quantization and superposition. If you're willing to allow things to not be always a one or always a zero what you can do is you can just start doing quantum computing. There are many other algorithms out there now these days these days they've been slowly working on them but these are some pretty good ones that might be interesting. we've got google we've got ibm it's very popular these days with big companies uh microsoft is in here as well looking at building these quantum machines both of these two both google and ibm are super conducting bits so these parts of the machine you see here are normally put into a doer and they're running at four degrees kelvin or something really cold. This particular type of quantum computing technology is not going to be in your laptop at least not in any laptop i'd want to put on my lap but there are other technologies including ion traps that potentially are pretty interesting. are particles like protons or electrons have this intrinsic spin and so now i got one and zero or up and down okay. A representation called the heisenberg representation looks at this uh messy physical situation like this which is either a zero or a one in these brackets. That represents spin up and spin down okay or vice versa depending on how you want to whether what it's looking like if you're with the field then that's a lower energy so that's probably spin zero it's probably zero now. light travel in fact instantaneous travel of information from the earth out to that far planet einstein really didn't like this he called it spooky action at a distance okay but in fact uh what's interesting about this is you can prove that there's no actual information transferred okay so however we can use this to do what's called teleportation um which is take information uh at one side do some measurements send some data to the remote side and recreate the data recreate the quantum state at the other side. high probability some answer that was hard to find that's what we would like okay and so if you look here um you know if the two n inputs are equally probable there could be two to the n outputs that are equally likely. What we'd like is the probability of the outputs to be piled up high on the answer we want and it turns out that something like fourier transform does the trick okay so if we can do a fourier transforms on some input we can actually get an interesting output. With a quantum computer what we can do and unfortunately i guess i don't have time to do this because we're running out of time but i can set up a situation where my input to my algorithm is all the possible k's uh if i take a bunch of values and i compute uh the the value x to that value and i add them all together as a superposition and i do a fourier transform what i'll find is that x to the r congruent to one as i have r go through all its possible values. actually investigated if you were to build uh that factoring algorithm and you could do it as quantum circuits that could run on a quantum computer what would that look like. We actually investigated ways of optimizing that and we could actually look at performance of different options for the shortest factoring algorithms. So we built a cad tool to do that so i i don't know i think it's a pretty interesting area right now and there's a lot of interest in it all right so um sorry i kept you guys way over but this is the last lecture i figured if anybody was interested we talked about key value stores.

ROUGE-1: 29.39, ROUGE-2: 28.56, ROUGE-L: 28.38
BERTScore: 71.79

==============================================
==================== [66/100] ====================
Summary:
Early astrologists did not recognize the lymphatics so they thought there are only three system there is a branch of portal or this branch of puerto bean. Originally doctors thought that every corner these three things are present and they call it portal triad. But later on of course the new it is not portal Triad it is portalTriad and we call it plural area. Blood is moving from the periphery to the center into these hepatic classic lobules and of course you should not forget to mention that what is this moving from center to the periphery. you understanding me right these bile ducts from every corner and this is left hepatic duct and what is this right hepatics duct and they will come common hepaticduct. I will drain the I will draw the whole structure here so once forever it must be clear hopefully is a side diagram but it has it is a very important anatomical implication. What is this here yes what is the structure thank Ria's great now what really happens. Thank You attic juices is that clear let's have a break here.

ROUGE-1: 28.06, ROUGE-2: 26.61, ROUGE-L: 27.91
BERTScore: 64.45

==============================================
==================== [67/100] ====================
Summary:
so in the next portion of today's lecture we're going to talk about how we can modify the policy gradient uh calculation to reduce its variance. In this way actually obtain a version of thepolicy gradient that can be used as a practical reinforcement learning algorithm. The first trick that we'll start with is going to exploit a property that is always true in our universe which is causality causality says that the policy at time t prime can't affect the reward at another time step t if t is less than t prime. that is it's the rewards from now until the end of time which means that it refers to the rewards that you have yet to collect basically all the rewards except for the ones in the past or the reward to go. The causality trick that i described before you can always use it you'll use it in homework two it reduces your variance. There's another slightly more involved trick that we can use that also turns out to be very important to make policy gradients practical and it's something called a baseline. grad log p by r of tau we multiply by r  where b is the average reward this would cause policy gradients to align with our intuition this would make policyGradients increase the probability of trajectories that are better than average and decrease the probabilities of those that are worse than average. We can show that subtracting a constant b from your rewards in policy gradient will not actually change the gradient in expectation although it will change its variance meaning that for any b doing this trick will keep your grading estimator unbiased. The average reward turns out to not actually be the best baseline but it's actually pretty good. different policy parameters you'll have one value of the baseline for parameter one a different value for parameter two. In practice we often don't use the optimal variance we just uh sorry we typically just use the expected reward but if you want the optimal baseline this is how you would get it all right so to review what we've covered so far we talked about the high variance of policy gradients algorithms. We talked about how we can lower that variance by exploiting the fact that present actions don't affect past rewards and we can use baselines which are also unbiased.

ROUGE-1: 26.66, ROUGE-2: 25.64, ROUGE-L: 26.60
BERTScore: 68.06

==============================================
==================== [68/100] ====================
Summary:
Today we're gonna talk about learning in the setting of games. What does learning mean? How do we learn those evaluation functions that we talked about? And then, er, towards the end of the lecture, we wanna talk a little [NOISE] bit about variations of the game- the games we have talked about. So, uh, how about if you have- how about the cases where we have simultaneous games or non-zero-sum games. And an example of that is rock, paper, scissors. So can you still be optimal if you reveal your strategy? It's actually not the size that matters. It's the type of strategy that you play that matters, so just to give you an idea. So, so the question is more of a motivating thing. We'll talk about this in a lot of details towards the end of the class. But, like, the reason that we have put this I guess at, at the beginning of the lecture is intuitively when you think about this, you might say, "No. I'm not gonna tell you what my strategy is, right?" at the utility, er, so if you're an- at an end state, we are gonna get utility of S, right? Like if you get to the end of the game, we get the utility. And so that was the recurrence we started with, and, and we looked at games that were kind of large like the game of chess. So, so instead of the usual recurrence, we decided to add this D here, um, this D right here which is the depth that un- until which we are exploring. And then when depth is equal to 0, we just call an evaluation function. In the game of chase- che- and chess example is, you have this evaluation function that can depend on the number of pieces you have, the mobility of your pieces. Maybe the safety of your king, central control, all these various things that you might care about. So, so the hand- like you can actually hand-design these things and, and write down these weights about how much you care about these features.and figure out what is a good evaluation function. So to do that, I can write my evaluation function, eval of S, as, as this V as a function of state parameterized by weights Ws. In the first part of the lecture, we're going to look at backgammon. And then towards the end of the class, we will talk about simultaneous games and non-zero-sum games. And, and that kind of introduces to this, this, um, temporal difference learning which we're gonna discuss in a second. It's very similar to Q-learning. Okay. So let's think about an example and I'm going to focus on the linear classifier way of looking at this just for simplicity. can actually, like, roll two dice and based on the outcome of your dice, you move your pieces various, various amounts to, to various columns. Uh, there are a bunch of rules. So your goal is to get all your pieces off the board. But if you have only one piece and your opponent gets on top of you, they can push you to the bar and you have to start again. So, so what are some features that you think might be useful? Remember the learning lecture? How did we come up with feature templates? Yes. These indicator functions. You might ask number of O's on the bar that's equal to 1, fraction of Os that are removed. So, so we have a bunch of features. These features, kind of, explain what the sport looks like or how good this board is. And what we wanna do is we wanna figure out what, what are the weights that we should put for each one of these features and how much we should care about, uh, each one, these features. "We generate episodes and then from these episodes, we want to learn. These episodes look like state action reward states and then they keep going until we get a full episode" "The reward is going to be 0 throughout the episode until the very end of- end of the game. Until we end the episode and we might get some reward at that point or we might not" "We go over them to make things better and better. So that's, kind of, the key idea" So s, take an action, you get a reward. You go to some s prime from that and you have some prediction. Your prediction is your current, like, your current V function. And then we had a target that you're trying to get to. And my target, which is kind- kind of acts as a label, is going to be equal to my reward, the reward that I'm getting. So I'm gonna treat my target as just like a value, I'm not writing it as a function of w, okay? The TD learning algorithm is based on gradient descent. It is very similar to Q learning. There are very minor differences that you'll talk about actually at the end of this section, comparing it to Qlearning. So, so I wanna go over an example, it's kind of like a tedious example but I think it helps going over that and kind of seeing why it works. Especially in the case that the reward is just equal to 0 like throughout an episode. So it kinda feels funny to use this algorithm and make it work but it work. Is it possible to have, an end state and not end state have the same feature vector, or no? If you use like, uh, initialize rates do not be zeros which you update throughout instead of just to the end. If there were kind of the same and have same sort of characteristics, it's fine to have feature that gives the same value. If it is always 0, it doesn't matter like what the weight of that entry is. So in general, you wanna have features that are differentiating and you're using it in some way. to 0.25 and 0.75 then it kind of stays there, and you are happy. All right so, so this was just an example of TD learning but this is the update that you have kind of already seen. And then a lot of you have pointed out that this is, this is similar to Q-learning already, right? This is actually pretty similar to update, um, it's very similar, like we have these gradients, and, and the same weight that we have in Q- learning. The idea of learning in games is old. People have been using it. In the case of Backgammon, um, this was around '90s when Tesauro came up with, with an algorithm to solve the game. And then more recently we have been looking at the game of Go. So in 2016, we had AlphaGo, uh, which was using a lot of expert knowledge in addition to ideas from a Monte Carlo tree search and then, in 2017, weHad AlphaGo Zero, which wasn't using even expert knowledge. all, like, based on self-play. Uh, it was using dumb features, neural networks, and then, basically the main idea was using Monte Carlo tree search to try to solve this really challenging difficult problem. So, um, I think in this section we're gonna talk a little bit about AlphaGo Zero too. So if you're attending section I think that will be part of that story. All right so that was learning and, and games. Now, so the setting where we take our games to simultaneous games from turn-based. And then, theSetting where we go from zero-sum to non-zero-sum, okay? So, so we have player A and player B. We have these possible actions of showing 1 or 2. And then, we're gonna use this, this payoff matrix which, which represents A's utility. If A chooses action A and B chooses action B. So, so you can kind of see like a whole mix of strategies happening. And this is a game that you are gonna play and talk about it a bit and think about what would be a good strategy to use when you are solving this game. Someone tells me it's pi A and pi B, I can evaluate it. I can know how good pi A is, from the perspective of agent A. But with the challenge here is we are playing simultaneously, so we can't really use the minimax tree. So what should we do? So I'm going to assume we can play sequentially. So that's what I wanna do for now. So, so I'm gonna focus only on pure strategies. I will just consider a setting- very limited setting and see what happens. If we have pure strategies, all right, going second is better. What if we have mixed strategies? Are we gonna get the same thing? So, so that's the question we're trying to answer. If someone comes in and tells me, "This is a mixed strategy I'm gonna follow," I'll have a solution to that and that solution will be a pure strategy. So that's actually what's happening in this general case. So I'm actually gonna make a lot of generalizations in this lecture. If you are playing a mixed strategy, even if you reveal your best mixed strategy at the beginning, it doesn't matter if you're going first or second. If you're minimizing or are maxim- or maximum or min- minimum of that value, it's going to be the same thing. So this is called the von Neumann's theorem. For every simultaneous two-player zero-sum game, with a finite number of actions, the order of players don't matter. So no matter what your opponent does, like you're gonna get the best thing that you can do. just telling you what's the pure strategy you're using, right? So that was kind of the first point up there. And then if you's using mixed strategies, it turns out it doesn't matter if you're going first or second. You're telling them what your mixed- best mixed strategy is and they're going to respond based on that. Okay? All right. So next 10 minutes, I want to spend a little bit of time talking about non-zero-sum games. In real life, you're kind of somewhere in between that, and, and he wants to motivate that by an example.

ROUGE-1: 21.09, ROUGE-2: 20.34, ROUGE-L: 19.88
BERTScore: 68.13

==============================================
==================== [69/100] ====================
Summary:
The famous example was posed by Comte de Buffon back in the 18th century. It marks the beginning of a subject that is known as the subject of geometric probability. The problem is pretty simple. We take a needle that has a certain length-- l-- and we throw it at random on the plane. So the needle might fall this way, so that it doesn't cross any line, or it might. end up crossing one of the lines. If the needle is long enough, it might actually even end up crossed two lines. Theta is defined as the acute angle that the direction of the needle is making with the lines, so that theta will vary over a range from 0 to pi over 2. The needle makes two angles with the part of the line. It's this angle, and the complimentary one, which one do we take? Well, we use a convention that thetas are defined as angles that the needle makes with the parts of the lines. This angle is equal to 2 over l over 2, so this is also the angle here. is this 4 with this 2 give us a 2. We have 2l over pi d. And then the integral from 0 to pi over 2 of sine theta. Now the integral of sin theta is minus cosine theTA. And we need to evaluate this at 0 andpi over 2. This turns out to be equal to 1. And this is the final answer to the problem that we have been considering. And now, a curious thought. How can you figure out the number pi? Take your needle, throw it at random a million times, and count the frequency with which the needle ends up crossing the line. So it gives you a good estimate of this particular number. unit cube. So you throw points. Some fault inside. Some fall outside. You count the frequency with which the points happen to be inside your set. And as long as you're throwing the points uniformly over the cube, then the probability of your complicated set is going to be the volume of that set. You estimate the probability by counting the frequencies with which you get points in that set, and so, by using these observed frequencies, you can estimate the volume. It turns out that these days, physicists and many engineers use methods of this kind quite often and in many important applications.

ROUGE-1: 31.97, ROUGE-2: 30.20, ROUGE-L: 30.80
BERTScore: 66.97

==============================================
==================== [70/100] ====================
Summary:
Professor: If you have a perturbative treatment, even if you carry to infinite order, you have, formally, divergences if you have resonant interaction. Professor: I want to show you what are the tools to treat those infinity source diverGences in a consistent and a systematic way. MIT OpenCourseWare offers high quality educational resources for free. To make a donation or view additional materials from hundreds of MIT courses, visit MIT Open courseWare at ocw.mit.edu. means to sum up an infinite number of diagrams. And OK, we want to understand the time evolution of this system. Our tool is a time evolution operator. And at the end of the class on Monday, I told you, well, let's simplify things. Let's get rid of those temporal integrations and multiple integrals by simply doing a Fourier transform. And this iterative equation where we get the nth order by plugging the n minus first order on the right hand side. and we want to know the matrix element between eigenfunctions k and l. The other thing we need is that G0. Remember the Fourier transform-- and I gave you sort a mini derivation here-- is just 1 over energy minus H. So therefore, if you write now this equation as matrix elements, the first part, the G0, gives us 1 minus 1 over Z minus Ek. Since we are diagonal in H0, it's delta kl. And well, now you see the structure if you sum over intermediate states, or if you introduce intermediate states. easy part, which has no divergence, we can make any kind of approximation we want without altering the physics. But the resonant part, this needs special attention. We want to figure out which of those expressions include this problematic term exactly twice or three times or four times. So we regroup those infinite sums, these algebraic terms, in such a way that we say, OK, which one has the occurrence of this once, twice, three times, four times? Colin: Is the definition of the S-matrix or the matrix matrix of the Fourier transform? we ask what happens when we allow more appearances of the state b, for each of them, we obtain another square box. So by looking at the terms which are bothersome and regrouping the infinite terms according to one occurrence, two occurrence, three occurrence of this divergent denominator, we have now found an exact expression for Gb of Z. And since this is now an algebraic equation with a geometric series, we can write it exactly as this minus Rb ofZ. In the lecture, I want to show you what we have exactly done for treating an atom in the excited state and for treating light scattering. I just go now and apply to an excited atomic state. So the state we are interested in is the atomic state b and no photons. And the property of the state is obtained when we know the function Gb of Z. Then, the perturbative expansion involves no divergent terms and can be performed. So therefore, we are now in a position to make approximations to the function R. The real part is this matrix element squared, but double sum. But what we use is the principle part of it, which is well defined in the theory of complex functions. So the imaginary part gets us Fermi's golden rule. And the real part has actually-- remember when we discussed the AC Stark shift. The AC Stark Shift has a 1 over [INAUDIBLE] dependence. And you recognize that here. So this is actually nothing else than theAC Stark shift not due to a laser beam, but due to one photon per mode. Now creating an AC Stark shift. And this is mathematically the expression. And such AC Stark shifts which appear as self energies, as energy shifts created by the state, this is nothing else than the famous Lamb shift. So that's what we get out here. I have already-- do I in this sum? What we have here is we have this function R in the real and imaginary part, which depends on the energy E. But remember, we worked so hard with diagrams to make sure that the triangle-- first the square, and then the triangle, and this is what we calculate here-- has no resonant structure at the energy Eb. imaginary part, which we can approximate by Fermi's golden rule. If we now Fourier transform back and obtain the time evolution of this state, it no longer evolves with the energy Eb. It has a shifted energy by this self energy. But in addition, because of the imaginary part, it has now an exponential decay. So what you can immediately read form here is that exponential decay is a simple approximation. It works very well. But at very early times, it will break down. Because then, the energy dependence matters. infinite summation of diagrams, if we had done a perturbative expansion, we would have never obtained exponential decay. We would have obtained some polynominal decay. So it's not really profound what I'm saying. It's pretty much an exponential function is non-perturbative. Other questions? So let me wrap up this chapter. What we have discussed here is-- I haven't really discussed resonant scattering. I've now focused on the function Gb. I focused on what happens to the state b. The excited state has [INAUDIBLE] really comes from an infinite number of absorption, of emission and reabsorption processes. Until maybe 10, 15 years ago here at MIT, we were not teaching that. And I felt often in discussion with students that a little bit more of a complete picture behind atom photon processes is needed. If you're interested in mathematical rigor, the green book, Atom-Photon Interactions, is pretty rigorous and still very physical. But on the other hand, I think you should have sort of this picture behind it, what really happens. many modes. It emits photons and reabsorbs them. And you can often neglect that in the simple description of your experiment. But if you take certain expressions seriously, they would have divergences. And that's what we discussed without this infinite number of processes which happen. Of course, yes, the whole other regime which I should mention-- and this is when you can completely neglect the coupling to many modes. If you do Rabi oscillation with resonant interaction, you don't need all that. Because then, you're really looking at discrete states. A small system which I just described by Schrodinger's equation, now follows the density matrix equation, has relaxation, the entropy increases, and such. And I want to show you, first with a simple example, but next week in more generality, how this completely changes the nature of the description of the small system. And in this cause in part one, we've dealt with it and looked at vacuum Robi oscillation and a few really neat things. But what happens is that the system is an open quantum system. We know the initial state or our system. We propagate it exactly with the correct time evolution, we get everything. And then, we could reduce by doing a partial trace. We could reduce the description. What is now the probabilistic description for density matrix of the atom? But this is rather complicated. But in the end, we want to have a formulation which simply tells us, what is the atomic density matrix as a function of the initial atomic density Matrix? So in other words, all that happens with the environment-- that there's initial state, that it gets entangled, and then we neglect maybe not keeping track of the photons-- we're not interested in all of that. it causes stimulated emission. And it causes absorption described by the Einstein b coefficient. And you have a similar equation for the excited state. So this is clearly the semi-classical limit of what we want to accomplish. We want to know more. We really want to find the full quantum time evolution. We have to be careful. The time evolution as a Hamiltonian, if you now bring in the environment, cannot be simply included by adding an imaginary term. This here violates the unitary time Evolution. The environment cannot do everything for you. The environment can only do for you what can come out of all possible Hamiltonians. So what we need is we need a description of the quantum noise, which comes from coupling to the environment. The tool which we use for that is the density matrix. This will actually play a major role. We will make certain models for damping. And it's really beautiful. On Monday, I will give you the beam splitter model for the optical Bloch equation. I really like it. processes. OK, any last questions? Well then, let's enjoy the open house with incoming graduate students, and I'll see you on Monday. I'll be back on Monday to talk to you about the classes we'll be teaching next year. Back to Mail Online home. back to the page you came from. Click here to read the full transcript of this interview. Back To the pageyou came from, click here to see the full Transcript of this Interview. Back in the page, please share your questions and comments.

ROUGE-1: 30.53, ROUGE-2: 29.19, ROUGE-L: 28.44
BERTScore: 65.47

==============================================
==================== [71/100] ====================
Summary:
David KAISER: In the last few class sessions, we were looking at some changes in high energy particle theory. And then in our most recent class session, we looked at some of the shifts within the fields of study. And today, we're going to focus on a kind of example of that new subfield, a relatively new sub field that's known as inflationary cosmology or simply cosmic inflation. So it's a framework for trying to understand the evolution of our universe over a huge expanse of time, increasingly using tools at the interface. lecture notes on the Canvas site which go into a little bit more detail of some of these parts from the lecture. So oftentimes, astronomers will describe the most salient features of our universe in terms of what they call large scale structure. It turns out that ordinary gravity-- even Newtonian gravity, let alone Einstein's fancier version that we looked at in class, general theory of relativity-- that these gravitational frameworks are sufficient to help us make sense of this hierarchy of scales, of structure across large distance scales. for an infinite expanse of time. But other colleagues showed at least it was consistent with his own equations to have universes that would change over time, that could either expand or contract. That was actually a prediction made by some of these colleagues even before some empirical evidence began to come in starting in the late 1920s. Hubble found this remarkable trend that the further away from us a given galaxy was, the faster it tended to be moving away from me further still. So you can actually then work backwards and say for how long has our observable universe been stretching? When did this stretching or expanding phase begin? George Lemaitre was an ordained Catholic priest and an MIT trained PhD astrophysicist. He studied briefly in Cambridge, England with one of the first converge to general relativity, Arthur Eddington. He then came to MIT to finish his PhD and then was finding many of these solutions to Einstein's field equations even before Einstein did. And, in fact, Einstein came thinking he must be wrong, and then Lemaitres kept being right. But Einstein started off by always being frustrated that Lemaitr found solutions that Einstein found abhorrent or disgusting. After the Second World War, new groups began coming back to these somewhat old questions. Some of the newer groups had experience with things like the Manhattan Project and in general were much better versed in things like nuclear physics than had been known even in Lemaitre's day. One of the most active groups soon after the war was based at the advanced-- excuse me-- the Applied Physics Laboratory. Here's a famous composite photograph. They're making a not so subtle gesture to the fact that Gamow was widely rumored to enjoy his drink. So his head is emerging from the vapors of Cointreau, of a liqueur. At early times in cosmic history, the universe should've been opaque. You literally wouldn't have been able to see anything because the mean free path of any given photon would be very, very short. When the average or ambient temperature fell below the average binding energy of a single hydrogen atom, the average energy per photon or per elementary particle would fall. Only at that time, a new phase in the universe would begin to unfold. The universe would be filled with neutral atoms of hydrogen. Light can pass through electrically neutral matter. It does so in our own universe. energy continues to redshift. They lose energy as the universe continues to expand. So the energy of those photons would've started at the equivalent of around 10,000 degrees Kelvin and now today would be much, much,much lower than that. The universe has been expanding and draining that average energy per particle over time [CLEARS THROAT] so that today the universe should be filled with this remnant glow. This is all work that they predict around 1948, '49, '50, Gamov, Herman, and Alpher. Scientists predicted as early as 1948 that there should be this remnant glow from the Big Bang. They were using a new horn antenna sensitive to radial microwave and radial band frequencies. This should've been among the most precise instruments available on the planet for that band of the spectrum. But they found this remnant hum in their electronics, and they couldn't get rid of a residual hum. The group here, these folks are rediscovering many of the ideas from George Gamow and his group, actually at the time unaware that Gamow had even done these calculations. In the early universe, there was a huge plasma everywhere, not just in one corner, but everywhere. The photons were everywhere. And it's like sitting in a bathtub full of these photons. And they're just losing their energy as the overall size of space continues to grow. The Big Bang happened at x equals 0, and x equals 1, equals 2. Any place we could put spatial coordinates to on this model, those all experienced the Big Bang at the same time. But that's the reckoning that people like Lemaitre got comfortable with starting in the '20s and '30s. Astronomers are able to measure the speeds with which objects are moving away from us. What's not so easy is to figure out how far away that thing is right now, the actual distance right now. Hubble measured a much quicker average rate of expansion than what we have mostly settled on today. But the basic picture was there. The picture was enough to get a small number of people to pay attention to Georges Lemaitre's otherwise quite obscure mathematical solutions. But it was Hubble's data that got the biggest splash, that made the biggest impact at the time. whether we agree with the number he inferred of the actual rate, it seemed pretty clear to many people at the time that this was consistent with an actual overall expansion, with the change over time. That made these seemingly pure mathematical solutions like Friedmann's and especially the follow up work by Georges Lemaitre, that made those mathematical solutions look much, much more curious and interesting than they had prior to Hubble's data. So this is a, I think, safe to say, remarkably successful set of ideas that eventually becomes called the Big Bang model. space in between is stretching. So the distance between, say, the Milky Way and the Andromeda galaxy at any given time, the physical distance really would change. This is actually called conformal time. That might remind you of our beloved friends, the 19th century Cambridge Wranglers. We're really doing a Wrangler-ish thing here, very similar idea, to adopt coordinates for the time, for the rate at which we think clocks should tick to be convenient that also takes into account that changing stretching rates over time. An example in space time of a conformal mapping of a sort that we all use every day like a Mercator projection. When people begin using these convenient coordinates, they also go back to some questions about or features of the Big Bang model, and they start having new questions. Robert Dicke introduced this conundrum in 1969, so soon after the discovery of the cosmic microwave background radiation when people began to take the Big bang model more and more seriously, including Robert Dicske. stuff per volume, the actual density of matter and energy per volume. If omega is larger than 1, you have more stuff for volume. You expect it's open or hyperbolic geometry. So far so good. Then Dicke plugged this quantity into Einstein's own equations. A universe should generically become more and more different from flat over time. And so this solution that looks like the Goldilocks solution, the spatially flat solution where you have just the right amount of stuff per volume is actually an unstable solution. Bob Dicke introduced the next big real conundrum for the Big Bang model. He found that the universe started out being close to but not identically equal to flat at early times. But it should look nothing like spatially flat at later times, he says. He points out that you have these exponential fine tunings for the universe to be even remotely close to a flat or Euclidean-like behavior today, which is looking more and more consistent with observations by the '60s and'70s and '80s. We receive a remarkably uniform signal on the sky today from opposite sides. But those photons were emitted at a finite age when the universe was only a short portion of its current age. It was only 380,000 years old as opposed to nearly 14 billion years old. So the horizon distance was actually a factor of 100 shorter than the smoothness scale across which we receive remarkably uniform information. And how could that be? If this portion of the sky never had a chance to become in any kind of physical equilibrium or even exchange a single tweet? is uniform across every direction we look in the sky today, even though it's coming from regions that when that light was emitted couldn't have possibly had any physical interaction with each other. That becomes known as the horizon problem. So why on Earth would the CMB be so uniform today to this exponential accuracy from regions of sky that were never ever in causal contact? Plus, why on earth do we have this distribution of scales that I started off in the beginning? Where does large scale structure come from? the Big Bang model had some amazing successes but some pretty stubborn quandaries as well. So I'll pause there again and ask the questions about that. Any questions on the shortcomings of the Big Bang as people began articulating them throughout the '60s and '70s? Feel free to jump in or use the chat or either way. And again, there's more on the quantitative details of that in that optional primer you can find on the Canvas site. So Fisher asks, is it useful to think of the universe as spherical still? Yeah. These pictures get pretty hard. like circa 1980. He was not originally asking questions about the cosmos, but he was haphazardly encountering some of those questions, again, very much like Tony Zee around the same time. What Alan was interested in was in things like spontaneous symmetry breaking and the Higgs mechanism. If there were a time during which the matter that's filling the universe could be temporarily stuck in a metastable state in which it had some non-zero potential energy, but it couldn't release, that could have remarkable cosmological implications. Alan Steinhardt discovered exotic Higgs-like fields that can get twisted up in some topological shape. He was wondering what happens if the universe gets stuck even temporarily such that the matter that dominates it, fills it, can't release or relax its potential energy arbitrarily quickly. That's called a metastable state. If the energy density, the stuff per volume, remains constant, then very counterintuitively, you have a runaway growth in the size of space, says Paul Albrecht. Paul: Alan is unbelievably anal retentive and writes everything down. in a stretching space time, if you take that stretching of space seriously, then you don't even need to cook up those exotic Higgs-like potentials that Alan was first thinking about. Quite generically, you'll have a damped oscillator behavior. This comes from the fact that space itself is stretching. And that alone it turns out is enough to find these self-consistent solutions in which the field moves very slowly. You can imagine it rolling down this hill, rolling down-- sorry-- rolling down slowly as a function of time. The latest measurement from the Planck collaboration using a satellite is that this parameter in our actual universe today is 1 to better than a percent level accuracy. This is a remarkable shift even over the course of 20 to 25 years, let alone since the days of Lemaitre and Hubble. The horizon problem was originally phrased because we thought there was an origin to all of time, this Big Bang surface, at tau equals 0. So we're adding more real estate along our time axis. We're unfurling a little bit extra time that hadn't been taken into account. in the standard Big Bang model. So if you allow for more time before what you would call the Big Bang, you can continue tracing those past light cones further and further back and say there should be some time earlier than what we were starting from. So then it would at least be plausible there's at least now a causally self-consistent mechanism by means of which the universe could have similar conditions everywhere because they were causally connected at a time before we had previously taken into account. So therefore, you could have the horizon distance, the maximum causal distance, becomes much larger than the smoothness scale that we measure. distance and conformal time. You can see that as you trace backwards from today, instead of going back to saying the universe at early times should've been on the order of 1 meter, you say at those early times, the universe was actually exponentially tinier than you had thought. It grew exponentially quickly to map onto where we see today. And so the universe could very easily have been in a kind of equilibrium or at least a causally self-connected state. And this became clear to people about a year or so after Alan and Paul and Andy and all those folks began writing the first papers on inflation. moment, that field would be subject to slight, slight quantum fluctuations in the distribution of energy across space. That starts to yield this tiny little fluctuation in why there's slightly more matter and energy in this region of space than the other one. So now those very tiny quantum scale fluctuations get stretched as the whole universe stretches. As the scale factor grows exponentially, you have the average length between the distance between crests of those tiny wiggles get stretched to galactic and even super galactic scales all within that blink of an eye. ground based measurements as well. And each of these came out 10 years apart with an increase of about a factor of 30 in the angular resolution of the sky. So now not only do we know do we live in a universe that is indistinguishable from flat as inflation suggests we should, but the actual pattern of those wiggles, the pattern of the very slight early unevenness in the sky matches predictions to, again, better than a percent level accuracy. I find that astonishing. I was a senior in college when the first of these released their data in September 1992. Inflation says similar kinds of things should've been happening in the earliest moments everywhere in space through this very violent, rapid stretching of space. Inflation arises from types of matter in interactions that we now know exist that are heart and soul of this particle cosmology community. And it addresses several of these long standing conundra about the standard Big Bang model. And the simplest models fit to unbelievable accuracy despite what my mean dormmates used to say in the mid '90s. Now we have extraordinary agreement with many, many of these predictions, albeit not the final one. why the universe is so messy is actually because Alan's been generating the mess in his own office, and it's expanded to cosmic scales. So if you want to study that part of today's lecture, it's probably the most important lesson, you'll ever take away. And I'll be glad to stay a bit longer if people have questions. Again, I'm sorry for running late. Feel free to drop off if you need. Any questions on that? The photos in Alan's office are on Canvas.

ROUGE-1: 31.52, ROUGE-2: 30.28, ROUGE-L: 30.21
BERTScore: 64.57

==============================================
==================== [72/100] ====================
Summary:
The Peloponnesian War was fought in 431 BC. After the war, Spartan power had grown to an unprecedented degree. For the first time there were lots of Spartans, who had lots of money. The Spartans had choices that they could take. They could either stay in the Pelop onnesus, or they could contest it in their power to control the entire Greek world in the east. Or they could have some control of the Aegean and the Hellespont. This elevated his influence and power, everybody wondered at him and so on. On the other hand, it presented a problem, because you can imagine how that went down among the aristocrats of Sparta. So, there was jealousy and resentment and fear at Sparta that something bad was going to happen to the Spartan way of life. Pausanias and his tradionalists bided their time for the opportunity to put a spike into this development. There were other things that were flowing from what I've already described that were threatening the traditional character of Spartan life. Sparta had been reduced to total defeat by the Spartans in the Peloponnesian War. Athens, which had been the greatest empire that the Greeks had ever seen, was also at the mercy of the Spartans. The Athenians feared that the same fate they had visited upon some states that had defied them would happen to them. The Spartans didn't do that. Instead, with Lysander very much in charge of the settlement that was going to be imposed on the Athenians, they placed in a small group of oligarchic Athenians. the great rhetorician and sophist Gorgias and he was also in the circle of Socrates, along with Plato and Xenophon and various other bright young men of the upper classes in Athens. Critias, in any case, was determined that Athens would not have a democracy. In fact, it looks like he was very much taken with the virtues of Sparta because Sparta had won the war. So, it's a phenomenon that is not amazing, even if this looks like the earliest example, I think, that we know. The Thirty ruled between September of 404 and May of 403, just a matter of months as it turned out. They established a council of 500--well, that's the same number as the Athenian council, but it was quite different. It was made up of extreme oligarchs; they were given judicial powers. Men who were identified as sycophants, and the Greek use of that term, was widely unpopular. They were very unpopular and the Thirty began with an act that was not only unpopular, but they would also put to death their political opponents. So, it was bloody from the first, but only the first. Pausanias says Thrasybulus was the first grave of Pericles. How could it be that these fellows who lived centuries afterwards said these things about Thrasy Bulus and we have never heard of him? He is the only Greek I know whose name fits a Yale fight song--Thrasybulu, ThrasyBulus. But we at last, and you have an obligation to future generations, must not let the name of Thrasybullus lie in obscurity again.

ROUGE-1: 10.34, ROUGE-2: 9.35, ROUGE-L: 9.37
BERTScore: 58.22

==============================================
==================== [73/100] ====================
Summary:
The coherent state has a simple definition, simple but subtle. It's an eigenstate of the annihilation operator, and it has a complex eigenvalue alpha. The person who popularized those states was Glauber, and he got amply rewarded for that. Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free. To make a donation or view additional materials from hundreds of MIT courses, visit MIT Open courseWare at ocw.mit.edu. today is that the coherent state is a complex number. The coherent state has a time evolution. It moves in a circle. And this is really the phasor of the electric field associated with it. So coherent state, the alpha value is directly related to an electric field. That's why this state is closely related to the classical limit of the electromagnetic field. We are now using the coherent states to look at any other quantum state of the. electromagnetic field, any statistical operator which describes photons by. forming the diagram matrix element of the statistical operator with alpha. In quantum mechanics, you cannot measure x and p simultaneously. The three different ways are Q, P, and W. Q of alpha is a real probability, it's always positive. The other guys, P of alpha, can be positive or negative. And also, W of alpha can be negative or positive. All three have their advantages and disadvantages. So they all have pluses and minuses. But the reason why I picked for the course Q ofalpha is that it's a real. probability, and this has to be positive. the coherent state is now not this Gaussian. It doesn't have thisGaussian distribution as a course of probability. It's what you want-- what maybe some of you wanted to see-- oh, by the way, it's a delta function. The probability of the coherent state alpha has a delta. function peak at alpha, which is sort of nice. And the number state is not a ring of a finite radius. You would naively expect the energy is sharp. The square root of the energy's electric field, shouldn't it be sharp? And indeed, it issharp. Professor: "If you want to really appreciate the quantum character, you have to know the classic description first" "So the fluctuations of the intensity are usually expressed by the second order temporal coherence function" "Quantum mechanically, we will see that the g2 function is not necessarily larger than 1, it can be smaller than 1" "It's only possible if you have a truly non-classical state. That's actually an interesting-- you can see-- litmus test for the quantumness" number state, the number of photons is an eigenvalue. Therefore, n squared average is n average squared. The Fano factor is minus 1. Sub-Poissonian distribution. And the g2 function, which classically cannot go below 1, is now n minus 1 over n. It is smaller than 1. And you see immediately that the biggest violation for g2 is to go to minus 1 for the case of a single photon state. Any questions? AUDIENCE: Shouldn't it be 0 for [INAUDIBLE]?? PROFESSOR: The g2function? No, if you put in-- wait. Gosh. I'll double-check. me first address one misconception. You can say, Well, let's just use a coherent state. Coherent states, as I've just shown you, are very classical. They've always a g2 function of 1. And attenuation is not changing it. Attenuation is preserving that. So you can now say that you take your coherent state and you attenuate it down that there is only one photon left. Is that a single photon state? The answer is no. It is an attenuated incoherent state. The creation of single photons has been sort of a small cottage industry over the last 10 or 15 years. Single photons are often needed for protocols in quantum computation. One of the leaders in this field is Professor Vladan Vuletic here at MIT. The equation of single photon is essential for studying non-classical light, but also since it's a very active frontier of our field. It's a little bit a way that we cannot control the bullets which are fired. But we can control the guns and we make sure that each gun can emit exactly one bullet. it's about-- it involves single atoms. But it's a little bit more demanding like that. So you want to take 1 atom home or 1 ion. But the problem is if the atom or ion emits a light, it can emit the light into all directions. And therefore, you have a single photon afterwards. But you have many, many different spacial modes. And in any given mode, it will not have a. single photon. So therefore, what you have to add to the single atom or single ion is-- you. have to put it in a cavity. The Hanbury Brown Twiss experiment was the first experiment which really looked at g2 functions correlations. It was the beginning of quantum optics and modern experiments with light. The classical limit is always a limit of high intensity, so at any given time, you have a ton of photons. If you put a light bulb into a cavity or couple the light from a light bulbs into a fiber, the light becomes spatially a single mode. That's the only way how you can distinguish a lightbulb from a laser beam. course practically, if you take a thermal source and filter it down to a single mode, you will be left with only a few photons. You cannot create an intense enough single mode light source unless you use stimulated emission. And that's a laser. So anyway, what Colin says is there are actually more different light sources that just the laser and the thermal light source. There are LEDs or semiconductor devices, which provide photons with interesting statistical properties. OK. We have to stop. I'll see you on Wednesday.

ROUGE-1: 18.12, ROUGE-2: 17.41, ROUGE-L: 15.90
BERTScore: 66.75

==============================================
==================== [74/100] ====================
Summary:
This lesson will first dive into some signal Theory and then move on into things that we're more familiar with things like deconvolutions and using Transformers for next note prediction. The first thing we want to talk about is how can we sample and quantize a continuous time signal. We then go into some geometric signal Theory with Transformers and finally how how we can kind of generate sounds using these. The next thing we're going to kind oftalk about is changing forms. We'll talk about the SARS ADC algorithm and how it's used to convert digital signals to analog. voice and Pitch it up very fast right what quantization level do we do we want there. Can we do a lossy pitch up with a uh with by filling in the the blanks in some intelligent way through prediction or kind of note fitting which is an interesting consideration I think given the fact that audio is a continuous time signal um the digitization process and the choices you make matter a lot and because of that this field is so interesting and there's a lot of really didactic work around how we can take these continuous signals. also increases we're at one Hertz you're almost perfectly fitting the polynomial on the the points. aliasing is the byproduct of poor sampling right so a lower wave resolution will result in a modified output signal as compared to the original input that we're trying to process. Different frequencies approximate our input wave differently right this wave you wouldn't really think is indicative of uh the original signal that you're tryingto process. This is an example of where aliasing would be present at the Nyquist trade so exactly at um double the highest frequency you can see we have an approximation. actual wave is the byproduct of a lot of of aliasing right where we have a curve that is not representative of our our sample at all because we're sampling at a rate that is uh is not adhering to our our aliasing uh the law. frequencies that are higher than one half of the sampling rate will be automatically transformed to lower frequency sees that's where information loss stems from. There's a lots of literature about um aliasing effects including spatial illnessing right um so I'd highly recommend checking out some of these links. um this is is kind of uh and and add add-on um to this presentation it uh doesn't really contribute exactly to what we're talking about but I thought this was incredibly cool um with one line in C you're able to to generate Melodies um which is incredibly coolUm yeah so please check it out if you if you guys have a chance uh the next thing we're going to talk about briefly is geometric signal Theory. Um and specifically you know what a projection is how it can be used to reconstruct signals and finally how that can tie in to reconstructing signals. familiar with perhaps in terms of how we can use those to reconstruct signals and ultimately how we Can use Those to generate audio right predict the best uh kind of next node um so looking at the next the next step here we want to use deep learning for reconstruction right where we are are reconstructing a low quality audio to high resolution audio right um and this is this is the kind of uh model framework um that we can used for this um you might notice it really closely resembles a unit which is something that we talked about during image segmentation. Transformers can help increase our sample of training data and generalize key scales and beats throughout a data set. A single song can be transformed into 12 songs of different Keys. The more data you have the better your model will be and the more generalizability you have in your Transformer the better it'll perform. Transformers will far outperform classical methods of of both computer vision and natural language processing. It's easier for machines to predict keys without flats and Sharps which has you know similar to what humans do. hidden state memory Transformer memory enables very fast inference for music prediction right we've done a lot of things to optimize for for our prediction we're including a beat embedding so that's not something it has to learn. We're able to get a sense of of relative position with Transformer XL whereas vanilla Transformers will use Absolution absolute position only. It's important for music models to know the position of each token relative to one another because positionality matters right the order that you're playing the notes really is is what matters the most and this is an additional to our positional beat encoding. things a try yourself uh yeah thank you guys for tuning in have a good one. Things you might want to try yourself. Things that you may want to give a try. uh yeah. things you might be interested in trying yourself. things that you might like to give yourself. uhYeah. things a try yourselves. uh Yeah. Things a try themselves. Things your might like yourself.things you may like to do yourself. thanks you guys. for tuning into this week's episode of The Daily Discussion.

ROUGE-1: 20.41, ROUGE-2: 18.74, ROUGE-L: 17.98
BERTScore: 65.53

==============================================
==================== [75/100] ====================
Summary:
The amygdala is closely connected to the basal forebrain. In discussions of aging in human pathologies, you always hear about the basalForebrain. It's because of the degeneration of acetylcholine containing neurons in the basal nucleus. Includes a collection of structures, including the olfactory tubercle right at the base. The other, broader, term is ventral striatum, which, in recent research, is proved to be more ventral medial striatum. The medial part of more dorsal striatum is really part of the limbicstriatum. Schizophrenics have larger brain ventricles, an indication of early damage. Many studies have indicated the hippocampus is important, as is the amygdala. Schizophrenics that are the hardest to treat often are in mental hospitals for most of their adult life. This just shows schizophrenian monozygotic twins, where you have one twin with schizophrenia and the other doesn't have it. And as we know, that larger ventricle is a result of early brain damage. And you can see it changes with age. that we know if you bind the receptor you'll reduce the effects of the [? rejections. ?] If the prefrontal cortex is functioning abnormally because of sprouting of these axons, also the basal forebrain, then binding to the receptors will move it more towards the normal. Just saying that it does [INAUDIBLE].. Let's talk about the other part of the basal ganglia, the larger part, the corpus striatum. Going back to these earlier pictures where I talked about the evolution, remember very early there was no dorsal Striatum. It was more of an olfactory structure. to the VA of the thalamus, which then projects to motor cortex. The one with the colliculus goes directly from the dorsal striatum to the nigra. Those are the two different paths. And then you need to know what we mean by doubled inhibition. Important for understanding the pathologyies when something goes wrong with these structures, as in Parkinson's disease, Huntington's chorea, and other basal ganglia disorders. So to control it you need some way to reduce-- either replace the misconnections or reduce the overexcitation.

ROUGE-1: 12.17, ROUGE-2: 11.29, ROUGE-L: 11.57
BERTScore: 65.39

==============================================
==================== [76/100] ====================
Summary:
Learn how the solar cell device converts sunlight to some usable output energy. Learn how to minimize the amount of light reflected or not absorbed inside of a solar cell. Learn about the physical underpinnings and the implementations of four to five advanced methods of reducing optical losses. Use the weekly Newsquiz to test your knowledge of stories you saw on MIT OpenCourseWare, and help students understand today's featured news stories. Back to the page you came from. Click here to return to the Beginners' page. of your solar cell device. We want to, obviously, maximize this part right here. So to begin, we give a quick review of light, the nature of light. It will be useful alternatively to think about light as a particle, quant of light,. or to thinkabout light asa wave, depending on what light management technique we're going to be describing. The visible photon wavelengths are usually in the hundreds of nanometers. And the solar spectrum peaks somewhere around 550, just good numbers to have in mind. You can have, for example, a decreasing depth of penetration of the light with increasing energy, whereas with x-rays, it's the exact opposite. So just to situate ourselves, I know we have a fair number physicists and chemists in the room. That's a message geared toward them. Let's describe how light interacts with matter. And first off, come up with a few variables. Define a few units that will make it easier for us to understand how light is interacting with Matter. which is describing the reflectance from air to a solid, in this case, from air where the refractive index is 1. The amount reflected can be described by this equation right here. It could also be thought of very loosely as the ability of an electromagnetic wave coming into material to slosh those electrons around. So I would advise taking this analogy as far as it will go until it breaks down. And you'll see at some point it actually does, but it's a useful place to start. If I add a coating, for instance, to a window that increases the reflectivity, then the amount of light that is able to escape from the inside to my eyes decreases. With normal incident light, there is a beautiful symmetry involved. So just the same way that I'm losing the ability to see inside, the folks inside are also losing the able to see out. If I'm outside on a sunny day, how much brighter is it outside versus inside right here? Factor of? 100, maybe? We need to understand how light gets absorbed inside of matter. We apply a very simple formulation inside of this class, which is called Beer-Lambert's Law. This is a very specific form of the reflectance from an air into a solid. As we increase the thickness of the material, we will plot the total transmitted light as measured by that photodiode. And so I'm going to come up with a hypothesis. What we're going to be doing, and Joe will explain this a minute, is taking many sheets of material. of what's going to happen. I'm going to say that if we double the thickness of the polyethylene that we're going to halve the amount of light going through. And let's see if the hypothesis is correct. It's not. And it's a logical thing you might assume. And then we'll walk through a derivation that will correct our missed logic. So go ahead, Joe. Take it away. JOE: Sure, so if you guys want to play along, that's fine too. to curve down. Exponential. It looks like one at least. And we can test whether or not the hypothesis is correct by an exponential fit, which happens to match pretty well. So if we assume that light is coming in a medium and light is decaying in some function to that medium and a certain amount of light is transmitted, we know, of course, from our little experiment that it follows some exponential function. But how do we justify that to ourselves? Well, first off, we're going to ignore reflections off the front surface. by some sort of scattering intensity within the medium-- and this sigma here can refer to a variety of processes. The alpha, on the other hand, is not a geometric parameter. It's an intrinsic material parameter. And the beauty of this formalism right here is that we can measure, experimentally just like we did right there, our alphas for materials. And so from an engineering point of view, we don't really-- to first order, it doesn't really matter whatsort of scattering or absorption process is happening inside of a material. We just need to know the alpha. states within the material, that light, depends on the energy of the light, depending on the frequency. That general equation is the same one that drives the reduction of light intensity as it travels through the atmosphere. Oftentimes we're operating in a wavelength regime of light wherein free charge is excited. But we can also keep increasing that the wavelength of light, say, out to 10 microns, very long wavelength light, very low energy light. And that can excite free carriers inside the material. We're going to look at two different materials, silicon and gallium arsenide. And we're Going to calculate the thickness necessary to absorb 90% of the incoming light at 550 nanometers. Did anybody manage to walk all the way through that calculation? TONIO BUONASSISI: I'm glad people are asking those questions. And then over the next few classes, we'reGoing to get to exactly what physical processes are going on. But I'm starting to get into the semester, so the energy level starts going down. Most solar cells that you see of crystalline silicon are on the order of 100 microns. The record efficiency of gallium arsenide solar cell is a few hundred nanometers thick. If you absorb 90% of the light on the first pass, you'll absorb 99% of it on two bounces, right? Or in one bounce, rather, and two trips, two optical path links through the material. We're just focusing on the beam that gets reflected off, instead of just going back out toward the sun. If you have a 10% reflectivity on the surface, you went from a 10%. reflectivity over here to a 1%. So texturization increases the probability that light will enter the device. One of the reasons why you see this white spacing, the white colored material in the solar cells is that the light doesn't get reflected off, between the cells. Instead of changing the back skin, what other might you change? The front, right? You might change the nature of the anti-reflection coating on the front, but the back may still look black. the backsides of solar cell devices. We wouldn't want necessarily specular reflectance. We might want to maximize the amount of light reflected off at particular angles. And there is, of course, research being done to figure out how to make light do that. And you don't only have to texture your back skin. You can also texture the bus bars. The bus bars are these little metal wires right here that are collecting the charge from each of the solar cells. And they're connecting essentially the front side of one cell to the backside of the next. There is a limit to how much we can trap light simply by modifying or corrugating the surfaces. A gentleman by the name of Eli Yablonovitch, who's now a professor in Berkeley calculated these parameters I think back in 1982. He came up with an upper limit to the optical path length. That's a pretty good litmus test for the ability of a material to trap light. If you have silicon, for instance, with a refractive index of, let's say, in the infrared some around 3.6, your Yabonovitch limit is around 50. In most solar cells, we want to suppress reflection. We go to great lengths to make sure that this thickness as well as the refractive index of the material is optimized for a particular system. So calculate for me what is the optimal thickness of an anti-reflection coating of silicon nitride? And we'll give it a refractiveIndex of, say, 2.1. Let's call it 2, just make our lives simple. And the peak of the solar spectrum is 550 nanometers. expressions that we just walked through. It's really important to understand the fundamentals behind any simulation software because you will get out of it what you put into it. You will not be able to pick up on obvious things that you might of-- for example, double clicked on this little material here and find the real component of the refractive index completely wrong. And so it's important that you understand what we've presented today. We want to ensure good light trapping inside of the absorber as well. Light management is necessary devices. Light trapping can still matter for thick devices, though. Once the thickness of your device starts approaching the optical absorption, then it really begins to matter in the absorption length. Snell's Law assumes that there's no shift of light as it transfers from one medium to another. If you introduce a constant gradient throughout the phase of the light, you can start doing some fun things with it. If we can eliminate the longer wavelength stuff out here, which is heat, performance of most solar cell suffers when they get hot. surface of a material, let's say right here, then you can cause each node, each point within your material, to lag by an increasing amount, so that your wave front now bends. And that will cause the light, essentially, if you trace through the points of maximum intensity, say the pink, you'll see that the light is bent. And so it's really exciting. There's stuff coming up every day on light trapping and light management. Mostly it's for photonic devices. But they can be transferred over into solar cells as well.

ROUGE-1: 23.68, ROUGE-2: 22.22, ROUGE-L: 21.93
BERTScore: 61.12

==============================================
==================== [77/100] ====================
Summary:
Professor Steven Smith: I want to look at two sets of issues. One is Locke's theory of the constitutional state, particularly focusing on the role of the executive, vis-a-vis the legislative branch of government. The other is thinking about Locke and the American regime and the current state of political philosophy, modern contemporary American political philosophy. Smith: Locke doesn't endorse necessarily one particular form of government from any other. He is an advocate of what we have come to call limited government, of constitutional government. LZ Granderson: John Locke gave the modern constitutional state its definitive form of expression. He says Locke's doctrine of consent and legislative supremacy should make him a hero to Democrats, to radical Democrats. LZ: Locke's conception of natural law, rights, government by consent, the right to revolution and all are all part of the cornerstone of our founding. Lz: A judgment on America is very much a judgment on the philosophy of Locke, if anyone is to be considered to be America's philosopher-king. John Rawls wrote a book in 1973 called A Theory of Justice. In many ways, Rawls' book was an attempt to update the liberal theory of the state. He invokes the idea of a state of nature, an original condition, as he calls it, a theory of rights. For Locke, rights derived from a Theory of self-ownership, according to his view, you will remember, everybody has a property in his or her own person. But both it seems to me go on to differ profoundly about the source of rights and the role that government has in securing conditions of justice. A return to Locke such as it is, even if such a return were possible, is by no means a panacea to what ails us. Locke's effort to build a kind of modern republican government on the low but solid foundations of self-interest and self-ownership could not help but generate its own forms of dissatisfaction. America, as a former teacher of mine once said, is the land where the many facets, the many faces of modernity are working themselves out. We are but a moment in the kind of comprehensive self-dissatisfaction that is modernity.

ROUGE-1: 12.27, ROUGE-2: 11.42, ROUGE-L: 11.55
BERTScore: 64.17

==============================================
==================== [78/100] ====================
Summary:
JACK HARE: Let's do a little recap on electron cyclotron emission, and then we will go on to a few other things. We didn't actually derive the emissivity of it, but we gave ourselves a hand-wavy reason why there may be multiple peaks here. And we can say this is m equals 1, m equals 2,m equals 3. And they're evenly spaced. And this is just for a single particle. And so, if you see some emission at some certain frequency, then you know it's been emitted by a region of plasma which has this magnetic field. different because the magnetic field is dropping off nice and monotonically throughout our device here. So we said that, for example, we might have three different regions, which we could label positions R1, R2, and R3. And each of these then have different magnetic fields, B1, B2 and B3. Each of these magnetic fields will then produce a spectra of lines at these different harmonics here. And at each ofThese points, if we measure the intensity, we then know straight away what the temperature is corresponding to the black body spectrum. temperature as a function of frequency. Then we can say, well, that frequency corresponds to a certain magnetic field. And then we can says that magnetic field corresponds to an spatial coordinate. This is a technique which will give us, by looking at the spectrum for lowish frequencies, the first or maybe the second harmonics of this cyclotron emission. We can work out what the temperature is as afunction of space. So this is why this is a powerful and popular diagnostic in MTF. want to understand in plasmas so that we can build an economically viable fusion reactor. Now, the fact that the noise is too high does seem like a big limitation. But there are some clever tricks that we play where we use correlations. And I'll talk now about what exactly these correlations are and how they provide us with information that allows us to get a signal out despite the overwhelming [INAUDIBLE]. So our setup here is borrowed from ASDEX Upgrade. And, in effect, I referred to Alex Creely's PhD thesis, which you can find online if you want more information. The size of our turbulence of R turb, is on the order of 100 microns. That's the width of a human hair. So we are trying to measure turbulent eddies inside a tokamak on this scale. And so, that means, we're not trying to cover this entire frequency range. We're zoomed in on only quite a small frequency range, which is 10 gigahertz. This is the sort of signal that we can actually digitize now. And that is quite an affordable digitizer compared to the ones you would need. Many oscillations. And so, it will average out to 1 or something like that. So it's some DC offset that we can subtract off after. So all this is doing here is mixing the signal down so it gets to a regime that we could effectively digitize. Yeah, another question. Anyone know how micro splitters work? Looking at you? STUDENT: RF. JACK HARE: The answer I got is that you can actually collect from a very small region on the order of 100 microns. was RF, which I don't think is much more satisfying than my answer. So, yeah, there are circuits which will split microwaves. What they were actually doing with this system is all of these were very tunable. And so, if they wanted to look at turbulence in the edge or turbulence near the center, they could actually tune all their bandpass filters to do that. But this is getting way beyond what I was hoping to talk about on this. So let's get on to correlation ECE see what that does. This technique is incredibly powerful because it's enabled people to measure, again, delta Te upon Te on the order of 1%. And they've done this at 13-- on ASDEX Upgrade-- 13 radial locations. So on very small scales, very fast time scales, we can now measure temperature fluctuations in tokamaks. And this is something that has revolutionized our understanding of and can finally characterize it. That was a lot. Any questions? We're going to move on from there. we have positioned these two volumes, which are producing frequencies omega 1 and omega 2. And we think that that distance is smaller than the size of our turbulent eddy. The temperature should be the same going up and down. And if you do this correlation and you get out nothing, that probably means your volumes are too far apart because there is-- this T correlation would just go T1 T2. And there's no good reason to believe those temperature fluctuations are correlated, because they'll be part of a different turbulent Eddy. situation where, at some point, your eddy has moved across, and these two are no longer correlated because the next two are correlated. So it-- yeah. I think we're reaching the limits of my knowledge. JACK HARE: I mean, there's no reason to believe that each of the fluctuations is the same size. So this is going to be a time-evolving system here.Yeah, exactly. Cool. Any other questions? Anything online? Now we're going to go to bremsstrahlung. here with the ions and electrons as point particles. There's a semi-classical approach where you start bringing in some quantum physics and treat, I think, the electron as a wave. And then there's a full quantum approach. What's remarkable about all of these approaches is they all give the same answer with just a very slightly different coefficient. So we get a small change in coefficient. The scalings are the same. So in some sense, although it's important to get the exact coefficient, it doesn't matter exactly which one of these techniques you use. This, therefore, has units of watts per hertz per meter cubed. And these constants are things like e, the electron charge, and the electron mass, and epsilon 0, and h bar, and c, all arranged in some way to make all the dimensions work. And so, this emissivity, as a function of photon energy, is a very simple function because it just decays exponentially with photon energy. So that is the spectrum coming out of a plasma that's emitting bremsstrahlung radiation here. at all frequencies, like a black body kind of spectrum here. That goes down to very low frequencies and goes up to very high frequencies. We're going to talk about lots of other effects which produce emissivity which is higher than the bremsstrahlung. So, I guess, you always need to worry about this, even if you managed to clear out all the impurities, you don't have any electron cyclotron emission. This is still going to be there. at synchrotron, yes. So cyclotron is when the particles are spiraling around the magnetic field. Synchrotrons are used as a source of X-rays for diagnosing many other things. So it's interesting in its own right, but I don't know whether people use it as a diagnostic. It depends mostly on the magnetic fields and the radius in a tokamak. But maybe there's a really clever diagnostic you can do, like fast particles or something like that. That entire acceleration yields one photon, one energy. And, indeed, it's going to have to pay back any debt it has in some other way. And you can do that because we're going to switch to a slightly quantum model of the atom. And the energies of these levels are given by this unit, Ry, which is the Rydberg z squared of our ion over n squared. It is not the density anymore. We're dealing still with the single-atom picture, one electron, one ion. fulfills this equation, which involves these discrete energy levels. And that equation is going to be the photon that is emitted. So that just has our familiar bremsstrahlung type coefficients, electron density times the ion density times z squared e to the 1/2. But now we have an additional term, which is a new Gaunt factor for level n. But, don't worry, these Gaunt factors are all about 1 again, so it doesn't really matter that much. And then, to get the total j, you have j brems plus the sum over. The spectrum, therefore, looks like we had something like this for bremsstrahlung before, and now we have a spectrum that contains a series of edges, like this. And this lowest edge here-- I should draw this as a straight line-- this is n equals 1, 2, 3, 4, and all the way back up here, where, again, this is our brems result. This is our recombination. So if you're asking, what happens if you can't fulfill that, it just doesn't happen. Quantum mechanically, that would be a forbidden transition. related to the overlap integral between those with a dipole operator, the dipole operators being the thing that emits the photon in between them. So, yeah, you're right, there are inverse processes for all of these that will affect the spectra that you get. One use of this is a diagnostic called bolometry. It cares not at all about the detailed spectrum of what the emission is. It just wants to know how much power is being radiated by the plasma. We'll go into some ways of actually making some use out of all this stuff in a moment. This is balanced by conduction losses and what we, at the time, we assumed were bremsstrahlung losses, but in general could be any sort of radiative losses here. And so, bolometry is focused on measuring the total power that's being emitted. So we take our beautiful spectrum like this, and we integrate over all of this. We lose all of the detail. But, of course, the detail matters when we're doing the integral because you can see the recombination increases the amount of emission. So there'll be more emission here. divider just drawn in a complicated way. The reason why this is interesting is that, in general, M is going to change with T. We're going to have some resistivity as a function of temperature. And so, we can have-- yeah. Have I broken this? Yeah. It was right in my notes and I decided to innovate-- it's a terrible mistake. Never do that. Thank you. right, that looks better. Right, that look better. Good. Bolometry was one of the first diagnostics that we had on many MCF devices. Now, this itself is a very simplistic system, and it does not work because it is very susceptible to noise. So we have to, as always, come up with a clever system, which is noise resistant. And then, using heterodyning techniques, we can then measure it very cleanly without noise. This may be an experimental technique you're familiar with with the head of a bolometer. This is often made out of gold. Gold is chosen because it absorbs all wavelengths relatively evenly. This thin layer of gold has been deposited on top of a substrate. And it's on the back of the substrate that you have your resistor. And depending on whether it is M or R, you either have this open to the plasma or you have a thick block some distance in front of it so it can't see the plasma. It's the heat capacity of the absorber, c, that goes into our previous equation. difference between those. neutron damage leads us to use much thicker substrates, which gives us a longer time response and so, therefore, a worse bolometer. So, ironically, the bolometers that will be used on [INAUDIBLE] are significantly worse than the bolometer used on existing devices. And I'm sure that [? Spark ?] will have exactly the same problem. Almost everyone in the world uses this design of bolometer that they pioneered on ASDEX Upgrade in the '80s. No one has come up with a better system yet.

ROUGE-1: 30.75, ROUGE-2: 29.73, ROUGE-L: 30.07
BERTScore: 68.29

==============================================
==================== [79/100] ====================
Summary:
So, last time we were starting to talk about the sort of the general overview of what reinforcement learning involves. We introduced the notion [NOISE] of a model, a value, and a policy. Can anybody remember off the top of their head what a model was in the context of reinforcement learning? So what we're gonna do today is, sort of, um, build up for Markov Processes, up to Markov Decision Processes. And this build, I think, is sort of a nice one because it allows one to think about what happens in the cases where you might not have control over the world. different actions or we could just think of those actions as being a_1 or a_2, where it's trying to act in the world. So in this case, the transition dynamics looks like this, which says that, for example, the way you could read this, is you could say, well, the probability that I start in a particular state s_1, um, and then, I can transition to the next state on the next time step is 0.4. So, let's say that your initial starting state is S four, and then you could say, well, I can write that as a one-hot vector. I multiply it by my probability. And that gives me some probability distribution over the next states that I might be in and the world will sample one of those. So, for example, if we were looking at state s_1, it has a 0.6 chance to abstain and 0.4 chance of transitioning. So in this case, we have similar dynamics from s_4. state s_3. Probability of 0.4 going to state s_4 or a probability of0.2 of staying in the same place. So, it's like the world you know what the dynamics, the dynamics is of the world and then nature is gonna pick one of those outcomes. And so now what is a Markov reward process? Again, we don't have actions yet just like before. But now we also have a reward function. And we're going to be interested in episodes because later we're gonna be thinking about rewards over those episodes. rewards for the Markov Decision Process can either be a function of the state, the state in action, or state action next state. Right now we're still in Markov Reward Processes so there's no action. So, in this case, the ways you could define rewards would either be over the immediate state or state and nextState. Once we start to think about there being rewards, we can then think about returns and expected returns. If the process is deterministic, these two things will be identical. But in general if the process are stochastic, they will be different. General case, we are gonna be interested in these stochastic decision processes which means averages will be different than particularly runs. So, for an example of that well, let me first just talk about discount factor and then I'll give an example. Discount factors are a little bit tricky. They're both sort of somewhat motivated and somewhat used for mathematical convenience. We'll see later one of the benefits of mathematic, uh, benefits of discount factors mathematically is that we can be sure that the value function sort of expected discounted sum of returns is bounded. The value function of a Markov Reward Process is simply the immediate value of a reward from the current state. So, in this case you know what might happen in this scenario we start off in s_4. And then on time-step s_7 we get a reward of 10. But that has to be weighed down by the discount factor which here is 1/2. And so the sample return for this particular episode is just 1.25. And of course we could define this for any particular, um, episode and these episodes generally might go through different states. So, if we're in a finite state MRP we can express this using matrix notation. So, we can say that the value function which is a vector is equal to the reward plus gamma times the transition model times V. If one of the transitions can be back to itself, um wouldn't it be become a circular to try to express V(s) in terms of V(S)? So, that's the analytic way for computing this. The other way is to use dynamic programming. Markov Decision Processes are the same as the Markov Reward Process except for now we have actions. So, the agent is in a state they take an action, they get immediate reward, and then they transition to the next state. Now, if we think about our Mars Rover MDP. You can think about these things as the agent trying to move left or right but it's also perhaps easier just to think about in general them as sort of these deterministic actions for this particular example. And the reward can either be a function of the immediate state, the state and action to the state action and next state for most of the rest of today we'll be using that. before we do this let's think about how many policies there might be. So there are seven discrete states. In this case it's the locations that the robot. There are two actions. I won't call them left and right, I'm just going to call them a_1 and a_2. Then the question is how many deterministic policies are there and is the optimal policy for MDP always unique? So kind of right we just take like one minute or say one or two minutes feel free to talk to a neighbor. that quantity for each state. But the strange thing is that we're not gonna follow the old policy from then onwards. We are going to follow this new policy for all time. So, it should be at least a little unclear that this is a good thing to do [LAUGHTER]. Should be like, okay, so you're saying that if I were to take this one different action and then follow my old policy, then I know that my value would be better than before. But what you really want is that this new Policy is just better overall. strict inequality if the old policy was suboptimal. So, why does this work? So, it works for the following reasons. Let's go ahead and just like walk through the proof briefly. Okay. This is- what we've said here is that, um, V^pi_i(s), that's our old value of our policy. Has to be less than or equal to max a of Q#pi. Is equal to R(s, pi_i+1(s) The next questions that might come up is so we know we're gonna get this monotonic improvement, um, so the questions would be if the policy doesn't change, can it ever change again? And is there a maximum number of iterations of policy iteration? So, what do I mean by iterations? Here iterations is i.pi_i+1 value is by definition at least as good as the previous value function. So, why don't we take like a minute and just think about this maybe talk to somebody around you that you haven't met before and just see what they think. In policy iteration, the idea is you always have a policy, that is- that you know the value of it for the infinite horizon. Value iteration is an alternative approach. It says you always know what the optimal value in policy is, but only if you're gonna get to act for say k time steps. So how does value iteration work? The algorithm can be summarized as follows. You start off, you caninitialize your value function to zero for all states. And then you loop until you converge, um, or if you's doing a finite horizon, which we might not have time to get to today. on sort of the contraction operator. So, if an operator is a contraction it means that if you apply it to two different things, you can think of these as value functions. The distance between them shrinks after, um, or at least is no bigger after you apply the operator compared to their distance before. So how do we prove this? Um, we prove it- for interest of time I'll show you the proof. Again, I'm happy to go through it,. um, I- or we can go throughIt in office hours et cetera.

ROUGE-1: 19.15, ROUGE-2: 18.61, ROUGE-L: 18.56
BERTScore: 65.62

==============================================
==================== [80/100] ====================
Summary:
 angular momentum is a set of operators that provide observables, things we can measure. We showed that any component of angular momentum, be it lx, ly, or lz, commutes with l squared. It's a general theorem that two operators that commute, you can find simultaneous eigenstates of those two operators. We found that they satisfy a series of commutators in which lx with ly gave ih bar lz. We figured out by looking at this differential equation that if we wanted single valued wave functions-- wave functions would be the same at phi. Professor: If you have a system you want to figure out what are the properties of the states. In general, you will be led in any physical problem to look for the maximal set of commuting operators. We'll work back to the Schrodinger equation to finally obtain the relevant differential equation we have to solve. Then we'll look at the hydrogen atom and this task why? Having a proton and an electron we can reduce this system to as if we had one particle in a central potential. definition solves the differential equation star. This takes a little work to check. I will not check it, nor the notes will check it. It's probably something you can find the calculation in some books. But it's not all that important. The important thing to note here is the following. That this provides solutions for absolute value of m less or equal to l. And therefore m in between l and minus l. There's no great honor in finding zero solution of this equation. These are no solutions. The spherical harmonicas are going to be those wave functions. And they have a normalization, n l m, an exponention, and all that. So let me write, just for the record, what a y l m looks like with all the constants. The only one I really remember is that y 0 0 is a constant. It's 1 over 4 pi. That's simple enough. No dependents. Here is another one. y1 plus minus 1 is minus plus square root of 3 over 8 pi e to the plus minus i phi sine theta.

ROUGE-1: 31.54, ROUGE-2: 30.29, ROUGE-L: 29.50
BERTScore: 68.78

==============================================
==================== [81/100] ====================
Summary:
In order to do that, I basically have to do the integral. So here it is. We have psi of x and t. It's integral dk phi of k e to the ikx minus omega of kt. If you want to see the distortion, you have to keep that [INAUDIBLE]. We'll do that in a week from now. And then, you probably need to think a second. And you say, look. There's lots of things making it look like a difficult integral, but it's not as difficult as it looks. absolute value of a pure phase is that. psi at x minus d omega dk k0 t comma 0. The new norm of the wave at any time t looks like the wave looked at time equals 0 but just displaced a distance. If at time, psi had a peak when x is equal to zero, it will have a peak at time. And that corresponds to x equals to d omegadk times t, showing again that the wave has moved to the right by d Omega dk times T.

ROUGE-1: 30.58, ROUGE-2: 29.29, ROUGE-L: 30.58
BERTScore: 66.16

==============================================
==================== [82/100] ====================
Summary:
There are three common uses of a rotation matrix. The first is to represent an orientation. The second is to change the frame of reference of a vector. And the third is to rotate a vector or frame. To demonstrate these, I will use these three coordinate frames, representing the same space with different orientations. To help you visualize these frames in 3 dimensions, I'll use my handy tinkertoy frame. In the next video, we will learn how to represent the angular velocity of a frame.

ROUGE-1: 23.67, ROUGE-2: 23.19, ROUGE-L: 23.67
BERTScore: 67.50

==============================================
==================== [83/100] ====================
Summary:
Coded imaging is a co-design between how you capture the image and how you process the image. The concept of a position or superposition applies to all three types, shadows- or refraction- or reflection-based techniques. We'll see how-- we already have some projects that are inspired by biological vision. And I believe Santiago-- where's Santiago? Oh, yeah, his triangle-- the piston, kind of-- so some really great ideas. It is going to be very popular. When you take a blurry photo, a lot of the high-frequency details are lost. If you try to apply some deblurring, you'll get a result that looks like this. The culprit here is really this box function, which is equivalent to opening the shutter, opening the-- release your shutter button-- opening it open for exposure duration and closing it. So what if you change that? What if you changed that? And instead of keeping the shutter open for the entire duration, you open and close it in a carefully chosen binary sequence. It's actually preserving all frequencies in the image. frequencies-- they're all preserved. Of course, they're attenuated. It's not as high as-- it's not 1.0.0, it's reduced. Maybe it's 0.1 or so. So there is still some hope to recover this photo back from this because, in the denominator, we will not have seen. This is a very simple idea. Instead of keeping the shutter open for the entire duration and getting a well-exposed photo, the shutter is open for only half of the time. And that's your 1010 inquiry. CNN's Ravi Agrawal is a senior producer at CNN.com. He is the founder and CEO of a company that uses artificial intelligence to improve camera technology. He says the technology can be used to blur images based on distance and speed of objects. He shows how the technology could be used in a mobile phone app to help people figure out license plate numbers of cars and cars on the street. The company is based in New York and has been around since the 1970s. of that object moves in a straight line, OK. It doesn't matter which direction and what speed. So the problem here really is the point spread function or the blurred function is very critical. And this is what we want to study about half of the class. And the concept is very interesting because light is linear. So eventually, it's very linear. What happens to a point happens to the rest of the object. So that's the same concept here. You just want to call leading the world, take a picture, and see how it works. to engineer activity of the camera. So in this particular case, a point that was moving created a blur like this. And by engineering the time point spread function, it stops looking a bit like that. And then it just turns out that this one is easier to deal with than this one. So this is very counterintuitive because you would say, let me just build the best lens and the best exposure time. And so that kind of mimics the human eye. But when it comes to actually extracting information from that scene, it turns out you need to strategically modify how the camera works. Coded imaging is to come up with clever mechanisms so that we can capture light. The circuit is very, very simple. You just take the hot shoe of the flash, and it triggers. When you lose the shutter, it triggers the circuit. And then you just cycle through the code that you care about. What can we do for defocus blur that is for motion blur? What can you do fordefocus blur? We, again, want to engineer the point spread function. system. It took almost two years to realize that to put this coded aperture in a camera, there are only a few places where you can put it to get good results. So out of that came this particular experiment. When you focus with this, it works in very interesting ways. It has to deal with chromatic aberration, geometric aberrations, such as radial distortion, and so on. So the multiple lenses are moving every time I move this. And they're moving because they're guided through these groups. But there's one particular location that does not change in this. When you think about a visual camera, you make this very simplistic assumption. That is a pinhole, and there's a sensor. When you put a lens, we assume that the center of the lens is the central projection. The center of production of this lens is very carefully designed by camera makers to be the same plane where you put your aperture. When it's in sharp focus, you just see the point. But let's say that your autofocus here. What will you see? You will see the same exact [INAUDIBLE].. In 1D, the Fourier transform of a 52-length vector is broadband. It has energy at all the frequency. In 2D, it's more distributed instead of just all being near the center. If I just take a square aperture, a traditional one, and a square transform, it will look something like this. So a Fouriertransform of 7 by 7 will have a peak in the middle. But the truly Fourier transforms will have the rest of the peak. the values will be constant. So if we're placing a broadband code, certainly we have an opportunity to recover all the information. It's much easier to think about convolution and deconvolution in frequency domain than in primal domain. The bokehs are-- it depends on your-- I mean, for your average consumer, I don't know whether this matters. But you're right. If you're looking at something that's-- we have bright lights in the scene, take our false photo. They will all look like this. motion case, we had to know how much the motion is. The size of the blur is dependent on what? AUDIENCE: Belt. RAMESH RASKAR: The belt. But not just depth-- depth from the plane of focus, right? So that's an extra parameter you would estimate somehow. Maybe you can use a rangefinder or something like that, or just a software. There are methods you can employ. We said, OK, let me try to refocus. When it comes into sharp focus, my edges, that must be the right depth. I said, by the time I come tomorrow morning, I'll find a really good code. And I came back next morning. Nothing had happened. I waited all day. It was still running. And it never came out of that. So 2 the 52 is pretty challenging. But even if you use a cluster, it's still a pretty big number. So you can do some approximation. So [? harder ?] [? mark ?] code, which we learned about a few weeks ago or so-called broadband codes, they all have polynomial solutions. filter to the beginning of the signal. This particular filter is actually not circular, but it's linear. So when you apply the filter here, when you start applying the filter at the end of the image, you don't go back to the front. It turns out, for circular convolution, the match is very clean and beautiful and smoother course work. Or for linear convolution,. there is no good mechanism. So we came up with our own code called RAT code, R-A-T, which is after three quarters. In astronomy, you have circular convolution because they use either two mirror tiles and one sensor or one mirror tile and two sensors. If you tile aperture, you'll get really horrible frequency response, unfortunately, because if you put two tiles, that means certain frequencies are lost. But that's because our eyes are not very good at thinking about what the original image could be, given either this one or the previous one. So given this, I can challenge you that you're not able to predict that it has all this structure, right? Ramesh Raskar: Coded imaging is elegant and beautiful and sometimes complicated. He says there are many ways of engineering the point spread function. RaskAR: For any continuous code, there is a corresponding binary code that will do an equally good job. "It's just one of those things. It's like we are sick of it, so we don't want to do it, but I think it's worth trying," he says of orthogonal motion blur. smart people who invented this. It's very sad Because that part is done. So they just wanted the technology. And it's in a lot of cameras. There's another company called Tessera, which has a very similar solution. But within this region, the thickness will be a bonus. So you can either think of it as adding small matchsticks on top of the main lens-- or the way they do it is they actually put one single sheet that looks like that, an additional layer of support, a face mask. the name. The solution is very similar. I'm sure they're fighting out in court right now. Same solution. Instead of putting this particular guy, that's just going to add some extra glass, but mostly in a minor form. It's just [INAUDIBLE] on that one. So basically the same solution but creating different focal length for different [? partners. ?] AUDIENCE: Yeah. Although you said, I mean, there's this portion there, where if you have another blur [INAudIBLE],, right? RAMESH RASKAR: Right. blur is only about 10 pixels, no matter where you [INAUDIBLE]. So maybe that was the matter. If you have a point of access, it's still going to create an image that's blurred 10 pixels. This is, again, very counterintuitive, where you go to make the image intentionally blurred. It's just that it's blurred everywhere. And then we also saw this one very early on, where the point spread function-- typically when something goes in and out of focus, it looks like a point. Some scientists have been able to reconstruct three-dimensional neuronal structures. The z-dimension of a microscope is now down to about 10 nanometers. A single-pixel camera was listed as one of the big things in 2005 by Technology Review. The idea is, instead of taking one single photo, what you're going to do is to take a single photodetector and aim it at a set of micrometers. It's a very cool idea, by the way, as a scientist, I really like it. Ramesh Raskar: Compressive sensing or compressed imaging comes up. He says it's taking the picture with a hardware and compressing the software. RaskAR: The theory of compressive sensing is that, using some basis, I can transform the image and measure in [? your ?] measurements. The claim is that by using this understanding that my image can be represented in some transform basis-- in this case, Fourier basis-- using very few coefficients, can be exploited while I'm sensing. a very, very active field. The secret of success for film, of film photography, is that if somebody had given you this problem before the invention of film, that there is a scene-- and I want to give you a sensation of the same scene-- time shifted or space shifted. If I can see the scene and understand it, I can just present that as is and let it go through. And this is how we have been treating photography all this time. It's a record of visual experience, which is great for humans, but not so great for computers. The debate about whether it's really better or not is photography? RAMESH RASKAR: Tomography, yeah. When you have to recover the sky, you want to take as few measurements as possible. So compressive sensing allows you to take less measurements. But the problem is you need to actually have more information about the scene before you take the measurement, which is another measurement. That's a different problem for the dual photography code for the camera. It's a very different thing. which is how to write a paper and wishlist for photography. Which isHow to Write a Paper and Wishlist for Photography: How to Write A Paper and Write A Wishlist For The Camera. For more information on writing a paper or wishlist, go to: http://www.cnn.com/2013/01/30/photography/how-to-write-a-paper-and-wishlist-for-photography-how- to- Write-A-Paper-And-Wishlist.html.

ROUGE-1: 26.37, ROUGE-2: 24.28, ROUGE-L: 24.64
BERTScore: 65.40

==============================================
==================== [84/100] ====================
Summary:
Prof: You know why I am dressed up? When I do this course and when I do the first half of the French course I do a lecture on the bourgeoisie, the middle classes. Middle class was a form of self-identity that was constructed in the way being a worker was constructed, or being a noble. When you look at me dressed like this, please try to think, knowing me a little bit as you do, why it was that it meant a lot to dress like this in the nineteenth century. of class identity for ordinary people, for working people, the bourgeoisie had as strong a sense of self-identity as any social class you could imagine. It was, as I'll make the point in a minute, difficult to get into that class if you weren't born into it. The fear of falling out of it was something that helps motivate lots of political things in the nineteenth century. Once we've got an increase in the wealth of the middle classes, then you wanted the political power. You wanted access to information through the press and print culture. There's always this tension between families who needed children's income, however small that was. The Society for the Protection of Cruelty to Animals is one of the classic examples of bourgeois voluntary associations doing good things. For all the bad press that the middle class has had, there is also this good side that should be evoked as well. In terms of organized religion, the middleclass goes to church more than ordinary people, than workers, for sure. In the case of peasants it depends on where. Religion was a fundamental part of the British middle class's view of itself. The percentage of people who went to church could be exaggerated. In France after the Paris Commune of 1871 they start building churches in the working class districts perched on the edge of cities. Religion for the middle classes has a greater role in their lives than in working class cities. In the case of the peasants, there weren't any peasants left in England. The first real censuses do not come until the nineteenth century almost everywhere. In Paris, between 17 and 19 percent of the population in the first half of the nineteenth century would have been considered bourgeois. In Britain the percentage is higher. The further east you get, the smaller the middle class gets. At the very top--think of Zurich--are the great bourgeoisie, the big bourgeoisie. These are people who financiers, big wholesale merchants who are making bundles of shipping things from here to there. You won't find them yet, but they will become more important for perfectly obvious reasons. There's a revolution in France in 1830, yet another one that you can read about. Arguably--Marx says this and in a way it's sort of true--what it does is it brings to power in France the big bourgeoisie. Louis-Philippe's view of himself was that he was the Citizen King. That's what he calls himself. He's still the king. He was not any bourgeois. But in the official paintings of him you see people dressed like me who are coming into the throne room. They have power. The petty bourgeoisie had a self-identity. They shared things together. Schoolteachers were a way of social mobility for peasant families. Master artisans own the tools that their journeymen work with. They rent or own their shops. When things are going pretty well they do pretty well themselves. But when things aren't going well, they don't do well. That's why they're on the barricades all these all these these these barricades. It's really pathetic. Can you imagine going to a conference like the World Congress of the Petty Bourgeoisie? "Hi, my name is Albert." times, as you know, in the French Revolution--;the French revolutions, and in the revolutions of 1848. These folks are here, too. People are always dumping all over them needlessly. I will give you some example. If you've ever read the great French novelist--;he was paid by the word, but Balzac is really the novelist of the bourgeoisie. When he describes Paris and the seventeen to nineteen percent of the population who are increasingly living in the western part of Paris, he describes it as a jungle. In Europe 1816-17--don't write this down, if you do, you're compulsive--I'm compulsive. 1826-27,1840-41, really bad one, 1846-47,1855, those are the really bad years. If you don't get credit, that's what's going on now, here. But yet lots of people get up and the ranks of the middle class increases everywhere in the nineteenth century, in Russia, too, everywhere. That's simply the case. man doing? He's counting his money. That's a very nineteenth-century profession, as it is for every subject. Look at our guy on the left. He's dreaming about being this guy. He'll work very hard and he's educated. He had probably not secondary education. Most people didn't go to high school, secondary, lycée in France or gymnasium. They still had arranged marriages. Love could count for something, but marriages were still essentially, less so for the middle classes than for ordinary people. Where you live in a building reflected how much money you had. The more you go up there, you're still within the middle class. You've got an artist up here with not much money, but he still has a little bit of furniture, not much, his nosey neighbor looking at his painting. On the top you've got the poorest of them all, besides the cat who's on the roof up there. The piano replaces the harpsichord.in Germany, et cetera, etcetera. It represents this world. The notion of childhood, childhood didn't exist for ordinary people. Nobles did not send their children to public schools or even to private schools. The whole salon, the idea of going to art shows starts in the seventeenth century. The middle class wants to be seen rather like the Dutch. They wait in line in line to go to theatres. This is all Daumauau, the piece that you're obliged to swallow after dinner. Here's the little girl being trotted out to play a few notes. birthday, papa." You didn't take time out to celebrate a birthday if you were an ordinary person having to get to the fields at 4:00 in the morning in the summer, or going to work during the day. There's a whole notion, and here again this would probably fit rather awkwardly into the birth control description, but there's this whole sense of being prepared that emerges with the middle class. The bourgeoisie, the middle classes, and this is particularly true of Germany and France, and of England, too. until you have universal male suffrage, by how much taxes you paid and how much property you own. Property reflects one's belief in one's own social worth. No longer was it the worth of blood. So, they formed these national guards, particularly after revolutions and after 1848, or after 1830. For a while they go march around. But these are mainly there to protect them against the workers. Should one day all of these people try to rise up, climb up this ladder, you'll be down there to stomp on their fingers or to shoot them down. Transnonain, where this happened in the center of Paris, simply disappeared. It didn't quite disappear from the collective memory of people thinking about Parisian things. In conclusion, the middle classes extremely vary. They have a common material culture. They share a belief in achieved status, as measured by the amount of property that you had. They want a collective voice in decisions. Have a good weekend. See you on Monday. Back to Mail Online home. back to the page you came from.

ROUGE-1: 28.73, ROUGE-2: 27.06, ROUGE-L: 27.03
BERTScore: 62.33

==============================================
==================== [85/100] ====================
Summary:
Researchers have confirmed a second smaller space Rock smashed into the sea off the coast of West Africa creating a large crater during the same era. Scientists say it would have caused a tsunami at least 800 M High to tear across the Atlantic Ocean. The asteroid that's believed to have wiped them out 66 million years ago was not the only one researchers have confirmed. The discovery is exciting that it happens to be potentially close to the same time as the chicku event known to be the the main cause of the extinction event that killed the dinosaurs.

ROUGE-1: 38.76, ROUGE-2: 35.80, ROUGE-L: 31.78
BERTScore: 63.39

==============================================
==================== [86/100] ====================
Summary:
In automotive design Dynamics plays a very important part because it's not rigid body Dynamics it's a bunch of rigid bodies with springs the Springs are called starts with an S suspens suspensions right and there's a trade-off between how comfortable the ride is and how tightly the car handles. In Dynamics there two sides to it one is here's the system what is its trajectory going to be in other words how will its various degrees of freedom behave over time if you you know stretch it and let it go and it goes twang. Dynamics end of the course we're going to solve the dynamical equations dou4 is controls where you actually try and put in extra things like cruise control so to make the uh system behave in a way you want it to behave. I should also warn you I'm a little woozy today uh I had some serious drugs this morning prescription drugs and the result is that I might I think Sam might be a little bit of a bit of an idiot today. I'm sorry Sam. was saying if I might start babbling but you won't notice because I Babble anyway right so all right Sam didn't say that I said that I bet but anyway s was very respectful okay so uh in the last class we did uh uh we did a problem essentially the whole Pro class was we looked at you know the skier situation and there's a handout AJ's published right on the web where we do the energy formulation and we solve the problem. The answer is the same both ways so today we're going to kind of flow the accelerator a little bit we've been slow and steady. There's no class next Monday and um we posted the solution to that problem from class uh just one last thing I won't have officers today only because you don't want to hear me Babel I'm really sick um but I'm also going to change my officers um several people suggest so the timing isn't right um so we'll talk about the end of class but I I might go to like a Monday off M um like later on a Monday or maybe Wednesday later or something like that. point right so let us say I could have done it around o but here's the deal when you do angular momenta it's very useful to pick some point that's convenient you'll see this a lot right if I'm trying to calculate the angular momentum of that door I prefer to do it about the axis for various reasons right forces vanish all sorts of cool stuff happens so I need to tell you which point I just want to to pick a random point and do it that that way so let's say it's Q so what's the angularmomentum of particle of Point p and by the way let's assume the mass is m What's the angle of momentum quick say it h lvm yeah but now let's do it in Vector form that's correct by theway how would you define it you can say it I you know yep R well R is a vector should angle momentum be a vector very good excellent it is this. it will be useful just take it from me for the time being I need the a here because there's an a here. If I use a different velocity to you know it's going to be a different angle of momentum. I'm going to write torque on a particle with a tow because Little T looks like time so I write as a tow. I need to use the chain Rule and yeah nothing special right everything is completely reasonable according to the laws of math and physics that we've learned so far. This is a really badly designed classroom in the sense that there's not enough room but it's a huge classroom so I'm going to write it big right and I'm not that neat so you're going to have to bear with me so this is a velocity of Point P minus a Velocity of Point Q cross product M velocity P. I'm just going to put a dotted line so you know that that's what this is is this I don't think you'll disagree let me just write some draw some lines I'm trying to save space as I said. There is this term and you need to know about it okay it just so happens the term vanishes in many situations but it's key that you know see for Force f is equal to D by DT of P momentum completely coer Crystal Clear when you come to angular momentum it's say artifice. 99% 99% of all engineering graduates who take a car class in Dynamics Berkeley Stanford Princeton might not even see this. The reason this will become important is that when you look at rigid bodies it actually you will end up doing something so this is a silly term. A lot of MIT grads old friends work all over the world very important positions around the world for example the foreign minister of the new British government is an MIT grad okay milbrand Benjamin Netanyahu who might end up being the next president of Israel MIT grad kofan MIT grad right but one of my friends Works in a congressional he she helps Congressional analyze things from a physics point of view so you know when Katrina occurred someone asked if it would be possible to change the temperature in the when a hurricane approaches to dissipate the temperature. call this Theta the initial length is L one and the initial velocity is we'll call it a scalar because this is how I'm defining the problem of V1 when we actually solve it we might have to define a vector okay. As the thing's going around this person is going to pull the string down and as it kind of goes around it's going to spiral in and end up it's a new length L2 and the question is what is V2 going to be okay. is so where does force get applied on this the string so the string is applying a tension force on this right are there any other forces gravity is not an issue here. linear momentum conservation is a vector equation which is the vector linear momentum is conserved right but if if it's not conserved in One Direction but there's no force in the other it's still conserve in the Other Direction. So let's so is angular momentum conserved let's think about it is there any talk on this particle yep go ahead well. There is no torque on this particle at any point in time it's just a very long way to say listen things going in circles and the only force is radial if we take a cross product it's going to vanish. In a lot of situations that pesky correction term doesn't make a difference you just need to be careful that it exists. The angular momentum is conserved because this goes to zero which means d by DT of ah p q is equal Z and what that tells you is understood. would have to do one of two things I would have had to either calculate this term or calculate or make you know make my frame attach it to the truck. I've just done it in a very precise way okay so in the end there's no surprise the whole point is to show you they could be surprises but be careful any questions about this all right snap quiz in the next 3 minutes I want you to calculate for me the final velocity literally 3 minutes because I have toDo the dumbbell problem. I do all the math the angular momentum as I get get it is a vector l^2 Theta 1 do s right so initial moment angle momentum is equal to final angular momentum. If I have the length of the cord the angular speed is going to double and the reason it's more interesting it could because when you watch a skater you know do the uh what's what's it called the twirl you know when figure skaters kind of rotate spin spin thank you yeah spin when they spin. and state it very clearly and I said it fast when I talking about energy so great question if the particle has an inward velocity when I look at that V2 State the L2 State then I missed some terms if however I State you which I I did not state I screwed up I made a mistake if I state to you that listen you start at length L1 is just going in a circle right and then I pull it in and bring it to an length L2 and I stop pulling. The question I ask now is how does this thing behave so let's examine this first of all how do I parameter or to use the more technical term what nonstandard coordinates do I need to describe the configuration of this dumbbell. If instead of a rigid bar if I had a string connecting them what would the kinematic constraint be less than equal to to R right but in this case it's a rigidbar so it's equal to R to R all right by the way just so you know if in rigid bodies you can make an equality constraint um in non- rigid in if it were a string it's an inequality constraints. different you'll find out later I would have picked the center of mass which would not have been the geometric Center Center okay but you don't know why so I can't tell you why. I just did some slate of hand and I made the masses equal all right a lot of terms cancel out that's the only difference I could still have by the way I could have picked a point on that line anywhere there would still be reasonable generaliz coordinates there wouldStill be three just the math is easier that's it okay. this R OC which is some component this way plus some components this way and an angle that we know the configuration of this dumbbell at any point in time. First we will do some free body diagrams then we'll figure out the accelerations of both particles. We'll write f is equal to Ma and we have differential equations to solve it right and you'll see that they come out to be very beautiful. You'll see out of the primordial soup you'll recognize the shape say hey that's a moment of inertia we'll do that. cool formula and tell me what's the acceleration of particle p a c yeah look at all the terms is is there a b acceleration of P that's zero What's the next term isthere a CH Stone no right because the points p and Q are rigid they're not moving with respect to frame B right. Is there a centripedal term yes and that's going to be what what well that's the oiler term so there'll be an Oiler term as well right. stick if I have something at the end of a stick can I apply like a you know does it only have to be a force inwards right. Next week we'll pick up on this and essentially what we'll do is I'll do the three body diagram. Define angular acceleration and moment of inertia and a more General sense okay so let's stop here because we are over time. CL you I mean you're getting that's right so there are a couple of ways to interpret this okay it's a massless rod and I'm invoking the strong form of Newton's third law.

ROUGE-1: 33.31, ROUGE-2: 32.59, ROUGE-L: 31.31
BERTScore: 63.99

==============================================
==================== [87/100] ====================
Summary:
Differentiable programming is closely related to deep learning. It allows you to build up an increasingly more sophisticated model without losing track of what's going on. In a three layer neural network, we start with our feature vector. Now I have a vector and now I can do the same thing again. I apply a matrix, add a bias term, apply an activation function. Apply a matrix which happens to be a vector. And I get a score which then I can happily drive regression or take the sign to drive classification. we're going to see a lot of these box diagrams which are going to represent functions that we can reuse and have a nice interpretation. So the FeedForward function takes in an input vector x and produces an output vector which could be of a different dimensionality. And the way to interpret what people are doing is performing one step of processing. In particular what that processing is, is taking this input vector, multiplying it by a matrix, adding a bias term and applying an activation function. So this is a very compact way of writing something that would otherwise be quite complicated. Play with ConvNets, you can actually click here for Andrej Karpathy's excellent demo. You can actually create and train ConvNet in your browser. So Conv takes an image and the image is going to be represented as a volume which is a collection of matrices, one for each channel, red, green, blue. Each matrix has the same dimensionality as the image, height by width. And what the Conv is. going to do is it's going to compute another volume of a slightly different size, usually the height and width of this volume is going. to be equal or maybe slightly smaller. going to slide a little max operation over every 2x2 or 3x3 region. So the max over these four numbers is going to be used to build this [INAUDIBLE] and so on. If you want to go into the details, you can check out this demo or you can learn more in 231. But again, I want to highlight that there's these two modules. One for detecting patterns and one for aggregating, to kind of reduce the dimensionality. And with these two functions along with FeedForward, now we can define AlexNet. Some words are ambiguous, like product can be-- multiplication or output. So there's a lot of processing that needs to happen and it's hard to kind of specify in advance. So we're going to define an abstract function. An abstract function is something that has an interface but not an implementation. A SequenceModel is going to be something that takes a sequence of input vectors and produces a corresponding sequence of output vectors. I'm going to talk about two implementations of the sequence models. One is recurrent neural networks and one is transformers. A simple RNN works by taking a hidden state, multiply by a matrix, take the input and multiply by the matrix. And then I add these two and I apply an activation function. So at the end of the day, I have the sequence model because that maps input sequence into an output sequence. And I notice that each vector here now depends on not just the input vector but [INAUDIBLE] So if you look at h3, h3 depends on x3, x2, and x1 following this computation map. RNNs are generally fairly well, but they suffer from one problem is that they're fairly local. And so one problem that-- so is a problem that we're going to try to address with transformers. So introducing transformers is fairly involved. So I'm going to step through, introduce a few things before actually defining it. So the core part of a transformer is the attention mechanism. And the Attention mechanism takes in a collection of input vectors and a query vector and it outputs a single vector. Here is one of the input vectors. x1, x2, x3, x4. I'm going to reduce its dimensionality to also 3 dimensions. And now I can take the dot product between these x's and y's. So that's going to give me a four-dimensional vector of dot products intuitively measuring the similarity between the x and the y. Now I can use those probabilities, those weights, when I multiply by x to take away the combination of the columns of x here. themselves. So in contrast with the RNN, you have representations that have to kind of proceed step by step. So each of these vectors is comparing a particular input vector with the rest of the input vectors and doing some processing. So that's an attention mechanism. You can think about this as a sequence model that just takes input sequence and contextualizes the input vector into output vectors. Layer normalization and residual connections. These are really kind of technical devices to make the final neural network easier to train. The basic building block for generation is, I'm going to call it GenerateToken. And you take a vector x and you generate token y. And this is kind of the reverse of EmbedToken which takes a token and produces a vector. And then that gives you just one vector and you can use that to generate a token. Finally we can take language models and we can build on top of them to create them. So the basic idea is that you have input, which is a sequence of words, and you are trying to generate one that's closest to the word that you want to generate. encourage you to consult the original source if you want kind of the actual, the full gory details. Another thing I haven't talked about is learning any of these models. It's going to be using some variant of stochastic gradient descent, but there's often various tricks that are needed to get it to work. But maybe the final thing I'll leave you with is the idea that all of differentiable programming is built out of modules. Even if you kind of don't understand or I didn't explain the details, I think it's really important to pay attention to the type signature of these functions.

ROUGE-1: 34.50, ROUGE-2: 33.47, ROUGE-L: 33.11
BERTScore: 69.01

==============================================
==================== [88/100] ====================
Summary:
Professor Amy Hungerford: Today it is my very great privilege and pleasure to introduce Andrew Goldstone, a TF in this course. Andrew is a fourth-year student in the Ph.D. program in English, and he is writing a dissertation on the autonomy of the work of art in modernism. In preparation for that, for next week I'd like you to finish the novel and then read his essay, "On a Novel Entitled Lolita" It should be bound at the back of your book. T.S. Eliot's "Gerontion" is a spoof of Nabokov's "Humbert" Eliot says poems should be autotelic, that means they should be an end unto themselves. Eliot in some ways comes very close to the kind of ideas about art that Nabokovsky holds. Eight of the eight features of literary modernism that are all important to Nabokova are: obsession with art's autonomy, the idea that art is its own law, that it has no other purpose than its own. sense that civilization itself is being overturned. The idea that culture itself is the saving, most important activity that people can engage in. Fourth--and this goes along with that--a rejection of convention, especially sexual convention, sexual morality. Fifth, spatial form, the idea that in place of a linear narrative you have a system of cross-references and repeated motifs that give the structure of works. Sixth, this is a term from the critic Joseph Frank: spatial form. And then, this anticipates my last points: Modernism is self-consciously international. The strategy of the knight's move is to frustrate your expectations, to leap over the apparently important events into something else characterized by a kind of aesthetic play. Nabokov, in 1966 he said this: "The greatest masterpieces of twentieth-century prose" are, in this order: Joyce's Ulysses, Kafka's Transformation"--that is, "Kafka's Transformation," "Ulysses" and "Lolita" The parentheses are a real icon of that. A critic has counted 450 sets of them in this novel. Proust is himself gay. One of his big subjects is homosexuality, and Nabokov's reaction to this is really homophobic. It's about a relationship to predecessors who are seen as too similar. And it should just make you wonder whether pedophilia is in itself a kind of knight's move from homosexuality. In other words, is there another form of perverted desire hiding behind the one that's in front of us? Just a suggestion: look on page 20, still in Humbert's early life, near the bottom. Nabokov's relationship to this modernist past is not just the burlesque that he visits on Eliot. An element of admiration is also present, and that's really part of his relationship to Joyce. I'm just going to name for you four features of Joyce's style that are important to Nabokov: stylistic virtuosity, the ability to imitate any style; at the same time, a scrupulous attention to the banality of everyday life and all its detail. Third characteristic, the constant use of a superimposed structure. A list of names leads up to this aesthetic sensation, the revelation of a poem. The ordinary materials of life become the basis for a kind of artistic achievement. Even the ordinary names are kind of plants, because almost every name on this list comes back elsewhere in the book. McFate is the icon of the difference between the realistic world of Joyce and the already artificial, already aestheticized world of this novel. No one was ever really named McFate. McFates are a parody of real. The artificial has taken the place of the real here, and this novel really reminds you of that all the time. The thing that stands for randomness in this book, the thing that looks like ordinary detail, has already been arranged to give you artistic pleasure. So, this is the, kind of, hand of Nabokov, taking a narrative of real events and twisting it into something that makes a kind of sense, taking fate and making it McFate. And I want to show you one more example of that, in the scene where Humbert and Lolita have reached the hotel, the Enchanted Hunter. Nabokov's novel is a response in particular to exile. An exile, living in a foreign country, lives in a kind of denaturalized world. Nothing is initially known to make sense; everything has to be figured out and reinvented. In that afterword to this book, Nabokov says he had to invent America. That's because he didn't know it already; it wasn't given to him. But it has a payoff, kind of, a payoff which is the possibility precisely of inventing. because Humbert is a foreigner--into something you can laugh at, something you Can enjoy, something that you can apply the knight's move to. The foreigner's love for this kind of move is a response to this denaturalized world of the exile. Even as it's the prototype for originality, it's also something very disturbing and harmful. And that conjunction has to do with the traumatic event of having had to emigrate, having to take up another language. It's important, in this connection, to remember that the knight’s move as a way of avoiding obstacles, in particular, keeps skipping over forms of violence. the fear that it's too like what he wants to do. But the main point here to think about is that feeling of damage. On page 152, evocation of the landscape: By a paradox of pictorial thought, the average lowland North American countryside had at first seemed to me something I accepted with a shock of amused recognition. "Inutile loveliness" is kind of the key word of Nabokov's novel, and it's the source of the most appealing writing. technique, and he says the novel has as its only purpose to provide aesthetic bliss. So, a European artist actually appears again there, with Claude Lorrain, but kind of made strange: given that knight's move, given a new twist. So--instead of familiar, incorporated into this profoundly strange, vast landscape that gets Humbert's most appealing rhetoric--the rhetoric of an exile. But, I don't want you to think that this just means everything's okay. Of course, everything is not okay. Nabokov tried to become an American writer, as Nabokov says he's doing: trying to invent America, trying to bridge the gap between Russian and English. He translated Lolita back in to Russian later on, and he added a second afterword where he said: That wondrous Russian tongue that, it seemed to me, was waiting for me somewhere, was flowering like a faithful springtime behind a tightly locked gate whose key I had held in safekeeping for so many years, proved to be nonexistent.

ROUGE-1: 29.52, ROUGE-2: 28.07, ROUGE-L: 26.85
BERTScore: 63.81

==============================================
==================== [89/100] ====================
Summary:
NORVIN RICHARDS: Today is phonetics, which means that today we begin making funny sounds at each other. Let's see. I'm trying to remember if there's anything that I ought to announce. You remember, maybe, that problem set 1, which confusingly is your second problem set, is due on Thursday. Normally, it would be due on Tuesday. But because I am technologically challenged, it's due onThursday. I just figured out how to get the projector to project over there instead of in the middle so that I won't have to write everything twice. Linguists have a system for writing sounds down so that we'll all know what kind of sound we're talking about. A lot of the symbols of the International Phonetic Alphabet resemble letters of the English alphabet. The symbol for the sound at the beginning of "paint" is the letter p. There are also what are called labiodental sounds, which involve your top teeth and your lower lip. We'll talk about other kinds of articulation that English doesn't use, but I think that one just doesn't exist. The International Phonetic Alphabet was invented by English speakers. It was also invented by speakers of German, which they use this symbol for. the "y" sound at the beginning of "year" The symbol for the sound of the start of "face" is an f, and the symbol for "vase" is a v. These are the sounds of the beginnings of words like "thistle" and "this," where you're sticking your tongue between your front teeth and making air flow out. In the velar sound, the body of your tongue is up against what's called the velum, which is the soft tissue at the back of your mouth. English doesn't make a huge amount of use of the glottal stop, but it's what shows up at the beginnings of words like "uh-uh," that catch that you're getting in your throat. There are places in English where things that we write as "glottal stops" are actually, in fact, sounds. in mind? STUDENT: No. NORVIN RICHARDS: Oh, OK. We'll get to "r," eventually. People investigate this kind of thing in all kinds of ways. These days, people do a lot of MRIs. There are charts of all of the sounds that we're going to talk about plus many more together with MRIs of the insides of people's mouths making these sounds so that you can see the anatomy. You won't just have to think about it. "Cat" and "dog" end in sounds that differ in voicing. We say that "z" is voiced and that "s" is voiceless. It's like whistling with a blade of grass, or playing a reed instrument, right? It's getting something to vibrate really fast. And that's what you're hearing and feeling if you put your hand right here when you're doing a "z." That's that buzzing sound that you're seeing and feeling. or-- what's my other example here-- "th" [pronounced as in "thistle"] and "th," right? If you whisper "safe" and "save"-- (SPEAKING NORMALLY) you have the feeling that you can hear the difference between them, yeah? I think if you were to whisper one of these-- so do a controlled experiment. Whisper to your dorm mate, (WHISPERING) "safe." And then find out what they think you said. what it means is that your vocal cords are vibrating. But I think maybe what we're learning is that you do some other things, too, to optimize the flow of air so that you will get a good vibration going. There's experimental work on this. This is the kind of thing people try to figure out. OK. So you have voiced sounds and you have voiceless sounds. So "s" and "z" are alveolar. But "s," "t," and "d' are voiceless. And "z," "d" are voiced. Does that all sound right? In Polish, "g" becomes "k" at the end of a word. Voiced "b" becomes voiceless. "s" and "t" are both alveolar. And they're both voiceless, but the way we distinguish them is via what we call manner of articulation. For "s and "z," the airflow is restricted, but you hear his sound as a "shsing" and his "fth" "f" is a fricative. Fricatives are sounds in which you don't hear the flow of air. English doesn't allow words to begin with velar nasals. But there are languages that do. Tagalog is one. Cantonese is one, and there's a bunch of others. English, for example, has bilabial stops, "p" and "b" But it doesn't have a labial fricative, which sounds like a candle, just like blowing out. Are there questions about any of this? Is anybody looking at this and saying, whoa, this table has grown out of my ability to keep up? English has interdental fricatives -- thuh and thuh. There are languages out there that have what are called dental stops. Part of your job, if you're learning Tagalog, for example, is to learn to make dental "t"s instead of alveolar "t," because that's what they've got. If you're studying another language, this is the kind of thing to think about because sometimes, your teacher will not be thinking about this. And then you're like, what will you do with your tongue to make the best sounds as you can? uvula, which is the little doohickey that hangs down there at the back. That's your uvula. We do not have these in English, at least when we're feeling well. But there are languages out there the do. Inupiaq has a uvular stop. Pharyngeals involve constriction near the pharyngeal wall. Arabic has these. The Berber languages have these. You're getting the back of your tongue to get against theBack of your vocal tract. uvulars, retroflexes. And for some reason, the dental stops are still red. Have to fix that. OK. Now people keep asking me about sounds that I've been carefully avoiding, so let's talk about them. There are what are called approximants. Approximants are not stops, and they're not fricatives. And they are sometimes divided into glides and liquids. And I'm hoping that nobody will ask me how you know whether something is a glide or a liquid. English has a very large number of vowels and a not-very-good system for writing them. One way of classifying vowels is in terms of height. For "ee," your tongue is tense. And then for "ooh," where does it go? It moves, right? So you aren't just rounding your lips and leaving your tongue where it was. In fact, that's why when you take photographs of people, you have them say something with an "ee" vowel in it, like "cheese" English is supposed to have five vowels. It does not have five. It has 14. Why do we only have five letters for vowels? Who gave us this alphabet? The Romans, right? In Latin, there in fact are 5 vowels, which can be either long or short. And so we've ended up with 12, 14 vowels in English. Different dialects of English are different. And we have 5 letters to spell them with. And this is why we have spelling bees, yeah-- one of the reasons. English doesn't have words that end in "ih," "uh," or "eh," with the possible exception of "meh" English monosyllables can't end in lax vowels that are either front or high. Not all speakers of English distinguish schwa from wedge. Not every dialect of English has all of these vowels or has them all in the same places. As we go along, I'm going to be asking you to read things in IPA. We'll do this exercise next time.

ROUGE-1: 20.18, ROUGE-2: 18.59, ROUGE-L: 18.45
BERTScore: 63.52

==============================================
==================== [90/100] ====================
Summary:
Expectation is a basic question that will come up again and again when we look at random variables and probability theory. We're imagining n independent flips of a coin with bias p. The probability of heads is p. It would be biased in favor of heads if p is greater than 1/2. And we want to know how many heads are expected. So what's the expected number of heads? Well, we already know-- we've examined the binomial distribution B n,p.

ROUGE-1: 19.54, ROUGE-2: 18.43, ROUGE-L: 15.40
BERTScore: 71.67

==============================================
==================== [91/100] ====================
Summary:
Aristotle was born 384,15 years after the trial of Socrates. He was sent by his father to go to college. Unlike most of you, Aristotle did not spend four years at the Platonic Academy. He remained attached to it for the next 20, until the death of Plato. Unlike his intellectual godfather, Socrates, who wrote nothing but conversed endlessly, Aristotle wrote disciplined and thematic treatises on virtually every topic. He collected constitutions, 158 of them in all, from throughout the ancient world. Aristotle says man is more a political animal than any kind of bee or herd animal. He isn't saying that man is political by nature, he says, but that he is possessed of the power of speech. The city is natural in that it allows human beings to achieve and perfect what he calls their telos. A person without a city must either be a beast or a god, says Aristotle. He says participation in the life of the city is necessary for the achievement of human excellence. Aristotle says that man is a political animal and the city is by nature a polis. The form of association that leads to our perfection is necessarily something particularistic. Only a small city, small enough to be governed by relations of trust, can be political. The alternative to the city, the empire, can only be ruled despotically. A good citizen of a democracy will not be the good citizens of another kind of regime. Partisanship and loyalty to one's own way of life are required by a healthy city. condensed in very deep ways, carry a great deal of freight. There's a lot in there that needs to be unpacked. We need to avoid the temptation, in many ways understandable as it might be, to airbrush or sanitize Aristotle, to make him seem more politically correct for modern readers. The question is what did Aristotle mean by slavery? Who or what did he think was the slave by nature? Until we understand what he meant, we have no reason to either accept or reject his argument. the preserve of the few, of a minority capable of sharing in the administration of justice and in the offices of a city. It seems to be a very elite teaching. Would you agree? Unappealing to us, perhaps, for that reason, very contrary to our intuitions and the way we have been brought up. But before we dismiss Aristotle's account as insufferably inegalitarian and elitist, we have to ask a difficult question. What else is Yale, but an elite institution intended to educate, morally and intellectually, potential members of a leadership class?

ROUGE-1: 15.88, ROUGE-2: 15.06, ROUGE-L: 15.23
BERTScore: 57.17

==============================================
==================== [92/100] ====================
Summary:
Mathematically, a consumer is trying to maximize his utility. And this utility maximization has to be done with respect to some constraint and the constraint the budget constraint we take P 1 x 1; P 2 x 2 should be less than or equal to I. In real life, it is possible that a person derives some satisfaction from having some money left in his pocket, but the way this problem has been framed here the person’s satisfaction depends only on his level of consumption of good 1, and good 2. same consumption bundle; it is clear. So, if we solve it what do we get? Here let us say for x 1 instead of using star let me use term M I will explain what does this M, this is x 1 m is a function of P 1 P 2 and I here u naught is not fixed. What is e? E is nothing but P 1 x 1 plus P 2 x 2. We are checking we have set up the problem such that this I is equal to this U naught.

ROUGE-1: 17.34, ROUGE-2: 16.99, ROUGE-L: 17.34
BERTScore: 67.95

==============================================
==================== [93/100] ====================
Summary:
MIT OpenCourseWare is a free, online education platform. MIT OpenCourse Ware is available in English and French. The weekly Newsquiz tests students' knowledge of MIT courses. This week's quiz includes questions about the i3, Mr. Sticky, and the smart car. The winner will receive a prize for their best knowledge of an MIT course in English or French. Back to Mail Online home.Back to the page you came from.. The quiz: Do you have a concept you want to share? If so, please share it with us. defined could almost apply to a Tramway as well. If this was like a streetcar, don't you think it would apply to that as well? So I think the fact that it's a personal vehicle, I think it's important. So the key in this is, describe the concept using few words precisely, but to set it apart from neighboring concepts. What about Rolex Center? It's a single-layer building with multiple straw used as a library for people to meet and study. built. The first thing you do is understand where is the value-- the stakeholders and the stakeholder analysis and the requirements definition. And then you interpret and incorporate some of the needs into goals, which become requirements. And so the goals then are an instrument of the primary delivery value delivering process, which is your value proposition. To then actually deliver that value you need to design the product, the product system, and understand the operand, the thing that is being operated on or transformed by the primary value delivery process. EPFL's Olivier De WECK talks about how to reduce food spoilage rate. He says the key is to think in an abstract way and then pick a specific concept. He explains how bears use fat to transform it into fat and store it in their bodies. The process is similar to conserving grapes, beer and wine, he says. The concept was developed by EPFL's design and creativity unit. The project is now being developed by a company in the UK, and is expected to be completed by the end of 2014. A cooler has the chilling function, and then the sub-functions when we zoom in are holding the food, exchanging the heat between the food and the environment, reducing the heat load, interfacing, connecting, powering, regulating the temperature, et cetera. And then on the form side, the cooler itself is very simple structurally. And you can go very detailed here, even for something very simple like a cooler. One question and then we'll talk about the refrigerator and how it's different in a minute. is concept generation. So take the requirements and think creatively about how these requirements could be fulfilled. And then once you have several concepts, you've got to select among them. That's our topic next week is concept selection. So let me briefly talk about the NASA approach to this and then talk about some methods and tools for concept generation, which we'll talk about next week. It's about starting with the operand. What is the thing that the beneficiary, the stakeholder cares about, and how do we transform that? The logical decomposition process, as described in the NASA standard, is used to improve the understanding of the technical requirements. So we started with mission authority, stakeholder expectation, and then defining those high-level requirements. And then on the right side, you come out with the lower-level derived technical requirements, logical decompositions models. So you can see this relates very strongly to the system modeling languages that we talked about. And we really talked about much of this already. And in terms of the logical decompose flow diagram, you start with your basic high- level requirements and measures of performance. You essentially do your decomposition. and the logical decomposition work products, which are essentially lower-level definitions of what these subsystems look like. And then you can go off and do the detailed design and then the testing verification and so forth. So it's essentially focused on decomposition, which is an important part of architecting, but it's not the only thing you do. So let me talk about methods and tools for concept generation. So what are different ways of stimulating or organizing creativity? And what I'm showing you here is-- that's essentially a mind map of how to think about the creativity space. which I'm going to mention, but we're not going to do as part of the class, which is stimulants. So this is the idea that somehow people are more creative when their brain, when you put yourself into some other state. So bio-inspired design would be you go in nature, or you read books about seashells and animals and you really try to understand from nature. So you putting yourself in nature and be inspired by what you see. Random inputs, provocations, challenges, and then things like alcohol, and even drugs. There's got to be a driving question for the brainstorming session. There's an ideal group size, and it says 5 to 10 here, but I should probably revise this to be-- what do you think? 7 plus minus 2. The idea is to produce a large amount of ideas and then at some point, maybe 30 to 60 minutes, Brainstorming session that last four hours is probably really good and then the rest is kind of shot and there's not a lot of new ideas coming. Leonardo was a head of his time in many ways. So he's really been identified as an exceptional individual. How to Think Like Leonardo, Seven Steps to Genius, is a book on creativity. The seven da Vincian principles of creativity are here in Italian. They include curiosita, lifelong quest for learning. Dimostratzione, testing your knowledge through experience, trying things out. Sensazione, continual refinement of the senses. Mastering ambiguity, paradox, uncertainty. Arte/Scienza is the whole brain thinking, left-right brain. Corporalita, balance of body and mind. or an architecture. So the key decisions are the rows. There are factors in the rows, and then for each row you think about what are the number of possible alternatives for doing this. And then you enumerate all possible combinations. And the big challenge with this, of course, is if you have many factors, you could generate many infeasible architectures. Not all these combinations are actually feasible. So that's architecture enumeration, and there's different ways of doing this at different layers of abstraction. like 12 different tail geometries here. But at that higher abstraction layer, it's just a single tail. So how do you combine these using compositional rules? That's architecture enumeration. And we'll post some information on this if you want to try this out for your concepts. So let me summarize. System architecture is definitely very abstract, but it's also, potentially, the most influential activity that we do in system architecting. The concept is mapping function to form. A3 assignment A3 asks students to come up with new ways of brainstorming. Students must use mind maps, morphological matrices, and architecture enumeration. The assignment is due in two weeks and will be graded on a scale of 1-10. The final assignment will be given to students at the end of the month and the results will be published in the next few days. For more information on the assignment, visit the assignment website. It is open to students from all over the world and can be downloaded from the assignment site.

ROUGE-1: 30.37, ROUGE-2: 25.92, ROUGE-L: 26.17
BERTScore: 58.99

==============================================
==================== [94/100] ====================
Summary:
then we are going to continue with the second part of the lecture today which focuses on the problem what actually happens if the gaussian assumption that i have about my constraints doesn't hold you can have multiple reasons why this doesn't holding. As you will see in some small examples having this outliers in your optimization problem is something which hurts dramatically which actually screw up your solution so already a few outliers can lead to a environment model which is completely unusable for doing any navigation task. Where the geometry of what you computed doesn't fit to the real world geometry anymore and one of the questions actually how to handle that.  places may look identical although they are not identical if we hear this lecture room and we're in the next lecture room next door they look very very similar it may be very hard for a robot to distinguish that we are here whatever in room 18 and all in room 16. Having the ability to take into account multi-modal beliefs is actually helpful there's another real world example so this is the intel research lab data set that you have also experienced and if you look to those poses over here in the poses down here how those individual structures match. The original number of constraints i don't know how large it was but i think it's around three thousand something like this along those lines. If you put already ten wrong ones in there it's quite likely to screw up it was one hundred which is still a very small number compared to 3000 or just a small fraction. It will actually end up in dramatic mapping errors so the system is unusable so having good data cessations is really important so already screwing up a small number of places is something which can hurt your optimization. the first attempt to to solve this problem this would be our probability distribution what is the problem with this probability distribution so i say okay some of my constraints are these multiple multimodal constraints i will simply go ahead and implement that. What's the problem that you're going to experience if you make this is some so this is we know how to solve that right this is what we did so far let's say i want to go from here to here what's the Problem that shows up if you do it exactly that way you start coding your stuff. done you know he said oh damn i can't do that what would be the ugliest trick that you can do in order to make that work even worse no that's not quite what you're going to do i mean the sum is kind of the the the bad thing what can i do with the sum instead of the sum i can i can get rid of thesum in some nice way sorry oh the integral of some to the integral actually makes our life typically worth um so that's that's going what's going to fly. individual constraints so when you evaluate the error the evaluation of the error is somewhat more expensive because every constraint can have can be multimodal or bimodal in this case. There's not no big difference in the operation of those systems when you um if you use the red or the. or the or the blue plot this is exactly the trick that is used. If you just want to deal with outliers so the the red one is kind of the inlier and the blue function is the one which is the outlier. that's actually a nice thing so um another thing is it can handle both things at the same time data station errors as well as multimodal constraints. So the combination of outlier rejection and dealing with wrong data associations is actually kind of nice we also can do this obviously in 3d so this is again this data set with the sphere that we have seen before robot moving in a virtual sphere with constraints. It's hard to identify the sphere here and if he increases to 100 outliers this is just whatever a big mess whereas this one still is able to solve those things quite nicely. actually compute these so the main changes we go to this formulation we have the scaling factor over here and we need a good way to compute the scaling Factor so how can we actually do that and there's actually closed form you can derive that under certain properties where you end up with this operation here. If the error increases if you're further away from the mode this is what your error function looks like so the the error is weighted down the further i'm away however we still have a linearization point so if i compute the jacobian over here i still has a jacsobian which drags the system into the right direction. and flatter and flatter gaussian distribution the further the point is actually away there's also one technique which you can find which is also quite easy to implement because you just need to compute the scaling factor and multiply that with your information matrix for every constraint also something you can do quite efficiently. If you have constraint which introduce large errors these are these outliers this can actually screw up the optimization when computing the minimal error configuration. There are different ways of uh kind of row function that we can actually in there and then we're then trying to minimize. here that by changing this function you can't get much better behaviors kind of deciding which function to use for the underlying optimization problem is not on it's not always an easy and easy choice so this requires some expert knowledge some good intuition on coming up with the way with one of those functions. Next week which is the last week of the term i will briefly talk about front ends and give kind of a short summary on what typical front ends exist obviously we're not going to all the details as we did that here.

ROUGE-1: 28.02, ROUGE-2: 27.39, ROUGE-L: 27.68
BERTScore: 67.54

==============================================
==================== [95/100] ====================
Summary:
John Stuart Mill is the principle expositor of neoclassical utilitarianism. The rights-utility synthesis signals that we're looking for an attempt to put together both a commitment to utilitarian efficiency that's grounded in science and respect for individual rights. The doctrine that would come to be called emotivism was developed by Vilfredo Pareto and other figures in the modern history of economics. And they developed a system of thinking about economics and the theory of value that was going to be tremendously influential. The emotivist doctrine is an endpoint in a philosophical evolution that really begins in the seventeenth century. It was associated with a man by the name of Stevenson who wrote several books advocating the doctrine. Stevenson was thought to be a proponent of a kind of moral relativism because he linked ethics to our desires, and preferences, and emotions, and nothing else. And it's not until you get to the end of this narrative that you'll start to see why the transition from classical to neoclassical utilitarianism in economics was essential in moral philosophy. by-product is, you're going to get the whole of ECON 101 reduced to a single lecture. Because indeed it is true that enormously complex and subtle, and as sophisticated as the neoclassical theory of microeconomics is. It's all built out of three ideas that I'm going to spell out for you in what some of you might initially regard as laborious detail. But I think you'll see what I'm getting at once we get towards the end of today's discussion.  neoclassical economists wanted to understand the behavior of markets. They wanted to be able to more precisely to predict what prices were going to be in markets. Pareto, Marshall, and Edgeworth, and others who were in their circle, thought you could do this just with ordinal utility. So moving from cardinal to Ordinal utility is going to turn out to have huge ideological consequences, which I'm going to unpack for you towards the end of today's lecture. All All we would know about this person A, as I said, is that they prefer four to three, three to two, two to one, one to zero. But we can't say anything about how much they prefer those things because these distances don't actually mean anything. All we get is an ordered ranking. One other thing we can say is, that this is a no-no. These indifference curves cannot cross. Can anybody tell us why? Why can't they cross? Wait for the mic. Pareto drew a diagram with two people on it, A and B, and these axes, the X-axis, here, is A's utility function. So A has this much utility--if this is the status quo X, okay? A's happier than B, right? Wrong, A's not. Pareto said, "Let's draw a line north-south through thestatus quo, and let'sDraw a line east-west though the status status quo," okay? which the radical fangs of classical utilitarianism have been ripped out and it is now a doctrine that is very friendly to whatever status quo happens to be generated in a market system. So it ceases to be this radically redistributive doctrine, and in the process imports into utilitarianism a very robust, some would say, hyper-robust doctrine of individual rights. We'll see how that played out in political theory when we come to look at John Stuart Mills' harm principle next Monday.

ROUGE-1: 16.86, ROUGE-2: 16.04, ROUGE-L: 16.20
BERTScore: 66.09

==============================================
==================== [96/100] ====================
Summary:
HONG LIU: So, first, we talk about chiral fermions. So, previously, we showed that the Dirac equation requires, actually, psi to have four components. And the answer turns out to be, no, you can reduce it, OK? And so there are two ways to reduce it. One is called the Majorana fermion. So we first talk about the chiralfermion and one way to do it. And then, we will look at the specific representation of gamma matrices. HONG LIU: So if sigma is block diagonal, then that means this S is also block diagonal. So that means, when I write psi-- so, psi-- have four components. That means that the upper two component and the lower two components-- they don't transform to each other, OK? They only transform within themselves. They just transform among themselves. So they actually have well-defined Lorentz transformation as a smaller unit. You don't need four components to be able to transform. the algebra for the gamma matrices, you need four components. But we actually knew all along that two components is enough for Lorentz transformation. How do we actually know all along? HONG LIU: It is the mass, OK? So it is theMass m. If you want to describe a massive particle, then you must have four components, and it's the mass m which is the key. OK, good. Any questions on this? So, now, we have shown in this particular representation of gamma matrix, for this particular choice of gammaMatrix, and then the psi transform block diagonally. Gamma 5 is defined to be i gamma 0, gamma 1, gamma 2, gamma 3. So you take the product of all the gamma matrices together and then with a factor of i, OK? So the i there is for the purpose that if you-- you can check yourself-- that the gamma 5 is actually Hermitian. And you can also check yourself that gamma 5 squared is equal to 1. So, this, you can almost easily understand because it's all Gamma 0, Gamma 1, Gamma 3. HONG LIU: gamma 5 squared, squared to 1, is Hermitian, which means its eigenvalues are all real. He says if gamma 5 commutes with sigma mu, nu, then gamma 5 will commute with Sigma S. So gamma 5 acting on psi then just equal to psi L, and the gamma 5 psi R is equal to minus psi R, he says. HONG LIu: So you can easily see, by definition, that gamma 5 acts on psi L and psi R. HONG LIU: In physics, actually, the massless case actually gives you very much richer structure, normally, than the massive particle. In the Dirac spinor, which we have talked about so far, is four components. And then the chiral spinor we talked about-- essentially, you have two complex components. So the next one I'm going to talk about is the Majorana, in which case, I would argue, we have 4 times 1 real component. HONG LIU: The Majorana spinor was discovered by Majorana in 1937. Majorana is, like, half electron. Heuristically half electron, it has very stable topological properties, which a single electron does not have. He says if you can engineer in your condensed matter systems, Majoranaspinor then became a Holy Grail. HONG LIu: People say they have engineered Majorana Spinor in the lab, which I think has never been fully confirmed. we get a very nice relation that, under Lorentz transformation, is generated by this B, related to this B matrix. When you have the psi prime equal to that, let's just take the star of this equation, OK? So psi prime star just equal to S star, Lambda psi star, equal to B S. B minus 1 and B S-- B psi. So that means that the psi star should be equal to gamma 2 psi. That's the Majorana condition here. HONG LIU: So, by definition, discrete symmetries are symmetry, which don't have continuous parameters. So, for example, let's imagine we have a real scalar theory. This theory has a discrete symmetry because this is invariant under phi. It goes to minus phi, OK? So because you see all the terms are even, so it's no continuous parameter. And so this is often called the Z2 symmetry. And there are also spacetime discrete symmetry. So this is an internal discrete symmetry-- have nothing to do with spacetime. just reverse one direction or reverse two directions, OK? That seems also to be a discrete symmetry. And indeed. So if you just change the directions, say, in the x direction, that's also a discrete. And if you only change the direction in both x and the y direction, That's also an independent symmetry. But if you change-- if you do the reflection in two directions,. that's equivalent to a 90-degree rotation. And so it's part of the continuous symmetries. And now, when you change all three directions compared to change one direction, you differ only by changing two directions.

ROUGE-1: 17.93, ROUGE-2: 16.74, ROUGE-L: 17.27
BERTScore: 71.70

==============================================
==================== [97/100] ====================
Summary:
The best-case scenario for expansionary fiscal policy is when there are lots of underemployed resources in the economy. By increasing spending, the federal government can try to counteract falling aggregate demand. In one scenario, government spending doesn't have to be as large as the fall in "C," or consumption, to counteract the recession. That's because of the multiplier effect. But, as always, shifting lines on a graph is much more important than the long-run effects. easier than shifting around real resources in a multi-trillion dollar economy. Fiscal policy has many implementation challenges, and we'll turn to these next. You're on your way to mastering economics. Make sure this video sticks by taking a few practice questions. Or, if you're ready for more macroeconomics, click for the next video. Check out Marginal Revolution University's other popular videos, including the one about the U.S. debt ceiling and the next one on the debt ceiling.

ROUGE-1: 47.04, ROUGE-2: 42.19, ROUGE-L: 42.99
BERTScore: 63.85

==============================================
==================== [98/100] ====================
Summary:
Sarah thread sterner shows you how to wear and take off a mask. She explains how to determine which part of the mask is the front versus the back. She also explains some of the common mistakes that people make when wearing a mask such as wearing it under the nose or upside down or backwards. Finally, she explains some tips on how to perform hand hygiene when using a face mask with ear loops to ensure that the mask doesn't become contaminated. Back to Mail Online home. back to the page you came from. toward the cheekbone then grasp the nose piece of the face mask and pull the bottom of the mask under the chin when removing the mask it's important to remember that the front of the Mask is considered contaminated. dispose or clean and store for future use the mass / facilities protocol and then perform hand hygiene ok so that wraps up this video on how to wear and take off a mask and don't forget to check out the other videos in this nursing skills series. Click here for more nursing skills videos.

ROUGE-1: 47.31, ROUGE-2: 39.49, ROUGE-L: 39.66
BERTScore: 68.97

==============================================
==================== [99/100] ====================
Summary:
In this lecture, we're going to talk about how neurons function and how researchers are able to control that function in order to modify behavior. Then we'll talk about synapses and how synapses function to communicate between neurons. And this is going to involve also sort of understanding how certain antidepressants, like Prozac, work. And then we'll end by talking about how researchers did this experiment to wake up the mouse. It all starts with something that I told you about at the beginning of the semester, which is that the plasma membrane separates distinct compartments. There are different types of signals that nerve cells can send. Signals can be excitatory, meaning it will tend to depolarize the neuron. There are other signals that bind to different receptors that are inhibitory. Neurons are totally prepared to send signals to each other. They have everything ready to go when they get word from upstream, and they're ready to sent signals to the next cell. And that's because if we look at the synapse prior to action potential, everything is ready to be released. In optogenetics, we use light to activate specific neurons in a brain and that leads to the animal waking up. Light induces sodium channel opening that's going to depolarize the cell. And if you have a gene that you know is expressed in a certain type of neuron, you can take the promoter and hook it up to this single component of that gene. And this is a light-sensitive protein called Chrhodopsin2 that's specifically expressed in the neurons that you're trying to test. you to test the function of the neuron in the behavior of an organism. So, in this case, this mouse, the light is shined into its brain, and they're testing a specific type of neuron that is involved in arousal of the mouse. And it's going to wake up right now. There it goes. It woke up. You see now its muscle activity is going, OK? So you can test thefunction of specific nerve cells using this approach, and it's because you have a light-sensitive sodium channel.

ROUGE-1: 10.76, ROUGE-2: 9.96, ROUGE-L: 10.35
BERTScore: 67.12

==============================================
==================== [100/100] ====================
Summary:
In this problem, we're going to be dealing with a variation of the usual coin-flipping problem. But in this case, the bias itself of the coin is going toBe random. And we're told that the expectation of this bias is some mu and that the variance of the bias isSome sigma squared. And what we'll be asked is find a bunch of different expectations, covariances, and variances. We'll see that this problem gives us some good exercise in a few concepts, a lot of iterated expectations. An art that you learn through more practice. But one good rule of thumb is, when you have kind of a hierarchy or layers of randomness, it's useful to condition on the layer above where that is, in this case, the random bias of the coin itself. Once you condition on that layer above, that makes the next level much simpler. Because you kind of assume that you know what all the previous levels ofrandomness are, and that helps you calculate what the expectation for this current level.

ROUGE-1: 10.90, ROUGE-2: 10.35, ROUGE-L: 10.53
BERTScore: 70.24

==============================================
