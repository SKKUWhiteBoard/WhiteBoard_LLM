The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free. To make a donation or to view additional materials from hundreds of MIT courses, visit MIT Open CourseWare at ocw.mit.edu. PHILIPPE RIGOLLET: --124. If I were a man, I would be a member of the Communist Party of China. If you are a member, you are entitled to a copy of this article and the contents of this page. to repeat this 1,000 times, so every one of those 1,00 times they collect 124 data points and then I'd do it again. Then in average, the number I should get should be close to the true parameter that I'm looking for. The fluctuations in the data are what make up the average number of data points that are collected. The average number is the number that is close to what I want it to be, which is 124. That's what I'm trying to get. that are due to the fact that I get different samples every time should somewhat vanish. And so what I want is to have a small bias, hopefully a 0 bias. If this thing is 0, then we see that the estimator is unbiased. So this is definitely a property of the property of an unbiased estimator. If it's 0, we can say that it is unbiased, and if it's 1, then it's unbiased too. So it's definitely an unbiased property. that we are going to be looking for in an estimator, trying to find them to be unbiased. But we'll see that it's actually maybe not enough. So unbiasedness should not be something you lose your sleep over. Something that's slightly better is the risk, really the quadratics risk, which is really the risk of the estimator being wrong. That's what we're looking for, and that's the risk that we're going to look for in our estimator. is expectation of-- so if I have an estimator, theta hat, I'm going to look at the expectation of theta hats n minus theta squared. And what we showed last time is that we can actually-- by inserting in there, adding and removing the expectation, we actually get a more accurate estimate of what theta is expected to be. That's what we're trying to do here. We want to get a better idea of how theta works in the real world. get something where this thing can be decomposed as the square of the bias plus the variance. That came from the fact that when I added and removed the expectation of theta hat in there, the cross-terms cancel. That is just the expectation. of theTA hat minus its expectation squared. The cross- terms cancel. when you add and remove the expectation in there. That was the result of adding and removing the expectation, and the expectation squared is the squared squared of that. All right. So that was the bias squared, and this is the variance. And so for example, if the quadratic risk goes to 0, then that means that theta hat converges to theta in the L2 sense. And here we know that if we want this to go to0, we want the risk to be the same as 0. So we want to make the risk equal to 0. And that's what we did. We made a square root of the risk, and we got the risk of 0. since it's the sum of two positive terms, we need to have both the bias that goes to 0 and the variance that go to 0. And so there is usually an inherent trade-off between getting a small bias and getting a large variance. So we need both of those things to work out the best way to get the best result. We need to be able to control both of them at the same time, and that's what we're trying to do in this case. We want to get both bias and variance. a small variance. If you reduce one too much, then the variance of the other one is going to increase, or the opposite. That happens a lot, but not so much, actually, in this class. So let's just look at a couple of the examples we've seen so far. We're going to take a look at two of them, and then we'll go on to the next one. The next one is the one we'll take next week, and it's going to be a little bit different. examples. So am I planning-- yeah. So examples. So if I do, for example, X1, Xn, there are iid Bernoulli. And I'm going to write it theta so that we keep the same notation. Then theta hat, what is the theta hats that we proposed many times? It's just X. It's a hat that we propose many times, and it's called theta. That's what we're going to call it. Xn.bar, Xn bar, the average of Xi's. bar, and theta. So what is the bias of this guy? Well, to know the bias, I just have to remove theta from the expectation. Well, by linearity of the expectation, it's just the average. of the expectations. But it's more than that. It's the bias. It is the expectation of Xn. bar. And theta is just theta, and so on. since all my Xi's are Bernouilli with the same theta, then each of this guy is actually equal to theta. So this thing is actually theta,. which means that this isn't biased, right? Now, what is the variance of this guys? So if you forgot the properties of the variance, then this is a biased Xi. So if this is biased, then it's not biased. So it's biased to be biased to have the same variance as the other Xi's. for sum of independent random variables, now it's time to wake up. So we have the variance of something that looks like 1 over n, the sum from i equal 1 to n of Xi. So it's of the form variance of a constant times a random variable. So the variance is the sum of i and n, which is the number of independent random varying varies between i and n. It's the same as the variety of a constant and a random  variable. The variance of a times X is a-squared times the variance. When I pull out a constant outside of the variance, it comes out with a square. We know that the variance leaves on the square scale, so when we pull out the constant, we get the square. The square is the square of the number of times the constant is greater than or equal to 1.0. The cube is the ratio of the constant to the square, or the square to the factor of 1. of X, so this is equal to 1 over n squared times the variance of the sum. So now we want to always do what we wanted to do. We would like somehow to say that this is the sum of the X and the n squared variance of X. So we have the variances of X and N squared. We want to say the sum is 1 over N squared times n squared. This is the number of times the n square root of X is greater than 1. We have the total number of n squared squared. variances. And in general, we are not allowed to say that, but we are because my Xi's are actually independent. So this is actually equal to 1 over n squared sum from i equal 1 to n of the variance of each of the Xi's. And that's by independence, so that's what we are doing. We are saying that we are able to do this because we are allowed to do it. We can do it because we're able to say we're doing it. this is basic probability. And now, what is the variance of Xi's where again they're all the same distribution, so the Variance of Xi is the same as the Variability of X1. And so each of those guys has variance what? What is the variance of a Bernoulli? We've said. We've now said the varieties of Xi's and X1's are the same. And we've said that each of them has variance. We have now said that the Variables of Xi andX1 are the same. it once. It's theta times 1 minus theta. And so now I'm going to have the sum of n times a constant, so I get n times the constant divided by n squared. So one of the n's is going to cancel, so the whole thing here is actually just a series of n's and n is a constant. So now I have n times n squared, and so I have the total number of n, which is the number of square root of n. equal to theta, 1 minus theta divided by n. So if I'm interested in the quadratic risk-- and again, I should just say risk, because this is the only risk we're going to be actually looking at. Yeah. This parenthesis should really stop here. I really wanted to putquadratic in there. But I'm sorry, I just wanted to say risk. I just want to put risk in there, because that's what we're looking at, and it's the only one we're talking about. in parenthesis. So the risk of this guy is what? Well, it's the expectation of x bar n minus theta squared. And we know it’s the square of the variance, so we know the bias is 0. So it's 0 squared plus the variance. which we know is 0, so it's0 squared plus 0, which is the bias. So we know we’re at 0 squared, which means the risk is 0 square plus 0 squared. is theta, 1 plus theta-- 1 minus theta divided by n. So it's just theta,. 1 minus  divided by  is theta. So this is just summarizing the performance of an estimator, which is the random variable. I mean, it's complicated. If I really wanted to describe it, I would just say, 'It's just a function of the number of variables that make up a random variable' That's what I would do. tell you the entire distribution of this random variable. But now what I'm doing is I'm saying, well, let's just take this random. variable, remove theta from it, and see how small the fluctuations around theta. are in expectation. So that's what the quadratic risk is, right? That's what we're trying to get out of here. We want to get to the bottom of it. We don't want to go to the top of it, but we want to find out what the bottom is. is doing. And in a way, this decomposition, as the sum of the bias square and the variance, is really telling you that. It is really accounting for the bias, which is, well, even if I had an infinite amount of observations, is this thing doing the right thing? And in this way, it tells you that it is doing what it should be doing, and it's doing it in a very specific way. It's not going to do it all the time, but it's trying to. the other thing is actually the variance, so for finite number of observations, what are the fluctuations? All right. Then you can see that those things, bias and variance, are actually very different. So I don't have any colors here, so you're going to have to really follow the speed. So you're Going to Have to Really Follow the Speed. You're Going To Have To Follow The Speed. So You're going To Have to Follow The speed. The speed is the speed of the data. the order in which I draw those curves. All right. So let's find-- I'm going to give you three candidate estimators, so-- estimators for theta. So the first one is definitely Xn bar. That will be a good candidate estimator. The second one is going to be 0.5, because after after after 0.4, the third one is 0.6. The fourth one is 1.5. The fifth one is 2.0. The sixth one is 3.2. all, why should I bother if it's actually going to be-- right? So for example, if I ask you to predict. the score of some candidate in some election, then since you know it's going to. be very close to 0.5, you might as well just throw0.5 and you're. right? If you know you're going to lose, you're not going to try to predict the outcome. You're just going to go with your gut and hope for the best. not going to be very far from reality. And it's actually going to cost you 0 time and $0 to come up with that. So sometimes maybe just a good old guess is actually doing the job for you. Of course, for presidential elections or something like this, it's not going to go very far at all. It's not a good idea to try to predict the outcome of a presidential election. It is a good thing to be able to take a guess and go with it. very helpful if your prediction is telling you this. But if it was something different, that would be a good way to generate some close to 1/2. For a coin, for example, if I give you a coin,. you never know. Maybe it's slightly biased. But the good guess, just for now, is that it's going to be a 1/4 coin or a 2/2 coin. If it's not, it's probably a coin that's not going to go very far. looking at it, inspecting it, maybe there's something crazy happening with the structure of it, you're going to guess that it's 0.5 without trying to collect information. And let's find another one, which is, well, you know, I have a lot of observations. But I'm recording couples kissing, but I'm not trying to get any information out of it. I'm just trying to see if I can get some information from it, which I think I can. on a budget. I don't have time to travel all around the world and collect some people. So really, I'm just going to look at the first couple and go home. So my other estimator is just to be X1. I just take the first observation, 0, 1, and 0,1, and 1,0,1. And that's what I'm going to do for the rest of the film. I'm not going to get into too much detail, but I'll just say it's going to be a lot of fun. that's it. So now I'm going-- I want to actually understand what the behavior of those guys is. All right. So we know-- and so we know that for this guy, the bias is 0 and the variance is equal to theta, 1 minus theta divided by n. What is the variance of this guy? It's 0.5. It's just a deterministic number. So the bias, 0.4 minus 0.3, is 0.6. there's no fluctuations for this guy. What is the bias? Well, X1 is actually-- just for simplicity, I can think of it as being X1 bar, the average of itself, so that wherever I saw an n for this person, I could replace it by 1 and that will give me an n that is the same as the n for the other guy. That's how you get the same n for both of these guys. It's a very simple way to do it. Theta is equal to theta, 1 minus theta. So the bias is still going to be 0. So now I have those three estimators. Well, if I compare X1 and Xn bar, then clearly I have 0 bias in both cases. That's. my formula. And the variance is going to. be equal to 1 minus 1. So that's the formula. The formula is: 1 plus 1. And that's my formula for the variance of the X1 bar to the Yn bar. good. And I have the variance that's actually n times smaller when I use my n observations than when I don't. So those two guys, on these two fronts, you can actually look at the two numbers and say, well, the first number is the same. The second number is good, and I have to use my observations more often to get the results I want. And that's a good thing, because it means I'm getting better at what I'm trying to do. better for the other guy, so I will definitely go for this guy compared to this guy. Well, if I look at the bias, the variance is 0. It's always beating the variance of this man. So this guy is gone. But not this guy, because he's always going to beat this guy's variance. And if you look at his variance, it's always better than this man's. So I'm going to go with this guy over the other one. And I'm not going to change my mind. the bias, it's actually really not that bad. In particular, if theta is 0.5, then this guy is strictly better. And so you can actually now look at what the quadratic risk looks like. So here, what we're going to do is I'm going to take the bias and the theta. It's 0.4 minus theta, and that's what the bias is. And that's the risk of being in the wrong place at the wrong time. That's the bias. The risk is that you're in the right place but not at the right time. my true theta-- so it's going to range between 0 and 1. And we know that those two things are functions of theta, so I can only understand them if I plot them. And so now I'm going to actually plot-- the y-axis is going to be going to 0. and the x-axis will be going from 1 to 0, and so on. And I'll show you how to do it in the next section of this article. Back to Mail Online home. back to the page you came from. be the risk. So what is the risk of the estimator of 0.5? This one is easy. Well, it's 0 plus the square of0.5 minus theta. So when theta is very close to 0. 5, I'm very happy. When theta gets farther, it’s a little bit annoying. And then here, I want to plot therisk of this guy. So now the thing with the risk with this guy is that it will depend on n. So I will just just show you how it works. pick some n that I'm happy with just so that I can actually draw a curve. Otherwise, I'm going to have to plot one curve per value of n. So let's just say, for example, that n is equal to 10. And so now I need to plot the function of n to get the shape of the curve I want to draw. For example, let's say n is 10 and I want the shape to look like a circle. I'll draw a circle with a radius of 10 around it. Theta, 1 minus theta divided by 10 is a curve that goes like this. It takes value at 1/2. It thinks value 1/4. That's the maximum. And then it's 0 at the end. So really, if n is equal to 1, this is the shape of theta, theta plus theta. It's a curve with a maximum value of 1/3, and a minimum value of 2/4, and an average value of 4/3. If n is 1, the curve is a circle with a radius of 1. what the variance looks like. The bias doesn't count in the risk. Yeah. AUDIENCE: [INAUDIBLE] PHILIPPE RIGOLLET: Sure. Can you move? All right. Are you guys good? Allright. So now I have this picture. And I know I'm going up to 25. And there's a place where those curves look like they're going to go. And it looks like it's going to be a lot of work. Cross: Maybe you're actually better by predicting 0.5 if you know it's not going to go too far. But that's for one observation, so that's the risk of predicting too much. Cross: So if you're sure-- let's say you're talking about presidential election, you know that those things are going to be really close. You know that you're going to have to be very, very close to predict them. That's a risk of being too sure. X1. But if I look at the risk of Xn, all I'm doing is just crushing this curve down to 0. So as n increases, it's going to look more and more like this. It's the same curve divided by n. And so now I can just start to understand what's going on with X1. X1 is the number of people who have a risk of being killed by Xn. Xn is the amount of risk a person has that they can't afford to take. that for different values of thetas, now I'm going to have to be very close to theta is equal to 1/2 if I want to start saying that Xn bar is worse than the naive estimator 0.5. Yeah. AUDIENCE: Sorry. I know you explained a little bit before, but can you can explain a little more? Yeah. I'm sorry, I'm not sure what I said. I was just trying to explain a bit more about how theta works. you just-- what is an intuitive definition of risk? What is it actually describing? PHILIPPE RIGOLLET: So either you can-- well, when you have an unbiased estimator, it's simple. It's just telling you it's the variance, because the theta that you have over there is really-- so in the definition of the risk, theta is really the expectation of theta hat. So the risk is really telling you how much fluctuations I have around my expectation if unbiased. fluctuations I have in average around theta. So if you understand the notion of variance as being-- AUDIENCE: [INAUDIBLE] PHILIPPE RIGOLLET: What? AUDience: Like variance on average. PHILipPE Rigollet: No. It's just like variance. No. I mean, if you-- I mean to say it's like theta, right? So when you're talking about variance, that's what I'm talking about. Claim you understand what variance is, it's telling you what is the expected squared fluctuation around the expectation of my random variable. It's just telling you on average how far I'm going to be. And you take the square because you want to cancel the signs. Otherwise, you're going to go to the other end of the spectrum and end up in the wrong place. Claim you understand how variance works, and you can use it to make your own predictions about the future. If you don't, you may have to go back to the drawing board. get 0.0. Get 0. 0. Get 1. Get 2. Get 3. Get 4. Get 5. Get 6. Get 7. Get 8. Get 9. Get 10. Get 11. Get 12. Get 13. Get 14. Get 15. Get 16. Get 17. Get 18. Get 19. Get 20. Get 21. Get 22. Get 23. Get 24. Get 25. Get 26. Get 27. Get 28. Get 29. Get 30. the variance. But if I'm biased, then I have to account for the fact that I'm really not computing the-- AUDIENCE: OK. Thanks. PHILIPPE RIGOLLET: OK? All right. Are there any questions? So here, what I really want to illustrate is that the risk itself is a function of the variance. The variance is the difference between the risk and the reward. The risk is the same, but the reward is different. The reward is the risk, and it's the reward that determines the risk. theta most of the times. And so for different thetas, some estimators are going to be better than others. But there's also the entire range of estimators, those that are really biased, but the bias can completely vanish. So here, you see you have no bias,but the variance is much higher than in the other cases. So you have to be very careful with your estimators to get the best results. It's a very, very difficult task. can be large. Or you have 0 bias-- you have a bias, but the variance is 0. So you can actually have this trade-off and you can find things that are in the entire range in general. So those things are actually-- those trade-offs between bias and variance are usually usually not very large, but they can be very large. And that's what we're trying to find out here. We're looking for things that can be in that range, and we can find them. much better illustrated if we're talking about multivariate parameters. If I actually look at a parameter which is the mean of some multivariate Gaussian, so an entire vector, then the bias is going to be bigger. I can make the bias bigger by, for example, forcing all the coordinates of my estimator to be in the same place. That's how you get a bias of 1 in a million or 1 in 1,000,000 for a Gaussian. It's the same for a multivariable Gaussian as it is for a single-parameter. to be the same. So here, I'm going to get some bias, but the variance is actually going to be much better, because I get to average all the coordinates for this guy. And so really, the bias/variance trade-off is when you have multiple parameters to estimate, so you have to take into account some of the effects of the other parameters as well as the ones you have chosen to estimate for this particular case. And that's where the bias is going to come from. a vector of parameters, a multivariate parameter, the bias increases when you're trying to pull more information across the different components to actually have a lower variance. So the more you average, the lower the variance. That's exactly what we've illustrated. As n increases, the variance decreases, like 1 over 1 over n. The more information you have, the more likely it is that you'll get a lower variance on a given number of parameters. This is the case with multivariate parameters. n or theta, 1 minus theta over n. And so this is how it happens in general. In this class, it's mostly one-dimensional parameter estimation, so it's going to be a little harder to illustrate that. But if you do, for example, non-parametric estimation, that's all you do. There's just one way to do it, and it's called theta-theta estimation. And that's how it works in this class. bias/variance trade-offs all the time. And in between, when you have high-dimensional parametric estimation, that happens a lot as well. OK. So I'm just going to go quickly through those two remaining slides, because we've actually seen them. But I just wanted you to have somewhere a formal definition of bias/ Variance Trade-offs. And I just want to give you a little bit of an example of what that looks like. So, that's the first slide. The second slide is the second. what a confidence interval is. And so we fixed a statistical model for n observations, X1 to Xn. The parameter theta here is one-dimensional. Theta is a subset of the real line, and that's why I talk about intervals. An interval is a subsets of the line. If I had known that, I would have used a different model, but I didn't have the time to do it. I'm glad I had the time. It was a great learning experience. a subset of R2, for example, that would no longer be called an interval, but a region, just because-- well, that's just we can say a set, a confidence set. But people like to say confidence region. So an interval is just a one-dimensional conference region. And it has to be one of these types of region. For example, an interval would be a region if it was a subset of the R2 set. It would be called a confidence region, or a confidence interval. be an interval as well. A confidence interval of level 1 minus alpha-- so we refer to the quality of a confidence interval is actually called it's level. It takes value 1 minusalpha for some positive alpha. And so the confidence level -- the level of the confidence interval -- is called the confidence interval level. The confidence level is the level at which the interval is considered to be of good quality. It is the same level as a confidence  interval. It's the level that gives you the confidence that it's an interval. is between 0 and 1. The closer to 1 it is, the better the confidence interval. And so for any random interval-- so a confidence interval is a random interval. The bounds of this interval depends on random data. Just like we do with the confidence intervals in this article. For more information, visit the website of the National Institute of Standards and Technology (NIST) in Washington, D.C., or go to the NIST website at: http://www.nist.org/. had X bar plus/minus 1 over square root of n, for example, or 2 over squareroot of n. This X bar was the random thing that would make fluctuate those guys. And so now I have an interval. And now I've got its boundaries, but now the boundaries are not the same as the previous ones. I've also got the boundaries of the interval, but they're different from each other. I'm not sure how I'm going to get to the end of this. not allowed to depend on my unknown parameter. The confidence interval has to be something that I can compute once I collect data. Otherwise, it's not a confidence interval, just like an estimator that depends on the unknown parameter is not an estimators. And so what I want is that-- what I Want is that -- the confidence interval is something that you can compute after you've collected the data. And that's what I'm trying to achieve here in this article. It's a bit of a compromise, but it's a good one. so there's this weird notation. The fact that I write theta-- that's the probability that I contains theta. You're used to seeing theta belongs to I. But here, I really want to emphasize that the randomness is in I. And so the way you actually say it when you read it is I, not the other way around. That's the way I like to write it. I don't want to make it sound like I'm trying to say something else. this formula is the probability that I contains theta is at least 1 minus alpha. So it better be close to 1. You want 1minus alpha to be very close to1, because it's really telling you that whatever random variable I'm giving you, my error bars are actually error bars. Theta is the ratio of theta to theta, with alpha being the difference between theta and theta. This formula is based on the fact that theta is at least 1 minus alpha, and alpha is the ratio of theta to the ta. covering the right theta. And I want this to be true. But I don't know what my confidence-- my parameter of theta is, I want it to hold true for all possible values of the parameters that nature may have come up with from. So I want to make sure that this is the case for all of the possible values that nature could have come out with, and I want theta to be the right value for that. I want my confidence to be in the right place. want this-- so there's theta that changes here, so the distribution of the interval is actually changing with theta hopefully. And theta is changing with this guy. So regardless of the value of theta. that I'm getting, I want that the probability that it contains the theta to be larger. I want it to be more likely that the interval contains more theta than it doesn't. That's what I'm trying to achieve here. I'm hoping to get a bigger interval, but I don't know if that's possible. than 1 minus alpha. So I'll come back to it in a second. I just want to say that here, we can talk about asymptotic level. And that's typically when you use central limit theorem to compute this guy. Then you're not guaranteed that the value is at least 1 plus alpha. But that's what we're trying to get at here, so we're going to talk about that in the next section of this article. Back to the page you came from. minus alpha for every n, but it's actually in the limit larger than 1 minus alpha. So maybe for each fixed n it's going to be not true. But for as no goes to infinity, it'll actually become true. If you want this to hold for each n, you have to have a fixed n of 1 minus 1. For every fixed n, this would be true if you had a fixed number of fixed n's and a fixed alpha. But if you have more than one fixed n then it's not true, so you need a fixed value of 1 plus 1. you actually need to use things such as Hoeffding's inequality that we described at some point, that hold for every n. So as a rule of thumb, if you use the central limit theorem, you're dealing with a confidence interval with asymptotic level 1 minus alpha. And the reason is because of the reason we said that the confidence interval is 1 plus alpha. It's the same as saying that if you say the confidence intervals are 1 plus 1, you are dealing with an interval with alpha 1 minus 1. because you actually want to get the quintiles of the normal-- the Gaussian distribution that comes from the central limit theorem. And if you want to use Hoeffding's, for example, you might actually get away with a confidence interval that's actually true even non-asymptotically. It's just the regular confidence interval. And it's true even if the confidence interval is not as large as it appears to be, which is the case in this case. So this is the formal definition. It's a bit of a mouthful. But we actually-- the best way to understand them is to build them. Now, at some point I said-- and I think it was part of the homework-- so here, I really say the probability the true parameter. That's what I mean by the probability of winning the game. And I think that's the way to go, too. I mean, that's what we're trying to do. We're not trying to win the game, we're just building it. belongs to the confidence interval is actually 1 minus alpha. And so that's because here, this confidence intervals is still a random variable. Now, if I start plugging in numbers instead of the random variables X1 to Xn, I start putting 1,0, 0, 1, 0,. 0, 0. 1, like I did in the previous example. And that's how we get the confidence intervals in this example. It's the same with the other example, too. did for the kiss example, then in this case, the random interval is actually going to be 0.42, 0.65. And this guy, the probability that theta belongs to it is not 1 minus alpha. It's either 0 if it's not in there or it's 1 if it is in there. So in this example, if theta was in there, then the probability of it being in there was 0, not 1, and it was 1. It was 1, not 0. here is the example that we had. So just let's look at back into our favorite example, which is the average of Bernoulli random variables, so we studied that maybe that's the third time already. So the sample average, Xn bar, is a strongly consistent estimator of p. That was the example we were looking at. We were trying to find a way to get a more consistent estimate of p, which would be a function of the size of the sample. one of the properties that we wanted. Strongly consistent means that as n goes to infinity, it converges almost surely to the true parameter. That's the strong law of large number. It is consistent also, because it's strongly consistent, so it also converges in probability, which makes it consistent. It's a very strong property. We call it the 'Strong Law of Large Number' and it's a strong property of a large number as well as a property of probability. Thanks to the central limit theorem, we actually did this. We built a confidence interval at level 1 minus alpha. We've actually computed its quadratic risk. And now what I have is that if I look at-- thanks to thecentral limit theorem,. We actually do this. And we've seen that. We have seen that we're notbiased. We're not biased. We are not biased, we are notbiased, we're just not biased in the way that we think we are. so here, this is how we did it. Let me just go through it again. So we know from the central limit theorem-- so the central. limit theorem tells us that Xn bar minus p divided by square root of p1 minus p, square. root of n converges in distribution. So, we can say that if Xn bars minus p divides by squareroot of p 1 minus p we have a distribution of n. We can then say that this distribution is the same as the distribution of xn bar plus p. as n goes to infinity to some standard normal distribution. So what it means is that if I look at the probability under the true p, that's square root of n, Xn bar minus p divided by square root. of p1 minus p, it's less than Q alpha over 2, and it's under Q alpha of 2. It means that the probability is less than 1 in 2, or 1 in 1,000,000 under the normal distribution of n. It's also under 1 in 10,000 if n is a positive number. where this is the definition of the quintile. Then this guy-- and I'm actually going to use the same notation, limit as n goes to infinity. This is the same thing. So this is going to be equal to 1 minus alpha. That's exactly what I did last time. And I'm going to do the same with this one, too. I'm just going to show you how to do it. I'll be back in a few days with a new set of questions. This is by definition of the quintile of a standard Gaussian and of a limit in distribution. So the probabilities computed on this guy in the limit converges to the probability computed onthis guy. And we know that this is just the probability that the absolute value of sum is greater than the sum of all the probabilities for this guy. This is the probability of the sum being greater than the absolute value of the sum of probabilities for this guy. n 0, 1 is less than Q alpha over 2. And so in particular, if it's equal, then I can put some larger than or equal to, which guarantees my asymptotic confidence level. And I just solve for p. So this is equivalent to the limit as n goes to, and it's the same as the confidence level of the n-th power of a function of a power of 2, where n is the number of square roots of the power of n. infinity of the probability that theta is. between Xn bar minus Q alpha over 2 divided by-- times square root of p1 minus p divided by squareroot of n, Xnbar plus q alpha over. 2, square root  of p 1 minus p  divided bysquare root of n is theta. Theta is the ratio of squares of square roots of p and p1 to square root of p1 plus p to square root of p. larger than or equal to 1 minus alpha. And so there you go. I have my confidence interval. Except that's not, right? We just said that the bounds of a confidence interval may not depend on the unknown parameter. And here, they do. So we actually came up with a new confidence interval, which is the same as the confidence interval in the previous version of this article. It's the same confidence interval as the previous one, but it's bigger than the first one. two ways of getting rid of this. Since we only need this thing-- so this thing, as we said, is really equal. Every time I'm going to make this guy smaller and this guy larger, I'm only going to increase the probability. And so what we do is we actually make the thing smaller and the other guy larger. That's how we get rid of it. We make it smaller and we make the other person larger. And that's the way we get it. just take the largest possible value for p1 minus p, which makes the interval as large as possible. And so now I have this. I just do one of the two tricks. I replace p1 plus p by their upper bound, which is 1/4. As we said, p1minus p, As we say, p2plus p, p3+ p, and p4+ p. And now we have this: p2+ p3, p4 + p4, p5+ p5, p6+ p6, p7+ p7, p8+ p8, p9+ p9, p10+ p10, p11+ p11, p12+ p12, p13+ p13, p14+ p14, p15+ p15, p16+. the function looks like this. So I just take the value here at 1/2. Or, I can use Slutsky and say that if I replace p by Xn bar, that's the same as just replacing p byXn bar here. And by Slutski, we know that this is actually converging. And so we can use this to get the result we're looking for. The result is that the function is converging to a value of 1.2. also to some standard Gaussian. We've seen that when we saw Slutsky as an example. And so those two things-- actually, just because I'm taking the limit and I'm only caring about the asymptotic confidence level, I can actually just plug in consistent quantities in there, such as Xn bar. And that's what we're going to do in the next few minutes. We'll see how that works out for us. We're not going to go into too much detail. where I don't have a p. And that gives me another confidence interval. All right. So this by now, hopefully after doing it three times, you should really, really be comfortable with just creating this confidence interval, he says. "I think you probably did it," he says, "and that gives you a confidence interval," he adds. "And that's where you can do it again. And again, and again and again, until you get it right. And then you can go back and do it a third time" another couple times in your homework. So just make sure you're comfortable with this. That's one of the basic things you would want to know. Are there any questions? Yes. So Slutsky holds for any single response set p. But Xn converges [INAUDIBLE]. PHILIPPE RIGOLLET: So that's a good question. All right. So that was the first question. Is there a second question? Yes, there was a second. not Slutsky, right? AUDIENCE: That's [INAUDIBLE]. PHILIPPE RIGOLLET: So Slutsy's about combining two types of convergence. So Slutssky tells you that if you actually have one Xn that converges to X in distribution and Yn that convergence to Y in probability, then you can. Then you can do something like this, he says. Slutsey: If you have one Yn and one XN that converge to X, you can create a new type of distribution. actually multiply Xn and Yn and get that the limit in distribution is the product of X and Y, where X is now a constant. And here we have the constant, which is 1. But I did that already, right? Using Slutsky to replace it for the-- to replace P for the limit of a function. But we're not going to do that here. We're going to use the same technique. We'll just replace P with Xn instead of Xn. by Xn bar, we've done that last time, maybe a couple of times ago, actually. Yeah. PHILIPPE RIGOLLET: So of course, the short answer is [INAUDIBLE] can we set a finite [INAudIBLE]. PHILipPE RigOLLET. “We’re going to have to wait and see,” he says. � “I’m not going to be able to predict the future.” is no. So there's always the more conservative method. The first one, the only thing you're losing is the rate of convergence of the central limit theorem. So if n is large enough so that the central. limit theorem is true, then there's no need to use the second method. So here's how you would go about thinking about which method is better. The second method is the one you use if you want to find the answer to the question, "What is the best way to solve a game of Monopoly?" limit theorem approximation is very good, then that's all you're going to be losing. Of course, the price you pay is that your confidence interval is wider than it would be if you were to use Slutsky for this particular problem, typically wider. Actually, it is always wider, because Xn is always bigger than the confidence interval you're trying to work with, so you're always going to have a wider confidence interval than you would if you used Slutky's approximation. Bar is always less than 1/4 as well. Slutsky basically adds your relying on the central limit. Now of course, you don't want to be conservative, because you actually want to squeeze out the bar. And so that's the first thing you-- so Slutski basically adds. Your relying on asymptotics again. 1 minus Xn bar is always more than 1-1/2 as well as 1/2 plus Xn. 1 plus 1-2 is more than 2-2, and 1 plus 2 is 3-2. as much from your data as you can. So it depends on how comfortable and how critical it is for you to put valid error bars. If they're valid in the asymptotics, then maybe you're actually going to go with Slutsky so it actually gives you slightly narrower confidence intervals. If you're not comfortable with that, then you might want to use a different approach. For more information on how to get the most out of your data, go to www.cnn.com/sales. and so you feel like you're a little more-- you have a more precise answer. Now, if you really need to be super-conservative, then you're actually going to go with the P1 minus P. If you need to go even more conservative, go with Hoeffding's P1-P1-1. And if you want to be a little less conservative, then go with P1 plus P1. and P1 + P1 is the most conservative answer. so you don't even have to rely on the asymptotics level at all. But then you're confidence interval becomes twice as wide and twice asWide and it becomes wider and wider as you go. So depends on-- I mean, there's a lot of data in statistics which is gauging. But there's also a lot to be said about the way in which you measure confidence intervals. So it's not just a matter of looking at the data, it's also about how you measure it. how critical it is for you to output valid error bounds or if they're really just here to be indicative of the precision of the estimator you gave from a more qualitative perspective. AUDIENCE: So the error there is [INAUDIBLE]?? PHILIPPE RIGOLLET: Yeah. So here, there's basically a bunch of errors that are basically not significant enough to show up in the error bounds, so we're going to use a different way of looking at it. errors. There's one that's-- so there's a theorem called Berry-Esseen that quantifies how far this probability is from 1 minus alpha, but the constants are terrible. So it's not very helpful, but it tells you as n grows how smaller this thing grows-- becomes smaller. And then for Slutsky, again, again and again, it's the same thing. It's a very, very bad way to look at the probability of something happening. It just doesn't add up. you're multiplying something that converges by something that fluctuates around 1, so you need to understand how this thing fluctuates. Now, there's something that shows up. Basically, what is the slope of the function 1 over square root of X1 minus X around the value you're interested in? And so on and so on, until you get to the answer you're looking for. And so that's what we're trying to do here. We want to get the answer to the question you're asking. if this function is super-sharp, then small fluctuations of Xn bar around this expectation are going to lead to really high fluctuations of the function itself. So if you're looking at-- if you have f of XN bar and f around say the true P, if f is really sharp, then f is going to fluctuate a lot. If f is not sharp enough, then the P is not going to be as accurate as it would have been if f was really sharp. like that, then if you move a little bit here, then you're going to move really a lot on the y-axis. So that's what the function here-- the function you're interested in is 1 over square root of X1 minus X. So what does this function look like around the x-axis? It looks like a lot of movement around the square root. It's like moving around a circle around the circle. It looks a lot like a cube. It moves around a cube around the cube. point where you think P is the true parameter? Its derivative really is what matters. Any other question. OK. So it's important, because now we're going to switch to the real let's do some hardcore computation type of things. All right. So in this chapter, we's going to talk about the real computation. OK? Any other questions? OK. We'll be back next week with the next chapter in the series, which will focus on the theory of complex numbers. about maximum likelihood estimation. So those things are when we domaximum likelihood estimation, likelihood is the function, so we need to know how to use it. OK. Who has already seen maximumlihood estimation? And who knows what a convex function is? OK. So we'll do a little bit of reminders on those things. So these are the things you need to do when you're doing maximum likelihood estimations. We're going to go through them one by one and see if we can figure out what they mean. maximize a function. That's basically what we need to do. And if I give you a function, you need to know how to maximize this function. Sometimes, you have closed-form solutions. You can take the derivative and set it equal to 0 and solve it. But sometimes, you actually need to solve it in a different way. And that's what we're trying to do in this article. We're going to show you how to do it in the next section. to resort to algorithms to do that. There's an entire industry doing that. And we'll briefly touch upon it, but this is definitely not the focus of this class. OK. So before diving directly into the definition of the likelihood and what is the definition. of the maximum likelihood, we want to make sure we're clear about what we're talking about. We want to be clear that we are talking about the likelihood of a certain outcome, not the probability of a particular outcome. We don't want to give the impression that we're saying that the outcome is certain. estimator, what I'm going to try to do is to give you an insight for what we're actually doing when we do maximum likelihood estimation. So remember, we have a model on a sample space E and some candidate distributions P theta. And really, your goal is to estimate a maximum likelihood of a particular distribution, and we're trying to do that in this case. We're using a model that is based on E, P theTA, and a set of candidate distributions. true theta star, the one that generated some data, X1 to Xn, in an iid fashion. The goal of knowing theta. star is so that you can actually know what P theTA star. Otherwise, it has-- well, sometimes we said it has some meaning itself, but really you want to know what the distribution is. And so your goal is to actually come up with the distribution-- hopefully that comes from the family P theta-- that's close to P thea star. So in a way, what does it mean to have two distributions that are close? It means that when you compute probabilities on one distribution, you should have the same probability on the other distribution pretty much. So what we can do is say, well, now I have two candidate distributions, and I have to choose one of them. So I can choose the one that is most likely to be close to the other one. And that's the one I choose. And it's a pretty good choice. distributions. As a statistician, I'm supposed to come up with a good distribution. So if theta hat leads to a candidate distribution P thetahat, and this is the true theta star, it leads to the true distribution P Theta star according to which my data was drawn. That's my candidate. The true distribution is the one I used to draw my data. That is the P TheTA star. The theta is the most common distribution in the universe. candidate, and this is the truth. And what I want is that if you actually give me the distribution, then I want when I'm computing probabilities for this guy, I know what the probabilities for the other guys are. And so really what I Want is that. if I compute probabilities, I want to know what they are for all the other candidates, not just this one. And that's what I'm trying to achieve here, I'm hoping that we can get to that point. a probability under theta hat of some interval a, b, it should be pretty close to the probability under. theta star of a, a. And more generally, if I want to take the union of two intervals, I want this to be true. If I take just 1/2 lines, I should get the same result as if I took 1/3 of a line and 1/4 of a lines. And if I take 2/3 lines and 2/4 lines, the result should be the same. want this to be true from 0 to infinity, for example, things like this. And so what I do is that I write A for a probability event. And I want that P hat of A is close to being true for all of them at once. That's what I'm trying to do here. I want to make sure that A is true for every probability event in the world. I don't want it to just be true for one probability event, but for all probability events in the universe. to P star of A for any event A in the sample space. Does that sound like a reasonable goal for a statistician? So in particular, if I want those to be close, I want the absolute value of their difference to beclose to 0. And this turns out to be the case, at least for the first time in the history of the study. It's the first of a series of tests to look at the relationship between events in a sample space and their overall value. to be-- if I want this to hold for all possible A's, I have all possible events. And I'm going to look at the worst possible event on which theta hat can depart from theta star. And so rather than defining theta, I want to maximize over these events. That's what I'm trying to do here. I'm looking for the best possible outcome, and I'm hoping that's what this is going to turn out to be. I want it to be the best of all possible worlds. it specifically for theta hat and theta star. If I want to measure how close they are by how they can differ, I'm just going to say, well, if you give me two probability measures, P theta and P theTA prime, I want. to know how close. They're very different, but they're very, very close to each other, and that's how I'm going to try to measure it. It's a pretty good way to get a sense of how close the two are. when I measure the probability of some event, I'm just looking at the absolute value of the difference of the probabilities. That's a pretty strong notion. So if the total variation between theta and theta is greater than 1, then the probability is higher than 0.1. And I'm maximizing over the worst possible event that might actually make them differ. Agreed? that's aPretty strong notion, right? Well, it's true. It's also true that if the probability was 0.01, the probability would be 1.0. theta prime is small, it means that for all possible A's that you give me, then P theta of A is going to be close to P thena prime of A. All right. Let's say I just found the bound on the total variation distance, which is 0.01. If-- let's say we just found that the bound was 0.1, then we can say that P theda prime is the same as P theca prime. That's the same thing as saying P thea prime is equal to P thetaprime. So that means that this is going to be larger than the max over A of P theta minus P thena prime of A. And for any A-- actually, let me write P theTA hat and P thea star, like we said, theta hat and theta star. And that's what we're trying to do here. We're going to try to make it bigger than A of theta plus theta prime. And we're hoping that we can do that with any A. so if I have a bound, say, on the total variation, which is 0.01, that means that P theta hat-- every time I compute a probability on P thea hat, it's basically in the interval P theTA star of A, the one that I really wanted to compute, plus or minus one. That's what I'm trying to get at with the probability of a black hole in the universe. The probability of the black hole being in the right place at the right time is about 1 in a million. minus 0.01. This has nothing to do with confidence interval. This is just telling me how far I am from the value of actually trying to compute. That's where this max comes into play. It just says, I want this bound. And that's true for all A.S.A.R.E.M. calculations. It's not about the confidence interval, it's about how far away from it you are from the actual value you're trying to get. to hold for all possible A's at once. So this is actually a very well-known distance between probability measures. It's extremely central to probabilistic analysis. And it essentially tells you that every time-- if two probability distributions are close, then it means that everytime every time, the A's are likely to be true for all the possible A’s in the world. It’ll be interesting to see how this plays out in the future. It could be very interesting. I compute a probability under P theta but I really actually have data from P theTA prime, then the error is no larger than the total variation. OK. So this is maybe not the most convenient way of finding a distance. I mean, how are you going-- in reality, how do you go about finding the distance? I don't know. I'm just going to say that I'm going to try to find the distance. And I'll let you know if I get it. are you to compute this maximum over all possible events? I mean, it's just crazy, right? There's an infinite number of them. It's much larger than the number of intervals, for example, so it's a bit annoying. And so there's actually a way to compress it by just looking at the number. of events that have taken place in the last 24 hours. That's how you get the maximum number of events in a given time period. And that's how I got the maximum. the basically function distance or vector distance between probability mass functions or probability density functions. So I'm going to start with the discrete version of the total variation. So throughout this chapter, I will make the difference between discrete random variables and continuous random variables. It really doesn't matter. All of it is the same. It's just a different way of looking at the same thing. So that's what we're going to focus on in this chapter. We're not going to look at the whole thing. it means is that when I talk about discrete, I will talk about probability mass functions. And when I talks about continuous, I'll talk about probability density functions. When I talkabout probability mass function sums, I'm talking about sums. And I'm also talking about probability density function sums. So that's what it means to be a discrete and a continuous. It means that when you talk about a discrete function, you're talking about discrete sums. It's the same thing with a continuous function. integrals. But they're all the same thing, really. So let's start with the probability mass function. This is the function that tells me for each possible value that it can take, the probability that it takes this. It's a probability function of a discrete random variable. It tells me what the probability of a given value of a random variable is for that value. It is a function of the type of random variable that we have in our system. We can use it to estimate the probabilities of different values of a variable. The Probability Mass Function, PMF, is just the function for all x in the sample space. PMF tells me the probability that my random variable is equal to this little value. And I will denote it by P sub theta of X. So what I want is, of course, a random variable with a value of 0.0. Value of 0 is the random variable's random variable.value. The probability of this variable being equal to 0 is given by the PMF. that the sum of the probabilities is 1. And I want them to be non-negative. Actually, typically we will assume that they are positive. Otherwise, we can just remove this x from the sample space. And so then I have the total variation distance, I mean, it's supposed to be 1. That's what I'm trying to get. I want it to be a non-negligible number of degrees of freedom. I don't want to get it too high or too low. the maximum overall sets of-- of subsets of E, such that the probability of A minus probability of theta prime of A-- it's complicated, but really there's this beautiful formula that tells me that if I look at the total variation between P theta and P theTA prime, it's actually quite low. The maximum overallsets of E are the subsets that have the highest probability of having a prime in each of the two prime sets of E. They are called the "maximum overall sets" of E and are the "maxima sets" equal to just 1/2 of the sum for all X in E of the absolute difference between P theta X and P theTA prime of X. So that's something you can compute. If I give you two probability mass functions, you can computed this immediately. But if I give them both, you have to work out the difference between the two probabilities. That's a very difficult thing to do, but you can do it if you know how to do it. It's not that hard to do. just the densities and the original distribution, the original definition where you have to max over all possible events, it's not clear you're going to be able to do that very quickly. So this is really the one you can work with. But the other one is really telling you what it is you need to do, and that's the one we're trying to work with now. We'll have to see how it pans out over the next few weeks or so to see if we can make any progress. what it is doing for you. It's controlling the difference of probabilities you can compute on any event. But here, it is just telling you, well, if you do it for each simple event, it's little x. Now, if we have continuous random variables-- so by the way, by theway, we're not talking about random variables here. We're talking about continuous random variable probabilities. And that's what it's doing for us here. And it's actually simple.  discrete means Bernoulli, but not only those that have finite support. Poisson distribution can take an infinite number of values, all of which are Poisson-like. Binomial NP has support of size n-- there's n possible values it can take-- but also Poisson. All Poisson distributions can take all values, including Poisson poisson polynomials, which can take infinite numbers of values. The Poisson Poisson Distribution is a binomial distribution that can take any number of Poisson values. It can also take any other type of distribution. the positive integers, non-negative integers. And so now we have also the continuous ones, such as Gaussian, exponential. And what characterizes those guys is that they have a probability density. So the density, remember the way I use my density is when I want to compute the probability of belonging to a certain group of numbers. So I use the probability density to work out how many of those numbers are likely to belong to that group. The probability density is then used to compute that group's probability of being there. to some event A. The probability of X falling to some subset of the real line A is simply the integral of the density on this set. That's the famous area under the curve thing. So since for each possible value, the probability at X-- so I hope you remember -- is the same as the probability of A falling to X. So for every possible value of X, the probabilities at X and A are the same. That is, the same is true for all possible values of X. that stuff. That's just probably something that you must remember from probability. But essentially, we know that the probability that X is equal to little x is 0 for a continuous random variable, for all possible X's. There's just none of them that actually gets weight. So what we have is a set of probabilities that we can use to work out what the odds are of getting a given thing right. And that's what we're going to do with the data. We're not going to use the data to make any predictions about the future. The probability that it's in some interval, say, a, b, this is the integral between A and B of f theta of X, dx. So I have this density, such as the Gaussian one. And the probability of it being in some little region is the same as the density of that region. The probability of being in an interval is also the same, so I have the same density of the interval as I have of the region I'm trying to describe. probability that I belong to the interval a, b is just the area under the curve between A and B. If you don't remember that, please take immediate remedy. So this function f, just like P, is non-negative. And rather than summing to 1, it integrates to 1 when I am in a certain place in the interval. So instead of 1, this function integrates to 0 when I'm in the same place as I was in the previous interval. integrate it over the entire sample space E. And now the total variation, well, it takes basically the same form. I said that you essentially replace sums by integrals when you're dealing with densities. And here, it's just saying, rather than having 1/2 of the sum of the absolute values, you want to have 1% of the whole sum of absolute values. That's what we're trying to get with the densities we're looking at. We want to get 1% out of every 1,000. you have 1/2 of the integral of the absolute value of the difference. Again, if I give you two densities and if you're not too bad at calculus, which you will often be, because there's lots of them you can actually not compute. But if I gave you, for example, two different densities, you have one of the densities of the other. You have one density of the second density of the first density. And so on and so on. two Gaussian densities, exponential minus x squared, blah, blah,. blah, and I say, just compute the total variation distance, you could actually write it as an integral. Now, whether you can actually reduce this integral to some particular number is another story. But you could technically do it. So now, just go ahead and do it, and you'll see what I mean. I mean, you can do it if you know how to do it and you know what to do. you have actually a handle on this thing and you could technically ask Mathematica, whereas asking MathematicA to take the max over all possible events is going to be difficult. All right. So the total variation has some properties. So let's keep on the board the definition that involves, say, a large number of events, and let's say there are more than 100 events that could happen at the same time. Let's say that there are about 100 possible events that happen at once, and we're trying to work out how many of them there are. the densities. So think Gaussian in your mind. And you have two Gaussians, one with mean theta and one withmean theta prime. And I'm looking at the total variation between those two guys. So if I look at P theta minus-- sorry. TV between P thena and P thea, that's what I'm trying to get at. The total variation is the difference between the densities of the twoGaussians. That's what we're looking at here. Prime, this is equal to 1/2 of the integral between f theta, f thea prime. And when I don't write it-- so I don’t write the X, dx but it's there. And then I integrate over E. So what is this thing doing for me? It's just saying, well, if f.prime, f.theta prime,  is equal to  1/2  of  f theta prime. I have-- so think of two Gaussians. For example, I have one that is here and one that's here. So this is let's say f theta, f thena prime. This guy is doing what? It's computing the absolute value of the difference between f and f thea prime. You can check. You have to go to the Gaussian site to see what it's all about. It's a very interesting thing to look at, and it's very interesting to learn. for yourself that graphically, this I can represent as an area not under the curve, but between the curves. Now, this guy is really the integral of the absolute value. So this thing here, this area, this is 2 times the total variation. The scaling is the same as the scaling of the whole system, so it's the same scale as the whole of the system. It's just a different way of looking at it, and it's a different kind of scale. 1/2 really doesn't matter. It's just if I want to have an actual correspondence between the maximum and the other guy, I have to do this. So this is what it looks like. So we have this definition. And so we have a couple of properties that come into this. We have to be able to show that we are in fact in the same place. We can't be in the opposite place at the same time. We need to be in a position where we're in the exact same place at once. The TV of P theta and P thea prime is the same as the TV between P theTA prime and P Theta. If I flip those two, I get the same number. It's actually also true if the TV is symmetric. The TV of theta is also symmetric if it is a positive sign. It is not symmetric when it is negative. It can be a negative sign if the positive sign is negative or positive. It cannot be a positive or negative sign when it's negative. I take the maximum. Those things are completely symmetric in theta and theta prime. You can just flip them. It's non-negative. Is that clear to everyone that this thing is non- negative? I integrate an absolute value, so this thing will give me some non- Negative number. And so if you take that number and add it up, you get a non-Negative number. That's what I do. I take that maximum and I add it to the absolute value. And then I get a Non-negative Number. I integrate this non-negative number, it's going to be a non- negative number. The nice thing is that if TV is equal to zero, then the two distributions, the two probabilities are the same. That means that if the number is not negative, it must be positive. The fact also that it's an area tells me that it will be non- Negative. It will be a positive number if it is in the right place in the area. It's not negative if it's not in the wrong place. It could be a negative number if the area is too big. that for every A, P theta of A is equal to P theTA prime of A. For almost all X, f theta isequal to f, so that for almost all A, p theta prime of X is also equal to f. The first one is to say that if this integral is. equal to 0, that means that for most X,  f theta  is equal to  f. theta. The second is to assume that if the integral is 0, then f is the prime of all X. For most X that means  for most of them, f is a prime of f. theta prime. The only way I can integrate a non-negative and get 0 is that it's 0 pretty much everywhere. And so what it means is that the two densities have to be the samepretty much everywhere, which means that the distributions are the same. But this is not the case, and so we have to take a different way of looking at the space of points in the universe. We call it the "space of points" and it is a space of real numbers. really the way you want to do this, because you have to understand what pretty much everywhere means. That's the formal way of saying it. But let's go to this definition-- which is gone. Yeah. The max of those is the one here. that's the one in the dictionary. The maximum of those are the ones in the Wikipedia definition. That was the one I was going to use, but that's gone too. So I'm going to go with this one. two guys, if this maximum is equal to 0-- I have a maximum of non-negative numbers, their absolute values. Their maximum isequal to 0, well, they better be all equal to0, because if one is not equal to 1, then the maximum is notequal to 1. So if one of these guys is 1, the maximum of this guy is 1. If one of them is not 1, it's not 1. And so on and so on. So you get the idea. those two guys, for those two things to be-- for the maximum to be equal to 0, then each of the individual absolute values have to beequal to 0. So those two thing-- for a probability here is equal to this probability here for every event A, that means that the probability of this event A is the same as this probability of that event A. And that's what we're trying to get here. We want to get to a point where we can say that we know what the probability is of this thing happening. This is nice, right? That's called definiteness. If this thing being small implied that P theta could be all over the place, that's not what we want. So that's really some notion of distance. That's what we're after. We want to be able to say that the total variation equal to 0 implies that P Theta is equal to P Thea prime. That would be nice. And we want to know that we're not talking about something that's too small. compared to P theta prime, that would not help very much. Now, there's also the triangle inequality that follows immediately from the Triangle inequality inside this guy. If I squeeze in some f thetaprime prime in there, I'm going to use the triangleequality and get the triangle equality. That would be a good way to get some of the information in this article. If you have any questions, please contact us at jennifer.smith@mailonline.co.uk. for the whole thing. AUDIENCE: The fact that you need two definitions of the [INAUDIBLE],, is it something obvious or is it complete? PHILIPPE RIGOLLET: I'll do it for you now. So let's just prove that those two things are actually giving me the same definition. So what I'm saying is that there are two ways of looking at the world. And the way that the world looks is very different from the way we see it. going to do is I'm actually going to start with the second one. And I'm going to write-- I'mgoing to begin with the density version. But as an exercise, you can do it for the PMF version if you prefer. So I'm Going to Start with the fact that I'm a guy who likes to write. I like to write about things that I think are interesting. And so, I'll start with that. I'll write about what I think is interesting. f-- so I'm going to write f of g so I don't have to writing f and g. I just don't want to have to write indices all the time. Think of this as being f sub theta, and think of this guy as beingf sub theTA prime. That's what I'm trying to do here. I'm not trying to make this into an index. It's just a different way of looking at the world. I want it to be a little bit different. So I'm going to start with this thing, the integral of f of X minus g of X dx. This is an absolute value, so either the number in the absolute value is positive and I actually kept it like that, or I didn't keep it that way. The first thing I am going to do is this is anabsolute value, and I'm not going to say what it is. I'm just going to show you what it looks like when it's an absolute number, and it's either positive or negative. it's negative and I flipped its sign. So let's just split between those two cases. So this thing is equal to 1/2 the integral of-- so let me actually write the set A star as being the set of X's such that f of X is larger than g of f of g. That's what I'm going to do with this star. I'm just going to write it as a set of A stars. I don't want it to look like a star. X. So that's the set on which the difference is going to be positive or negative. So this, again, is equivalent to f of X minus g of X is positive. Everybody agrees? So this is the set I'm interested in. So now now we can see what the difference will be on that set. We can see that it will be positive, or negative, depending on which set we're on. So we can say that the difference on this set is positive, and that's what we want. I'm going to split my integral into two parts, in A, A star. On A star, f is larger than g, so the absolute value is just the difference itself. So here I put parenthesis rather than absolute value. And then I have plus 1/2 of the integral on A, B, C, D, E, F, G, H, J, K, L, M, N, R, S, T, W, Z, Y, Z. the complement. What are you guys used to to write the complement, to the C or the bar? To the C? And so here on the complement,. then f is less than g, so this is actually really g of X minus f of X, dx. Everybody's with me here? We're going to do this all the time. We're not going to stop until we get to the end of it. We'll keep going until we reach the end. We've got a long way to go. So I just said-- I mean, those are just rewriting what the definition of the absolute value is. OK. So now there's nice things that I know about f and g. And the two nice things is that the integral of f is equal to 1 and theintegral of g is 1. So that's a nice thing to know. And that's what I'm going to focus on for the rest of the day. I'm not going to dwell on it too much, I'm just going to say it's nice. g is equal to 1. This implies that the integral of f minus g is equalto what? PHILIPPE RIGOLLET: 0. And so now that means that if I want to just go from the integral here on A complement to the integral on A, I just have to flip the sign. So that implies that an integral on. A star complement of g of X minus f of X, dx, this is simply equal to the.integral on A star of f ofX minus g. of X, dx. All right. So now this guy becomes this guy over there. So that means that 1/2 half of the integral between of f minus g absolute value-- so that was my original definition, this thing. So I have 1-2 of this plus 1-1/2 of the same guy, so that means I have this guy and that guy. That's what I'm going to call it. I'm just going to say it's this guy, this guy. is actually equal to the integral on A star of f of X minus g of X, dx. And this is simply equal to P of A star-- so say Pf of A start minus Pg of A stars. Which one is larger than the other one? Philipe says it's the one with the most stars. The other one is the one that has the least stars in it. That's what we call the first star of the second stage of the story. The second stage is the third stage, which is the final stage. RIGOLLET: It is. Just look at this board. The first one has to be larger, because this thing is actually equal to a non-negative number. So now I have this absolute value of two things, and so I'm closer to the goal. AUDIENCE: [INAUDIBLE] PHILIPPE RIGOLlet: What? The firstOne has to. be larger. Because this thing. is actuallyequal to aNon-Negative Number. actual definition. But I still need to show you that this thing is the maximum value. So this is definitely at most the maximum over A of Pf of A minus Pg of A. That's certainly true. Right? We agree with this? Because this is just for one specific A. Because this was just for a specific A, right? Right? Because we agree that this was the maximum for that specific A? That's definitely true. We agree that that's the maximum. and I'm bounding it by the maximum over all possible A. So that's clearly true. So now I have to show you that the max is actually this guy, A star. So why would that be true? Well, let's just inspect this. Well, just look at this guy. He's a star. And he's the only A star in the universe. So he must be an A star, right? So why is he a star? He must be a star, because he is the only one in the galaxy. thing over there. If I take any other A in this integral than this guy A star, it's actually got to decrease its value. So we have this function. I'm going to call this function delta. And what we have is-- so let's say let's call it delta. We want to show that if I take. any other. A in that integral than that one, it has to decrease. Its value. It has to go down. It's not going to go up. We're going to show this. this function looks like this. Now it's the difference between two densities. It doesn't have to be non-negative. But it certainly has to integrate to 0. And so now I take this thing. And the A star, what is the set A star here? The A star is the density of a set of points. It's the number of points in the set that is the same as the value of that point. And that's the density. It is the number that is equal to that point or less than that. That's thedensity of the set. set A star is the set over which the function delta is non-negative. So that's just the definition. A star was the set. over which f minus g was positive, and f minus. g was just called delta. So what it means is that what I'm really integrating is delta, and that's what a star is. It's a set that's over which delta is not negative, and it's a star over which that's not true. That's the definition of a star. on this set. So it's this area under the curve, just on the positive things. Agreed? So now let's just make some tiny variations around this guy. If I take A to be larger than A star-- so let me add, for example, this part here. That means that when when this part is added to the set, it will look like a star in the middle of the set. And that's what we're going to do with the rest of it. I compute my integral, I'm removing this area under the curve. It's negative. The integral here is negative. So if I start adding something to A, the value goes lower. If I start removing something from A, like say this guy,. I'm actually removing this value from the integral. So I'm not adding anything to A. I'm taking something away from A and adding it to A and taking it from A. That's what I'm doing with my integral. there's no way. I'm actually stuck. This A star is the one that actually maximizes the integral of this function. So we used the fact that for any function, say delta, the integral over A of delta is less than the integral. over the set of X's such that delta is maximized over A. This is the star that maximizes delta over A, and it's the only star that can do it. It's the A star that's the most likely to be maximized. of X is non-negative of delta of X, dx. And that's an obvious fact, just by picture, say. Yeah? AUDIENCE: [INAUDIBLE] could you use like a portion under the axis as like less than or equal to the portion above the axis? PHILIPPE RIGOLLET: Yes, that's true for all A. Yeah, I'm sorry. I'm not sure if that's a good question. I was trying to make a point. It's actually equal. We know that the integral of f minus g is 0. So there's actually exactly the same area above and below. But yeah, you're right. You could go to the extreme cases. No. It's actually still be true, even if there are no extreme cases in the case of f plus g or delta plus f plus delta. But that's not what we're talking about here. We're just talking about the general case, which is exactly the case. was-- if this was a constant, that would still be true. Here, I never use the fact that the integral is equal to 0. I could shift this function by 1 so that the Integral of delta isequal to 1, and it would still remain true that it's maximized. If I shifted it by 1, the integral of delta would be equal to 1. That would still stay true, even if the integral was equal to -1. I don't want to use this as an example. when I take A to be the set where it's positive. Just need to make sure that there is someplace where it is, but that's about it. Of course, we used this before, when we made this thing. But just the last argument, this last fact does not require that. It's just a way of showing that it's possible to have a positive set of letters. It doesn't have to be perfect, but it's a way to show that it can be possible. All right. So we have this notion of distance between probability measures. I mean, these things are exactly what-- if I were to be in a formal math class and I said, here are the axioms that a distance is a distance. OK. So now we have the notion of-- I need the-- OK. We have the idea of a distance between a probability measure and the probability of a certain thing happening. And we have a notion of the distance between two probability measures, which is a probability of 1. should satisfy, those are exactly those things. So it's a distance. It's symmetric, non-negative, equal to 0, if and only if the two arguments are equal, then it satisfies the triangle. If it's not satisfying this thing, it's called pseudo-distance or quasi-distance. Or just metric or nothing at all, honestly. It doesn't have to be perfect, it just has to be a distance, and that's what we're trying to achieve. inequality. And so that means that we have this actual total variation distance between probability distributions. And here is now a statistical strategy to implement our goal. Remember, our goal was to spit out a theta hat, which was close such that P thetaHat was close to P ThetaHat. And that's the strategy we used to get the theta Hat to be close to the probability distribution of P Thetas, which is the probability of getting a thetas hat. Star. So hopefully, we were trying to minimize the total variation distance between P theta hat and P. theta star. Now, we cannot do that, because just by this fact, this slide, if we wanted to do that directly, we would just take-- well, let's take thetaHat equals thetaStar. But we can't do that. So we're going to try to minimize that distance as much as we can. And we'll see how successful we are. We know how to compute total variations if I give you the two arguments. And that's the minimum possible value we can take. The problem is that we don't know what the total variation is to something that wedon't know. We know that it's 0.star and that will give me the value 0. star. But that's not what we want. We want to know what it is. And we can't know it. We don't have the answer. We just know that we want it to be 0. P theta star is not known to us, so we need to estimate it. Just build an estimator of the total variation distance between P theta and P theTA star for all candidate theta. Now, if this is a good estimate, then when I minimize it, I should get something that's close to P thena star. So here's the strategy. This is my function that maps theta to the total variations between P and P. I know it's minimized at theta stars. That's definitely TV of P-- and the value here, the y-axis should say 0. estimator is. And I'm going to try to minimize this estimator now. And if the two things are close, then the minima should be close. That's a pretty good estimation strategy. The problem is that it's very unclear how you would build this. estimator of TV, of the Total Variation. It's a lot of work to get to the right number of people to watch a TV show at the same time as they're watching it on TV. So building estimators, as I said, typically consists in replacing expectations by averages. But there's no simple way of expressing the total variation distance as the expectations with respect to theta star of anything. So what we're going to do is we'regoing to move from total variationdistance to average variation distance. And that's what we'll be doing in this article. We'll be looking at how we can use averages to get a more accurate estimate of a person's potential to do something. another notion of distance that sort of has the same properties and the same feeling and thesame motivations as the total variation distance. So this surrogate for total variationdistance is actually called the Kullback-Leibler divergence. And why we call it divergence is because it's actually not a distance. It's not going to be symmetric. But for this guy, we will be able to build an estimate for it, because it will be of the form expectation of something. And we're able to replace the expectation by an average and then minimize this average. to start with. So this Kullback-Leibler or even KL divergence-- I will just refer to it as KL-- is actually just more convenient. But it has some roots coming from information theory, which I will not delve into. But if any of you is actually a Core 6 student, I'm happy to help you out with your Core 6 homework. If you have any questions about Core 6, please email jennifer.smith@mailonline.co.uk. sure you've seen that in some-- I don't know-- course that has any content on information theory. All right. So the KL divergence between two probability measures, P theta and P theTA prime-- and here, as I said, it's not going to be the symmetric, so it's very important for information theory, I think. I think that's a good way to look at it. It's not symmetric. It doesn't have to be symmetric to be a problem. you to specify which order you say it is. It's different from saying between P theta prime and P thea. And so we denote it by KL. Remember, before we had either the sum or the integral of 1/2 of the number of the square root of the power of two, we used to call it the theta-theta. We now use the term theta theta to mean the same thing as the prime of a number, but in a different order. distance-- absolute value of the distance between the PMFs and 1/2 of the absolute values of the distances between the probability density functions. And then we replace this absolute value by this weird function. This function is P theta, log P thena, divided by P theda. This is the function that we use to get the distance divided by 2. It's called the weird function, or the log function, which is the product of the weird and the distance. The weird function is then the log of the odd number, or P theta. P theta prime. That's the function. OK. So this was what we had, that's the TV. And the KL, if I use the same notation, f and g, is integral of f of X, log of f over g of X. It's a weird function. But that's what we have. We have a TV. We've got a TV, we've got the TV, and we have the KL. We're going to use the TV to see what the KL looks like. bit different. And I go from discrete to continuous using an integral. Everybody can read this. Everybody's fine with this. Is there any uncertainty about the actual definition here? So here I go straight to the definition, which is just plugging the functions into some integral and compute. So I go directly to thedefinition, and it's very simple. It's just a series of functions. It doesn't have to be complex. It just has to be discrete or continuous. And that's what we do. don't bother with maxima or anything. I mean, there is something like that, but it's certainly not as natural as the total variation. Yes? AUDIENCE: The total variation, [INAUDIBLE].. PHILIPPE RIGOLLET: Yes, just because it's hard to build anything fromtotal variation, because I don't know it. So it's very hard to do. It's very difficult to do anything from total variation because you don't understand it. difficult. But if you can actually-- and even computing it between two Gaussians, just try it for yourself. And please stop doing it after at most six minutes, because you won't be able to do it. And so it's just very hard to manipulate, like this integral of absolute values. But it's very, very difficult to do, but if you want to try it, just do it for a few minutes, and you'll find it very difficult, but you'll get it. of differences between probability density function, at least for the probability density functions we're used to manipulate is actually a nightmare. And so people prefer KL, because for the Gaussian, this is going to be theta minus theta prime squared. And then we're going tobe happy, and so those differences are not as big as they used to be, so KL is the preferred choice. For example, if you have a Gaussian and a KL, you can get a KL of theta plus theta. things are much easier to manipulate. But it's really-- the total variation is telling you how far in the worst case the two probabilities can be. This is really the intrinsic notion of closeness between probabilities. So that's really the one-- if we could, that's the one we would go with, if we were able to, to get to the bottom of it. If we couldn't, we wouldn't be able to do it. We would have to find a way to make it more complicated. After.after. Sometimes people will compute them numerically, so that they can say, oh, here's the total variation distance I have between those two things. And then you actually know that that means they are close, because the absolute value-- if I tell you total variation is 0.01, like we did, you know it's close. And so that's how you know they're close. After after. Afterafter. sometimes people will computed them numerally, so they can. say,Oh, here’s thetotal variation distance. I have. here, it has a very specific meaning. If I tell you the KL divergence is 0.01, it's not clear what it means. OK. So what are the properties? The KL divergence between P theta and P theTA prime is different from the KL divergences between theta prime and theta in general. Of course, in general, because if theta is equal to thetaprime, then this certainly is true. So there's cases when it is not true. The KL divergence is non-negative. when I asked what a convex function is. This is Jensen's-- the proof is just one step Jensen's inequality, which we will not go into details. But that's basically an inequality involving expectation of a conveX function of a random variable. All right. So you know what Jensen'sequality is. We'll go into more detail about that later on in the show. But for now, let's go to the next segment of the show and talk about the theory of convex functions. compared to the convex function of the expectation of a random variable. If you know Jensen, have fun and prove it. What's really nice is that if the KL is equal to 0, then the two distributions are the same. And that's something we're looking for. Everything else we're happy, and that's what we're aiming for. We're happy with the result. We don't need to know Jensen's secret. We just want to know that the distribution is the same, and if it's not, we're not happy. to throw out. And actually, if you pay attention, we're actually really throwing out everything else. So they're not symmetric. It does satisfy the triangle inequality in general. But it's non-negative and it's 0 if and only if the two distributions are the same. And that's all we care about, that's what we're trying to get to the bottom of. And so we're throwing out all the other stuff. We're throwing it out. We don't care about it. "The first time I saw it, I was just annoyed. I was like, can we just like, I don't know, take it," he says. "And that's what we call a divergence rather than a distance, and divergence will be enough for our purposes" "The fact that it's not flipping-- the first time he saw it,. I wasJust annoyed," he adds. "Can we just take it?" "Yes," he replies. "I don't want to take it." the average of the KL between P theta and P thea prime. The problem is that this will take two different values. You just symmatrize it by just taking theAverage of the two possible values it can take. This will give you the same result as taking the average of two values of P. theta. But it will be much more difficult to do than the previous method of symmatrizing the KL. It will take a long time to get the same answer as the original. still not satisfy the triangle inequality. And there's no way basically to turn it into something that is a distance. But the divergence is doing a pretty good thing for us. And this is what will allow us to estimate it and basically overcome what we could not do with the original model. It's what is allowing us to basically overcome the problem of how to estimate the distance of the Earth from the Sun, which is a very difficult problem to solve. It is the reason why we use the divergence to try and estimate it. the total variation. So the first thing that you want to notice is the total variation distance. The KL divergence, sorry, is actually an expectation of something. It's the integral of some function against a density. That's exactly the definition of an expectation, right? That's the way to look at the KL divergence. It is an expectation that something is going to happen in the future. That is what the KL divergences are for. They are an expectation for something to happen. So this is the expectation of this particular function with respect to this density f. So in particular, if I call this is density f-- if I say, I want the true distribution to be the first argument, this is an expectation withrespect to thetrue distribution from which from which this function is expected to be derived. This is a function called density f, and it's a function of the density f of a particular function, which is called a density. It's an expectation of a function that is related to the density of a given function. my data is actually drawn of the log of this ratio. So ha ha. I'm a statistician. Now I have an expectation. I can replace it by an average, because I have data from this distribution. And I could actually replace the expectation by anAverage and try to minimize the effect of the ratio. That's what I'm trying to do. I could replace it with an average and try and minimize the impact of the Ratio. That would be a good way to do it. here. The problem is that-- actually the star here should be in front of the theta, not of the P, right? That's P theta star, not P star theta. But here, I still cannot compute it, because I have this P thetta star that shows up. I don't know what to do with that star, so I'm trying to figure out what's wrong with the image. I think I've got it, but I'm not sure what it is. it is. And that's now where the log plays a role. If you actually pay attention, I said you can use Jensen to prove all this stuff. You could actually replace the log by any concave function. That would be f divergent. That's called an f divergence. But the log is what it is. It's what it's always been, and it's what we're always going to be. We're not going to get rid of the log, but we're going to change the way we look at it. Theta.itself is a very, very specific property, which allows us to say that the log of the ratio is the ratio of the log. Now, this thing here does not depend on theta. If I think of this KL divergence as a function of theta, then the first part is first part of KL divergence. The second part of the KL divergence is second part. The third part is the third part of this divergence, which is the fourth-part of the divergence. actually a constant. If I change theta, this thing is never going to change. It depends only on theta star. So if I look at this function KL, theta maps to KL P theTA star, P theta. When theta changes, thisthing is actually a constant, and it's not going to go away. It's just a function of theta that changes. And that's the way it works. It doesn't change when theta is changed. This is a fixed value. Actually, it's the negative entropy of P theta star. And if you've heard of KL, you've probably heard of it. And that's a good thing. We want something that reflects how close theta and theta stars are. But this thing is not going to change. Going to change is good, because it means we know how close we are to each other. But we don't want to know how far we are from each other at the same time. heard of entropy. And that's what-- it's basically minus the entropy. That's a quantity that just depends on theta star. But it's just the number. I could compute this number if I told you this is n theta stars 1. You could. compute this. So now I'm going to go to the next level and say the entropy of a star is the number of stars that are in existence at the same time. That number is called the n-theta star number. try to minimize the estimate of this function. And minimizing a function or a function plus a constant is the same thing. I'm just shifting the function here or here, but it's the same minimizer. OK. So the function that maps theta to KL of P theta star to P is called theta-Kelvin-P star function. It's a function of theta, KL, P, and a constant called P-Kl, or P-theta star. theta is of the form constant minus this expectation of a log of P theta. Everybody agrees? Are there any questions about this? are there any remarks, including I have no idea what's happening right now? OK. We're good? Yeah. So when you're actually employing this method, how do you do it? We'll have to wait and see, but I think we're good. We'll be back in a minute or two and we'll try it again. you know which theta to use as theta star and which isn't? PHILIPPE RIGOLLET: So this is not a method just yet, right? I'm just describing to you what the KL divergence between two distributions is. If you really wanted to compute it, you would need to know what P is. P is the difference between theta and theta stars in theta. Theta stars are the most common type of star in the universe. theta star is and what P theta is. PHILIPPE RIGOLLET: And so here, I'm just saying at some point, we still-- soHere, you see-- so now let's move onto one step. I don't know expectation of theta star. But I have data that comes from distribution P. theta. Theta is a type of star. It's a black hole in the middle of the Milky Way galaxy. It can't be seen from Earth, but it can be seen in the sky. theta star. So the expectation by the law of large numbers should be close to the average. And so what I'm doing is I'm replacing any-- I can actually-- this is a very standard estimation method. You write something as an expectation with respect to the data-generating process of some. Some of you may have noticed that I'm using a different method than the one I used before. That's because I'm trying to get a better idea of what the average is. function. And then you replace this by the average of this function. And the law of large numbers tells me that those two quantities should actually be close. Now, it doesn't mean that's going to be the end of the day, right. When we did Xn bar, that was the bar. That was the number of bars we had left to go on. But that's not going to stop us from going on. We're going to keep going until we get to the end. end of the day. We had an expectation. We replaced it by an average. And then we were gone. But here, we still have to do something, because this is not telling me what theta is. So this is now my candidate. Now I still have. to minimize this average. This is now. my candidate, and I'm going to do it. I'm not going to let it get out of hand. It's time to get back to work. I've got a lot of work to do. estimator for KL, KL hat. And that's the one where I said, well, it's going to be of the form of constant. And this constant, I don't know. I have no idea what this constant is. It depends on P theta star. But then I have minus something. And I'm not sure what it is. But it's not a constant. It's a minus. It means I have to be minus something, and that's not good. that I can completely compute. If you give me data and theta, I can compute this entire thing. And now what I claim is that the minimizer of f or f plus-- f of X or f ofX plus 4 are the same thing, or say 4 plus f. That's what I'm trying to prove. I'm not claiming to be a mathematician. I just want to prove that I can do it, and that's all I need to do. I don't need to know how to do it. of X. I'm just shifting the plot of my function up and down, but the minimizer stays exactly where it is. If I have a function-- so now I've got a function of theta. This is KL hat of P theta star. And it's of the form-- it's P theTA star. It's a form of the function of X of X, which is X of P. Theta star is a star in the constellation of the same name. It has the shape of a circle. a function like this. I don't know where this function is. It might very well be this function or this function. Every time it's a translation on the y-axis of all these guys. And the value that I translated by depends on theta star. IDon't know what it is. I've never seen it before. It's a very strange thing to look at. It looks like something from the future. But it's not. It doesn't look like anything from the past. But what I claim is that the minimizer is always this guy, regardless of what the value is. OK? So when I say constant, it's a constant with respect to theta. It's an unknown constant. But it's with respectto theta, so without loss of generality, I can assume that it's the same as the previous constant. OK?" "Yes," he says. "Yes, it is the same." "I can assume," he adds, "that it is." this constant is 0 for my purposes, or 25 if you prefer. All right. So we'll just keep going on this property next time. And we'll see how from here we can move on to-- the likelihood is actually going to come out of this formula. Thanks. Back to Mail Online home. back to the page you came from. Back from the page where you came From. Click here to go to the show page where we'll be talking about the probability of winning the lottery.