homework two is out now. This weekend sessions will be having some more background on deep learning. We're also gonna be reaching, uh, releasing by the end of tomorrow, what the default projects will be for this class. Um, and those proposals will be due, um, very soon, er, in a little over a week. Are there any other questions that people have right now? Yeah. The assignments, [inaudible] are they limited to TensorFlow? asked the question if, if the assignment is limited toTensorFlow. out on Piazza and double-check that, but I'm pretty sure any Oliver auto-graders is just set up for TensorFlow. We also released a tutorial for how to just sort of set up your machine last week. So, if you're having any questions with that, that's a great place to get started. Um, I'll believe you guys also should have access to the Azure credit. If you have any questions about getting setup without feel free to use the Piazzas channel. "We wanna be able to deal with really complex, um, information about customers, or patients, or students, where we might have enormous state and our actions spaces. And so, when we started talking about those, I was arguing that we either need representations of models. T, T or R or a state-action values Q, or V, or our policies," he says. "With the idea being that we may in fact never encountered the exact same state again. You might never see the exactsame image of the world again, um" as, um, a mean squared error. So, we can define our loss j, and we can use gradient descent on that to try to find the parameters w that optimize. And just as a reminder stochastic gradient descent was useful because when we could just slowly update our parameters as we get more information. And that information now could be [NOise] in the form of episodes or it [NOISE] could be individual tuples. When I say a tuple, I generally mean a state-action reward next state tuple. otherwise we wouldn't have to be doing all of this learning. And so, the two ways we talked about last time was inspired by a work on Monte Carlo, or on TD learning is we could either plug-in the return from the full episode. Or we could put in a bootstrapped return. So, now we're doing bootstrapping. Where we look at the reward, the next state, and the value of our next state. And in this case we're using a linear value function approximators for everything, which gave us a really simple form of what the derivative is. the last layer being a linear combination of those features. For most of the time when we're talking about deep RL with, um, a deep neural networks represent the Q function. So, linear value function is often really works very well if you're the right set of features, but is this challenge of what is the rightSet of features. Um, and there are all sorts of implications about whether or not we're even gonna be able to write down the true p- um, value function. to linear value function approximators. The intuition is that if you want to have sort of an accurate representation of your value function, um, and you're representing it by say, uh, local points around it. For example, with the k-nearest neighbor approach. then the number of points you need to have everything be close like in an epsilon ball scales with the- the dimensionality. Um, but they haven't far been used for in a very widespread way. is sort of by their name. Um, when you use this type of approximation, you don't- they're guaranteed to be- to be a non-expansion, which means that when you combine them with a bellman backup it's guaranteed it'd still be a contraction. So, that means these sort of approximators are guaranteed to converge compared to a lot of other ones. All right, but they're not gonna scale very well and in practice youdon't tend to see them. Deep neural networks are artificial connections with artificial neural networks inside our brain. They are used in unsupervised learning like predicting whether or not something is a cat or not or, you know, an image, uh, of a particular object, um, or for regression. They combine both linear and non-linear transformations, and they need to be differentiable if gradient descent is to be used to fit them. In the last 5 to 8 years, there's auto differentiation. So, you don't have to derive all of these gradients by hand instead. The question is about how for- for ReLU, there's a lot of it where it's flat. If your gradient is zero then your gradients can vanish there. We're not gonna talk about this at all in class but there's certainly a problem is you start having very deep neural networks. Um, but because of some of these functions you can sometimes end up sort of having, um, almost no signal going back to the- the earlier layers. We'll talk- we'll talk some about that in sessions. about what we're doing with linear value function approximators, it was clearly the case sometimes that you might have too limited features and you just wouldn't be able to express the true value function for some states. That will not occur for, um, uh, deep neural network if it is sufficiently rich. Another benefit is that potentially you can use exponentially less nodes or parameters compared to using a shallow net which means not as many of those compositions to represent the same function. Then the final thing is that you can learn the parameters using stochastic gradient descent. use similar sorts of input on our- our robots in our artificial agents. So, if you think about this, um, think about there being an image, in this case, of Einstein. Um, and there's a whole bunch of different pixels on Einstein. Let's say it's1,000 by 1,000. So,. we have 10 to the 6 pixels. This standard is often called feedforward deep neural network. Um,. you would have all of those pixels,Um, and then they would be going as input to another layer and so you can get a huge number of weights. Convolutional neural networks try to have a particular form of deep neural network that tries to think about the properties of images. So, in particular, images often have structure, um, in the way that our- our brain promises images also has structure and this sort of distinctive features in space and frequency. Having so operators again here are like our functions, h1 and hn, which I said before could either be linear or nonlinear and then convolutional network learn a particular structures for those. instead of saying, "I'm going to have totally different parameters each taking in all of the pixels," I'm gonna end up having sort of local parameters that are identical and then I apply them to different parts of the image. Because ultimately, the point of doing this is gonna be trying to extracting features that we think are gonna be useful for either predicting things like whether or not, you know, a face isn't an image or that are gonna help us in terms of understanding what the Q function should be. this case, if you have an input of 28 by 28 and you have a little five-by-five patch that you're going to slide over the entire image, then you're gonna end up with a 24 by 24 layer next. So, one thing is instead of having our full x input, we're just gonna take in- we're gonna direct different parts of the x input to different neurons which you can think of just different functions. Um, but the other nice idea here is that we'regoing to have the same weights for everything. of different parts of the image. So, this is sort of what that would look like. You sort of have this input, you go to the hi- um, the hidden layer and, yeah, you're sort of do- also down-sampling the image, um. Why would you want to do this? Well, we think that often, the brain is doing this. It's trying to pick up different sort of features. In fact, a lot of computer vision before deep learning was, um, trying to construct these special sorts of features, things like sift features or other features. then you could apply these different filters on top of it, which you can think of as trying to detect different features, and then you move them around your image, and see whether or not that feature is present anywhere. So you can do that with multiple different fype- types of filters. These give you different features essentially that are been extracted. The other really important thing in CNNs, is what are known as pooling layers. They are often used as a way to sort of down-sample the image. So, the final layer is typically fully connected. In 1994, we had TD backgammon which used Deep Neural Networks. And then we had the results that were kind of happening around like 1995 to maybe like 1998, which said that, "Function approximation plus offline off policy control, plus bootstrapping can be bad, can fail to converge" And so I think for a long time after that, the, the community was quite cautious about using them because they were clearly, even simple cases where things started to go really badly with function approximation. And so, perhaps it was natural that, like, around in like 2014, DeepMind and DeepMind combined them and had some really amazing successes with Atari. In practice, we can still get pretty good policies out. Now, we often don't know if they're optimal. Often, we know they're not optimal because we know that people can play better, but that doesn't mean that they might not be pretty good. Um, I think that some of the issues that were coming up in 1995 to 1998 in terms of convergence, there are some algorithms now that are more true stochastic gradient algorithms. They may not be guaranteed to converge to the optimal policy, um, so there's still lot of work to be done. I think there's also a couple algorithmic ideas that we're gonna see later in this lecture, that help the performance kind of avoid some of those convergence problems. Um, ah, they are doing deep learning in, in this case, Deep-Q Learning. And so it can still be very unstable, but they're gonna do something about how they do with the frequency of updates to the networks, to try to make it more stable. We'll see how it works here. Anyone else? Okay, cool. An Atari game generally, doesn't have a really high dimensional action space. It's normally somewhere between like four to 18, depends on the game. In these games, you typically need to have velocity. So, because you need velocity, you need more than just the current image. So what they chose to do is, you'd need to use four previous frames. So this at least allows you to catch for a velocity and position, observe the balls and things like that. But it's not always sufficient. Can anybody think of an example where maybe an Atari game, I don't know how many people played Atari. In a paper from 2015, a group of researchers used the same architecture and hyperparameters across all 50 games. They didn't have to use totally different architectures, do totally different hyperparameter tuning for every single game separately. It really was the sort of general, um, architecture and setup was sufficient for them to be able to learn to make good decisions for all of the games. And the nice thing is that, I think this is actually required by nature. They, they released the source code as well. that if you have s, a, r, s prime, a prime, r prime, double prime. If this is a deterministic system, the only difference between them will be R. So, so these are highly correlated, this is not IID samples when we're doing updates, there's a lot of correlations. Um, and also this issue with non-stationary targets. What does that mean? It means that when you're trying to do your supervised learning and train your value function predictor, um, it's not like you always have the same v pi oracle. standard approach, just uses a data point. In the simplest way of TD Learning or Q-learning, you use that once and you throw it away. Even though we're treating the target as a scalar, the weights will get updated the next round which means our target value changes. So, even though it's the same data point as before, it's gonna cause a different weight update. So this is nice because basically it means that you reuse your data instead of just using each data point once, you can reuse it. learn to model, a dynamics model, and then the planning for that which is pretty cool. So, this is getting us closer to that. But we don't wanna do that all the time because there's a computation trade-off and particularly here because we're in games. There's a directtrade-off between computation and getting more experience. Um, and if it's a fixed size buffer, how do you pick what's in it? Um, is it the most recent and- and how do we get things out? It's a really interesting question. Different people do different things. strike the right balance between continuing experience like new data points versus re-flagging it. Can we use something similar to like exploitation versus exploration. Um, essentially like with random probability just decide to re-flag [inaudible]. The question is about how would we choose between, like, what, um, you know, getting new data and how much to replay et cetera, and could we do that sort of as an exploration-exploitation trade-off. Could certainly imagine trying to optimally figure this out but that also requires computation. use in that value of S prime for several rounds. So, instead of always update- taking whatever the most recent one is, we're just gonna fix it for awhile and that's basically like making this more stable. Because this, in general, is an approximation of the oracle of V star. What this is saying is, don't do that, keep the weights fixed that used to compute VS prime for a little while and that just makes the target, the sort of the thing that you're trying to minimize your loss with respect to, morestable. our w minus, and then we use stochastic gradient descent to update the network weights. So, uh, I guess two questions like intuitively, why does this is help and, like, why do it make it more stable? And then, beyond the stability, is there any other benefits? In terms of stability, it helps because you're basically reducing the noise in your target. If you think back to Monte Carlo, um, there instead of using this target like this bootstrap target where we're using GT. This is just reducing the noise and the target that we're trying to sort of, um, if you think of this as a supervised learning problem, we have an input x and output y. The challenge in RL is that our y is changing. If you make it that you're- so your y is not changing, it's much easier to fit. Uh, assuming we want to do [inaudible] approximator. Is there something that's specific to the deep neural networks?. In practice, do people have some cyclical pattern and how can they refresh the- the gradient that's used to compute, uh, the gradients? Yes, yeah there's often particular patterns or- or hyper- it's a hyperparameter choice of how quickly and how frequently you update this. This has no guarantees of convergence still. This is hopefully gonna help but we have no guarantees. It will trade-off between propagating and not propagating. It's a great question, and these sort of Q learning are not true gradient descent methods. information fester, um, and possibly being less stable. If n is infinity, that means you've never updated it. There's a- there's a smooth continuum there. William? Uh, we notice, like, for w, there are better initializations than just like zero, uh, if you take into account, I guess like the mean and variance. Uh, would you initialize w minus just two w or is there like an even better initialization for w minus? Yeah, his questions is about, you know, the- the impact of how we, um,. uh, initialize w ca- can matter. E-greedy. So, this is what it looks like. You sort of go in and you do multiple different convolutions. They have the images, um, and they do some fully connected layers and then the output a Q value for each action. As you'd hope, as it gets more and more data, it learns to make better decisions. But one of the things people like about this a lot is that, uh, you can learn to exploit the reward function. Uh, so in this case, it figures out that if you really just want me to maximize the expected reward, what the best thing for me to do is to just kind of get a hole through there. the reward, it'll- it'll learn the right way to maximize the reward given enough data. When they did this, they then showed, um, some pretty amazing performance on a lot of different games. Many games they could do as well as humans. Um, and so this is really cool that sort of it could discover things that maybe are strategies that people take a little while to learn when they're first learning the game as well. Yeah? This might become a little bit of [inaudible] but is there a reason to introduce a pulling layer? Puling layer? There might be one in there. architecture, um. The question is whether or not there's a pulling layer in there. There has- they have to be going from images all the way up. But they have the complete architecture. So, the next thing that you can see here is that, um, they got sort of human level performance on a number of different Atari games. There's about 50 games up here. Um, there's been a lot of interest in these sort of games on the bottom end of the tail which often known as those hard exploration games. Replay is hugely important and it just gives us a much better way to use the data. Using that fixed Q here means you seem like a fixed target. You do replay and suddenly you're at 241. Okay, so throwing away each data point what- after you use it once is not a very good thing to do. But you could certainly imagine trying linear plus replay and it seems like you might do very well here, it might depend on which features you're using. There's some cool work, um, over the last few years looking also at, uh, whether you can combine these two. like Bayesian linear regression which is useful for uncertainty. Other people have just done linear regression where the idea is you- you sort of, um, uh, deep neural network up to a certain point. So, that can be much more efficient, but you still have a complicated representation. Some of the immediate improvements that we're going to go through really quickly here is, um,. Doubled DQN, prioritize replay, and dueling DQn. These are some of the early really big big big improvements. double DQN is kind of like double Q learning, which we covered very briefly at the end of a couple of classes ago. The thing that we discussed there was this sort of maximization bias, is that, um, the max of estimated state action values can be a biased estimator of the true max. So, this is a pretty small change, it means you have to maintain two different networks or two different sets of weights, and it can be pretty helpful. It's a fairly small change. change, it's very similar to what we were doing already for the target network, network weights. It turns out that it gives you a huge benefit in many, many cases for the Atari games. The second thing is prioritized replay. In Mars Rover we had this really small domain, we are talking about tabular setting through just seven states, and we're talking about a policy that just always took action a1 which turned out to mostly go left. That's one idea, and that's sort of a direct lift up from sort of, you know, double Q learning. So, let's say you get to choose two replay backups to do. Maybe it doesn't matter if you can just pick any of these, you're going to get the same value function no matter what you do. So, are there two- two updates that are particularly good, and if so, why and what order would you do them in? [NOISE] Hopefully you had a chance to think about that for a second. First of all, does it matter? So, I'm going to first ask you guys, uh, the question. Vote if you think it matters which ones you pick. All right. You've got to back-propagate from the information you're already [NOISE] have on step one to step two. So, if you pick, um, backup three, so what's backup three? It is, S2, A1, 0, S1. So that means now you're gonna get to backup and so now your V of S2 is gonna be equal to one. So it definitely matters. It matters the order in which you did, do it. have changed. Um, so ordering can make a big difference. Uh, so not only do we wanna think about like, what, um, was being brought up before but I think to say like what should we be putting in our replay buffer. What order should be in a replay buffer but also what order do we sampled them can makea big difference in terms of convergence rates. There's some really cool work from a couple of years ago looking at this formally of like how, at what the ordering, matters. might wanna be careful about the order that we do it and- so, their, intuition, for this, was, let's try to prioritize a tuple for replay according to its DQN error. So, if you have a really really big error, that we're gonna prioritize, updating that more. One method basically takes these priorities, raises them to some power alpha, um, and then normalizes And then that's the probability, of selecting that tuple. So you prioritize more things that are weights. prioritise replay versus, um, I think this is prioritise replay plus D, Um, double DQN versus just double D QN. Most of the time,Um, this is zero would be, they're both the same underneath. Above means that prioritize replay is better. Most, the time prioritize Replay is better and there's some hyper parameters here to play with. And it's certainly useful to think about, you know, we're order might matter. All right. We don't have very much time left so I'm just gonna do, short through this just so you're aware of it. could be super tempting to try to start, by like implementing Q learning directly on the ATARI. Highly encourage you to first go through, sort of the order of the assignment and like, do the linear case. Even with the smaller games, like Pong which we're working on, um, it is enormously time consuming. It's way better to make sure that you know your Q Learning method is working, before you wait, 12 hours to see whether or not, oh it didn't learn anything on Pogge.