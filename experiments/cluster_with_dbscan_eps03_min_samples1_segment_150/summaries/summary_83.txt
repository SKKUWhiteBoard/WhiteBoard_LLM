Lecture eight is about deep learning software. This is a super exciting topic because it changes a lot every year. As usual, a couple administrative notes before we dive into the material. Project proposals for your course projects were due on Tuesday. We're in the process of assigning TA's to projects based on what the project area is and the expertise of the TA's. So we'll have some more information about that in the next couple days I say. I want to get started. Remember to stop your Google Cloud instances when you're not working to try to preserve your credits. For assignment two you really only need to use GPU instances for the last notebook. For all of the several notebooks it's just in Python and Numpy so you don't need any GPUs for those questions. And the final reminder is that the midterm is coming up. It's kind of hard to think about, but we'll get those grades back to you as soon as we can.think. The midterm will be in class on Tuesday, five nine. It'll be sort of pen and paper working through different kinds of, slightly more theoretical questions to check your understanding of the material that we've covered so far. And I think we'll probably post at least a short sort of sample of the types of questions to expect. So just, Yeah, yeah, so that's what we've done in the past is just closed note, closed book, relatively just like want to check that you understand the intuition behind most of the stuff we've presented. Small tweaks on top of vanilla SGD, are relatively easy to implement but can make your networks converge a bit faster. We also talked about regularization, especially dropout. And we saw that this was kind of a general pattern across many different types of regularization in deep learning, where you might add some kind of noise during training, but then marginalize out that noise at test time so it's not stochastic attest time. And finally, we talked about transfer learning where you can maybe download big networks that were pre-trained on some dataset and then then use them to learn. fine tune them for your own problem. This is one way that you can attack a lot of problems in deep learning, even if you don't have a huge dataset of your own. So that's kind of the brief introduction to like sort of GPU CPU hardware in practice when it comes to deep learning. And then I wanted to switch gears a little bit and talk about the software side of things. The various deep learning frameworks that people are using in practice. But I guess before I move on, is there any sort of questions about CPU GPU? Yeah, question? This is a shot of my computer at home that I built. And you can see that there's a lot of stuff going on inside the computer. The CPU is the Central Processing Unit. That's this little chip hidden under this cooling fan. And the GPUs are these two big monster things that are taking up a gigantic amount of space. But the CPU is actually relatively small piece. It's a relatively small thing inside the case. So, maybe, hopefully you know what most of these parts are. The GPU is called a graphics card, or Graphics Processing Unit. These were really developed, originally for rendering computer graphics, and especially around games and that sort of thing. So the question is what are these things and why are they so important for deep learning? They're quite large. They have their own cooling, they're taking a lot of power. So, just in terms of how much power they're using, they are kind of physically imposing and taking up a lot. of space in the case. debate. So if you guys have AMD cards, you might be in a little bit more trouble if you want to use those for deep learning. And really, NVIDIA's been pushing a lot for deeplearning in the last several years. It's been kind of a large focus of some of their strategy. And they put in a lot effort into engineering sort of good solutions to make their hardware better suited forDeep learning. So in practice, you tend not to end up writing your own CUDA code for deepLearning. most people in deep learning when we talk about GPUs, we're pretty much exclusively talking about NVIDIA GPUs. So to give you an idea of like what is the difference between a CPU and a GPU, I've kind of made a little spread sheet here. And there's a couple general trends to notice here. Both GPUs and CPUs are kind of a general purpose computing machine where they can execute programs and do sort of arbitrary instructions. But they're qualitatively pretty different. For GPUs we see that these sort of common top end consumer GPUs have thousands of cores. and GPUs is this idea of memory. Right, so CPUs have some cache on the CPU, but that's relatively small and the majority of the memory for your CPU is pulling from your system memory, the RAM. Whereas GPUs actually have their own RAM built into the chip. There's a pretty large bottleneck communicating between the RAM in your system and the GPU. And for the Titan XP, which again is maybe the current top of the line consumer card, this thing has 12 gigabytes of memory local to theGPU. how it works and what are the basic ideas even if you're not writing it yourself. So if you want to look at kind of CPU GPU performance in practice, I did some benchmarks last summer comparing a decent Intel CPU against a bunch of different GPUs that were sort of near top of the line at that time. And these were my own benchmarks that you can find more details on GitHub, but my findings were that for things like VGG 16 and 19, ResNets, various ResNet, then you typically see something like a 65 to 75 times speed up when running the exact same computation on a Pascal Titan X. sort of caveat here is that you always need to be super careful whenever you're reading any kind of benchmarks about deep learning. And you kind of need to know a lot of the details about what exactly is being benchmarked in order to know whether or not the comparison is fair. So in this case I'll come right out and tell you that probably this comparison is a little bit unfair to CPU. I probably could have gotten these numbers a bit better. This was sort of out of the box performance between just installing Torch, running it on a CPU. code on GPU, you should probably almost always like just make sure you're using cuDNN. So if you're not careful you can actually bottleneck your training by just trying to read the data off the disk. 'Cause the GPU is super fast, it can compute forward and backward quite fast. If you're reading sequentially off a spinning disk, you canactually bottleneck yourTraining quite, and it can be a big problem for people who want to do this sort of thing. It's a lot of work, but it's worth it in the end. that can be really bad and slow you down. So some solutions here are that like you know if your dataset's really small, sometimes you might just read the whole dataset into RAM. You can also make sure you're using an SSD instead of a hard drive, that can help a lot with read throughput. Another common strategy is to use multiple threads on the CPU that are pre-fetching data off RAM or off disk, buffering it in memory, in RAM so that then you can continue feeding that buffer data down to the GPU with good performance. This is a little bit painful to set up, but again like, these GPU's are so fast. be you have this sequential process where you first read data off disk, wait for the data, then feed the minibatch to the GPU, then go forward and backward on the GPU. And if you actually have multiple, like instead you might have CPU threads running in the background that are fetching data off the disk such that while the, you can sort of interleave all of these things. And thankfully if you're using some of these deep learning frameworks that we're about to talk about, then some of this is possible. TensorFlow is a pretty safe bet for just about any project that you want to start new, right? Because it is sort of one framework to rule them all. However, you probably need to use it with a wrapper and if you want dynamic graphs, you're maybe out of luck. Some code ends up uglier but in my opinion, maybe that's kind of a cosmetic detail. If you're just writing research code, I think PyTorch is a great choice. But it's a bit newer, has less support out there, so it could be a bit of an adventure. overview of these things. So, remember that in the last several lectures we've hammered this idea of computational graphs in sort of over and over. Remember that these graph structures can get pretty complex in the case of a big neural net, now there's many different layers, many different activations. Many different weights spread all around in a pretty complex graph. And as you move to things like neural turing machines then you can can get to a much more complex graph structure as well. get these really crazy computational graphs that you can't even really draw because they're so big and messy. There's really kind of three main reasons why you might want to use one of these deep learning frameworks rather than just writing your own code. These frameworks enable you to easily build and work with these big hairy computational graphs without kind of worrying about a lot of those bookkeeping details yourself. Another major idea is that, whenever we're working in deep learning we always need to compute gradients. And we'd like to make this automatically computing gradient, you don't want to have to write that code yourself. for you so you can just think about writing down the forward pass of your network and have the backward pass sort of come out for free without any additional work. And finally you want all this stuff to run efficiently on GPUs so you don't have to worry too much about these low level hardware details. You kind of want all those messy details to be taken care of for you. So those are kind of some of the major reasons why you might choose to use frameworks rather than writing your own stuff from scratch. In Numpy, you can just kind of write down in Numpy that you want to generate some random data. And it's really easy to do this in Numpy. But then the question is like suppose that we want to compute the gradient of C with respect to X, Y, and Z. So, if you're working in Nupy, you kind of need to write out a set of operations that you can do to get the result you're looking for. A. Combine A and Z to produce B and then finally we're going to do some maybe summing out operation on B to give some scaler final result C. Numpy is definitely CPU only. And you're never going to be able to experience or take advantage of these GPU accelerated speedups if you're stuck working in Numpy. So, kind of the goal of most deep learning frameworks these days is to let you write code in the forward pass that looks very similar to Numpy, but lets you run it on the GPU and lets you automatically compute gradients. TensorFlow has this magic line that just computes all the gradients for you. So now you don't have to go in and write your own backward pass yourself. to give us these gradients. And remember the gradients are the same size as the weights. So this means that every time we're running the graph here, we're copying the weights from Numpy arrays into TensorFlow. So if your network is very large and your weights and gradients were very big, then doing something like this would be super expensive and super slow because we'd be copying all kinds of data back and forth between the CPU and the GPU at every time step. So that's bad, we need to fix that. The question is does tf.group return none? So this gets into the trickiness of TensorFlow. So when you execute the graph, and when you tell, inside the session.run, when we want it to compute the concrete value from updates, then that returns none. But because of this dependency we've told it that updates depends on these assign operations. These assign operations live inside the computational graph and all live inside GPU memory. So then we're doing these update operations entirely on the GPU and we're no longer copying the updated values back out of the graph. In the previous example we were computing the loss explicitly using our own tensor operations. TensorFlow gives you a bunch of convenience functions that compute these common neural network things for you. So in this case we can use tf.losses.mean_squared_error and it just does the L2 loss for us so we don't have to compute it ourself. And in this example we've actually not put biases in the layer because we're not using biases. So another kind of weirdness here is that we had to explicitly define our inputs and define our weights and then like chain them together in the forward pass using a matrix multiply. that would be kind of an extra, then we'd have to initialize biases, we'd need to get them in the right shape. We would have to broadcast the biases against the output of the matrix multiply and you can see that that would kind of be a lot of code. So as a result, there's a bunch of sort of higher level libraries that wrap around TensorFlow and handle some of these details for Tensor Flow. And once you get to like convolutions and batch normalizations and other types of layers this kind of basic way of working, of having these variables, having these inputs and outputs could be a little bit unwieldy. We're also passing an activation=tf.nn.relu so it's even doing the activation, the relu activation function inside this layer for us. And in fact if you run this code, it converges much faster than the previous one because the initialization is better. Question? [student's words obscured due to lack of microphone] Question is does the xavier initializer default to particular distribution? I'm sure it has some default, I'm not sure what it is. Top of TensorFlow and handles sort of building up these computational graph for you up in the back end. By the way, Keras also supports Theano as a back end, so that's also kind of nice. And in this example you can see we build the model as a sequence of layers. We build some optimizer object and we call model.compile and this does a lot of magic in theback end to build the graph. And now we can call. model.fit and that does the whole training procedure for us magically. that out to keep the code clean. But you saw at the beginning examples it was pretty easy to flop all these things between CPU and GPU and there was either some global flag or some different data type or some with statement. So there's actually like this whole large set of higher level TensorFlow wrappers that you might see out there in the wild. And it seems that like even people within Google can't really agree on which one is the right one to use. So Keras and TFLearn are third party libraries that are out there on the internet by other people. But there's these three different ones, tf.layers, TF-Slim and TF.contrib.learn that all ship with Tensor Flow. all kind of doing a slightly different version of this higher level wrapper thing. There's another framework also from Google, but not shipping with TensorFlow called Pretty Tensor that does the same sort of thing. And I guess none of these were good enough for DeepMind, because they went ahead and released their very own high level Tensor Flow wrapper called Sonnet. So I wouldn't begrudge you if you were kind of confused by all these things. But you have a lot of options, so that's good. PyTorch provides pretrained models. And this is probably the slickest pretrained model experience I've ever seen. You just say torchvision.models.alexnet pretained=true. That'll go down in the background, download the pretrained weights for you if you don't already have them, and then it's right there, you're good to go. PyTorch also has, there's also a package called Visdom that lets you visualize some of these loss statistics somewhat similar to Tensorboard. different where we're actually building up this new computational graph, this new fresh thing on every forward pass. That's called a dynamic computational graph. For kind of simple cases, with kind of feed forward neural networks, it doesn't really make a huge difference, the code ends up kind of similarly. But I do want to talk a bit about some of the implications of static versus dynamic. And what are the tradeoffs of those two? So one kind of nice idea with static graphs is that because we're kind of building up one computational graph once, and then reusing it many times, the framework might have the opportunity to go in and do optimizations on that graph. use one weight matrix, if Z is negative we want to use a different weight matrix. And we just want to switch off between these two alternatives. In PyTorch because we're using dynamic graphs, it's super simple. Your code kind of looks exactly like you would expect, exactly what you would do in Numpy. You can just use normal Python control flow to handle this thing. And the code is very clean, easy to work with. Now in TensorFlow the situations is a little bit more complicated because we build the graph once, this control flow operator. have loops. We can just kind of use a normal for loop in Python to just loop over the number of times that we want to unroll. Now depending on the size of the input data, our computational graph will end up as different sizes. But that's fine, we can just back propagate through each one, one at a time. Now in PyTorch this is super easy. We just want to compute this same recurrence relation no matter the length of our sequence of data. Super new paper being presented at ICLR this week in France. Initial impression was that it does add some amount of dynamic graphs to TensorFlow but it is still more awkward to work with than the sort of native dynamic graphs you have in PyTorch. So one option is recurrent networks. So you can see that for something like image captioning we use a recurrent network which operates over sequences of different lengths. In this case, the sentence that we want to generate as a caption is a sequence and that sequence can vary depending on our input data. the thing where depending on the size of the sentence, our computational graph might need to have more or fewer elements. So that's one kind of common application of dynamic graphs. For those of you who took CS224N last quarter, you saw this idea of recursive networks where sometimes in natural language processing you might, for example, compute a parsed tree of a sentence. So having a neural network that kind of works, it's not just a sequential sequence of layers, but instead it's kind of working over some graph or tree structure. point. So this type of thing seems kind of complicated and hairy to implement using TensorFlow, but in PyTorch you can just kind of use like normal Python control flow and it'll work out just fine. Another bit of more researchy application is this really cool idea that I like called neuromodule networks for visual question answering. So here the idea is that we want to ask some questions about images where we maybe input this image of cats and dogs, there's some question, what color is the cat, and then internally the system can read the question. dogs? Now we have maybe the same basic set of modules for doing things like finding cats and dogs and counting, but they're arranged in a different order. So we get this dynamism again where different data points might give rise to different computational graphs. But this is a bit more of a researchy thing and maybe not so main stream right now. But as kind of a bigger point, I think that there's a lot of cool, creative applications that people could do with dynamic computational graphs and maybe there aren't so many right now, just because it's been so painful to work with them. Caffe.is this framework from Berkeley. Which Caffe is somewhat different from the other deep learning frameworks where you in many cases you can actually train networks without writing any code yourself. You need to define, now instead of writing code to define the structure of your computational graph, instead you edit some text file called a prototxt. Here the structure is that we read from some input HDF5 file, we perform some super new, it was only released a week ago. [laughter] So I really haven't had the time to form a super educated opinion about Caffe 2 yet. inner product, we compute some loss and the whole structure of the graph is set up in this text file. One kind of downside here is that these files can get really ugly for very large networks. So for something like the 152 layer ResNet model, which by the way was trained in Caffe originally, then this prototxt file ends up almost 7000 lines long. So people are not writing these by hand. People will sometimes will like write python scripts to generate these prototext files. just run the Caffe binary with the train command and it all happens magically. Cafee has a model zoo with a bunch of pretrained models, that's pretty useful. Caffe has a Python interface but it's not super well documented. You kind of need to read the source code of the python interface to see what it can do, so that's kind of annoying. But it does work. So, kind of my general thing about Caffe is that it's maybe good for feed forward models, it'smaybe good for production scenarios, because it doesn't depend on Python.