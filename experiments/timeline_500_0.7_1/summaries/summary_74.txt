This lesson will first dive into some signal Theory and then move on into things that we're more familiar with things like deconvolutions and using Transformers for next note prediction. The first thing we want to talk about is how can we sample and quantize a continuous time signal. The next thing we're going to kind oftalk about is changing forms right if if we're playing a song or playing a piece of music. And finally how how we can kind of generate sounds using these models. The analog to digital converter uses something called the sample and hold circuit. The digital analog converter is kind of the opposite except here we use a low pass filter. How many bits do we want to use to approximate a certain signal and the level of quantization here correlates very directly to the dynamic range of the signal that you're quantizing. How much time do you have to process this if this is something where for example a lot of musicians they want to sample their voice and Pitch it up very fast. is less than double of the highest frequency present aliasing will happen. This asserts that you need at least two samples per period. aliasing is the byproduct of poor sampling. A lower wave resolution will result in a modified output signal as compared to the original input that we're trying to process. There's a lot of literature about um aliasing effects including spatial illnessing. The aliasing phenomenon is incredibly interesting this happens both visually and auditorily. It's a very prevalent problem in any problem. that can tie in to reconstructing signals right um we we are also going to talk about um using deconvolutions. We talked about the unit architecture very thoroughly used for image segmentation we talked about it in our survey of uh computer vision techniques. This is something that uh that is kind of going to come back in terms of how we can reconstruct signals so the inner product and projections are an application of inner products where one vector can be projected onto another Vector and you can you can kind of see what the projection is based on. is that as our our basis differs as long as these are orthogonal vectors and they span the complete basis of a certain Vector we're able to reconstruct this Vector. The idea behind the the last two sections here was to give you motivation for for how signals work and how kind of classical reconstruction can occur using math that we're all familiar with. The next step here we want to use deep learning for reconstruction right where we are are reconstructing a low quality audio to high resolution audio. Transformers can help increase our sample of training data and generalize key scales and beats throughout a data set. A single song can be transformed into 12 songs of different Keys. The more data you have the better your model will be and the more generalizability you have in your Transformer the better it'll perform. Transformers will far outperform classical methods of of both computer vision and natural language processing. It's easier for machines to predict keys without flats and Sharps which has you know similar to what humans do. are the original Pachelbel's Canon um as you can see this does deviate a bit but honestly it sounds pretty good. The Transformer model is able to do this next note next sequence prediction pretty pretty well. So yeah there's a a lot to do in this field um a lot of really cool things happening um and yeah I hope you guys learned something about uh about generative audio today and are inspired to kind of give some of these things a try yourself. thank you guys for tuning in have a good one.