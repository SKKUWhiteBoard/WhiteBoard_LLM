homework two is out now. What the default projects will be, uh, for this class. Um, and you guys will get to pick whether or not you wanna do your own construction project or the default project. And those proposals will be due, um, very soon, er, in a little over a week. Are there any other questions that people have right now? Yeah. The assignments, [inaudible] are they limited to TensorFlow? asked the question if, if the assignment is limited toTensorFlow. I'm pretty sure that everything relies that you're using Tensor Flow. Last week we discussed value function approximation, particularly linear value function approximations. Today we're gonna start to talk about other forms of valuefunction approximation in particular, um, uh, using deep neural networks. We're mostly not gonna talk so much about enormous action spaces, but we are gonna think a lot about really large state spaces. And so, instead of having a table to represent our value functions, we were gonna use this generic function approximation where we have a W now, which are some parameters. Deep neural networks represent the Q function. linear value function is often really works very well if you're the right set of features. But there are all sorts of implications about whether or not we're even gonna be able to write down the true p- um, value function. So, one alternative that we didn't talk so much about last time is to use sort of a really, really rich function approximator class. The problem is, um, that the number of data points you need tends to scale with the dimension. The chain rule is used to try to do stochastic gradient descent. It means that you can take our output signal and then propagate that back in terms of updating all of your weights. So, you can use the chain rule to propagate all of this the- the gradient of, um, your loss function with respect to w, all the way back down all of these different compositions. The common choices are either linear so you can think of hn is equal to whn minus one. If it's nonlinear, we often call this an activation function. These sort of non-linear activation functions can be things like sigmoid functions or ReLU. thing is that, um, if you have at least one hidden layer, Um, if we have a sufficient number of nodes. nodes you can think of as a- if you're not familiar with this is basically just sort of a sufficiently complicated, uh, combination of features and functions. Um, this is a universal function approximators which means that you can represent any function with the deep neural network. So, that's really nice. We're not gonna have any capacity problems if we use a sufficiently expressive function approximation. instead of saying, "I'm going to have totally different parameters each taking in all of the pixels." I'm gonna end up having sort of local parameters that are identical and then I apply them to different parts of the image to try to extract, for example, features. So, the point of doing this is gonna be trying to extracting features that we think are gonna be useful for either predicting things like whether or not a face isn't an image or that are gonna help us in terms of understanding what the Q function should be. Deep learning can be used to detect different features in an image. It can do this by applying different weights to different parts of the image. You can also do this for multiple different types of features. There's a really nice, um, discussion of this that goes into more depth from 231-n, which some of you guys might've taken. Um, and there's a nice animation where they show, okay, imagine you have your input, you can think of this as an image, and then you could apply these different filters. as pooling layers. They are often used as a way to sort of down-sample the image. They can do things like max pooling to detect whether or not a particular feature is present, um, or take averages or other ways to kind of just down, ah, and compress the, the information that you got it in. In general, as soon as we start doing this function approximation even with the linear function approximator, then you can start to have this, uh, challenging triad, which often means that we're not guaranteed to converge.  RL is using deep function approximators to do Atari games. Atari games are often hard for people to learn. In these games, you typically need to have velocity. So, because you need velocity, you need more than just the current image. So what they chose to do is, you'd need to use four previous frames. So this at least at least, they're gonna use a particular input state. The state is just gonna be the full image. The action is the equivalent of actions of what you could normally do in the game. allows you to catch for a velocity and position, observe the balls and things like that. It's not always sufficient. Can anybody think of an example where maybe an Atari game, I don't know how many people played Atari. Um, uh, that might not be sufficient for the last four images still might not being sufficient or the type of game where it may not be. Yeah. Microbes exactly right. So like things like Montezuma's Revenge, things we often have to get like a key and then you have to grab that key. So you have. to sort of remember that you have it in order to make the right decision. approximators act. And the nice thing is that, I think this is actually required by nature. So you can play around with this. So how did they do it? Well, they're gonna do value function approximators. They're going to minimize the mean squared lost by stochastic gradient descent. Uh, but we know that this can diverge with value function approximation. And what are the two of the problems for this? Well one is that there is this or the correlation between samples. considered one update to take a tuple and update the weight. It's like one stochastic gradient descent update. Now notice here because your Q function will be changing over time. So, even though we're treating the target as a scalar, the weights will get updated the next round which means our target value changes. So this is nice because basically it means that you reuse your data instead of just using each data point once, you can reuse it and that can be helpful. learn to model, a dynamics model, and then the planning for that which is pretty cool. But we don't wanna do that all the time because there's a computation trade-off and particularly here because we're in games. There's a directtrade-off between computation and getting more experience. Can we use something similar to like exploitation versus exploration. Um, essentially like with random probability just decide to re-flag [inaudible]. The question is about how would we choose between, like, getting new data and how much to replay. that it does is it has fixed Q targets. So, what does that mean? Um, so to improve stability, and what we mean by stability here is that we don't want our weights to explode and go to infinity which we saw could happen in linear value function. Um, we're gonna fix the target weights that are used in the target calculation for multiple updates. We're gonna have, um, we still have our single network but we're just gonna maintain two different sets of weights for that network. In terms of stability, it helps because you're basically reducing the noise in your target. If you think of this as a supervised learning problem, we have an input x and output y. The challenge in RL is that our y is changing, if you make it that you're- so your y is not changing, it's much easier to fit. Dian's question is whether or not, um, we ever update the- Minus at all, or is that [inaudible]. Great question. w minus, yes we do. We pu- can periodically update w minus as well. So, in a fixed schedule, say every 50 or, you know, every n episodes or every n steps, you, um, update sort of like every- and you would set w minus dw. William? Uh, we notice, like, for w, there are better initializations than just like zero, uh, if you take into account, I guess like the mean and variance. Uh, would you initialize w minus just two w or is there like an even better initialization for w minus? It stores the transition in this sort of replay buffer, a replay memory, um, use sample random mini-batches from D. So, normally sample in mini-batch instead of a single one. You do your gradient descent given those. Um, you compute Q learning using these old targets and you optimize the mean squared error between the Q network and Q learning targets, use stochastic gradient descent, and something I did not mention on here is that we're typically doing E-greedy exploration. a lot, like, it wasn't sure of its movements like it moved around places often, like [OVERLAPPING]. Yeah. From the agent's perspective, particularly if there's a cost to moving, then it may just be kind of babbling, uh, and doing exploration just to see what works. So, there's been a lot of interest in these sort of games on the bottom end of the tail which often known as those hard exploration games. We'll probably talk- uh, we'll talk a lot more about exploration later on in the course. Replay is hugely important and it just gives us a much better way to use the data. Using that fixed Q here means you seem like a fixed target. You do replay and suddenly you're at 241. Okay, so throwing away each data point what- after you use it once is not a very good thing to do. Um, and then if you combine replay and fixed Q you do get an improvement over that but, uh, it's really that you get this huge increase, um, at least in break out in some of the other games by doing replay. Doubled DQN is kind of like double Q learning, which we covered very briefly at the end of a couple of classes ago. The idea was that we are going to maintain two different Q networks. This is to try to separate how we pick our action versus our estimate of the value of that action to deal with this sort of maximization bias issue. The second thing is prioritized replay. It turns out that it gives you a huge benefit in many, many cases for the Atari games. So, uh, this is something that's generally very useful to do. back to the Mars Rover example. So, let's say you get to choose two replay backups to do. Vote if you think it matters which ones you pick, in terms of the value function you get out. If you pick backup three, so what's backup three? It is, S2, A1, 0, S1. So that means now you're gonna get to backup and so now your V of S2 is gonna be equal to one. So you've got to back-propagate from the information you're already [NOISE] have on step one to step two. The number, of, um, updates you need to do until your value function converges to the right thing can be exponentially smaller, if you update carefully and you, you could have an oracle tells you exactly what tuple the sample. But you can't do that. You're not gonna spend all this. It's very computationally, expensive or impossible in some cases to figure out exactly what that uh, that oracle ordering should be. But it does illustrate that we, we might wanna be careful about the order that we do it and- so, their, intuition, for this, was, let's try to prioritize a tuples according to its DQN error.