In this problem, we're given a collection of 10 variables, x1 through x10, where each i, xi, is a uniform random variable between 0 and 1. And we'd like to develop a bound on the number of variables that make up a random variable. So each i is uniform between 0 to 1, and all 10 variables are independent. We want to know how many variables make up the random variable that makes up the collection of variables we're trying to work with. probability that some of the 10 variables, 1 to 10, being greater than 7 using different methods. So in part A we'll be using the Markov's inequality written here. That is, if we have a random variable, positive random variable x, the probability x is greater than a, where a is a. In part B we'll use the same inequality to work out how many variables are in the same range. We'll start with the 10th variable and work our way up. is again some positive number, is bounded above by the expected value of x divided by a. And let's see how that works out in our situation. In our situation, we will call x the summation of i equal to 1 to 10xi, and therefore, E of x is simply 1 to 1. We will call E of X the sum of i and x, and this will be the same as 1 to  10xi. We can now say that E of  x is 1 to 1, and E of X is 1 to 1. The expectation of the sum of the random variable is simply the sum. of the expectations. Now, we can invoke Markov's Inequality. It says x greater or greater than x1. This gives us 5.10 times E of x1, the individual ones, and this gives us 4.5 times E. We can also use the linearity of expectation such that the expectation is 5.1 times E, or 5.2 times E for a random variable. This is the same as the expectation of x greater than X. equal to 7. This is less than E of x over 7, and this gives us 5 over 7. For part B, let's see if we can improve the bound we got in part A using the Chebyshev inequality, which takes into account the variance of random variable x. Again, again, we get the same result, but this time we get it at a lower bound. We get the bound for part B of 5 over 6. This gives us the bound on part A, which is 5 over 5. The Chebyshev Inequality says the probability that x deviates from its mean E of x, by more than a is bound above by the variance of x divided by a squared. So we have to actually do some work to transform the probability we're interested in to the one we want. We use the formula below to work out how the probability of x deviating from the mean E is bound by the variance of x divided by the squared number of x. The probability of x minus 5 greater or equal to 2 simply by moving 5 from the right to the left. The Chebyshev Inequality.in, which is x greater orequal to 7, can be rewritten into the form that's convenient to use. To do so, we'll rewrite this probability as the probability ofx minus 5 less than 2. The probability ofX minus 5 more than 2 is x less than 7, or x plus 5 greater than 7. This can be written as x plus 1 greater than 2 or x minus 1 less than 5. reason we chose 5 is because 5 is equal to the expected value of x from part A as we know before. And in fact, this quantity is alsoequal to the probability that x minus 5 less or equal to negative 2. To see why this is true, recall that 5 is also the probability of x plus 5 being less than or equal in size to x plus 2. For example, if x is greater than 5 and x is less than 5, then the probability is that x + 5 is less orequal to 5. that x is simply the summation of the xi's, the 10 x i's, and each xi is a uniform random variable between 0 and 1. Each xi, the distribution of which is symmetric around its mean 1/2. So we can see that after we add up all the xo's, that x is the sum of all the xo's and the 10 i's. And therefore, each xo is the same as the sum of all the Xo's and the 10 i's. the resulting distribution x is also symmetric around its mean 5. And as a result, the probability of x minus 5 greater than 2 is now equal to the probability that x plus 5 less than negative 2. At this point, we have transformed the probability. of x greater or equal to 7 into the form right here, such that. such that that x greater than 7 is equal to 1/2 the probability x minus. 5 absolute value greater or. equal to 2, because this term right here is simply the sum of both terms here and here. we can apply the Chebyshev's Inequality basically directly. And we'll write the probably here being less than or equal to 1/2 times. Now, 2 is the same as a right here, and this gives us 1/8 times-- now, the probably is 1/4 times the probably of a right. And so on and so on, until we get to the point of the probably being 1/6 times the likely of a left. And that's the probably. variance of x, we know is 10 times the variance of a uniform random variable between 0 and 1, which is 1/12, and that gives us 5/48. Now, let's compare this with the number we got earlier using the Markov Inequality, which was 5/7. We see that 5-48 is much better than 5-7, and we can use this to get a more accurate estimate of the number of variables we are trying to test for. smaller, and this tells us that, at least for this example, using the Chebyshev Inequality combined with the information of the variance of x, we're able to get a stronger upper bound on the probability of the event that we're interested in. Now, in part C, we'll use a somewhat more powerful approach in addition to the Chebybsev In inequality, the so-called central limit theorem. Let's see if we can even get a bound that is stronger than the bound given by Markov's Inequality. a better bound. To remind you what a central limit theorem is, let's say we have a summation of i equal to 1 to some number n of independent and identically distributed random variables xi. We take the sum right here, and we say that xi is equal to the sum of n and i. Now, the central limit theory says the following. We say that xi is equal to the sum of n and the number of independent and identically distributed random varieties xi is equal to 1. subtract out its means, which is E of the same summation, and further, we'll divide out, what we call normalize, by the standard deviation of the summation. In other words, the square root of the variance of the sum of xi. So if we perform this procedure right here, then we'll get the result we're looking for. We'll call this the "normalization" function. We can also call it the "square root" function if we want to get the same result. as the number of terms in the sums going to infinity, here as in goes to infinity. We will actually see that this random variable will converge in distribution in some way that will eventually look like a standard normal random variable with means 0 and 1. And since we are looking at a random variable, we can see that it will eventually converge in a way that looks like a normal distribution with means 1 and 0. We can also see that the distribution will converge to look like this in the future. know how the distribution of a standard normal looks like, we can go to table and look up certain properties of the resulting distribution. So that is a plan to do. So right now, we have about 10 variables. It's not that many compared to a huge numbering, but again, again, it's a plan for the future. We're working on it. We have a lot of work to do, but we're getting closer to the end of the first phase of the project. if we believe it's a good approximation, we can get some information out of it by using the central limit theorem. So we are interesting knowing that probability summation of i equal to 1 to 10 x1 greater or equal to 7. We'll rewrite this as 1 minus the probability the sum of i and xi equals 1 to 1 and 10/12. And we'll do the same on the other side, writing it 7 minus 5 divided by square root of 10-12. of 10/12. If we believe 10 is a large enough number, then this will be roughly equal to 1 minus the CDF of a standard normal evaluated evaluated. Now, if we compute out the quantity right here, we know that this quantity is roughly 2.19, and by the central limit theorem, this is roughly 1 minus 1 minus CDF. This means that the quantity of 10 is about 1 plus CDF, or about 1 minus 10. This is roughly the same as 1 plus 10. at 2.19. And we could look up the number in the table, and this gives us number roughly, 0.014. Now let's do a quick summary of what this problem is about. We're asked to compute the probability of x greater or equal to 7, where x is the sum of x and the number of times it is greater than or less than 7. We could look at the table and this would give us number 0.19, which is roughly the same as 0.01. 10 uniform random variables between 0 and 1, so we'll call it xi. We know that because each random variable has expectation 1/2, adding 10 of them up, gives us expectation of 5. So this is essentially asking, what is the chance that x is more than two away from x? We'll call this xi, and it will be a function of the number of random variables we add up to get the result. xi is a function that takes the total number of random variables into account, and the result is called x. its expectation? So if this is a real line, and 5 is here, maybe x has some distribution around 5. So the center what the expected value is at 5, we wonder how likely is it for us to see something greater than 7? Now, let's see where do we go from here? We'll see where we end up in the next section of this article. Click here to read the rest of the article. The next section will focus on the next step in the analysis of the data. land on the probably spectrum from 0 to 1. Well, without using any information, we know the probability cannot be greater than 1, so a trivial upper bound for the probability right here will be 1. For the first part we use Markov's Inequality and that gives us some some some estimates of how likely it is to be a certain type of person. We then use the same method for the second part of the experiment. The result is that the probability of being a certain kind of person is 1. number, which is roughly equal to 0.7. In fact, we got number 5/7, and this is from Markov's Inequality. Oh, it's better than 1, but can we do better? Well, the part B, we see that all the way, using part A and part B. We see that we can do better than 2/2, and we can get to 1/1. We get to 2/1, and that is better than 3/3, and so on. the additional information variance, we can get this number down to 5/48, which is roughly 0.1. Already, that's much better than 0.7. Can we even do better? And this is the Chebyshev, and it turns out we can indeed do better. Using the central limit theorem,. we can squeeze this down to 0.2, which would be a much better result than the 0.8 we had before. The result is 0.3, which will be a good enough result for us to work with. number all the way down to 0.014, almost a 10 times improvement over the previous number. This is from central limit theorem. As we can see, by using different bounding techniques, we can progressively improve the bound on the probability of x exceeding 7, and from this problem we learned more about the central limit of the problem. For more information on central limit, go to central limit.org or go to http://www.centrallimit.org/. For more on bounding, see bounding.org. that even with 10 variables, the truth is more like this, which says that the distribution of x concentrates very heavily around 5, and hence, the probability of x being greater or equal to 7 could be much smaller than one might expect. That is, x is more likely to be greater than 7 than it is to be less than 7, and so the probability is much lower than one would expect. It is also more likely that x is greater than or less than 5, rather than equal to 6 or 7.