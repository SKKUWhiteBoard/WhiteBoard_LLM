Machine learning is about how to acquire a model, from data and experience. In the end, we want to build good systems. Where do you get accurate systems? You get them from good models. Good models come from good data, and we're going to look at that last part now. What we are going to do today is we're Going to start with model-based classification, and, as an example of that, we'reGoing to work through some details of how the Naive Bayes models work. takes on machine learning that are going to highlight different subsets of the big ideas on this topic. We're going to have a couple running examples. One of them is that spam classifier that pulls out all the emails you don't want from your email. And something like digit recognition, we'll start to give you a window into how other vision tasks work. We'll see more in-depth examples and more structured examples of these kinds of problems later on when we talk about applications. The boundary between what is actually spam, unsolicited commercial email, and emails you just don't want, can be a fuzzy boundary. Some people just click on an email they wish people hadn't sent them, even if it's like from their mom. "To be removed from future mailings, simply reply to this message and put Remove in the subject. 99 million email addresses for only $99." "I know this is blatantly off-topic, but I'm beginning to go insane. Had an old Dell Dimension XPS sitting in the corner and decided to put it to use" is going to be unique. It'll be at least one pixel off of something else you've seen. So you can't just collect all the data. You can get data that is similar, but then, in the end, you're going to have to generalize. And so, we'll talk about how that's going to work. What features might you use to detect digits? Well, somebody puts a grid of numbers. Your eyes and your visual processing system is already doing all kinds of processing. There's tons of classification tasks. It's probably the most widely-used application of machine learning. Classification, you're given inputs, you predict labels, or classes, y. Medical diagnosis could be classifications. Fraud detection could be, think about your credit card company. Automatic essay grading, auto grading, this can be a machine learning problem.. Review sentiment. Here's a bunch of reviews of my product. Which ones are good? which ones are bad? Have they gotten better in the past 10 days since the new announcement? And so on. In model-based classification, rather than directly learning from errors that you make in the world from experience, think like reinforcement learning model-free. The Naive Bayes assumption is an extremely radical assumption as far as probabilistic models go, but for classification, it turns out to be really effective and it goes something like this. You build a model where the output label and the input features are random variables. There's going to be some connections between them, and maybe some other variables too that might help you build a better model. A general Naive Bayes model places a joint distribution over the following variables, y, which is your class, and some number of features, which you get to define. You're going to have to write code which extracts them from your input. So if your spam feature is, have more than 10 people received this email in the past hour? The machine learning will do the work to connect the probability of that taking on a certain value up to the class. And that means when you go to make a prediction, it decomposes into a product of a bunch of different feature conditional probabilities. is we're going to need to figure out the prior probability over labels, and for each feature-- which is each kind of evidence-- we'll need to compute a bunch of conditional probabilities for each class. These things collectively, all these probabilities that we use to plug and chug and get our numbers out, are called the parameters of the model. So it has to come from data, which parameters we want. All right, let's see some examples of what these conditional probabilities in a Naive Bayes model would look like. So in this particular vector, in these parameters, each class 1 to 0 is equally likely. The Naive Bayes classifier predicts the probability of each word in an email. The most likely word for spam is the word free, and the most likely for ham is too. The probabilities are calculated incrementally, as the words come in, position by position. The terms that are going to show up on the left here are the terms for each word as it comes in, which is the product of the prior and the evidence for each class. The total probability of our running running here would be 033 for spam, and 066 for ham. model, the way they're aggregated is multiplying their conditional probabilities. Gary, would you like to lose weight while you sleep? And then once you've seen the whole email, and you look at the end, you'll notice two things. One, the total probability is very small, because I multiplied a bunch of probabilities. That's always true, in Bayes net inference. I have to divide both of these very small numbers by their sum to get the conditional posterior that I want. word depends on the class and also the previous word. This Is a better model of language. If you started, if you did prediction in this, and you cranked out a pretend document, it would look significantly more like a real email than if you just did a bag of words. Will it be more accurate for classification? It really depends. In general, it will be a little more accurate, but at a cost of having significantly more complicated conditional probabilities to estimate. All right. Let's take a break now. We are now into the machine learning section which means we are done with the spooky Halloween time ghost-busting section. But I have candy. Machine learning theory is based on trying to say something precise about the connection between data and training. The main worry is that you over-fit. The principle of empirical risk minimization goes something like this. We would like to find the model, classifier, whatever, that does the best-- whatever the best means-- on our true test distribution. We don't actually know that true distribution. So how do you make progress? Well, what you do know is you have a training set. And you can't pick the parameters which are going to do best. Accuracy is not a great metric for spam detection. The actual loss-- or cost-- of different kinds of mistakes may not be the same. Over-fitting means fitting the training data very closely, but not generalizing well. And so there are a lot of different metrics people have for different kind of tasks. And again, we're going talk a lot today and next time about over-fitting and generalization. We want a classifier which does well on the test data. And then we try to come up with methods where the training accuracy is going to mean something about the test accuracy. opposite, which is under-fitting, where you're just like, I don't know what's going on. That's not over-fit. It's not going to work very well. The problem here is not that you're test accuracy is low, but your training accuracy was also low because you didn't learn anything. Spam is being generated by people who are trying to defeat spam filters. Others are going to make it through, and spammers aregoing to double down on what's working. Now there's sort of an arms race here. over time, spam classification doesn't actually look like a standard classification problem because it's adversarial. Here's an example of this tradeoff. In general, we're going to do discrete classification. But for this example, let's imagine the thing we're trying to do is to fit a curve to this data. So I can pick a model. You don't want it to be too small, because if you over-fit, you're not going to be able to generalize. than the quadratic. It's about fitting your data to the point where the patterns that you are capturing are ones which generalize to test, and that's a tricky balance. Over-fitting shows up not just on these continuous functions. It also shows up, for example, let's imagine in a hypothetical digit classification, we might say, here is an image I've never seen before. So what would we do? We'd do our running total. We'd say, all right. Well, before I look at any features, the numbers are the numbers two and three,let's say, are equally likely. in a corner where there's no number. This is an example of over-fitting, because this probability versus this probability, that is about the idiosyncrasies of the samples I have in my data. What do you think, in my training data for ham versus spam, things with the highest odds ratio for ham would be? These are things that are significantly more likely for ham than for spam. Words like Gary, except when I look at my data, it's actually a mess. It turns out, there are a bunch of words in this data which occur in spam once, and in ham zero. of over-fitting, where the exact details of which sample points you drew when you collected your data get captured in a way that doesn't generalize. To do better, we need to smooth, or regularize, our estimates. In Naive Bayes probabilistic models, over- fitting usually shows up as sampling variance. For other methods, it's going to show up in totally other ways. The exact mechanics of over-fits are going to vary from model to model. The maximum likelihood estimate, basically, you have to work this out, right? Maybe we can just go back and do it real quick. OK. So let's say r is my probability of red, and one minus r isMy probability of blue. What is the probability of D? So, that's a thing you can do, and it's totally reasonable. But in practice, you need some smoothing. So I guess we have some Halloween ghosts after all, even though we're now into the post-Halloween lectures. But we want no surprises to our model. of this data? Well, it's basically I got an r, and then I got another r. And then I've also got the other thing, which is one minus r. So as I change the probability of red, this term is going to go up and down. And the balance, the point where that's going to be maximized you can sort of, if you set it up carefully, take derivatives, find the extreme point, you'll get the relative frequency answer out. the sun will once not rise. So I know that this estimate is wrong, and I need some way of mechanically incorporating the fact that there are events which I haven't seen, but which I know to be possible, or at least that I'd like to model as being possible. Laplace said, well, basically, it's a pretty good idea to take into account the probabilities in your observation, but you should hold out an extra observation for everything you didn't see to reflect it potentially happening at some point in the future. In a real classification problem, you have to smooth if you're going to use Naive Bayes. This is the top of the odds ratios for ham on the left, and favoring spam on the right. If you see money, that's a good sign that it's spam. There are some things that indicate ham. This looks like general English text. What is going on there? Helvetica vs. Verdana. If I crank down k, I fit more, and so I now have a dial which can trade off the amount of fitting against generalization. In general, your model is going to make errors. The k that's going to be most accurate on my training data is zero. That's the maximum likelihood estimate. We learn our parameters from the training data. We tune them on some different data, like some held-out data, because otherwise, you'll get crazy results. And then eventually, you're going to take the best value, do some final tests, test run. We're talking a bit more about features, because it's important for when we start to get to neural nets. In general, you're going to need more features. In spam classification, we found out that it wasn't enough to just look at words, you've got to look at other sort of metadata from the ecosystem. For digit recognition, you do sort of more advanced things than just looking at pixels, where you look at things like edges and loops and things like that. You can add these as sources of information by just adding variables into your Naive Bayes model, but we'll also talk in the next few classes about ways to add these more flexibly.