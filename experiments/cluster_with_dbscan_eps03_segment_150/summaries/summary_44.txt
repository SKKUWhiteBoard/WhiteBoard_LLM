need to know what gradient descent is. The bigger question we are trying to address here is that why many optimization algorithm. are designed for convex functions. But why they can still work for nonconvex functions? So why they still work and actually, pretty well in practice forNonconveX functions in deep learning. Of course, this is not a 100% statement because it depends on which function you are talking about, what you have, and so forth. But I'm just saying, for most of the cases in deep. learning, stochastic gradient descent or gradient descent seems to work pretty well. Strict-saddle condition is basically saying that you assume your function doesn't have this kind of somewhat subtle possible candidate of local minimum, right? So every point, whether it's a local minimum or not, can be told from examine only the first order gradient and the second order gradient, second order directives. So if you certify the condition, then you remove those pathological cases which requires high order derivatives. But there are also-- my bad. One, two doesn't tell you exactly whether a point is local minimum per se if you don't have that assumption. methodology also applies here when you talk about the Hessian. You just iteratively expand it, Taylor expand it into something like g of x plus some epsilon times some vector. And then if you have this, then this basically corresponds to v dot g square gxv. So if you apply these kind of techniques, you can get the. Hessian like this. So the quadratic form of the Hessia is equals to something like this, right? So we have to have this. And we know that the Hessians that are larger than 0 is equivalent to that. assume that the input are linearly separable, then there is a proof for this. And there are a bunch of other cases where you can have some partial results. Next week, in the next lecture, maybe the second half of next lecture,. I'm also going to give another result, which is somewhat more general. It applies to many different architectures, but it has other kind of constraints. First of all, it doesn't really show exactly these kind of landscape properties. It shows that these kinds of properties holds for a region, for a special region in the parameter space. We are going to start talking about the optimization perspective in deep learning for two lectures. In most of the cases, people observe that nonconvex functions in machine learning can be optimized pretty well by gradient descent or stochastic gradient descent. We're going to show some very, actually, simple cases where we can prove this. But I guess, as I mentioned, there is some caveat about whether you can even converge to a local minimum. So I'm not going to prove any of the theorems here.