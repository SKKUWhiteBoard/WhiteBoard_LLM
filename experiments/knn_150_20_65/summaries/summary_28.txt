okay so welcome to the last lecture of this course here in this winter term and what we discussed so far in the course were mainly the so-called backends or optimization engines or probablistic estimation techniques that were running kind of in the background solving the same problem. Today I would like to give a very very of course brief short overview about front ends and one important aspect inside successful front ends on how to determine if a constraint is likely to be a correct one so we are still interested in avoiding to add roam constraints although we've learned that we there are techniques which can deal with outliers in the data Association. to begin by matching observations so we have different observations depending on what platform. Other approaches use features for example we had those the Victoria Park where trees have been extracted from the laser range data. Third class of approaches uses feature descriptors most popular ones are for example sift and surf. These are three popular techniques or sensor information that front-ends use in order to make the data Association and way we actually look into those who is very short example examples during this course today okay so you're typically in the situation we say okay the robot is currently let's say at location a and let's assume the blue circle over here is a sensor range. moment and that's my sensor range I can compute where are those other pulses so in this case B 1 and B 2 just two examples could be more obviously and then I can also estimate what is the uncertainty of those poses B 1 or B 2 relative to a do that by eliminating the note a from my linear system and then inverting the resulting Hessian and looking to the main diagonal blocks this gives me the uncertainty here indicated by these dashed lines. Based on this information I know I can never have an estimate of given my current pose where's b1 where's B2 together with The Associated uncertainties certainty estimates. I reach all the posts I'm interested in looking into but this does is ignores the loop closures so the uncertainty estimates are too big but you can still argue that okay uncertainty estimates I get are toobig but I can compute this extremely efficient and I may inspect a few places too much but I should get all the places which I need to inspect in order to make sure I find the course with whatever 95 percent probability this is what is done. What is done in practice to what inverting this matrix age there so far we really tried to explicitly invert it. ICP is sensitive to the initial guess so one thing you can do is try to find arrange things into Maps instead of single scans this helps or we can separate the local perceptions into some parts let's take this wall so there's obstacle that you stick out and try to match them first we are less likely to end up in a local minima. If you have descriptors like feature descriptors it can actually help you to find good estimates where you can be so you don't have to try all camera polls and see if the camera poses match. darkest stripes these are simply small alignment errors of these individual maps they can they can see kind of small steps over here here that was also probably an alignment error which simply leads to her step which was let's say bigger than and all five centimeters in the ground and therefore it's classified as not reversible anymore and therefore everything is red over here. You can actually use 3d data to build a map of the environment the next example this is an autonomous car is a parking lot or a 3d model of a parking lots where yellow again means drivable areas and red means non drivable area. an initial inertial measurement unit and one of the advantage of this system is it gives you also the gravity vector this quite accurately at a high frequency. If you know the gravity back door you can eliminate already two of the six dimensions from your state space because the roll angle and the pitch angle can be determined. Based on that I can buy a triangulation compute where are the points in the 3d space give me that I only need to know two features two of those descriptors with their corresponding 3d coordinates in the image in the best 100 features which match my current. God used exactly the same approach but only a single camera and the SONA which was measuring the depth information and so this was a blend. The tas of the blimp was always to hover on top of this location which was here marked by the book i think somewhere over here or here and so it's always trying to hover. Whenever it hovered this is the hovering location someone took it and throw it away so robot was going somewhere else building a map of the scene trying to find again a place which has seen before. an existing part of the environment has a good estimate where it is that is what we refer to as localization and the last part which is loop clothing so given I kind of I don't know where I am some a large uncertainty and I you can use this approach to see how well do the features that I see at the moment mattress features have seen in the past and try to find an alignment for this this is a good alignment you may accept that or you may try this for a couple of consecutive frames that not just kind of one bad image screws up everything. Tasker builds a map online and use the map in order to make navigation decisions of where it should actually go. It's an online process which requires us to built the map to update the map and always come up with an consistent estimate of the map. Tasker then generates steering commands which always guide the platform back to the desired location in this case again it should hover here at one location okay these are two problems with this we discussed or we said how can we get around them. The platform is actually a little bit tricky for the platform to always hover or people walking by and yeah and then someone again pushing the platform away. talk which I Neff was more over more whatever like wait overview about how different approaches work was not going to too many details. The second part of the talk today I would like to talk about ambiguities in the environment and what are good ways for dealing with them. How can we actually even though we have environments with ambiguity build accurate maps consistent maps of the environment so they are are so or the main assumption here is not we simply ignore all n big you T's and say the environment has no ambiguity. There are multiple hypotheses how it can match inside and they overlap therefore it's local the other ones non-overlapping its global so I don't know how this a fits in here so does this guy over here fits this one this one or this one so either here here or here simply something I it's good fit maybe yeah doesn't sound too bad just add them to kind of a temporary constraint list and this is shown here in red so this one can Michigan this pot this post this post is against this opposes both this post and this guy again these two poses some of them will be likely to be right. same author I've been all since ethanol in this group who develop this approach and later on max mixtures so this would be one nice application for mac semesters but assuming we don't have max mixture we have to treat those things separately okay and what can you we can actually do two tests the first one is a global sufficiency test so we want to say there is no possible disjoint match in the uncertainty lives it means a cannot be completely somewhere else it's not possibly that a can be somewhere come at a completely different place. Every vector consists of zero and once it is one hypothesis about the consistency of my matches. If all of them are zero which means it's completely incorrect it's just one. If I have a group where everything agrees I can add all the ones once once once in this vector and I will get a high score. I get scores among the groups but not between each other. I have this function just high values for both elements and low values for bad hypotheses. I try to find the vector B which maximizes this fraction that's exactly what is done. this constraint that is an np-hard problem this is a corresponding densest subgraph problem which is an NP- hard problem sort of find the best v actually need to try out all possible solutions which is something which for a large number of constraints simply doesn't work out therefore the ID years okay III know how to compute it but it's to computationally demanding to do that let's see if you're trying to find the approximation out of that and actually find a pretty good approximation for that by saying okay I simply don't treat my vector V as discreet I just allow continuous variables because then I can actually optimize this and then get a solution. and if I have multiple solutions for that get MA multiple. The larger the eigen values are the better the score so there's a proof that i get a perfect combination I get a couple of eigen vectors with current putting eigenvalues. If I visualize this so one situation over here so this is kind of the first eigenvector second eigen vector third force this is Lambda i if this is a kind of a high value. This is the best solution that I have and all other solutions are much worse in performance.