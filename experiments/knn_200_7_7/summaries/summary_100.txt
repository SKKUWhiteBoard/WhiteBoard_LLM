The EM algorithm is an unsupervised version of the k-means GMM algorithm. In GMM, you guess randomly an assignment of every point to the cluster, the probability. You guess where they're probabilistically linked, that is, what's the probability of these points belong to cluster one, this point belongs to cluster two. And then once you have that, you then solve some estimation problem that looks like a traditional supervised learning. The decomposition is quite important. And we're going to try and kind of abstract that away. CNN's John Sutter takes a look at an algorithm that maximizes a likelihood function. The algorithm is called EM and is based on convexity and Jensen's inequality. Sutter: "If you understand this in the simplest way, then understanding the algorithm will make a lot of sense" "This is the rough algo on the thing I'm just giving you," Sutter says. "I'm not giving you enough math because otherwise, the math is kind of bizarre-looking" In the next lecture, we're going to see a different example, which is called factor analysis, where z has a very different form. And it's meant to constrain the problem in a different way. We went through in this lecture a couple of different steps. We started with that convexity piece so we could get an intuition for what these functions look like. And then we went through the EM algorithm, in the near future. And I think that's all I want to say. Any last questions before we head out? A function is going to be convex if its graph is, OK? As I said. OK, so let's draw an example of this. So here's 0. Here's minus 1, and I draw this character. So this is f equals x squared. It's a parabola, kind of a here. I pick b here, and the line between them is below the set. This is not convex. So let's look at this in symbols. Does that make sense? Just translating the definitions directly. Amitai Etzioni: I'd much rather answer these than badly draw the pictures that come next. We're going to get to those pictures no matter what, so there's really no saving us. All right, so I'll try and leave this more or less on the screen. Here's the algorithm. This is a picture, which maybe won't make perfect sense to start with, but we'll get there. I hope this is OK for people to see. If not, let me know. Great question. So what's going on here if you remember Taylor's theorem is you can keep expanding, and then you have the last term, which is the remainder term. The remainder term says, there exists some point that lives in a to b such that this holds with equality. By the way, this is really not important for your conceptual understanding. The real reason we want to go through the derivative thing is, otherwise, this thing, which we actually do care about, is strongly convex. Jensen's inequality says that no matter how I pick the parameters of that curve, anywhere that lives on this thing, that's a probability distribution, a bunch of numbers that sum to 1 in the discrete case. And that's going to allow me to build a lower bound for my function, and I'm going to hillclimb using it. We're going to use that in a second to draw some curves of a likelihood function that will hopefully be easier to optimize than the original function. Now, everything is defined in the literature traditionally for convex. If you take convex analysis, it's the way we define things. We actually don't want to use a convex function here because we're maximizing likelihood. And this is just notational pain, right? Like, if we were-- maybe we should have minimized unlikelihood. So we need concave functions. And what are concave Functions? g is concave if and only f minus g is convex, all right? So we flip it upside down.  EM algorithm has max likelihood-- I'll actually put MLE. MLE is a way for us to compare different parameters. We have some data, i from 1 to n. These are our data points. We take a log of the probability that we assign to the data given our parameters. And recall, these are the parameters, params, and this is the data. So far, so good? All right, now, we're working with latent variable models. P(x; theta)-- this is a generic term, right? This is just one of the i terms-- says the function factors. what is probability of x parameterized by theta actually represent in this case, in that photon example? Yeah, exactly. So P(x; theta), we've been using forever. We used that from the supervised days. We just inserted z and said, well, there's this wild z that we can't observe, but it somehow constrains x. It means that x-- like, the relationship between theta and x. And that's what the model does. The algorithm starts with an initial guess. It's always below the loss, so it's kind of a surrogate that I'm not overestimating my progress, and it's tight. Then we maximize that, and this is formalizing the back and forth. We take that new maximum that we have, which is our new best effort of parameters, optimize that thing in a second, pick our new theta(t) then repeat and do another curve that's present. It kind of looks like a supervised thing. So you're picking these concave kinds of functions, which are easy to maximize. curve, L of t. And then the M-step, and together, EM, given L of T, set phi t plus 1 equal to argmax phi Lt(phi). Cool. Just could you reiterate? Like, why are we not using gradients on the original turbulence? Right, so we could imagine doing some kind of gradient descent here, but it's not clear how to deal with this marginalization that happens in the middle. So if we did some marginalization or some sampling, we could do something that looked like that. LZ: How do we construct L of t? LZ: I claim we know everything else. So let's look at a single term in our equation, OK? So I'm going to grab one of these characters, just one, and work with that. So I want to pick Q such that log P of x, z; theta over Q(z) equals C. Now, before, I had all kinds of freedoms so on? You could also-- we'll see later-- replace it with an integral if you had something really fancy. have some theta at some time. You know the data point that you're looking at. And you say, what are the most likely values of the cluster linkage-- as we were talking about before, the source linkage-- for this particular point? You get a probability distribution over those. You set them to Q(i)z. That's really what's going on. It's your estimate of how likely that is. Then you take an M-step. Theta t plus 1 equals argmax over theta of Lt(theta), which equals-- so Lt( theta) equals this ELBO sum. This is an x. This is a Q, this is a theta. Do you use that equation [INAUDIBLE]?? Right. So it's just as before. t starts at 0. We have that initial guess, and then we go from there. Theta is our current guess. Cool. All right, why does this terminate? And it's basically for something that's kind of not very interesting or satisfying, but it does. This gives you a sequence that is monotonically increasing or nondecreasing. we wanted to understand-- what was the probability? This says, the probability that i, the i-th component, belongs in j given what we've observed about x(i) and what we know about the cluster shapes and their frequencies. So if you remember, we had this diagram that I drew quite poorly the last time that said we had these two bumps, which were our two Gaussians, let's say, in one dimensions that looked like this. This is a Gaussian distribution with center j. And the question is, you give me a point here. How likely is it to belong to 1 or 2, to cluster1 or 2? Right? That's basically what we're asking. phi 2 was hugely bigger than phi 1, right-- a billion points came from the second source. So to automate this, this is Bayes' rule. It just weighs those two probabilities and tells us what should happen. That's it. We ran through exactly those calculations last time. All right, let's take a look at the M-step now. We have to compute derivatives. So we're maximizing here over all the parameters, phi and mu and sigma, sigmas. always use brackets for this. It's historical, and I would love to beat it out of myself if it were possible. Does the covariance depend on j? Right now, the covarianance does not depend on z. So it depends on j. All right, so now we can compute some fun derivatives, OK? So let's compute mu j of fi of theta. We have to estimate the mean, right? Now. Let's do it-- actually, I'm going to do something slightly harder. It'll be just one extra line because it's all linear. Yeah, w doesn't have anything to do with lambda j. So it should be crossed out. Is that true? Let me show-- let me see something crazy here. But it will be-- sorry, I see what's going on. This is 1/2, and this is a minus. w(i)j is taken the derivative of the log. So when can we pull this thing out that's repeated? Because it's full rank, we can pull it out, and it's linear. So we want to set this to 0 and use that sigma j inverse. you what happens in phi j. Would you mind showing [INAUDIBLE] scrolling up? Sure. No, wait. I just want the last one. OK, sure. Also I would say, ahead of time, I do post all the notes online. Please feel free to take our reference to those notes too. They will have potentially fewer typos than me trying to answer questions, draw, and generally be distracted. Can't focus that long. They still do have typos, though, so always look at the notes. It's along the line, OK? So the question is, how do you encode that information that you want to kind of screen off information that's orthogonal to the line? And I'll write up a little note to show this whole thing. What you do is you introduce this thing called Lagrange multipliers. And if you haven't seen them, don't worry. These are super easy to teach. Just say this-- it's just an extra term here. And this multiplier is basically the thing that screening off things that are orthogonally to these constraints. It's just doing the average, which was weird to look at before. But that allows us to compute all the things in the way we would expect. Here, it's totally natural. In this case, it makes total sense, though, because these numbers have to sum to 1. So if you don't have a normalization constant here, you're adding up a bunch of numbers which individually sum up to n, right? The sum over all of them is n. And this is just the principle that tells you, you have to normalize them by this n factor, OK? which we formalized as kind of back and forth with using these curves over time. Once we had those curves, what was happening was we would pick and optimize on those curves. The Q(i)'s played a starring role. Those became our w's here, and they kind of add nastiness to all of the equations. They just add little weights and expectations everywhere. And then we ran exactly the kind of standard supervised machine learning, if you like, or the stuff that you've been doing for MLE for the entire quarter.