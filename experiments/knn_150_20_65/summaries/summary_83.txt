Lecture eight is about deep learning software and how the hardware works. We'll talk a little bit about CPUs and GPUs and then we'll talk about several of the major deep learning frameworks that are out there in use these days. We're in the process of assigning TA's to projects based on what the project area is and the expertise of the TA's. So we'll have some more information about that in the next couple days I fine tune them for your own problem. But before I move on, is there any sort of questions about CPU and GPU? Remember to stop your Google Cloud instances when you're not working to try to preserve your credits. For assignment two you really only need to use GPU instances for the last notebook. For all of the several notebooks it's just in Python and Numpy so you don't need any GPUs for those questions. And the final reminder is that the midterm is coming up. We're also in the process of grading assignment one, so stay tuned and we'll get those grades back to you as soon as we can. The midterm will be in class on Tuesday, five nine. It'll be sort of pen and paper working through different kinds of, slightly more theoretical questions to check your understanding of the material that we've covered so far. And I think we'll probably post at least a short sort of sample of the types of questions to expect. So just, Yeah, yeah, so that's what we've done in the past is just closed note, closed book, relatively just like want to check that you understand the intuition behind most of the stuff we've presented. about fancier optimization algorithms for deep learning models including SGD Momentum, Nesterov, RMSProp and Adam. And we saw that these relatively small tweaks on top of vanilla SGD, are relatively easy to implement but can make your networks converge a bit faster. We also talked about transfer learning where you can maybe download big networks that were pre-trained on some dataset and then GPU. But that being said, I think there are still pretty substantial speed ups to be had here. this point about what exactly these things are and why one might be better than another for different tasks. So, who's built a computer before? Just kind of show of hands. So this is a shot of my computer at home that I built. And you can see that there's a lot of stuff going on inside the computer, maybe, hopefully you know what most of these parts are. And the CPU is the Central Processing Unit. That's this little chip hidden under this cooling fan right here near the top of the case.  Torch is in Lua, not Python, unlike these other things. Torch doesn't have autograd. Torch is also older, so it's more stable, less susceptible to bugs. PyTorch is newer, there's less existing code, it's still subject to change. So it's a little bit more of an adventure. But at least for me, I kind of prefer, I don't really see much reason for myself to use Torch overPyTorch anymore at this time. Caffe was from Berkeley, Torch was developed originally NYU and also in collaboration with Facebook. Theana was mostly build at the University of Montreal. These kind of next generation deep learning frameworks all originated in industry. So it's kind of an interesting shift that we've seen in the landscape over the last couple of years is that these ideas have really moved a lot from academia into industry. And now industry is kind of giving us these big powerful nice can compose together these modules to build big networks. In Numpy, you kind of need to write out of these frameworks. In TensorFlow, you can really just, like with one line you can switch all this computation between CPU and GPU. You'll first have a bunch of code that builds the graph and then you'll go and run the graph many many times. So this is the really, this is kind of the big common pattern in Tensor Flow. And now here is where we're actually running the graph. So we're just creating concrete arrays for X, Y, and w1 and w2 using Numpy and then storing these in some dictionary. TensorFlow gives you a bunch of convenience functions that compute common neural network things for you. In this example we were computing the loss explicitly using our own tensor operations, TensorFlow can always do that. But in this case we can also use tf.mean_squared_error and L2 so we don't just use the loss for us, we can just use L2 for us. And here we're now only feeding in the data and labels X and Y and the weights are living inside the graph. And then you might think that this would train the network, but there's actually a bug here. TensorFlow is smart and it only computes the parts of the graph that are necessary for computing the output that you asked it to compute. When we tell TensorFlow we want to run a tensor, then we get the concrete value. But because of this dependency we've told it that updates depends on these assign operations. These assign operations live inside the computational graph and all live inside GPU memory. So then we're doing these update operations entirely on the GPU and we're no longer copying the updated values back out of theGraph. So we've got this full example of training a network in TensorFlow and we're kind of adding bells and you. So one example that ships with Tensor Flow, is this tf.layers inside. So now in this code example you can see that our code is only explicitly declaring the X and the Y which are the placeholders for the data and the labels. And now we say that H=tf.l layers.dense, we give it the input X and we tell it units=H. This is again kind of a magical line because inside this line, it's kind of setting up w1 and b1, the bias. It's setting up variables for those with the right shapes that are kind of inside the graph but a little bit hidden from us. Top of TensorFlow handles sort of building up these computational graph for you up in the back end. Keras also supports Theano as a back end, so that's also kind of nice. In this example you can see we build the model as a sequence of layers. We build some optimizer object and we call model.compile and this does a lot of magic. And now we can call. model.fit and that does the whole training procedure for us magically. But most of the time you will probably not need to define your own autograd operations. want it to optimize over the parameters of the model. Giving it some learning rate under the hyper parameters. And now after we compute our gradients we can just call optimizer.step and it updates all the parameters. So another common thing you'll do in PyTorch a lot is define your own nn modules. So typically you'll write your own class which defines you entire model as a single new nn module class. And a module is just kind of a neural network layer that can contain either other other modules or trainable weights or other other kinds of state. So now here in the initializer of the class we're assigning this linear1 and linear2. a chance to play around with this myself so I can't really speak to how useful it is. Tensorboard actually lets you visualize the structure of the computational graph. And Visdom does not have that functionality yet. PyTorch is kind of an evolution of, kind of a newer updated version of an older framework called Torch. You kind of need to relearn a whole separate set of control flow operators. And if you want to do any kinds of control Flow inside your computational graph using TensorFlow. different where we're actually building up this new computational graph, this new fresh thing on every forward pass. That's called a dynamic computational graph. With a static graph you can imagine that you write this code that builds up the graph and then once you've built the graph, you have this data structure in memory that represents the entire structure of your network. Whereas with a dynamic graph, because we're interleaving these processes of graph building and graph execution, you kind of need the original code at all times. TensorFlow this becomes a little bit uglier. And again, because we need to construct the graph all at once up front, this control flow looping construct again needs to be an explicit node in the TensorFlow graph. So I hope you remember your functional programming because you'll have to use those kinds of operators to implement looping constructs. But what this basically means is that you have this sense that Tensor Flow is almost building its own entire programming language, using the language of computational graphs. point. So this type of thing seems kind of complicated and hairy to implement using TensorFlow, but in PyTorch you can just kind of use like normal Python control flow and it'll work out just fine. Another bit of more researchy application is this really cool idea that I like called neuromodule networks for visual question answering. So here the idea is that we want to ask some questions about images where we maybe input this image of cats and dogs, there's some question, what color is the cat, and then internally the system can read the question. inner product, we compute some loss and the whole structure of the graph is set up in this text file. One kind of downside here is that these files can get really ugly for very large networks. So for something like the 152 layer ResNet model, which by the way was trained in Caffe originally, then this prototxt file ends up almost 7000 lines long. So people are not writing these by hand. People will sometimes will like write python scripts to generate these prototext files.