The final contest, which is due tonight, is to design an agent that plays together with another agent to try to collect food pellets while not getting eaten by ghosts. submissions for that, your last chance to submit are tonight at midnight. And on Thursday in lecture, we'll discuss the results. What else is left? I think there is a project due next week. There is still a section this week. And I think that homework is all wrapped up, but you would still have a self-assessment of your last homework that will be due next. And then there's a final exam the week after that. be mostly on advanced applications. The idea behind these two lectures is to look at advanced applications, where we have covered a good amount of the material in the ideas behind those applications. We will not quiz you on these application lectures, on the final exam, or anything like that. It's more meant to give you more perspective rather than extra study materials for the final. So far, I've looked at foundational methods for search, for acting adversarial environments, for learning to do new things, and for dealing with uncertainty. Today's state-of-the-art in Go is that there are computer players better than the best human players. But actually, if you went back to March 2016, that was not the case yet. So how do you make an AI for Go? Let's go back to what we were looking at in lecture on games, MiniMax. MiniMax is about solving games in adversarial environments. And you reason about it. And in what it did, we had to update this graph that you see at the very beginning of the course, where we had already checkers fully solved. For a game like Tic-Tac-Toe, you will find out that you can force a draw, and that means fully solving the game. For Go, this is actually pretty hard to do, and it's even much harder than chess. And why? Let's take a look at chess. It's a 19 by 19 board, so there's 19 times 19 positions to choose from in the first move. And then one less, of course, next move. But the branching factor is enormous. So if you tried to run an exhaustive search through this kind of game tree, it's not going to work. DeepMind's AlphaGo is a computer program that can play the game of Go. It uses a deep neural network to learn how to search for the best moves. It can also learn to predict who is likely to win from a given situation. DeepMind is now working on a system that can learn to play Go without human help. It's called AlphaGo Zero, and it's being developed by DeepMind's Sergey Brin and Yann Leibovitz. It will be unveiled at the World Economic Forum in Davos this week. moment, ?] another [INAUDIBLE] PROFESSOR: There is something called fictitious play. In fictitious play, you kind of play against yourself in a slightly more complicated setting than this. So I believe it includes this, the result. By playing yourself, you're guaranteed to reach an equilibrium. Actually, what reaches the equilibrium is the average version of all your past selves. So rather than your actual current network being the equilibrium solution, it's the average of all past. AlphaGo Zero has no prior knowledge. It starts at a pretty, actually, negative elo rating. Loses all the time initially. It's playing itself, but then gets tested against a set of players. After 21 days, it goes past where AlphaGo Lee Sedol was. And then it was still creeping up after 40 days. Go, even though it's a master game, in principle, has a solution. Once you reach that level, essentially, there's no further to go, because you solved the game. With reasonable compute power, it traverses the whole tree. Even with alpha beta pruning, I don't think that'll happen anytime soon. It's a really big tree. If you do a more kind of brute force style approach-- not 100% brute force. It might be. But pretty much no one would say less than 20, and most people would say 50 or more. And that's maybe when we reach human level play. But that's a given. Now, with infinite computing power, for sure. Then you can definitely solve it. even longer. But the more brute force style approach, where you don't have good value functions, you donâ€™t have good policy functions, was expected to take decades before we reached compute levels that can do that. It could be that by using human knowledge, you're in some kind of based enough attraction. For a local [? optim, ?] that maybe not as good as another one that might be out there. I don't know if that would be the case or not, but that's a possibility. Some examples in a few other problems settings soon. And then the reinforced learning on top of that to further improve. That's kind of the standard way to solve a problem. From a research point of view, it's very interesting to see where you can get from scratch. OK, switching gears to helicopters. Here's a motivating example. How do you get a helicopter to do this autonomously? And by the way, this is done autonomously here, but how do we get to something like this? like this? Well, what does it mean to fly a helicopter? What are the challenges? There's two key challenges. One is tracking where the helicopter is at any given time, because if you don't know where it is, you'll not be able to control it very reliably. And then the other thing is to decide what to do, what controls is sent to the helicopter. Typically, have a remote control, which has two joysticks. You send controls from that to the helicopters. S1.1: How do you control a helicopter? We have four control channels, two in each joystick. A collective is the action for the main rotor collective page. It's the average angle of attack as the blade goes through the air, which modulates how much vertical thrust you generate. There is cyclic controls, longitudinal and latitude cyclic control, which determine the difference in angle from back and left right. So that way, you can generate a torque that allows your helicopter to roll or pitch based on how much differential thrust. pretty good. And then we run it on our helicopter. So this is our helicopter, indeed, flipping, which is great. It's moving more than we want it to move. And it went into the trees. So let's think about what happened here. What happened in the real world is we're trying to follow this path that we design where the maximal reward is. And so we had asked it to fly a path that's not flyable. So it starts deviating from that path. As it deviates, what it learned in the simulator becomes less and less relevant. saw it making these wild motions, overcompensating. It pushed the controls so hard that the engine died. The engine just couldn't push it, died. You lose control over your helicopter, more or less, at that point. Then what happened is our human pilot took back control to try to save the helicopter. And believe it or not, they actually saved this. It landed a little harder than you want to land, but it landed on its feet and it could be recovered from that. the helicopter to follow a path that's not flyable. So how do we get a specification of what we should be flying? Well, we could learn the trajectory from these as noisy observations. What methodology do we have for that? Hidden Markov models. If we collect paths from a human pilot and then ask the helicopter to fly those paths, they tend to be noisy. But if we collect many demonstrations, we figured, then, this set of demonstrations captures the essence of what it means. have something we don't know that evolves over time, but we have some noisy measurements of it, we can run an HMM to recover what we actually want. We use something called dynamic time warping. So what does that do? You can align two trajectories. After you do that, then you can run the inference and hidden Markov model, just standard based net inference, which, in this case, was just a guess. Then you run dynamic time Warping, which aligns each trajectory with the hidden one. But in the process, also aligns all the demonstrations, because they're all aligned with this one reference. is an extended common filter/smoother, which is a forward/backward path, similar to what we covered, but done for continuous variables rather than for discrete variables. Let's then re-infer, through probabilistic inference, what the hidden state might be. Keep repeating this till we reach some fixed point, and that will be our target for our helicopter to fly. What does this look like? Here is, in white, the target found through the hidden Markov model inference, and in color, still the demonstrations. We have collected data to learn a dynamics model for the helicopter. We can now penalize our award, penalized for deviating from the target. And then we can run reinforcement learning in simulation, let's say, in this learn simulator to find a good controller and run it on the real helicopter. The controller we learn in simulation is still a little optimistic about really following that path. So while we fly the helicopter, we'll do depth-limited search to improve what we have. able to look ahead only two seconds, rather than needing to look Ahead much further. A value function tells us, OK, how good is it to end up here? We also have a reward at each time tick. And our search over those two seconds is what results in the control we apply. Here's what we get. So fully autonomous. Takeoff, flipping over during takeoff. Hover. So this method can also learn to hover, no problem. Then it goes into forward flight and it's going to do something called split [? us, ?] where you do a half roll, half loop. The fastest we flew this helicopter was close to 55 miles per hour, so almost highway speeds. The algorithm's only this big, so it's pretty fast for something of this size. Inverted flight. Knife edge fall. Stall turn again, coming out tail first. Hurricanes are fast backward flying circles. And then now are actually some of the hardest maneuvers to execute on. Why is flying 55 miles an hour not harder than this? Well, when you do something like this and hear the tick tock, the hardest maneuver in this air show. Berkeley PhD student Woody Hoburg set up the helicopter to learn from scratch. Hoburg shut it off whenever it starts tilting itself, so it lands on some pretty wide landing gear so it's more stable. He was able to have it learn to hover reliably with the only human input being shut off when it looks like it might start doing something dangerous. But that was the onlyHuman input required. We did not push that further to flying those maneuvers. There is some work to be done. at OpenEye, there's been some work on robots learning to do back flips. And that was kind of one step further. It wasn't just shutting it off. You would watch your robot try things. And human input would be not specifying a reward function, which is very hard to do for things like back flips, just like it was hard here. But what they did is they said, the human watches it and says, which one is better or worse among a set of them? it has more time, and if it already has a recovery controller, then you can imagine that. And Claire Tomlin's group here at Berkeley has done some work in that direction, where they have a safe controller and a learned controller. And the learning controller is learning on its own while the safe controller keeps things in check so the helicopter doesn't crash. So what we used there is a model-based reinforcement learning method. So we learned a dynamics models for simulator from data that was collected. To learn a good controller in the simulator, we used something called iterative LQR. is a separate linear feedback controller for each time slice. And the way you learn that feedback controller is by doing a forward pass to see what your current sequence of controllers achieves. And then you can do a backward pass, which is, essentially, a value attrition pass over that same trajectory to find the optimal sequence of feedback controllers. So essentially, value iteration, but in a continuous space. A continuous space is always harder to represent things, so that's why you make a simplifying assumption. We assume that we are just going to use a sequence of linear controllers. follow a new path. Along that new path, we linear the dynamics, because dynamics' not really linear. We approximate it linearly. We do another backward pass along that, and keep repeating this until it's converged. Once we've done that, we have value functions everywhere and we have linear feedback controllers everywhere. If there is no wind, you can actually just run the linear feedback control. It will be fine. But if there's some wind gusts that could throw you off, you want to use the value functions and the two second look ahead against those value functions. Walking tends to be harder than flying for robotic control. If you're flying, you're up in the air. Everything is the same. Unless there's some weird air flows, that's the only thing that really changes. When you're walking, the surface could change all the time. A lot more change in your environment. Here's an example of how hard this can be. This is a video from 2015, there was the Doppler Robotics Challenge, which was held in Pomona, just east of Los Angeles. of Los Angeles. People had two years to work on this. And what did the robot have to do? It had to, essentially, drive a car or walk, but driving the car was recommended. Then get out of the car, walk a little bit, open a door, grab a drill, drill a hole, walk some more. So doesn't sound that complicated. But actually, it turned it's very complex to get a robot to do that. And so it did not sense correctly what is going on in the world and so did not react correctly. or not you're already making contact, and making contact or not. You can be very close, but not have contact. It's a very subtle thing. You don't have contact, you don't get to apply any forces. So it's very hard to do this. Now, what's changed recently in the past few years is that through advances in deep learning, it's been possible to better map from raw sensory information to controls you might want to apply. And ultimately, it learns to walk. The reinforcement learning algorithm can be reused directly onto other robots and can learn to control these other robots. This can work directly for building video games. You build video games, you want your main character to move in a realistic way. You can have it sequence together motions like these and dynamically simulate how they interact with the world, rather than key frame every little detail. The reward function is the closer your head is to standing head height, the better. So sitting is better than lying on the ground and standing is even better. High level control problem, that's actually a star search. If you have a cost function for this terrain, what would the cost function be? Where do you want to place your feet? Well, you maybe don't want to be next to a big cliff. The three on the ground-- the support triangle of that-- if you project down the center of mass of the robot, she'll maybe fall within that support triangle, because then it won't fall over. So there's a bunch of considerations you might have. When we thought about this problem, we had 25 features we came up with that we thought matter. When you run the search, or the value iteration, which is, more or less, equivalent to find a path across this terrain. But if you choose the trade-off between the features differently, you'll find different paths. So reward learning. How you do reward learning, you demonstrate a path. Demonstrating doesn't mean just drawing a line. It actually means choosing a sequence of footsteps that it executes on, and assuming it does well. It assumes you have a low level controller, but that's well understood how to do that. Stanford robot becomes first vehicle in history to drive 132 miles by itself. Five robots remain on the course. To finish, they must wind through a treacherous mountain pass. After months of tireless effort, there's a lot at stake. The first time it's ever been done, autonomous vehicles. A vision they all share will now be put to the test. The Dartmouth Grand Challenge is held every year at Dartmouth University in Dartmouth, Dartmouth, New Hampshire, and Dartmouth College in Hanover, New York. work, and was very impressive that, in fact, four cars finished the 150 miles. That's a Berkeley entry. Only motorcycle in the race. Now, what goes onto those cars? There's a few different things. There is IMU, like right on a helicopter, a lot of computers. The GPS compass. Regular GPS to get position. Lasers, where you shoot out laser beams. And based on how long it takes them to get back, you know how far away the nearest obstacle is in that direction. With a camera, you can often look further ahead. LIDAR sends out a laser beam, measures how long it takes to get back. Raw measurements might give you 12% false positives. With HMM, you get 0.02%false positives of where there might be obstacles.an obstacle. It would see that the readings are different and decide it needs to steer around that, hopefully. If you're an urban environment, there'll be a lot more obstacles. A lot of progress has been made this is video from 2013. So after 2005, 2005 was a desert race. The Google Self-driving Car Project has been working on self-driving cars for a few years. This was before deep neural networks were heavily used for this kind of thing. It's only getting better to recognize what's in scenes, thanks toDeep neural networks. So what does it tell us? Well, the devil is really in the details, in the long term. The future is very bright for self- driving cars, but we have a long way to go before they're ready for prime time. tail of special events that can happen when you're driving. You can measure progress by just demo videos, which is one way, and it gives you some kind of feel for what's going on. But the 2013 video is already very impressive. So another way to measure progress is to see how are these cars doing relative to human drivers. So left and right are the same plots, but the ride is on a log scale so you can see more detail. It's a number of events per 1,000 miles driven. Red there is human fatalities. Then yellow is human injuries. is between 10 and negative 2 and 10 negative 3 per 1,000 miles of human driving. In green is the Google slash [? wave ?] mode disengagement. It's when the driver decides they want to take control because they don't trust the autonomous system right now to avoid an accident. And we see that it's going down how often that needs to happen, but still a bit removed from where humans are at. Where does this data come from? If you test in California, you have to report this data to the DMV. so many decisions. If they're gigantic, use a lot of power. That's a problem. Let's see what we can do to build smaller networks to make decisions. What else did we not cover yet? Personal robotics. I want to spend a little more than two minutes on that, so let's keep that for Thursday. that's it for today. Bye. [SIDE CONVERSATIONS] [Side CONversation] [sideconversation.com: Do you know more about this topic? Email us at jennifer.smith@cnn.com].