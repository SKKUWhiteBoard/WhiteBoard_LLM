going to discuss this more next time. So the high level thing is just that something else is driving the norm to be small. Thanks. Going to talk more about this in the next few days. Back to Mail Online home. back to the page you came from. Back To the pageYou came from: Back to thepage you camefrom. Back into the page You came from was from: The Daily Mail. Back onto the pageyou came from, the DailyMail.com page you were from. So I think I just spend like five minutes, just briefly review on the backpropagation last time. So I didn't have time to explain this figure, which I think probably would be useful as a high level summary of what's happening. Like this is the forward path. This is how you define network and the loss function. So you start with some example x, and then you have some-- I guess, this is a matrix vector multiplication module, and you take x inner product multiplied with w and b. And then get some activation, the pre-activation. And you get some post activation.