Causality says that the policy at time t prime can't affect the reward at another time step t if t is less than t prime. This is another way of saying that what you do now is not going to change the reward that you've got in the past now. In the next portion of today's lecture we're going to talk about how we can modify the policy gradient uh calculation to reduce its variance and in this way obtain a version of the policy gradients that can be used as a practical reinforcement learning algorithm. always true for any process where time flows forward the only way this would not be true is if you had time travel and you could take an action or travel back into the past and change your action but we're not allowed to do that all right. i'm going to claim that the policy gradient that i've derived so far does not actually make use of this assumption and that it can be modified to utilize this assumption. i've simply rewritten it and what i've done here is i use the distributive property to distribute the sum over rewards into thesum over grad log pies. over time steps from 1 to capital t of grad log pi at that time step multiplied by another sum over another variable t prime. At every time step i multiply the grand log probability of the action by the sum of rewards over all time steps in the past present and future. If we generate enough samples eventually we should see that all the rewards at time steps t prime less than t will average out to a multiplier of zero and they will not affect the log probability at this time step in fact we can prove that this is true. the proof is somewhat involved so i won't go through it here but once we show that this is true then we can simply change the summation of rewards. Instead of summing from t prime equals one to capital t simply sum from t Prime equals t to capitalt basically discard all the rewards in the past because we know the current policy can't affect them now. For a finite sample size removing all those rewards from the past will actually change your estimator but it will still be unbiased so this is the only change that we made. that is it's the rewards from now until the end of time which means that it refers to the rewards that you have yet to collect basically all the rewards except for the ones in the past or the reward to go. We will get much more into this in the next lecture when we talk about extra critical algorithms but for now we'll just use a similar symbol with a hat on top to note that it's a single sample estimate all right now the causality trick that i described before you can always use it you'll use it in homework two. think back to this cartoon that we had where we collect some trajectories and we evaluate the rewards and then we try to make the good ones more likely and the bad ones less likely that seemed like a very straightforward elegant way to formalize trial and error learning as a grain ascend procedure but is this actually what policy gradients do well intuitively? We can show that subtracting a constant b from your rewards in policy gradient will not actually change the gradient in expectation although it will change its variance meaning that for any b doing this trick will keep your grading estimator unbiased. this is equal to b times the gradient with respect to theta of one but the grading withrespect to thea of one is zero because one doesn't depend on theta. For a finite number of samples it's not equal to zero so what this means is that subtracting b will remain will keep our policy gradient unbiased but it will actually alter its variance so subtracting a by a baseline is unbiased in expectation the average reward which is what i'm using here turns out to not actually be the best baseline but it's actually pretty good. baseline to optimally minimize variance so to start with we're going to write down variance. The variance of the policy gradient is equal to the expected value of the quantity inside the bracket squared minus the whole expected value squared. The first term in the variance doesn't depend on b but the first term does so then in order to find the optimal b i'm going towrite down the derivative d var db and solve for the best b. The baseline actually depends on the gradient which means that if the gradient is a vector with multiple dimensions if you have multiple parameters you like to have a different baseline for every entry in the gradient. different policy parameters you'll have one value of the baseline for parameter one a different value for parameter two. In practice we often don't use the optimal variance we just uh sorry we typically just use the expected reward but if you want the optimal baseline this is how you would get it all right so to review what we've covered so far we talked about the high variance of policy gradients algorithms. We talked about how we can lower that variance by exploiting the fact that present actions don't affect past rewards and we can use baselines which are also unbiased.