RAMESH RASKAR: So this is a position, and this is superposition. And that concept of a position or superposition applies to all three types, shadows- or refraction- or reflection-based techniques. So we saw this last time, and we'll see how-- we already have some projects that are inspired by biological vision. You know, Matt is trying the chicken. And I think it's going to be-- [LAUGHTER] It is going to. Coded imaging is a co-design between how you capture the image and how you process the image. In a typical film camera, or even it is digital camera, you take the picture, and that's basically the end of the story. Here, you're trying to do something clever about how the picture is taken. You can either take a really short exposure photo. But that's going to be very dark. If you take a high ISO, you can recover some information. Or you can just take a long-exposure photo by keeping the shutter open. Photographer RamESH RASKAR: When you try to recover this information, you start getting this banding artifacts. And we'll see it in the next slide, why that happens. If you keep the shutter open for even longer, it'll blur correspondingly longer. So you can basically represent that as a sharp photo, where there is a convolution of the sharp photo with some kind of a Convolution filter. But then you will get a blurry photo, which is well exposed, but a lot of high-frequency details are lost. have basically a 1D convolution that's converting this image into this image. But the Fourier transform has some zeros, so you cannot divide those frequencies by 0 and recover an image. So the culprit here is really this low pass filter where some of the even lower frequencies are also being nullified. And there's nothing you can do to recover those frequencies because in the Fouriers domain, all you have to do is take the Fouries of this and divide by this. And it will give you the image. box function, which is equivalent to-- when you release the shutter, opening the-- release your shutter button-- opening the shutter and keeping it open for exposure duration and closing it. Instead of keeping the shutter open for the entire duration, you open and close it in a carefully chosen binary sequence. So at the end, you still get just one photo. But now something magical has happened because first of all, if you look at this number one, you'll see that it's not the same as before. It has-- it seems to have these replicas. frequencies-- they're all preserved. Of course, they're attenuated. It's not as high as-- it's not 1.0.0, it's reduced. Maybe it's 0.1 or so. There is still some hope to recover this photo back from this because, in the denominator, we will not have seen. this because of the shutter. So of course, if you try to implement this mechanically, where you open the shutter and then mechanically try to close the shutter, that will be problematic. The idea is very simple. Instead of keeping the shutter open for the entire duration and getting a well-exposed photo, the shutter is open for only half of the time. The support for the representation of the Fourier domain of that function that you describe there is infinite, right? So you actually truncate this in order to-- RAMESH RASKAR: It's not infinite because you still have some width. AUDIENCE: Right, but you have infinite high frequencies there by the sharp conditions. I mean, you can think of this one goes to infinity. But there's hardly any energy left. So although it went to infinity, there is not much energy left in the process. When you get to invert the process then, that's why you're still not getting the perfect images to-- RAMESH RASKAR: In this case as well. You still lost some high frequency, right? You haven't seen the results yet for this. But it's a very controlled experiment in a laboratory. but not 0. If you invert the process-- RAMESH RASKAR: From here, this is what you get. It cuts off and corresponding to the width. The width of this post was very short than yesterday were very far away. And that's this loss of these frequencies also shows up as these artifacts at regular frequencies, at regular intervals. So, again, this one-- this doesn't go to infinity all the way. It's open for two milliseconds, open for four milliseconds. filter from space? RAMESH RASKAR: It corresponds automatically to filter in space. So your 52 vector is going to stretch or shrink based on how fast the object is moving. And you mostly have to think about image space motion because the speed in the real world and-- the distance are they get-- you divide to normalize by the distance. If you're in a dark room, you can just strobe the light, rather than opening and closing the shutter. So you only have to worry about the image space distance. mobile demo of that scene. You need to know the motion or the direction of the motion. If we did 100 milliseconds, it picks up speed. Some parts will go faster and slower. It's acceleration in the measurement, but in the real world, it's still constant speed. You can either go to object space or you can come back to image space to make sure there is no acceleration. It is all linear. So you get a reasonable result. But going back, what are the limitations of this method? Yes. have multiple cars, for example, and they're all independent, then it's fine. As long as it's moving in a straight line at a constant speed, you're OK. But if the two cars overlap, what happens? Our model fails again. If two cars are partially overlapping during the exposure, it's possible, but it's more challenging because you don't know exactly how fast the two car are moving. So it's just like-- AUDIENCE: OK, but that's just so you can get more light. When I take a picture, the camera automatically decides what the exposure time should be. Similarly, you should look at the speed of how things are moving maybe with an ultrasound Doppler or whatever. You need to know how much the blur is. Another major disadvantage is let's say I want to take this bottle. And if I just rotate it and motion blur that, it will not work. For any point in the front that you're looking at it, it'll work. But the point that was in the back, that all of the 52 sequence-- maybe for the first 10, it was occluded. And the remaining 42, it's seen. during that 52 window. In general, the technique works well when things are moving naturally. But if somebody wants to do this kind of an experiment, or move things behind an occluder and move out, those are very challenging scenarios. Can you combine both horizontal and vertical [INAUDIBLE] masks? RAMESH RASKAR: Vertical, horizontal is fine. You can-- it doesn't matter. It could be moving vertically. Basically, your point spread function-- the blur function will be vertical rather than horizontal. of that object moves in a straight line, OK. It doesn't matter which direction and what speed. So the problem here really is the point spread function or the blurred function is very critical. And this is what we want to study about half of the class. And the concept is very, very,Very interesting because light is linear. So eventually, it's very linear. What happens to a point happens to the rest of the object. So if I have a car that's moving, and I tell you how exactly one point of the car is behaving in the image, I can tell you automatically how the other points are behaving. All of it is going to have the same behavior. same spread image. So you can either-- for experiments, you can just put an LED on the car and see how that LED moves. And that tells you everything. There's also an impulse response. And when you're trying to find a speed of a car, [INAUDIBLE],, a very small impulse. And it answers and comes back. It does. The point spread function for your time of flight. So that's the same concept here. You just want to call leading the world, take a picture, andsee how it works. to engineer activity of the camera. So in this particular case, a point that was moving created a blur like this. And by engineering the time point spread function, it stops looking a bit like that. And then it just turns out that this one is easier to deal with than this one. So that's the basic concept, engineering or actively changing the point spreadfunction. So this is very counterintuitive because you would say, let me just build the best lens and the best exposure time. And so that kind of mimics the human eye. hope, there is some computational technique, that will allow you to go from here to here. As far as I know, animals don't have deconvolution circuitry or deep-learning circuitry. I can look at a blurry image and kind of figure out. I mean, this was a challenge for you, right? Right. So we have pretty sophisticated eyes, but we're still not able to deep learn what this is. If you have some prior knowledge of how the Volkswagen logo looks like, maybe you can say, OK, maybe that was this. But on the other hand, you're immediately willing to believe that this photo is a blurred version of this photo. to recover some information. The goal of coded imaging is to come up with clever mechanisms so that we can capture light. The circuit is very, very simple. You just take the hot shoe of the flash, and it triggers. When you lose the shutter, it triggers the circuit. And then you just cycle through the code that you care about. What can we do for defocus blur that is for motion blur? What can you do fordefocus blur? We, again, want to engineer the point spread function. would you apply spatial coding? AUDIENCE: Coded aperture? RAMESH RASKAR: C coded aperture. So this is coded exposure, coded aperture-- very easy. And all you're going to do is put some kind of a code in the aperture of the lens. And this is how, actually, it started in the days of-- in scientific imaging, especially in astronomy, coded apertures are very well known. So I've been following this for a long, long time. And I thought, it must be useful for something in photography. system. It took almost two years to realize that to put this coded aperture in a camera, there are only a few places where you can put it to get good results. So out of that came this particular experiment. And that was just a graph paper. And then we said, OK, let's come back and think about this. What's going on? Why don't we get good Results? So it took nearly two years. To see the full interview, go to CNN iReport.com. This is a standard film lens, which, of course, can also be used with a digital camera. It's 100-millimeter focal length lens. When you focus with this, it works in very interesting ways. It has to deal with chromatic aberration, geometric aberrations, such as radial distortion. So it has to move all these lenses with corresponding ratios, OK? So I'll pass this around, and you'll see that there are these notches on this lens that are in a parabolic fashion. When you think about a visual camera, you make this very simplistic assumption. That is a pinhole, and there's a sensor. When you put a lens, we assume that the center of the lens is the central projection. So when you change your f-stop and decrease it and increase it, it's all happening in theCenter of projection. For professional lenses, that's not true. But for normal cameras, you have theCentral projection. But again, conceptually assume that all the rays are going through that point. Ramesh Raskar: If you put the sensor all the way here, you'll see the whole code. If you start moving away, the code will shrink. When it's auto focus, we just see the code, all right? By the way, this is the same idea behind another project, which is [INAUDIBLE]. So the idea came around at the same time of how to make this happen. And eventually, when you put it here, we get another code. That's exactly what's happening here. I'm in a different mode. If I look at this picture, you will see that-- so this is a sharp photo. It's blurred with disc. And it's blurred With that function. You can already see that it seems to preserve slightly more information. But it's still-- you won't be able to with your naked eye. You'll not be able. to figure out what underlying patterns are. After the blurring, you can. do these simple tricks, where the person you're interested in is out of focus. But then you can refocus digitally. So this is the input photo and the stock photos, all right? its Fourier transform is 52 long. So there are 52 entries here, and almost all of them are the same. If I just take a square aperture, a traditional one, and take asquare transform, it will look something like this. So a Fourier transforms of 7 by 7 will have a peak in the middle. But the rest of the time, it's more distributed instead of just all being near the center. It's more like a crossword-puzzle-shaped item, should be easy. In communication theory, everything is [INAUDIBLE].. We think about carrier frequencies of radio stations in frequencies. And convolution, deconvolution-- much easier to think in frequency domain. Although all the analysis in the frequency domain, at the end, the solution is very easy-- just flutter the shutter or just put. the values will be constant. And that's the magic of a broadband code. So if we're placing a broadband. code, certainly we have an opportunity to recover all the information. a coded aperture. Extremely simple solution to achieve that. It's very similar to the [INAUDIBLE].. AUDIENCE: Half the light. Very good. RAMESH RASKAR: Are there disadvantages? Or challenges? Not really disadvantage. Remember, in the. in the photo, at a distance, take our false photo. They will all look like this. At a distance,. At adistance, takeOur false photo, they will allLook like this, or you could put hearts in it, or, like-- motion case, we had to know how much the motion is. But the size of the blur is dependent on what? AUDIENCE: Belt. RAMESH RASKAR: The belt. But not just depth-- depth from the plane of focus, right? So that's an extra parameter you would estimate somehow. Maybe you can use a rangefinder or something like that, or just a software. There are methods you can employ. That's what you would do, like, in a light field, when we did the refocusing. the depth. When it comes into sharp focus, my edges, that must be the right depth. Unfortunately, it doesn't work out in this case. The main reason is that, because it's coded aperture, no matter where you refocus, it still looks like it has very high frequencies. So that makes it challenging. Yes. And we won't go into the detail, but the main reason for that is that it's a coded aperture. So you need to find this 7-by-7 pattern or even the previous case, the 52 pattern. And you take a random sequence. I said, by the time I come tomorrow morning, I'll find a really good code. And I came back next morning. Nothing had happened. I waited all day. It was still running. And it never came out of that. So 2 the 52 is pretty challenging. But even if you use a cluster, it's still a pretty big number. And there are all these theories about how to create different codes for different applications. So you can start with some code and do a gradient descent and so on. good solutions for 2D. But for 1D, there are some really good solutions to come up with that. For 2D, for certain dimensions, they call it one more 4 or three more 4. Basically, when you divide by 4, the remainder can be 1 or 3. And there are certain sequences that are beautiful mathematical properties, of which sequences could have broadband properties and which may not. So it turns out you cannot-- there's a little bit of cheating going on here. filter to the beginning of the signal. This particular filter is actually not circular, but it's linear. So when you apply the filter here, when you start applying the filter at the end of the image, you don't go back to the front. For circular convolution, the match is very clean and beautiful and smoother course work. Or for linear convolution,. there is no good mechanism. So we came up with our own code called RAT code, R-A-T, which is after three quarters. convolution-- I mean the linear convolution is basically circular convolution with a lot of padding of 0s. Finding a code that's 1,000 long is nearly impossible. So, yeah, so it seems like can just choose a random sequence and get a similar property. But actually, it doesn't work. The chances of arandom sequence doing the right thing for you is very, very low. Instead of [INAUDIBLE].. [LAUGHTER] AUDIENCE: Are astronomy people are already using-- In astronomy, you have circular convolution because they use either two mirror tiles and one sensor or one mirror tile and two sensors. If you tile aperture, you'll get really horrible frequency response, unfortunately, because if you put two tiles, that means certain frequencies are lost. But that's because our eyes are not very good at thinking about what the original image could be, given either this one or the previous one. So given this, I can challenge you that you're not able to predict that it has all this structure, right? yet. There is no medium filtering or smoothing or anything. It's just pure x equals b, x equals a backslash b. What's amazing about coded imaging is that the math is elegant and beautiful and sometimes complicated, but the implementation is very easy. At the end, all I had to do is put this code or shutter it, and very easy to explain. All right, so let's move on. OK, let me finish this one. We only saw two ways of engineering the point spread function. question. RAMESH RASKAR: Yes, go ahead. AUDIENCE: What if the mask was not quite? If you have some information by the board so that you could set up approximate [INAUDIBLE].. RAMESHRASKar: So what would you have? RASkar: In case of aperture, yes. It doesn't have to be opaque or transparent. It could be a continuous value. It turns out that, for any continuous code, there is a corresponding binary code that will do an equally good job. so far. And that's because in a binary code, you get to play with the phase function. Mike? AUDIENCE: Has anyone tried to combine the coded aperture and the coded [INAUDIBLE]? RAMESH RASKAR: That's a great idea. People talk about it, but nobody has done it. It's like we are sick of it, so we don't want to do it. But I think it's worth trying. And because those are orthogonal motion blur. Can you use both at the same time and record? There are orthogonal technologies, basically. RAMESH RASKAR: Exactly. So it's amazing because motion is time, and the focus is space. They're completely Orthogonal. So you can play with it. It's very interesting. Eventually, it's going to have a 2D projection. Yeah, eventually, it'll be able to do that. It'll be very interesting to see how it'll work in 2D. It will be very different from what we've seen before. at the top-- I'm sorry, at the bottom. The bottom one goes at the top. And when you think about the cross-section of all the straws, it's kind of cylindrical, when they all come together. So no matter where you are, the image is out of focus but by the same amount. It turns out that from that, you can recover images. Like, so this is open aperture. We discussed it in the class, so I hope you remember that. saw this right in the very first class, by the way. And the benefit of that, it turns out, is that it preserves the spatial frequencies, and it has the benefit that, no matter which steps you are at, you have the same defocus blur. So the disadvantage of coded aperture was that you need to know what the depth was to be able to deblur. But now, because it's independent of depth, you can just apply the same deconvolution and get back a sharper image. adding small matchsticks on top of the main lens-- or the way they do it is they actually put one single sheet that looks like that, an additional layer of support, a face mask. A face mask basically means you are changing the face of incoming light. That's why, as we learned about at the beginning, if you have something very far away, this slows down a little bit. This is the Syrian optic solution or the [INAUDIBLE],, which is actually bought as another company. the name. The solution is very similar. I'm sure they're fighting out in court right now. Same solution. Instead of putting this particular guy, that's just going to add some extra glass, but mostly in a minor form. It's just [INAUDIBLE] on that one. So basically the same solution but creating different focal length for different [? partners. ?] AUDIENCE: Yeah. Although you said, I mean, there's this portion there, where if you have another blur [INAudIBLE],, right? RAMESH RASKAR: Right. blur is only about 10 pixels, no matter where you [INAUDIBLE]. So maybe that was the matter. If you have a point of access, it's still going to create an image that's blurred 10 pixels. This is, again, very counterintuitive, where you go to make the image intentionally blurred. It's just that it's blurred everywhere. And then we also saw this one very early on, where the point spread function-- typically when something goes in and out of focus, it looks like a point. some point, they'll stay the same. RAMESH RASKAR: The xy still remains traditional microscope 1 micron, 1/2 micron. But the z-dimension is 10 nanometers. They are still working on a lot of these concepts. OK? So let's very briefly look at compressed sensing because it's something you should be familiar with. It's a very cool idea, by the way. As a scientist, I really like it. But when somebody like Technology Review or Wired Magazine says, Top 50, Top 10, of course, I wish I'm listed among them. Raskar: Single-pixel camera was listed as one of the big things in 2005 by Technology Review. The idea is, instead of taking one single photo, what you're going to do is-- let's say that's your scene. You're go to turn on-- you go to take a single photodetector and aim it at a set of micrometers. And one at a time, if you go through this million pixels, you will get a million megapixel image, right? you had this photo as a JPEG. In a composite, it might take up only about tens of thousands of bytes. So if I can represent the image with 10,000 bytes, and I'm going to. take a photo and compress it down to 10,00 bytes, I can't just directly measure only 10.000 values in the scene so that I save on everything, OK? So I can take this picture effectively with just 10, 000 pixels but recreate a million-pixel image. That's where the concept of compressive sensing or compressed imaging comes up. can transform the image and measure in [? your ?] measurements. In general, this scheme doesn't work. But you will continue to see people who come to you and say, you know, I have this magical thing I just heard or compressive image something, and that will just solve a problem. There are certain images, like cartoons, that can be represented with very few samples because they have flat regions, sharp boundaries, and fluctuations. But a natural image, unfortunately, cannot be transformed that easily. Danish photographer RamESH RASKAR: Compressive sensing allows you to build a camera with a single sensor. But do we really want it just to do compressed sensing? He says if somebody can build this and show that you can take fewer measurements and recover the image, that's a breakthrough. He says the secret of success for film, of film photography, is that if somebody had given you this problem before the invention of film, that there is a scene-- and I want to give you a sensation of the same scene. Photography is a record of visual experience, which is great for humans, but not so great for computers. What computers care about are all these high-level features. That's why we're going back to the drawing board and saying, let's build cameras that are not mimicking human eye but actually extracting more information, like [? apertures ?] that we remove the flash camera, or additional information with light-field cameras or multi-spectral cameras and so on. of-- so Brett was asking, why would you want to do [? precisely? ?] When do you have to reduce the number of measurements? And I think one of the problem [INAUDIBLE].. I don't know. The debate about whether it's really better or not is photography. RAMESH RASKAR: Tomography, yeah. Tomography is a very high-dimensional signal. There are only a few places. If you think about taking a CAT scan of your body, there are only four or five types of materials. Compressive sensing allows you to take less measurements. But the problem is you need to actually have more information about the scene before you take the measurement, which is another measurement. Compressive sensing is not used in anything commercially currently, but a lot of people are getting grants. It's a high-dimensional signal, 2D camera and 2D projector, but what you're trying to recover is two dimensional. So tomography is the same, it's 4D capture for 3D representation. which is how to write a paper and wishlist for photography. Which isHow to Write a Paper and Wishlist for Photography: How to Write A Paper and Write A Wishlist For The Camera. For more information on writing a paper or wishlist, go to: http://www.cnn.com/2013/01/30/photography/how-to-write-a-paper-and-wishlist-for-photography-how- to- Write-A-Paper-And-Wishlist.html.