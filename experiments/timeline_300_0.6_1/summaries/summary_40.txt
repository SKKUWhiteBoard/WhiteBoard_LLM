Week 6 of CS224N is now past the halfway point. Today is the day that you have to have done the mid-quarter survey by. If you haven't, this is your last chance to get the half-point for that. Final project proposals are due. We really encourage you to try and hand them in on-time. And then today, delighted to have our first invited speaker. Danqi Chen, one of the foremost researchers in question answering, and she's particularly well known in recent work. here to give this lecture on question answering. The goal of question answering is to build systems that can automatically answer questions posed by humans in our natural language. Question answering, or, let's say QA in short, is one of the earliest NLP tasks, and the early systems can even date back to the 1960s. The question and answer has enabled a lot of really useful real world applications. For example, today today we can answer questions like, "What do worms eat?" and they'll finally retrieve the answer, that's the grass. if you just put your question in a search engine like Google. And those kind of systems are also able to handle more complex questions like how-to questions. So the question is, how can I protect myself from COVID-19? So there isn't really a simple and short answer to this question. So you can see that the system actually returns a very long paragraph, including the best way to prevent illness is to avoid being exposed to this virus. So actually this paragraph is actually a summary from CDC article. So this is also one type of question answering problems. Question answering is probably one of those fields that we have seen the most remarkable progress in the last couple of years driven by deep learning. In this lecture, I will be mostly focusing on the text based, or textual question answering problems. And another class, bigger class of the question Answer problems is called visual question answering. So this problem basically requires both understanding of the questions and also images, and is actually a very active field between the computer vision and NLP. So if you have interest in these type of problems, I encourage you to check out those problems, but I'm not going to go into them. dig into these problems today. So next, I'm going to start with a part 2, reading comprehension. So reading comprehension is a basic problem that we want to comprehend a passage of text and answer questions about the content. So basically to answer this question, so you need to find this sentence, like, in 1861, Tesla attended this school where he studied German, arithmetic, and religion, and only German is a language. So this is really just similar to how we humans actually test the reading comprehension test to evaluate how well we actually understand one language. Stanford Question Answering Dataset is a supervised reading comprehension dataset. It consists of 100K annotated passage question and answer triples. Each answer is a short segment of text, or we called it span in the passage. And this kind of a large scale supervised dataset are also very key ingredient for training the effective neural models for reading comprehension. But also just to caveat, that this is also a limitation. Because not all questions have the answers in this way. But today, so this dataset, I forgot to say that. This dataset was collected in 2016 by several researchers at Stanford. Stanford, so it's called Stanford Question Answering Dataset. Today, after four or five years now, so SQuAD still remains the most popular reading comprehension data set. So the state-of-the-art AER systems still have time to just train or sequence tagger on top of the word. Danqi, one question you might answer is, so if you can do other tasks like named entity recognition or relation extraction by sticking something ontop of BERT and fine tuning for it or do it as a question answering, does one or the other method work better? answer to that. So next, I'm going to talk about how to build neural models for reading comprehension, and in particular, how we can build a model to solve the Stanford Question Answering Dataset. So the input of this problem is let's take a context or paragraph. And also we take our question, Q. And the question consists of n tokens q1 to qN. And because the answer has these constraints, the answer must be a section of text in the passage. We are going to predict a start and then end.  query-to-context attention is trying to measure the importance of these context words with respect to some question words. So by taking the max for each row in this matrix, so it's basically trying to see, OK, which question word is actually most relevant to this context word? And then you can just apply your softmax. And then this will give you a probability that OK, what is the probability of this condition i, would be based on the start position of the final answer string. the dot product between w end and this vector, and this can produce all the probability over all the conditions which predict how likely this position will be the end the position of the answer. So by passing the mi to another bedirectional LSTM, their reasoning is that they're trying to capture some kind of dependence between the choice of the start and end. OK, so this model is actually achieved-- like on SQuAD data set, it achieved a 77.3 F1 score. If you remove the context-to-query attention, the performance will drop to 67.7 F1 score. And then if you remove this part, it will drop a 4-point F 1 score. So basically this theory tells us that these kind of attention scores can actually capture the similarity between the question words and the context words. And now here is our attention visualization to show that how this smorgasbord of attention actually can capture the similarities between the questions and the contexts. BERT is a deep bidirectional transformer encoder pre-trained on large amounts of text. It is trained on the two training objectives, including masked language modeling and the next sentence prediction. The BERTbase has 110 million parameters and the BERT-large model has 330 million parameters. If you just take this BERT model, and by just optimizing all the parameters together, it can give you a very high performance. And even if you use a stronger pre-training models, they can even lead to better performance on SQuAD. proposed in SpanBERT. So first idea is that instead of using only the masking of individual words, we propose that we want to mask a contiguous spans of words in the passage. So we are trying to mask out all these possible answer spans from the passage as our training objective. And the second idea proposed in SpanberT is-- because at the end of it, it goes around to predict an answer span. So it's actually essentially try to predict two endpoints as well as the answer. Danqi can stay for a bit to answer questions, but not forever. Because she doesn't have a Stanford login, we're going to do questions inside Zoom. So if you'd like to ask a question, if you use the raise hand button, we can promote you so that you appear in the regular Zoom window and can just ask questions and see each other. If you hang around and don't leave the Zoom for more than a few minutes, maybe we'll just promote everybody who's still there into people in theregular Zoom for some bits of discussion. If we can leverage a very large and very powerful pre-trained language model, there is a possibility that we can actually do the question answering well with only a small number of examples. And also there are some other promising directions, including unsupervised question answering so by using some kind of approach like some form of un supervised machine translation. So this kind of model is huge, like what number? How many parameters I forgot in the GPT Stream model, yeah. So it's avery large variety of of model but-- OK. Do you want to ask a question? Yes. Next is PM on the East Coast of the U.S. Next is a question about the future of NLP. Do you think that in order to solve NLP in the sense that you can perform on par with humans on all NLP tasks it's sufficient to only interact with text data? Or do you think we'll eventually need the sort of experiences and common sense that we get only from seeing and viewing the world and having a set of interactions that we as humans have? Next is an open-ended question. yeah, I guess I'm just a little worried about who comes up with the test cases? Who determines what the right answer is? I mean, we will have more discussion of toxicity and bias coming up very soon, including actually Thursday's lecture as well as a later lecture, not specifically about QA though. OK. Next person is-- Thank you for the lecture. Yeah, my question is also related to the open domain question answering. So there is some very specific designs like domain server alignments and efficient level disentanglement techniques that has shown some interesting performance on other tasks. The key difference between the generative model and the extractive model is that for generative models, you can actually leverage more input passage together and do the generation. So for this model, there isn't any retrieval. So you can always find the answer from the question, right? So this model really has you relying on all the parameters you memorized, all the information. So it's actually very hard to-- yeah, it's a definite balance between memory and the generalization from it, yeah. There is a lot of interest in extending these question answering techniques or just encoding techniques, embedding techniques to recommender systems. The first question is whether these techniques can be generalized to other languages. If we have, actually, I think that the techniques could be generally applied to other language. There has been a lot. of work trying to do cross-lingual question answering and stuff like that. But there has been some constraints that a. lot of models or systems that I described here actually require very strong pre-trained language model. and also requires lots of training examples for the Pure-DSS.