When they were in the period of statistical NLP that we've seen in other places in the course. And then the idea began, can we start with just data about translation i.e. sentences and their translations, and learn a probabilistic model that can predict the translations of fresh sentences? So suppose we're translating French into English. We can say, what's the probability of different English translations? And then we'll choose the most likely translation. And we're not really expecting you to understand the details here. But I did then want to say a bit more about how decoding was done in a statistical machine translation system. In the period from about 1997 to around 2013, statistical machine translation was a huge research field. In 2014, the first modern attempt to build a neural network from machine translations and encoded-decoder model. Within two years' time, Google had switched to using neural machine translation for most languages. There are still lots of difficulties which people continue to work on very actively. And you can see more about it in the Skynet Today article that's linked at the bottom. We'd like to translate languages for which we don't have much data. model end to end on parallel sentences. And it's the entire system rather than being lots of separate components as in an old fashioned machine translation system. So these neural network architectures are called sequence to sequence models or commonly abbreviated seq2seq. And they involve two neural networks. There's one neural network that is going to encode the source center. And we're going to feed it in directly as the initial hidden state for the decoder, or RNN. But we do the same kind of LSTM computations and generate a first word of the sentence. In neural machine translation we are directly calculating this conditional model probability of target language sentence given source language sentence. So we have a source sentence. And that's going to strongly determine what is a good translation. And so to achieve that, what we're going to do is have some way of transferring information about the source sentence from the encoder to trigger what the decoder should do. And the two standard ways of doing that are you either feed in a hidden state as the initial hidden state to the decoding, or sometimes you will feed something in. that can sometimes improve performance. And we actually have that trick in the assignment 4 system. And you can try it out. So we generate along and generate our whole sentence in this manner. And that's proven to be a very effective way of getting more information from the source sentence more flexibly to allow us to generate a good translation. I'll stop here for now and at the start of next time. I will finish this off by going through the actual equations for how attention works. In week four, we take a break from learning more and more on neural network topics, and talk about final projects, but also some practical tips for building neural network systems. Today's lecture is the primary content for what you'll be using for building your assignment 4 systems. In the early 1950s, there started to be work on machine translation. Claims were made that the computer would replace most human translators. When you go in for full scale production what will the capacity be? We should be able to do about with a modern commercial computer about one to two million words an hour.