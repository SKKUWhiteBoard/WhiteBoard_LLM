This is part 3 in our series on distributed word representations. We're going to be talking about vector comparison methods. To try to make this discussion pretty intuitive, I'm going to ground things in this running example. On the left, I have a very small vector space model. We have three words, A, B, and C. And you can imagine that we've measured two dimensions, dx and dy. And then you can see graphically that B and C are pretty close together. And A is kind of lonely down here in the middle. corner, the infrequent one. Let's start with Euclidean distance, very common notion of distance in these spaces and quite intuitive. We can measure the distance between vectors u and v if they share the same dimension n by just calculating the sum of the squared element wide differences, absolute differences, and then taking the square root of that. So here we have our vector space depicted graphically, A, B, and C, and Euclidan distances measuring the length of these lines. As a stepping stone toward cosine distance, which will behave quite differently, let's talk about length normalization. There is valuable information in raw frequency. If we abstract away from it, some other information might come to the surface. But we also might lose that important frequency information in distorting the space in that way. And it can be difficult to balance these competing pressures. Finally, I'll just close with some code snippets. We're going to start to massage and stretch and bend our vector space models. And we will see much better results for these neighbor functions and everything else as we go through that material.