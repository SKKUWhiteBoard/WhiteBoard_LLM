This lesson will first dive into some signal Theory and then move on into things that we're more familiar with things like deconvolutions and using Transformers for next note prediction. The first thing we want to talk about is how can we sample and quantize a continuous time signal. We'll then go into some geometric signal theory and with Transformers and finally how how we can kind of generate sounds using these. The lesson will end with a soft introduction to digital signals and then we'll move on to the next lesson. not very easy for a computer to do given that every operation needs to be in on a continuous time signal. The way that we we can fix that is through the process of discretization or to to make a an analog signal a digital signal for us to be able to process. The two kind of main ways that we can make our signal easier to process is one by taking samples at certain time periods and two by quantizing our level so instead of dealing with A continuous scale we can quantize at certain levels for example a frequency of like 2 4 6 8 Hertz. signal to an analog output we can kind of start by talking about the ADC circuit. This uses something called the SARS ADC algorithm essentially what this is is a binary search to figure out what is my best digital approximation of my analog signal right so I have a continuous signal pass it through my sample and hold circuit. From here we take in our our input and our output is based on the amount of bits of precision that we want to have so depending on whether we want a two-bit approximation a three bit approximation. here we use a low pass filter which is also covered in courses like 16b um the motivation behind this is that we we maintain a signal that's the pass band. We allow every signal to pass when we hit certain cutoff frequency we Wane our signal by a certain factor and that factor dictates the slope of this line. Our signal will continuously Wane after that cutoff frequency is hit on the right is kind of the picture of the approximation where we can approximate a different quantization levels different bit parodies based on the Stars ADC algorithm. are the kinds of bins that we can put our signal into if we have three bits we have a lot more Precision we increase the amount of information that we have. With a bit depth of two our approximation isn't very great with a big depth of three um it's definitely getting better our errors reduce significantly. There is a trade-off though the trade off is compute power how much time do you have to process this if this is something where for example a lot of musicians they want to sample their. voice and Pitch it up very fast right what quantization level do we do we want there. Can we do a lossy pitch up with a uh with by filling in the the blanks in some intelligent way through prediction or kind of note fitting which is an interesting consideration I think given the fact that audio is a continuous time signal um the digitization process and the choices you make matter a lot and because of that this field is so interesting and there's a lot of really didactic work around how we can take these continuous signals. with a whole bunch of things in digital signal processing. If the sampling frequency is less than double of the highest frequency present aliasing will happen. This asserts that you need at least two samples per period. The aliasing phenomenon is incredibly interesting this happens both visually and um auditorily um and aliasing is an entire um topic just based on itself. As you you can see here your approximation gets very different as your your sampling rate increases and decreases if your assembling rate is 0.3 Hertz. also increases we're at one Hertz you're almost perfectly fitting the polynomial on the the points um something interesting to consider is that at a sampling rate of of 0.3 you are fitting a polynictional right because you're you're creating apolynomial through the points that you have. However this polynomal is very not indicative of the actual signal that you're trying to process right while we we can see that this this does kind of capture the trends of our data and that it falls when our data has fallen.  aliasing is the byproduct of poor sampling. A lower wave resolution will result in a modified output signal as compared to the original input that we're trying to process. frequencies that are higher than one half of the sampling rate will be automatically transformed to lower frequency sees that's where information loss stems from. There's a lot of literature about um aliasing effects um including spatial illnessing right um you see that uh there's there's a lots of different ways that aliasing takes place. we we take certain points along this you can see that the image changes we get kind of the gist of this um but this is a a compressed image it contains similar information it allows us to kind of see what's happening but it contains this in a very compressed format super sampling is where um you know this this image is uh take in this this 4x4 uh image and we're kind of running a kind of stride over top where uh we're losing some information in the background as you cansee it really Blends together but in the foreground. um this is is kind of uh and and add add-on um to this presentation it uh doesn't uh really contribute exactly to what we're talking about but I thought this was incredibly cool. With one line in C you're able to to generate Melodies um which is incredibly cool um yeah so please check it out if you if you guys have a chance. The next thing we're going to talk about briefly is geometric signal Theory and how that can tie in to reconstructing signals. Projections are an application of inner products where one vector can be projected onto another Vector and you can kind of see what the projection is based on that. The projection formula is the dot product between X and Y this is I guess you can think of it as a similarity measure between these two vectors. You're dividing this by the uh the norm squared and multiplying this back on X um and through that process you're applying a transformation that allows you to shift this vector. A high value means that they're co-linear right that they's they're very similar to each other. by a certain angle um and then rescale it onto a given vector so projections for reconstructions right um like I mentioned earlier a vector can be reconstructed with a linear combination from its projections onto another set of vectors if and only if the set used is a basis. By defining a Subspace of vectors of certain vectors linear combinations of these vectors can form any Vector in the vector space that I'm interested in um so given that a basis is required the set of vector used must be orthogonal to one another and this is very important as it ensures that that we're maximizing the amount of information gained. The idea behind the the last two sections here was to give you motivation for for how signals work and how kind of classical reconstruction can occur using math that we're all familiar with. This really covers any Vector in R2 um e0 uh we can Define as one zero so a horizontal Vector E1 is zero one a vertical Vector if we look at where we we're trying to project some Vector X onto uh the the e0 space right we can kind of see how the math works out here. familiar with perhaps in terms of how we can use those to reconstruct signals and ultimately how we could use them to generate audio right predict the best uh kind of next node um so looking at the next the next step here we want to use deep learning for reconstruction right where we are are reconstructing a low quality audio to high resolution audio right um and this is this is the kind of uh model framework um that we can used for this um you might notice it really closely resembles a unit which is something that we talked about during image segmentation. to uh kind of increase uh the uh the resolution of a certain low quality image form so as you can see if this is our initial wave our our final wave is is much more populated right we're able to gain information through this unit process. At each layer the number of filter Banks is doubled so that while the the mention along the waveform is is halved the filter Bank Dimension is increased by two. We're reducing the size of our uh of our image but we're increasing the dimension the dimensionality of it. we're developing a representation of these these audio signals so as we pass through the bottleneck layer which is constructed identically to a down sampling block right these connect eight up sampling blocks which have residual connections to the down sampling blocks. What this allows you to do is it allows you. to preserve features and share features that are learned from the low resolution representation of the image into our higher resolution output. The final convolutional layer um has a restacking operation and it does the reordering operation following our our sub pixel deconvolution. after this restacking step um and the the loss function used throughout this process specifically was was kind of a mean squared error loss function so by playing around with different kinds of loss functions you might be able to yield better performance. I thought this was incredibly cool um as it kind of it parallels a lot of Concepts that we talked about earlier on um units um the swin Transformer keeping residuals we talked very very uh very very in depth about how residuals are able to um kind of keep information across solves a bunch of problems vanish ingredients um as your network gets bigger. kinds of techniques are used in a variety of ways to reproduce music from bands from the 1900s. The reconstructed Spectrum after the passing through the unit um has a lot more depth. The SNR signals noise ratio goes down between the the down sample and the reconstructed Spectrum. The listening experience for the end user is uh is improving so yeah these are some of the results that were found from uh this uh this audio um where you have a true Spectrum um and uh you can see the waveform here. waveform and the reconstructed waveform. As you can see on the reconstructed Waveform matches um the kvps of the the true waveform, we're adding color to our our downsampled waveform to to ultimately get something that hopefully resembles our true waveforms better. We're going to kind of uh kind of transition now into Transformers for audio generation right how can we use Transformers to predict the next note for example for example. We want to take notes and for example produce the next notes in a Melody. music realm does provide a different issue than it did for our other two representations um of of of image and text we want to then build the model and train it to predict the next token right the first step takes the form of converting data which is music files into a token sequence which is individual notes. So eventually we wants to start with with this kind of notes format and we Want to tokenize it into something that our model can understand right um this presents a unique problem that we're going to be talking about extensively. The Transformer music model is trying to do sequence generation through next token prediction. We can tokenize this into a series of tokens that all correspond to our vocabulary which is very easy to map by a dictionary to certain Keys. So where our key is are our actual vocabulary and our our value is associated with that. We want to be able to predict that this this will happen um and we we want to get our our end of sentence token as well as well um so going into the actual tokenization right. intuitive what we can do for uh for music is We can approximate this which is a series of notes in a piano roll right or a graph that has our offsets and our pitches right so our pitches span you know a certain uh pitch set um and we want our our information to be captured in multiple Dimensions right we want the the pitch of the note as well as the length of thenote for example these two notes at the bottom right they correspond to to e and A2 we want these to be half notes we want them to last for half as long as our C4. piano roll right which is a plot of frequency across time um we know that um a single music note is a collection of values you can think of these as the different features that make up a music note right um the two naive ones that you can immediately think of are pitch and duration um but this can be expanded into multiple things right um as shown other attributes you can use as part of this are instrument type Dynamics Tempo can be used for more complex representation. We need to figure out how to tokenize this 2D data now this this representation into a single Dimension to be fed into our transform model. have a value and then you have a duration um and this this is kind of a a naive approach to to how we can represent a certain music node. So yeah they're they're kind of different approaches we can use. We can use notes which is one to many um where you have individual notes and you have many of them where you're trying to encode a single node into a sequence of tokens um and combine the values into a single token right whereyou have c chord node D chord node e half note right. and you have less control over predictions given that your vocabulary size will grow as you add nodes right um the other kind of thing is polyphony right taking many notes and mapping it for one specific kind of tokenized set where you play note sequentially if it's separated by a SCP a separator token if not we want to play all nodes together right so this keeps track then of where notes will start and end right we have a series of notes that we wants to play together and then aseries of notes right. n62 which is this actual note representation on piano. A single song can be transformed into 12 songs of different Keys which can help increase our sample of training data and generalize key scales and beats throughout a data set. Data augmentation provides an amazing data sub multiplier to to Simply get more data. The more data you have the more data that can be used to improve training data. We're able to capture information in a a very sequential and very structured way so putting this all together you can get your initial translation right where you have a um our our initial translation. the better your model will be and the more generalizability you have in your Transformer the better it'll perform right we talked about Occam's razor. A generalized Transformer a generalized solution can fit the Goldilocks of what we want in a model. It's easier for machines to predict keys without flats and Sharps um which has you know similar to what humans do it's easier to focus on the regular keys on piano um so this specific example was trained on on those um however the glass andSharps with specific augmentations and specific training processes can also be added to our vocabulary. example um this specifically I believe this uses like a music 21 framework but it's able to you know tokenize a certain item and then you're able to transpose this to a certain amount of notes or a certain key. Just by transposing you'reable to increase your your training sample size which is very cool and can improve model performance by a ton. The next thing to consider is positional beat encoding right we want to include some metadata to feed into our model to give it a better sense of musical timing because the position of the token in our our tokenized representation it doesn't correspond to its actual position in time. then it won't really learn a a very good representation of the data that you're providing right we want to be able to mask information that it previously had as well as mask information it'll have in the future. We want to apply an attention mask to keep the model from peaking and essentially leaking information at the next token it's supposed to predict we can do this by kind of observing this model right um so here we at each at each step the model can only see itself right at the first step where zero is a token you can see one is a tokens you can't see. 2 here at the first step you only see yourself right at the Second Step you don't even see what token you're on. You're essentially enforcing a window size of two where you're only updating the information you see every two time steps. Right at the very end you can't see the final two bits of information and that lack of information is very important to a Transformer as it allows you to predict several steps ahead and will ideally produce a more generalized model. This is a reverse teacher forcing where we're masking future tokens and potentially pass tokens depending on what window masks we're applying. hidden state memory Transformer memory specifically to this model enables very fast inference for music prediction right we've done a lot of things to optimize for for our prediction we're including a beat embedding so that's not something it has to learn. We're able to get a sense of of relative position with Transformer XL whereas vanilla Transformers will use Absolution absolute position only. It's important for music models to know the position of each token relative to one another because positionality matters right the order that you're playing the notes really is is what matters the most. our positional beat encoding which we're including for the the model to have um so I'd kind of like to end with a little demo generated by by somebody who who use this model to kind of predict the end of Canon in in D Major by by Pachelbel so here'sPachelbel's Canon I'm kind of in the spirit of Christmas coming up as well um yeah so as you can see there um this is the original pocket balls Canon and this is what's predicted. So yeah there's a a lot to do in this field um a lot of really cool things happening um. things a try yourself uh yeah thank you guys for tuning in have a good one. Things you might want to try yourself. Things that you may want to give a try. uh yeah. things you might be interested in trying yourself. things that you might like to give yourself. uhYeah. things a try yourselves. uh Yeah. Things a try themselves. Things your might like yourself.things you may like to do yourself. thanks you guys. for tuning into this week's episode of The Daily Discussion.