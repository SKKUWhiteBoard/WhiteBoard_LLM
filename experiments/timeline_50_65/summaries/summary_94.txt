Part 2 in our series on natural language inference. We're going to focus on the three data sets that we'll be concentrating on this unit. SNLI-- the Stanford Natural Language Inference Corpus-- MultiNLI, and Adversarial NLI. I think they're interestingly different, and they're different from each other, and that's what we're trying to get out of them. We'll be focusing on SNLI, MultiN LI, and the Adversary NLI data sets. all big benchmark tasks that can support the training of lots of diverse kinds of systems. So let's begin with SNLI, which is the first to appear of these three. The associated paper is Bowman, et al., 2015. Sam Bowman was a student in the NLP group, and I was a member of the team that developed SNLI. The project was led by Sam Bowman and his colleagues at the University of California, Los Angeles, and the California Institute of Technology, San Diego. his advisor, along with Chris Manning, and a bunch of us contributed to that paper. An important thing to know about SNLI is that the premises are all image captions from the image Flickr30K data set. So that's an important genre restriction that you should be aware of when you use SNLI. It's a great tool, but it's not for everyone, so be careful when you're using it, and don't use it if you don't know what you're doing. think about training systems on this data. All the hypotheses were written by crowdworkers. They were given this premise, which was an image caption, and then they wrote three different texts corresponding to the three NLI labels. But the idea is they were given a premise and then a text corresponding to it. And they were asked to write the text that corresponded to that premise. They wrote the texts that correspond to each of the threeNLI labels, and they were then given a prompt to write their text. Unfortunately, as is common with crowdsourced data sets, you should be aware that some of the sentences do reflect stereotypes. I think this traces to the fact that crowdworkers, trying to do a lot of work, are faced with a creative block. And the way they overcome that is by using stereotypes to help them with their work. It's a good way to start, but it's not the best way to finish a big project, so don't rely on it too much. falling back on easy tricks, and some of those involve stereotypes. Completely understandable, and this is something that the field is trying to come to grips with as we think about data set creation. It's a big data set. It has over 550,000 training examples. And it has dev and dev anddev and dev. And dev. and dev, it's a very, very busy place, and it has a lot of challenges. It is a very exciting time, and I'm looking forward to the future. test sets. Each have 10,000 examples balanced across the three classes. Here's a look at the mean token lengths. It's just sort of noteworthy that premises are a little bit longer than hypotheses. I guess that comes down to the fact that crowdworkers were writing these sentences. In terms of the total number of examples, there are more than 100,000 in each set. There are also more than 50,000 hypotheses in each class. There is also a large number of hypotheses that can't be proved. clause types, mostly, we talk about NLI as a sentence task. But in fact, only 74% of the examples are sentences that is S-rooted in their syntactic parses. It has a large vocabulary, but may be modest relative to the size of the data set, and that might come back in the future. It's a big task, but it's a good one, and it's fun to play with, says the author of the book, which is published by Oxford University Press. to the fact that the genre is kind of restricted. We had about 60,000 examples that were additionally validated by four other annotators. And I'll show you the response distributions, which suggests some sources of variation. They had high interannotator agreement. So given that validation, about 60% examples had a high level of agreement with each other. The results were published in the open-source journal, The Open Invention Project, on November 14. For confidential support, call the Samaritans on 08457 90 90 90, visit a local Samaritans branch or see www.samaritans.org for details. unanimous gold label. And we rate the overall human level of agreement at about 91.2% for the gold labels. And that's the measure of human performance that's commonly used for SNLI. And the overall Fleiss kappa measured interannotator agreement was 0.7, which is a high rate of agreement. And then we rate it at 0.8 for the interannotators, and 0.9 for the non-interannotators. That's a very, very high rate. For the leaderboard, you can check out this link here. Sam has been good about curating all the systems that enter, and you can get a sense for which approaches are best. It's clear at this point, for example, that ensembles of deep learning methods are the best for this. For the leaderboards, you could check out the link here, or you can click here to go to the page you came from. For more information, visit Sam's blog. problem. I think it's worth thinking about precisely what happened here. So here's the crowdsourcing interface. There's some instructions up here. Here's the caption-- that is, the premise sentence in our terms-- a little boy in an apron helps his mother. And then the caption is: "A little boy helps his mom" And then there's a picture of the boy helping his mother, and it's captioned: "He helps his mum." And then it says: "The boy in the apron is helping his mom." crowdworker had to come up with three sentences. One definitely correct -- that's an entailment case. One may be correct-- that is our gloss on neutral. And one definitely incorrect, which isOur gloss on contradiction. So you can see here that there's an attempt to use informal language connecting with informal language. It's a way to connect with people. It doesn't mean that everything is always the same, but it's a good way to try to connect the dots. informal reasoning, common sense reasoning in the prompt here. And then those get translated into our three labels for the task. And here are some examples from the validated set. And I think they're sort of interesting, because you get high rates of agreement, but you do find some examples of poor reasoning as well as good reasoning. And so that's kind of interesting to look at, because it's a little bit of a mixed bag, but it's interesting to see what people can do. that have a lot of uncertainty about them, like this last one here. And I think that might be a hallmark, actually, of NLI problems. Now, one really fundamental thing that I mentioned in the overview screencast as definitely worth being aware of relates specifically to the contradiction relation. And that is that there is a contradiction relation in the NLI system. And it's not always clear what that relation is, or how it relates to the rest of the system. That's a really fundamental part of the problem. There's discussion of this in the paper. It's a tricky point. What we say for SNLI, using these simple examples here, is that both of them are in the contradiction relation. The first one is "a boat sank in the Pacific Ocean." It has premise and hypothesis, "a Boat sank" The second one is, "A Boat Sinked in The Pacific Ocean," and it has premise, " a Boat Sank in The South Pacific." It's the same thing. in the Atlantic Ocean." You might ask, of course, those could be true together. They should be neutral, not contradiction. The reason we call them contradiction is because we make an assumption of event coreference, that we're talking about the same boat in the same event. And therefore, the locations can't be the same, even if they are the same vessel. The locations can be different, but they are not the same person, and that's what we're trying to get at. contradict each other in a common sense way. And the second example is an even more extreme case of this. Ruth Bader Ginsburg was appointed to the Supreme Court and I had a sandwich for lunch today. We say those are in the contradiction relation. Of course, they could be mistaken for each other, as they are not the same thing. But they are both in the common sense relation, and that is what matters. It's a good way to start a conversation. true together. But they couldn't, in our terms, be true of the same event. They're describing very different events. And for that reason, they get the contradiction label. If a premise and hypothesis probably describe a different photo, then the label is contradiction. That's kind of anchoring back into our understanding of the world. It's a kind of an anchoring point for us to think about the world in a way that's very different from what we think it is. underlying domain that you might have in mind. We can mark progress on SNLI, because Sam has been curating that leaderboard. As I mentioned before, we estimate human performance up here at almost 92. And along this x-axis here, I've got time. And you can see that very quickly, the game is going very, very well. It's going to get better and better, I'm sure, as we go on. I'm looking forward to it. community has hill-climbed toward systems that are superhuman, according to our estimate. But down here at 78 is the original paper. That was from an era when deep learning systems were really not clearly the winners in this kind of competition, but SNLI helped change that by introducing a lot of new features. The original paper was published in the Proceedings of the National Academy of Sciences in 1974. It was the first paper to show that deep learning can be used to predict human behavior. of new data. So a very rapid rise in system performance, and then basically monotonic increase until 2019, when we saw the first systems that were, in these restrictive terms, better than humans at the SNLI task. Let's move to MultiNLI, which was a kind of successor to SNLI. This is a very fast-growing area of the computer science world. We're seeing a lot of progress in the field of computer science. We've seen some very exciting developments. The train premises, in this case, are going to be much more diverse. They're drawn from five genres-- fiction; government reports, and letters and things; the Slate website; the Switchboard corpus; which is people interacting over the phone; and Berlitz.was collected by Idina Williams and colleagues, including Sam Bowman. The train premises were collected byIdina Williams, Sam Bowman and others, including IdinaWilliams and Sam Bowman, for CNN.com's iReport. travel guides. And then interestingly, they have additional genres just for dev and test. And this is what they call the mismatched condition. And those are the "9/11 Report," face-to-face conversations, fundraising letters, and nonfiction from Oxford University Press, as well as articles about linguistics. So this is noteworthy because it's not just travel guides, it's also nonfiction, too, and it's about language and culture and history and politics and politics. in the mismatched condition that MultiLNI sets up, you are forced to train on those training examples and then test on entirely new genres. And you can just see how different, for example, Berlitz travel guides might be from the "9/11 Report" I think this is an interesting early example of how MultiL NI can be used to test a new skill or skill set. It's a great way to test your knowledge of a new genre or skill or task. of being adversarial and enforcing our systems to grapple with new domains and new genres. And I think that's a really productive step in testing these systems for robustness. It's another large data set, slightly smaller than SNLI. But actually, the example lengths tend to be longer. They did the same thing with the U.S. version of the game. It was a lot of work, but it was worth it. It gave us a really good insight into how the game works. same kind of validation, and that gives us our estimates of human performance. And once again, I would say that we can have a lot of confidence. There was a high rate of agreement. 92.6% is the traditional measure of humanperformance here. For MultiNLI, the test set is available for download from the company's website. The test set for MultiN LI is available on the companyâ€™s website for downloading from the website. For more information on the MultiNNI test set, visit www.multiNLI.com. MultiNLI was distributed with annotations that could help someone kind of do out-of-the-box error analysis. What they did is have linguists go through and label specific examples for whether or not they were correct. The project was created as a Kaggle competition, and you can check out the project page here. It was created by a group of linguists at the University of California, Los Angeles, who are known for their work in the field of linguistics. They created MultiNLI as a competition to test their knowledge of the language. they manifested specific linguistic phenomena, like do the premise and hypothesis involve variation in active-passive morphology? Which might be a clue that the sentences are synonymous or in an entailment relation, but nonetheless hard for systems to predict because of the change in word order. We also have things like things like the fact that the word order of the sentences is different, which could be a sign that the sentence is not synonymous or is in an entailsment relation. But it's hard to predict. whether there are belief statements, conditionals, whether coreference is involved in a nontrivial way, modality, negation, quantifiers-- things that you might think would be good probes for the true systematicity of the model you've trained. And you can use these annotations to kind of benchmark yourself there. I think that's a good way to get a sense of how systematic your model is, and how well it's working in the real world, if you're training it. incredibly productive. How are we doing on MulitiNLI? So again, we're going to have our score over here and on the x-axis, time. We have that human estimate at 92.6%. And since it's on Kaggle, we can look at lots more systems. For SNLI, we just have the published papers. For Muliti NLI, you can go to the SNLI site and look at published papers for SNLI. You can also go to Muliti's site and search SNLI for papers. But on Kaggle, lots of people enter and they try lots of different things. As a result, you get much more variance across this. It's much less monotonic. But nonetheless, you can see that the community is rapidly hill climbing toward superhuman performance on this task, as well. And again, again, it's much, much more diverse than on the competition site, which is much more focused on a few tasks at a time. And it's a lot of fun to watch. I would just want to reiterate, recalling themes from our introductory lecture, this does not necessarily mean that we have systems that are superhuman at the task of common sense reasoning, which is a very human and complex thing. Instead, systems are just narrowly outperforming humans on this task. This does not mean that they are superhuman, but rather, they are narrowly outperform humans on it. This doesn't mean we have superhuman systems, it just means that we narrowly outperformed humans on the task. one particular very machine-like metric, which gives us our estimate of human performance here. Still, startling progress. And then finally, adversarial NLIs, kind of a response to that dynamic that looks like we're making lots of progress. But we might worry that our systems are benefiting from idiosyncrasies and artifacts in the data sets, and that they're not actually good at the kind of human reasoning that we're truly trying to capture. And that gave rise to the Adversarial NLI project. another large data set. A little bit smaller, but you'll see why it's special in some respects. The premises come from very diverse sources. We don't have the genre overfitting you might get from SNLI. And the hypotheses were again, written by crowdworkers. But here, crucially, they were written not by crowds, but by people in the real world. It's a very different kind of data set to the ones you get in SNLI or other data sets. in the abstract, but rather with the goal of fooling state-of-the-art models. That's the adversarial part of this project. And this is a direct response to this feeling that results in findings for SNLI and MultiNLI, while impressive, might be overstating the extent to which we've made progress on the subject. It's not just a matter of trying to fool SNLI, it's also about fooling MultiN LI. And that's what this project is all about. underlying task of common sense reasoning. So here's how the dataset collection worked in a little more detail. The annotator was presented with a premise sentence and one condition, which would just correspond to the label that they want to create. They write the label they want, and the label is created by the annotator. I think this is a fascinating dynamic, and it's a great example of how data can be used to help people understand the world. It's a very powerful tool, and I'm excited to see what's next. a hypothesis, and a state-of-the-art model makes a prediction about the premise-hypothesis pair, basically predicting one of these three condition labels. If the model's prediction matches the condition, the annotator returns to step 2 to try again with a new sentence. if the model was fooled, though, the premise hypothesis is the one that should have been chosen. The annotator then tries again with the new sentence to see if it matches the hypothesis label again. pair is independently validated. So in this way, we're kind of guaranteed to get a lot of examples that are very hard for whatever model we have in the loop in this process. Here are some more details. So it has three rounds, this data set, for its first release. It has three years of data set to work with. We're going to release it to the public in the next few months. It's going to be a very exciting time for us. We hope it will be a big success. Overall, that results in that large data set. And you can see that in subsequent rounds, the model is going to be expanded to include previous rounds of data, in addition, possibly, to other data resources. And so what we're hoping is that as we progress through these rounds, these data sets will get bigger and bigger and more and more accurate, and that we'll be able to use them in the future for new products and services. That's what we want to do. examples are going to get harder and harder in virtue of the fact that the model is trained on more data and is getting better as a result of seeing all these adversarial examples. In terms of the splits, the train set is a mix of cases where the model's training set is split into two sets. The model's split is based on the number of cases in each set where the training set splits into two groups. The train set splits are based on how much data the model has been trained on. predictions were correct and where it was incorrect. Sometimes in that loop, the annotator was unable to fool the model after some specified number of attempts. And we keep those examples, because they're nonetheless interesting training data. However, in the dev and test sets, we have only examples that were correct or incorrect in some way. We don't have examples of incorrect predictions or incorrect predictions in test sets. We just have examples that we think are interesting enough to show in training data for the model. fooled the models. So with respect to the best model for each round, the test set is as adversarial as it could possibly get. The model has gotten every single example wrong. And Adversarial NLI is exciting because it's given rise to a whole movement around creating adversarial datasets. And it's a great way to test your knowledge of the world around you, and to learn more about how to improve your own knowledge of what's going on in the world. that's represented by this open-source project, Dynabench. And we just recently published a paper that's on the Dynaben effort, reporting on a bunch of tasks that are going to use approximately adversarial NLI techniques to develop datasets. And you've actually seen one of them, which is on this site, here. It's called the "Dynabench" project, and it's based on the idea that you can use NLI to create adversarial datasets. These in the Dynasent dataset from our previous unit on sentiment analysis. And here's the Dynabench interface. And I guess I'm just exhorting you, if you would like to get involved in this effort, it's a community-wide thing to develop better benchmarks that are going to get us closer to the goal of having a better sentiment analysis system. These are in the dataset from the previous unit. And we're looking for people who want to join us in developing better benchmarks. assessing how much progress we're actually making. And then finally, there are a lot of other NLI data sets that I didn't mention. So let me just run through these. The GLUE benchmark has a lot. of NLI tasks in it, as does SuperGLUE, which is. its successor. I mentioned the NLI benchmark, but there are also a number of other data sets. that I haven't mentioned yet. The NLI benchmarks are based on a set of data sets called the GLUE benchmarks. These data sets are used to measure how well NLI is working. before, in the context of ANLI, this NLI-style FEVER dataset. FEVER is fact verification, and I've just translated the examples into NLI ones. Here's an NLI corpus for Chinese, and here's one for Turkish. The Chinese examples are all original, and the Turkish one is a translation with validation of the Chinese examples. The Turkish example is a translated version of the original Chinese example with validation for the Turkish examples. Click here for more information on ANLI. SNLI and MultiNLI into Turkish. XNLI is a bunch of assessment data sets that is dev-test splits for more than a dozen languages. Those are human-created translations that could be used to benchmark multilingual NLI systems. And then there are a few others down here down here. The full set of tools is available for download on the NLI website. For more information, go to NLI.org and XNNI.org, or visit the NNI website. kind of pointing out trying to get genre diversity, and then NLI for specialized domains. Here is medicine and science. And those could be interesting for seeing how well a model can grapple with variation that comes in very specific and maybe technical domains. So there's a wide world of possibilities for NLI, and it could be very interesting to see how well it can work in that world, too, as well as in other areas of science and technology, for example, in the future. tasks you can explore, and I think that makes NLI a really exciting space in which to develop original systems, and projects, and so forth. I think it's really exciting to be able to work on projects that are really unique to you, that you can't do anything else with. It's a really interesting space to be in, and it's great to be a part of, I think. I'm looking forward to seeing what the future holds for NLI in the future.