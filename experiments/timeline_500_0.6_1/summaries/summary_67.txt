So in the next portion of today's lecture we're going to talk about how we can modify the policy gradient calculation to reduce its variance. In this way we'll obtain a version of the policy gradients that can be used as a practical reinforcement learning algorithm. The first trick that we'll start with is going to exploit a property that is always true in our universe which is causality causality says that the policy at time t prime can't affect the reward at another time step t if t is less than t prime. variance we just uh sorry we often don't use the optimal baseline we typically just use the expected reward but if you want the optimal baseline this is how you would get it all right so to review what we've covered so far we talked about the high variance of policy gradients algorithms. We talked about how we can lower that variance by exploiting the fact that present actions don't affect past rewards and we talked about how we can use baselines which are also unbiased.