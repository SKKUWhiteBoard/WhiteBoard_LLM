In the second half of the course, we'll talk about multisequence alignment. We'll also talk about the ClustalWustal algorithm. And then when we get into the transcriptome part of the transcript, we will talk about a star alignment. The course is offered by MIT OpenCourseWare, which is licensed under a Creative Commons license. For more information, visit ocw.mit.edu/opencourses and follow us on Twitter @MITOpenCourses. Bayes' theorem: Probability of a model and the probability of the sequence are prior probabilities. These are probabilities which are not conditional. They do not depend on something else. Now let's see what all this Bayesian stuff is useful for. We're going to be doing-- of the various applications, we had recognition discrimination and database search. We'll have two models, a model that we actually have a hydrolase and themodel that we have randomness. So we want to report all the sequences where the probability that that sequence is better than that sequence given a null model or random amino acids is significant. Student: Do you know the basis for thinking that the context for a dinucleotide is either an ocean or an island, in other words, only two states? Why couldn't the context be five states? GEORGE CHURCH: OK. Question. Student: Why can't there be more than two states for the context of a dinoinucleotide? Student: I want to know why there can't be more states for dinosucleotides. George Church: I don't know, but I think there must be. will be a recursive algorithm where the score of a two-character string is defined in terms of the maximum of various shorter strings. The number of different cases we have here-- before, it was 3 for a global alignment, which was k, being the number of sequences, was 2. Now k is three for a three-way comparison. And all possible subsets is 2 to the k minus 1, in this case, so it's 7. So seven cases, best score for S1 with each of the others.