All right welcome to the third lecture on Foundation mulative AI. Today we're going to cover chat GPT. chat GP was the the tool or the the AI that really made people understand this is different now. Next time we'll talk about stable diffusion image generation and then we will talk about emerging Foundation models basically Foundation models generative AI in the commercial space. Then we'll end with the lecture on AI ethics and regulation as well as a panel okay so what have we talked about before? Foundation models geni apply this self-supervised learning where we learn without label data so we can we can get as much data as we want because there's no human being in the loop. What we get from this you know by learning from observation and learning from the data directly is a very contextual and relational understanding of meaning. The robot learns to rate responses that's supervised learning because yes it is supervised learning and then the actual model is generating the response that's a great Point. Force things to comply to Simple Rules right it kind of abandons our ability to understand and compress what we're seeing and deals with that chaos directly that's why AI is so powerful and so humanlike um so also like when I talk about this in CHP we try to make very high level um statement but of course the nuances matters and I think it's quite interesting. I took this quote from a general from the 18 and 1700s and he says this quote that P Theory which sets itself in opposition to the mind and what he meant was that he's a general so he fights in battles and War. so we have a sequence of words and and then we're just going to try to predict uh the next word based on previous words so let's say we have uh we start with i here as input. Then we want to someh predict the Target right so we know we know or the computer knows somehow by just downloading the text that what this whole sequence is. When it trains this AI model it hides part of it right so it just inputs I to the AI model and then it's supposed to do something with it. We're going to create scores or predictions for all words in the L like in the human you know vocabulary in the English vocabulary that sounds extremely expensive and it is quite expensive and so I have different tricks to make this work.then it kind of gets it right and then you give some positive feedback back and we'regoing to kind of do this um we'reGoing to maximize this. Then you give this feedback to to the model it's called back propagation so you given some feedback through model it should push the score or the probability distribution for the correct one to be bigger or larger and then reduce all other ones. like a a specific token that says we're happy until we complete a complete sentence for example um and this is kind of expensive uh to do because you have to generate one thing at a time but of course training is is much faster because then you can just you don't need to generate and run on your own input. Al: We have a great model that's been fine tune on dialogue and it's able to generate really good answers still uh to different prompts so uh we're going to run this model now on a collection of prompts. Transformer was trained at a scale with an amount of data and parameters that we never seen before. Just training the final model cost around $5 million just in in Compu electricity bills right that's how huge and much compute they spent on this. Transformer has much less structure and has to relearn a lot of this structure. Since now we have St super plus learning we can train by just downloading text from the internet and there's no human being to train it. It's a very very simple approach but it's a certain scale that's that's never been seen before and really that's what a big part of open eyes Transformer is. well we're just going to go all in and just make this bigger and bigger and big and and and then like in hindsight like maybe it makes sense but it could have been a case like it wouldn't work and then people like oh that's a stupid bet like why would you think such a simple idea and approach would lead to to such sophisticate intelligence but it did okay so we covered thetive pre-train part right so uh you know we've now said how we basically are going to uh train our model but how does this model look like likeHow does this kind of engine look like. is that for every step here that's label with the same uh digit you know they can all be done in parallel. step three can be run after step two is been run because step threes don't rely on each other and so on. This is key because in in deep learning we use this uh um computer is called gpus. If we can make multiple steps into single step in parallel this is a single cost we want to run things in parallel as much as possible. This works only during training now right okay and I'll come to that actually later. these are extremely extremely popular and a version of them called uh lstm long short-term memory networks um it performs really really well and some people say it performs you know almost better than Transformers a lot of times. But they just take them longer to train because we're going to realize why it one a point but they work really really good and also notice here somehow that uh this was very very intuitive for researchers to say like well text we read text from left to right we process words one at a time and therefore our models should to to to tble to learn effectively. things flow forward this in kind of this sequential way right so to get from uh you know for the information from I to go to the information prob being processed step number nine basically right when you want to predict the period has to travel eight or nine steps here to to to uh be used so let's think about this a little bit start we start off now in a Transformer which basically starts off the same way so we we let the first we know we just process the first word uh and we prict the next Target based on that. and it's going to be much faster to run so it's much less uh uh well that's a modification but uh it's a little bit less sensitive in a sense uh we care about both being fast uh and yeah I mean but somehow H this is going tobe much much faster than a recurr network so you're going to get much much better performance. During training we can do this because we not upend the words we just see them in the sequence we can doing this. just goes off a little bit like here for example when you say you know I went to the financial and then just you know some small error happens and it goes off the road to restaurant like somehow you know they started seeing that okay now it's basically go Haywire because it went off and it's in a different space than it's been trained on. So somehow we want to be able to say like well if you find yourself you know alittle bit off the the the path you should be able. to find your way back to be as as robust as possible. to any prompt that we have so let's say we and this is very cheap to do so we have a million prompts that we found online now we run our model four times on each prompt with different random seeds we we sample uh four different answers so now we have 1 million prompts with four uh candidate answers okay and then we're going to pay people to actual human beings to label these they'reGoing to rank this this uh prompt or the answers that these models produce to these prompts so we'regoing to pay pay actual human being to score them and say are they good or not. good or bad dialogue so we've solved that okay and the last two problems we are going to solve by using reinforcement learning so what is reinforcement learning well we talked about this a little bit before but uh something is very important and characteristics of reinforcement learning is this delayed feedback. In order to generate you know this exploration you you know you want to be a do a very very targeted exploration around language is still kind of make sense so the robot gives you good feedback and you actually can start you know making progress. gratification actually leads to very non- GRE and and independent robustness so these are the consequence of applying reinforcement learning where you only get feedback at the very end so there's less you know supervision right you're more on your own. H deal with an uncertainty of not having constant feedback you have to figure out things by yourself which leads to you being more robust and also again in reinforcement learn here the only thing we care about is the signal at the end so we don't care about making the best next step. We care about optimizing the whole output so we're now addressing these things. well because if you now have a better model you kind of want to you want to go to the human beings and get more feedback that's more relevant to this model because this model now is is doing better than the previous one. I think open a runs this two or three times um okay cool. We can just run this step uh and do it all again and you know you can done you can do this as much as you want of course uh maybe with some uh you know decreasing returns I don't know exactly. and self Suess learning care more about both aspects somehow somehow. Next time we will talk about uh we'll do a similar Deep dive into stable diffusion there will be self supervised learning and Foundation mod an AI but uh I think it's going to be slightly more conceptually interesting um so should be a lot of fun and yes please go to the website for more information Etc and if you have any questions feel free yeah can I something can I assume that probability on that after there changes based on the subject of the totally yes yes yes great question okay. word mask it and try to predict it based on the surrounding words um yeah the answer to that is we used to do that way and it works better uh but due to engineering you can Bas basically kind Transformer you can maybe this can be like a homework for you but if you look at Transformer Works uh if you mask a word uh then you will like then you'll only um okay I if you do this you know if only predict the last word based on previous words you can make this okay attention. oh actually works better given the same amount of compute yeah does that give you some sense of answer we can talk more about it offline as well. What are like the main challenges of these models nowadays and are there other like language models that are like better but they do like more expensive than or like this is like the best um yeah I think. I mean we want to able to rely on it as much as possible and if they if they don't behave like we want them to we don't want to make up things right. the World by looking at videos right you can even sort to understand how human beings work even better because you can see people being upset or sad or happy whatever right in in a video and start picking these cues up. You can connect the vision part to the text part and get a multimodality model that's able to do both in a really really sophisticated way uh also something that I think these these people are working on all right thank you thank you for your time and good luck with your book.