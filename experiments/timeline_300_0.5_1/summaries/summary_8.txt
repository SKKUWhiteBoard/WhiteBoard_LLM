The final contest, which is due tonight, is to design an agent that plays together with another agent to try to collect food pellets while not getting eaten by ghosts. Submissions for that, your last chance to submit are tonight at midnight. And on Thursday in lecture, we'll discuss the results. The idea behind these two lectures is to look at advanced applications. We will not quiz you on these application lectures, on the final exam, or anything like that. It's more meant to give you more perspective rather than extra study materials. Search is very prevalent in language processing, and we'll cover some of that on Thursday. Adversarial games. Learning to acquire behaviors. We'll look at some real robot behaviors in flight and in legged locomotion. And dealing with uncertainty. So AlphaGo. Today's state-of-the-art in Go is that there are computer players better than the best human players. But actually, if you went back to March 2016, that was not the case yet. And in what it did, we had to update this graph that you see at the very beginning of the course. CNN's John Defterios: How do you get a helicopter to do something like this? He says it's hard to stabilize a helicopter when you only know where you are up to a couple meters. Defterio: We set up a hidden Markov model, where we considered the state unknown. He says we can build a model for learning a bunch of parameters that predict the next state, and solve it at this point in the learning.to until this experiment was run. To learn more, visit CNN iReport. The helicopter uses roughly a fixed amount of fuel, anyway, per time. So it's more that it has less weight to carry as it has used more fuel. This helicopter had inverted slide, where it has more power, 3 Gs. It can generate three times the power of gravity. OK, let's take a short break here. And after the break,. let's do legged locomotion and manipulation. All right, let't restart. Any questions about the first half? Yes? is a separate linear feedback controller for each time slice. The way you learn that feedback controller is by doing a forward pass to see what your current sequence of controllers achieves. And then you can do a backward pass, which is, essentially, a value attrition pass over that same trajectory to find the optimal sequence of feedback controllers. If there is no wind, you can actually just run the linear feedback control. It will be fine. But if there's some wind gusts that could throw you off, you want to use the value functions and the two second look ahead against those value functions. The devil is really in the details, in the long tail of special events that can happen when you're driving. You can measure progress by just demo videos, which is one way, and it gives you some kind of feel for what's going on. Another way to measure progress is to see how are these cars doing relative to human drivers. It's 10 to the negative 5 events per 1,000 miles driven for humans. Red there is human fatalities. Yellow is human injuries. And then in green is the Google slash [? wave ?] mode disengagement. doing this. Early on, they were the only ones, as far as I know, but there's more companies now. And you can look at them on these plots and see which companies, how far along, in terms of how many disengagements they need per 1,000 miles driven. Still, so far, quite far removed from human accident rate levels. Another thing people have been pushing, as a consequence of all this, is lower power neural networks. So for example, Kurt Kuetzer at Berkeley has projects on this called SqueezeNet. If they're gigantic, use a lot of power. That's a problem. Let's see what we can do to build smaller networks to make decisions.