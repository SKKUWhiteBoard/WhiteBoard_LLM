homework two is out now. We're gonna be having some more background on deep learning this weekend. You're not expected to become, or, or to be a deep learning expert to be in this class, but we, you only need to have some basic skills in order to do homework two, um, be able to use function approximation with a deep neural network. Um, we're also gonna be reaching, uh, releasing by the end of tomorrow. Uh, but the sessions will be a good chance to catch up on that material. [NOISE] Um, just a quick humble, which of you have used TensorFlow or PyTorch before? The assignments, [inaudible] are they limited to TensorFlow? asked the question. I'm pretty sure that everything relies that, er, you're using Tensor Flow. Um, I'll believe you guys also should have access to the Azure credit. If you have any questions about getting setup without feel free to use the Piazza channel. We also released a tutorial for how to just sort of set up your machine last week. So, if you're having any questions with that, that's a great place to get started. Deep Q Learning is a form of reinforcement learning. It uses deep neural networks to learn to make complex decisions. We'll be covering the basics of Deep Q Learning in this tutorial. You can look at the tutorial, or you can reach out to us on Piazza with any other questions. We hope to see you in the next class on November 14th and 15th, at 10:30am and 11:00am. Back to the page you came from. Click here for more information about the class. when we thought about doing this, we're gonna focus on function approximations that are differentiable. Um, and the nice thing about differentiable rep-representations is that we can use our data, and we can estimate our parameters, and then we can take use gradient descent to try to fit our function. And that information now could be [NOISE] in the form of episodes or it could be individual tuples. When I say a tuple, I generally mean a state-action reward next state tuple. is the same as the full gradient update. Our objective function is again the mean squared error. And the key hard thing was that we don't know what this is. So, this is the true value of a policy. But the problem is we donâ€™t know what the trueValue of a Policy is, and so we can't get a value for a policy that we think is the real value of the policy. And so, we have to change the way we think about a policy to get a true value. otherwise we wouldn't have to be doing all of this learning. And so, the two ways we talked about last time was inspired by a work on Monte Carlo, or on TD learning is we could either plug-in the return from the full episode. Or we could put in a bootstrapped return. So, now we're doing bootstrapping. Where we look at the reward, the next state, and the value of our next state. And in this case we're using a linear value function approximators for everything. all for linear value function approximation, but there are some limitations to use the linear valuefunction approximation, even though this has been probably the most well-studied. So, if you have the right set of features, and historically there was a lot of work on figuring out what those rights set offeatures are. They often worked really well. And in fact when we get into, I think I mentioned briefly before. When we start to talk about deep neural networks you can think of, a deep neural network is just a really complicated way to get out features. and how easy is it for us to converge to that. So, one alternative that we didn't talk so much about last time is to use sort of a really, really rich function approximator class. Um, where we don't have to, have to have a direct representation of the features. Er, and some of those are Kernel based approaches. These actually have a lot stronger convergence results compared compared to other approaches. The problem is, um, that the number of data points you need tends to scale with the dimension. The intuition is that if you want to have sort of an accurate representation of your value function, um, and you're representing it by say, uh, local points around it. For example, with the k-nearest neighbor approach. then the number of points you need to have everything be close like in an epsilon ball scales with the- the dimensionality. So, basically you're just gridding the space. Um, and everyone just [inaudible] name first please to stop me. a square, you're gonna need four points so that everything can be somewhat close to one of the points. Generally, the number of points you need this going to scale exponentially with the dimension. A really cool thing about averagers is sort of by their name. They're guaranteed to be a non-expansion, which means that when you combine them with a bellman backup it's guaranteed it'd still be a contraction. So, that means these sort of approximators are guaranteed to converge compared to a lot of other ones. These for things like, um, health care applications and how do you sort of generalized from related patients. So, they can be useful but they generally don't scale so well. What we're gonna talk about today is thinking about deep neural networks which also have very flexible representations but we hope we'll be able to scale a lot better. Um, now, in general we're going to have almost no theoretical guarantees for the rest of the day, and- but in practice they often work really really well. In this example, we're going to use a loss function called j which is like our Q. Then we're gonna push that into another function, and throw in some more weights. I'm gonna do that a whole bunch of times, and then at the very end of that you can output some y. Then, we can output that to some loss function j. These are- happen a lot in unsupervised learning like predicting whether or not something is a cat or not or, you know, an image of a particular object. functions, um, you could represent really complicated functions by adding and subtracting and taking polynomials and all sorts of things you could do by just composing functions together. But the nice reason to write it down like this is that you can do the chain rule to try to do stochastic gradient descent. So, we really want, you know, dj with respect to all these different parameters. We can write down dj of hn and dhn of dwn and we can do- do this kind of everywhere. your weights. When I first learned about deep neural networks, you have to do this by hand. But I think one of the major major innovations that's happened over there, you know, roughly what? Like last 5 to 8 years is that there's auto differentiation. So, that now, um, you don't have to derive all of these, uh, gradients by hand instead, you can just write down your network parameters and then you have software like TensorFlow to do all of the differentiation for you. and as usual we need a loss function at the end. Typically, we use mean squared error. You could also use log likelihood but we need something that- that we can differentiate how close we are achieving that target in order to update our weights. [NOISE] Yeah? Name first. So, this ReLU function is not differentiable, right? It is differentiable. It's ended up being a lot more popular than sigmoid recently, though I feel like it [OVERLAPPING]. Yes. But I don't see how gradient [inaudible] is gonna work on the part where it's flat. Convolutional neural networks are used extensively in computer vision. They can represent any function with the deep neural network. If you have at least one hidden layer, um, if you have a sufficient number of nodes. You can learn the parameters using stochastic gradient descent. All right. So, that's pretty much that, you know, deep neural networks in like five seconds. Now, we're now gonna talk a little bit about convolutional Neural Networks. We'll talk- we'll talk some about that in sessions. Convolutional neural networks try to have a particular form of deep neural network that tries to think about the properties of images. So, in particular, images often have structure, um, in the way that our- our brain promises images also has structure and this sort of distinctive features in space and frequency. Having so operators again here are like our functions, h1 and hn, which I said before could either be linear or nonlinear and then convolutional Neural Network learn a particular structures for those. Deep learning is a new way of looking at images. It uses layers of data that are applied to different parts of the image. The layers are called a filter and a receptive field. They are then used to extract different features from the image, such as whether or not there's an edge in a particular patch. The results can then be used to create different types of depth maps, which can be used in other ways, like facial recognition or other types of image analysis. Back to the page you came from. then you could apply these different filters on top of it, which you can think of as trying to detect different features. Um, the other really important thing in CNNs, is what are known as pooling layers. They are often used as a way to sort of down-sample the image. So you can do things like max pooling to detect whether or not a particular feature is present, um, or take averages or other ways to kind of just down, ah, and compress the, the information that you got it in. for really high dimensional input and kind of average and slow down until we can get to, um, a low dimensional output. So, the final layer is typically fully connected. We're kind of computing this new feature representation of the image, and at the very end, we can take some fully connected layer, where it's like doing linear regression, and use that to output predictions or scalars. So these type of representations, both Deep Neural Networks and Convolutional Neural Networks, are both used extensively in deep reinforcement learning. How we could use these type of approximations for Atari. So why was the surprising? I just sort of wandering back. In around 1994, personally in 1994, we had TD backgammon which used Deep Neural Networks. Well, they used neural networks. I think there was someone that deep, and I think out like a world-class backgammond player out of that. So, that was pretty early on. And then we had the results that were kind of happening around like 1995 to maybe like 1998, which said that, "Function approximation plus offline off policy control, plus bootstrapping can be bad, can fail to converge" success and then there were these results in sort of the middle of the nineties that indicated that things could be very bad, and the risk was some of the- In addition to the theoretical results, there were sort of these simple test cases, that, you know, these simple cases that went wrong. So, it wasn't just sort of in principle this could happen, uh, but there were cases which failed. And so I think for a long time after that, the, the community was sort of backed away from Deep Neural Networks for a while. DeepMind, DeepMind combined them and had some really amazing successes with Atari. And so I think it sort of really changed the story of how people are perceiving using, um, this sort of complicated function approximation, meters, and RL, and that yes, it can fail to converge. But it is also possible of them that despite that- you know, the fact that we don't always fully understand why they always work, that often in practice, we can still get pretty good policies out. the mid '90s? Or, is it just that kind of through increases in computational power and the ability to gather a lot of data, that when it failed, it kinda doesn't matter, and we can try some different, like we- you know, try it again and kinda put it together and just keep trying until it works? I guess my question is, did we actually overcome any of the problems that arose in the late ' 90s, or is itjust that we're just kinda powered through? The question is how we sort of fundamentally resolve some of the issues of the late my '90's, or, um, we kind of brute forcing it. I think there's also a couple algorithmic ideas that we're gonna see later in this lecture, that help the performance kind of avoid some of those convergence problems. So with the Atari case specifically, did you- did you avoid that problem? Well, sort of, that if you tried it by having on policy control? I just don't know. Um, and it's a great question for me. We'll see how it works here. Anyone else? Okay, cool. So, we'll- we'll see an example for breakout shortly, um, of what they did. are the important things that they, um, did in their paper, this is a nature paper from 2015, is they use the same architecture and hyperparameters across all games. Now just to be clear, they're gonna then learn different Q functions and different policies for each game. But their point was that they didn't have to use totally different architectures, do totally different hyperparameter tuning for every single game separately. It really was the sort of general architecture and setup was sufficient for them to be able to learn to make good decisions for all of the games. approximators act. And the nice thing is that, I think this is actually required by nature. They, they released the source code as well. So you can play around with this. So how did they do it? Well, they're gonna do value function approximators. They're going to minimize the mean squared lost by stochastic gradient descent. Uh, but we know that this can diverge with value function approximation. And what are the two of the problems for this? Well one is that there is this or the correlation between samples which means that if you have s, a, r, s prime, aprime, a prime, r prime, double prime. lot of correlations. Um, and also this issue with non-stationary targets. What does that mean? It means that when you're trying to do your supervised learning and train your value function predictor, um, it's not like you always have the same v pi oracle that's telling you what the true value is. That's changing over time because you are doing Q-learning to try to estimate what that is and your policies changing. So you don't have a stationary target when you are even just trying to fit your function. data instead of just using each data point once, you can reuse it and that can be helpful. So, even though we're treating the target as a scalar, the weights will get updated the next round which means our target value changes. This is like saying that periodically I- like let's say I went s1 a1 r1 s2 and then I keep going on and now I'm at like s3 a3 r3 s4. That's really where the data is going. I am in the world, I'm now in state four. It's like I suddenly pretend that I'm back in s1, took a1, got r1, and went to s2 and I'm gonna update my weights again. The reason that that update will be different than before is because I've now updated using my second update and my third update. So, it'll cause a different weight update. In general, one thing we talked about a long time ago is that if you, um, uh, do TD learning to converge it, which means that you go over your data mu- like, like, an infinite amount of time. learn to model, a dynamics model, and then the planning for that which is pretty cool. But we don't wanna do that all the time because there's a computation trade-off and particularly here because we're in games. There's a directtrade-off between computation and getting more experience. Can we use something similar to like exploitation versus exploration. Um, essentially like with random probability just decide to re-flag [inaudible]. The question is about how would we choose between getting new data and how much to replay et cetera. based on the experience replay versus getting, um, putting new samples into there. So, generally right now is really heuristic trade-off. Could certainly imagine trying to optimally figure this out but that also requires computation. This gets us into the really interesting question of metacomputation and metacognition. Um, but if, you know, your agent thinking about how to prioritize its own computation which is a super cool problem. Which is what we solve all the time. use in that value of S prime for several rounds. So, instead of always update- taking whatever the most recent one is, we're just gonna fix it for awhile and that's basically like making this more stable. Because this, in general, is an approximation of the oracle of V star. What this is saying is, don't do that, keep the weights fixed that used to compute VS prime for a little while. Um, one is gonna be this weight and the other is this weight. minus. I'll call it minus because, um, well there might be other conventions but in particular it's the older set of weights, the ones we're not updating right now. Those are the ones that we're using them as target calculation. So, when we compute our target value we, again, can sample and experience tuple from the dataset from our experience replay buffer, compute the target value using our w minus, and then we use stochastic gradient descent to update the network weights. help, um, in terms of stability? In terms of Stability, it helps because you're basically reducing the noise in your target. If you kept your target fixed forever, you would learn the weights that- that minimize the error to a constant function. That would then be stable because you always have the same target value that you're always trying to predict. And eventually you'd learn that, and that would eventually be stable. And that's what we're trying to do with GT. This is just reducing the noise and the target that we're trying to sort of, um, if you think of this as a supervised learning problem, we have an input x and output y. The challenge in RL is that our y is changing. If you make it that you're- so your y is not changing, it's much easier to fit. Uh, assuming we want to do [inaudible] approximator. Is there something that's specific to the deep neural networks? This is really just about stability and that's- that's true for the experience replay too. Experience replay is just kinda propagate information more- more effectively and, um, this is just gonna make it more stable. Do you every update the- Minus at all, or is that [inaudible]. Great question. Dian. Dian's question is whether or not we ever update w minus, yes we do. We pu- can periodically update wminus as well. In a fixed schedule, say every 50 or, you know, every n episodes or every n steps, you would set w minus dw. information fester, um, and possibly being less stable. If n is infinity, that means you've never updated it. There's a- there's a smooth continuum there. We notice, like, for w, there are better initializations than just like zero, uh, something, if you take into account, I guess like the mean and variance. Uh, would you initialize w minus just two w or is there like an even better initialization for w minus? Yeah, his questions is about, you know, the- the impact of how we, um,. uh, initialize w ca- can matter. It stores the transition in this sort of replay buffer, a replay memory, um, use sample random mini-batches from D. So, normally sample in mini-batch instead of a single one. You do your gradient descent given those. Um, you compute Q learning using these old targets and you optimize the mean squared error between the Q network and Q learning targets, use stochastic gradient descent, and something I did not mention on here is that we're typically doing E-greedy exploration. of what the agent is doing. So, remember the agent's just learning from pixels here how to do this. Um, and one of the interesting things about it is that as you'd hope, as it gets more and more data, it learns to make better decisions. So this is really cool that sort of it could discover things that maybe are strategies that people take a little while to learn when they're first learning the game as well. Yeah, so, um, you might see, uh, I think she is talking- she is referring to the fact that the paddle was moving a lot. clearly sort of an inexperienced player to do that. That would be a strange thing but from the agent's perspective, that's completely reasonable. Um, and it does not give him positive or negative reward from that. So, it can't distinguish between, you know, stay in stationary versus going left or right. If you put it in a cost for movement that could help. This might become a little bit of [inaudible] but is there a reason to introduce a pulling layer? Puling layer? There might be one in there. uh, they're not talking about how long it took them or their agent to learn and as you guys will find out for homework two, it can be a lot of experience. So, they did very well on some domains. Some domains, they doing very poorly. Um, I think that it's clear that the really important feature is replay. We'll probably talk- uh, we'll talk a lot more about exploration later on in the course. so, what was critical? So, I- I like the, uh, there's a lot  lovely things about this paper and one of the really nice things is that they did a nice ablation study.  replay is hugely important and it just gives us a much better way to use the data. Using that fixed Q here means you seem like a fixed target. You do replay and suddenly you're at 241. Okay, so throwing away each data point what- after you use it once is not a very good thing to do. Um, and then if you combine replay and fixed Q you do get an improvement over that but, uh, it's really that you get this huge increase, um, at least in break out in some of the other games. So, the question is like, "Well, maybe we could use, like, linear-" also I guess I should be clear. So, we've done some work, um, using a Bayesian last layer, using like Bayesian linear regression which is useful for uncertainty. But you could certainly imagine trying linear plus replay and it seems like you might do very well here, it might depend on which features you're using. There's some cool work over the last few years looking also at, uh, whether you can combine these two. a talk about reinforcement learning, and like 40 people would show up, but most of them you knew, and, um, and then, uh, then it started really changing. I think I was maybe in 2016, when, er, ICML, I was in New York and like suddenly there were 400 people in the room for reinforcement learning talks. And then, this year at NLP's which is one of the major machine-learning conferences, it's sold out in like eight minutes. So, there's been a huge amount of excitement based on this work. double DQN is kind of like double Q learning, which we covered very briefly at the end of a couple of classes ago. The thing that we discussed there was this sort of maximization bias, is that, um, the max of estimated state action values can be a biased estimator of the true max. This is to try to separate how we pick our action versus our estimate of the value of that action. It turns out that it gives you a huge benefit in many, many cases for the Atari games. back to the Mars Rover example. Um, er, so, in Mars Rover we had this really small domain, we are talking about tabular setting through just seven states, um, and we're talking about a policy that just always took action a1 which turned out to mostly go left. So, it was this. And the first visit Monte Carlo estimate of v for every state was 1110000, and the TD estimate with alpha equal one is this. That was when we talked about the fact that TD only uses each data point once and it didn't propagate the information back. Vote if you think it matters which ones you pick, in terms of the value function you get out. Back-propagate from the information you're already [NOISE] have on step one to step two. So, if you pick backup three, so what's backup three? It is, S2, A1, 0, S1. So if you do the backup, that's, zero, plus gamma V of S prime. And this is one. So that means now you're gonna get to backup and so now your V of. S2 is gonna be equal to one. you get to backup that information. So, um, if you wanted to get all the way to the Monte Carlo estimate. What you would wanna do here, is you'd wanna do S3, a1, 0, S2 which would allow your V of S3 to be updated to one. So it definitely matters. It matters the order in which you did, do it. And that's the same as the last time I [inaudible] That's right. Yes. The number, of, um, updates you need to do until your value function converges to the right thing can be exponentially smaller. If you update carefully and you, you could have an oracle tells you exactly what to do. But you can't do that. You're not gonna spend all this. It- it's very computationally, expensive or impossible in some cases to figure out exactly what that. oracle ordering should be. But it does illustrate that we might wanna be careful about the order that we do it. In the old ways and uh, example we're just going through, after you like propagated the one back once, you wouldn't be able to do anymore because your value's totally zero. So there's gonna be this tension between, when you fix, um, uh, your w minus, then, if you were looking at our case that we had before, then you would be able. to continue propagating that back, because you wouldnâ€™t update yet, yet, that's exactly right. things versus her propagating information back. So why does it, what does ordering matter, that if you're fixing, and so you are not changing, uh, like, then it wouldn't matter what order we sampled those previous ones, right? Uh, okay. So basically, ordering matter at all, in that case. It still matters because we're still gonna be doing replay, o- over. So that buffer could be like, million and you might re-update your weights like every 50 steps or something like that. among the existing tuples? So out- so Pi is our, uh, sort of basically our DQN error. If we set Alpha equal to zero, you know, it's right. Yeah. So, so this sort of trades off between uniform, no prioritization to completely picking the one that, um, like if alpha's infinity then that's gonna be picked the one with the highest DQn error. So it's a trade-off. Most, the time prioritize replay is better and there's some hyper parameters here to play with. Duoing.do is an architectural choice and learning a recombine these for the Q. The idea is that, if you want to, make decisions in the world, they're working some states that are better or worse, um, and they're just gonna have higher value or lower value. What you really wanna be able to do is, figure out what the right action is to do, in a particular state. So, what they do to do this is that in contrast to DQN, where you output all of the Q's. It could be super tempting to try to start, by like implementing Q learning directly on the ATARI. Highly encourage you to first go through, sort of the order of the assignment and like, do the linear case, make sure your Q learning is totally working. Even with the smaller games, like Pong which we're working on, um, it is enormously time consuming. It's way better to make sure that you know your Q Learning method is working, before you wait, 12 hours to see. whether or not, oh it didn't learn anything on Pogge. So, that, that- there's a reason for why we, sort of build up, the way we do in the assignment. Um, another practical, to a few other practical tips, feel free to, to look at those, um, and then we were on Thursday. Thanks. Back to Mail Online home.back to the page you came from. Back from the page where you come from. back to MailOnline home.