SNLI is the Stanford Natural Language Inference Corpus-- MultiNLI, and Adversarial NLI. The premises are all image captions from the image Flickr30K data set. SNLI is an important genre restriction that you should be aware of when you use the data set, says Christopher Potts. The associated paper is Bowman, et al., 2015. It was written by Sam Bowman, who was my advisor in the NLP group at the University of California, San Francisco. think about training systems on this data. All the hypotheses were written by crowdworkers. Unfortunately, as is common with crowdsourced data sets, you should be aware that some of the sentences do reflect stereotypes. I think this traces to the fact that crowdworkers, trying to do a lot of work, are faced with a creative block. And the way they overcome that is by falling back on easy tricks, and some of those involve stereotypes. It's a big data set. It has over 550,000 training examples. And it has dev and test sets. unanimous gold label. And we rate the overall human level of agreement at about 91.2% for the gold labels. The overall Fleiss kappa measured interannotator agreement was 0.7, which is a high rate of agreement. And then for the leaderboard, you can check out this link here. Sam has been good about curating all the systems that enter, and you can get a sense for which approaches are best. It's clear at this point, for example, that ensembles of deep learning methods are the best for this problem. crowdworker had to come up with three sentences. One definitely correct -- that's an entailment case. One may be correct-- that is our gloss on neutral. And one definitely incorrect, which isOur gloss on contradiction. So you can see here that there's an attempt to use informal language connecting with informal reasoning, common sense reasoning in the prompt here. And then those get translated into our three labels for the task. And here are some examples from the validated set. And I think they're sort of interesting, because you get high rates of agreement, but you do find some examples that have a lot of uncertainty about them. There's discussion of this in the paper. It's a tricky point. What we say for SNLI, using these simple examples here, is that both of them are in the contradiction relation. The reason we call them contradiction is because we make an assumption of event coreference, that we're talking about the same boat in the same event. And therefore, the locations contradict each other in a common sense way. And the second example is an even more extreme case of this. Ruth Bader Ginsburg was appointed to the Supreme Court and I had a sandwich for lunch today. Of course, they could be true together. true together. But they couldn't, in our terms, be true of the same event. And for that reason, they get the contradiction label. If a premise and hypothesis probably describe a different photo, then the label is contradiction. That's kind of anchoring back into our underlying domain that you might have in mind. We can mark progress on SNLI, because Sam has been curating that leaderboard. And you can see that very quickly, the community has hill-climbed toward systems that are superhuman, according to our estimate. of new data. So a very rapid rise in system performance, and then basically monotonic increase until 2019, when we saw the first systems that were, in these restrictive terms, better than humans at the SNLI task. Let's move to MultiNLI, which was a kind of successor to SNLI. This was collected by Idina Williams and colleagues, including Sam Bowman. The train premises, in this case, are going to be much more diverse. They're drawn from five genres-- fiction; government reports, and letters and things; the Slate website. MultiLNI is an interesting early example of being adversarial and enforcing our systems to grapple with new domains and new genres. It's another large data set, slightly smaller than SNLI. But actually, the example lengths tend to be longer. And once again, I would say that we can have a lot of confidence. There was a high rate of agreement. 92.6% is the traditional measure of human performance here. For MultiNLI, the test set is available for download. MultiNLI was distributed with annotations that could help someone kind of do out-of-the-box error analysis. What they did is have linguists go through and label specific examples for whether or not they manifested specific linguistic phenomena. We also have things like whether there are belief statements, conditionals, whether coreference is involved in a nontrivial way, modality, negation, quantifiers-- things that you might think would be good probes for the true systematicity of the model you've trained. incredibly productive. How are we doing on MulitiNLI? So again, we're going to have our score over here and on the x-axis, time. We have that human estimate at 92.6%. And since it's on Kaggle, we can look at lots more systems. But nonetheless, you can see that the community is rapidly hill climbing toward superhuman performance on this task, as well. This does not necessarily mean that we have systems that are superhuman at the task of common sense reasoning, which is a very human and complex thing. One particular very machine-like metric, which gives us our estimate of human performance here. Still, startling progress. And then finally, adversarial NLIs, kind of a response to that dynamic that looks like we're making lots of progress. But we might worry that our systems are benefiting from idiosyncrasies and artifacts in the data sets, and that they're not actually good at the kind of human reasoning that we're truly trying to capture. And that gave rise to the Adversarial NLI project. in the abstract, but rather with the goal of fooling state-of-the-art models. That's the adversarial part of this project. And this is a direct response to this feeling that results in findings for SNLI and MultiNLI, while impressive, might be overstating the extent to which we've made progress on the underlying task of common sense reasoning. So here's how the dataset collection worked in a little more detail. The annotator was presented with a premise sentence and one condition, which would just correspond to the label that they want to create. The train set is a mix of cases where the model's.pair is independently validated. So in this way, we're kind of guaranteed to get a lot of examples that are very hard for whatever model we have in the loop in this process. And so what we're hoping is that as we progress through these rounds, these examples are going to get harder and harder in virtue of the fact that the model is trained on more data and is getting better as a result of seeing all these adversarial examples. predictions were correct and where it was incorrect, because sometimes in that loop, the annotator was unable to fool the model after some specified number of attempts. Adversarial NLI is exciting because it's given rise to a whole movement around creating adversarial datasets. And we just recently published a paper that's on the Dynabench effort, reporting on a bunch of tasks that are going to use approximately adversarialNLI techniques to develop datasets that are adversarial in lots of domains. These in the Dynasent dataset from our previous unit on sentiment analysis. And here's the Dynabench interface. And I guess I'm just exhorting you, if you would like to get involved in this effort, it's a community-wide thing to develop better benchmarks that are going to get us closer to assessing how much progress we're actually making. And then finally, there are a lot of other NLI data sets that I didn't mention. So let me just run through these. SNLI and MultiNLI into Turkish. XNLI is a bunch of assessment data sets that is dev-test splits for more than a dozen languages. Those are human-created translations that could be used to benchmark multilingual NLI systems. So there's a wide world of tasks you can explore, and I think that makes NLI a really exciting space in which to develop original systems, and projects, and so forth. And those could be interesting for seeing how well a model can grapple with variation that comes in very specific and technical domains.