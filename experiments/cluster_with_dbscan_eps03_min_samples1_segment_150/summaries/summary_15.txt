The dagger algorithm aims to provide a more principled solution to the imitational and distributional Shi problem so uh as a reminder the problem with distributional shift intuitively is that your policy makes at least small mistakes even close to the training data. When it makes small mistakes it finds itself in states that are more unfamiliar and there it makes bigger mistakes and the mistakes compound. So far a lot of what we've seen so far is that the dagger algorithm is actually something that you're going to be implementing in your homework. "Dagger" is a book about how the U.S. Department of Health and Human Services trains its policy in the real world. The goal is to make the policy more effective in the states it visits. The book is the result of a series of experiments to see how the policy works in real-world states. It is published by Simon & Schuster, a division of Penguin Random House, which also publishes the book "Let's Be Clever" " dagger" is out now and is available on iTunes. see which states it visits and ask humans to label those States so the goal is to collect data in such a way that P Pi Theta uh that the train data comes from PiPiTheta instead of P data and we're going to do that by actually running our policy so here's the algorithm now we'regoing to need labels for all those States we'reGoing to train our policy first on our training dat just on our demonstrations to get it started and then we'll run our policy and we'll record the observations. the original data set and this additional label data set that we just got and then go back to Step One retrain the policy and repeat so every time through the Sloop we run our policy we collect observations we ask humans to label them with the correct actions for those observations and then we Aggregate. It can actually be shown that eventually this algorithm will converge such that eventually the distribution of observations in this data set will approach the distribution that the policy actually sees when it runs. As long as you get closer each each time each time the algorithm runs the distribution is closer than the initial one. step eventually you'll get to a distribution where that the policy can actually learn and then you'll stay there forever so then as you collect from it more and more eventually your data set becomes dominated by samples from the correct P Pi Theta distribution so that's the algorithm. It's a very simple algorithm to implement if you can get those labels here's a a video of this algorithm in action this is in the original diag paper this was a about 12 years ago where they actually used it to fly a drone through a forest. actually get up to fly pretty reliably through a forest dodging trees now there is of course a problem with this method and that has to do with step three uh it's sometimes not very natural to ask a human to examine images after the fact and output the correct action. When you're driving a car you're not just instantaneously making a decision every time step about which action to choose you are situated on temporal process you have reaction times all that stuff so sometimes the human labels that you can get offline in this sort of a counterfactual way can be not as natural as what a human might do when they were actually operating the system. Dagger alleviates the distributional shift problem it actually provably address it so you can derive a Bound for dagger and that bound is linear in t rather than quadratic but of course that comes at the cost of introducing this much stronger assumption that you can collect the additional data okay so that's basically the list of methods I wanted to cover for how to address the challenges of behavior cloning. We can be smart about how we collect an augment our data we can use powerful models that make very few mistakes or we can change the data collection procedure and use dagram.  humans need to provide data for imitation learning which is sometimes fine but deep learning works best when the data is very plentiful so asking humans to provide huge amounts of data can be huge limitation. If the if the algorithm can collect data autonomously then we can be in that regime where deep Nets really Thrive without exorbitant amounts of human effort. humans are not good at providing some kinds of actions so humans might be pretty good at specifying whether you should go left or right on a hiking trail uh or controlling a quadcopter. copter rotors to make it do some really complex aerobatic trick if you want humans to control all the joints in a complex humanoid robot that might be even harder maybe you need to rig up some really complicated harness for them to wear. If you want to control a giant robotic spider well good luck finding a human who can operate that um and humans can learn things autonomously and just intellectually it seems very appealing uh to try to develop methods that can allow our machines to do the same. experience and they can continuously self-improve and get better and better in principle exceeding the performance of humans now in order to start thinking about that we have to introduce some terminology and notation. We have to actually Define what it is that we want if our goal is no longer just to imitate but we want to do something else. Maybe instead of matching the actions in the expert data set we want want to bring about some desired outcome. We want to minimize the probability that we will land in a state S Prime which is an eaten by Tiger State. The cost function and the reward function are really the same thing they're just negatives of one another and the reason that we see both sometimes is the same kind of a cultural distinction that I alluded to before remember I mentioned that we have S a which comes from the study of dynamic programming that's where the reward comes from in optimal control. In optimal control it's it's a bit more common to deal with costs I don't know if there's a cultural commentary here well you know optimal control originated in Russia maybe it's more common in America. thing you actually want like reaching your destination or avoiding a car accident and then use those with more the more powerful reinforcement learning algorithms. In future weeks, we'll cover more of the powerful algorithms that we'll be covering in the next few weeks. We'll also cover how to use these algorithms to help you get what you want in the real world. Back to the page you came from. Follow us on Twitter @CNNOpinion and @cnnOpinION. We'd like to hear from you.