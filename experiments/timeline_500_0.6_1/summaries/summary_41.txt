homework two is out now. What the default projects will be, uh, for this class. Um, and you guys will get to pick whether or not you wanna do your own construction project or the default project. And those proposals will be due, um, very soon, er, in a little over a week. Are there any other questions that people have right now? Yeah. The assignments, [inaudible] are they limited to TensorFlow? asked the question if, if the assignment is limited toTensorFlow. I'm pretty sure that everything relies that you're using Tensor Flow. Last week we discussed value function approximation, particularly linear value function approximations. Today we're gonna start to talk about other forms of valuefunction approximation in particular, um, uh, using deep neural networks. We're mostly not gonna talk so much about enormous action spaces, but we are gonna think a lot about really large state spaces. And so, instead of having a table to represent our value functions, we were gonna use this generic function approximation where we have a W now, which are some parameters. Deep neural networks represent the Q function. linear value function is often really works very well if you're the right set of features. But there are all sorts of implications about whether or not we're even gonna be able to write down the true p- um, value function. So, one alternative that we didn't talk so much about last time is to use sort of a really, really rich function approximator class. The problem is, um, that the number of data points you need tends to scale with the dimension. Instead of having our full x input, we're just gonna take in- we're gonna direct different parts of the x input to different neurons which you can think of just different functions. We think that often, the brain is doing this. It's trying to pick up different sort of features. So, we want to sort of extract features that are relevant for deciding whether or not a face, for example, is a face or not. This means also that rather than computing this sort of variance in translation, you can do this all the way across the image. DQN, deep Q-learning addresses these is by experienced replay and fixed Q-targets. Experienced replay, prime number if you guys have heard about this, if you learned about DQN before is we're just gonna stroll data. So, even though we're treating the target as a scalar, the weights will get updated the next round which means our target value changes. So this is nice because basically it means that you reuse your data instead of just using each data point once, you can reuse it and that can be helpful. that it does is it has fixed Q targets. Um, so to improve stability, and what we mean by stability here is that we don't want our weights to explode and go to infinity which we saw could happen in linear value function. We're gonna fix the target weights that are used in the target calculation for multiple updates. So, instead of always update- taking whatever the most recent one is, we're just gonna fix it for awhile and that's basically like making this more stable. Replay is hugely important and it just gives us a much better way to use the data. Double DQN is kind of like double Q learning, which we covered very briefly at the end of a couple of classes ago. Greedy Policy is where we average networks, average reward networks, and then use one of the Qs as the target for the other network. Then we update Q2 with 50 percent probability, we pick the next action from the next network, this is a pretty small change. back to the Mars Rover example. So, let's say you get to choose two replay backups to do. Vote if you think it matters which ones you pick, in terms of the value function you get out. If you pick backup three, so what's backup three? It is, S2, A1, 0, S1. So that means now you're gonna get to backup and so now your V of S2 is gonna be equal to one. So you've got to back-propagate from the information you're already [NOISE] have on step one to step two.