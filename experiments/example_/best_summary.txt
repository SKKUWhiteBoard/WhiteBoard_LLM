Best index: 0

Problem set 1 is due on the 17th. That's two weeks from today. You have exactly two weeks to work on it. You can take up to, um, two or three late days. There is a good amount of programming and math you, uh, you need to do. The solutions need to be uploaded to Gradescope. One submission will be a PDF file, which you can either use a LaTeX template that we provide or you can handwrite it as well. You're gonna see two assignments in Gradescope. One is for the written part, the other is for programming. And there is a separate coding assignment, uh, for which you'll have to submit code as a separate GradesCope assignment. Today, we're gonna cover the perceptron algorithm. And then, you know, a good chunk of today is gonna be exponential family and, um, generalized linear models. And we'll- we'll end it with softmax regression for multi-class classification. Logistic regression uses the sigmoid function, which essentially squeezes the entire real line from minus infinity to infinity between 0 and 1. The perceptron algorithm uses a somewhat similar but, uh, different function. Both of them have a common update rule, which, you know, on the surface looks similar. We also saw that it was similar for linear regression as well. And we're gonna see why this- this is, um, that this is actually a- a more common- common theme. The algorithm is learning one example at a time, and a new example comes in. The new example happens to be a square, uh, or a box. And the algorithm has mis- misclassified it, right? So- so it updated the, um, decision boundary such that x is now included in the positive class. The- the idea here- here is that, Um, we want Theta to be similar to x in general, where such- where y is 1. And we want it to be not similar to X when y equals 0. used in lots of algorithms where if you add a vector to another vector, you make the second one kind of closer to the first one, essentially. So this is- this is, uh, the perceptron algorithm. You go example by example in an online manner, and if the al- if the, um, example is already classified, you do nothing. You get a 0 over here. If it is misclassified, you either add the- add a small component of the example itself to your Theta or you subtract it, depending on the class of the vector. like a software version of, uh, the perceptron itself in a way. You can- you're not guaranteed that you'll- you'll be able to get every example right. For example here, no matter how long you learn you're never gonna find a learning boundary. So it's- it's up to you when you wanna stop training. Uh, a common thing is to just decrease the learning rate with every time step until you stop making changes. All right. Let's move on to exponential.  exponential families is a class of probability distributions, which are somewhat nice mathematically, right? They're also very closely related to GLMs, which we will be going over next. An exponential family is one, um, whose PDF can be written in the form of the probability mass function, or the log-partition function. For all the distributions that we're gonna be seeing today, uh, or in this class, t of y will be equal to just y. So y is the data. Eta is the parameter of the distribution. T of y is called a sufficient statistic. As long as the expression integrates to 1, you have a family in the exponential family, right? Uh, what does that mean? For a specific choice of, say, for, for some choice of a, b, and t. This can actually- this will be equal to say the, uh, PDF of the Gaussian, in which case you, you got for that choice of t, a, and, and b,you got the Gaussian distribution. All right. algebraic massaging to bring it into this form, right? And then you do a pattern match to, to and, and, you know, conclude that it's a member of the exponential family. So, uh, we have [NOISE]. So, a Bernoulli distribution is one you use to model binary data. And our goal is to take this form and massage it into that form, and see what, what the individual t, b, and a turn out to be. plus Eta to the minus Eta. So this, this kind of, uh, verifies that the Bernoulli distribution is a member of the exponential family. So note that this may look familiar. It looks like the sigmoid function. We'll see how it kind of relates to logistic regression in a minute. So another example, um [NOISE]. So, uh,. a Gaussian with fixed variance. Right, so, um, aGaussian distribution, um,. has two parameters. the mean and the variance, uh, for our purposes we're gonna assume a constant variance. So, this gives the PDF of a Gaussian to look like this, p of y parameterized as mu. And then we set up a link between the canonical parameters and the natural parameters, that's part of the massaging exercise that we do. And we are only interested in, um, Gaussians with fixed variance and we are going to assume, assume that variance is equal to 1. equal to 1 over root 2 pi, minus over 2. E to the minus y squared over 2, times EX. If the variance is unknown you can write it as an exponential family in which case eta will now be a vector, it won't be a function of eta. Any questions on this? Yeah. So this is the Gaussian PDF with, um, with- with a variance equal to 1, right, and this can be rewritten as- again, I'm skipping a few algebra steps, you know, straightforward no tricks there. The NLL is therefore convex, okay. What does this mean? Um, each of the distribution, uh, we start with, uh,. a of eta, differentiate this with respect to eta. And that function will- is, is the mean of the. distribution as parameterized by eta,. and similarly the variance of y parameterized. by Eta, is just the second derivative, this was the first. derivative, and this is the second. derivative. So, um, the reason why this is nice is because in general for probability for probability, in general, for probability we want to minimize the negative log likelihood. distributions to calculate the mean and the variance, you generally need to integrate something, but over here you just need to differentiate, which is a lot easier operation, all right? And, um, and you will be proving these properties in your first homework. You're provided hints so it should be [LAUGHTER]. All right, now we're going to move on to, uh, generalized linear models. This- this is all we wanna talk about exponential families, any questions? Yep. are only dealing with the y, which in, in our case, it'll kind of map to the outputs. But, um, we can actually build a lot of many powerful models by, by choosing, uh, an appropriate family in the exponential family and kind of plugging it onto a, a linear model. So, so the assumptions we are going to make for GLM is that one is that y given x parameterized by Theta is a member of an exponential family. By exponential family of Theta, I mean that form. uh, uh, scenario that you have, it could take on any one of these, um,. uh, distributions. So, um, so there is the exponential family, and there is also a distribution called the exponential distribution, which are, you know, two distinct things. So depending on the kind of data that you. have, if your y-variable is, if you're trying to do a regression, then your y. is going to be say, say a Gaussian. If you have counts, you can use a Poisson, and if you have binary, a Bernoulli. And the mean of that distribution will be the prediction that we make for a given, for agiven x. So Eta is what we call the natural parameter. Mu is like the mean of the distribution. Phi is a Phi for Bernoulli, Mu and Sigma square for Gaussian, Lambda for Poisson. G is also the derivative of the log partition function with respect to Eta. And, um, between these two, you have g to go this way and g inverse to come back this way. Theta transpose x will give you a natural parameters. We choose to reparameterize Eta by a linear model, a linear of- linear in your data. The choice of what distribution you are going to choose is really dependent on the task that you have. If your task is classification, where your output is binary 0, or 1, you choose a distribution that models binary data. If you want to model the number of visitors to a website which is like a count, you know, you want a distribution like a Gaussian. So the task in a way influences you to pick the distribution. And most of the times that choice is pretty obvious. GLMs are just a general way to model data, and that data could be, you know, um, binary, it could be real value. As long as you have a distribution that can model, ah, that kind of data, it can be just plugged into a GLM and everything just works out nicely. So, uh, [NOISE] so the assumptions that we made. Let's start with regression, right? So for regression, we assume there is some X. And with this as the mean, let's- let's think of this as Y. does it mean? It means, um, you're given X, and let's- let's say this is Y. So you would have examples in your training set that- that may look like this, right? The assumption here is that, for every X there is a Gaussian distribution that started from the mean over here. And from thisGaussian distribution this value was sampled. And we are trying to find a line, [NOISE] ah, which is- which will be like your theta transpose X from which these Y's are most likely to have sampled. Every x we have a different, uh, Bernoulli distribution. And from this, you have a data generating distribution that would look like this. And so given- given this data, we wanna work backwards to find out what Theta was. What's the Theta that would have resulted in a sigmoid like curve from which these- these y's were most likely to have been sampled? That's- and- and and this is 0.5, right? And now, um, again our goal is to stop.  softmax regression is, um, so in the lecture notes, softmax regressions is explained as, uh, as yet another member of the GLM family. In- in- in today's lecture we'll be taking a non-GLM approach and kind of seeing- and- and see how softmax is- is essentially doing what's also called as cross entropy minimization. We'll end up with the same- same formulas and equations. You can- you can go through theGLM interpretation in the notes. In multicl- multiclass classification, our goal is to learn a model that can, given a new data point, you know, make a prediction of whether this point is a circle, square or a triangle, right? So, you're just looking at three because it's easy to visualize but this can work over thousands of classes. And, um, so what we have is so you have x_is in R_n. So the label y is, uh, is 0, 1_k. The goal is to get, uh, a probability distribution over the classes. In order to do that, we perform a few steps. So we exponentiate the logics which would give us- so now it is x above Theta class transpose x and this will make everything positive so it should be a small one. And next, we normalize this. By normalize, I mean, um, divide everything by the sum of all of them. So n- once we do this operation, we now get a probability. distribution where the. sum of the heights will add up to 1, right? softmax? Okay. So we'll, we'll break for today in that case. Thanks. Softmax? We'll, I'll be happy to talk to you again later today. Thanks, Softmax.softmax: We'll see you later today, SoftMax. softmax.com. Soft max: We will be back later this afternoon. SoftMax: We are back. Soft Max: We're back. soft max.com: Back to the page you came from. SoftMAX: We've been talking to you a lot.