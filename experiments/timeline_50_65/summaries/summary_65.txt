NASEM is the National Academy of Science, Engineering, and Medicine. NASEM is an august body of old people with lots of gray hair who have done something important enough to get elected. The NASEM committee is made up of people who have served in the military, politics, science, engineering and medicine. The committee was formed a little over a year ago by President Barack Obama and his wife, Michelle, who are both former members of the NASEM. For more, visit CNN.com/soulmatestories. to these academies. And their research arm is called the National Research Council and has a bunch of different committees. One of them is this Committee on Science, Technology, and the Law. It's chaired by David Baltimore, who used to be an MIT professor until he was fired last year. And it's a very interesting committee, and I'm looking forward to working with it. I think it will have a very positive impact on the future of science and technology in the U.S. Judge David Tatel is a member of the US Court of Appeals for the District of Columbia circuit. He also happens to have a Nobel Prize in his pocket and he's a pretty famous guy. He went on to become president of Caltech and is now a judge at the US Supreme Court. He is also a former president of the University of California, Los Angeles. He was awarded the Nobel Prize for Literature for his work on "The Godfather of Soul" in 1973. He's also a judge on the U.S. Supreme Court for the D.C. circuit. important circuit court. He happens to sit in the seat that Ruth Bader Ginsburg occupied before she was elevated to the Supreme Court. So these are heavy hitters. And they're heavy hitters, too. They're from the Court of Appeals, which is one level below theSupreme Court. It's a pretty big deal, so this is aPretty big deal. And it's a very, very important circuit court, too, so these areheavy hitters, and they're very, Very Heavy hitters. convened a meeting to talk about the set of topics that I've listed here. So blockchain and distributed trust, artificial intelligence and decision making, privacy and informed consent in an era of big data, science curricula for law. So that's the part that I got invited to talk to. And then there's the other part of it, which is obviously the part I'm talking about. And that's artificial intelligence. And so that's what I'm going to be talking about today. Schools, emerging issues, and science, technology, and law. The issue of using litigation to target scientists who have opinions that you don't like. And the more general issue of how do you communicate advances in life sciences to a skeptical public. So this is dealing with the sort of anti-science anti-scientist rhetoric that we see in the U.S. Today's Daily Mail is on the front page, with stories from across the world. For more, visit www.dailymail.com. tenor of the times. Cherise Burdee: I was a little bit surprised by the focus because Hank really is a law school professor at Stanford who's done a lot of work on fairness and prejudice in health care. She says she was also surprised that the group of us that talked about AI and decision making, was all women. Burdie: I'm a woman, and I'm proud to be one of the women in this room. I'm so happy to be a part of this. is at something called the Pretrial Justice Institute, and her issue is a legal one which is that there are now a lot of companies that have software that predict, if you get bail while you're awaiting trial, are you likely to skip bail or not? And so this is an important issue for the future of bail in the U.S. and around the world, she says. "It's a matter of justice," she says, "and it's not just a legal issue. It's also a moral issue." Matt Lundgren is a radiology professor at Stanford and has done some of the really cool work on bail. Lundgren: Judges make the decision about how much bail to impose and whether to let you out on bail at all or to keep you in prison, awaiting your trial. He says bail is an important part of a person's life and should be considered when deciding whether to release you on bail or not. He also says bail should be a part of the decision that judges make about whether or not to charge you with a crime. building convolutional neural network models to detect pulmonary emboli and various other things in imaging data. Suresh Venkatasubramanian is a professor. He was originally a theorist at the University of Utah but has also gotten into thinking a lot about privacy and fairness. And he's also a writer, and has a blog, and a book coming out next year. He's also on Twitter at @sureshvenkatasUBramanian and Facebook at www.facebook.com/SureshVenkatesubramania. so that that was our panel, and we each gave a brief talk and then had a very interesting discussion. One of the things that I was very surprised by is somebody raised the question of shouldn't Tatel as a judge on the Circuit Court of Appeals hire people like Mr. Tatel. That was very interesting to me. I was surprised by that, and I thought that was a very good question. It was very good to have a discussion about the role of judges on the appeals court. you guys to be clerks in his court? So people like you guys who also happen to go to law school, of which there are a number now of people who are trained in computational methods and machine learning but also have the legal background. And he said something very very interesting. He said, 'I want people who have a legal background to be in my court' And he was talking about people who also have a computer science background, he said. And I was like, 'Oh my God, that's a very good idea' interesting to me. He said, no, he wouldn't want people like that, which kind of shocked me. And so we quizzed him a little bit on why, and he said, well, because he views the role of the judge not to be an expert but to be a judge. To me, that was very interesting to me, because I don't think that's what a judge is supposed to be. I think a judge should be able to make a decision without being an expert." be a balancer of arguments on both sides of an issue. And he was afraid that if he had a clerk who had a strong technical background, that person would have strong technical opinions which would bias his decision one way or another. So this reminded me-- my wife was--my wife was aBalancer of Arguments on both Side of an Issue. And she was a balancing force. And so was I, and I'm sure I'll be, too, for the rest of my life. a lawyer, and I remember, when she was in law school, she would tell me about the classes that she was taking. And it became obvious that studying law was learning how to win, not learning to find the truth. And there's this philosophical notion in the law that law is about winning, not finding the truth, he says. He says that's not true in the real world, and that's why he's trying to change the law to make it more fair. Lawyer: "Your duty as a lawyer is to argue as hard as you can for your side of the argument" "In law school, they teach them, like in debate, that you you have to argue hard," he says. "The truth will come out from spirited argument on two sides of a question," he adds. "That's what lawyers do in law school," he writes. "And that's what you should do in life, too. That's why you're a lawyer." should be able to take either side of any case and make a cogent argument for it. And so Tatel sort of reinforced that notion in what he said, which I thought was interesting. Well, just to talk a little bit about the justice area because this is a very, very important area for the U.S. justice system. It's a very important place to be, and I think Tatel reinforced that in his remarks. I thought that was interesting, as well as the way he said it. is the one that has gotten the most public attention. Governments use decision automation for determining eligibility for various kinds of services, evaluating where to deploy health inspectors and law enforcement personnel, defining boundaries along voting districts. So all of the gerrymandering discussion that you hear about is all about the use of decision automation to draw voting districts, he says. He says it's a good way to make sure that people get the most out of their votes and that they don't get the worst of the political process. using computers and actually machine learning techniques to try to figure out how to-- your objective function is to get Republicans or Democrats elected, depending on who's in charge of the redistricting. And then you tailor these gerrymandered districts in order to maximize the probability that you're going to have a Republican or a Democrat in the House or Senate. And that's what they're trying to do in the U.S. House of Representatives and the Senate, where they're doing it now. the majority in whatever congressional races or state legislative races. So in the law, people are in favor of these ideas to the extent that they inject clarity and precision into bail, parole, and sentencing decisions. Algorithmic technologies may minimize harms that are the products of human judgment. So we need to make sure that we're using them to the best of our ability, and that they're being used in a fair way, and not just for the sake of being able to do it. know that people are in fact prejudiced, and so there are prejudices by judges and by juries that play into the decisions made in the legal system. So by formalizing it, you might win. However, conversely, the use of technology to determine whose liberty is deprived and on what terms is not a good use of our technology. We should not be using technology to decide who is deprived of liberty, but to determine who is entitled to it and when it should be used. raises significant concerns about transparency and interpretability. So next week, we're going to talk some about transparency. But today's is really about fairness. So here is an article from October of last year-- no, September of lastyear, saying that as of October of this year, if you if you are a member of the military, you are not required to reveal your military status. And if you're not a military member, you're allowed to say anything you want about your military service. And you can say anything. "The decision of whether you get bail or not is going to be made by a computer algorithm, not by a human being, OK? So it's not 100%. There is some discretion on the part of this county official who will make a recommendation, and the county official will make the recommendation," he says. "The decision ... will be made in a very computer-based way," he adds. "It will be based on a number of different factors, and it will be decided by an algorithm" judge ultimately decides, but I suspect that until there are some egregious outcomes from doing this, it will probably be quite commonly used. Now, the critique of these bail algorithms is based on a number of different factors. One is that the algorithms reflect a severe racial bias. So for example, the algorithms for bail in New York City are based on the fact that black people are more likely to be released on bail than white people. So the algorithms are very likely to have a racial bias in them. example, if you are two identical people but one of you happens to be white and one of them is black, the chances of you getting bail are much lower if you're black. Now, you say, well, how could that be given that we're both black, and we're white? Well, the answer is, it's because we're different. We're not the same race. And that's why we have different bail conditions. And we're also different. learning this algorithmically? Well, it's a complicated feedback loop because the algorithm is learning from historical data, and if historically, judges have been less likely to grant bail to an African-American than to a Caucasian-American, then the algorithm will learn that that's the right thing to do and will do it. The algorithm will then do its best to make sure that the bail is given to the right person, and it will do so in a way that makes sense to the judge. incorporate exactly that prejudice. And then the second problem, which I consider to be really horrendous, is that in this particular field, the algorithms are developed privately by private companies which will not tell you what their algorithm is. You can just pay them and they will tell you, and that is a really horrendous thing to do. I think that's a really, really bad idea. I don't think it's a good idea to pay people to develop algorithms for you. It's a very, very bad idea to do that. answer, but they won't tell you how they compute it. And so it's really a black box. You have no idea what's going on in that box other than by looking at its decisions. and so you have no know what is going on there. They won't even tell you what data they used to train the algorithm. AndSo you have to look at the decisions to get an idea of what's happening in the box. And then you can try to figure out how to solve the problem. the data collection system is flawed in the same way as the judicial system itself. So not only are there algorithms that decide whether you get bail or not, which is after all a relatively temporary question until your trial comes up, although that may be a long time, but that may also be the case with the bail system as well. The bail system in the U.S. is very different from that in other countries. It's based on a much more complex set of algorithms that are designed to make sure that people are held until their trial. there are also algorithms that advise on things like sentencing. So they say, how likely is this patient to be a recidivist? Somebody who, when they get out of jail, they're going to offend again. And therefore, they deserve a longer jail sentence because you want to keep them off. There's also an algorithm that says, 'How likely is it that this person is going to commit another crime after they're released from jail?' and so on. And so on and so forth. the streets. Wisconsin's Supreme Court ruled against a man who wanted to use an algorithm. The court said his knowledge of the algorithm's output was a sufficient level of transparency in order to not violate his rights, which I think many people would agree with, says CNN's John Sutter. Sutter: This is a story about a particular person in Wisconsin, and shockingly, the state Supreme Court rules against this guy, saying that knowledge is not enough to violate a person's rights. He says, "This is a case that many people in the U.S. would disagree with" consider to be kind of an outrageous decision. I'm sure it'll be appealed and maybe overturned. Conversely-- I keep doing on the one hand and on the other-- algorithms could help keep people out of jail. So there's a Wired article not long ago that says we can use algorithms to keep people from going to jail. That's what I'm trying to do, I think, on both sides of the issue. I think that's what we need to do. I don't think it's a bad idea at all. to analyze people's cases and say, oh, this person looks like they're really in need of psychiatric help rather than jail time. "So perhaps we can divert him from the penal system into psychiatric care and keep him out of prison and get him help," he says. "We can get people out of jail and get them help and not send them to prison" "We need to get people off the streets and out of the jails," he adds. "That's what we're trying to do here." There is also a long discussion-- you can find this all over the web-- of, for example, can an algorithm hire better than a human being. So that's the positive side of being able to use these kinds of algorithms. Now, it's not only in criminality. So if if an algorithm hires better, that's a good thing, too. So on and on, and so on. It's not just in criminality, but it's also in other areas, as well. you're a big company and you have a lot of people that you're trying to hire for various jobs, it's very tempting to say, hey, I've made lots and lots of hiring decisions and we have some outcome data. I know which people have turned out to be good employees. That's a very tempting thing to say. But it's also a very difficult thing to do. It's very hard to get people to want to work for you if they don't know what you're doing. and which people have turned out to be bad employees. We can base a first-cut screening method on learning such an algorithm and using it on people who apply for jobs and say, OK, these are the ones that we're going to interview and maybe hire because they're good at what they do. That's what we're trying to do here. We're looking for people who are good at their jobs, and we're looking to hire them because of that. We want to make sure we're getting the best people for the job. look like they're a better bet. When I was an undergraduate at Caltech, the Caltech faculty decided that they wanted to include student members of all the faculty committees. And so I was lucky enough that I served for three years. Now, I have to tell you a personal story. I was a student member of the faculty committee at Cal Tech. And I was on the committee for the first three years of my college career. And it was a great experience. I'm very proud of what I did. as a member of the Undergraduate Admissions Committee at Caltech. And in those days, Caltech only took about 220, 230 students a year. It's a very small school. And we would actually fly around the country and interview about the top half of all the applicants in the applicant pool. It was a very exciting time for all of us, and we're all still very proud of what Caltech has done for our country and for the world," he said. "It's been an incredible journey for us and we'll never forget it" So we would talk not only to the students but also to their teachers and their counselors and see what the environment was like. And I think we got a very good sense of how good a student was likely to be based on that. So one day, after the day, we would go back and do the same thing. We would see how the students were doing. And we would see if they were improving or not. And that's how we decided to start the program in the first place. admissions decisions have been made, one of the professors said. "Here's what we ought to do. We ought to take the 230 people that we've just offered admission to and we should reject them all and take the next 230 people, and then see what happens," he said. 'We should reject all of them and then take the people we want to take,' he added. 'That would be a good way to start the admissions process. It would be kind of a thought experiment' whether the faculty notices. Because it seemed like a fairly flat distribution. Now, of course, I and others argued that this would be unfair and unethical and would be a waste of all the time that we had put into selecting these people, so we didn't do that. But then, then, we decided to do it anyway, and it was a good move. It was a very good move, and I'm glad we did it. I'm very happy to have done it. this guy went out and he looked at the data we had on people's ranking class, SAT scores, grade point average, the checkmarks on their recommendation letters about whether they were truly exceptional or merely outstanding. He built a linear regression model that predicted the person's sophomore level grade. And he built a Linear Regression Model to predict a person's freshman grade. It predicts a student's sophomore grade at the high school level. And it predicts the student's junior grade at that grade level at the university. point average, which seemed like a reasonable thing to try to predict. And he got a reasonably good fit, but what was disturbing about it is that in the Caltech population of students, it turned out that the beta for your SAT English performance was negative. So if you did particularly well in English on the SAT, you were likely to do worse as a sophomore at Caltech. And so we thought about that a lot, and of course, we decided that that would be really unfair to penalize somebody for being good. at something, especially when the school had this philosophical orientation that said we ought to look for people with broad educations. So that's just an example. And more, Science Friday had a nice show that you can listen to about this issue. So let me ask you, what do you think? What do you want to do about it? Tell us on Twitter @CNNOpinion or @cnnopin. Back to the page you came from. Back from the pageYou came from! mean by fairness? If we're going to define the concept, what is fair? What characteristics would you like to have an algorithm have that judges you for some particular purpose? Yeah? AUDIENCE: It's impossible to pin down sort of, at least might in my opinion, one specific definition, but for fairness, what do you think? What do you want an algorithm to do for you? What are the characteristics that would make it fair for you to be judged for that purpose? the pre-trial success rate for example, I think having the error rates be similar across populations, across the covariants you might care about, is a good start. PETER SZOLOVITS: OK, so similar error rates is definitely one of the criteria that people use in talking about the benefits of a new drug. I think it's a good starting point for us to talk about the potential benefits of the new drug, but it's not the end of the world, it's just the beginning. about fairness. And you'll see later Irene-- where's Irene? Right there. Irene is a master of that notion of fairness. Yeah? Audience: When the model says some sort of observation that causally shouldn't be true, and what I want society to look like. PETER SZOLOVITS: So I'm not sure how I'm going to get Irene to do that. I mean, she's a great modeler, but I don't know how she'd do it. to capture that in a short phrase. Societal goals. But that's tricky, right? I mean, suppose that I would like it to be the case that the fraction of people of different ethnicity who are criminals should be the same. That seems like a good goal for fairness. How do you do that? I don't know. I'm trying to think of a way to do it. I just don't think it's that easy, though. I think there's a way. I achieve that? I mean, I could pretend that it's the same, but it isn't the same today objectively, and the data wouldn't support that. So that's an issue. Yeah? AUDIENCE: People who are similar should be treated similarly, so engaged sort of independent of the [INAUDIBLE] attributes or independent. OK. I'm going to go to the bathroom. I'll be back in a minute. I've got to go. I love you all. of your covariate. PETER SZOLOVITS: Similar people should lead to similar treatment. Yeah, I like that. AUDIENCE: I didn't make it up. It's another of the classic sort of notions of fairness. That puts a lot of weight on the distance function, right? In what way do you think that would work for you? I don't know, but I think it would be a good idea. I'm not sure how it would work. are to people similar? And what characteristics-- you obviously don't want to use the sensitive characteristics, the forbidden characteristics in order to decide similarity, because then people will be dissimilar in ways that you don't wants. All right, well, let me show you. Allright, well. Let's see you. Let me showYou. AllRight, well,. let's show you how to do it. How do you do it? You can do it by looking at a picture of a person. a more technical approach to thinking about this. So we all know about biases like selection bias, sampling bias, reporting bias, et cetera. These are in the conventional sense of the term bias. But I'll show you an example that I got involved in. Raj Manrai was a MIT Harvard professor. He was a co-author of the book, "The Bias in Your Brain" and co-authored the book "Bias in the Brain" The book was published by Oxford University Press. HST student, and he started looking at the question of the genetics that was used in order to determine whether somebody is at risk for hypertrophic cardiomyopathy. That's a big word. It means that your heart gets too big and it becomes sort of flabby and it stops pumping. It's a very serious condition, and it's very rare, but it can lead to heart failure. It can also lead to stroke, heart failure, and even death. It is a very, very rare condition. well, and eventually, you die of this disease at a relatively young age, if, in fact, you have it. So what happened is that there was a study that was done mostly with European populations where they discovered that a lot of people who had this disease had a certain genetic variant. And so it became accepted wisdom that if you had that genetic variant, people would counsel you to not plan on living a long life. And this has all kinds of consequences. Imagine if you're thinking. about having a kid when you're in your early 40s, and your life expectancy is 55. Would you want to die when you have a teenager that you leave to your spouse? So this was a consequential set of decisions that people have to make. Now, what happened is that people are starting to realize that having a child can be a good thing, and that it can help you live a longer, healthier, more fulfilling life," he said. "It's a good time to have a child, and it's a great time to start a family," he added. in the US, there were tests of this sort done, but the problem was that a lot of African and African-American populations turned out to have this genetic variant frequently without developing this terrible disease, but they were all told that they were going to die, basically. And it was a big problem. It was a very big problem, and it's still a problem today. It's a very, very serious problem. And we're still trying to figure out what to do about it. only after years when people noticed that these people who were supposed to die genetically weren't dying that they said, maybe we misunderstood something. The population that was used to develop the model was a European ancestry population and not an African ancestry population. And what they misunderstood was that the population was of European ancestry and not of African ancestry and so they didn't have the same genetic make-up as they thought they did. It was a mistake to think that the African population was the same as the European population. So you go, well, we must have learned that lesson. So this paper was published in 2016, and this was one of the first in this area. Here's a paper that was published three weeks ago in Nature Scientific Reports that says, genetic risk factors identified in populations of European. So you go,. well, well-we must havelearned that lesson, right? Well, no, we haven't, but we've learned a lot, and we're going to continue to learn. descent do not improve the prediction of osteoporotic fracture and bone mineral density in Chinese populations. So it's the same story. Different disease, the consequence is probably less dire because being told that you're going to break your bones when you're old is not as bad as being told you'll break them in your old age. But it's exactly the same. It's a very, very serious disease. And it's not going to get any better with age, it's going to stay the same way. bad as being told that your heart's going to stop working when you're in your 50s, but there we have it. OK, so technically, where does bias come from? Well, I mentioned the standard sources, but here is an interesting analysis. This comes from Constantine Aliferis from a number of different sources, including CNN.com and The New York Times. It is based on interviews with people who have been in the business for more than 20 years. It also comes from a study of more than 1,000 people who were in the industry for at least 10 years. years ago, 2006, and he says, well, look, in a perfect world, if I give you a data set, there's an uncountably infinite number of models that might possibly explain the relationships in that data. I cannot enumerate an unc Countable number of Models, and so what I'm going to do is what I've been doing all my life: I'm looking at the data and trying to figure out what's going on in it. And I think I've got a pretty good idea of what's happening. is choose some family of models to try to fit, and then I'm going to use some fitting technique, like stochastic gradient descent or something, that will find maybe a global optimum, but maybe not. Maybe it'll find the local optimum. And then there is noise. And so his observation is that there is no such thing as a perfect fit. He says the best way to find out is to try and find out what the noise is, and try to find a solution. is that if you count O as the optimal possible model over all possible model families, then the bias is essentially O minus L. If you count L as the best model that's learnable by a particular learning mechanism, and you call A the actual model that has been learned, then O plus L is the bias. So the bias of a learning mechanism is essentially the difference between O and L. The bias of an optimal model family is O + L, or O minus O, so the bias in this case is O - L. its limitation of learning method related to the target model. The variance is like L minus A, it's the error that's due to the particular way in which you learned things, like sampling and so on. You can estimate the significance of differences between different models by just permuting the different models into a single model. For more information on how to apply this technique to your own research, visit the University of California, San Diego's iReport.org website. For further information, visit the University of California's research site and the University of California, San Diego's website. the data, randomizing, essentially, the relationships in the data. And then you get a curve of performance of those models, and if yours lies outside the 95% confidence interval, then you have a P equal 0.05 result that this model is not random. So that's the typical way of going, and it's a typical way to look at the data as well as the results of the models. It's not a perfect system, but it's the best way to go. about this. Not discrimination in the legal sense, but Discrimination in the sense of separating different populations. And so you could say, well, yes, but some basis for differentiation are justified. And some basis of differentiation is justified in machine learning. But it's not the same thing as discrimination in a legal sense. It's a different kind of discrimination. And that's what we're trying to get rid of in the machine learning world. We want to make machine learning fairer and fairer. are not justified. So they're either practically irrelevant, or we decide for societal goals that we want them to be irrelevant and we're not going to take them into account. So one lesson from people who have studied this for a while is that discrimination is domain specific. So you don't want to discriminate against people based on their race, gender, sexual orientation or other factors that are not relevant to your own life. That's a very, very bad idea. It's not a good one. can't define a universal notion of what it means to discriminate because it's very much tied to these questions of what is practically and morally irrelevant in the decisions that you're making. And so it's going to be different in criminal law than it is in medicine, than it's in medicine. It's a very different field of law, and it's not the same thing in every area of life," he says. "It's a much more complex field than it used to be," he adds, "and it can be very difficult to define." in hiring, than it is in various other fields, college admissions, for example. And it's feature-specific as well, so you have to take the individual features into account. Well, historically, the government has tried to regulate these domains, and so credit is regulated by the Equal Credit Opportunity Act, education isregulated by the Education Credit Act, and it's a big part of the job market. It's a huge part of our job market, and we need to make sure that we're getting the most out of it. by the Civil Rights Act and various amendments, employment by the Civil rights Act, housing by the Fair Housing Act, public accommodation by the civil rights Act. More recently, marriage is regulated originally by the Defense of Marriage Act, which as you might tell from its title, was against things such as gay marriage. Marriage is now regulated by the Marriage Act of 2013. The marriage act was originally passed by the House of Representatives in 1996. It was amended by the Senate in 1998. The act was passed by a vote of 50-0. like people being able to marry who were not a traditional marriage that they wanted to defend, but it was struck down by the Supreme Court about six years ago as being discriminatory. It's interesting, if you look back to probably before you guys were born in 1967, until 1967, it was not legal to marry someone who was not a member of the same sex. It was not allowed to do that. It wasn't allowed to marry people who were outside of the traditional marriage. And it wasn't legal to do it. it was illegal for an African-American and a white to marry each other in Virginia. It was literally illegal. If you went to get a marriage license, you were denied, and if you got married out of state and came back, you could be arrested. This happened much later. Trevor is the son of former Virginia Gov. Robert F. F. Kennedy and the daughter of former Gov. William J. Jefferson. He is the grandson of former Governor Robert F.-F. Kennedy, and the great-great-grandson of William F. Jefferson, Jr. Noah, if you know him from The Daily Show, wrote a book called Born a Crime, I think. His father is white Swiss guy and his mother is a South African black. So it was literally illegal for him to exist under the apartheid laws that they had. He was born a crime, and his father was white Swiss. His mother was black. And so he was born an illegal immigrant in South Africa. It was illegal to be a white man in South African society at the time. He had to live in Switzerland. He had to pretend to be-- his mother was his caretaker rather than his mother in order to be able to go out in public, because otherwise, they would get arrested. So this has recently, of course, also disappeared, but these are some of the regulatory issues. So here are the latest developments in the case. The trial is expected to last until the end of the year, when the judge will make a decision on whether or not to press charges. The jury will be sequestered for the trial. Some of the legally recognized protected classes, race, color, sex, religion, national origin, citizenship, age, pregnancy, familial status, disability, veteran status, and more recently, sexual orientation in certain jurisdictions, but not everywhere around the country. OK, so given those examples, there are two legal doctrines about discrimination, and one of them is that it's illegal to discriminate against people on the basis of their race or color or religion or national origin or gender. The other is that discrimination is illegal if it's based on a person's sexual orientation. them talks about disparate treatment, which is sort of related to this one. And the other talk about disparate impact and says, no matter what the mechanism is, if the outcome is very different for different racial groups typically or gender groups, then there is prima facie evidence that there is disparate impact, he says. He says that's what the study shows, and that's why it's so important to look at the results of the study. The study will be published in a forthcoming book. is something not right, that there is some sort of discrimination. Now, the problem is, how do you defend yourself against, for example, a disparate impact argument? Well, you say, in order to be disparate impact that's illegal, it has to be unjustified or avoidable. So for example,. suppose I'm suppose I're a woman and I'm a mother, and I say, 'I'm a woman. I'm not a mother. I am a woman' trying to hire people to climb 50-story buildings that are under construction, and you apply, but it turns out you have a medical condition which is that you get dizzy at times. "I might say, you know what, I don't want to hire you, because I don’t want you plopping. down on the floor," he says. "That's what I would do to you if you were to apply for a job with me. I would say to you, 'You're not going to get the job, because you're going to fall down on your head.'" off the 50th floor of a building that's under construction, and that's probably a reasonable defense. A perfectly good defense is, yeah, it's true, but it's relevant to the job. If I brought suit against you and said, hey, you're discriminating against me on the basis of this medical disability, a perfectly goodDefense is that it's a medical disability. That's a reasonable Defense. If it's not a medical Disability, that's a perfectly Good Defense. So that's one way of dealing with it. Now, how do you demonstrate disparate impact? Well, the court has decided that you need to be able to show about a 20% difference in order to call something disparate impact. So, the question, of course, is can we change our hiring process? And the answer is, yes, we can. We can change our process. We just need to find a way to do it. And that's what we're trying to do. policies or whatever policies we're using in order to achieve the same goals, but with less of a disparity in the impact. So that's the challenge. Now, what's interesting is that disparate treatment and disparate impact are really in conflict with each other. So disparate impact is about distributive justice and minimizing equality of outcome. Disparate treatment is about procedural fairness and equality of opportunity, and those don't always mesh. In other words, it may well be that Equality of opportunity still leads to differences in outcome. and you can't square that circle easily. Well, there's a lot of discrimination that keeps persisting. There's plenty of evidence in the literature. And one of the problems is that, for example, take an issue like the disparity between different races or different ethnicities. It turns out that we don't have enough data on that, and we need to look at it more closely to find out what's going on, he says. He adds: "We need to make sure that we're doing everything we can to stop it from happening again" have a nicely balanced set where the number of people of European descent is equal to the people of African-American, or Hispanic, or Asian, or whatever population you choose descent, and therefore, we tend to know a lot more about the majority class than we know about these. We tend to be more interested in the lives of the people in the middle class than those in the lower class. This is a good thing, because it means we get to know more about these people than we do about the lower classes. minority classes, and just that additional data and that additional knowledge might mean that we're able to reduce the error rate simply because we have a larger sample size. OK, so if you want to formalize this, this is Moritz Hardt's part of the tutorial that I'm stealing from in in the tutorial. This is the part of Moritz's tutorial that he stole from in the video tutorial. It's the part where he talks about how to use the data to reduce error rates. this talk. This was given at KDD about a year and a half ago, I think. Moritz is a professor at Berkeley who actually teaches an entire semester-long course on fairness in machine learning, so there's a lot of material here. And so he formalizes the problem this way. It's a very interesting way to look at the problem of how to use machine learning to improve fairness in the real world. The talk was given by Moritz, who is a Professor of Computer Science at Berkeley. He says, look, a decision problem, a model, in our terms, is that we have some X, which is the set of features we know about an individual. We have some said A, which are the set. of protected features, like your race, or your gender, or. your age, and we have to decide what to do with all of them. He says, "We have to make a decision about what we think is the best way to go about it" or whatever it is we're trying to prevent from discriminating on, and then we have either a classifier or some score or predictive function that's a function of X and A in either case. Then we have some Y, which is the outcome that we're interested in predicting. So we have a combination of X, A, Y, and some predictive function. And that's what we're looking at in this case. We're looking for something that's going to be a good outcome. now you can begin to tease apart some different notions of fairness by looking at the relationships between these elements. So there are three criteria that appear in the literature. One of them is the notion of independence of the scoring function from sensitive attributes. So this says that R is independent of sensitive attributes, and that R can be used to score a game in a way that is fair to all the other players in the game. This is a very complex concept, but it can be worked out by analysing the data. is independent from A. R is a function of X and A, so obviously, that criterion says that it can't be afunction of A. Null function. Another notion is separation of score and the sensitive part of the score. Another idea is that R is independent from X and X is independent of X. The idea is to separate the score from the sensitive parts of it. The concept of score separation is called score separation. It's a concept that's been around for a long time, but it's not well understood. attribute given the outcome. So this is the one that says the different groups are going to be treated similarly. In other words, if I tell you the group, the outcome, the people who did well at the job and the people Who Did poorly at the Job, then the peopleWho Did Well At The Job will be treated the same. The same is true for the different types of people who do not do well at a job. The different groups will betreated the same way, and that is the way it should be. scoring function is independent of the protected attribute. So that allows a little more wiggle room because it says that the protectedattribute can still predict something about the outcome. It's just that you can't use it in the scoring function given the category of which outcome category that individual belongs to. And then sufficiency is the inverse of that. It says that given the scoring. function, the outcome is independent from the protected. attribute. That says, can we build a fair scoring function that separates the outcome from the. protected attribute? So here's some detail on those. The probability of a particular result, R equal 1, is the same whether you're in class A or class B in the protected attribute. So what does that tell you? That tells us a lot about the type of information we have to look at to be sure we're in the right place in the system. It's also called independence, and it's also known by various other names. It says that the probability of getting a certain result is the the same if you're a class B or a class A. you that the scoring function has to be universal over the entire data set and has to not distinguish between people in class A versus class B. That's a pretty strong requirement. And then you can operationalize the notion of unfairness either by looking for an absolute difference between those two groups or looking for the difference between the two classes in the same data set. That would be the way to look at unfairness in this case. It would be like looking for a difference between class A and class B in the data set, rather than between class B and class A. probabilities. If it's greater than some epsilon, then you have evidence that this is not a fair scoring function, or a ratio test that says, we look at the ratio, and if it differs from 1 significantly, then it's an unfair scoring function. And by the time you get to the end of the game, you've got all the evidence you need to make a decision. You've got to be able to make the right decision. And if you can't, you're not going to get the right answer. the way, this relates to the 4/5 rule, because if you make epsilon 20%, then that's the same as the 4-5 rule. Now, the problem-- there are problems with this notion of independence. So it only requires equal rates of decisions for hiring, or giving somebody a liver for transplant, and that's a big problem. It's not enough to have equal rates for hiring or giving someone a liver. It has to be equal for all of them. Hiring is based on a good score in group A, but random in B. So you might wind up with a situation where you wind up hiring the same number of people, the same ratio of people in both groups. Well, the outcomes are likely to be better for a group A than for group B, which means that you're developing more data for the future that says, we really ought to be hiring people inGroup A.or whatever topic you're interested in. And so what if hiring is based in group B? So for example, what if we know a lot more information about group B than we do about group A? because they have better outcomes. So there's this feedback loop. Or alternatively-- well, of course, it could be caused by malice also. I could just decide as a hiring manager I'm not hiring enough African-Americans so I'm just going to take some random sample ofAfrican-Americans and hire them, and so on. And that's how it could play out in the U.S., of course. But it could also be the other way around, too, if you want to think about it. then maybe they'll do badly, and then I'll have more data to demonstrate that this was a bad idea. So that would be malicious. There's also a technical problem, which is it's possible that the category, the group is a perfect predictor of the outcome, in which case, of course, it's not a good idea. It's also possible that it's just not a very good idea at all, and that's why we're trying to do it in the first place. they can't be separated. Now, how do you achieve independence? Well, there are a number of different techniques. One of them is-- there's this article by Zemel about learning fair representations, and what it says is you create a new world representation, Z, and it's called a fair representation. It's a way to create a world that's independent of the world you're living in. And it's a very powerful way to do it, because you can't have two worlds that aren't independent of each other. which is some combination of X and A, and you do this by maximizing the mutual information between X and Z. So this is an idea that I've seen used in machine learning for robustness rather than for fairness, he says. He says it could be used in the future to help improve the accuracy of machine learning algorithms, for example, or to improve the speed of a search engine that can find a human-like answer to a question about a series of words. "We could use it to help speed up the search engine," he says, "and it could also be used to help us find the right answers to questions about a sequence of words." where people say, the problem is that given a particular data set, you can overfit to that data set. One of the ideas is to do a Gann-like method where you say, I want to train my classifier, let's say, not only to work well on getting the data, but also to get the right results. It's a way to train a classifier that's going to be able to predict what the data is going to look like in the future. right answer, but also to work as poorly as possible on identifying which data set my example came from. So this is the same sort of idea. It's a representation learning idea. And then you build your predictor, R, based on this representation, which is perhaps not perfectly independent of the data set it's based on, but it's a good starting point for a model to start with. And so that's what we do in our example. We build our predictor based on that representation. the protected attribute, but is as independent as possible. And usually, there are knobs in these learning algorithms, and depending on how you turn the knob, you can affect whether you're going to get a better classifier that's more discriminatory or a worse classifiers that's less discriminatory. So you can can affect the outcome by changing the knob in the algorithm. And that's what we're trying to do in this study. We're trying not to change the outcome, but to make sure that the outcome is independent of that. do that in pre-processing. You can do some kind of incorporating in the loss function a dependence notion or an independence notion. And say, we're going to train on a particular data set, imposing this notion of wanting this independence between A and R as part of our desiderata. And that can be done in a number of ways, including using a loss function that is based on a dependency notion or a independence notion in the data set. It can also be done by using a data-flow algorithm that uses a loss-function based on the dependence notion, for example. so you, again, are making trade-offs against other characteristics. Or you can do post-processing. So suppose I've built an optimal R, not worrying about discrimination, then I can do another learning problem that says I'm now going to build a new F, which takes R and the protected attribute into R. Or I could do another problem where I'm trying to build an optimal F, not worry about discrimination. or I could use a different type of R. or a new type of F, for example. account, and it's going to minimize the cost of misclassifications. There's a knob where you can say, how much do I want to emphasize misclassification for the protected attribute or based on the protected attributes? So this was still talking about independence. The next notion is separation, that separation, and that's what we're going to do in the next round of changes. That's what the next step will be, and we'll see how that plays out in the future. says given the outcome, I want to separate A and R. So that graphical model shows that the protected attribute is only related to the scoring function through the outcome. So there's nothing else that you can learn from one to the other than through the outcomes. So this recognizes that protected attributes are related only to the outcome of the game, not to the score or the score function. The protected attribute can only be changed by the outcome and not by the score, so this recognizes this. that the protected attribute may, in fact, be correlated with the target variable. An example might be different success rates in a drug trial for different ethnic populations. There are now some cardiac drugs where the manufacturer has determined that this drug works much better in certain subpopulations than it does in others. It is possible that the protected attributes of a drug could be correlated to its effectiveness in certain areas of the body. For more information, go to www.cnn.com/2013/01/28/health/drugs/top-10-drugs-that-work-better-in-different-subpopulations. does in other populations, and the FDA has actually approved the marketing of that drug to those subpopulations. So you're not supposed to market it to the people for whom it doesn't work as well, but you're allowed toMarket it specifically for the people who it does work, he says. "You're not allowed to market to people who don't need it as much as you do," he says, "but you can market to those who do need it more than you do" well. And if you think about the personalized medicine idea, which we've talked about earlier. The populations that we're interested in becomes smaller and smaller until it may just be you. And so there might be a drug that works for you and not for anybody else in the class. Well, we're not there yet, but we're getting closer to the point where we might be able to make a difference in the lives of some people, if not all of them, at some point. but it's exactly the right drug for you, and we may get to the point where that will happen. We can build such drugs and where we can approve their use in human populations. Now, the idea here is that if I have two populations, blue and green, I can have a drug that works for both of them. And I can use it in both populations at the same time. And that's what we're trying to do here. We want to make sure we have the right drugs for both populations. and I draw ROC curves for both of these populations, they're not going to be the same, because the drug will work differently for those two populations. But on the other hand, I can draw them on the same axes, and I can say, look any place within this colored area. That's what I'm trying to do. I'm not trying to draw a map of the U.S., I'm drawing a picture of the world. I want to show the world what the world looks like. region can be a fair region in that I'm going to get the same outcome for both populations. So I can't achieve this outcome for the blue population or this outcomes for the green population, but I can achieve any of these outcomes for both. populations simultaneously. And so that's what I'm trying to do. And that's why I think it's a fair area, because I'm able to achieve all these outcomes simultaneously, and that's how I want it to be," he says. one way of going about satisfying this requirement when it is not easily satisfied. So the advantage of separation over independence is that it allows correlation between R and Y. So R could be a perfect predictor for Y. And it gives you incentives to learn. It gives you an incentive to improve your knowledge of the world. It also gives you a chance to learn more about the world around you. It's a win-win situation for all of us, and it's a great way to start your career. to reduce the errors in all groups. So that issue about randomly choosing members of the minority group doesn't work here. That would suppress the ROC curve to the point where there would be no feasible region that you would like. So for example, if it's a coin flip, if you're in a minority group, you're not going to be able to pick the right person. You're going to have to pick someone who's in the same group as you, and that's going to make it more difficult. then you'd have the diagonal line and the only feasible region would be below that diagonal, no matter how good the predictor was for the other class. So that's a nice characteristic. And then the final criterion is sufficiency, which flips R and Y. So it says that the regressor would be the only one that could predict the outcome of the other classes. That's a really nice characteristic, too, because it means that the predictor would have to be very good to predict the outcomes of both classes. or the predictive variable can depend on the protected class, but the protectedclass is separated from the outcome. So for example, the probability in a binary case of a true outcome of Y given that R is some particular value, R and A is a particular class, is the probability of the outcome of A and R being true. The probability of an outcome being true of Y is the same as the probability that A is true of R and R is true for A and A. For example, in the binary case, Y is likely to be true if R is higher than A, but not if A is lower. same as the probability of that same outcome given the same R value, but the different class. So that's related to the sort of similar people, similar treatment notion, qualitative notion, again. So it requires a parody of both the positive and the negative predictive values across different groups. So we need to be able to predict both positive and negative outcomes in the same way. We can't predict the positive or negative outcomes for the same group of people. We need to predict them for different groups of people in different ways. that's another popular way of looking at this. So for example, if the scoring function is a probability, or the set of all instances assigned the score R has an R fraction of positive instances among them, then the scoringfunction is said to be well-calibrated. So we've talked about probability, and we've also talked about scoring function. Now, let's look at the other side of the coin, and see if we can find a way to make the score more accurate. If R is not well-calibrated, you can hack it. Put it through a logistic function that will then approximate the appropriately calibrated score, and then you hope that that calibration will give you the score you want. That's what I've done before in the class. It's possible to hack R to get the score that you want, if you know what you're looking for. If you don't know what the score is, you'll have to hack it to get it. degree of calibration will give you a good approximation to this notion of sufficiency. These guys in the tutorial also point out that some data sets actually lead to good calibration without even trying very hard. So for example, this is the UCI census data set, and it's a binary data set. It's a good place to start if you want to get a good sense of how much data you have to work with in a given area of your computer. For more information on how to calibrate your computer click here. prediction of whether somebody makes more than $50,000 a year if you have any income at all and if you're over 16 years old. And the feature, there are 14 features, age, type of work, weight of sample is some statistical hack from the Census Bureau. Your education level, marital level, your education level and your type of job are also included in the feature. The feature is available on the site until the end of the month, when it will be rolled out to all users. status, et cetera, and what you see is that the calibration for males and females is pretty decent. It's almost exactly along the 45 degree line without having done anything particularly dramatic in order to achieve that. On the other hand, if you look at the calibration curve by race, you see that it's pretty decent, too, and that's a bit of a surprise. It doesn't look like they've done much to change the way they calibrate the system, but it's a good start. for whites versus blacks, the whites, not surprisingly, are reasonably well-calibrated. So you could imagine building some kind of a transformation function to improve that calibration, and that would get you separation. Now, there's a terrible piece of news, which is that you can't do that with blacks, because they're not as well-Calibrated as the whites are. You can do it with whites, but it's going to cost a lot of money, and it's not going to work. can prove, as they do in this tutorial, that it's not possible to jointly achieve any pair of these conditions. So you have three reasonable technical notions of what fairness means, and they're incompatible with each other except in some trivial cases. This is not good. And I'm not going to go into detail about how this works, because I don't want to give the wrong impression. It's not a very good idea to try to explain it to someone who doesn't understand it. to have time to go into it, but there's a very nice thing from Google where they illustrate the results of adopting one or another of these notions of fairness on a synthesized population of people. So it's a kind of nice graphical hack. There is one other problem that they point out which is interesting. Again, it'll be on the slides, and I urge you to check that out, but I'm not going to have time for it. I'll be back in a few days. So this was a scenario where you're trying to hire computer programmers, and you don't want to take gender into account. We know that women are underrepresented among computer people, and so we would like that not to be an allowed attribute in order to decide to hire someone. We would like women to be more represented in the computer industry. We want women to have a voice in the decision-making process. That's what we want to see in the future. We hope to see more women in the tech industry. There are fewer women who are programmers than men. It turns out that visiting Pinterest is slightly more common among women than men, according to a new study. Who knew? There are more women programmers in the U.S. than there are men. There are also more women in the tech industry than there were in the 1990s, the study found. There were also more men in the technology industry than in the 90s. There was a time when there were more men than women working in the IT industry. GitHub visits are more common among programmers than among non-programmers. Pinterest visits are also more common than they are for non- programmers. So if you want an optimal predictor of whether somebody's going to get hired, it should actually take both Pinterest visits and GitHub visits into account, say the experts at LinkedIn. They say that's because those are the best predictors of future job success. For more information, visit LinkedIn.com/Hire. Back to Mail Online home.back to the page you came from. go back to gender, which is an unusable attribute, they don't like this model. Here's another scenario that, again, starts with gender and says, look, we know that there are more men than women who obtain college degrees in computer science, and so there's an optimal separated score. And so we can create a different score which is not the same as the optimal score, but is permitted because it's no longer dependent on your sex, on your gender. That's a different scenario. influence there, and computer scientists are much more likely to be programmers than non-computer science majors. If you're were a woman-- has anybody visited the Grace Murray Hopper Conference? A couple, a few of you. So this is a really cool conference. Grace MurrayHopper invented the notion bug or bug or idea of a bug. She also invented the idea of the Internet. She was a pioneer in the field of computer science and the development of the Web, among other things. Grace Hopper was a really famous computer scientist starting back in the 1940s when there were very few of them. There is a yearly conference for women computer scientists in her honor. So clearly, the probability that you visited the Grace Hopper Conference is dependent on your gender. The conference was held in Washington, D.C. and was attended by more than 2,000 people. It was the first conference of its kind and was run by the National Institute of Standards and Technology. gender. optimal score is going to depend basically on whether you have a computer science degree or not. It's also dependent on if you're a computer scientist, because if you are a historian, you're not likely to be interested in going to that conference. And so in this story, the optimal scores are going to be based on the gender of the person, not the degree they have, but whether they're computer scientists or not, or if they're a historian or a computer engineer. the separated score will depend only on your gender, which is kind of funny, because that's the protected attribute. And what these guys point out is that despite the fact that you have these two scenarios, it could well turn out that the numerical data, the statistics from which you get your score, could well be different. It could be that the separated score could be different for men and women, depending on the gender of the person you're trying to score against, for example. estimate these models are absolutely identical. In other words, the same fraction of people are men and women. They have the same relationship to those other factors, and so from a purely observational viewpoint, you can't tell which of these styles of model is better. It's very difficult to tell which style is better from an observational viewpoint. It depends on how you look at it, of course, but it's very hard to tell the difference between the two styles of estimate. It can be very confusing. is correct or which version of fairness your data can support. So that's a problem because we know that these different notions of fairness are in conflict with each other. So I wanted to finish by showing you a couple of examples. So this was a paper based on Irene's work. And this was an example of how we can use data to make decisions about how to use the data in a way that is fair to all. And we can do this by using different ways of looking at the data. Irene, shout if I'm butchering the discussion. I got an invitation last year from the American Medical Association's Journal of Ethics, which I didn't know existed, to write a think piece for them. I decided that rather than just bloviate, I wanted to wanted to talk about fairness in machine learning. So Irene, shout If I'm Butchering the Discussion. I want to talk About Fairness in Machine Learning. Work.work. work. Work, Work. to present some real work. And so Marcia, who was one of my students, and I convinced her to get into this, and we started looking at the question of how these machine learning models can identify and perhaps reduce disparities in the U.S. "Irene had been doing somereal work," he says. "So we started to look at how these models could be used to help people in need. And that's what we're trying to do here. We're looking at how to use machine learning to improve people's lives." general medical and mental health. Now, why those two areas? Because we had access to data in those areas. So the general medical was actually not that general. It's intensive care data from MIMIC. And mental health care is some data that we hadAccess to from Mass General and other hospitals. For confidential support call the Samaritans in the UK on 08457 90 90 90, visit a local Samaritans branch or see www.samaritans.org for details. In the U.S. call the National Suicide Prevention Line on 1-800-273-8255. McLean's hospital here in Boston, which both have big psychiatric clinics. So yeah, this is what I just said. So the question we were asking is, is there bias based on race, gender, and insurance type? So we were really interested in socioeconomic status, but we didn't have that in this study. We were looking at race, age, gender and socioeconomic status. We didn't find any bias in that area, so that's what we were trying to find out. the database, but the type of insurance you have correlates pretty well with whether you're rich or poor. So we did that, and then we looked at the notes. If you have Medicaid insurance, for example, you're poor, and if you have private insurance, the first approximation, you’re rich. So, we were able to get a pretty good sense of what people’s financial situation was, and what they were looking for, and how they were going to respond. we wanted to see not the coded data, but whether the things that nurses and doctors said about you as you were in the hospital were predictive of readmission, of 30-day readmission. So these are some of the topics. We also wanted to know whether you were likely to come back to the hospital. We wanted to find out if you were more likely to be admitted to a hospital for more than a few days. We found that you were, in fact, more likely than you thought to be hospitalized. We used LDA, standard topic modeling framework. And the topics, as usual, include some garbage, but also include a lot of recognizably useful topics. So for example, mass, cancer, metastatic, clearly associated with cancer, Afib, atrial, Coumadin, fibrillation, associated with heart function, et cetera, in the ICU domain. In the In the Hospital domain, the topics are more general. The topics include: cardiomyopathy, cardiopulmonary resuscitation, pneumonia, respiratory treatments, surgical procedures, and more. psychiatric domain, you have things like bipolar, lithium, manic episode, clearly associated with bipolar disease, pain, chronic, milligrams, the drug quantity, associated with chronic pain, et cetera. So these were the topics that we used. And so we said, what happens when you look at the different topics, how often do they occur? And so that's what we looked at in this study. We looked at how often they occur, and how many times they occur. White patients have more topics enriched for anxiety and chronic pain. Black, Hispanic, and Asian patients had higher topic enrichment for psychosis. Male patients had more substance abuse problems. It's interesting. The different topics arise in different subpopulations. And so what we found is that, for example, white patients have. more topics that are enriched for Anxiety and Chronic pain, whereas black, Hispanic,. and Asian Patients had higher topics enrichment for Psychosis. It’s interesting. the different topics arose in different Subpopulations, and so on. Female patients had more general depression and treatment-resistant depression. What about insurance type? Well, private insurance patients had higher levels of anxiety and depression, and poorer patients or public insurance patients. So if you want to create a stereotype, men are druggies and women are depressed, according to this data. The study was published in the Journal of Affective and Mood disorders, published by the American Psychiatric Association. For confidential support, call the Samaritans on 08457 90 90 90, visit a local Samaritans branch or click here for details. had more problems with substance abuse. In the ICU population, men still have substance abuse problems. Women have more pulmonary disease. And we were speculating on how this relates to sort of sort of mental health issues in the U.S. and abroad. And then you could look at-- that was in the psychiatric population. And you could also look at the general population and see that they were more likely to suffer from mental health problems as well as physical problems. And so on and so on. of known data about underdiagnosis of COPD in women. By race, Asian patients have a lot of discussion of cancer, black patients of kidney problems, Hispanics of liver problems, and whites have atrial fibrillation. So again, stereotypes of what's most common in these different groups are most common, she says. For more information, visit the National Institutes of Health's COPD Center at: http://www.nhc.gov/ COPD/index.html. Those with public insurance often have multiple chronic conditions. Public insurance patients have atrial fibrillation, pacemakers, dialysis. Private insurance patients, on the other hand, have higher topic enrichment values for fractures. So maybe they're richer, they may have more chronic conditions, says Dr. David Frum, an expert on health care economics at the University of California, Los Angeles. For confidential support call the Samaritans on 08457 90 90 90, visit a local Samaritans branch, or see www.samaritans.org. play more sports and break their arms or something. Lymphoma and aneurysms. Just reporting the data. So these results are actually consistent with lots of analysis that have been done of this kind of data. Now, what I really wanted to look at was this question of, 'What is the effect of playing more sports on a person's health?' And that's what I was trying to get to the bottom of, which is why I wanted to do this study. It's just the facts. can we get similar error rates, or how similar are the error rates that we get, and the answer is, not so much. So for example, if you look at the ICU data, we find that the error Rates on a zero-one loss metric are much lower for men than men. The error rates are also lower for women than for men, according to the study. The study was published in the Journal of the American College of Obstetricians and Gynaecologists. they are for women, statistically significantly lower. So we're able to more accurately model male response or male prediction of 30-day readmission. Similarly, we have much tighter ability to predict outcomes for private insurance patients. We are able to predict more of ICU mortality for the ICU than we are for men. We have a better understanding of private health insurance patients than we do for women. We can predict better outcomes for women than we can for men, and that's a big difference. than for public insurance patients with a huge gap in the confidence intervals between them. So this indicates that there is, in fact, a racial bias in the data that we have and in the models that we're building. These are particularly simple models. In psychiatry, when you look at psychiatry, you can see that there's a huge racial bias, especially in the treatment of mental health patients, the researchers say. The study was published in the Journal of the American Academy of Psychiatry and Psychology. the comparison for different ethnic populations, you see a fair amount of overlap. One reason we speculate is that we have a lot less data about psychiatric patients than we do about ICU patients. So the models are not going to give us as accurate predictions. But you still see, as we have seen in the past, that there is a fair bit of overlap between the two groups of patients in the ICU and in the psychiatric ward. It's not a perfect comparison, but it's a good one. for example, a statistically significant difference between blacks and whites and other races, although there's a lot of overlap here. Between males and females, we get fewer errors in making predictions for males, but there is not a 95% confidence separation between them. And for private versus public insurance, for example, there's not a statisticallysignificant difference between private and public insurance. For women and men, there is a significant difference in the accuracy of predictions for women. For men and women, it's a little less pronounced. we do see that separation where for some reason, we're able to make better predictions for the people on Medicare than we are-- or Medicaid. So just to wrap that up, this is not a solution to the problem, but a way to try to make sure that people get the care they need, and that they get what they pay for, if they're eligible for it. That's what we're trying to do, and we're doing a good job of doing so so far. it's an examination of the problem. And this Journal of Ethics considered it interesting enough to publish just a couple of months ago. The last thing I want to talk about is some work of Willie's, so I'm taking the risk of speaking before the people who actually did the work. It's not about Willie, it's about the ethics of what we do and how we do it. And that's what we're going to focus on in this talk. We're not going to be talking about Willie's work. work here and embarrassing myself. So this is modeling mistrust in end-of-life care, and it's based on Willie's master's thesis and on some papers that came as a result of that. So here's the interesting data. If you look at African-American patients, and these are patients in the MIMIC data, they're more mistrustful of doctors. And they're also more distrustful of hospitals. And so on and so on. And that's the way it goes. set, what you find is that for mechanical ventilation, blacks are on mechanical ventilation a lot longer than whites on average. There's a pretty decent separation at the P equal 0.05 level, so 1/2% level between those two populations. Now, of course, we don't know exactly why. It could be any of a lot of different factors, it could be anything from the physiological to the insurance to God knows. It's not known if it's because there is a physiological difference or because it has something to do with their insurance. but that's the case. The eICU data set we've mentioned, it's a larger, but less detailed data set, also of intensive care patients, that was donated to Roger Marks' Lab by Phillips Corporation. And there, we see, again, a separation of mechanical ventilation duration roughly comparable to what we saw in the eICu data set. But that's not the case in the other data set that we've seen, which is a more detailed set of patients. in the MIMIC data set. So these are consistent with each other. On the other hand, if you look at the use of vasopressors, blacks versus whites, at the P equal 0.12 level, you say, well, there's a little bit of evidence, but not strong enough to reach any conclusions. So you say there's not enough evidence to draw any conclusions at this point in the study, but it's consistent with what we've seen in the past. Or in the eICU data, P equal 0.42 is clearly quite insignificant, so we're not making any claims there. So the question that Willie was asking, which I think is a really good question, is, could this difference be due not to physiological differences or even these sort of socioeconomic differences or anything like that? I think that's a good question. And I think it's a question we should be asking ourselves as a society. I don't know if we've got the answers to that yet. or social differences, but to a difference in the degree of trust between the patient and their doctors? It's an interesting idea. And of course, I wouldn't be telling you about this if the answer were no. And so the approach that he took was to look for cases where there was a difference between the patients and the doctors. It's a very interesting idea, and I'm glad to be able to share it with the world," says Dr. David Perry, the head of the U.S. Department of Health and Human Services. there's clearly mistrust. So there are red flags if you read the notes. For example, if a patient leaves the hospital against medical advice, that is a pretty good indication that they don't trust the medical system. If the family-- if the person dies and the family refuses to allow, that's a red flag. That's an indication that the person doesn't believe in the system. It's a sign that they're not confident in the care they're getting. It could be a sign of a lot of other things. them to do an autopsy, this is another indication that maybe they don't trust the medical system. For example, patient refused to sign ICU consent and expressed wishes to be do not resuscitate, do not intubate, seemingly very frustrated. So there are these sort of red letter indicators of mistrust, says Dr. David Houghton of the University of California, Los Angeles, where he is chairman of the Department of Neurosurgery and Cardiothoracic Surgery. and mistrusting of the health care system, also with a history of poor medication compliance and follow-up. So that's a pretty clear indication. And you can build a relatively simple extraction or interpretation model that identifies those clear cases. This is what I was saying about autopsies. So the problem is the problem, and the solution is to get people to talk to each other, to get them to talk about what's going on in their lives, what's causing the problem. of course, is that not every patient has such an obvious label. In fact, most of them don't. And so Willie's idea was, can we learn a model from these obvious examples and then apply them to the less obvious examples in order to get a kind of a bronze? He said it was a way of looking at the world and trying to find out what was going on in the body of a patient. He said: 'Can we learn from the obvious examples. And then apply that model to the lesser obvious examples?' standard or remote supervision notion of a larger population that has a tendency to be mistrustful according to our model. And so if you look at chart events in MIMIC, for example, you discover that associated with mistrust is a higher level of mistrust in the general population. But it's not as explicit a clear case of mistrust, as in those examples, as it is in the example of mistrusting people in the U.S., for example. It's more of a general mistrust of people in general. those cases of obvious mistrust are features like the person was in restraints. They were literally locked down to their bed because the nurses were afraid they would get up and do something bad. Not necessarily like attack a nurse, but more like fall out of bed or go wandering. The patients were locked down because they were in restraints, not because they had done anything bad, the court heard. The case was settled out of court and no charges were brought against the patients, the judge said. off the floor or something like that. If a person is in pain, that correlated with these mistrust measures as well. And conversely, if you saw that somebody had their hair washed or that there was a discussion of their status and comfort, then they were probably less likely to trust you. If you saw somebody have their hair washing or that they were talking about their status, then you were probably more likely to mistrust them. If they were saying something about their comfort or status, you were more likely not to trust them. be mistrustful of the system. And so the approach that Willie took was to say, well, let's code these 620 binary indicators of trust and build a logistic regression model to the labeled examples. And then apply it to the unlabeled examples of people for whom we don't have such indicators. And that's how he came up with the model. He says it's a way of looking at how people are more likely to be distrustful of their government, for example. a clear indication, and this gives us another population of people who are likely to be mistrustful and therefore, enough people that we can do further analysis on it. So if you look at the mistrust metrics, you have things like if the patient is agitated on some agitation scale, that's an indication of mistrust. If you are agitated on a certain scale, then that's a sign of mistrust, and that means you're more likely to have a negative view of the healthcare system. So that's what we're looking at. they're more likely to be mistrustful. If, conversely, they're alert, they are less likely to. So that means they're in some better mental shape. And if the patient was restrained, then trustful patients have no pain, if they're not in pain, they aren't as mistrustful, et cetera. If they were restrained, they weren't in pain and were more alert, so they were less trusting. So they were in a better state of mind, and they were more trusting. or they have a spokesperson who is their health care proxy, or there is a lot of family communication. But conversely, if restraints had to be reapplied, or if there are various other factors, then they're more likely to be mistrustful. So if you look at that prediction, what you can expect is that people will be more mistrustful of the government in the future. That's what we're trying to get out of the way of, so that we can move forward. find is that for both predicting the use of mechanical ventilation and vasopressors, the disparity between a population of black and white patients is actually less significant. So what this suggests is that the fundamental feature here is that there is a disparity between high trust and low trust patients. The study was published in the Journal of the American College of Cardiothoracic Surgeons (JACS) and the American Journal of Cardiology (AJSC). For confidential support call the Samaritans in the UK on 08457 90 90 90, visit a local Samaritans branch or click here for details. that may be leading to that difference is, in fact, not race, but is something that correlates with race because blacks are more likely to be distrustful of the medical system than whites. Now, why might that be? What do you know about history? I mean, you took the city. I mean,. you take the city, you're in charge. You're in control. You can do whatever you want to do, but you can't do anything about it if you don't trust it. training course that had you read the Belmont Report talking about things like the Tuskegee experiment. I'm sure that leaves a significant impression in people's minds about how the health care system is going to treat people of their race. My mother barely lived through Auschwitz, and so I'm Jewish. I don't want people to think that the way we treat people based on their race is the way they're going to be treated. That's not the way it's supposed to be. I understand some of the strong family feelings that happened as a result of some of these historical events. And there were medical people doing experiments on prisoners in the concentration camps as well, so I would expect that people in my status might also have similar issues of mistrust. It's a long way from a normal family life, but it's a good start, I think, and I hope it will lead to a better life for all of us. I'm looking forward to the day when we can all get together and celebrate together. Is mistrust, in fact, just a proxy for severity? Are sicker people simply more mistrustful, and is what we're seeing just a reflection of the fact that they're sicker? And the answer seems to be, not so much. So if you look, it seems mistrust is a sign of severity, but it's also a sign that people are more trusting of each other. And that's a good thing, if you're trying to get people to trust you more. at these severity scores like OASIS and SAPS and look at their correlation with noncompliance in autopsy, those are pretty low correlation values, so they're not explanatory of this phenomenon. And then in the population, you see that, again, there is a significant difference in sentiment expressed in the notes. And that's not just in the autopsy notes. It's also in the general population. And it's in the way people feel about their bodies. And so it's a very real phenomenon. between black and white patients. The autopsy derived mistrust metrics don't show a strong relationship, a strong difference between them. So I'm out of time. I'll just leave you with a final word. There is a lot more work that needs to be done, and I'm sure you'll agree with me that there's a long way to go. I'm sorry to say it, but there's no room for complacency in this country. There's no place for it in our society. done in this area, and it's a very rich area both for technical work and for trying to understand what the desiderata are and how to match them to the technical capabilities. There are these various conferences. One of the people active in the area, one of the pairs of people, is a former member of the Royal College of Physicians and a former director of the National Institute of Mental Health. It's a rich area for work, both in terms of technical work, as well as for understanding what theDesiderata is, and trying to match it to the capabilities. people, Mike Kearns and Aaron Roth at Penn are coming out with a book called The Ethical Algorithm, which is coming out this fall. I've not read it, but it looks like it should be quite interesting. And then we're starting to see whole classes in the area of artificial intelligence. It's a popular pressbook, and it's going to be very interesting to see how people use it in the future. It will be a big part of the future of the field. Fairness popping up at different universities. University of Pennsylvania has the science of Data ethics, and I've mentioned already this fairness in machine learning class at Berkeley. This is, in fact, one of the topics we've talked about. I'm on a committee that is planning the activities of the new Fairness Project. It will focus on data ethics, fairness, and the role of data in our society. It's a very exciting time for data scientists and data scientists in the U.S. and around the world. Schwarzman College of Computing has not yet started. "This notion of infusing ideas about fairness and ethics into the technical curriculum is one of the things that we've been discussing," he says. "The college obviously hasn't started yet, so we don't have anything other than this lecture and a few other things like that in the lecture series," he adds. "It's going to be interesting to see what happens in the next few years," the professor says of the new college. the works, but the plan is there to expand more in this area. The plans are still in the works, and there are plans to build more in the future. The project is expected to cost about $20 million to $30 million to build out in the next few years. The first phase of the project is set to be completed by the end of the year. The second phase will be completed in 2015, and the third in 2016, with plans to expand the project.