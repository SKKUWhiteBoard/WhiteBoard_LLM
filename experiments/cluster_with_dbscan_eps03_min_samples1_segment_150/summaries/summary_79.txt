This week's lecture will focus on machine translation. In the second half of the week, we take a break from learning more and more on neural network topics. We'll talk about final projects, but also some practical tips for building neural network systems. This is an important content full lecture. We hope to see you in the next week or so for our final week of lectures. Back to Mail Online home. back to the page you came from.. Click here for the next lecture.  assignment 3 is due today, assignment 4 is out today. Today's lecture is the primary content for what you'll be using for building your assignment 4 systems. For assignment 4, we give you a mighty two extra days. So you get nine days for it. And it's due on Thursday. On the other hand, do please be aware that assignments 4 is bigger and harder than the previous assignments. So do make sure you get started on it early. And then as I mentioned Thursday I'll turn to final projects. In the early 1950s, there started to be work on machine translation. Machine translation is the task of translating a sentence x from one language to another language. So we start off with a source language sentence x. L'homme, and then we translate it and we get out the translation man is born free, but everywhere he is in chains. OK. So there's our machine translation, and now let's get to the neural machine translation part of the story. Get straight into this with machine translation and we'll go through the prehistory of machine translation in a bit. This video clip shows some of the earliest work in machine translation from 1954. One of the first non-numerical applications of computers. $500,000 simple calculator, most versatile electronic brain known, translates Russian into English. Instead of mathematical wizardry, a sentence in Russian, it could be -- instead of Mathematical wizardry,. It could be-- instead of mathematical Wizardry. It was hyped as a way of keeping tabs on what the Russians were doing during the Cold War. The computer will be able to do about with a modern commercial computer about one to two million words an hour. This will be quite an adequate speed to cope with the whole output of the Soviet Union in just a few hours of computer time a week. When do you hope to achieve this speed? I our experiments go well, then perhaps within five years or so. And finally, Mr. McDaniel, does this mean the end of human translators? I say yes for translators of scientific and technical material. But as regards to poetry and novels, no, Despite the hype it ran into deep trouble. So the experiments did not go well. And so in retrospect, it's not very surprising that the early work did not work out very well. I mean, this was in the sort of really beginning of the computer age in the 1950s. That it was also the beginning of people starting to understand the science of human languages, the field of linguistics. So really people had not much understanding of either side of what was happening. So what you had was people were trying to write systems on really incredibly primitive computers, right? using to translate. And so effectively, what you were getting were very simple rule based systems and word lookup. So it was sort like, dictionary look up a word and get its translation. But that just didn't work well. Because human languages are much more complex than that. Often words have many meanings and different senses as we've sort of discussed about a bit. Often there are idioms. You need to understand the grammar to rewrite the sentences. So for all sorts of reasons, it didn't working well. And this idea was largely canned. When they were in the period of statistical NLP that we've seen in other places in the course. And then the idea began, can we start with just data about translation i.e. sentences and their translations, and learn a probabilistic model that can predict the translations of fresh sentences? So suppose we're translating French into English. We can say, what's the probability of different English translations? And then we'll choose the most likely translation. And we're not really expecting you to understand the details here. But I did then want to say a bit more about how decoding was done in a statistical machine translation system. The Rosetta Stone allowed the decoding of Egyptian hieroglyphs because it had the same piece of text in different languages. In the modern world, there are fortunately for people who build natural language processing systems quite a few places, where parallel data is produced in large quantities. The European Union produces a huge amount of parallel text across European languages. The Canadian Parliament conveniently produces parallel text between French and English, and even a limited amount in Inuktitut, Canadian Eskimo. And then the Hong Kong parliament produces English and Chinese. So there's a fair availability from different sources. And we can use that to build models. So how do we do it though? All we have is these sentences. And it's not quite obvious how to build a probabilistic model out of those. So in this case, what we're going to do is introduce an extra variable, which is an alignment variable. And so if we could induce this alignment between the two sentences, then we can have probabilities of how likely a word or a short phrase is translated in a particular way. is working out the correspondence between words that is capturing the grammatical differences between languages. So words will occur in different orders in different languages depending on whether it's a language that puts on the subject before the verb, or the subject after the verb. And the alignments will also capture something about differences about the ways that work languages do things. So you can have words that don't get translated at all in the other language. So in French, you put a definite article "the" before country names like Japon. So when that gets translated to English, you just get Japan. One French word gets translated as several English words. You can get the reverse, where you can have several French words that get translated as one English word. So here we sort of have four English words being translated as two French words. But they don't really break down and translate each other well. These things don't only happen across languages. They also happen within the language when you have different ways of saying the same thing. So another way you might have expressed the poor don't have any money is to say the poor are moneyless. That's much more similar to how the French is being rendered here. We had a simple way for language models, we just generated words one at a time and laid out the sentence. But here we need to deal with the fact that things occur in different orders in source languages and in translations. And so we do want to break it into pieces with an independence assumption like the language model. But then we want a way of breaking things apart and exploring it in what's called a decoding process. So we start with a source sentence. So this is a German sentence. And as is standard in German. So that's probably not in the right position for where the English translation is going to be. we have is based on the translation model. We have words or phrases that are reasonably likely translations of each German word, or sometimes a German phrase. And so then inside that, making use of this data, we're going to generate the translation piece by piece kind of like we did with our neural language models. So there's a search process. But one of the possible pieces is we could translate "er" with "he", or we could start the sentence with "are" translating the second word. So we could explore various likely possibilities. language model, it's probably much more likely to start the sentence with he than it is to start it with "are" though " are" is not impossible. And so we explore forward in the translation process. And in the process, I'll go through in more detail later when we do the neural equivalent. We sort of do this search where we explore likely translations and prune. And eventually, we've translated the whole of the input sentence. And I've worked out a fairly likely translation. In the period from about 1997 to around 2013, statistical machine translation was a huge research field. In 2014, the first modern attempt to build a neural network from machine translations and encoded-decoder model. Within two years' time, Google had switched to using neural machine translation for most languages. There are still lots of difficulties which people continue to work on very actively. And you can see more about it in the Skynet Today article that's linked at the bottom. We'd like to translate languages for which we don't have much data. model end to end on parallel sentences. And it's the entire system rather than being lots of separate components as in an old fashioned machine translation system. So these neural network architectures are called sequence to sequence models or commonly abbreviated seq2seq. And they involve two neural networks. There's one neural network that is going to encode the source center. And we're going to feed it in directly as the initial hidden state for the decoder, or RNN. But we do the same kind of LSTM computations and generate a first word of the sentence. "he." And so then doing LSTM generation just like last class, we copy that down as the next input. And we've translated the sentence, right? So this is showing the test time behavior when we're generating the next sentence. For the training time behavior, when we have parallel sentences, we're still using the same kind of sequence to sequence model. But we're doing it with the decoder part just like training a language model, where we're wanting to do teacher forcing and predict each word. Everywhere else as well. So you can do summarization. You can think of text summarization as translating a long text into a short text. But you can use them for other things that are in no way a translation whatsoever. So they're commonly used for neural dialogue systems. So the encoder will encode the previous two utterances, say. And then you will use the decoder to generate a next utterance. Some other uses are even freakier but have proven to be quite successful. So if you have any way of representing the parse of a sentence as a string. arc, right arc, shifts like the transition system that you used for assignment 3. Feed the input sentence to the encoder and let it output the transition sequence of our dependency parser. These models have also been applied not just to natural languages, but to other kinds of languages, including music, and also programming language code. So you can train a seq2seq system, where it reads in pseudocode in natural language, and it generates out Python code. And if you have a good enough one, it can do the assignment for you. In neural machine translation we are directly calculating this conditional model probability of target language sentence given source language sentence. So we have a source sentence. And that's going to strongly determine what is a good translation. And so to achieve that, what we're going to do is have some way of transferring information about the source sentence from the encoder to trigger what the decoder should do. And the two standard ways of doing that are you either feed in a hidden state as the initial hidden state to the decoding, or sometimes you will feed something in. each step, as we break down the word by word generation, that we're conditioning not only on previous words of the target language, but also each time on our source language sentence x. Because of this, we actually know a ton more about what our sentence that we generate should be. So if you look at the perplexities of these kind of conditional language models, you will find them like the numbers I showed last time. They usually have almost freakily low perplexities, that you will have models with perplexities that are something like 4 or even less. Both of those in a bit more detail. So the first step is we get a large parallel corpus. And we grab a lot of parallel English French data from the European parliament proceedings. So then once we have our parallel sentences, what we're going to do is take batches of source sentences and target sentences. We'll encode the source sentence with our encoder LSTM, and feed its final hidden state into a target L STM. And this one, we are now then going to train word by word by comparing what it predicts is the most likely word to be produced, versus what the actual first word, and then the actual second word is. generating the correct next word "he" and so on along the sentence. And the crucial thing about these sequence to sequence models that has made them extremely successful in practice is that the entire thing is optimized as a single system end to end. So starting with our final loss, we backpropagate it right through the system. So we not only update all the parameters of the decoder model, but we also update all of the parameters in the encoder model which in turn will influence what happens next. conditioning gets passed over from the encoder to the decoder. So this moment is a good moment for me to return to the three slides that I skipped. I'm running out of time at the end of last time, which is to mention multilayer RNNs. And having a multilayers RNN allows us the network to compute more complex representations. So simply put the lower Rnns tend to compute lower level features, and the higher RNN's should compute higher. level features. And just like in other neural networks, whether it's feed forward networks, or the kind of networks you see in vision systems, you get much greater power and success by having a stack on multiple layers of recurrent neural networks. And multilayer or stacked RNNs are more powerful. Can I ask you, there's a good student question here? What would lower level versus higher level features mean in this context? Sure. So I mean, in some sense, these are lower level features. are somewhat flimsy terms. The meaning isn't precise. But typically, what that's meaning is that lower level features and knowing sort of more basic things about words and phrases. So that commonly might be things like what part of speech is this word, or are these words the name of a person, or a company? Whereas higher level features refer to things that are at a higher semantic level. So knowing more about the overall structure of a sentence, knowing something about what it means, whether a phrase has positive or negative connotations. as the initial, as the initial hidden layer into then sort of generating translations, or for training the model of comparing the losses. So in particular, to give you some idea of that, a 2017 paper by Denny Britz and others, that what they found was that for the encoder RNN, it worked best if it had two to four layers. And four layers was best for the decoder Rnn. And the details here like for a lot of neural nets depend so much on what you're doing, and how much data you have, and things like that. But as rules of thumb to have in your head, it's almost invariably the case that having a two layer LSTM works a lot better than having a one layer L STM. become much less clear. Normally to do deeper LSTM models and get even better results. You have to be adding extra skip connections of the kind that I talked about at the very end of the last class. Next week, John is going to talk about transformer based networks. They're typically much deeper. But we'll leave discussing them until we get on further. So that was how we train the model. So let's just go a bit further and look at the data. So that we have our LSTM, we start, generate a hidden state. It has a probability distribution over words. And you choose the most probable one the argmax, and you say "he", and you copy it down and you repeat over. So doing this is referred to as greedy decoding. Taking the most likely word on each step. And it's sort of the obvious thing to do, and doesn't seem like it could be a bad thing toDo. But it turns out that it actually can be a fairly problematic thing todo. stuck with it. And you have no way to undo decisions. So if these examples have been using this sentence about, he hit me with a pie going from translating from French to English. But once you've generated it, there's no ways to go backwards. And so you just have to keep on going from there and you may not be able to generate the next word. And there are lots of reasons it could think so. Because after hit most commonly, there is a direct object now and then he hit a car, right? So that sounds pretty likely. translation you want. At best you can generate, he hit a pie, or something. And well, what could we do? Well, I sort of mentioned this before looking at the statistical empty models. Overall, what we'd like to do is find translations that maximize the probability of y given x, and at least if we know what the length of that translation is. And that's where that then requires generating an exponential number of translations. And it's far, far,far,far away. Far too expensive. So beyond greedy decoding, the most important method is something called beam search decoding. beam search's idea is that you're going to keep some hypotheses to make it more likely that you'll find a good generation while keeping the search tractable. So what we do is choose a beam size. And for neural MT, the beam size is normally around 1.5 to 2.5 times. It's not the only other decoding method. Once when we got on to the language generation class, we'll see a couple more. an example to see how it works. So in this case, so I can fit it on a slide. The size of our beam is just 2. Though normally, it would actually be a bit bigger than that. And the blue numbers are the scores of the prefixes. So these are these log probabilities of a prefix. So we start off with our start symbol. And we're going to say, OK. What are the two most likely words, to generate first according to our language model? I was, I got. OK. So we have four partial hypotheses. We work out the scores of each of them. And then we can say, which of those two partial hypotheses? Because our beam size, k equals 2, have the highest score? And so they are, I was, and he hit. We keep those two and ignore the rest. And so then for those two, we are going to generate k hypotheses for the next word. And we can do that by taking the previous score that we have the partial hypothesis and adding on the log probability. the most likely following word. He struck me and I was, I don't know, he struck me. And he hit a. So we keep just those ones. And then for each of those, we generate the k most likely next words tart, pie, with, on. Then again, we filter back down to size k by saying, OK, the two most likely things here are pie or with. And at this point, we would generate end of string. And say, OK,. We've got a complete hypothesis. then trace back through the tree to obtain the full hypothesis for this sentence. So that's most of the algorithm. There's one more detail, which is the stopping criterion. So in greedy decoding, we usually decode until the model produces an end token. And when it produces the end token, we say we are done. In beam search decoding, different hypotheses may produce end tokens on different time steps. And so we don't want to stop as soon as one path through the search tree has generated end. n complete hypotheses. And then we'll look through the hypotheses that we've completed and say which is the best one of those. And that's the one we'll use. OK. So at that point, we have our list of completed hypotheses and we want to select the top one with the highest score. Well, that's exactly what we've been computing. But it turns out that we might not want to use that just so naively. Because that turns out to be a kind of a systematic problem, which is not as a theorem. In a newspaper, the median length of sentences is over 20. So you wouldn't want to be having a decoding model when translating news articles that says, huh, just generate two word sentences. They're just way high probability according to my language model. So the commonest way of dealing with that is that we normalize by length. So if we're working in log probabilities, that means taking dividing through by the length of the sentence. And then you have a per word log probability score. you can argue that this isn't quite right. In some theoretical sense, but in practice it works pretty well and it's very commonly used. Neural translation has proven to be much, much better. It has many advantages. It gives better performance. The translations are better. In particular, they're more fluent because neural language models produce much more fluent sentences. But also, they much better use context because neural. language models give us a very good way of conditioning on a lot of contexts. all parameters of the model end to end in a single large neural network has just proved to be a really powerful idea. The models are also actually great in other ways. They actually require much less human effort to build. There's no feature engineering. There're in general, no language specific components. You're using the same method for all language pairs. Of course, it's rare for things like this to happen, but it's not unheard of. We'll come back to the costs of that later in the course. Neural machine translation systems also have some disadvantages compared to the older statistical machine translation system. They're less interpretable. They also tend to be sort of difficult to control. So there are various safety concerns. The best way to evaluate machine translation is to show a human being who's fluent in the source and can make a judgment on the quality of the translation. I'll show a few examples of that in just a minute. But first, before doing that, quickly how do we evaluation machine translation? target languages the sentences, and get them to give judgment on how good a translation it is. But that's expensive to do, and might not even be possible if you don't have the right human beings around. So a lot of work was put into finding automatic methods of scoring translations that were good enough. The most famous method of doing that is what's called BLEU. And the way you do it is you have a human translation or several human translations of the source sentence, and you're comparing a machine generated translation to those pre-given human written translations. Machine translation with statistical models had been going on since the mid 2000s decade. But by the time you entered the 2010s, basically progress in statistical machine translation had stalled. And you were getting barely any increase over time. Most of the increase you weregetting over time was simply because you're training your models on more data. In those years, around the early 2010s,. the big hope that most people had was that machine translation was going to get better and better. And it didn't. Even our best multilayer LSTMs aren't that great of capturing sentence meaning. There are particular problems such as interpreting what pronouns refer to. For languages that have lots of inflectional forms, these systems often get them wrong. So here's just sort of quick funny examples of the kind of things that go wrong, right? So if you asked to translate paper jam. Google Translate is deciding that this is a kind of jam just like this. And so this becomes a jam of paper. Many languages don't distinguish between things masculine or feminine. When that gets translated into English by Google Translate is that the English language model just kicks in and applies stereotypical biases. So if you want to help solve this problem, all of you can help by using singular they in all contexts when you're putting material online. And that could then change the distribution of what's generated. And people also work on modeling improvements to try and avoid this. Here's one more example that's kind of funny. People noticed a couple of years ago. That if you choose one of the rarer languages that Google will translate, that the gender neutral sentences get translated into, she works as a nurse. such as Somali, and you just write in some rubbish like ag ag ag. Freakily, it had produced out of nowhere prophetic and biblical texts, as the name of the Lord was written in the Hebrew language. As far as I can see, this problem is now fixed in 2021. So there are lots of ways to keep on doing research. NMT certainly is a flagship task for NLP and deep learning. And it was a place where many of the innovations of deep learning NLP were pioneered, and people continue to work hard on it. For assignment 4 this year, we've decided to do Cherokee English machine translation. Cherokee is an endangered Native American language that has about 2000 fluent speakers. It's an extremely low resource language. So it's just there isn't much written Cherokee data available period. And particularly, there's not a lot of parallel sentences between Cherokee and English. So we can see how far we can get. But we have to be modest in our expectations because it's hard to build a very good MT system with only a fairly limited amount of data. For languages for which there isn't much parallel data available, commonly the biggest place where you can get parallel data is from Bible translations. So you can have your own personal choice wherever it is over the map as to where you stand with respect to religion. But the fact of the matter is if you work on indigenous languages, what you very, very quickly find is that a lot of the work that's done on collecting data on indigenous language is based on Bible translations for many indigenous languages. So here's the initial bit of a story long. The Cherokee writing system has 85 letters. The reason why it has so many letters is that each of these letters actually represents a syllable. So many languages of the world have strict consonant vowel syllable structure. And so each of the letters represents a combination of a consonant and a vowel. And that's the set of those. You then get 17 by 5 gives you 85 letters, which is the number of letters in the Cherokee alphabet. Although you can also do the same thing in other languages. Most Cherokee now live in Oklahoma. There are some that are in North Carolina. The writing system that I showed on this previous slide, it was invented by a Cherokee man, Sequoyah. So he started off illiterate and worked out how to produce a writing system. And given that it has this consonant-vowel structure, he chose a syllabary which turned out to be a good choice. So here's a neat historical fact. So in the 1830s and 1830s, a lot of the Native Americans from the Southeast of the US got forcibly shoved a long way further West. 1840s, the percentage of Cherokee that were literate in Cherokee written like this was actually higher than thepercent of white people in the southeastern United States at that point in time. And so we had this model of doing sequence to sequence models such as for neural machine translation. And the problem with this architecture is that we have this one hidden state, which has to encode all the information about the source sentence. So it acts as a kind of information bottleneck. And that's all the info that the generation gets. is conditioned on. The order of words is very important to preserve. It seems like we would do better, if somehow, we could get more information from the source sentence while we're generating the translation. If you're a human translator, you read the sentence that you're meant to translate. And you maybe start translating a few words. But then you look back at the source sentences to see what else was in it and translate some more. And in some sense, this just corresponds to what ahuman translator does, right? that can sometimes improve performance. And we actually have that trick in the assignment 4 system. And you can try it out. So we generate along and generate our whole sentence in this manner. And that's proven to be a very effective way of getting more information from the source sentence more flexibly to allow us to generate a good translation. I'll stop here for now and at the start of next time. I will finish this off by going through the actual equations for how attention works.