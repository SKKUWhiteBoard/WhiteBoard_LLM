Today will be our first lecture on machine learning. Up until now, we have been assuming that somebody gives us a model. We want to build accurate systems. You get them from good models. And good models come from good data, and we're going to shift gears and look at machine learning, which is about how to acquire a model, from data and experience. The main topic for today is Naive Bayes models, but the real context here is that today will be Our first Lecture on Machine Learning. Machine learning is all about learning hidden representations and hidden concepts. We can learn parameters. These are the individual numbers and other details that determine exactly how our model works. For example, the probabilities that live in each conditional probability table of a Bayes net, that's an example of parameters. We could learn structure, so we could learn, for example, given a bunch of random variables, and then a. bunch of data showing observations of those random variables. We're going to work through some details of how the Naive Bayes models work. Then in the next few lectures, we'll work through a sequence of different different models. takes on machine learning that are going to highlight different subsets of the big ideas on this topic. We're going to have a couple running examples. One of them is that spam classifier that pulls out all the emails you don't want from your email. And something like digit recognition, we'll start to give you a window into how other vision tasks work. We'll see more in-depth examples and more structured examples of these kinds of problems later on when we talk about applications. as spam, and congratulations. You've just labeled some training data for a classifier. Somebody has to hand-label this data, and that can be expensive, or it can just fall out of the ecosystem that you have around you. So how are we going to make these decisions? Well, we have a bunch of data like what's shown on the right. I'll read you some of these. These are from an actual corpus, collection of data, that's labeled, that people use. This one is relatively small, and it's an older corpus, but this is a corpus of email that people used to test this problem. "To be removed from future mailings, simply reply to this message and put Remove in the subject" "99 million email addresses for only $99" "I know it was working pre-being stuck in the corner, but when I plugged it in, hit the power, nothing happened" "Had an old Dell Dimension XPS sitting in the corners and decided to put it to use." "I'm beginning to go insane. I know this is blatantly off-topic, but I'm beginning-to-go insane" very much an individual question which emails you want to receive or not. The boundary between what is actually spam, unsolicited commercial email, and emails you just don't want, this can be a fuzzy boundary. Some people just click on an email they wish people hadn't sent them, even if it's like from their mom. What is it about the top two emails that let you conclude that they're spam, and how could we automate this? Well, machine learning is going to do some amount of work, but something has to power this. probably something about "only $99" that's probably a sign of spam, but it's not like those words couldn't occur in ham. For example, maybe any dollar sign followed by some numbers is a bad sign, and that's a feature that abstracts over individual words. And so what we're going to need to be able to do is build a model that can aggregate that information, combine all of those little bits of weak evidence, manage that uncertainty, and then give us a prediction. In practice, for actual spam detection, a lot of the evidence of spam versus harm comes not from the word or even the content of the email in any way, but rather, its relation to other things in the ecosystem. For example, is the sender of this email in your contacts? Well if it is, this is probably not spam, even if it's got some sort of marginal contents. Has this email been widely broadcast within a short amount of time? Your email account can't tell this, but your email account provider can. these features, and then some match is going to happen in the middle where we're going to build a model and make predictions. We want to be able to predict labels of new images that are not the ones we've already seen, OK? So that's actually subtle, but it's super-important. We are not, like this is not-- This is not the sort of the Pokemon collection task here. We have to collect every digit, every image, right? Every image you see of a digit. is going to be unique. It's going to have to be at least one pixel off of something else you've seen. So you can't just collect all the data. You can get data that is similar, but then, in the end, you're going to need to generalize. What features might you use to detect digits? Well, somebody puts a grid of numbers. Your eyes and your visual processing system is already doing all kinds of processing. People will think about computer vision, replicate some of that processing. really noisy, and you're training set, they might be hard, expensive to label, because they're noisy. And then at test time, you're going to make mistakes because machine learning is not perfect. So people think about computer vision, think about invariances. What are better? We're just not all even going to agree on what the heck that's supposed to be. We need to find a way to make computer vision more accurate and more reliable, and that's where machine learning comes in. representations that if the thing gets tilted or it's a little bit lighter. It's not the exact pixels being on that we care about. But the pixels are something we could use. We could look at how many connected components of ink there are. What's the aspect ratio? How many loops are there? It's increasingly the case, especially for problems like this, that we feed in low-level features like pixels, and higher level features like edges. We'll talk about that in a couple weeks when we talk about narrow nets. some account activity and you want to red flag accounts that are suspicious. Automatic essay grading, auto grading, this can be a machine learning problem. Customer service email routing. You'd like to automate the routing of that. Review sentiment. Here's a bunch of reviews of my product. Which ones are good and which ones are bad? Have they gotten better in the past 10 days since the new announcement? And so on. You can do that with classification. You gotta do that before you can do things like translation. you will have enough information to go and build a basic classifier. But there's a whole bunch of detail behind all this. First thing we're going to do is talk about model-based classification. After today we'll look at the model-free methods. So what are the challenges? What structure should the Bayes net have? Today, we'reGoing to give it the simplest structure that could possibly work, and it turns out it often does. And then, the thing we are going to mostly talk about today is called variable illumination. We can do that with probabilistic inference, for example. is how should we learn the parameters of a model from data once we've decided it's structure? So here's what a Naive Bayes model would look like for digits. It's in fact super-simple as Bayes nets go. We say that the probability distribution over y, the class, and all the features which collectively basically are your input x, is the prior probability of the class. That's what lives at the bottom of the page. We'll be back in a few minutes. A general Naive Bayes model places a joint distribution over the following variables, y, which is your class, and some number of features, which you get to define. You're going to have to write code which extracts them from your input. The machine learning will do the work to connect the probability of that taking on a certain value up to the class. In addition, the way this joint probability is going to work, is you'regoing to have a prior probability of the feature. The Naive Bayes model will be linear in the number of features, whereas the joint probability distribution that you're implicitly describing is exponential. The thing we actually build is quite compact. All we have to have is y parameters, one for each class value, and then, for each feature, we need to have a little description of how likely that feature is for eachclass. And that's it. There's nothing new here. In fact, it's a simple case of inference by enumeration. to compute, the thing I actually want to compute is the probability of y, that is the distribution over all the different class labels, given my features. But I know that I can just compute the joint version of that, Y comma the features. What is that? Well if that were a lower case y, it would be a scalar. It would be an entry of the joint probability table. So it's a one-dimensional vector here that has an entry for each value of y. If I had all of these probabilities and I normalized, I would have the conditional probability of the class. The Bayes net assumes all the features are conditionally independent, given the label. Each one is the product of a prior probability of the class, which says whether or not before you see the evidence, this is a class that's common or not. How likely is it that the center pixel will be on for the number seven? These questions can only be answered by going to data.local probabilities. All right, so you compute all those products. You normalize them. You're done, OK? That's it. is we're going to need to figure out the prior probability over labels, and for each feature-- which is each kind of evidence-- we'll need to compute a bunch of conditional probabilities for each class. These things collectively, all these probabilities that we use to plug and chug and get our numbers out, are called the parameters of the model. So it has to come from data, which parameters we want. All right, let's see some examples of what these conditional probabilities in a Naive Bayes model would look like. 1 to 0 is equally likely. If you're looking at lots of round numbers, maybe it's 0. You can think about why that might be. So these come from data. And this actually underscores the point that depending on the data, depending on where you are, it depends on what you are looking at. And that's what we're trying to figure out here in this article. We want to know what you think is most common in real data. Or are they all equally common? So 0 might be common. how you collect the data, it can actually shape the distributions that you are imagining are going to exist at test time. We'll come back to that as well when we talk about risk minimization. All right. In addition to the prior probability of each label, we can compute things like, what is the probability that pixel 3 comma 1 is on, given each class? This isn't a distribution over on or off. These are just the probabilities-- what I'm showing here-- just the probability of that pixel for each class. The standard model for text is to say that the features are the words and that the random variables are the word at each position. So you would say something like, well, I have a joint probability over the class-- which could be spam or ham or document classes, or positive or negative sentiment, or whatever-- along with the rest of the document, the words. And so again, this is a naive Bayes model, and-- but the random variable now are not something like we're-- they're not-- the presence or absence of individual words, they're the product across each position of. the document of what word is at that position. So this is actually interesting, because I have-- in the image case, I had lots and lots of features, because it's a big grid. But each one was either on or off, right? With a document, one, they can be of any length. And two, each position has a large event space. There's a whole bunch of words that can be at position 23 in the document. But for the purposes of detecting spam versus ham, it doesn't really matter whether a word occurs at position 24. that comes in is going to get a probability for each class. And there's going to be a race to see which class wins in terms of probability. So what am I going to do? I'm going to have to see the words that come in, but I'mgoing to, in the end, compute two things. Whichever one is bigger, that's going-to be my prediction. If I want to know the probability that I assign, I have to take these two numbers and divide them by their sum. In this example, 2/3 of the emails are ham, and 1/3 are spam. This underscores that just because there's some distribution that your data reflects, and then there's the real world. And if there's major, major systematic ways in which your data and the distribution it was drawn from in its construction do not match the distribution in thereal world, you're going to have issues. One major issue you can have is reduced accuracy. And in spam, the most likely word is the. word free. predict 2/3 chance of ham. Gary is not particularly likely under either distribution. Direct address, actually knowing who you're writing to, that's much more likely in ham. Most people aren't named Gary. All right. But if you look, suddenly now, if you stopped me and you said, OK right right right, I would say, "Oh, my God, what is this?" I would be shocked. I would go, "What, minus 1.1. What is that?" Now that I've seen the word Gary, I'm 10 times more confident that spam is real. Gary, would is one of those words that's actually quite common, but it's not asymmetric. You can think about these as belief updates. Evidence is coming in, my beliefs update as I multiply in terms of probability. Maybe makes me think a little more ham, because would is a nice, harmless, common words that occurs in natural text. But you can see how, like, it is not like I'm going to delete every email that has the word you in it. model, the way they're aggregated is multiplying their conditional probabilities. Gary, would you like to lose weight while you sleep? And if you look at this now, it thinks it's spam. Somewhere in there, somewhere around lose weight it changed its mind. You can see, weight is a pretty strong indicator. Apparently so is sleep. OK. So this is what it's like to be a Naive Bayes model. Features come in, you aggregate all of the weak evidence, and then you output. be weighed, and that's what's going on here in the conditional model. It's actually very common when you're multiplying probabilities to just add log probabilities instead. In the end, when you want to turn it into probabilities, you do need to sum them. And summing the logs won't do that. You need to do a sort of log sum, which one way to do that is to convert them back to probabilities by taking exponentials. That's actually not the way you would do it. You would sort of shift them by their minimum or their maximum as appropriate. word depends on the class and also the previous word. This Is a better model of language. Will it be more accurate for classification? It really depends. In general, it will be a little more accurate, but at a cost of having significantly more complicated conditional probabilities to estimate. How much more accurate will depend on how much more complex the probabilities are to estimate, and how many words there are in the whole word set. If you started, if you did prediction in this, and you cranked out a pretend document, it would look significantly more like a real email. to what degree the bag of words assumption is dangerous. If you're looking for a class which is not kind of strongly connected to the actual ordering, Naive Bayes is really good. Otherwise, you add other correlations like this to fix it. All right. Let's take a break now. We are now into the machine learning section which means we are done with the spooky Halloween time ghost-busting section. But I have candy. So during the break, everybody get up and come grab candy if you would like. Come up and grab some, please. I'm not allowed to take it home. [NO SPEECH] All right, we're going to get started again. Let me say, your candy consuming I would rate as middle of the road. You can come back up to the-- at the end of the class if you would like to grab more. Allright, so let's talk about training and testing. So we talked a little bit about we want to build classifiers. We're going on the basis of data. How the heck am I supposed to know what pixel 7 comma 3 is for the number eight? I've got to get that from the data. all of the spam with high accuracy. So there's got to be some connection between what's going on in your data and this future used to which you're going to put the classifier to. A lot of machine learning theory is really based on trying to say something precise about the connection between those two things. We're going talk about a couple things relating to training, the mechanics of how you do parameter estimation in this particular model. But really, the reason we're doing this is not just for Naive Bayes, or probabilistic model estimation in general, but to see examples of more general phenomena. of empirical risk minimization goes something like this. We would like to find the model, classifier, whatever, that does the best on our true test distribution. We don't actually know that true distribution. So instead, we try to pick the best model on our training set and hope there's a connection between those two. Today, we're going to appeal more to just directly estimation of probabilities, but by the time we get there, we'll have a much better idea of what the true distribution is. to optimization-based methods over the next couple of lectures, you'll see this more generally. It's usually an optimization problem. Optimize some quantity on my training set in the hopes that that quantity will remain optimized on the test set. So what can go wrong here? The main worry, and this will be a little abstract now. The mainorry is that you over-fit. That's going to be a problem. So you worry about over-fitting. There's ways you can-- first of all, there's a couple of different things that can gowrong here. Machine learning theory has the most to say about the first few things, right? How-- what's the danger in having a small amount of training data, which means high sampling variance? And then, you can have tons of data drawn from the wrong distribution. What would this mean? You spend weeks doing practice exams for CS189, and then you walk into the CS188 final. You're like, something's wrong here. I learned really well. I understand the concepts, but it's just not lining up. In general, when somebody gives you a big vat of data, you're going to split it into pieces. You take one piece and you say, this is my training set. Someday it'll all be your training set if you want it to be, but when you're doing experiments, you want to try a bunch of different things. Does this Naive Bayes thing work? Maybe a neural net. So things like that. This idea of the test distribution sort of being not stationary against the training distribution is something that's really important in the real world. to have a test set, which is not the real future test use that it's going to be put once it's deployed, but you need something that is not in your training data to check. This is why you might, when you're studying, take some of those practice exams and not look at them until right before the exam, because you need to check your understanding. And so we take our data and we break into training, where we learn our parameters, and tests where we check our accuracy. You go through an experimentation cycle, it looks something like this. Get your model and learning already, and then you're going to learn the parameters. parameters are things like what's the probability of pixel 73 for the number eight? Then there's hyper-parameters, like, do I want to have features for the lowercased version of the words in case I've seen the word, but never uppercased? Right? These are questions about, is this or this orthis going to work better? You always know you're training data. The question is, do you generalize? This can happen to your classifier too, so you always want to test your performance on data that was not used to train it. And there can be a slow leak of your test data into your training data if you're not careful. So you try not to peek at the test set, and that's another reason why I say don't test your classifiers on the test data.to see that today. we have held-out data, which gives you something you can peek at. I ran 20 experiments. How did they go? Am I doing well? Is this thing good enough to release? You need to have some metric, and there's a lot of possible metrics. An easy one is accuracy. For how many of these emails did I make the correct decision? Fraction of instances predicted correctly, but actually, that's actually not a great metric for spam detection. Any ideas why? What's wrong with accuracy? cost-- of different kinds of mistakes may not be the same. And so accuracy isn't always what you want. What you really want, is you want a utility here. You want to know what was my utility, and you should have different costs for these things. There are also cases like machine translation, where you're always going to be a little bit off, a little word here or there, but there's a difference between being completely off and a tiny bit off. And again, we're going talk a lot today and next time about over-fitting and generalization. Spam detection is, in some ways, a very poor example of a canonical classification problem. The problem here is not that you're test accuracy is low, but your training accuracy was also low because you didn't learn anything. We'll investigate these things formally in a few lectures. I had a really good question during the break, which I want to answer for everybody, which is, couldn't you just defeat this Naive Bayes spam classifier by pasting the word Gary 100 times to the end of your offer to lose weight while you sleep? Spam is being generated by people who are trying to defeat spam filters. Spammers are going to double down on what's working. And so if you have features that are like, did the same email get sent to a lot of different people? What do spammers do? They're going to start modifying that email in some templated way. Now you have some feature that detects templates. Now there's sort of an arms race here. and so in that sense, in a sense, there's a spam arms race. over time, spam classification doesn't actually look like a standard classification problem because it's adversarial. OK, so in these images, you want to fit the hat right. You don't want it to be too small, because if you over-fit, you're not going to be able to generalize. Here's an example of this tradeoff. In general, we're going to do discrete classification. But for this example, let's imagine the thing we're trying to do is to fit a curve to this data. the data points of the squared distance or something. So what is my constant approximation to this? Does anybody want to hazard a guess? Let's call it five. OK, did I fit something about this data? Yeah, I felt something about the data. I fit basically it's mean. Did I capture the major trends? No. All right, let's try again. Let's fit a linear function. It's close, right? It's a better fit than the constant function. Notice that when I went to linear function, the space of hypotheses grew. than the quadratic. It's about fitting your data to the point where the patterns that you are capturing are ones which generalize to test, and that's a tricky balance. Over-fitting shows up not just on these continuous functions. It also shows up on discrete functions. And so, you can't basically just judge by your training accuracy. You need some measure of whether you've gone too far in the fitting process. And in this case, we talked about hyperparameters. A hyperparameter could be something like, what's the maximum degree of polynomial I'm allowed? In a hypothetical digit classification, we might say, here is an image I've never seen before. Let's use Naive Bayes to classify it. We'd say, all right. Well, before I look at any features, the numbers are the numbers two and three, let's say, are equally likely. So, so far, a three is winning. But eventually, I'm going to get to some pixel, maybe like this one here. And in my training data, this is almost never on. This is. in a corner where there's no number. This is an example of over-fitting, because this probability versus this probability, that is about the idiosyncrasies of the samples I have in my data. I can instead ask, when I do these multiplications together, into that running product, which are the words which sort of swing the product most one way or the other? I can look at odds ratios, which is the ratio of the probability in the two. And if two's going to win, because it didn't have that zero, that's bad. the ratio is one, it means it's equally likely. Whether it's common or uncommon, it doesn't affect the competition. It's things that are more common in one than the other that have a big impact on these odds ratios. What do you think, in my training data for ham versus spam, things with the highest odds ratio for ham would be? These are things that're significantly more likely for ham than for spam. Words like Gary, except when I look at my data, it's actually a mess. It turns out, there are a bunch of words in this data which occur in spam once, and it could occur in once and occurs in spam zero. of over-fitting, where the exact details of which sample points you drew when you collected your data get captured in a way that doesn't generalize. In Naive Bayes probabilistic models, over- fitting usually shows up as sampling variance. For other methods, it's going to show up in totally other ways. OK. To do better, we need to smooth, or regularize, our estimates. So let's figure out some ways to do that, to just illustrate what it would look like to limit over-fits. you shrink a hypothesis space, you fit less. Using it too much, you under-fit. So let's take a look at the distribution of a random variable, just to sort of show why we need to do these kinds of things. We can do elicitation, right? You can ask a human. You can go to a doctor and say, hey, I'm building a classifier. What fraction of people with meningitis will present with a fever? And a doctor can give you a guess. You could also do that empirically. of patient treatment or something like that. And this is basically what learning does. You take your training data, you take the trends out of the training data. The simplest version of this is for each outcome to look at the empirical rate. So, for example, if I am a jelly bean-counting robot, and I am trying to figure out in this vat of jelly beans, how many reds versus blues there are, and it's two reds and two blues, well, what can we do? In practice, you need some smoothing. But we want no surprises to our model. We want our model to assign probability to events it's never seen. So that one errant pixel or word that is rare doesn't completely torpedo an otherwise very nuanced balancing of evidence. All right, so what was the maximum likelihood estimate? You have to work this out, right? Maybe we can just go back and do it real quick. OK. So let's say r is my probability of red, and one minus r isMy probability of blue. What is the probability of D.for red. of this data? Well, it's basically I got an r, and then I got another r. And then I've got the other thing, which is one minus r. So as I change the probability of red, this term, the likelihood of the data, is going to go up and down. And the balance, the point where that's going to be maximized you can sort of, if you set it up carefully, take derivatives, find the extreme point, you'll get the relative frequency answer out. the parameters which maximize the product of this, which is what we were doing before. This denominator here doesn't matter. It's just some constant. But there's this extra term, p of theta. And so what this says, is it says, if I want to know what parameter or what probability is most likely, I need to weigh the likelihood of the data against how likely I think that parameter is in the first place. OK, so this doesn't have a closed-form solution without giving you more information. But here's a basic idea of how you might approach it. the sun will once not rise. So I know that this estimate is wrong, and I need some way of mechanically incorporating the fact that there are events which I haven't seen, but which I know to be possible, or at least that I'd like to model as being possible. So basically, add one to all your counts, including the ones that are zero. So the maximum likelihood estimate for red, red, blue, if I say, what's the probability of red, comma, probability of blue? 2/3, 1/3. two reds, there's three. There's those ones, plus my pretend red. And instead of one blue, there're two, because I have my pretend blue. Now what do I get? I get 3/5 and 2/5. Red is still winning, but this distribution has gotten flatter. And if there had been zero blues it would no longer be given probability zero. So pretty reasonable. We can do better. And so if I add zero, if I take Laplace's extended method and I addzero, then I just get my 2/3, 1/3 estimate from red, red, blue. there's 100 blues. Now how many reds do I have? Well, I do my computations as if I had 102 reds and 101 blues. And suddenly, even though there are still more reds than blues, in my posterior estimate here, it's pretty close to 50-50. So as I crank up k, I have a stronger prior, and I fit less. If I crank downk, I fit more, and so I now have a dial which can trade off the amount of fitting against generalization. example, I can go into my spam, and instead of computing odds ratios on the maximum likelihood-- or empirical relative frequency estimates-- I can instead do some smoothing. And suddenly things that only occurred once, they don't percolate to the top, because they haven't occurred enough to overwhelm that flat prior that I'm associating them with. So this is the top of the odds ratios for ham on the left, and favoring spam on the right. Some of these maybe make sense. Like, there it is. Free is probably in there somewhere. If you see money, that's a good sign that it's spam. But you might be wrong. Sometimes things surprise you, and that's why it's always good to like actually look into your model and see what has been learned here? Is there something that I can learn about this problem from what the machine has learned about the problem? All right. We talked about tuning. So let's say I build my Naive Bayes model for spam, for digits, whatever. I've got my features. Let's say they're mostly words and pixels. On your projects, you'll see you can do better. And I have some tuning to do. In general, your model is going to make errors. So we learn our parameters from the training data. We tune them on some different data, like some held-out data, because otherwise, you'll get crazy results. And then eventually, you're going to take the best value, do some final tests, test run. We're talking a bit more about features, because I think it's important for when we start to get to neural nets, where the stories here are going to change. In general, in general, you're going to need more features. In spam classification, we found out that it wasn't enough to just look at words. For digit recognition, you do sort of more advanced things than just looking at pixels, where you look at things like edges and loops and things like that. Try to do things that are more advanced than just pixels, like looking at loops and edges and edges, and try to look at other metadata from the ecosystem as well as just words. are invariant to rotation and scale and all of that the vision folks think about. You can add these as sources of information by just adding variables into your Naive Bayes model. We'll also talk in the next few classes about ways to add these more flexibly, and also ways to induce these. All right, I'm going to stop there for today, and as you go, please come up and grab some more candy. Thank you. Back to the page you came from.