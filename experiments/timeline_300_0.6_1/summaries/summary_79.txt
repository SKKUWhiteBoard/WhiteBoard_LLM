So, last time we were starting to talk about the sort of the general overview of what reinforcement learning involves. We introduced the notion [NOISE] of a model, a value, and a policy. Can anybody remember off the top of their head what a model was in the context of reinforcement learning? So what we're gonna do today is, sort of, um, build up for Markov Processes, up to Markov Decision Processes. And this build, I think, is sort of a nice one because it allows one to think about what happens in the cases where you might not have control over the world. The definition of a return is just the discounted sum of rewards you get from the current time step to a horizon and that horizon could be infinite. If the process is deterministic, these two things will be identical. But in general if theProcess is stochastic, they will be different. So, if we go back to our Mars Rover here and we now have this definition of reward, um, what would be a sample return? So, let's imagine that we start off in state s_4 and then we transitioned to s_5, s_6,. s_7 and we only have four-step returns. Markov Decision Processes are the same as the Markov Reward Process except for now we have actions. So, the agent is in a state they take an action, they get immediate reward, and then they transition to the next state. The reward can either be a function of the immediate state, the state and action to the state action and next state for most of the rest of today we'll be using that it's the function of both theState and action. If it's trying to do action A1 and for action A2 it'll move to the right unless it hits a_7 and then it'll stay there. Otherwise it will generally stay in that state. you do this computation. Just to quickly check that the Bellman equation make sense. So the immediate reward of this is zero plus gamma times [NOISE] 0.5. And that's just an example of, um, how you would compute one Bellman backup. So that's back to my original question which is you seem to be using V_k without the superscript pi to evaluate it. Oh, sorry this should, yes. This should have been pi. That's just a typo. There exists a unique optimal value function. The optimal policy for an MDP and an infinite horizon finite state MDP is deterministic. There are two choices for every state and there are seven states. The [NOISE] number of policies is |A| to the |S|. It's a mapping from states to actions so it's gonna be 2 to the 7th. We'll talk about- probably a little bit clearer to when we talk about contraction properties later. Um, any one want to take a guess of whether or not the optimal policy is always unique? I think there might be cases where it's not. strict inequality if the old policy was suboptimal. So, why does this work? So, it works for the following reasons. And the next questions that might come up is so we know we're gonna get this monotonic improvement, um, so the questions would be if the policy doesn't change, can it ever change again? And is there a maximum number of iterations of policy iteration? Here iterations is i. It's a kind of how many policies could we step through? So, in terms of policy iteration, this is very similar to what we saw before you can think of it in these Bellman operators and doing this argmax. Wanna see if we can get to a little bit on sort of the contraction operator. So this is what, um, value iteration does. It's a very similar policy iteration and evaluation. So, if an operator is a contraction it means that if you apply it to two different things, these are value functions, then the distance between them shrinks after.