James W. Swan: I hope everybody saw the correction to a typo in homework 1 that was posted on Stellar last night and sent out to you. We don't want to give you a homework assignment that's punishing. We're still going to talk about transformations of matrices. We looked at one type of transformation we could utilize for solving systems of equations. Today, we'll look at another one, the eigenvalue decomposition, and on Monday, we will look at the singular value decomposition. be stuck. It won't proceed after that. So it's the difference between getting a solution and writing a publication about the research problem you're interested in and not. So how do you do reordering? Well, we use a process called permutation. There's a certain class of matrix called a permutation matrix that can swap rows or columns. So if I want to swap columns, I multiply my matrix from the right, IP transpose. If I swap the rows and then I swap them back, I get back what I had before. James W. Swan: We discussed sparse matrices and a little bit about reordering and now permutation. He says permutation is a form of preconditioning a system of equations. He shows a simulation that tells us how probable it is to find the Plinko chip in a particular column. Swan: We could construct an alternative model that didn't have that part of the picture that we wanted to have. He's happy to answer questions on.at and he'll be back in a few minutes. For a real N-by-N matrix, there will be eigenvectors and eigenvalues, which are the amount of stretch. Eigenvalues are special vectors that are stretched on multiplication by the matrix. They're non-linear because they depend on both the value and the vector, the product of the two, for N plus 1 unknowns. So we can never say what an eigenvector is uniquely. We can only prescribe its direction. And that describes the eigen vector-eigenvalue pair. roots of the secular characteristic polynomial. They are the eigenvalues of the triangular matrix. Eigenvalues have certain properties that can be inferred from the properties of polynomials. These can sometimes come in handy-- not often, but sometimes. The eigen values of that matrix are going to tell us something about how different rate processes evolve in time. We'll see in a minute what those eigenvectors are in those transformation that reflect that transformation in physical processes. We want to know the eigenvector of the rate matrix having eigenvalue 0. This should correspond to the steady state solution of our ordinary differential equation. Can you do that? Can you find this eigen vector? Try it out with your neighbor. See if you can do it. And then we'll compare results. Are you guys able to do this? Sort of, maybe? Here's the answer, or an answer, for the eigenector. It's not unique, right? It's got some constant out in front of it. James Swan: Can you find the eigenvalues and some linearly independent eigenvectors of this matrix? And if you find them, what are the algebraic and geometric multiplicity? He explains why this is important in a second, but understanding that this can happen is going to be useful for you. He says the matrix then is said to have a complete set of eigendecomposition pairs, and that's useful for solving differential equations, ordinary differential equations or for transforming systems. triangular form for this matrix. We'll talk next time about the singular value decomposition, which is another sort of transformation one can do when we don't have these complete sets of eigenvectors. You'll get a chance to practice these things on your next two homework assignments, actually. So it'll come up in a couple of different circumstances. I would really encourage you to try to solve some of these example problems that were in here. Solving by hand can be useful.