Danqi Chen is one of the foremost researchers in question answering. She is the professor at the Princeton University. Danqi once upon a time was the head TA of CS224N. She's quite familiar with the context of this class. So today I'm very happy to introduce to you some of the fundamentals in this field, as well as on cutting edge and state of the art topics. So here's my plan for this lecture. I would give a brief introduction of what question answering is. And then today, delighted to have our first invited speaker. is question answering, and what kind of problems that people are studying today. Then I'm going to spend the most of this lecture focused on one type of question answering problems called reading comprehension. And then at the end of the lecture, I'm hoping to spend hopefully like 10-ish minutes to talk about a more practical, and in my opinion, more exciting problem called open domain question answering. And so my point is to try to quickly go over some of those state of the art measures in this area. OK so let's just get started. Our open- domain questions or closed-domain questions are simple questions versus more complex or compositional questions. And for the answer type, it can also be a short segment in the text, or a paragraph, and a list or even the yes or no questions. All these problems may require very different techniques or different data, or even different evaluation metrics to evaluate all these different problems. And, the question and answer has enabled a lot of really useful real world applications. For example, today if you just put your question in a search engine like Google. Question answering is probably one of those fields that we have seen the most remarkable progress in the last couple of years driven by deep learning. Almost all of the states are question answering systems today built on top of the end to end training of the deep neural networks and the pre-trained language models such as BERT. In this lecture, I will be mostly focusing on the text based, or textual question answering problems. So basically, we are trying to answer questions based on text.timer. the unstructured text. There are many other really big question answering problems. And each of them can be really like a subfield in NLP and they actually have very different challenges and also model designs. So next, I'm going to start with a part 2, reading comprehension. I just want to quickly check if there are any quick questions I can answer before I start us on part 2. OK. So let's talk about the reading comprehension then. So basically we wanted you to question build answering systems to answer questions that can answer questions over a very large database. Reading comprehension has been viewed as a very important test bed for evaluating how well computer systems understand human language. This is really just similar to how we humans actually test the reading comprehension test to evaluate how well we actually understand one language. So this is also the way that we actually post questions to test the machine's language understanding ability. It actually has been formally stated back in 1977 by Wendy Lehnert in her dissertation. She says that, since questions can be devised to query any aspect of text comprehension, the ability to answer questions is the strongest possible demonstration of understanding. Personal subject, Barack Obama. So we want to fill in what is-- fill in this question mark and figure out, OK, where Barack Obama was educated at. So one way to solve this problem is basically trying to convert this relation into a question. So where did Barack Obama graduate from? And taking all of that relevant piece of text and then by applying a reading comprehension problem. Then basically, we can find out-- the correct answer should be Columbia University. That is also the output of this information extraction system. And another example is actually called a semantic role labeling. sets have been also collected, basically runs this size around 100K. So for these datasets-- so the passages is like a single paragraph selected from the English Wikipedia, which usually consists of 100 to 150 words. And the questions are crowd-sourced, basically like from Mechanical Turking. And this is a very important property of the dataset, is that each answer is a short segment of text, or we called it span in the passage. So we can see that there is an exact match score between the predicted answer and any of the gold answers. answer to that. So next, I'm going to talk about how to build neural models for reading comprehension, and in particular, how we can build a model to solve the Stanford Question Answering Dataset. So the input of this problem is let's take a context or paragraph. And also we take our question, Q. And the question consists of n tokens q1 to qN. And because the answer has these constraints, the answer must be a section of text in the passage. We are going to predict a start and then end. called context-to-query attention. The idea is for each context word, can we find the most relevant words from the question for the query words. And then the second type of attention is called the query to context attention. So this is actually just an intuition of these two types of attention. And this is also why this model is called a bidirectional attention flow because there's a context- to- query attention and there is also a query-to. context attention in this model. means this context word is not very relevant. So basically, that's why we take the softmax-- take softmax over on top of the max, yeah. OK. Do you want even more or do you want to go on? I probably should go on. We have a lot of slides, but I'm happy to answer questions after those ones. Yeah. So the last part of this model is actually the easiest, the most simplest one. Lastly, these are two layers, modeling layer and output layers. the dot product between w end and this vector, and this can produce all the probability over all the conditions which predict how likely this position will be the end the position of the answer. So by passing the mi to another bedirectional LSTM, their reasoning is that they're trying to capture some kind of dependence between the choice of the start and end. OK, so this model is actually achieved-- like on SQuAD data set, it achieved a 77.3 F1 score. They found that both attentions in two directions are actually important. the models are actually a very similar ballpark. So numbers range from the highest number here, 79.8 until after the ELMo was introduced, the numbers have actually improved quite a bit. And now here is our attention visualization to show that how this smorgasbord of attention actually can capture the similarity between the question words and the context words. So basically this theory tells us that these kind of attention scores can actually capture those negative scores pretty well, yeah. OK, so next, I'm going to talk about BERT, how to use the BERT model to solve this problem. BERT models are pre-trained while BiDAF models only builtd on top of the GloVe vectors. Pre-training basically can just change everything and it gives a very large boost in performance. Even if you use a stronger pre-training models or modern, like a-- stronger models than the BERT models, they can even lead to better performance on SQuAD. And then finally, if you see even the latest pre- trained language models, including the XLNet or RoBERTa or Albert, these models can give you another like 3, 4 F1 score. pre-training has been so important. Next I will quickly talk about-- OK, a question here is that can we actually even design better pre-training objectives for reading comprehension or question answering? And the answer is actually yes. So this is actually a work I did with Mandar Joshi and other folks one year ago called SpanBERT. So think about this. So for SQuAD and other a lot of extractable reading comprehension datasets, the goal is trying to predict the answer span from the passage. SpanBERT greatly outperformed Google BERT and other BERT basically across all of the datasets. This number has already exceeded even the human performance on SQuAD. So that really tells us that OK, even if we are not going to increase the model size or increase the data, by designing better pre-training objectives can also go a long way and do a much better job at least in the question answering and the reading comprehension datasets. OK. So so far, we've demonstrated that by using BiDAF model and by using BERT models, we can get a very good performance on the SQuad dataset. is another paper that actually just came out in 2020. So there has to be a lot of evidence showing the similar things. So today we compute a very good reading comprehension data set on the individual data sets. But these systems trained on one dataset basically cannot really generalize to other datasets. And all the other numbers in this table basically shows that if you train one system on one datasets and then evaluate on another dataset, the performance will drop quite a lot. So it basically really cannot generalize from one dataset to another dataset. a large collection of documents. So one example is just taking the whole Wikipedia, which has five million articles. And we're going to return the answer for any open-domain questions. So this problem, there isn't any single passage, so we have to answer questions against a very largeCollection of documents or even the whole web documents. This is actually a much more challenging and also more practical problem. So if you look at the example of Google example I showed at the beginning, so these techniques will be very useful in the practical applications. Danqi can stay for a bit to answer questions, but not forever. Because she doesn't have a Stanford login, we're going to do questions inside Zoom. So if you'd like to ask a question, if you use the raise hand button, we can promote you so that you appear in the regular Zoom window and can just ask questions and see each other. If you hang around and don't leave the Zoom for more than a few minutes, maybe we'll just promote everybody who's still there. unsupervised question answering so by using some kind of approach like some form of unsupervised machine translation, this kind of idea. That can be borrowed-- can borrow the idea from that and can also work pretty well, reasonably well in unsuper supervised question answering. So my question is I guess it's kind of interesting that there's not really that strong of a transfer effect between data sets that are kind of ostensibly similar. So have there been any research done on how close I guess the formatting and the semantic content of these question answering datasets actually adheres to the data that BERT is pre trained on? And if so, has there been sort of any effect found between those similarities or differences? The goal of this project is trying to ingest all the phrases in the Wikipedia. So these conversations are built using the training set of the question answering datasets. So if we use say a different data set that does not present the information using the structure presented in Wikipedias, this model may not work as well as.kind of the intuition behind the dense phrases apart from the answers will probably be in close proximity. And what if the datasets has answers to a specific question like very far from the actual information? Say, the answers to your question may not resided in close. proximity to the words in the question. the answer? So why do you think the nearest neighbor-- I mean, you always can find something, right? The question is that whether it's close enough or not. So the question is what if in the datasets that the answer is not close enough? Yeah, that's a good question. If you really come up with something that is really very far away from all the questions that we have been seeing in the training set, that could be possible. Basically it depend on how the text are formatted. thank you for bringing this one up. OK. Next is-- Hi, thanks for taking the time. In what extent can in context learning help models to be more robust with respect to different domains? Can you tell me what you mean by in context? So like basically you provide the template generated by BERT. And then instead of directly predicting the classes of text classifications, you just use some word to represent that class or predict the wordings there. OK, actually, I have been through this something related to that recently. Do you think that the current sort of benchmark data sets are maybe a little bit too easy for- [INTERPOSING VOICES] Or just like that.solve the easy problems. So all these things need to be resolved. One final thought is that having a lot of transversely trying to have a lot. of humans in the loop of the frameworks to evaluate these kind of systems. Just try to break the current system, come up with some harder questions. Are you still game for a couple more questions? Next question is from Danqi, who was one of the co-organizers of the EfficientQA task. Danqi: How concerned should we be about potential encoding of biases into these record labels or how we evaluate them, or is that just more of a concern for more open ended questions? Yeah, this is definitely very important. I mean, we will have more discussion of toxicity and bias coming up very soon, including actually Thursday's lecture as well as a later lecture, not specifically about QA though. people also leveraged similar things for question answering. So I was just wondering to what extent these kind of techniques can work on one group of tasks, not just limited to question answering, but mainly question Answer. Yeah, I don't know. At least for the work that I have seen so far, it all applied or operated at a very simple sentence classification task. OK then we've got-- and maybe we should call this the last question. Hi, what is the intrinsic difference between solving question answering with generative models like T5 versus encoders like BERT? The key difference between the generative model and the extractive model is that for generative models, you can actually leverage more input passage together and do the generation. So for this model, there isn't any retrieval. So you can always find the answer from the question, right? So this model really has you relying on all the parameters you memorized, all the encodings. And then Generative models are you're remembering the whole question and you try to retrieve the memory when you answer the question. The model is very large, like 11 billion parameters. So the parameters are basically trying to memorize a lot of information that has been.information. So by just taking this input, it has to just rely on the parameters to infer this answer. So it's actually very hard to-- yeah, it's a definite balance between memory and the generalization from it. All right, thanks. Do you want to call it a night or do you want one more question? Either way, yeah. collect so many examples for other languages. And there has been also a lot of work trying to do cross-lingual question answering and stuff like that. If we have, actually, I think that the techniques can be generally applied to other languages, he says. "I think that we have a lot to learn from each other," he adds. "We have a long way to go, but I think we're making progress." "We've got a lot more to learn," he says, "and we're getting better at it."