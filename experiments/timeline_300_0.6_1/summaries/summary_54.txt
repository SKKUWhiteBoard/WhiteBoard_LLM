Sadhana sadhana is a machine learning scientist at Themis Ai and the lead TA of the course intro to deep learning at MIT. She'll be teaching us more about specifically the bias and the uncertainty of AI algorithms which are really two key or critical components towards achieving this Mission or this vision of safe and trustworthy deployment of AI all around us. Sadhana will talk about how we can build very modular and flexible methods for AI and building what we call asafe and trustworthy Ai. behalf of Themis so over the past decade we've seen some tremendous growth in artificial intelligence across safety critical domains in the Spheres of autonomy and Robotics. A lot of these Technologies were innovated five ten years ago but you and I don't see them in our daily lives so what is what's the Gap here between Innovation and deployment. We'll talk about how Themis is innovating in these areas in order to bring new algorithms in this space to Industries around the world after we talk about the ramifications for this for real world AI. Commercial facial detection systems are everywhere you actually played around with some of them in lab two when you trained your vae on a facial detection data set. The biases in these models were only uncovered once an independent study actually constructed a data set that is specifically designed to uncover these sorts of biases by balancing across race and gender. There are other ways that data sets can be biased that we haven't yet talked about so so far we've assumed a pretty key assumption in our data set which is that the number of faces in ourData is the exact same as theNumber of non-faces in our Data set. A de-biasing algorithm that automatically uses the latent features learned by a variational autoencoder to under sample and oversample from regions in our data set. This approach basically approximates the latent space via a joint histogram over the individual latent variables. Once we apply these this debiasing we have remarkable results this original graph shows the accuracy gap between the darker Mills and the lighter Mills and this is the devising algorithm where once we apply the debiased algorithm we're pretty pretty pretty. lecture so so far we've been focusing mainly on facial recognition systems and a couple of other systems as canonical examples of bias however bias is actually far more widespread in machine learning. Consider the example of autonomous driving many data sets are comprised mainly of cars driving down straight and sunny roads in really good weather conditions with very high visibility. In some specific cases you're going to face adverse weather bad um bad visibility near Collision scenarios and these are actually the samples that are the most important for the model to learn. tend to amplify racial biases a paper from a couple years ago showed that black patients need to be significantly sicker than their white counterparts to get the same level of care. That's because of inherent bias in the data set of this model. In all of these examples we can use the above algorithmic bias mitigation method to try and solve these problems and more so we just went through how to mitigate some forms of bias in artificial intelligence and where these Solutions may be applied. We talked about a foundational algorithm that Themis uses that UL will also be developing today. the core idea behind uncertainty estimation so in the real world uncertainty estimation is useful for scenarios like this this is an example of a Tesla car driving behind a horse-drawn buggy which are very common in some parts of the United States. The exact same problem that resulted in that video has also resulted in numerous autonomous car crashes so let's go through why something like this might have happened there are multiple different types of uncertainty in neural networks which may cause incidents like the ones that we just saw we'll go through a simple example that illustrates the two main type of uncertainty that we'll focus on. model won't be able to compute outputs for the air points in this region accurately because very similar inputs have extremely different outputs which is the definition of data uncertainty we also have regions in this data set where we have no data so if we queried the model for a prediction in this part of the data set we should not really expect to see an accurate result because the model's never seen anything like this before and this is what is called Model uncertainty when the model hasn't seen enough data points or cannot estimate that area of the input distribution accurately enough to Output a correct prediction. names to the types of uncertainty that we just talked about the blue area or the area of high data uncertainty is known as aliatoric uncertainty it is irreducible as we just mentioned and it can be directly learned from data which we'll talk about in a little bit. The green areas of this just the green boxes that we talked about which were Model uncertainty are known as epistemic uncertainty and this cannot be learned directly from the data. This can be reduced by adding more data into our systems into these regions. epistemic uncertainty let's go back to our real world example let's say the again the input is the same as before it's a RGB image of some scene in a city and the output is a pixel level mask of what every pixel in this image belongs to. We can use bias and uncertainty to mitigate risk in every part of the AI life cycle. analyzing the data before a model is even trained on any data we can analyze the bias that is present in this data set and tell the creators whether or not they should add more samples. Themis is a company that develops and deploys trustworthy AI across Industries and around the world. We're hiring for the upcoming summer and for full-time roles so if you're interested please send an email to careers themesai.io or apply by submitting your resume to the Deep learning resume drop and we'll see those resumes and get back to you thank you foreign thank you for joining us today. We'll use capsa in today's lab to thoroughly analyze a common facial detection data set that we've perturbed.