foreign I'm really excited especially for this lecture which is a very special lecture on robust and trustworthy deep learning by one of our sponsors of this amazing course themus AI. Themis AI is a startup actually locally based here in Cambridge our mission is to design advance and deploy the future of AI and trustworthy AI specifically. I'm especially excited about today's lecture because I co-founded Themis right here at MIT right here in this very building in fact this all stemmed from really the incredible scientific innovation and advances that we created right here. Sadhana is a machine learning scientist here at Themis Ai and the lead TA of the course this year. She'll be teaching us more about specifically the bias and the uncertainty Realms of AI algorithms. Sadhana: Over the past decade we've seen some tremendous growth in artificial intelligence across safety critical domains in the Spheres of autonomy and Robotics. But there's another question that we need to ask which is where are these models in real life a lot of these Technologies were innovated five ten years ago but you and I don't see them in our daily lives. Bias is what happens when machine learning models do better on some demographics than others. Uncertainty is when models don't know when they can or can't be trusted. These are the two big challenges to robust deep learning we'll also talk about solutions for them that can improve the robustness and safety of all of these algorithms for everyone. We'll talk about how to do this and methods for mitigation of this bias algorithmically and how Themis is innovating in these areas to bring new algorithms in this space to Industries around the world. Facial detection systems exhibit very clear selection bias and evaluation bias. The biases in these models were only uncovered once an independent study actually constructed a data set that is specifically designed to uncover these sorts of biases by balancing across race and gender. There are other ways that data sets can be biased that we haven't yet talked about so so far we've assumed a pretty key assumption in our data set which is that the number of faces in ourData set is the exact same as theNumber of non-faces in our Data set. let's first start by training a vae on this data set the Z shown here in this diagram ends up being our latent space and the latent space automatically captures features that were important for classification. Now that we have our latent structure we can use it to calculate a distribution of the inputs across every latent variable. We can estimate a probability distribution depending on that's based on the features of every item in thisData set. Then we can over sample denser or sparser areas of this dataSet and under sample from denser areas of the data set. data point x will be based on the latent space of X such that it is the inverse of the joint approximated distribution we have a parameter Alpha here which is a divising parameter and as Alpha increases this probability will tend to the uniform distribution and if Alpha decreases we tend to de-bias more strongly. This gives us the final weight of the sample in our data set that we can calculate on the Fly and use it to adaptively resample while training. Once we apply these this debiasing we have pretty remarkable results this is the original graph that shows the accuracy gap between the darker Mills and the lighter Mills. tend to amplify racial biases a paper from a couple years ago showed that black patients need to be significantly sicker than their white counterparts to get the same level of care and that's because of inherent bias in the data set of this model. In all of these examples we can use the above algorithmic bias mitigation method to try and solve these problems and more so we just went through how to mitigate some forms of bias in artificial intelligence and where these Solutions may be applied and we talked about a foundational algorithm that Themis uses that UL will also be developing today. of a regression task the input here x is some real number and we want it to Output f of x which is should be ideally X cubed so right away you might notice that there are some issues in this data set assume the red points in this image are your training samples so the boxed area of this image shows data points in our data set where we have really high noise. If we queried the model for a prediction in this part of in this region of the data set we should not really expect to see an accurate result. through aliatoric uncertainty so the goal of out estimating alliatoric uncertainty is to learn a set of variances that correspond to the input keep in mind that we are not looking at a data distribution and we are as humans are not estimating the variance we're training the model to do this task. The crucial thing to remember here is that this variance is not constant it depends on the value of x. We can determine how accurately the sigma and the Y that we're predicting parametrize the distribution that is our input. a data set called cityscapes and the inputs are RGB images of scenes the labels are pixel wise annotations of this entire image of which label every pixel belongs to and the outputs try to mimic the labels they're also predicted pixel wise masks. Why would we expect that this data set has high natural alliatoric uncertainty and which parts of this dataSet do you think would have aliatoric uncertainty. Even if your pixels are like one row off or one column off that introduces noise into the model the model can still learn in the face of this noise but it does exist and it can't be reduced. Training an ensemble of networks is really compute expensive even if your model is not very large. By introducing some method of Randomness or stochasticity into our networks we're able to estimate epistemic uncertainty. At Themis we're dedicated to developing Innovative methods of estimating epistemic uncertainty that don't rely on things like sampling so that they're more generalizable and they're usable by more Industries and people. Themis view learning as an evidence-based process so if you remember from earlier we were training The Ensemble and calling multiple ensembles on the same input we received. epistemic uncertainty let's go back to our real world example let's say the again the input is the same as before it's a RGB image of some scene in a city and the output is a pixel level mask of what every pixel in this image belongs to. We can use bias and uncertainty to mitigate risk in every part of the AI life cycle. analyzing the data before a model is even trained on any data we can analyze the bias that is present in this data set and tell the creators whether or not they should add more samples.