A set is convex if for any a and b element of omega, the line between them is in omega, OK? So what does that mean? So let's draw the picture first, and we'll draw the math. Here's the convex set. So it means no matter how I pick a-- here's a-- and no matterhow I pick b, the straight line between. them, the geodesic between them, is in a set. OK? Now, we're going to apply this to functions. Jensen: Q is a probability distribution over the states such that the sum over Q(z) equals 1. Jensen: Q can also be written as an expected value, where z is distributed like Q of this weird-looking quantity. No matter how I pick the probability distribution, this chain of reasoning goes through, Jensen says. The key holds for any Q satisfying star, he says, and that's how we ground it into an example. The result is a curve that's always a lower bound everywhere. be theta t plus 1, OK? And we're going to, again, create some new curve, Lt plus 1 of theta, based on that point. And the key aspects of the point that I'll write in a second is this point is a lower bound. This curve is always below the loss, so it's kind of a surrogate that I'm not overestimating my progress, and it's tight. It meets at exactly that point, so I wouldn't think and get fooled that there was a higher loss function somewhere else. T sigma inverse j x(i) mu j. All right, and so just so you're clear what's going on here, the log turns these multiplications into additions. So we want to set this to 0 and use that sigma j inverse. Sigma j is full rank. And that will become clear in a second why that matters so much, because when we pull it out, what do we get? We get here s Sigma j inverse times sum, which is an unfortunate collision. do this when you have something that sums to 1. It's not more complicated than what I wrote here, but make sure independently you go through it and ask questions. In the next class, as I said, what we're going to see is this notion of factor analysis. And that is going to tell us how to apply EM to a different kind of setting, which, at first glance, will look kind of impossible to do without a latent variable model. And I think that's all I want to say. The EM algorithm is an unsupervised version of k-means GMM. In GMM, you guess randomly an assignment of every point to the cluster, the probability. You guess where they're probabilistically linked, that is, what's the probability of these points belong to cluster one, this point belongs to cluster two. And then once you have that, you then solve some estimation problem that looks like a traditional supervised learning. The decomposition is quite important. And we're going to try and kind of abstract that away.