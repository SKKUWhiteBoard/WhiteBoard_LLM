SNLI is the Stanford Natural Language Inference Corpus-- MultiNLI, and Adversarial NLI. The premises are all image captions from the image Flickr30K data set. All the hypotheses were written by crowdworkers. Only 74% of the examples are sentences that is S-rooted in their syntactic parses. The overall Fleiss kappa measured interannotator agreement was 0.7, which is a high rate of agreement. The data set has over 550,000 training examples. crowdworker had to come up with three sentences. One definitely correct -- that's an entailment case. One may be correct-- that is our gloss on neutral. And one definitely incorrect, which is our Gloss on contradiction. So you can see here that there's an attempt to use informal language connecting with informal reasoning, common sense reasoning in the prompt. And then those get translated into our three labels for the task. And here are some examples from the validated set. And I think they're sort of interesting, because you get high rates of agreement, but you do find some examples that have a lot of uncertainty about them. SNLI was a competition between deep learning systems and humans. SNLI is based on premise and hypothesis that probably describe a different photo. MultiNLI is a kind of successor to SNLI. The train premises, in this case, are going to be much more diverse. They're drawn from five genres-- fiction; government reports, and letters and things; the Slate website; the Switchboard corpus; which is people interacting over the phone; and Berlitz travel guides. And then interestingly, they have additional genres just for dev and test. MultiLNI is an interesting early example of being adversarial and enforcing our systems to grapple with new domains and new genres. MulitiNLI is another large data set, slightly smaller than SNLI. They did the same kind of validation, and that gives us our estimates of human performance. And then finally, adversarial NLIs, kind of a response to that dynamic that looks like we're making lots of progress. But we might worry that our systems are benefiting from idiosyncrasies and artifacts in the data sets. in the abstract, but rather with the goal of fooling state-of-the-art models. That's the adversarial part of this project. And this is a direct response to this feeling that results in findings for SNLI and MultiNLI, while impressive, might be overstating the extent to which we've made progress on the underlying task of common sense reasoning. Adversarial NLI is exciting because it's given rise to a whole movement around creating adversarial datasets. And that's represented by this open-source project, Dynabench. SNLI and MultiNLI into Turkish. XNLI is a bunch of assessment data sets that is dev-test splits for more than a dozen languages. Those are human-created translations that could be used to benchmark multilingual NLI systems. So there's a wide world of tasks you can explore, and I think that makes NLI a really exciting space in which to develop original systems, and projects, and so forth. And those could be interesting for seeing how well a model can grapple with variation that comes in very specific and technical domains.