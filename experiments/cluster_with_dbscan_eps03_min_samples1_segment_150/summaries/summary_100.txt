K-means GMM and EM are our first two unsupervised algorithms. We're going to try and generalize what happened there so that we can use it in many different settings and move on from there. The big idea we encountered was this idea that photons that we were trying to fit Gaussians to, maybe three Gaussian that look like that. Don't worry about if you don't remember the details, just roughly what we're dealing with. We've kind of got our hands dirty with k- means and GMM. of a latent variable. And the latent variable in this setting, if you remember, was this fraction of points that come from a source. So we didn't know how many points were coming from each one of those light sources that were out there. We had to estimate that. Once we estimated that, then we would be able to go back and fit all the different parameters that are there. So-- and the fraction of Points, we also had to figure out the linkage, the probability that every source was coming from a point. And then we could do the estimation. that is, what's the probability of these points belong to cluster one, this point belongs to cluster two, so on? And then once you have that, you then solve some estimation problem that looks like a traditional supervised learning. So the decomposition is quite important. And we're going to try and kind of abstract that away. And then we would estimate the other parameters. That's what I mean by a kind of traditional supervised thing. It doesn't mean that that's what we're doing there. Maximal likelihood is just a framework. It happens to be the framework that we use throughout most of the class. There are others in machine learning by the way. But this is the one we're going to use. So before I get started on the rundown, any questions there? I'll start to write. Oh, please. Yeah, for things we start by casting the views, what is the first step in GMM? What do we guess? We could guess randomly an assignment of every point to the cluster, the probability. some other heuristic guess. That was what was going on in the k-means++. We have a smarter initialization. Once the process is started, we just keep running those two loops again and again, and hopefully, it will improve. And we'll capture in what sentence it improves. You'll see this weird picture of a curve that we go up, and that's going to be the loss function. Awesome. OK, so we're going to look at the EM for latent variable algorithms, and this is where it applies. This is what it's for is dealing with various different notions of latent variables. What they're doing, basically, is they have that decoupling property. If we knew this thing that we couldn't have observed, then, all of a sudden, it becomes a really standard statistical estimation problem. That's the real key idea. We can abstract all the algorithmic details into EM same way we did for the exponential family stuff, OK? Now, before we get started, I want to take a technical detour. And so it's really important that we have signposting here because you'll say, why is this guy drawing here? The algorithm will be used in the next step. It will actually, in some sense, be the entire algorithm. I want to make sure that you understand this key result, which is convexity and Jensen's inequality. Those are the things that we're going to think about as we go through it, OK? I'm going to try and show it to you in pictures because I think it's weird. These weird pictures? The technical detail is I want you to understand this in the simplest way. the most intuitive way to understand the basic cases. If you already know it, don't worry. It's just another proof that you'll see. Then this will allow us to go to doing the EM algorithm as MLE. And what I mean is we're going to be able to write down a formal loss function, a likelihood function, right? That's what MLE is. We write down this loss function. Then we maximize the likelihood. Then I'm going to come back, and I'mgoing to put GMM into this framework. And this will answer some of the questions that we kind of intuitively, kind of heuristically answered. so it would give us a principle to solve for all the weights. We'll go through what we call factor analysis, OK? And factor analysis is another model. The reason I want to show it to you is it's different than GMMs, so it occupies a different space, and it will kind of force you to look at the kind of decisions you're making, right? What are you doing? And then we almost certainly will not have time for this today, but I combine the notes, and we'll go continue to go through them on Wednesday. you modeling here? And in particular, we'll model a situation where traditional Gaussians couldn't fit the bill because we're modeling something that's huge and high dimensional. And by comparing these two and what's similar to them, hopefully, you get a pretty good sense of what EM is and all the different places that it runs. All right, OK, so far, so good? So if there are no questions, we're going to go right into our technical detour, which will lead, then, into the EM algorithm as MLE. A set is convex if for any a and b element of omega, the line between them is in omega, OK? So what does that mean? So let's draw the picture first, and we'll draw the math. Here's the convex set. So it means no matter how I pick a-- here's a-- and no matterhow I pick b, the straight line between. them, the geodesic between them, is in a set. OK? Now, we're going to apply this to functions. this, OK? So the shaded region here-- so this function, by the way, is going to be f is equal to x squared. It's a parabola, kind of a bowl-shaped function, right? Now, no matter how I pick the points-- and clearly, I should really only have to worry about picking on the edge. So if I pick a point here, a, and I pick another point b, the line between them goes here, all right? It's not necessarily a straight line across. I'll just say there's a point z that's going to live in the middle, and this is b lives here. Let me erase 0 and 1 because we don't really need them. Their values are kind of unimportant to us. We'll draw a. All right, awesome. Now, what is this function? Well, this is a-- I think it's x minus looks like this. And then its graph is everything up here. And this is not convex for the same reason. said was a function graph, and the other one, you didn't. The function graph was open to the top, but that shouldn't be really disturbing. All right, so what does this mean? Means for all Lambda element of 0, 1, f of a plus 1 minus Lambda f of b is an element of omega. It means that if I take any z that's on the path, lambda a, to 1 minus lambda b, then it had better be the case that z is greater than f of z. make sense? Just translating the definitions directly. In more cryptic language, we usually just tell you every chord is below the function. Here, that's not case. I just found two points so that the chord between them is actually below thefunction. So it's not convex. And intuitively, the reason I drew these shapes is that convexity for shapes probably makes more intuitive sense, 2D shapes. But now, hopefully, you see they're really the same thing. If f is twice differentiable and, for all x, f double prime of x is greater than 0, then f is convex, OK? So this says these functions really are bowl-shaped, right? Second derivative being positive means that they have this kind of positive curvature that looks like the U's. Their first dimension-- first derivative goes up and down, but they're kind of always trending. That first derivative is always getting more positive,right? It's negative on the left-hand side, positive on the right-handside. out a Taylor series for this. f double prime-- see the a, a minus z square. OK? And this a to a is just something in a, b. So I'm saying there's some point on the interval where this is true, OK? Same thing for b. This isn't super important for your conceptual understanding, by the way. Like, this is just to show that you can do what you want to do here, that this makes sense to you. OK, minus z squared, handled it. Now, I claim it's convex. take what is the obvious thing to do. I'm going to multiply this by lambda. I have to make a statement about this, right? That's what's in my definition above, OK? Well, that's just the same as adding f of z for Lambda, plus 1 minus Lambda. So that's good. That appears. That shows that this thing, this inequality, holds, f ofZ. Please. Oh, so you've seen the double [INAUDIBLE],, is that [INAudIBLE]? Yeah, this is Taylor's theorem. Great question. So what's going on here if you remember Taylor's theorem is you can keep expanding, and then you have the last term, which is the remainder term. The remainder term says, there exists some point that lives in a to b such that this holds with equality. By the way, this is really not important for your conceptual understanding. You can freely forget this and just use the fact, this fact, in the course, OK? OK, stalling done. Any more questions? Awesome. Jensen's inequality is a test for convexity. Compute the derivative. You'll see. But it's the one that looks like the two bumps, right? It's a quartix. So that's what it looks like. And if you have a stronger condition, you can get this strong or strict conveXity, OK? All good. Now, if I've done my job well, this mysterious-looking statement, once I show you the connection, you go, oh, OK, that makes sense. something about convexity, but it's got a fancy name, and it's so useful. The expected value of f of x is greater than f of theexpected value of x so long as f is convex, OK? Why the heck would this happen? Let's take one example. Suppose x takes value a with prob of lambdas. Then x takesvalue b with prob 1 minus Lambda. Then what is it saying? It's saying the expected value is equal to f of a plus 1 minus lambda f of b. That's exactly the definition of convex. I pick a curve, one way to define a curve. And that curve is going to be as a result of sweeping some parameters in a high-dimensional weird space. But basically, it says, no matter how I pick the parameters of that curve, anywhere that lives on this thing, that's a probability distribution. This inequality holds. That's going to allow me to build a lower bound for my function, and I'm going to hillclimb using it. We'll see that in just a minute. do something fancier if you want something that's a full probability distribution. This holds even if E is a continuous distribution. The reason you'll always get the inequality the right way is you'll draw the picture of the function and see the chord is always above it. We actually don't want to use a convex analysis. We'll stop at kind of high school calculus. Sound good? All right. Now, everything is defined in the literature traditionally for convex. If you take convexAnalysis, it's the way we define things. convex function here because we're maximizing likelihood. And this is just notational pain, right? Like, if we were-- maybe we should have minimized unlikelihood. So we need concave functions. And what are concave function? g is concave if and only f minus g is convex. So if I take a chord of this function-- that's a chord-- it's below. Which is what I should hope. If I flipped it upside down, the chord would be above. Cool. conceptualize it is we solve for some hidden parameter. We solve, and that gives us an entire family of possible solutions. Let me draw the picture after I give you the formal set, OK? Oops. All right, so EM algorithm has max likelihood-- I'll actually put MLE. There's some theta that lives out there. We have some data, i from 1 to n. These are our data points. We take a log of the probability that we assign to the data given our parameters. structure. P(x; theta)-- this is a generic term, right? This is just one of the i terms-- says the function factors this way. Looks like a sum over z, where z is our hidden or latent variable. So we have to sum or marginalize over all the possible choices of z. This is basically saying, I don't know what z is. I have some probability distribution that I can compute over my data and z given theta. And this will get me back a probability for x. sorry, probability for x. Is that clear? Yeah, ask a question, please. So where is the z going to go again? Like, is that property of the parameters [INAUDIBLE]?? Yeah, wonderful question. In a real sense, when we make a modeling decision, and we say, there exists some structure out there, like there exists a probabilistic assignment between photons and point sources. One version of the prior is, I tell you exactly where every photon comes from. That's clearly a very strong prior. models. In GMM, this was exactly the z there. The notation isn't an accident. It's the same z. So that's one. We'll have more examples later. But I want to get through the algorithm in this abstract form, and we can shoehorn more things into it. And what I'll do afterwards is put GMM right down in this language. We need a couple more things. Please. Back to Mail Online home.back to the page you came from. what is probability of x parameterized by theta actually represent in this case, in that photon example? Yeah, exactly. So remember, if you-- I think it was said yesterday by someone here on that side of the room. So I don't know if that's spatial recognition helps you in the last lecture. But it was like, imagine I was guessing all the photon models that were out there. And then what I'm thinking about is what I want over that is that, across all those thetas, no matter how I instantiate z, each one gives me a different probability distribution. I can sum them up, and that tells me. that from the supervised days. We just inserted z and said, well, there's this wild z that we can't observe, but it somehow constrains x. It means that x-- like, the relationship between theta and x. And that's what the model does. Awesome question. Very cool. These are wonderful questions. I'd much rather answer these than badly draw the pictures that come next. We're going to get to those pictures no matter what, so there's really no saving us. All right, let's get to the bad pictures. have-- and I apologize. I will use a bunch of colors. I hope this is OK for people to see. If not, let me know. This is my loss function, l(theta), OK? So this is-- I'll write that in black there. This Is l( theta) here. Now, remember, it's not a nice concave or convex function, right? We wouldn't expect it to be. We would hope, because we're going to minimize it, that it's concave. weird bends. So how does the algorithm work? We start with an initial guess. Then what happens is this is mapped up to here, which is l of theta t. This is just the value of the loss that I currently have. I suspect there's something up this way I'd like to get. That's all we're after. We settled for that in KMM, for k means, and we're going to settle forThat in GM, OK? So we had to settle-- that's another way of copping out and saying, we hadto settle for these local iterative solutions. Jensen: The problem of optimizing over all those z's seems daunting, directly optimizing the l's. Instead, I'm going to come up with a local curve, OK, and I'll call this curve Lt of theta. It's another function. We're going to try and get that kind of easy-to-optimize function. And then, [MOUTH POP] we then do it again and create another curve. The whole algorithm is going to be Jensen's, and that's the whole algorithm. be theta t plus 1, OK? And we're going to, again, create some new curve, Lt plus 1 of theta, based on that point. And the key aspects of the point that I'll write in a second is this point is a lower bound. This curve is always below the loss, so it's kind of a surrogate that I'm not overestimating my progress, and it's tight. It meets at exactly that point, so I wouldn't think and get fooled that there was a higher loss function somewhere else. about the algorithm, but hopefully, it's clear what's going on. Easy-to-train surrogate, and we kind of slowly hillclimb with that easy- To-Train surrogate, alternating back and forth. And this is what we were doing in K means. And just so it's super clear here, phi t plus 1-- this is nothing more than the argmax over theta of Lt of theta-- means I do the optimization on the surrogate curve that I created. curve, L of t. And then the M-step, and together, EM, set phi t plus 1 equal to argmax phi Lt(phi). Cool. Just could you reiterate? Like, why are we not using gradients on the original turbulence? Right, so we could imagine doing some kind of gradient descent here, but it's not clear how to deal with this marginalization that happens in the middle. So if we did some marginalization or some sampling, we could do something that looked like that. But it's because we have this decomposition. just means I have an internal solver that's fast and I kind of trust, and I have something on the outside that's a latent variable that I'm like splitting up the modeling. It's one of a number of decomposition strategies. Doesn't mean it's the only way to solve it, though. Cool. All right, so the question is, how do we construct L of t? And I claim we know everything else. So we'll come back to that claim in a second. be thinking because I told you that Jensen's will have something to do with this. Now, what we're going to do to put it in the form where Jensen's could be used looks wholly unmotivated. But it's to shoehorn into what we's doing, and there's some motivation, but it's kind of opaque, let's say. So here, I'm just introducing Q. This is true for any Q, right? Let's not worry about support issues. Let's just put in something that divides by 1-- seems sort of unmotivation to do this. Jensen: Q is a probability distribution over the states such that the sum over Q(z) equals 1. Jensen: Q can also be written as an expected value, where z is distributed like Q of this weird-looking quantity. No matter how I pick the probability distribution, this chain of reasoning goes through, Jensen says. The key holds for any Q satisfying star, he says, and that's how we ground it into an example. The result is a curve that's always a lower bound everywhere. because, term by term, it's going to be less than or equal to. Now, it doesn't satisfy all our requirements, because we have to make it tight. So I have to pick a certain Q to make this operational. That's the piece. Go ahead. Would then start and then Q has to be greater than 0? Yeah, yeah. So, yeah, I said I was going to ignore the support. But right now, I have a way of going term byterm from the likelihood function and getting lower bounds. And it'll be a lower bound no matter where I am, OK? We can imagine just for the sake of this lecture that it's strictly greater than 0, so I don't run into weird things about what I mean by divide by 0. Here, because I'm controlling the multiplication ahead-- ahead of time, it does make sense, but you're right to point that out. All right, so now how do we make it tight? So what we have to do is that we want to make Jensen's inequality tight. And the idea is if what's inside is constant-- imagine there was a constant inside, that this term was constant for all the different values of z-- then the expectation clearly doesn't matter, right? value, alpha, and then you would get a sum over all the alphas that were there. So as long as this term is a constant-- that is, it doesn't depend on z-- I'm in business, all right? So what that means is I want to pick Q such that log P of x, z; theta over Q(z) equals C. Now, before, I had all kinds of freedoms to pick whatever Q I wanted. Now this is where the probability comes in. a constant independent of the-- just for some constant c. We don't care what its value is. We just care that it doesn't depend on z in any way, and then it will be exact equality. Then Jensen's will be equality, OK? All right, so what is the natural choice? Well it's that Q(z) should equal P of z given x; theta. Why is that? Well, this is also equal to-- so this is because P of x of z of theta equals p of z x. So if I plug these in, they cancel out. and x. So we're going to have this notation, Q(i) of z because it depends on each different value. So each data point is going to get its own different Q, which is the log of how likely this thing is, OK? And we picked those for each i. So the ELBO of x, Q, z equals x,Q, z. OK? This thing has a very famous name, so I'll write that while I kind of stall for more questions. What we've defined here is called the Evidence-based Lower BOund. the sum over z Q(z) log P(x, z; theta) over Q(Z), OK? And what we've shown is that l(theta) is less than or equal to-- or is greater than orequal to the sum, because we did this term by term, of the ELBO of x of i, Q of i. Sorry, the z is marginalized away. The z can't appear there. Only thing that appears there, OK-- for any Q(i) satisfying star. both on the original loss. These are just saying, this is the Lt here, capital L. Each one of these is capital L, basically, right? And then this one here is saying that, at that particular point for that t-th instantiation,. this is where we are. Yeah. All right, so the wrap-up is as follows-- this is how-- we can now write down the algorithm and the kind of full generality with mathematical precision, although it may still be a little bit opaque. So this says that you're going to pick the Q(i) distribution that says, what's the probability that's most informed. have some theta at some time. You know the data point that you're looking at. And you say, what are the most likely values of the cluster linkage-- as we were talking about before, the source linkage-- for this particular point? You get a probability distribution over those. You set them to Q(i)z. That's really what's going on. It's your estimate of how likely that is. Theta t plus 1 equals argmax over theta of Lt( theta), which equals Lt(theta) That gives me a new guess of parameters, which defines-- you get a new curve, a Q(i) for each one of what's going on. And I'm inconsistent with the semicolons too. So you move this. So there's a good visual distance. This is an x, this is a Q, and theta is our current guess. All right, why does this terminate? And it's basically for something that's kind of not very interesting or satisfying, but it does. sequence that is monotonically increasing or nondecreasing, OK? So it's possible that it would grind to a halt. But eventually, it has to be strict. And so to derive a counterexample, you would just find a likelihood function that had those two bumps. And you would run it in that particular lower bound setting. And what it will do is it will gradually hillclimb. And this is actually not great. Like, it can't go back downhill, right? It's got to just continue to go up. So in summary, what we saw here is we derived EM as MLE as promised, OK? So just to recap what happened here, we started with this notion around Jensen's and convexity. We wanted to use concave functions, which are these kind of downward-facing things. Those are the loss functions because we wanted to maximize them. The reason that was important is we had to do this back-and-forth iteration. Given a set of parameters, we were going to find a surrogate. That surrogate was going to be concave. entire curve because we wanted to optimize it. So it wasn't enough to find a point in a lower bound. We needed to find the whole thing that was underneath it so we could run our argmax step. And that was the setting where we would learn all of the parameters and estimate that in a way that was hopefully nice and easy to do. It's a lot of notation because we're abstracting out a huge number of things that we're doing. We'll run through an example of that. But in the end, it's not so bad, right? You take the Q(i)'s. And this way, set the thetas, do a descent on them, or ascent in this case. OK. All right, so let's see it for our Gaussian mixture model. Please. For this, [INAUDIBLE] this termination condition event. Oh, so the termination condition is not really important, or in the classical sense. The thing is that it's nondecreasing so that, eventually, there's a convergent subsequence of it. see if the loss of the likelihood function is not changing too much. Depends on your data, depends on the problem. Sometimes if you have only a small amount of data, you want to get to machine precision and 10 to the minus 16. And so that's the way you decide when to do it. This just says that it's not going to oscillate wildly. It's a very weak statement I'm making. Yeah, please. Can you explain what specific part of this is linked to the MLE sort of aspect of it? model, where we were saying, the way we're going to think about the world was to maximize the likelihood. That's less disturbing to this group than it is to, I guess, generally worldwide who think about this, because this is the only framework we've used in the course. We started with l(theta) as what we were optimizing, and then we derived this as a set of concerns. We didn't get to a global optimum. So I don't mean that we definitely guaranteed that we got the maximum likelihood estimation, just that you can phrase what's going on as MLE. sense? Yeah, yeah. Awesome. Thank you for the question. Why is this tight? Which one? Oh, it's tight because we went through this small piece here, which was that if we selected it as a constant in this particular way-- so before we could pick any Q and it was a lower bound, as long as we did this, then actually this line was no longer an inequality but was actually exact equality. And it depended, though-- that selection of Q depends on theta and x. Cool. EM for mixtures of Gaussians, or we call them GMMs, sorry. All right, so what's the E-step? Huh? Yeah, I'm just going to copy down the thing. So let's get the generic algorithms. Let me get thegeneric algorithm. Allright, just so we have it on the screen. So here's our warm-up-- not really awarm-up because we're almost out of time, but here's-- remember, if we saw how this worked-- P x(i) and z(i). size or variance, mu j. All right, z(i) is our latent variable. What does EM actually do here? So what is EM? EM is very general. You can instantiate it, right? SoWhat does it mean here? Now, what actually happened here when we wanted to understand-- what was the probability? This says, the probability that i, the i-th component, belongs in j given what we've observed about x( i) and what we know about the cluster shapes and their frequencies. The last time that said we had these two bumps, which were our two Gaussians, let's say, in one dimensions that looked like this. This was mu 2 sigma 2 square. And the question is, you give me a point here. How likely is it to belong to 1 or 2, to cluster1 or 2? Right? That's basically what we're asking. What's the probability that at this point, this i-th point here, comes from 1 or2? phi 2 was hugely bigger than phi 1, right-- a billion points came from the second source. So to automate this, this is Bayes' rule. It just weighs those two probabilities and tells us what should happen. That's it. We ran through exactly those calculations last time. All right, let's take a look at the M-step now. We have to compute derivatives. I want to highlight only one thing here because it's something that causes people pain when they do their homeworks. So we're maximizing here over all the parameters, phi and mu and sigma, sigmas, sorry, all the covariants. This whole thing, we're going to call fi. This is fi of theta. It hides a ton in our notation, all right? So this thing is-- let's write it out because the gory details will help us. Oh, please. You have a question. Do you mind defining what theta is? Do you want to define it? is latent and what is not? Yeah, so in our terminology, z is just latent. So I'm giving you the intuition that it's something that's hidden or not observed. But formally, it's just going to be anything that's a z. z is latent. That's our definition. Please. So the fi is just like the ELBO [INAUDIBLE].. Exactly right. Yeah, this is exactly the instantiation of what we had above. We reasoned about this through ad-hoc reasons last time, but it is exactlyThe ELBO that we're now going to minimize with derivatives. Now, that's how this whole method works, just abstracted three orders of magnitude more than it should be, OK? So let's see that piece. The z(i) that said we're summing over? Yeah. That's going to be-- so I'm just using that notation to make sure it's clear that it depends on the i. It's actually just a z that you're suming over. And it's summing. over all of the different clusters that are possible there, all the different sources. so on? You could also-- we'll see later-- replace it with an integral if you had something really fancy that was there. P z(i) j-- to compute that, remember, we expanded it by Bayes' rule. We had, if you knew you were in a cluster, how likely is the data point? And then we had a term that said, how Likely is the cluster? And those were the two components that we used. So I glossed over this really, really quickly because it was the same calculation we did last time. functions that we put in and broke down by Bayes' rule. It's exactly the same. You've got it perfectly. All right, so let me write out this monstrosity just because it will be potentially-- it has been in the past educational. Who knows if it's educational in the future, and the future being, like, two seconds from now? Allright, I'm going to use a notation, and hopefully it doesn't confuse you-- Q(i) equals z j. So this is the piece there. equal to the sum over j-- because now I'm summing over the cluster centers, right? The z(i) notation was still very abstract-- wj(i), which was summing. over this part here, log-- and help us all, 1 over 2 pi-- this is a covariance, 1/2. This is the exp of Oh, I decided to write this in four general things. Why do I care about that? Oh,I see why. OK. Transpose sigma inverse x( i) mu j times phi j. This is a Gaussian distribution with center j. I did use a higher dimensional covariance because it's something you're going to have to compute. The notation doesn't change except for this is what the Gaussian looks like instead of a square. And then there's the phi j, which is just multiplied times this horrible expression. And this exp parentheses is so I don't have to write it in superscript, right? Just expo the function, just a bad habit that I have. always use brackets for this. It's historical, and I would love to beat it out of myself if it were possible. Does the covariance depend on j? Right now, the covarianance does not depend on z. So it depends on j. OK? All right, so now we can compute some fun derivatives, OK? So let's compute mu j of fi of theta. We have to estimate some fun derivative of mu j. That's it. Yeah, good catch. T sigma inverse j x(i) mu j. All right, and so just so you're clear what's going on here, the log turns these multiplications into additions. So we want to set this to 0 and use that sigma j inverse. Sigma j is full rank. And that will become clear in a second why that matters so much, because when we pull it out, what do we get? We get here s Sigma j inverse times sum, which is an unfortunate collision. doesn't depend on mu either, so I'm left with these terms. Please, go ahead. Oh, what is the physical meaning of the fi here? fi? fi is just the term here in a a function. It is the likelihood function after we've picked Q at the particular iteration. So it's just notation so I don't have to write this monstrosity every time. Yeah, but here, it's-- It's the ELBO. OK. It's exactly theELBO. and we computed this before, and it's just a matter of computing the derivatives. The one that I actually care about showing you, by the way, is phi j, so let me just jump to that because we only have a minute or two left. And I want to show you what happens in phiJ. So phi J is constrained. Would you mind showing [INAUDIBLE] scrolling up? Sure. No, wait. I just want the last one. OK, sure. show this one thing, phi j is constrained. You need a Lagrangian, OK? No, you haven't seen it? That's fine too. If you want, I'll post notes about how to compute Lagrangians as well. I don't actually know when anyone learns anything. Anytime I say something like that, my students always get upset with me, so I should just stop. But I assume you've seen it before this moment, how about that? so if you just take this and compute the derivative, it doesn't account for the constraint. So you have a bunch of numbers that must sum to 1. But what if the gradient is perpendicular to the line? Like, its wants to push you only perpendicular and has no component moving you along the line, right? In that case, this is still a critical point. It's still potentially a minimum. Does that make sense? Because it's not telling you that there's a minimum to your left and right. It's along the line, OK? So the question is, how do you encode that information that you want to kind of screen off information that's orthogonal to the line? And I'll write up a little note to show this whole thing. What you do is you introduce this thing called Lagrange multipliers. And if you haven't seen them, don't worry. These are super easy to teach. Just say this-- it's just an extra term here. And this multiplier is basically the thing that screening off things that are orthogonally to these constraints. that's OK. You get to screen that off. And I'll just post a one-page write-up for you. Please remind me in the thread, and I will definitely do that. If you don't do that, you'll get the wrong answer. That's also a motivation to learn it. And so what ends up happening here is you get something that says, I get sum i goes from 1 to n w(i)j over phi j plus lambda equals 0. And this implies that phi of j is equal to 1 over Lambda sum i equals 1. When you have a constrained probability distribution, you have to use a Lagrange multiplier. In this case, it makes total sense, though, because these numbers have to sum to 1. So if you don't have a normalization constant here, you're adding up a bunch of numbers which individually sum up to n, right? The sum over all of them is n. And this is just the principle that tells you, you must normalize them by this n factor, OK? So all I care that you take away, if you've seen this a thousand times, is that you understand. before, don't worry. If you've never seen this before, I just want to flag for you, when you minimize a function that's constrained to make sure you use Lagrange multipliers. You do not need to spend a bunch of time on them. Just have a little light bulb to go off that says, OK, I've got to look up how to do it in this case. That's all I care about, OK? And you'll trace through it in the notes. Please. So this minus lambda is going to be equal to 1/n. So I'm just going to write the final expression. Maybe that would be less-- yeah. So also, why is it [INAUDIBLE]? Because it's a probability distribution. So again, the issue here is phi j is constrained by the model. So if we go back to this model, this is a constraint onphi j. So whenever you have a probability. distribution, a multinomial probability distribution, it's not just that the phi i's. are nonnegative, which the constraint-- we're almost ignoring-- but it's that the phi i's equal 1. So you couldn't, for example, set your probabilities to be 0.5 and 0.8, right? They have to add up to 1 here because it's a multinomial. So that means these phi j's-- we have constrained them to equal to 1. And it shows up in this extra term here, which is the Lagrange multiplier. haven't seen this before, it'll look quite mysterious. But what I was trying to do is I'm not going to teach you the Lagrange multipliers in this class. I'll put up something. But the piece is here that it gets you back to an expression which makes sense in this setting. And you needed something to average over because these numbers sum up to something that looks like n. If you just compute it naively, you'll get something that doesn't make any sense. the same j's. No, but [INAUDIBLE]. Which line? Just says phi j i. Oh, oh,Oh, I see. I see, I See, Isee. Sorry, sorry, sorry. Thank you for the clarification. Apologies for that. Yes, it's this constraint here. Sorry,. this is the constraint that was in our head. Yeah, and it just makes a mysterious reappearance here, all right? All right, awesome. OK, so what is the message that I want you to take away from this? in a different way. We started with that convexity piece so we could get an intuition for what these functions look like. And then we went through the EM algorithm, which we formalized as kind of back and forth with using these curves over time. Once we had those curves, what was happening was we would pick and optimize on those curves. The Q(i)'s played a starring role. Those became our w's here, and they kind of add nastiness to all of the equations. They just add little weights and expectations everywhere. MLE for the entire quarter on those properties. Then we introduced a ton of typos to keep you on your toes. And then the second thing that I would tell you to do is when you have constraints, you have to know how to optimize them. You don't need to know the general theory of how you optimize against nonlinear constraints, but you should review how to do it. And so then we saw the two things that I cared about to highlight. One is how to find means, and these are just weighted means. do this when you have something that sums to 1. It's not more complicated than what I wrote here, but make sure independently you go through it and ask questions. In the next class, as I said, what we're going to see is this notion of factor analysis. And that is going to tell us how to apply EM to a different kind of setting, which, at first glance, will look kind of impossible to do without a latent variable model. And I think that's all I want to say.