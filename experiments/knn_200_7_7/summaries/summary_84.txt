Part 3 in our series on distributed word representations. We're going to be talking about vector comparison methods. To try to make this discussion pretty intuitive, I'm going to ground things in this running example. On the left, I have a very small vector space model. We have three words, A, B, and C. And you can see graphically that B and C are pretty close together. And A is kind of lonely down here in the corner, the infrequent one. Euclidean distance is capturing the first perspective that we took on the vector space, which unites the frequent items B and C as against the infrequent one A. As a stepping stone toward cosine distance, which will behave quite differently, let's talk about length normalization. Given the vector u of dimension n, the L2 length of u is the sum of the squared values in that matrix. And then the actual normalization of that original vector u involves taking each one of its elements and dividing it by that fixed quantity. in your reading or research. The first class are what I called matching based methods. They're all kind of based on this matching coefficient. And then Jaccard, Dice, and Overlap are terms that you might see in the literature. These are often defined only for binary vectors. Here, I've given their generalizations to the real valued vectors that we're talking about. And the other class of methods that you may see come up are probabilistic methods which tend to be grounded in this notion of KL divergence. to correct this. We're going to start to massage and stretch and bend our vector space models. And we will see much better results for these neighbor functions and everything else as we go through that material. It will be a big step forward for us in terms of our understanding of how the universe works, and how it can be improved for the benefit of the human body. It's going to be a very exciting time for us, and I'm looking forward to it. I think we'll see a lot more progress in the coming years.