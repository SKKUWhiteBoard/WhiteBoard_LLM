So, last time we were starting to talk about the sort of the general overview of what reinforcement learning involves. We introduced the notion [NOISE] of a model, a value, and a policy. Can anybody remember off the top of their head what a model was in the context of reinforcement learning? So what we're gonna do today is, sort of, um, build up for Markov Processes, up to Markov Decision Processes. And this build, I think, is sort of a nice one because it allows one to think about what happens in the cases where you might not have control over the world. different actions or we could just think of those actions as being a_1 or a_2, where it's trying to act in the world. So in this case, the transition dynamics looks like this, which says that, for example, the way you could read this, is you could say, well, the probability that I start in a particular state s_1, um, and then, I can transition to the next state on the next time step is 0.4. So, let's say that your initial starting state is S four, and then you could say, well, I can write that as a one-hot vector. I multiply it by my probability. And that gives me some probability distribution over the next states that I might be in and the world will sample one of those. So, for example, if we were looking at state s_1, it has a 0.6 chance to abstain and 0.4 chance of transitioning. So in this case, we have similar dynamics from s_4. state s_3. Probability of 0.4 going to state s_4 or a probability of0.2 of staying in the same place. So, it's like the world you know what the dynamics, the dynamics is of the world and then nature is gonna pick one of those outcomes. And so now what is a Markov reward process? Again, we don't have actions yet just like before. But now we also have a reward function. And we're going to be interested in episodes because later we're gonna be thinking about rewards over those episodes. rewards for the Markov Decision Process can either be a function of the state, the state in action, or state action next state. Right now we're still in Markov Reward Processes so there's no action. So, in this case, the ways you could define rewards would either be over the immediate state or state and nextState. Once we start to think about there being rewards, we can then think about returns and expected returns. If the process is deterministic, these two things will be identical. But in general if the process are stochastic, they will be different. General case, we are gonna be interested in these stochastic decision processes which means averages will be different than particularly runs. So, for an example of that well, let me first just talk about discount factor and then I'll give an example. Discount factors are a little bit tricky. They're both sort of somewhat motivated and somewhat used for mathematical convenience. We'll see later one of the benefits of mathematic, uh, benefits of discount factors mathematically is that we can be sure that the value function sort of expected discounted sum of returns is bounded. The value function of a Markov Reward Process is simply the immediate value of a reward from the current state. So, in this case you know what might happen in this scenario we start off in s_4. And then on time-step s_7 we get a reward of 10. But that has to be weighed down by the discount factor which here is 1/2. And so the sample return for this particular episode is just 1.25. And of course we could define this for any particular, um, episode and these episodes generally might go through different states. So, if we're in a finite state MRP we can express this using matrix notation. So, we can say that the value function which is a vector is equal to the reward plus gamma times the transition model times V. If one of the transitions can be back to itself, um wouldn't it be become a circular to try to express V(s) in terms of V(S)? So, that's the analytic way for computing this. The other way is to use dynamic programming. Markov Decision Processes are the same as the Markov Reward Process except for now we have actions. So, the agent is in a state they take an action, they get immediate reward, and then they transition to the next state. Now, if we think about our Mars Rover MDP. You can think about these things as the agent trying to move left or right but it's also perhaps easier just to think about in general them as sort of these deterministic actions for this particular example. And the reward can either be a function of the immediate state, the state and action to the state action and next state for most of the rest of today we'll be using that. before we do this let's think about how many policies there might be. So there are seven discrete states. In this case it's the locations that the robot. There are two actions. I won't call them left and right, I'm just going to call them a_1 and a_2. Then the question is how many deterministic policies are there and is the optimal policy for MDP always unique? So kind of right we just take like one minute or say one or two minutes feel free to talk to a neighbor. that quantity for each state. But the strange thing is that we're not gonna follow the old policy from then onwards. We are going to follow this new policy for all time. So, it should be at least a little unclear that this is a good thing to do [LAUGHTER]. Should be like, okay, so you're saying that if I were to take this one different action and then follow my old policy, then I know that my value would be better than before. But what you really want is that this new Policy is just better overall. strict inequality if the old policy was suboptimal. So, why does this work? So, it works for the following reasons. Let's go ahead and just like walk through the proof briefly. Okay. This is- what we've said here is that, um, V^pi_i(s), that's our old value of our policy. Has to be less than or equal to max a of Q#pi. Is equal to R(s, pi_i+1(s) The next questions that might come up is so we know we're gonna get this monotonic improvement, um, so the questions would be if the policy doesn't change, can it ever change again? And is there a maximum number of iterations of policy iteration? So, what do I mean by iterations? Here iterations is i.pi_i+1 value is by definition at least as good as the previous value function. So, why don't we take like a minute and just think about this maybe talk to somebody around you that you haven't met before and just see what they think. In policy iteration, the idea is you always have a policy, that is- that you know the value of it for the infinite horizon. Value iteration is an alternative approach. It says you always know what the optimal value in policy is, but only if you're gonna get to act for say k time steps. So how does value iteration work? The algorithm can be summarized as follows. You start off, you caninitialize your value function to zero for all states. And then you loop until you converge, um, or if you's doing a finite horizon, which we might not have time to get to today. on sort of the contraction operator. So, if an operator is a contraction it means that if you apply it to two different things, you can think of these as value functions. The distance between them shrinks after, um, or at least is no bigger after you apply the operator compared to their distance before. So how do we prove this? Um, we prove it- for interest of time I'll show you the proof. Again, I'm happy to go through it,. um, I- or we can go throughIt in office hours et cetera.