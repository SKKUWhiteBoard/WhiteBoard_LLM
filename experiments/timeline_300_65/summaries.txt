==================== [1/100] ====================
Summary:
CASEY RODRIGUEZ: We proved these two theorems last time, and we used them for-- and we had a couple of applications of them. So the first theorem, simple theorem, was that a sequence converges to x if and only if the limit as n goes to infinity of the absolute value of xn minus x goes to 0. And then we also had the squeeze theorem, that if you have three sequences-- a sub n, b sub n,. and x sub n. to n of n choose k, x to the n minus k y to the k. And here n chooses k-- this is equal to n factorial over k factorial divided by n minus-- times n minusk factorial. So using that, we can then define how to take a real number to a rational power-- although there needs to be something that's checked to make sure this is well-defined, because you can always write a rational number not uniquely as one integer over another. positive real number to a positive real number or to a real number of power doesn't require the introduction of the exponential or logarithm. We're just going to use the basic properties of exponents throughout all this, so we don't-- we haven't even talked about continuity, or derivatives, or anything like that. So for the first one, we'll prove this actually just using the definition of the limit, which, remember, means for every epsilon positive there, we should be able to find a capital number. the binomial theorem. Let xn equal n to the 1/n minus 1, which we note is bigger than or equal to 0 for all n. So my goal I want to show is limit as x as n goes to infinity of x sub n equals 0, because then that proves that this converges to 0. And since this is equal to its absolute value, that means that n to  1 over 1 equals 1. And we'll use that inequality to get a little bit of a different inequality that we'll used for number 3. they're always non-negative as well. This sum is always bigger than or equal to 1 term from the sum. So it's bigger than  equal to k equals 2 [INAUDIBLE] y2-- you'll see-- x sub n squared. Now, the limit as n goes to infinity of 2 over n minus 1 is 0. Square root of that also converges to 0. That's a fact we did from the end of last time. So this whole thing converged to 0, by the squeeze theorem. And that completes the proof. Convergent sequences are bounded, so-- and they have convergent subsequences. Limsup and liminf are also two important objects that arise in analysis. If they exist, there are going to be certain limits, so it's not clear that they exist at all to begin with. But we'll show that they always do exist. We define limsup x sub n, and sometimes I'll write n goes to infinity underneath. And the liminf is similar, except it's now with infs. The smallest thing in a set is a lower bound for this set, so if you like the Archimedean property, you can always find something from the set less than that. So let's look at two examples of what's a little bit and let me just make a few remarks. So we're looking at now the set 1 over k, where n is-- where k is bigger than or equal to n. So the limsup of minus 1 to the n is 1. The liminf equals 0. The supremum is equal to 1/n. So as n goes to infinity, the limit of the supremum here is 0. To get your hands on something, it doesn't require you to show something as strong as showing there is a sequence converging to that. It says that what you really need to do, and which is much more straightforward, or simpler, or impossible really, is to show that that sequence of inputs that you put into your machine to get the outputs is a bounded sequence. So that's enough of that bit of rambling about why this theorem is so useful. It's also useful in the study of PDEs, which is what I study. than or equal to 1 simply by how this is defined as a supremum, and by the exercise from assignment 3. a sub 1 minus 1 is not an upper bound for the set a sub k's. Therefore, I should be able to find an element from this set strictly bigger than that. Since a sub 2 plus 1 equals the supremum of the x of k, there exists an n sub 3 bigger than n sub 2, such that a sub n sub 1 plus 1, minus now a third, is less than x sub nSub 3.

ROUGE-1: 18.48, ROUGE-2: 17.95, ROUGE-L: 17.64
BERTScore: 74.56

==============================================
==================== [2/100] ====================
Summary:
The configuration of a particle is given by, or described by, a wave function psi of x. In 3D, the wave function would be a function of all three positions x, y and z. If there are two possible configurations the system can be in, it's possible to find the system in a superposition of those two psi is equal to some arbitrary linear combination alpha psi 1 plus beta psi 2 of X. The superposition principle is an important challenge for us in the next couple of lectures. Professor: Any reasonable function psi of x can be expressed as a superposition of more equal wave functions, or more precisely easily interpretable wave functions. Fourier theorem said look, take any wave function-- take any function, but I'm going to interpret in the language of quantum mechanics. Any function, we're saying, can be express as a. superposition by summing over all possible values of k, all possible different. wavelengths. Professor: There's no such thing as an infinite number of ways to express a function. Any given vector can be expressed as a superposition of some pair of basis vectors. There's no God given basis for the universe. We look out in the universe in the Hubble deep field, and you don't see an arrow going x. So there's no natural choice of basis, but it's sometimes convenient to pick a basis. And similarly, this is an expansion of a function as a sum. And you could have done this in any good space of functions. We'll talk about that more. Professor: Which particle has a larger momentum? Think about it, but don't say it out loud. If it has higher momentum, what do you just intuitively expect to know about its energy? It's probably higher. OK next one. Compared to the wave function psi of x, it's Fourier transform, psi tilde of x contains more information, or less, or the same, or something. So let's vote. A, more information. B, less information. C, same. OK, good you got it. of length l, 0 outside of that region. f has a single well defined wavelengths. Or f is made up of a wide range of wavelengths? Think it to yourself. OK, now before we get talking about it, I want everyone to close their eyes. Or close the eyes of the person next to you. That's fine. And now and I want you to vote. A is f has one well defined wavelength. B isf has a wideRange of wavelengths. It's closer to true. A sine wave is continuous, and it's continuous everywhere, right? It's also differentiable everywhere, because it's a cosine. So how would you ever reproduce a thing with a discontinuity using sines and cosines? Well, you'd need some infinite sum of sine and cosine where there's some technicality about the infinite limit being singular, because you can't do it a finite number of sines or cosines. That function is continuous but its derivative is discontinuous. So it's going to take an infinite number to reproduce that little kink at the edge. question. The question here is look, there's two different things you can be talking about. One is arbitrarily large and arbitrarily short wavelengths, so an arbitrary range of wavelengths. And the other is an infinite number. But an Infinite number is silly, because there's a continuous variable here k. You got an infinitenumber of wavelengths between one and 1.2, right? It's continuous. So which one do you mean? So let's go back to this connection that we got a minute ago from short distance and high momentum. features, I'm going to talk about it as something with large momentum. And that's because in a quantum mechanical system, something with short wavelength is something that carries big momentum. In order to do that, we need to define uncertainty. So that's the optional problem 8 on problem set 2. Other questions? PROFESSOR: Cool, so that's it for the clicker questions. Sorry for the technology fail. So I'm just going to turn this off in disgust. That's really irritating. quantum physicist: Average age is the sum over all possible ages of the number of people with that age times the age divided by the total number. Average need not be an observable value, professor says. Average value of the square of ages is, well, I'm going to do exactly the same thing. It's just a squared, right? 14 squared, 15 squared, 16 square, 16 squares, 16 squared. The expected value of a is equal to the sum of a times the probability of measuring that value. in this fashion or this fashion. And the notation for this is delta a squared. Different probability distributions are going to give me different delta a's. So one thing that's sort of annoying is that when you write delta a, there's nothing in the notation that says which distribution you were talking about. So really it should be parentheses [INAUDIBLE]. PROFESSOR: Yeah, it's just this is notation that's used typically, so I didn't put the parentheses around to alert you to the stupidities of this notation. couple of ways to improve this notation. One way is to write the expected value of x in the state psi, so you write psi as a subscript. Another notation that will come back-- you'll see why this is a useful notation later in the semester-- is this notation, psi. And we will give meaning to this notation later, but I just want to alert you that it's used throughout books, and it means the same thing as what we're talking about given a particular state psi. This is a thing with dimensions of what? Length, right? But over on the right hand side, we have a length and a probability, which is a number, and then another length. So why are we getting something with. dimensions of length, not something with Dimensions of length squared? And the answer is this is not a probability. It is a probability density. So it's got units of probability per unit length. And so what we're doing is we're summing over all such domains the probability times the value. The expectation value of x in the state psi, or psi x psi, can be pronounced one of two ways. It's the same idea, but they have different flavors, right? So whatever your native language is, you've got some analog of this. This means something in a particular mathematical language for talking about quantum mechanics. And this has a different flavor. It carries different implications, and we'll see what that is later. Yeah? Why is there a double notation of psi? Roughly speaking, it's because in computing this expectation value, there's a psi squared. have chosen some other definition of uncertainly. So why this one? And one answer is, indeed, the uncertainty relation works out quite nicely. But then I think important to say here is that there are many ways you could construct quantities. This is a convenient one, and we will discover that it has nice properties that we like. There is no God given reason why this had to be the right thing. I can say more, but I don't want to take the time to do it, so ask in office hours. Professor: It doesn't make sense to say that our particle has a definite position x and correspondingly a definite momentum p. Being in a state with definite position means being in a superposition of states with arbitrary momentum and vice versa. So what we want is we want some good definition of p given that we're working with a wave function which is a function of x. We have a couple of hints. Hint the first is that a wave-- we know that given a wave with wave number k, which is equal 2pi over lambda, is associated to a particle. a wave, a plane wave e to the iks, how do I get h bar k out of it? Note the following, the derivative with respect to x. Multiply by h bar, divide by i, derivative withrespect to x e to ikx. That's suggestive. And I can write this as pe to the ikX. So let's quickly check the units. What are the units of h bar? Momentum times length divided by length number momentum. question slightly differently. We've followed the de Broglie relations, and we've been led to the idea that there's some relationship between the momentum, the observable quantity that you measure with sticks, and meters, and stuff. So if this is supposed to be true in some sense, what is momentum have to do with a derivative? Momentum is about velocities, which is like derivatives with respect to time, right? Times mass. Mass times derivative with respectto time, velocity. So what does it have toDo with the derivative withrespect to position? And this ties into the most beautiful theorem in classical mechanics. algebra, looked at the problem and was like I don't even know what it means in classical mechanics. So she went back to classical mechanics and, from first principles, came up with a good definition of momentum, which turns out to underlie the modern idea of conserved quantities and symmetries. Noether tells us the following statement, to every symmetry-- and I should say continuous symmetry-- to every symmetry is associated a conserved quantity. OK? So in particular, what do I mean by symmetry? Well, for example, translations. x goes to x plus some length l. This could be done for arbitrary length l, but these are translations. These are conservation of momentum. Less trivial is conservation of energy. translate by L. And what translate by L does is it takes f of X and it maps it to f of x minus L. So this is a thing that affects the translation. And why do I say that's a translation by L rather than minus L? Well, the point-- if you have some function like this, and it has a peak at 0, then after the translation, the peak is when x is equal to L. OK? So define this operation, which takes a function of x and translates it by L but leaves it otherwise identical. A derivative with respect to a position is something that tells you, or controls, or generates infinitesimal translations. If you exponentiate it, you get a macroscopic finite translation. In quantum mechanics, momentum is represented by a specific operator, i.e. i times h. And when you work with the rest of the postulates, including what's coming next, you reproduce the real physics of quantum mechanics. And we're going to see these pop up over and over again. a priori did the world have to look like. Physics tells you this is a good model. And to the degree that it doesn't fit the data, it's wrong. This isn't something we derive. This is something we declare. We call it our model, and then we use it to calculate stuff, and we see if it fits the real world. Out, please, please leave. Thank you. [LAUGHTER] I love MIT. I really do. We live in a world governed by probabilities. the momentum. And these coefficients are going to turn out to contain all the information about the probability of the system. This is the probability when norm squared that will measure the system to have momentum k1. So I think the current wave function is something like a superposition of 1/10 psi pirates plus 1 minus is 1/100 square root. To normalize it properly psi no pirates. And I'll leave you with pondering this probability. See you guys next time. [APPLAUSE] your memory. Gainst death and all oblivious enmity shall you pace forth. Your praise shall still find room, even in the eyes of all posterity. So no judgment arise till you yourself judgment arise. You live in this and dwell in lover's eyes. May your day be filled with love and poetry. Whatever state you're in, we will always love you. Signed, Jack Florian, James [INAUDIBLE]. [LAUGHTER] PROFESSOR: Thank you, sir.

ROUGE-1: 30.39, ROUGE-2: 28.88, ROUGE-L: 29.66
BERTScore: 62.52

==============================================
==================== [3/100] ====================
Summary:
the Lord Byron George Gordon Lord Byron very interesting very outgoing very flamboyant personality he stood out I mean if you can't tell by looking at that picture you know he was a lot different than a lot of people his back history and such isn't necessarily testable stuff but look I mean it just really shows you how against the grain he was. He was accomplished in many activities and sports the literary celebrity down at the bottom had spoken that he was born into his title he didn't do anything to achieve it okay Hinda. Heath Ledger was a very bold and very dangerous individual okay if you look in the right-hand page and I want you to read this page when you have time you know people like James Dean you know those people who have who lived and played hard and you know we're brilliant at whatever they did but you know they they died young you know it was kind of like a tragic type thing people said in the last handful of years Heath Ledger would have been something similar he was a bold person he was rebellious you know in nature and such and he had a you know kind of a tragic end. "I love not man less but nature more from these our interviews in which I steal from all I may be or have been before to mingle with universe and feel what I can narak spread cannot all conceal roll on" "I have loved the ocean and my joy of youthful sports was on thy breasts to be born like thy bubbles onward from a boy I want and with thy breakers they to me were a delight" "If the freshening sea made them a terror twas a pleasing fear for I was as it were a child of thee and trusted to thy billows far and near" is addressing the ocean the water about how big and how timeless start you know he mentioned on that page and we'll get there. These ships that are like okhla vaya thens are really just like toys and if you think about it I mean just the sheer size of that water and those waves what they can do. There are aircraft carriers that arelike cities floating cities with tens of thousands of people I imagine I don't know definitely thousands of sailors okay but you know they've land and they move around and they have food. you know he's very pleased with the ocean he loves the ocean you know talking about you know there is a you know pleasure and rapture in this and I love man the less but I love nature more but ILove nature more and think back we've talked about this a lot in this particular unit about all of these nature elements okay nature and the nightmares and kids all those things that influence writing and here we have another individual as to why this particular piece is a good representation of that of that uh era.

ROUGE-1: 35.35, ROUGE-2: 34.49, ROUGE-L: 35.29
BERTScore: 65.54

==============================================
==================== [4/100] ====================
Summary:
When you do not have unlimited amount of everything you will have to make a choice, what to produce? In what quantity you should produce a particular good. So, what do we mean by allocation? Allocation is nothing but assignment, allotment, share, but in economics we are more technical about this particular term allocation. Allocation here means solving these three fundamental or basic questions of economics, and the first question is what toproduce? The second is how to produce and the third is for whom to produce. command economy, decisions are made by individuals. mixed economy, here it is somewhat a mixture of command and market-based economy. All the economies of this world we can say we can characterize them near to command economy or market based economy, but truly speaking none of the economies are purely command economy. After 1991 after economic liberalization we are slowly moving towards market basedEconomy, but we are not yet there so, still I would say that we have mixed economy. But realistically speaking these are 2 extreme command economy, and market economy. the government. So, this is a concept, an extreme concept that we typically do not find, but you would see in newspapers talk about moving towards laissez faire or moving away from laissezes faire. The government. is not a government, it is a body that has the power to decide what happens to people's lives. The. government is not the government. It is a group of people who have the power. to make decisions about people’s lives. It has the ability to make changes to the law.

ROUGE-1: 41.79, ROUGE-2: 34.39, ROUGE-L: 31.67
BERTScore: 63.61

==============================================
==================== [5/100] ====================
Summary:
Albert Meyer: Random variables are an absolutely fundamental concept in probability theory. He says in a game called the bigger number game, two teams try to pick the larger number. Meyerer: Team 2 picks one of the pieces of paper and turns it over and looks at the number on it. And then, based on what that number is, they make a decision, stick with the number they have or switch to the other unknown number on the face down piece of paper, he says. picks its larger and smaller cards randomly, it's going to be another example of a number produced by a random process. And likewise, the number of the smaller card. So that's enough examples. This little game has a whole bunch of random variables appearing in it. And in the next segment, we will look again officially, what is the definition of a random variable? We will also look again at the concept of "randomness" in the game of cards and dice.

ROUGE-1: 13.26, ROUGE-2: 11.63, ROUGE-L: 11.70
BERTScore: 65.06

==============================================
==================== [6/100] ====================
Summary:
foreign and I should turn off the zoom background blur Ry options like this oh it does show up uh yeah there's like speaker notes on your screen but there's be careful because I accidentally just put something else in the first longer okay. I apologize that was kind of a rough introduction that was uh that was me making a couple of last minute edits that probably hurt more than they helped so I want to just apologize. I just think I was just too ready to go I usually uh yeah as our slides were they and put which is the product describing to replace the names. review um convolutions and and the architecture of a CNN to make this more clear and put it put it into perspective how it relates to um just standard um dense neural networks I think it's fine um so we talked I think I think most people felt okay about um the actual mechanics of doing a convolution. When you have images and a CNN you can simply just do convolutions instead of your normal matrix multiplication. You have pooling layers if you want to decrease the size of this volume because it can get quite unwieldy. but it's still ideally still sort of captures all the main information that was in that uh that feature volume um yeah it's it's more just used so we can get our feature volume down to a reasonable size so it doesn't take forever to run convolutions on it just makes it a lot quicker yeah no well so we're going to talk about um segmentation which is where you have like an image like a person in it and you need to Output another image except each pixel has basically been like labeled with like there's a person inside of like here and then everything else is background so in that case your output is the same size as your input which gets a little bit weird. different activation Maps one from each filter that we obtained so your your Channel's Dimension uh would be 10 Deep um yeah it just corresponds to like if if if this filter is corresponds to horizontal edges and this filter corresponds to edges like this. You can just sort of look along the channel and see like okay like was there an edge that went this way was there a feature that went that way. If there are any more questions then I will hand it over to Rohan who's going to talk about more advanced CNN architectures. a little timeline here starting from uh alexnet and moving forward. laneet is something that was created quite a while ago. Alexnet in 2012 was a a big groundbreaking feat in that it proposed stacking layers of convolutions Max poolings following it up by fully connected layers. This really sets the stage for uh the the future of advancement in this field so yeah convolutional Nets were proposed in 1990 by Yen lacun and he kind of pioneered that pattern for today he's currently head of AI at Facebook AI research so furthering the metaverse. because you're not negating the problem this is highly dependent on what system you're using to compute these as well as where the model is eventually running so yeah um yeah dude that was an example of a very long uh residual net adding skip connections makes the identity easier to learn because you're quite literally adding a previous identity to the resultant of a transformation. If you add if you put a million layers on a dense neural network it's going to just learn like garbage like it's not going to work at all but like that's kind of weird because we should just trivially be able to add more layers. shouldn't produce accuracy when in reality like if this is scaled it can yeah awesome group wise uh your Dimensions so yeah this is a really good point um so if your layer is a convolution um the dimension can change which is why often this is result like kind of viewed as f of x plus W of X where W is a transformation that you do on X two to make it the same Dimension exactly. Global average pooling is designed to replace fully connected layers in cnns. The typical dense layer that you previously had that's facilitating these connections does not enforce correspondences between feature maps and categories. As you go through the three dense layers at the end of the network there's no parameter to optimizing global average as well which saves overfitting time. As we get deeper the number of channels can get large which leads to a lot of parameters. What if we process each Channel separately and then intelligently combine those at theend how can we still retain data from each Channel while reducing the computations that you're doing? number of uh computations that we need to do the answer is depth wise separable convolutions so this is a a pretty decent visualization of how that works. If you have like a three channel uh like some X by X image you're applying a uh like a feature map to it and then you're left with one product by one channel. Instead of that what if we took each Channel individually we applied a smaller or a lower Dimension feature map and created three depthwise layers and then combine those with a convolution. those so you can think about it instead of doing one step that results in one map and multiplying that by the number of filters you have you're not doing one. You're applying this one step to every channel and then your next step is applying a different sized convolution to do your filter multiplication. Instead of one step you have two steps that are being combined um which reduces complexity quite a bit alrighty I guess we can quickly go over like squeezing anxiety networks basically uh you squeeze you apply this through a couple of dense layers and then you rescale. CNN.swap out if you're very well CNN building plus um you know yeah I think understanding like the the base of how optimizations are held and the problems that certain optimizations face and others don't really sets the stage for like future networks like the efficient net in 2020. There will also be a quiz at least is there a homework position closer or not [Music] um and do next Friday [ music] thank you for sure. Thank you guys for coming thank you Guys for coming.

ROUGE-1: 23.96, ROUGE-2: 22.95, ROUGE-L: 22.46
BERTScore: 70.37

==============================================
==================== [7/100] ====================
Summary:
The principles of baking looks at whether using a preferment when cold fermenting is worth it. We'll make three breads one with the brief mint and two without one of the breads without the bereavement will be bulk fermented in the fridge for 24 hours. When it comes to cold fermentation versus brief mint there is no better method what is better is up to the baker. The final product will have a much lighter and airier crumb or at least it is able to and it can make for a thinner crust too. When it comes to combining Cobalt fermentation and bereavements that's where I say simply fermenting for longer is better than using prevent and then fermenting the dough in fridge. When making the final dough I did use slightly less yeast in the dough that was made with the bravement and that was to compensate for the amount of yeast coming from the bereavement itself as it rises it multiplies. If you want to find the written formula it is in the link below the video title okay we'll do the final shaping now we'll Place those in love tins we'll let them rise and that will bake them. comparison for yourself then tell us of your results you might be surprised. What do you think of my test and my results have you ever tried something like this before do you use preference when cold fermenting let me know down in comments. If you want to see more videos like this one click over here subscribe to the channel click right here that's all I have for you today thank you so much for watching I'll see in the next one and I'll be back in a few days.

ROUGE-1: 36.76, ROUGE-2: 34.20, ROUGE-L: 33.54
BERTScore: 63.20

==============================================
==================== [8/100] ====================
Summary:
Bayard Rustin was the chief organizer of the 1963 March on Washington for Jobs and Freedom. Rustin grew up in a Quaker household, and began peacefully protesting racial segregation in high school. He was jailed in 1944 as a conscientious objector to World War II. In the 1980s, he publicly came out as gay, and was instrumental in drawing attention to the AIDS crisis until his death in 1987. In 2013, fifty years after the March On Washington, President Barack Obama posthumously awarded him the Presidential Medal of Freedom. are or who we love.” “I’m so proud of you,” she says. “You’re so beautiful.’ ““I love you, too,’ I say. I love you so much.“” I’ll always love you. ”I will never forget you. ””“We’ve been through a lot. We’d rather be here than there. ’’”

ROUGE-1: 27.06, ROUGE-2: 22.61, ROUGE-L: 22.81
BERTScore: 56.37

==============================================
==================== [9/100] ====================
Summary:
Christine Hayes: Deuteronomy describes God's choice of Israel as the chosen one. She says the privilege of being chosen or singled out entails obligations and responsibility. Hayes: Israel was chosen by Yahweh in an act of spontaneous love--;it does not imply her perfection. She argues that the idea of a superiority complex is a moral danger involved in the notion of election, and that Israel was not chosen by any special virtue or merit, but by God's initiative. and is no cause for Israel to boast. drive you out just as he drove out the Canaanites. That's a theme in Deuteronomy. God's providential love and care for Israel is expressed through various metaphors in the Bible. So we have a lot of sort of love and marriage imagery, husband and wife imagery, used for God and Israel. But we also have this parent and child imagery that appears.… It almost seems to play on the idea that God is not perfect, but that is their choice. They favor them. At the end of Deuteronomy, the promises still are not fulfilled. The people are still outside the land. Some have suggested that this is quite purposeful. It points to an exilic date for the work's final composition. When it was finally redacted, the redactors were in exile. And the Deuteronomist wants to make it clear that it is fidelity to the Torah, rather than residence in the land, that is critically important. But in any event, it's also the first part of a much larger, longer literary work. There is a great deal of ideological baggage that is involved in the dating of the sources. Anti-priest, anti-cult sentiment is apparent in the history of biblical scholarship. P espouses a communal ethic, and post-exilic priests are going to turn to an individual ethic. Many sections of P do not seem to assume a central sanctuary. The idea of the central sanctuary really took hold in 622, Josiah's reform: that the central reform was Josiah and not Israel. the Bible is divided into two parts we refer to as the "Former Prophets" and then the "Latter Prophets." The Former Prophets will concern us for the next few lectures. The Latter Prophets is a collection of books, each of which bears the name of the individual whose prophecies it purports to contain. These prophets delivered their oracles at critical junctures in Israel's history, in the nation's history. So their words are only going to make sense if we first understand the particular historical crises that they are addressing. redaction of these books, would put the materials together by inserting verses and speeches that would frame the older sources and link them together. The whole unit, as a whole, was redacted after 622: that's clear. It assumes and insists upon the centralization of the cult. The last dated event that is mentioned in 2 Kings is something that occurred in 562. So the work was probably concluded shortly after that date: so in exile or towards the end of the exilic period. There are key features of Deuteronomistic thought that are evident from Joshua through 2 Kings. In the past 4000 years more wars have been fought for the possession of the tiny strip of land known as Canaan. Canaan is about 150 miles long and 70 miles wide, about the size of Rhode Island. Control of these international highways brought a great deal of wealth to the area. In times of peace it would bring prosperity, but, of course, in times of war the land was perpetually invaded as armies would crisscross the land going off to do battle with the great powers.. Israel boasts great geographical diversity, says historian. Three main geographical subdivisions run in strips from north to south, he says. Being somewhat isolated, inhabitants of each region developed distinctive economic and cultural character, says Shmuley Boteach. "Within this relatively tiny area there are radically diverse regions, and this fact held important implications for Israel's history," says Shuley Botesch. "We call this the wilderness, the wilderness of Judea between Jerusalem and the Dead Sea," he adds. city dwellers. You have merchants and traders who are handling the commerce on the trade routes. So that's the geographical setting for what we are about to read in the Book of Joshua. The structure of Joshua is really somewhat simple. We can really divide it into two major parts. The first 12 chapters form a unit that conveys the invasion and conquest. The story of the Battle of Jericho is really a composite of two accounts that have been woven together into a single narrative. The account of the conquest in Joshua 2 through 12, is concerned to express the idea that Israel's victories would not have been possible without Yahweh. In Joshua, military skill is much less important than ritual preparation and purity. Why the claim of the utter destruction of the Canaanites when evidence points to close Canaanite origins? This practice, which I mentioned before and is known as herem or the ban, is not unique to Israel. The imperative of preserving a distinct identity--based on giving up the worship of other gods or older gods--is reiterated in Joshua's farewell address in Joshua 23. And the central idea is that there is only one proper response to God's mighty acts on behalf of Israel.

ROUGE-1: 18.71, ROUGE-2: 17.19, ROUGE-L: 17.21
BERTScore: 62.98

==============================================
==================== [10/100] ====================
Summary:
Bogdan Fedeles: Today we're going to be talking about Problem 2 of Problem Set 4. We'll be discussing in detail the mechanism of HMG-CoA synthase, a key enzyme in central metabolism. The enzyme is responsible for making the five carbon building blocks from which all sterols, such as cholesterol and steroid hormones, are made. The reaction will start by forming this thioester between the acetoacetyl- CoA carbons and the cysteine in the active site of the enzyme. a look. Here is an oxygen ester that shows a proton in alpha position. And as you know, the lone pairs on this oxygen can conjugate with the carbonyl group. So the electrons can move like this, and then we're going to have a negative charge here and a positive charge here. And this is possible because the electrons on both oxygens are found in orbitals of comparable energies. By contrast, in the case of a thioester, we have a sulfur. experimental work and evidence. The study of the effects of drugs on the human body was conducted in the 1970s and 1980s. The results of the study were published in a book called "The Effects of Drugs on the Human Body" The book was published by Oxford University Press in 1989. It was the first of its kind to be published in the U.S. and is published by Simon & Schuster, a division of Simon and Schuster Inc. in New York. The book is available in hardback and paperback.

ROUGE-1: 11.65, ROUGE-2: 9.05, ROUGE-L: 9.91
BERTScore: 62.08

==============================================
==================== [11/100] ====================
Summary:
Alberto Riva is an instructor at CHB. He will talk to you about the most important resources for finding and using biomedical information, especially information connected with the study of the human genome. Most of them will be websites, where you can find information, and to talk about how this information is stored and represented, how it's accessible, and what it can be used for. You're going to see a long list of references to sites, websites, with URLs. Don't worry if you can't remember all of them because, of course, I'm going to distribute the slides. The genotype is digital, because each base pair in our DNA can be exactly represented using one of four symbols, A, P, G, C. The phenotype is analog, because most phenotypes are qualitative in nature. They cannot be measured exactly or precise, they cannot even be defined precisely in most cases. You always have to take into account the effect of environmental factors that, again, are very hard to describe in a quantitative way. How is all this information represented? What are the different ways that we can store and describe this information? Where does it come from, where is it stored, how do we find and use it? The first thought is of inherited traits date back to Mendel in 1866. It took over 80 years for this concept to be proven. The definite proof that genes are made of DNA dates back to 1952. And finally, the Human Genome Project, that was officially declared a success last year, brought us to the point where we now know the exact base pair sequence of our genome. So we're going to talk about the role of DNA in genes in a way that we haven't done before. There are differences between the DNA of two any human beings. These differences are due to polymorphisms, like single nucleotide polymorphisms. Microsatellites, repeats, insertions, deletions, translocations, these are all things that can happen to your DNA sequence that can modify in ways that, of course, are not enough to turn you into another animal. You're going to find approximately one difference every 1,000 bases. This is one of the most interesting problems in current bioinformatics and molecular biology. The process of transcribing the DNA sequences into RNA is, of course, at the basis of expression analysis. Differentiable analysis, clustering, and so on, these are all the usual things that can be done using microarrays. Starting homology and conservation of proteins across different organisms can give you a very good idea of the importance of some proteins. And finally, something that is very challenging, and it's receiving a lot of attention lately, is the automatic construction and analysis of metabolic pathways and regulatory pathways. try to build, in a computational way, the kind of pathway maps that biologists have been drawing by hand for decades. Of course, we're still very far away from being able to do this in the general case. But these are all very challenging problems that, of course, are still very much open. And finally, we get to the phenotype then, we could put a very long list of things here, but could talk about population genetics, about association studies. Association studies are studies that try to correlate the presence of a certain genotype with an observed phenotype. GenBank is the largest repository of DNA sequence data. It accepts direct submissions from researchers. GenBank is at the basis of the NCBI cluster. NCBI is a branch of the NIH, that has the task of assembling the largest possible number of databases of biomedical information. The most recent data that I could find from one year ago. It contained more than 22 million sequences and 100,000 distinct organisms. With a total of almost 30 billion nucleotides. And this is the URL for GenBank. If you have enough sequences from the same organism, you can try assembling them, putting them together, and trying to reconstruct the entire genome. This is what was done to assemble the human genome, for example, and all the other genomes that are being sequenced. You start with-- you look at the sequences you have, and if you can find overlaps, then you know that these two sequences are related in some way, and you proceed from there. And in the end you're going to build a map that tells you where all these fragments should be positioned. six times for validation. So and the details of how this process has been implemented and CVI are here. And in addition to the human genome, of course, we have a lot of other genomes that are completed, or near completion. These numbers are probably higher by now we have over 1,000 viruses. And then many other organisms from different domains of life. Of course, eukaryotes are the hardest organisms to sequence. The human genome is considered finished by now. It's hard to go above this level of accuracy. Golden Path was the first site to provide something like this. It gives you the absolute position of all the known elements of our genome. It provides arbitrary DNA sequences-- so you can ask for any region of any human chromosome, you'll get back the exact DNA sequence for that region. There's another way of looking at the genome using the NCBI map viewer. It's a graphical browser to look at genomes and annotations of genomes. It can rely on the whole set of NCBI databases, so basically everything that could possibly want to know is in there. SNPs are the most common form of variation in our genome. They're important because for example, they can be used as genomic markers. If you have a SNP that introduces a change is beneficial, then you will see that the frequency of the SNPs increases in the population. If a SNP is neutral, then there is no selective pressure, and it will either go away by chance, or will stay at a certain basic level of frequency. The largest database of SNPs that we have again, is at NCBI, it's called the dbSNP. is known about it. And if you have a SNP that affects one of them, that SNP, in turn, might cause a protein to work-- it can change the function of a SNP-- of a protein. So I'm just reporting here a list of all the domains that contain that location, but they are important because they do something. And we get so many because all these domains are overlapping. And so you see for example, this first domain covers almost all of the protein. LocusLink is a curated directory of genes from 13 organisms. LocusLink provides a nomenclature of genes. UniGene is an automated system, so it's actually an automated procedure that looks at the GenBank sequences that refer to a region of the genome where a gene is known to be. And I think that one year ago, this number was something like 14, and now it's growing very fast. This one is probably the only one that could be affected by the presence of a SNP. So don't get confused by this place. Just a list of Swiss [INAUDIBLE] domains that include that location. a software system for the automated annotation of genomes. It's basically means it's a system that discovers genes and tries to find as much information as possible about these genes. So it's essentially a combination of LocusLink, dbSNP, HomoloGene, and a few other things. It has a very powerful data access interface. So you can do queries on this huge database in a relatively simple way. But of course it's not-- they don't necessarily match very well. is a consequence of the fact that there is a very complex machinery behind it that determines which genes are active or not, and how much, in different conditions. This is actually a system that integrates a lot of different factors that might include the following, in no particular order-- The tissue, we know very well that the set of genes that are expressed in one tissue is very different from the set that is expressed in another tissue. External signals, of course, all response to external stimuli. And it also depends on the expression state of any number of other genes. The first step, again, we're moving the first steps. The first thing you need to do is need to be able to reliably identify which transcription factors bind to a given gene. And that, in turn, will determine the spatial, temporal, dependent expression of the target gene. If a method like this, if you have a meter to detect binding sites that works well, then you can think on a large scale, looking for all the binding sites for a certain factor for a human gene. This is one of the things we are working on in our lab. GEO is a database of gene expression and hybridization array data. The Stanford microarray database, again, is a repository of all the-- of a large number of micro experiments performed at Stanford. Other resources for gene expression are found in different PGA projects, PGA are programs for genomic applications, they are are large projects managed by the NIH. The [? tracks ?] PGA, for example, offers 565 microarrays from mouse and rat models of sleep, infection, hypertension, pulmonary disease. The protein world is much more complex than the DNA and RNA world for the reasons that I've explained at the beginning. The biggest database is SwissProt, 120,000 sequenced centuries, 9,000 human proteins in SwissProt. OMIM is a catalog of human genes and disorders by the NCBI. The Hopkins PGA, again more than 500 microarrays from several human diseases. Cardio genomics provide microarray data on mouse models of cardiac development and signal transduction. And finally, human gene expression index. Gene ontology is to build a dynamic controlled vocabulary that can be used to describe biological concepts. It's organized in three taxonomies that try to describe everything that is known about molecular functions, molecular biological process, and several components using a standardized nomenclature. It is a work in progress-- still very far from being complete. It could find exact definition of all the terms that people use, especially in this field is very hard. But this is where they are now, and it's aWork in progress, so it will keep growing in the future. a view of taxonomy, for example, for biological process. If you are talking about site communication, then response to external stimulus is a subclass of communication. And if you want to talk about the immune response, you can cite this biology term, and everybody will be able to go to gene ontology. OK, I think we're out of time. Well, just a conclusion slide that I'll just let you read, because I think it's just repeating what we're saying so far that we are drowning in data and converting this data into knowledge is not easy.

ROUGE-1: 29.11, ROUGE-2: 27.93, ROUGE-L: 26.56
BERTScore: 68.30

==============================================
==================== [12/100] ====================
Summary:
This week in week three, we're actually going to have some human language, and so this lecture has no partial derivative signs in it. The other thing that happens in assignment three is that, we start using a deep learning framework PyTorch. And so we hope that you can put together what you learned about neural networks last week and the content of today, and jump straight right in to building a neural dependency parsers. And since I missed my office hours yesterday, I'm gonna have a shortened office hour tomorrow from 1:00 to 2:20. Linguists have thought about the structure of sentences in two ways. One is called phrase structure, or phrase structure grammars. The other is called context-free grammar. The idea of phrase structure is to say that sentences are built out of units that progressively nest. So, we start off with words that, cat, cuddly, et cetera, and then we're gonna put them into bigger units that we call phrases. And then you can keep on combining those up into even bigger phrases, like, "The cuddlely cat by the door" In this system of dependencies I'm going to show you, we've got in as kind of, um, a modifier of crate in the large crate. And well, then we have this next bit by the door. And as I'll discuss in a minute, well, what does the by thedoor modifying? It's still modifying the crate, it saying, ''It's the crate by the doors.'' Okay. So, the structure you get may be drawn a little bit more neatly when I did that in advance like this. to your friends is that you just blab of something, and I understand what you're saying, and, um, what goes on beyond that, is sort of not really accessible to consciousness. But well, to be able to have machines that interpret language correctly, we sort of need to understand the structure of these sentences. Unless we know what words are arguments and modifiers of other words, we can't actually work out what sentences mean. And I'll show some examples of that as to how things go wrong immediately, because a lot of the time there are different possible interpretations you can have. "What can go wrong?" is a way of saying, ''What can't go wrong?'' Okay. So here, is a newspaper article. Uh, ''San Jose cop kills man with knife''. Um, now, this has two meanings and the two meanings, um, depend on, well, what you decide depends on what, you know, what modifies what? "The cop stabs the guy. The second meaning the sentence can have is, that's the man has a knife" what is modifying what? Um, here is another one that's just like that one. Um, scientists count whales from space. [LAUGHTER] Okay. So again, this sentence has two possible structures, right? That we have, the scientists are the subject that are counting and the whales are the object. So, obviously what you want is this one is correct and thisOne is here wrong. So this choice is referred to as a prepositional phrase attachment ambiguity, and it's one of the most common ambiguities in the parsing of English. if that's not what you want, um, you have to use parentheses or indentation or something like that. If we think of something like C or a similar language, it's just deterministically, the else goes with the closest if. That's not how human languages are. Human languages are this prepositional phrase can go with anything proceeding, and the hearer is assumed to be smart enough to work out the right one. If you want to have artificial intelligence and smart computers, we then start to need to build language understanding devices who can also work on that basis. acquisition by Royal Trustco Limited of Toronto for $0.27, $27 a share at its monthly meeting. Boring sentence, but, um, what is the structure of this sentence? Well, you know, we've got a verb here, and we'veGot exactly the same subject, and for this noun,Um, object coming after it. But then what happens after that? well, here, we have a prepositional phrase. You've just got a see fourPrepositional phrases in a row. And so, well, what we wanna do is say for each of these prepositions what they modify. For $27 a share is modifying acquisition, right? [NOISE] So now, we leap right back. Now, is now the acquisition that's being modified? And then finally, we have at its monthly meeting is modifying? [Noise] Approved. Well, the approved, right. It's approved, yeah. Okay. [NOise] I drew that one the wrong way around with the arrow. Sorry, it should have been done this way. I'm getting my arrows wrong. Um, um. of places when they're tree-like contexts. So, if any of you are doing or have done CS228, where you see, um, triangular- triangulation of, ah, probabilistic graphical models and you ask how many triangulations there are, that's sort of like making a tree over your variables. And that's, again, gives you the number of them as the Catalan series. But- so the point is, we ha- end up with a lot of ambiguities. In English, you can use kind of just comma of sort of list intonation to effectively act as if it was an "And" or an "Or", right? So, here, um, we have again two possibilities that either we have issues and the dep- and the dependencies of issues is that there are no issues. Um, and then it's sort of like no heart or cognitive issues. So, "Heart" has a depend- has a coordinated dependency of "Issues". That's one one. "Mutilated body washes up on Rio beach to be used for Olympics beach volleyball." Um, wha- what are the two ambigui- What are the. two readings that you can get for this one? [NOISE] We've got this big phrase that I want to try and put a structure of to be use for Olympic beach volleyball, um, and then, you know, this is sort of like a prepositional phrase attachment ambiguity. Um, and you have it's the first experience and it goes like this. that, um, we can have here is another noun phrase muti- mutilated body, and it's the mutilatedBody that's going to be used. Um, and so then this would be, uh, a noun phrase modifier [NOISE] of that. Okay. So, you know, this is back to the kind of boring stuff that we often work with of reading through biomedical research articles and trying to extract facts about protein-protein interactions from them or something like that. But it's, it's sort of, okay, youknow, I was using funny examples for the obvious reason, but, you Know,This is sort of essential to all the things that we'd like to get out of language most of the time. can kind of think of these two things as sort of patterns and dependencies that we could look for to find examples of, um, just protein-protein interactions that appear in biomedical text. Okay. So, Dependency Grammar postulates the what is syntactic structure is is that you have relations between lexical items that are sort of binary asymmetric relations. We have a system of dependency labels. All we're doing is making use of the arrows. And for the arrows, you should be able to interpret things like prepositional phrases as to what they're modifying. In the later parts of the first millennium, there was a ton of work by Arabic grammarians and essentially what they used is also kind of basically a Dependency Grammar. There was this guy Wells in 1947 who first proposed this idea of having these constituents and phrase structure grammars. And where it then became really famous is through the work of Chomsky, which love him or hate him is by far the most famous, um, linguist and also variously contributed to Computer Science. Computational linguistics. Some of the earliest parsing work in US Computational Linguistics was dependency grammars. Universal Dependencies is project I've been strongly involved with. The goal of universal dependencies was to say what we'd like to do is have a uniform parallel system of dependency description which could be used for any human language. If you have a- a big calling to say I'm gonna build a Swahili Universal Dependency um, treebank, um, you can get in touch. We want models that can kind of capture what's the right parse. For each word we want to choose what is the dependent of. We want to do it in such a way that the dependencies form a tree. Most dependencies are fairly short distance. They not all of them are. So, if we sort of said, Bootstrapping, um, was a dependent of talk, but then we had things sort of move around, this goes to here, and so I'm gonna cycle that's bad news. We don't want cycles, we want a tree, and this example here is actually an instance. is a transition based parser. It was mainly popularized by this guy, walk him Joakim Nivre, he is a Swedish computational linguists. Um, and what you do it's- it's sort of inspired by shift-reduce parsing. And this is sort of like a shift- reduce parser, apart from when we reduce, we build dependencies instead of constituent. So, what I wanna to do is parse the sentence "I ate fish". And yet formally what I have is I have a why I start, there are three actions I can take. Joakim Nivre showed that you could predict the correct action to take with high accuracy. In the simplest version of this, there's absolutely no search. You just run a classifier at each step and it says "What you should do next is shift" and you shift. He proved, no, he showed empirically, that even doing that, you could parse sentences withhigh accuracy. Now if you wanna do some searching around, you can do a bit better, but it's not necessary. And in the last bit of lecture I want to show you what people have done in the, um, neural dependency world. Using a neural network to make the decisions of Joakim Nivre Style shift-reduce parser, we could produce something that was almost as accurate as the very best parsers available at that time. The secret was we're gonna make use of distributed representations like we've already seen for words. So for each word, we're going to represent it as a word embedding, like we're all what already seen. And in particular, we are gonna use word vectors as the represent- the starting representations of words in our Parser. more to do with each other than others. So maybe we could have distributed representations, a part of speech that represent their similarity. And so, we built a representation that did that. Now for- so, you know starting from- starting from the next lecture forward, we're gonna sort of s- start using a more complex forms of neural models. But for this model, um, we did it in a sort of a very simple straightforward way. We said, well, we could just use exactly the same model. Our neural network is just a very simple classifier of the kind that we are talking about last week. So based on the configuration, we create an input layer. We do Wx plus b and then put it through a ReLU or a non-linearity to a hidden layer. And then on top of that, we're simply gonna stick a softmax output layer. So multiplying by another matrix, adding another, um, bias term, and then that goes into the softmax which is gonna give a probability over our actions. right action. By having it use these dense representations, it meant that we could get greater accuracy and speed than Nivre's parser at the same time. Google said, "Well, maybe we can get the numbers even better if we make our neural network, um, bigger and deeper and we spend a lot more time tuning our hyper-parameters" All of these things help when you're building neural networks. Sometimes the answer to making the results better is to make it bigger, deeper and spend more time choosing the hyper- parameters. There's still room to do better. I mean, at the unlabeled attachment score, it's actually starting to get pretty good. But there's stillRoom to doBetter. Um, yeah. So Beam search, the final thing that they did was- that we're not gonna talk about here, is the sort of more global inference to make sure it's sensible. [inaudible] [OVERLAPPING] [NOISE] So that's a good question which I haven't addressed. um, humans don't always agree. Google developed these models that they gave silly names to, especially the Parsey McPa- parseFace, um, model of parsing. So that then- that's sort of pushed up the numbers even further so that they were sort of getting close to 95 percent unlabeled accuracy score from these models. And actually, this work has kind of, you know, deep learning people like to optimize. So we- we're still going on but I think I'd better stop here today.

ROUGE-1: 30.16, ROUGE-2: 28.93, ROUGE-L: 28.98
BERTScore: 65.01

==============================================
==================== [13/100] ====================
Summary:
Okay folks let's get started how's everybody doing pretty good Santa Claus has come to town and you know Santa does with naughty kids Hees them finals he gives them finals. He gives them very evil finals is what he does okay so look out for Santa Claus he's really a really a bad guy. Let's see let's think about a couple of things in terms of announcements and we have a couple surprises today one of which is standing in front of you with all this on and there's more surprises as well. I've written the exam it has 150 points it has um the first section which is the short answer has 75 points that's half of the exam. The second section is the problem solving section has 36 points and the last section which are the longer answer has 39 points now it hasn't been duped yet so there may be some changes. One of those changes I'm kind of got to decide today is will we have an extra credit question or not I don't know so maybe you guys can help convince me today. I think this looks like a like an average exam in that respect if you did not get a note card on uh Wednesday when I passed them out uh I don't have them here you'll have to come to my office to get them from me I will remind you that with the exam you have to turn in a note cards that you got from me. If you don't or use a different card or something you will lose points and again I need to make sure that you're using these cards and you're not passing them on to your roommates because that's that's important. Glycogen phosphor is U an enzyme that's regulated in several ways. It exists in two forms it exists in the glycogenosphor a form which is the form that has the phosphate on that people describe as the more active and it has the form without the phosphate known as the glycogens phosphor B that people say is less active. Def phosphorilation involves a kise or phosphatase right the r&t involve allosteric affectors okay all right okay so that's where we start now. There are a variety of what are called glycogen storage diseases and these diseases arise as a result of efficiency of certain enzymes. The one I think is the most interesting is actually mcardle's disease gunite. A person lacking glycogen phosphor in their muscle loses ability to do strenuous exercise. The person with McArdle disease sees ADP levels go high and then it falls meaning that the cells are catching up and making ATP now for that last chance at a CD my question to you is what's making this possible. the Cory cycle right what does the Corey cycle do glutose the liver has a normal enzyme right and so when the muscles start running out of energy what happens oh wow we need some glucose they can't get glucose from breaking down of glycogen but the liver sure as he can do that and the Cory cycle kicks in okay so in essence it's a a backwards thing to the lactate that you talked about but it's the fact that the phosphor in the liver is perfectly normal that makes sense.  heard before okay so um you guys know I like the Beatles I write a lot of stuff to Beatles music and you know the Beatles were like this but I really think they should have been like this. Let me introduce the next bees that are going to turn the music world upside down through the bio comical choir please come down I have some people to help me sing today so they're going to drown me out yes Applause is appropriate yes thank you thank you let me introduce our participants including one person in the class starting with Santa on the end this is Bonnie. Valeria and Linda so uh thank them all for uh thank you all for coming to help me with this I thought about you know introducing them you know we think about flash mobs I thought maybe we could just call these guys the world's first flush mob you know but that might be a little mean so I didn't do that so okay so we're going to do some of Britney's favorite hits we're oops we'reGoing to do it again and you guys know the rule about singing loud right okay. 5m I'll have my car pack with information so I don't have to memorize it and I'll feel like aart with my party just one to go and then ho I'll be all right I think you got it thank you. [Applause] oh yes thank you thank you [Music] sh if if if you're too crowded for the final and I don’t get a chance to say it I hope you have a great holiday and I will see you next time thank you indeed you too study hard.

ROUGE-1: 25.22, ROUGE-2: 24.61, ROUGE-L: 25.14
BERTScore: 70.28

==============================================
==================== [14/100] ====================
Summary:
JUDY HOYT: Hopefully everybody's recovered from their Thanksgiving feast. What I'm showing up here is the schedule to orient us. This is lecture 22. We'll talk about silicides, device contacts, and I've added in a new material this year on novel gate materials. And then we have one more lecture, which is on strained silicon and silicon germanium growth and processing. Next week we have two class periods scheduled. There'll be four speakers in each. And those will be the oral reports and the student reports given on Tuesday and Thursday. In this lecture, we'll talk about the formation of silicides. We'll also talk about local interconnects, which are used to connect different parts of a chip. The lecture is kind of the first link to the backend technology course. If you have any questions about the final project, just please email me or contact me after class. The notes for today's lecture are given in handout 36. The final project is 6.773, which is the back-end technology course in electrical engineering. is not considered desirable for making a good contact. An ohmic contact on the other side, or a tunneling contact, occurs when you can actually tunnel right through this barrier. And it tends to happen, particularly with aluminum and most metals, if you make the surface doping very high. And so if you look at the current voltage characteristics-- so current plotted on the y-axis, voltage on the x-axis-- you see it's very symmetrical, and you get a very, very high current for a very small voltage in either direction. it's got a units of resistance times an area-- and divided by the area of the contact. So if I make a smaller contact area for a given specific contact resistivity, you're going to have a higher resistance. And this is a problem if I'm scaling devices. As I make the area the current goes through smaller, the resistance of that contact goes up. Unless we do something to make better contacts as we scale devices, just because of this geometric factor. And we'll see some specific numbers on this. A Schottky contact is governed by thermionic emission. So it's a thermionic process emitting carriers over this barrier. So that's going to be a property of the doping in the semiconductor and what kind of metal you put on it. So for low contact resistivity, to get this number down, I need a small barrier. And you would choose a particular metal that gives you a relatively small barrier height. So you need, most importantly, very high doping. And then you will get an IV characteristic that is essentially ohmic. The contact passivity doesn't scale as we shrink technology. People are looking for new methods, new materials, whatever, some way of getting the doping up, or a new method of making contact. That's kind of a fundamental issue. In fact, here on slide 11, I show that this is really one of the concerns. This is a table that I took, table 71a, from the 2003 international technology roadmap for semiconductors. And just to remind you, in the upper right corner, this is what the MOSFET looks like. are going to be silicides, either cobalt silicides or nickel silicide. They're a contact between a metal and a heavily doped semiconductor, which is silicon. In 2003, they wanted to have about 2 times 10 to the -7th ohm centimeter square. In 2004, you want to lower it according to ITRS about 1.6 times10 to -7. That's all in yellow, which means their manufacturable solutions are known. They haven't yet been integrated. And when we get to 2008 in the red, there aren't any known solutions that are manufacturable. silicide sheet resistance in ohms per square, it needs to be in this range of 6 to 10 ohmsper square. That's not a problem. None of that-- that's all in white, which means people know how to do that. The real problem is making these really good low resistivity contacts to the silicon. I just want to show an example of how-- so you get a feel for how the numbers work of how one does a contact resistance calculation. This is for a MOSFET. So that's the area. If we go back to ITRS, assuming we are here roughly in 2002 and the contact resistivity is about times 10 to -7 ohm centimeter squared, then can calculate the resistance of just the drain contact itself. The resistance is just that rho C divided by the area of that contact. And you get 100 ohms. So we're talking about 200 ohms of the total resistance of the device just being due to the contacts because of the size. And that's a reasonably large resistance. figure 11-35. There's a reference to it in your textbook, a paper that an article that talks about it in much more detail on how it's actually made. And what you do is you have these dark regions here that are funny shaped are considered to be the N plus region that you want to contact. The dashed lines represent the metal. So that would be the metal level. So this is going to take at least three masks to powder. And these little square regions with the X's going through them are the contacts. Kelvin structure is a very common structure to measure contact resistivity. It uses separate probes to measure voltage and current. You don't want to have extra contact resistance, say, of your probes going down. So you just divide the voltage drop divided by the current. And that gives you a resistance number of 32 ohms, which is equal to rho C divided by area of the contact, L squared. The voltage drop is about 320 microvolts, and the current is 10 microamps. multiplying 32 ohms by the area of the contact. And you get 3.2 times 10 to -7 ohm centimeter squared. Still a little bit high. It's still about a factor of 2 too high. But it just gives you an idea. Typically you use several different dimensions. So you either use a 1 by 1, a 5 by 5, and a 10 by 10 square. And for different size areas, you should get the same number. If you don't, then you probably have current crowding effects. You don't want excess carbon or other things at the interface because that's going to cause an increase in your contact resistance. That's one thing to get low-contact resistance. The second thing you want is good thermal stability. After all, there will be subsequent thermal processing. Once you make those contacts, you still have to make all that multi-level metal. And those multi- level metal schemes involve annealing steps or deposition steps that could be in the range of 400 to 500 degrees. Aluminum has a finite solubility for silicon. The silicon actually gets sucked into the aluminum and causes this spiking effect. Today if you do this with any kind of reasonably shallow junction, when you go to heat the thing up to do the final forming gas anneal, you're going to spike the junction. The way people go today is to form what they call a barrier layer. It forms a barrier between this aluminum and the silicon. So this prevents the chemical interdiffusion between the silicon and the aluminum. In CMOS technology, a metal silicide is used to form a contact between the silicon and the source, gate, and drains where it's in contact with silicon. The first one that was used historically was Ti-silicide. Cobalt has a little less lateral encroachment over the oxide spacer. The latest silicide that people are exploring in research and development-- and at some point will probably be in production-- is nickel silicide. But nickel is a very fast-user, remember, in silicon. There are three resistances we really want to be able to think about. These back-of-the-envelope calculations are terrific because you can do them in five minutes in class or in 10 minutes in your office or whatever. There is another resistance called the spreading resistance. It's not something you can simply calculate by hand, but it can be a major contributor to test devices. There will be a little extra, which is spreading resistance, which you have to simulate in a two-dimensional simulator. That's how we make contacts. The traditional technology is polysilicon, which can only be doped so high. People want to replace the poly with a gate, with a metal gate. But poly is an extremely easy material to integrate. It's not reactive with SiO2. That's not the case for metals. So people are thinking of replacing poly, but it's got a lot of complex process integration. The FUSI process for the gate, you actually need to have two different siliciding steps. You need to cover them and protect them then, then use a silicide to try to fully the gate. So it's still tricky. The self-aligned silicide process, or also abbreviated salicide, has been the mainstay of technology for many, many years now. Silicides can also function as a barrier layer to prevent spiking, potentially. control the PT or adjust the PT a little bit. It's still a very tricky process. The reaction temperature or the thermal stability temperature is reasonably low. So you cannot take these and then take them to a backend process that is too hot. It was only published by IBM a couple of years ago. about your oral report or whatever, please get back to me. OK, thanks. About your oralReport.com. About. Your oral report. About Your Oral Report. Please get. back tome. about your oralreport or whatever. about. your oral Report or whatever,. please getback to me about.your oral Report. about Your OralReport.org. OK. Thanks, thanks, thanks for your report. You can send it to me via e-mail at jennifer@dailymail.co.uk.

ROUGE-1: 21.42, ROUGE-2: 19.90, ROUGE-L: 19.65
BERTScore: 67.12

==============================================
==================== [15/100] ====================
Summary:
The dagger algorithm aims to provide a more principled solution to the imitational and distributional Shi problem. The idea in dagger is to actually run the policy in the real world see which states it visits and ask humans to label those States. The goal is to collect data in such a way that P Pi Theta can actually learn from the data it's trained on. The basic version of dagger works like this and that's the version that you will all be implementing in your homework. It's a very simple algorithm to implement if you can get those labels. It can actually get up to fly pretty reliably through a forest dodging trees. copter rotors to make it do some really complex aerobatic trick if you want humans to control all the joints in a complex humanoid robot that might be even harder maybe you need to rig up some really complicated harness for them to wear. If you want to control a giant robotic spider well good luck finding a human who can operate that. Humans can learn things autonomously and just intellectually it seems very appealing uh to try to develop methods that can allow our machines to do the same. The cost function and the reward function are really the same thing they're just negatives of one another and the reason that we see both sometimes is the same kind of a cultural distinction that I alluded to before remember I mentioned that we have S a which comes from the study of dynamic programming that's where the reward comes from in optimal control. In optimal control it's it's a bit more common to deal with costs I don't know if there's a cultural commentary here well you know optimal control originated in Russia maybe it's more common in America. thing you actually want like reaching your destination or avoiding a car accident and then use those with more the more powerful reinforcement learning algorithms. In future weeks, we'll cover more of the powerful algorithms that we'll be covering in the next few weeks. We'll also cover how to use these algorithms to help you get what you want in the real world. Back to the page you came from. Follow us on Twitter @CNNOpinion and @cnnOpinION. We'd like to hear from you.

ROUGE-1: 33.90, ROUGE-2: 29.54, ROUGE-L: 28.63
BERTScore: 60.09

==============================================
==================== [16/100] ====================
Summary:
Jonathon Gruber: What stops people from just bingeing on everything? It's their budget constraint. He says the median American household has $400 in the bank. Grubers: We'll spend a couple lectures at the end of the semester talking about what happens when people can save or borrow. We'll then come to talking about how consumers make constrained choices, and then we'll end with an example of food stamps, he says. The course is taught at the University of California, Los Angeles. you as well. We write the budget constraint as saying that your resources, your income Y, can be spent on either pizza or cookies. You can essentially devote your income to some combination of pizza and cookies, but you have to consider how much they actually cost in doing that. The rate at which you can trade off pizza for cookies is minus 1/2, OK? That is every extra cookie that you buy, holding your income constant, lowers the amount of pizza you can have by p sub p. The more you spend on one, the less you get of another. By having more of one, we're getting less of the other. The opportunity cost of a slice of pizza is two cookies. And that's the sense in which you're transforming pizza into cookies or cookies into pizza. So essentially, by setting up this marginal rate of transformation in life, you can help with a lot of decisions with a little bit of help from your brain. It lets you set relatively relatively low relative prices and then optimize across those goods. Audience: How do you determine your marginal rate of transformation? How do determine your-- like say it wasn't just pizza and cookies. Like say it was more products. How would you determine that value? JONATHAN GRUBER: That's a great question, and we're going to actually answer that question next lecture very explicitly. Hold on to that question. Next lecture, we'll compare explicitly why income changes differ from price changes and what are the underlying mechanisms. Yeah? our indifference curves. We're simply going to ask what is the highest indifference curve you can reach given your budget, right? So let's consider the same utility from last time. Utility is square root of P times C, OK? And let's ask where can you go with that. And what we see is that point D is the furthest out indifference curves you can achieve while still meeting your budget. And, therefore, we say that the optimum, graphically, is the tangency between your indifference curve and your budget constraint is the optimal constrained bundle. is the optimum is going to occur when we set your marginal rate of substitution, which, remember, we defined as minus MUc over MUp. And this is the fundamental equation of consumer choice. If you understand this equation, you can solve virtually every consumer choice problem I'll give you, OK? That basically, at the optimum, the ratio of marginal utilities equals the ratio prices. That is the rate at which you want to trade off pizza for cookies is the rates at which the market will allow you to do so. The marginal utility is the derivative of the utility function with respect to the number of slices of pizza. The marginal utility of the next slice of pizza is 1 over the square root of 10. Marginal utility of cookies is 2.5 over theSquareRoot of10. Therefore, you should trade in your pizza for cookies, OK? That is a meaningful concept. Utils are not, but that is that, and we only care about it in ratios, so we need the ratio. Jonathon Gruber: You're willing to give up 2.5 slices of pizza for one cookie. He says the market is saying we'll let you have a cookie for half a slice of pizza. "That's what that number means. And that is a meaningful number," he says. "You basically want to trade pizza for cookies until these things are equal" "Eat less pizza. Eat more cookies. That will unambiguously make you happier," Grubers says. how we do constrained optimization. If you understand this, you're sort of done with consumer theory, OK? This is sort of the core of what consumer theory is all about. So that is how we think about constrained choice. Now I want apply it by looking at the example of food stamps. Now food stamps are not actually called food stamps anymore. It's basically a program the government has that provides money for individuals to buy food if they're low income. The poverty line is essentially a measure of what's a minimum level of resources you need to live in America. Economist: If you really just care what makes people happiest, you should give them cash. Jonathon GRUBER: We've run an experiment on this, OK? We're going to talk in this class a lot about empirical results in economics. We literally flip a coin. Heads, you keep your food stamps. Tails, we replace those food stamps with an equal amount of cash. Then we watch what happens. What happens is that people spend about 15% less on food when you give them money instead of food. stop there. We will come back on Monday, and we'll talk about how we actually go from this stuff to the demand curves we started the class with. Back to the page you came from. back to CNN.com home. Follow us on Twitter @cnnireport and @CNNOpinion. Follow CNN Living on Facebook and Twitter. For more, go to www.cnn.com/lifestyle and www.dailymail.co.uk/lpin.

ROUGE-1: 18.29, ROUGE-2: 16.76, ROUGE-L: 15.30
BERTScore: 68.05

==============================================
==================== [17/100] ====================
Summary:
Professor Shelly Kagan: Would you want to live a life on the experience machine? Would you be happy or would you be unhappy, to discover that you actually have been living a life in a scientist's lab? Shelly: There's more to the best kind of life than just having the right mental states, than just getting the insides right. Different theories of well-being might answer that in different ways, Professor Kagan says. But if you're just spending your life floating in the scientists' lab, you're not actually accomplishing anything, she says. a variety of things you wanted. You wanted to know your place in the universe, but you don't even have that kind of knowledge either, because you think you're writing novels, finding the cure for cancer, climbing Mount Everest. You're completely deceived about all those things. So, we might have to distinguish between any old accomplishment and genuinely valuable accomplishments. But again, just put those details aside. We can say that there are certain things that are good above and beyond experiences--the right kinds of accomplishments, the right kind ofknowledge. thing to know what was the average rainfall in Bangkok in 1984. I'm not clear that that kind of knowledge gives a whole lot of value to your life. The crucial point is that it takes more to have the best kind of life than just getting the insides right. Having in your life not just experiences but the right kinds of goods or accomplishments or whatever term we use for it. So, we could still evaluate rival lives. My life would've gone better had I chosen to become a farmer instead. How good it is to be alive is a matter of adding up all of the--call it the contents of life. It's as though we've been assuming, and I have been assuming up to this moment, that being alive per se has no value. How valuable--how well off you are, how valuable your life is, is a function of the contents, the pleasure and the pain. Of course, these are "valuable container" theories, so we have to remember that the mere fact that I'm alive gives my life some value. subtract the ignorance and deception. Even if your content subtotal was negative ten, that doesn't mean you're not better off alive. Modest versions of the valuable container theory say, although being alive per se is good, if the contents of your life get bad enough, that can outweigh the value of being alive so that the grand total is negative. If you're a fan of the neutral container theory, you won't have anything extra to add, because life is just a zero. Bob Greene: It's bad that I'm going to die, but I'm not a monster. It makes me feel even worse that everybody else is stuck dying, he says. Greene: There's a great deal of variation in how much life we get. Some of us make it to the ripe old age of 80,90 a 100 or more, Greene says. He says we care more about being short-changed than we do about being, as we might put it, overcompensated. The story "good to bad" is the kind of story we don't want for ourselves. We want the bad behind us, not the bad in front of us. We worry that because of the unpredictability of death that our lives may not have the ideal shape. A lot of us might feel that a life like this, where we peak but then we stick around--you know, isn't as desirable in which we end with a bang. It's not as though you think, "All right, the dénouement must occur at the very last page" Without predictability you don't know where to put the peak. If you try to aim for peaking later, you might not make it to that. All of this suggests then that the unpredictability of our death adds an extra negative element. It makes it harder to plan what the best way to live my life would be. But then we have to ask, would it really be better to know? would you want the birthmark? Would you want to know exactly how much time you've got left? All Right. See you next time.

ROUGE-1: 17.09, ROUGE-2: 16.16, ROUGE-L: 16.64
BERTScore: 64.40

==============================================
==================== [18/100] ====================
Summary:
Michael Short: How do you know what, let's say, your dose distance relationship is? And how do you calculate all this stuff? So I've actually laser-cut out a little Geiger counter jig from a previous class. And you guys can all do this too. Who here has been to the IDC before? A couple. The international design center-- so they've got a laser cutter that you can sign up to use. And it's set to just take a Geigercounter and put your sources at some fixed distance away. dose and distance or measured activity and distance? Yeah, Luke. AUDIENCE: [INAUDIBLE] r cubed. MICHAEL SHORT: Close. It's, let's say, the measured activity would be proportional to 1 over r squared. Does anyone know why this formula would break down? What happens to our solid angle or our approximation for ourSolid angle is kind of the analog to regular old angle, except in 3D. So instead of looking at things in radians, this has the unit of what's called steradians. The class will be measuring the activity of one banana. One banana contains a minuscule but measurable amount of radioactivity. The ashes of 50 pounds of bananas will be used to boost your signal strength. The class will also be measuring how radioactive is it to work in a smoke shop. The final project is due Thursday, not Tuesday, because we have no class on Tuesday, but whatever. We don't get holidays-- just you guys. Back to the page you came from. Click here to read the full transcript. Most of the Boston area is 21, but once you leave Boston-- MICHAEL SHORT: It varies. From Poisson statistics, you can say that the standard deviation of that count rate is actually just the square root of the count rate divided by time. Let's say you were to measure some count rate in some experiment. How long would you actually have to bring a detector in and count in order to be sure that there's any sort of measurable difference? And so, without deriving all of this stuff about binomial, Poisson, and normal statistics, that's in the reading for today. The slower the count rate, the less certain you can be that the number that you're measuring is actually accurate. So by counting for longer you can decrease your standard deviation. This is going to take forever. It actually takes about 67 minutes, because we've already done this calculation, to get a 95% confidence on 5% uncertainty for this sort of background count. It's actually random. There is no correlation between when one particle leaves and the next particles going to leave. Because it's a truly random process, these errors in the background rate and the gross rate could add together or could subtract from each other. With enough statistics, if you count for long enough or you count enough counts, then these things, on average, are going to add in quadrature. So in this way, you're accounting for the fact that more error in each experiment does increase the error on whatever net experiment you're doing, but not linearly. If you go plus or minus 1 sigma away from your true average right here, you've filled in 68% of the area under this normal distribution. There's actually societies called 6 sigma societies. And the way that they get their name is we're so confident of things we can predict them to six sigma. radiation detector. How long do you have to be here, looking all weird? You want to have an answer. And so if you get some initial estimate of C g, you can tell him this is my approximate t g, at which point he or she will say yes or no, depending on how they're feeling. So why don't we just start, divide by 2, right? Divide by 2. 0.025. We can square both sides. And there's a C n there. Square both sides, and we end up with 0.000625 C n squared. the total uncertainty down to let's say 5% error with 95% confidence, you can't actually run that experiment. Because these uncertainties are added in quadrature, if you're trying to reduce sigma down to a value below that already, how can you do that? You can't have a negative standard deviation, right? So what this actually means is that when you're designing this experiment, even if you count for 67 minutes at 25 counts per minute, that might not be enough to discern the activity. activity in the smoke shop to within some confidence and some error. In New Hampshire, the background count's quite a bit higher, because there's a lot of granite deposits. You can use background counts as a radiation altimeter. One of my graduate students actually built a Geiger counter interface to an Arduino, where you could actually tell what the height you were flying at is by the amount of background radiation increase. So certainly it's going to depend where you are, right? But you want to make sure that you're in an area, to answer Sean's question, representative of where the smoke Shop is. Use this formula right here to estimate how much time you'd have to wait. So for example, let's say you go in there and you get a count rate of 100 counts per minute. You'd only have to count for an extra 28 minutes to nail that net count rate with 95% confidence to 5% error. So do you guys see the general interplay between confidence, percent error, counting time, and counting rate? Who here is built an NSE Geiger counter before? Awesome. radiation quanta or whatever that enter the detector, how many interact, and how many leave out the other side? That's we're going to be spending most of the next month on when we do ion, photon, electron, and neutron interactions with matter. So we'll find out-- what's the probability per unit length that each one undergoes an interaction, what kind of interactions do they undergo, and then we'll complete this actual picture. So you can take a source of, let's say, unknown activity, put it a known distance away from a known detector with a known efficiency, and back out what the activity of that source is with accuracy. on average. Then you can get to the actual activity of the source. Once you know the activity of this bag of bananas, you can then divide by either the mass of one banana, or the number of bananas. That's what we're going to spend the rest of today doing. So since it's getting on five out of five of, do you guys have any questions about what we covered today or what we are about to go do? AUDIENCE: You said that for solid angle you wouldn't do this. MICHAEL SHORT: Yep. thetas are actually subtended by your detector. And the value of that actual surface integral gives you the real solid angle. That's the super simple one if you just know the area of something and you know that you're kind of far away. But again, whenever possible, use the exact formula. So any other questions? Yeah, Sean. Sean, what are the units of cobalt 60 [INAUDIBLE]?? MICHAEL SHORT: It's just two gamma rays per atom. per disintegration. So you've got to know what material you're looking at in order to know how many gamma or how many betas or more that you're going to get. Follow me to the counting lab. These are three high-purity germanium detectors. They're hooked up through a little electronic box and go into the computer over there that does all the peak height analysis. The idea here is that, again, if you just add the errors up, you're probably overestimating the error and selling yourself short. out the thermal noise. Because you're looking for really tiny little signals here, so you cool everything down. And that way, it's not too noisy. These guys are OK warming up. It doesn't destroy the detector. The old detectors you had to keep cold all the time. And if they warmed up, then they were just paperweights. So this is just the counting lab. I've got an actual sample counting in here right now. We'll take a look at the spectrum in a minute. Tungsten 186 activates into tungsten 187. So you could, knowing how big that peak is, what the efficiency of the detector is for collecting that peak in that geometry, the half-life, the cross set-- that whole mess of parameters-- back-calculate how much tungstan is in the sample. So that's kind of how NAA works, which I assume you've explained. MICHAEL AMES: But that's not how I do NAA. When you're running NAA, you really want to avoid having all these fast reactions. There's usually an energy threshold for the fast reactions, like 1 meV or so. If you're near the reactor, you're also getting some fast neutrons, which can give you an n p reaction. And that's a pain in the neck, because if you've got iron, you've always got a little cobalt floating around-- you maybe need to do a correction.any questions for Mike on what you've just heard? Well-timed, because we were just talking about this stuff all week. that's how I measure nickel, using n p reaction. And I need to put the rabbits into where I've got a fast flux in the reactor. Other analytical methods have gotten a lot better, and so they've kind of caught up to NAA. So the environmental side of this has kind of quieted down. But it's still useful for a bunch of things. And so I do some work here now. I also work in the NCORE group. So that's a lot of my time, rather than just this lab. Fish samples that we actually did the fresh fish samples. And you want to kind of homogenize those. And we had this kind of titanium blender-- you remember the Bass-O-Matic? We had this titanium blender that we dropped the fish in. And then you completely homogenized the fish, and then you took a little sample of it, and freeze dried it. Then analyzed it for mercury. MICHAEL SHORT: [INAUDIBLE] MICHAEL AMES: Yeah, right. Because, I mean, the rabbits are only this big, and the samples I want are only that big. "This is just a fine powder. And it's fly ash from a coal-fired power plant. Fly ash means the ash that goes up the smokestack, as opposed to bottom ash which is what falls down. And so, they collect a whole hundreds of kilograms of fly ash, just homogenize it, sieve it, send it out to a lot of labs to analyze. And because we did a fairly short of radiation, after a while the activities died down and we gave the samples back. And we found that it didn't correlate with the well water or the time when the contamination was the worst" the reactor to the core of the reactor in the graphite. I usually run shorts. I'll usually irradiate for about 10 minutes. We usually let the sample sit in the reactor for a little while. So the very short half-life stuff decays away, and then it comes back out here. And then pop open the rabbit, and in that hood, pull the samples out. Iusually try to repackage the samples. Because then I can take it out of whatever it was irradiated, put it in a clean bag or vial. it on a detector, and we count it. When we're doing shorts, I'll irradiate two samples at a time, because I have two detectors. I'll do a fairly quick count-- five minutes-- right after I get the sample down there. The shortest half-life I look for is for aluminum. It's 2 and 1/4 minutes. But things usually have a lot of aluminum in them, so I see aluminum pretty well. I usually do a 10-minute irradiation for shorts. NAA is good for rare earth elements, which are hard to measure by other methods. By picking out various rare earths and the ratios, it can help identify where things are from in the world. Michael AMES: My usual description of what size sample I like is if it's a piece that you would pick up with a pair of tweezers. "We'll look at what comes in, and-- yeah, I might veto some things or not. But we'll see what we got," he says. working on and myself. The full size bricks-- like, that size, 2 inches by 4 inches, by 8 inches, weighs about 25 pounds. There's usually a bunch of them floating around. The dangerous thing is dropping lead bricks on your feet. And they move much bigger things in the reactor. And that's the other dangerous thing in there, dropping really big things. We've never dropped anything that big. I think somebody dropped a steel plate on their. And you know, these have been here longer than I have. foot once. That was about the worst of it. You know, like, four-foot, half-inch steel-- boom. That's what happened to my foot. The only way you can actually do these manipulations are if you're in my training program-- I'm the training supervisor for the facility. And the program you guys are in fits that definition. So I just want to show you the controls. To actually do this experiment, we need two licensed people in here, one at least has a senior reactor operator. The experiment is basically change reactor power by half a megawatt. We're currently at 500 kilowatts and we're going to bring the reactor up to 1 megawatts and then bring it back down. We have about nine different instruments that tell us what the reactor power is at all times. We'll show you the proper way to make the entries and then do the actual movement itself. And since it's linear, it'll be double that-- so 17.1 megawatts. The reactor is on autocontrol. When we do these manipulations, the reactor operator is going to take manual control. That'll cause an alarm to come in. And this will only happen for the first time. We're a factor of 10 lower than where we would automatically scram at so it would be very difficult for you to get to someplace where it would cause a problem without us being able to stop it. If, at any time, you don't feel comfortable doing something, let us know. up a valve and let more neutrons in. And when you get to the place where you want to be, you basically close that valve again. So you basically add reactivity and then stop that reactivity addition by bringing the absorbers back to about where they started from. When she's done, the shim blade will end up about at the same point where it started, the 13.42 inches out of the bottom of the core. It might not make it all the way back up to [INAUDIBLE].

ROUGE-1: 29.64, ROUGE-2: 28.51, ROUGE-L: 27.02
BERTScore: 67.42

==============================================
==================== [19/100] ====================
Summary:
Professor: We're going to start with a short review problem on rugged landscapes. And then we'll get into the core topic of the class, which is evolutionary game theory. We'll discuss why it is that you don't need to invoke any notion of rationality, Professor says. Then we'll try to understand this difference to know what a Nash equilibrium is in the context of game theory versus an evolutionary stable strategy in this context. And we'll say something about the evolution of cooperation and experiments that one can do with microbial populations in the laboratory. Professor: We're going to start in the 0, 0 state with 1,000 isogenic individuals. And the question is, what's going to happen eventually? And in particular, what path will be taken on this landscape here? Professor: You can start thinking about it while I write out some possibilities that we can vote for, and I'll give you a minute to think about it. If we wait long enough, the population will get there, and the 1, 1 genotype will fix in the population. the other of them. The probability of getting both mutations in one generation is going to be 10 to the minus 12. And then there's another question, which is, will 0, 1 actually fix in the population before you later fix this population? And actually, I think the answers to all these questions are in principal already on the board. And really, this is in some ways a very simple problem. But in another way, you have to keep track of lots of different things, and which regime we're in and so forth. a group of D's and a group of B's here, which means that everybody-- AUDIENCE: Let's fight. PROFESSOR: All right, so everybody thinks that everybody agrees with them, but you just need to look a little bit more long distance. So turn to a pseudo-neighbor. You should be able to find somebody there. It's roughly even here. So I don't see much in the way of vibrant discussion and argument. You guys should be passionately defending your choice here. Professor: In order for this individual to fix, he has to survive stochastic extinction. And the 1, 0 individual has to go extinct, which happens 90% of the time. So this is, indeed, answering the question that if you had one copy of each of these two mutant individuals in the population, that's the answer. But that's a slightly different question than if we ask, we're going to start with an entire population at 0, 0, and now these mutations will be occurring randomly at some rate. In a chemical reaction, where we have some chemical state here, we have two rates. There's the k going to 0, 1, the k to 1, 0. And that's everything we need to know to calculate the relative probabilities of taking those states. In this case do we have to worry about going backwards? No. It's very unlikely. In practice, it doesn't actually matter, because this acts as a ratchet. Because all these mutations are non-neutral, once you fix this state or this one, you can't go back. big number is our problem. So we should be able to figure this out, though. Because this new r is 1/1.02. OK, so there's less than a 1% probability of it fixing. Is this believable? AUDIENCE: It's about right. PROFESSOR: Yes, sorry, I was just saying this doesn't-- so this is why I'm saying you always check to make sure that your calculation makes any sense at all. So it's not this. But it's tiny, right? fixation of a neutral mutation. This is a deleterious mutation. It's not even nearly neutral. So it has to be much less than 1 over N, right? So this whole thing is 10 to the minus 10, or something like that? OK, 4 times 10 to minus 8? Well, OK, whatever, it's something small. So this times the probability of fixation, which is 10 minus 9-- this is how you would calculate the rate of going backwards. rate, you don't even do successive fixations. So it may be that neither state ever actually fixes, because it could be that the 1, 0 state is growing exponentially, but is a minority of the population. Instead, you simply have mutations that sample different strategies. And then you have differences in fitness that just lead to evolution towards the same solutions of the game. It's evolution to the game solutions, so the Nash equilibrium, for example. And we'll see how this plays out in a few concrete examples. it can happen. But then it's easy to then go back to the lab and forget that it's true. The basic insights are all intact. In practice, you basically get the categories of outcomes that we saw there. So maybe what I'll do is-- so what we're going to do is think about competition between two individuals A and B. And often, we talk about these things in the context of the two-player games, where we have A, B, C, D. In a symmetric game, everybody interacts with everybody else with equal probability. Depending upon the strategy that these guys are following, you get different payouts. The Prisoner's Dilemma is the standard model of cooperation in the field of game theory. If you set up these jail sentences in the right way, then it could be the case that each individual has the incentive to confess, even though both individuals would be better off if they cooperated. In these questions, they're actually allowed to lie to the person being questioned. is indeed the Nash equilibrium. It's to do this strategy D that we're saying here. All right, now the question is why? And part of the challenge here is just understanding how to read these charts. Now, if you look at this chart, you say, well, gee, that is a shame. Because 1 is just not the biggest number you see here. And indeed, the important point to note here is that if both players had followed this strategy C for cooperate, D for defect, then both individuals would be getting fitness 3, or payout 3. In a population, if you have genetic A's and genetic B's that are each giving birth to their own type, then you evolve to some coexistence of genotypes. So long as you have some members of both A and B in the population, you'll always evolve to the same equilibrium. Whereas the mixed Nash equilibrium-- this is a situation where you have, in principle, genetic homogeneity. This is a single genotype that is implementing phenotypic heterogeneity. And indeed, one of the things that we've been excited about exploring in my group is this distinction. If the population or the opponent is playing this Nash equilibrium in these games, then it just does not matter what you do. You can do A, actually, in any fraction. So since A and B have the same fitness, you can choose between them at any frequency you want. So that's saying it's a Nash equilibrium. It's not a strict Nash equilibrium, but it's equal to. The condition it has to be greater than or equal to, which means it is a Nash. equilibrium. not the definition of that. But this thing is true, which means that it's a Nash equilibrium. And if you have questions about this, I'm happy to answer it. It's explained in the book as well. We are out of time, so I should let you go. But good luck on the exam next Thursday. If you have Questions and you want to meet with me, I am available on Tuesday. So please let me know. I'll be happy to talk to you.

ROUGE-1: 22.88, ROUGE-2: 22.14, ROUGE-L: 22.36
BERTScore: 65.83

==============================================
==================== [20/100] ====================
Summary:
In this video i'm going to be covering sinus tachycardia. sinus tells us that we're dealing with a heart rhythm that originates in the sa node. The sa node is really the starting point of the electrical conduction system. If this process is occurring like it should it should cause this heart to beat at about 60 to 100 beats per minute on the ecg. On the flip side let's say that this sa nodes is disease it's causing the heart to beating slowly. Sinus tachycardia can be caused by disease or non-disease related causes. It can be a sign that a patient's disease is worsening. It's a warning sign that hey this needs to be investigated more. If a patient has sinus tack you want to investigate it you don't want to just write it off. It may be their thyroid is yelp and any time they have pain that is not being controlled it can definitely lead to sinus tachycardsia.

ROUGE-1: 8.14, ROUGE-2: 7.13, ROUGE-L: 7.51
BERTScore: 63.43

==============================================
==================== [21/100] ====================
Summary:
In this problem, we're given a collection of 10 variables, x1 through x10, where each i, xi, is a uniform random variable between 0 and 1. And we'd like to develop a bound on the probability that some of the 10 variables being greater than 7 using different methods. In part A we'll be using the Markov's inequality written here. And in part B, let's see if we can improve the bound we got in part A using the Chebyshev inequality. In part B, we saw that by using the additional information of the variance combined with the Chebyshev Inequality, we can improve upon bound given by Markov's Inequality. Now, in part C, we'll use a somewhat more powerful approach in addition to the Chebyhev inequality, the so-called central limit theorem. We're asked to compute the probability of x greater or equal to 7, where x is the sum of 10 uniform random variables between 0 and 1, so we'll call it xi. that even with 10 variables, the truth is more like this, which says that the distribution of x concentrates very heavily around 5, and hence, the probability of x being greater or equal to 7 could be much smaller than one might expect. That is, x is more likely to be greater than 7 than it is to be less than 7, and so the probability is much lower than one would expect. It is also more likely that x is greater than or less than 5, rather than equal to 6 or 7.

ROUGE-1: 28.01, ROUGE-2: 24.51, ROUGE-L: 22.98
BERTScore: 74.70

==============================================
==================== [22/100] ====================
Summary:
So now that we've combined pulley A, string 2, platform, and washer as our system, we can now address our question. If we measure the acceleration of the person, what is the force that the person pulls the rope down with? Well, of course, that will just be the tension in the string. And with this simple system,we can now apply Newton's second law, F equals ma. And so by thinking about how to choose a system, what could be a very complicated problem, with lots of equations, is simply one equation.

ROUGE-1: 49.74, ROUGE-2: 48.95, ROUGE-L: 49.74
BERTScore: 80.28

==============================================
==================== [23/100] ====================
Summary:
In order to be successful whenever you're drawing blood or starting IVs you really have to know a couple things number one you need to know the name of the vein that you're going to use and its location along with what can that vein actually handle. Some veins can only handle about a 20 or a 22 gauge IV cannula versus some of them can handle 18 gages 16 gages. I also like to use accessories cephalic vein along with the median vein of the forearm and of course those hand mains the dorsal venous network.

ROUGE-1: 13.00, ROUGE-2: 12.62, ROUGE-L: 13.00
BERTScore: 67.49

==============================================
==================== [24/100] ====================
Summary:
When we talk about complement and substitute, we have to be clear whether we are talking about the demand side or the supply side. So, what do we mean, when do we say a good is complement in production or complement in supply? Can you give an example, first substitute think about it? Plastic chair to iron chair like. Boeing is a manufacturer of airplanes; it makes civilian airplanes as well as military aircrafts. If there is an increase in price of military aircraft, what would happen? Boeing devote more a space to military. To manufacturing, to manufacture military. aircrafts, it would go down. So, now we have enough knowledge. So that, to talk about the factors affecting the supply schedule or supply function. So, now that we have that knowledge, we can talk about how to get the most out of the resources that are available to us. That's what we're trying to do here. We're going to try and get the best of both worlds. We'll see how it goes from here. It'll be interesting to see how we get on with it. We've got a long way to go, but we're getting there.

ROUGE-1: 40.39, ROUGE-2: 29.58, ROUGE-L: 29.99
BERTScore: 62.83

==============================================
==================== [25/100] ====================
Summary:
Rebecca Sae: I think that when people talk about social cognition they do actually mean all of those things. Rebecca: Trying to get a coherent account of everything from your hand motions and your perception of other people's hand motions all the way to politics and sociology is daunting and, frankly, deeply unlikely. Rebecca: I'm going to take almost the opposite approach, which is say, there's one thing that's probably there a lot. Let's try to study that one thing in many different ways and contexts. In an experiment, I ask you guys to make a moral judgment of a character. Her name is Grace. She's on a tour of a chemical factory. She puts some of the poison in the coffee, and when the girl drinks it, she dies. Now how much blame does Grace deserve for putting the powder in the Coffee? The more wrong it was, the higher your hand goes. And everybody has to vote. In the story I told you, everything was the same from the beginning, the scenario where Grace was. been studied from kind of relatively simple perceptual phenomena like assigning intentions and goals to simple moving characters in an animation. This ability has been studied all the way to understanding some of the most complex, abstract ideas that we ever encounter. So to the degree that our minds let us make any sense of that at all, we're using our ability to make sense of other people's minds. And I'm almost exclusively going to talk about the first one, so how we think about what other people see, think, and know-- but not want or feel. The false belief task was set up as a litmus test for our ability to think about other people's minds in the late '70s. The reason why this task became so famous is that not all participants perform the same way. The question of whether this capacity is shared with other animals has gone on continuously since the late 1970s and has not been resolved. And so one class of participants who've become the focus of intense scrutiny is slightly younger kids, namely three-year-olds. FMRI has been both an incredible gift to our ability to understand the human mind and also imposes a huge number of limitations on what we can discover. And so one question is-- is theory of mind, the ability to think about other people's thoughts, in any sense its own problem? Or are we just studying the whole problem of human intelligence and capacity? So that was sort of the first question that we set out to answer. We and a number of people did this. And the way we did it is that we had people in an fMRI machine doing basically an adult version of the pirates task. belief. This is a very simple encapsulation of our ability to represent what somebody else thinks and separate it from the state of the world. We're using everything from your eyes to your fingers and most of the brain in between. And then the question is, the part that required you thinking about thoughts-- is there any sense in which that's special or different from the whole rest of the logical and cognitive capacity of your brain? To ask that question we designed a control condition in which you similarly read stories that involve something that was true and becomes false. The right TPJ is selectively involved in theory of mind, and so selectively depends on all the experiments I didn't show you. The signal is ridiculously strong and reliable. The difference between thinking about somebody else's thoughts and other logical problems is comparable to the difference between looking at gradings and not looking at gratings in V1. It's just this unbelievably strong signal, literally unbelievable. It should not possibly be true on any a priori story, except for maybe the story Ken just told you about how social cognition is the fundamental basis of everything. V1 is called V1 because information goes from your eyes to the LGN of your thalamus. It's the first cortical stop of visual information. One way that we know that it's very involved in vision is that if you're seeing visual stimuli, you get a big response in V1. But that's a selectivity type measure. What we want to know about V1 is what transformations over the information coming from LGN is V1 implementing. That's why theories like Marr's theory-- which say that there are receptive fields, that it depends on the contrast, the position, and the orientation of the information in the field. that prefer a different orientation. And so from the relative activity in those two voxels, we could still tell you the orientation. That was the intuition behind multivoxel pattern analysis when it was first proposed. And it's now sweeping the fMRI world, many different versions of these analyses. And I'm just going to show you two different ways MVPA is done concretely in my lab to try to get you more of a sense of what's going on. And we can come back to these more general issues of what it's measuring and what that means. If MVPA was ever going to be able to decode a feature of other people's mental states, we should start there. That was the idea. In this experiment, we make one tiny change. So we make, for example, a change from you had no idea to you knew. We change on average two to four words in this whole long scenario. It's a complicated stimulus, but the change we make is very small and totally changed the meaning of the whole.that judgment. story by just changing your mental state. Whether you knew or you didn't know about your cousin's peanut allergy is really important to the moral judgment of what happened. The right TPJ is tracking the important information about what you think. And so it's activated for both of these kinds of stories. So that's a univariate analysis. Now what's a multivariate analysis? So here's the key intuition behind a mult variables analysis. The idea is, think in a very abstract similarity space. And the intuition is, because in both cases it matters what you Think. person who wrote the essay was in the room when you said that publicly shaming thing? A different story is about demonstrating your karate skills and knocking out your classmate-- again, totally new moral scenario. But again, this one feature-- did you know or not know that your classmate was there when you did the kick? Now here's the idea. Even though each of those new scenarios is completely different, if there are different subpopulations within your right TPJ responding when you knew you were going to cause harm-- compared to when you didn't know. The amount of activity in the right TPJ is a big signal. The relative activity between one voxel and another is a tiny signal. And it's superimposed on a lot of noise. But if there's anything there at all, then you'll still be able to pick up a little more similarity for pairs that are matched on the feature of interest. That's the logic behind a Haxby style analysis. The first question is-- is that real, or is that a coincidence? That is the first thing you want to know when you see data. did it is, we said you knew about the allergy or you didn't know about the allergies. And what you want to know is, are you decoding the abstract thing-- that she knew she was causing harm or not-- or something less abstract, like whether the story has negation in it. And so in experiments B and C, we had done it this way. It's also in the third person, not the second person. So if we find the same result, then it generalizes across all these incidental features of the way the experiment was run. MVPA is a technique for getting more information out of the same data. It's an analysis technique, not a way of getting data. People are reading these stories, and they're making moral judgments. Some people tend to go more with what the person thought, whereas others go with what they caused. Everybody agrees that it's worse to kill someone than to not kill them. But the moral judgments of these stories vary across people. The right TPJ contains information about whether or not the person who committed the murder knew what they were doing at the time. knowingly murder than to unknowingly murder. But there is variability in how much worse. Some people think that basically what you thought you were doing is all that matters in these stories, whereas other people think both of those things matter. So there's individual variability. And one thing that we can look at is, how does the. individual variability in the behavior relate to the individual. variability. in the representation. Does that make sense? They're pretty correlated. The more that you represented knowing harm as different from unknowing harm in your right TPJ, the more you judged them as different when we asked you for moral judgment. Haxby style correlations were the first form of MVPA introduced. They were introduced by Jim Haxby in 2001, so actually a long time ago. The idea is, take a region you care about and ask this basic question. Is the correlation across neural responses more similar when the stimuli share that feature than when they don't? And so in the right TPJ, what we found is that stories about seeing are more similar to other stories about hearing than when you cross that feature. This particular method, using spatial correlations, is very stable and robust. But it's a special case of a much more general set. There's a whole bunch of limitations of Haxby style correlations. The answer you get for anything you test is that there is or is not information about that distinction. There is no continuous measure here. It's just that two things are different from one another or they are not different from each other. And so once people started thinking about this method, it became clear that this is actually just a special cases of a more general way of thinking about fMRI data.

ROUGE-1: 29.69, ROUGE-2: 28.35, ROUGE-L: 27.67
BERTScore: 65.17

==============================================
==================== [26/100] ====================
Summary:
Joanne Stubbe's lab works on the only cool enzyme in the world-- ribonucleotide reductase. It's the only way in all organisms that you make the building blocks de novo that are required for DNA biosynthesis and repair. If you inhibit this enzyme, you have no building blocks. You can't survive. So from a practical point of view, it's the target of drugs they use therapeutically in the treatment of cancer. And I think in probably not so distant future in the antibacterials because there are sufficient differences between humans and bacteria reductases. and do the same chemistry, but they have different metal cofactors depending on where they evolved. The function in all cases is to generate a radical in the active site and then the chemistry is the same in all these things. And the function of the metalcofactors in all case is to create a radical, which is the key to the chemistry in all of these cases, says Dr. Michael Bociurkiw, a professor of chemistry at the University of California, San Diego.

ROUGE-1: 38.51, ROUGE-2: 33.92, ROUGE-L: 31.74
BERTScore: 70.88

==============================================
==================== [27/100] ====================
Summary:
GILBERT STRANG: Differential equations is the big application of calculus. He says it's interesting to see what information and ideas from calculus actually get used in differential equations. Strang: You really do need to know basic derivatives, the derivative of x to the n, the derivatives of sine and cosine. And above all, the chain rule, he says, that's really-- that it's chains, not just derivative of f plus g, but derivative of g plus f. of functions that really blow open the functions or we can deal with. The derivative of the integral of a function is this. Here is y and the integral goes from 0 to x. The x is the limit of integration. I won't discuss that fundamental theorem, but it certainly is fundamental and I'll use it. I just like that the use of the fundamental theorem. OK one more topic of calculus we need. This is the tangent to the graph. And what we need is y of t plus delta t. function because that's a nice function, e to the t. We can recreate that function from knowing its height, its slope, its bending and all the rest of the terms. That's called the Taylor series named after Taylor. Kind of frightening at first. It's frightening because it's got infinitely many terms. And the terms are getting a little more comp-- for most functions, you really don't want to compute the nth derivative. But the formula is beautiful because you see the pattern, that's really what mathematics is about.

ROUGE-1: 25.08, ROUGE-2: 23.11, ROUGE-L: 23.75
BERTScore: 71.79

==============================================
==================== [28/100] ====================
Summary:
Last lecture of winter term course on front ends. How to determine if a constraint is likely to be a correct one. What kind of conditions should such a constraint fulfill or the the metrics of the local environment fulfill in order to reduce the probability that this is actually an outlier mesh. What my goal is for today I will introduce three kind of small front end systems on a very abstract level just giving you the idea on how they work with different sensors and then in the second part of this talk today I would like to stress on what kind of condition should such an constraint fulfill. when the robot observes the same part of the environment and so far we always assume these constraints are given so these errors here we assume to be given and today we would like to look at the case how do actually generate those arrows. The two main components of the slam system are the back end and the front end. The back end optimize the graph and returns a new node positions back to the frontend. The front end uses this information together with the new centre information to generate new constraints. to begin by matching observations so we have different observations depending on what platform that can be whatever stereo camera. Different types of sensory modalities and for every sensor of course there's a different way of obtaining those constraints and those constraints may take into account what the sensor actually sees how kind of unique is the data that the sensor generates for specific area or it's a look all corridors exactly the same if you only rely on laser range data and the corridors all look the same from the Rays range data point of view then we may take that into account. one feature or one landmark and the robot map those landmarks by checking for say fitting circles into the laser range data and whenever it found a good circle say that's likely to be a chunk of a tree and use it as a landmark the third class of approaches uses feature descriptors most popular ones are for example sift and surf. These are two features which you can extract from image data and which kind of describe the local surrounding of the place where this descriptor is computed. You can match surprisingly large databases of images using those descriptors. moment and that's my sensor range I can compute where are those other pulses so in this case B 1 and B 2 just two examples could be more obviously and then I can also estimate what is the uncertainty of those poses B 1 or B 2 relative to a do that by eliminating the note a from my linear system and then inverting the resulting Hessian and looking to the main diagonal blocks this gives me the uncertainty here indicated by these dashed lines. Based on this information I know I can never have an estimate of given my current pose where's b1 where's B2 together with The Associated uncertainties certainty estimates. was here indicated with a which can where I could I can obtain by inverting the hessian. In practice this is a pretty expensive operation so you actually want to try to avoid inverting this larger matrix. So you can do an approximation what actually most system in practice do to do that more efficiently. This does is ignores the loop closures so the uncertainty estimates are too big but I can compute this extremely efficient and I may inspect a few places too much but I should get all the places which I need to inspect. A very simplistic front end which will try to identify constraints which uses iterative closest point so scan met a former scan matching and tries to find a match. If this is above a certain threshold I accept the match and if this is the same here I go that's a constraint. If your customer and I've proposed you this solution what would be maybe your argument against that would you buy that this approach which I propose you good what do you see potential problems of this approach from given what you have seen so far absolutely correct. ICP is a combination of the iterative closest point algorithm and the initialization. ICP is sensitive to the initial guess and as a result of that we may end up in a local minima so in something which looks like a match but in reality is not a match. Other things we may identify is how do i sample possible locations where the platform can be if this is kind of the uncertainty is a very large area I may need to sample a lot of different poses in order to do that efficiently. showing you three different examples of systems that we have built here in Freiburg. Some of the mapping techniques we developed here have been used to at least tested on that car so this is a pioneer a two robot which has a two d-day the rangefinder sitting here and sits on a pencil unit so it moves always like this song so it's called a nodding laser and this way generates 3d data you get 3d information about the scene and then it tries to build sorry a 3d map of the environment using this technique. have less local minima so we match those Maps and get a six degree of freedom constraint in this case XY that your patron roll so six dimensional constraint and then we this is actually one of those constraint here then we can accumulate all constraints and then do a graph optimization and obtain a local map so this also a parts of this is building 79 building 51 52 the Mensa building and of course some parts here the green area which robot hasn't seen how does it align those two scans of these are two examples of two scans. darkest stripes these are simply small alignment errors of these individual maps they can they can see kind of small steps over here here that was also probably an alignment error which simply leads to her step which was let's say bigger than and all five centimeters in the ground and therefore it's classified as not reversible anymore. Everything is red over here but what do you see here for example other the bikes which are parked over here and the individual trees here something it's like to have gone wrong or maybe there was some copy. is a parking lot or a 3d model of a parking parking lot where yellow again means drivable areas and red means non drivable area and then you can actually use this this map over here in order to localize the vehicle. So these were kind of in this case 1600 local 3d maps and this was this in this example done with the grid for 20 by 20 centimeter grid cells and this by lining those grid cells you canactually get maps off and say this quality that's something you can expect to get with this technique. an initial inertial measurement unit and one of the advantage of this system is it gives you also the gravity vector this quite accurately at a high frequency. If you know the gravity back door you can eliminate already two of the six dimensions from your state space. The roll angle and the pitch angle can be determined but just by knowing the gravity so you have only one angular component and XY that that you need to estimate and therefore adding this IMU to the tourist error system is is highly advantageous. Surf features provide a local description of the scene of a small noise a scene of the small part of the image and so if you see every of those for each of those points here one of those descriptive Alice is computing their computer. Based on the position of my stereo camera I try to build a local model of the surrounding so what we want to estimate is the X Y that and three angles your roll pitch and yaw. By knowing the gravity vector I kind of get rid of the roll in the pitch and this reduces my problem from six dimensions to four dimensions for every node. The technique is used in three different ways in this approach. The first one is for for visual odometry so there is no wheel encoder on the camera. The second is for matching your current observations against a small part of the environment. The last part which is loop clothing so given I kind of I don't know where I am some a large uncertainty and I can use this approach to see how well do the features that I see at the moment match the features I've seen in the past. Tasker builds a map online and use the map in order to make navigation decisions of where it should actually go. The ICP is sensitive to the initial guess so one thing you can do is try to find arrange things into Maps instead of single scans this helps. If you have descriptors like feature descriptors it can actually help you to find good estimates where you can be so you don't have to try all camera polls and see if the camera poses match you can use your feature descriptor to already pre select images you may consider for for potential loop closures. talk which I Neff was more over more whatever like wait overview about how different approaches work was not going to too many details. The second part of the talk today I would like to talk about ambiguities in the environment and what are good ways for dealing with them and how can we actually build accurate maps consistent maps of the environment so they are are so or the main assumption here is not we simply ignore all n big you T's and say the environment has no ambiguisms and I just consider they are none of them. be the same place but there might be something else which looks exactly than in this place a here so it may not be a good idea to add this constraint unless we have seen all this part over here. In order to make this constraint view maybe it's better to first explore all this scene over here before we can make this data Association. There may be different places where the system can be which are just which are which do not intersect with the place I'm currently considering and therefore I should not do a match but you could you could. or not so maybe a bit imprecise from what I talked before so there here is a global ambiguity which is something we don't want to have and they are they example if the uncertainty is small and I have this Metro they say okay that looks good that's kind of what's called global sufficiency so the opposite side so this is a globally sufficient match so there's no other place in the uncertainty area of that node where the system can fit in these are the things I am interested in finding exactly those it's all situation and say simply the match can't be anywhere else. hard for me to you to to identify and this is also called what's called the picket fence problem so good offense you seem you don't know which part of the fence matches to what you see so far it's a very very long repetitive structure. These are things where you also don't want to add a constraint the curses simply do not know is this is this locally ambiguous or not. You could use the max mixture approach and say simply it'm a multi-modal constraint that I may add I'm either here here or here. The approach that every Doulton and his team proposed we're saying okay we have all slam back and it gives me an estimate into the prior the current estimate of the of the graph that I have and I do a it does a post poll scan measuring very similar to what you've seen so far. Based on the scan matching we can actually do a topological grouping so pulses which are nearby which are in the same part of the environment I just grouped together it's kind of can see this is a small local map Zushi. good fit maybe yeah doesn't sound too bad just add them to kind of a temporary constraint list and this is shown here in red so this one can Michigan this pot this post this post is against this opposes both this post and this guy again. Some of them will be likely to be right some of them are verylikely to be wrong. The questions how do I identify which one a right image or not wrong again the first thing we do is we want to test for local unambiguous so we take one of those groups and check okay. How do we actually get those locally consistent matches that's one of the key questions here the the key trick in here is we have a large number of constraints pairwise constraints between nodes. We want to find we want to check in to how many consistent subgroups are there so if I can assign a kind of a group ID to every constraint and the goal is that among one group within one group they all consistent with each other that's would be the perfect thing. The idea is to say okay in order to check which are consistent I need to check if they kind of transform the environment in the same way and it's done here by taking what we call the prior edge of. these are those add edges which result from odometry or incremental scan matching if I start from this node over here I can take my little madama tree constrain to go here I take my constraint HJ to jump into the second trajectory for the point in time when I visited the place a second time move along the odometry again and then go back kind of with the inverted H I and go back to the same place. If I have this kind of loop of constraints if they are all perfect and agree and consistent I should add up at an identity transformation if I concatenate all of them. and how accurate can you actually align your scans so of course Moodle oh that yeah okay so I have whatever a number of those hypotheses what I can do is I can actually build up my matrix a I J where this simply depends how consistent are the hypothesis using the hype of this I and a hypothesis J together with the odometry so this is this is IJ so I just cannot make this walk around in my graph and say how close am I with respect to the identity if I'm closer than you say that's pretty good if I're far away from identity as they are something has gone wrong here. and if you don't understand what the matrix means it will be hard so every entry of this matrix IJ tell us how well do hypothesis hypothesis J agree with each other just looking to this this pair of it's just a pairwise consistency mention the small if they're small Wireless in there I mean they don't agree they are high values and Daisy but they agree that may be good again so the goal is just to find this vector and then later on find a way and how can we determined what V this vector be all right. once once in this vector and I will get a high score if I have ever two groups in there I have I get one. I get scores among the groups but not between each other. I divide by again a large number of apostasy so we get a small value. I have this function just high values for both elements and low values for bad hypotheses so what can I do again treated as an optimization problem. I try to find the vector B which maximizes this fraction that's exactly what is done. problem I should come up with a very similar solution there's no guarantee that this is the case so it is definitely an approximation is a different problem that I solve if I go from discrete value from this credo from zero and once for binary variables to continuous variables but the assumption that I'm not too far away but there'sNo no theoretical guarantee for that ok how do I maximize this function i compute the first derivative I set first derivative to zero they don't want to go into the details how the derivative is obtained but it turns out solving this equation is equivalent to solving this equations. and if I have multiple solutions for that get MA multiple. The larger the eigen values are the better the score so there's a proof that i get a perfect combination. If I visualize this so one situation over here so this is kind of the first eigen vector second eigenvector third force this is Lambda i. If you have more than one solution I can just say okay what the ratio between the largest eigenvalue and the second largest eigenvalue. If this is a value which is let's say 1 or between 1 and 2's again yeah this is very likely to be a picket fence prom. second time this is the case I mean say I can't be sure that this is a concern it could be a constraint but I'm not sure that it really is a constraint because there's simply an area in the uncertainty on the relevant uncertainty area which I haven't seen so far. What we would in theory need to do we need to really check this area I can see if we can fit it in here an approximation for that is just compute the size of the ellipse on this circle and theEllipse over here. do the global ambiguity test or global sufficiency test so it's a globally sufficient order they say a global ambiguity if there's number you D don't add anything and otherwise I have a loop over which I can say with a high likelihood it's globally consistent and there's no local and acuity and that's a way actually most a lot of different slam system works also the SEM system that we use in here you've seen this video already where the system we have the robot mapping our campus over here. strongly depends on the sensors that I'm using and I need to the better I can exploit the individual properties of my sensor the better it is. The approach of this global ambiguity tests and the local ambiguity test these are important things that a good front that should consider in order to avoid adding false positives constraints. This is done with a single graph partitioning approach this was kind of the techniques there the technical - Olson which uses this and yeah again so regarding the the position uncertainty of the platform the higher the uncertainty is.

ROUGE-1: 38.80, ROUGE-2: 37.70, ROUGE-L: 38.26
BERTScore: 66.84

==============================================
==================== [29/100] ====================
Summary:
Marketing is about four things creating communicating delivering and exchanging value. Value is a function it's a function of the price the quality and the benefits so let's make sure that we don't think that value means low price. Marketing is about creating right excellent good job. It's about creating communicating and delivering value. It is about exchanging value that's in chapter one. marketing is about creating communication and delivering value. Marketing is about creating communicating deliver value and exchanging value that's capital n a. for something to be of a good value it doesn't need to be a low price it could be a high price but it's a very high quality and it has a lot of benefits do you agree who could explain that further good tell us your name albert alvin alvin  alvin albert i'm getting bad coaching here alvin go aheadTell us your items yes luxury items so if you purchase a 55-inch samsung 4k high-definition led monitor that has smart capability and 3d for 2500 you might very well consider that to bea good value. clothing yes go ahead like so like i find people saying like buying a pair of jeans and zara is expensive because there's like 65 but they last longer than buying these at all maybe unless so teresa is saying you could buy a pair. of jeans at old navy for 20 for 20 or you could. buy for 300 apair of torn jeans at diesel for 300. The way that we communicate the value is through the brand now the brand is what's wrapped around the product so all products in a given category have the same generic functionality. i think they kind of exceeded their brand promise that's not what people had expected go ahead what else so we have toyota infinity mercedes-benz audi bmw chevy lamborghini jaguar range rover tesla g porsche so all of those all those brands is what makes each of those products unique. What makes one car unique from the other is the brand and we're going to talk more about perceptual maps where our brand is positioned in the marketplace relative to our competitors. You want to have a commercial for example that's going to resonate with your target audience usually you're not going to be able to have one that resonates with everybody in your target market. So when you're showing a commercial do you think that those men that are seeing the commercial between the ages of 18 and 29 want to see somebody in the commercial who's 60 or 65 that's probably not something that'll resonate with them right unless maybe if it's coach or bruce willis or hulk hogan maybe they might still find the commercial of interest. Marketing is about creating and communicating communicating is really code for advertising we're gonna advertise so we could change the perceptions that the consumers have through marketing communications. The first thing is to identify an unmet need that consumers have so marketing when we talk about marketing the first activity is to determine a need that is not being met in the market. The order is important so when we say marketing marketing is about create communicating delivering and exchanging value so from 30,000 feet if you will that's a broad overview of what marketing is. being met so what would be a good example how about shampoo i know right but an unmet need is a shampoo that is safe for hair that's what that's dry or that that's curly oily that's perm. The way we identify the need is how guess guess oh you're not they're not good at guessing that's not good for exam day you got to be good guessers supply and demand right the way we're going to determine the needs is through marketing research we're gonna do marketing research. dollars that's tremendously valuable to do focus groups because we're going to get a tremendous amount of insight from those for example who bake those who cook those who um who use tablets or smart tvs whatever it is that we're researching it's very helpful to do that. What that's going to do is inform our quantitative research a form of quantitative research is what we're doing this semester which is a questionnaire. If 350 million people complete the questionnaire that's called the what census a census is when a hundred percent of the population participates in the research. For marketing research we need something that's statistically significant in the united states in most categories it really doesn't need to be more than 1500 and how much is that going to cost a hundred and fifty thousand dollars to do more intercept in multiple cities and get approximately 1500 respondents now when we do focus groups we also do multiple rounds of focus groups because each round is iterative so we learn from the first round of focus group and then we incorporate that in the next round before we do the quantitative research so we could do certainly qualitative research i recommend that do the focus groups.

ROUGE-1: 44.83, ROUGE-2: 42.75, ROUGE-L: 41.32
BERTScore: 66.39

==============================================
==================== [30/100] ====================
Summary:
This morning we're going to carry on talking about Jeremy Bentham and classical utilitarianism. And I'm going to begin by making a few points about the measurement of utility, which we bumped into in a glancing kind of way last time. And then we are going to move from that into talking about utility and distribution in Bentham's theory. I think you'll start to see why classicalilitarianism became such an ideologically powerful doctrine in the eighteenth and nineteenth centuries. The idea being that if you think of, in this case, a very simple two-person society, and you Think of that as the status quo, A has that much utility, B has that many utility. rare case that you get it right about yourself. And it's the objective scientific calculus that's going to tell us what maximizes people's utility. Now, you might say, "Well, how is that actually going to work?" So there are two steps here. The first one is that he thinks all utility is quantifiable. It follows from that that utility is reducible to a single index. Money is going to be the measure of utility in his scheme, and that means that we could think of these units of utility as having a kind of dollar value. Professor: Diminishing marginal utility is the principle of diminishing marginal utility of all good things. If you don't have a car and somebody gives you a Porsche Turbo, your utility's going to go up a huge amount. But if you already have a Porsche, you're going to get less new utility from the second Porsche than you had from the first. The new utility you get diminishes at the margin. Each new Porsche is less valuable to you than the previous Porsche. Professor: Does it mean that rich people will care less about money? Student: Why is the answer is no. Student: What about other values like integrity? If you have a little bit of integrity and you get some more. Prof: Okay, so it's a possibility. Any other examples of where this becomes problematic? I mean, think about beer. So there might be some goods like integrity that are not easily capture-able in this logic. We should put that out there, but yeah, over here? Professor Ian Shapiro: Health. It's tricky to think about redistributing health, right? Although you'll see we will come up against some pretty bizarre cases. Every serious economist since the eighteenth century has assumed that the principle of diminishing marginal utility is true, including Jeremy Bentham. So the main thing is that the fungibility of utility and its expressibility in terms of money, although as was pointed out here, we shouldn't think that that makes you care less about money the richer you get. Professor Shapiro: When are we going to stop at the point of perfect equality, we're going to keep redistributing until they have the same amount. that the rich will burn their crops before giving them to the poor might not be true. And even if we get to less extreme circumstances like South Africa before and after the transition, this is what we see. Ronald Reagan comes in and says, "If we cut taxes, the pie will get bigger for all and they'll be actually more revenue," and so utilitarianism says do it. And you will find, if you go back now and look at what happened during the 1980s, perfectly credible economists will line up on both sides because they cut the taxes. oil goes up, or commodities collapse, or the dollar, or this, or that. Do or don't change the value of their currency. So that when it gets down to it, you're never going to get a definitive answer to the question what is the point of practical equality. You're getting to this very messy world of macroeconomic prediction, if you want to put some limits on the radical edge of classical utilitarianism. And as a matter of history, how it went was to rethink the analytical structure of utilitarianism in a way that completely defanged its radical redistributive edge. classical to what we're going to all neoclassical utilitarianism is a subject with which I will begin on Wednesday. See you then for the next installment of this week's Daily Discussion, which will focus on the role of utilitarianism in the development of the modern world. Back to Mail Online home. back to the page you came from. Click here to read the first installment of the Daily Discussion. Follow us on Twitter @CNNOpinion and @jennifer_newton.

ROUGE-1: 22.61, ROUGE-2: 20.93, ROUGE-L: 20.72
BERTScore: 73.62

==============================================
==================== [31/100] ====================
Summary:
Lipids are rich in carbon-carbon and carbon-hydrogen bonds. They are molecules that are mostly hydrophobic. Some of the lipids are what are known as amphipathic, and they include Hydrophobic and hydrophilic components. In this phospholipid structure, they have long chain fatty acids attached via esters to this glycerol unit, so there's one here and the second one here, and then what's known as the polar head group. Semi-permeable membranes are made up through the non-covalent, supramolecular association of phospholipid monomer units. They are self-healing, so if you poke them, you poke a hole in a cellular membrane. Most commonly, things like oxygen or water and other small hydrophobic molecules can pass readily in and out of cells. But other things, things that are charged, Things that are big, need a different mechanism to get into and out. The building blocks of your protein macromolecules comprise 50% of all of the macromolcules. Proteins are too big to dissolve in that membrane through passive mechanisms, so we're going to have to figure out how to get proteins in and out of cells. Neurotransmitters, such as this, this is GABA, or gamma aminobutyric acid, are charged. It just can't get through without a transporter of some kind, and it's actually proteins that end up doing the heavy lifting of the transport processes that we'll see. The amino acids that are encoded in our proteins are known as alpha amino acids. The amino acids are also chiral, but you'll learn more than you ever wanted to know about chirality in 512, so I won't weigh you down with any of those properties. Alanine has a methyl group, for example, where I've shown the R, that would be alanine. And they get increasingly big. Some of them have quite extended size chains. Other ones have side chains with rings with double bonds in them. Amino acids are assembled in a unique linear polymer of defined order, and we designate that defined sequence the primary sequence. The primary sequence determines the fold, and it's the fold of the protein that mandates its function. The fold, the three-dimensional form, defines the function, OK? So that's very important. And I think it's absolutely amazing that with a relatively limited set of building blocks, we can define so many different functions of all the proteins in our body. limited set of building blocks, OK? Now, let's now talk about peptides because one gets a little frustrated looking at single amino acids. They don't tell us so much about the peptidic structure. OK, so this is a dipeptide, two amino acids, and there are some characteristics I want you to remember. When we write out peptides, we always write them N to C. So in that peptide, this would be the carboxyl terminus. Everyone that we always write from left to right, the sequence of peptides. Many of these bonds show free rotations. All of these show freedom of rotation. But the amide, or peptide bond, is unique in that there's restricted rotation about that bond. So it's as if you've got a linear polymer, but every third bond has kind of stuck in a particular orientation. Once you know that, the next few slides will make a lot of sense as we talk about higher-order structure of proteins. Figure out what the one-letter code spells there. Just take out your table with all the amino acids. It's appended to the back of your P-set. All right, I don't want you working it out while you're here. You've got to listen to me for the time being. OK? Not just yet. Now, one little clue that people will-- you might see and you might be confused, but instead of really showing it in detail, they might show it as a cylinder. in place due to the side chains of amino acids interacting with each other or with the backbone structures. When you fold up these motifs, when the secondary structure is in place, a lot of the side Chains are near each other, and they can engage in long-distance contacts. So for example, I'm going to show you interactions between side chains, between side Chains and the peptide backbone, or side chains and water. But what I want to do is take a look at this and see, can you put any of those potential interactions on the drawing on your handout? CNN's John Sutter shows you how to fold a protein in a simulation. He also shows you the structure of a protein that holds reversibly under appropriate conditions. Sutter: Protein folding is a puzzle that can be solved computationally by maximizing thermodynamic interactions. In the next segment, Sutter will talk about quaternary structure, a structure that holds up to quaternARY elements in the same way as a helix or helix-like structure. He'll also talk about how to make a protein into a fluorescent light-emitting device. A single amino acid change in the primary sequence of collagen can destabilize the structure, so it is no longer viable. The disease type I'm going to talk to you about is a set of diseases known as collagenopathies, and the particular one is called osteogenesis imperfecta. A lot of babies with this defect can't even be born through the birth canal because it would crush the bones, and many of them don't survive very long at all. Some survive with different kinds of cases, but their lives are greatly impacted. extended, and I show you three strands in this polymeric structure, a yellow, a red, and a green. And these rolled together into a three helix bundle that has a fibrillous structure, and then all these structures come together to make the macromolecular structure that is collagen. And there are many genetic defects of collagen, and what's so important to think about is if you have a defect in one strand that defect will propagate through every single strand. well-aligned as the rest of the structure. And then that defect gets propagated into all the fibrils and results in the weakening of the bones. Either the collagen fails to form properly, or the collagen, when it forms, it has much less mechanical stability. So I think that's a good place to stop and I'll pick up next time with hemoglobin. There's a great link on the website to the Protein Data Bank to see how enzymes work. These slides are posted with these reading assignments, and they're posted in color if you want to look at them again.

ROUGE-1: 25.47, ROUGE-2: 24.07, ROUGE-L: 24.34
BERTScore: 63.27

==============================================
==================== [32/100] ====================
Summary:
Professor: What exactly happens with the half l states? We're gonna talk about that in some detail in a couple of weeks, but let me give you a quick preview. Professor: It is impossible to represent the half integer states with a wave function which represents a probability distribution on a sphere. If you take that wave function, if you rotate by 2pi-- in any direction-- if you rotated by. 2pi, then you have a half-integer state. And we quickly deduced that that was impossible. The value of the wave function at some point, is equal to the wavefunction at that point. So, you can't write a wave function if it has to be equal to minus itself at any given point. The important thing here, is that if you have a magnetic field which has a gradient, so if it's zero0.2pi thewave function comes back tominus itself. So that's a strange thing. But the question is, look, these furnish perfectly reasonable towers of states respecting these commutation relations. If you send a magnet through a region, it'll get deflected in one direction or the other. How far it gets deflected is determined by how big of a magnet you sent through. You send in a bigger magnet, it deflects more. So, let's put this together. Imagine we take a charged sphere, we send it rotating with same angular momentum. We send it through a field gradient, a gradient for magnetic field. What we'll see is we have a force. And that force is going to be proportional to how big your magnet is. Stern Gerlach Experiment: Take an electron and send it through a magnetic field gradient. If it gets deflected, you will have measured the magnetic moment. If the electron weren't rotating, it would just go straight through, right? It would have no angular momentum, and it'd have no magnetic moment, and thus it would not reflect. But electrons carry a form of angular momentum that can't be represented in terms of rotations on a sphere. It's like an angular momentum in every possible way, except it cannot be represented. that fixed the fact that I was probing Lz is that I made the magnetic field have a gradient in the Z direction. So, what I was sensitive to, since the force is actually proportionally to mu dot B-- or, really, mu dot the gradient of B, so, we'll do this in more detail later. The direction of the gradient selects out which component of the angular momentum we're looking at. This is also true of the L equals 1 states. What about Lx? Lx also takes one of three values, those three values. we're gonna talk about real, physical systems in three dimensions. And as we'll discover, it's basically the same as in one dimension, we just have to write down more symbols. But the content is all the same. So, this will make obvious the reason we worked with 1D up until now, which is that there's not a heck of a lot more to be gained for the basic principles, but it's a lot of more knowing to write the expressions. And that is a beautiful abuse of notation-- in spherical coordinates. In 3D, I want to think about systems in 3D which are spherically symmetric. This is going to be a particularly simple class of systems, and it's also particularly physical. Things like the isotropic harmonic oscillator, things like hydrogen, where the system is rotationally independent, the force of the potential only depends on the radial distance, all share a bunch of common properties. Along the way, we'll solve a toy model for hydrogen, and I'll show you how to do it. can be written in a nice form. This is minus h bar squared, 1 upon r dr squared r. And from the second term, L squared over r squared upon 2m plus 1 over 2mr squared L squared plus u of r. OK, and this is the energy operator when the system is rotational invariant in spherical coordinates. These commute. So, we can find common eigenbasis which are eigenfunctions both of E and of L squared. We can also simplify our lives if we also use phi phi. Professor: "This is for any given value of l. This is a constant over r squared, with a plus sign" "Can r be negative? No. It's defined from 0 to infinity. So, that's like having an infinite potential for negative r" "Oh shoot! Oh, I'm sorry! I'm terribly sorry!" "I've abused the notation terribly. Let's-- Oh! This is-- Crap! Sorry" "I'm really sorry, I did not realize how confusing that would be" God! OK. Let's check our sanity, and walk through the logic. So, the logic here is, we have some potential, it's a function only of r, yeah? As a consequence, since it doesn't care about the angles, we can write things in terms of the spherical harmonics. Here's the energy eigenvalue equation. We discover that because we're working in spherical. harmonics, the angular momentum term becomes just a function of. r, with no other coefficients. And now we have to ask, what exactly is the effective potential? if it's 1 over r. So, we get an effective potential-- that I'll check-- there's the effective potential. And finally, the third fact is that r must be strictly positive, so as a 1D problem, that means it can't be negative, it's gotta have an infinite potential on the left. If you see this, declare in your mind a brief moment of triumph, because you know what technique to use. You can do this sort of rescaling by a power of r. And more generally, if you have a differential equation that looks like. something like a derivative with respect to r plus a constant over r times phi, you know how to solve this. if there were nothing else, equals zero. If there were no other terms here, then this would say, ddr plus c over r is phi, that means when you take a derivative it's like dividing by r and multiplying by c. But if phi goes like r to the minus c, that's not the exact solution to the equation, but I can write phi is equal to r to minus c times u. And then this equation becomes ddr, dot, dot u equals zero, OK? Very useful little trick-- not really a trick, It's just observation-- and this is the second order version of the same thing. Professor: angular momentum barrier is just an expression of that. It's just saying that as you come to smaller and smaller radius, holding the angular momentum fixed, your velocity-- your angular velocity-- must increase. The reason is that the electron can radiate away energy and angular momentum, and so l will decrease and decrease, and can still fall down. So, we still need a reason for why the hydrogen system, quantum mechanically, is stable. [? Why do ?] [? things exist? ?] The wave function, phi sub E, which goes near r equals 0, like u of r over r. So, what should be true of u? Can u diverge? Is that physical? Does u have to vanish? Can it take a constant value? So, I've given you a hint by telling you that I want to think about there being an infinite potential, but why? Why is that the right thing to do? Well, imagine U of r went to a constantvalue near the origin. That's maybe not so bad. The energy is going to go like, p squared over 2md squared. But here's an important fact, d squared-- the Laplacian-- of 1 over r, well, it's easy to see what this is at a general point. As you approach the origin from any direction, the function is going like 1 over R. So, the second derivative has to go as you go across this point. But it changes from plus infinity in this direction, to plus infinity. That's badly singular. exactly the way you need to get the delta function. OK, which is pretty awesome. So, what that tells us is that if we have a wave function that goes like 1 over r, then the energy contribution-- energy acting on this wave function-- gives us a delta function at the origin. If we calculate the energy, we'll discover that the energy is badly divergent. It does become divergent if we don't have u going to 0. That's about the derivative of u, as you approach the origin from Lucatau's Rule. would've got the same equation. And that means that the energy eigenvalue can depend on l, but it can't depend on m, right? So, that means for each m in the allowed possible values, l, l minus 1, [? i ?] minus l-- and this is 2l plus 1 possible values-- for each of these m's, the energy is the same. And so, here we have a degeneracy. And this degeneracy isn't fixed by rotational invariance. of E and Lz. Can I also find a common eigenbasis of ELz and Ly? Are there common eigenevectors of ELZ and Ly. Are thereCommon eigenfunctions of Lz andLy? No, because they don't commute. E commutes with Lx plus iLy and Lx minus iLy, L plus/minus. So, this tells you that if you have an eigen function of E, and you act with a raising operator, you get another eigenfunction of E. And thus, we get our 2L plus 1 degeneracy. some examples. So, the first example is gonna be-- actually, I'm going to skip this spherical well example-- because it's just not that interesting, but it's in the notes, and you really need to look at it. OK, so, the spherical well. And if it's a spherical infinite well, then I'm gonna say, the potential is infinite outside of some distance, l. OK? And it's 0 inside. So what does this give us? Well, in order to solve the system, we know that the first thing we do is we separate out with yLms. case-- when there's zero angular momentum, little l equals 0. E-- and I should call this u sub l-- Eu sub 0 is equal to h bar squared upon 2m. This is saying that the energy, a constant, times u is two derivatives times this constant. So, u0 can be written as a cosine of kx-- or sorry-- kr plus b sine ofkr. And so, this tells you what the energy is. It's just exactly like when the 1D system. that the eigenfunctions are? And let me do that here. So, therefore, the wave function phi sub E0 of r theta and phi is equal to y0m. But what must m be? 0, because m goes from plus L to minus L, 0. And so, there's an overall normalization constant, that I'll call n. OK, so, we get that our wave function is 1 over r times sine of n pi over Lr. do I have a 0 at the origin? Is that the question? AUDIENCE: Yeah. PROFESSOR: OK. There's nothing special about the origin, except for two things. Little u has a 0, but it gets multiplied by 1 over r. So, the wave function, in fact, is non-zero, there. The physical thing is the probability distribution, which is the [? norm ?] squared of the wavefunction. And it doesn't have a0 at theorigin. Professor: I am not about to solve for you the problem of hydrogen. I am going to construct for you a nice toy model, which turns out to be an excellent first pass at explaining the properties observed in hydrogen gases. This is a model. It is a bad model, It doesn't fit the data. But it's pretty good. And we'll be able to improve it later. So, it is the solution of the Coulomb potential. And what I want to emphasize to you, I cannot say this strongly enough, physics is a process of building models that do a good job of predicting. We know that the wave function is this little u times 1 over r times yLm, for some l and some m. So, the first thing we should do any time you're solving an interesting problem is do dimensional analysis. E squared must be an energy times a length. Also known as p squared l, momentum squared over 2m, 2 times the mass times the length. It's useful to put things in terms of mass, momentum, and lengths, because you can cancel them out. The ground state has what energy? Some finite energy. It doesn't have infinite negative energy. What do you expect to be roughly the ground state energy of this system? Roughly minus e0. That seems like a pretty good guess. It's the only dimensional sensible thing. Maybe we're off by factors of 2. But, maybe it's minus e 0. So, that's a good guess, a first thing. And if you actually take mu e to the 4th over h bar squared, this is off by, unfortunately, a factor of 4. Solving a differential equation is a sort of involved undertaking. Professor: Just dimensional analysis gives you this answer. "This is one of those miraculous differential equations where we can actually write down the solution by doing the series approximation" "Most differential equations of some, maybe if you're lucky, it's a special function ... but most don't have a simple solution like a Gaussian or a power large, or something. Most of them just have some complicated solution," he says. "You know what those solutions look like, they look like exponentials," he adds. function, which I'll call v, little v. Little v of rho, and this, asymptotically, should go to a constant near the origin and something that vanishes slower than an exponential at infinity. So then, we take this and we do our series expansion. And the series expansion has a solution, which is a sub j plus 1. So, what that tells us is that for some maximum value of little j, for some aj max plus 1 is equal to 0. root 2 epsilon times j maximum plus l plus 1 is equal to minus 1. But that gives us a relationship between overall j max, little l, and the energy. And if you go through, what you discover is that the energy isequal to 1 over 4 n squared. And so, by solving the differential equation exactly, which in this case we kind of amazingly can, the energy eigenvalues are, indeed, exactly 1/4 of e0. And they're spaced with a 1 over n squared, which does two things. nice bit of experimental data, but we've discovered the energy is, in fact, not just independent of m, but it's independent of l, too. Why? What symmetry is explaining this extra degeneracy? We'll pick that up next time. Back to Mail Online home. back to the page you came from. Click here to read the rest of the article. Back To the pageyou came from, click here to reads the rest. Back into the pageYou can now read the full article on this site.

ROUGE-1: 34.28, ROUGE-2: 32.53, ROUGE-L: 32.48
BERTScore: 67.36

==============================================
==================== [33/100] ====================
Summary:
John Guttag: Welcome to the 60002, or if you were in 600, the second half of 600. There are problem sets. They'll all be programming problems much in the style of 60001. We do want you to hone your programming skills. There'll be a few additional bits of Python. Today, for example, we'll talk about Lambda abstraction. And the course is really less about programming and more about dipping your toe into the exotic world of data science. The main topic is what I think of as computational models. Science is moving out of the wet lab and into the computer. We'll talk about three kinds of models-- optimization models, statistical models, and simulation models. An optimization model is a very simple thing. We start with an objective function that's either to be maximized or minimized. We then often have to layer on top of that objective function a set of constraints, sometimes empty, that we have to obey. We can build models that sort of explain how the climate has changed over the millennia, and then a slightly different model that might predict what it will be like in the future. A greedy algorithm is a way of looking at problems in a way that maximizes the value of an object. In this lecture, we'll look at a specific optimization problem called the knapsack problem. The problem involves a burglar who breaks into a house and wants to steal a bunch of stuff. The burglar has to solve the optimization problem of stealing the stuff with the most value while obeying the constraint that it all has to fit in theknapsack. We use these things all the time. We can't avoid using optimization algorithm as you get through life. The most obvious solution is brute force. I enumerate all possible combinations of items; that is to say, I generate all subsets of the items that are available. I can now go through and sum up the weights and remove all those sets that weigh more than I'm allowed. And then from the remaining combinations, choose any one whose value is the largest. So it's pretty obvious that this is going to give you a correct answer. You're considering all possibilities and choosing a winner. There is no algorithm that provides an exact solution to this problem whose worst case running time is not exponential in the number of items. The sad answer to that is no for the knapsack problem. But that should not make you sad because while there's no perfect solution, we're going to look at a couple of really very good solutions that will make this poor woman a happier person. So let's start with the greedy algorithm. It could hardly be simpler. We say while the knapack is not full, put the best available item into the knappers. red there's a parameter called keyfunction. That's going to be-- map the elements of items to numbers. So it will be used to sort the items. So I want to sort them from best to worst. And then for i in range len of items sub copy-- I'm being good. I've copied it. I don't want to have a side effect in the parameter. In general, it's not good hygiene to do that. And so for-- I'll go through it in order from best-to-worst. actually works? I hope not because I think it does work. Let's ask the next question. How efficient do we think it is? What is the efficiency of this algorithm? Let's see where the time goes. Who wants to make a guess? By the way, this is the question. So please go answer the questions. We'll see how people do. But we can think about it as well together. The first thing is at the sort. And we heard from Professor Grimson how long the sort takes. See who remembers. Lambda is used to create an anonymous function, anonymous in the sense that it has no name. Lambda does is it builds a function that evaluates that expression on those parameters and returns the result of evaluating the expression. Here we're going to be using greedy by density to allocate-- actually, sorry, this is greedy by cost. We don't want to pass in the cost, right, because we really want the opposite of the cost. And we want the cheaper items to get chosen first. TestGreedys.that chooses a burger, the pizza, and the wine for a total of 284 happiness points. On the other hand, if we use greedy by cost, I get 318 happiness points and a different menu. The problem is that a greedy algorithm makes a sequence of local optimizations, chooses the locally optimal answer at every point, and that doesn't necessarily mean the best answer for the situation at that point in the game. And here's another solution with 318, apple, wine-- yeah, all right. So I actually got the same solution, but it found them in a different order. add up to a globally optimal answer. This is often illustrated by showing an example of, say, hill climbing. So you might choose as a greedy algorithm if you can go up, go up. If you can't going up, you stop. And that's the problem with greedy algorithms, that you can get stuck at a local optimal point and not get to the best one. Now, we could ask the question, can I just say don't worry about a density will always get me the best answer? Well, I've tried a different experiment. and I'm going to allow myself 1,000 calories. Well, here what we see is the winner will be greedy by value, happens to find a better answer, 424 instead of 413. So there is no way to know in advance. Sometimes this definition of best might work. Sometimes no definition ofbest will work. You can't get to an optimal solution with a greedy algorithm. On Wednesday, we'll talk about how do you actually guarantee finding an optimal Solution in a better way than brute force.

ROUGE-1: 33.61, ROUGE-2: 31.91, ROUGE-L: 29.15
BERTScore: 59.11

==============================================
==================== [34/100] ====================
Summary:
Expectations play a huge role in economics, says Ricardo Caballero. Expectations about future conditions play a big role in the decision of all economic actors. This is going to be a very compressed version, adapted version, of chapters 15 and 16. But in terms of material mapping into the book, those are the relevant chapters. The main idea here is that the IS-LM model as we have described it up to now really overweights the present. It's quite clear that the reason firms invest is not because of the current condition. Milton Friedman: What really matters to you in a consumption decision is not so much your current income, but it's what you expect to get on average during your lifetime. How wealthy you are will pin down more or less the consumption you have more than yourcurrent income. And the very rich often often have no income at least at least labor income. All the returns on assets come from the return on assets, again, they mostly borrow against that. You have a lot of human capital, so that's also a very important concept. Investments that give you a return, a quick return, are worth more than things that have a pay-off in the very long run. The decision, for example, of buying a machine needs to look at the price of the machine right now and then at the expected present discounted value of the cash flows. The market doesn't need to have expectations, about the interest rate it needs to decide whether to invest or not, RICARDO CABALLERO says. But I said, but there is something that I could look at and that I really know that I can't really know in the market. In principle, a better investment function-- remember, we wrote an investment function as investment, a function of output, current output, and then the interest rate. The higher is V, the higher is the expected present discounted value of buying a machine given the price, the larger is the investment. In practice, current cash flows also matter a lot, OK? So in the same sense as in the case of the consumption function, we said, in principle, it's only wealth that matters. But in practice, there's lots of consumers that are financially constrained. So current income also matters. much or is not as optimistic as the firm is and so on. So it may not borrow-- the firm may not be able to borrow as much as it would want. The bank may say, you know, I'm going to be more conservative here since I'm lending you the money. And one way that firms use, actually, to get around financial constraints is simply by returning-- retaining their earnings, meaning they generate a cash flow, and they save. So if current activity is high, sales are high, firms are less likely to be financially constrained. More realistic model. So you go back to IS-LM and put this type of consumption function and investment functions, and they're going to make a lot of sense. Again, the concept of something persistent-- persistent things should matter a lot more than temporary things. And the same is true for interest rates. If I expect-- since interest rates are high today, but we expect them to go down in the near future, then that's not going to affect a lot the discounting of future profits. If the Fed cuts the interest rate but doesn't persuade anyone that this rate will remain low in the future, then it is going to get very small effect on output. However, if we convince people that there will be future changes, that the rates will remain lower for a long time, that means that this IS now will shift to the right. So again, for central banks, it's a lot like-- it's mostly about expectations management. It can be quite tricky here, actually. It may be a big cut in interest rates that may be an anticipation of expansion. example is abused by almost anyone that wants to cut taxes and things like that. But there are experiences. There is a whole spectrum of experiences. But in situations that are as extreme as this one, it clearly proved to be very effective. So that's that. So the role of this lecture was to say something that I sort of should have said earlier on, but I would have been a bit confusing. But it's very important. Expectations plays play a central role in economics. In particular, expectations influence aggregate demand. was expansionary. That was contractionary. But it was overwhelmed or offset, more than offset by the improvement in the outlook that you had. And that also happens with monetary policy. Countries that have high inflation problems and so on sometimes get-- and they have to go through dramatic tightenings. Yes, most of them get very short-lived recession. But sometimes they are veryshort-lived recessions because eventually the reduction of the instability caused by high and unstable inflation sort of ends up dominating any direct contractionary effect.

ROUGE-1: 21.22, ROUGE-2: 20.11, ROUGE-L: 20.41
BERTScore: 66.08

==============================================
==================== [35/100] ====================
Summary:
CNN's John Defterios takes a look at three different views of fiscal policy. Keynesian, classical and supply-side views are shown in the atas model. The Keynesian view is based on the idea that if you borrow money and make aggregate demand increase, eventually it will increase. The supply side view is more of a Keynesian approach and focuses on what happens when aggregate demand is increased rather than how much it increases. Defterius: We have three main views of how well fiscal policy works or how well it doesn't work and we're just gonna look at these. change the demand for loanable funds so you have this market right if you're gonna have the government go out and borrow money they're going to borrow the money. When the interest rate goes - I - what happens to private borrowing what is it at now do it q2 is the total that's private and public how much is private borrowing yeah it's less than q1 it's a way over here I'll call this guy q0.private investment private borrowing goes down I didn't even stay atq1 it went down interest rates went from i1 to i2 so let's think about what this guy's says. get over here we wanted to get the YF right and we said oh look we can just easily program the number into the our Excel spreadsheet and so exactly how much to increase government spending. We're just gonna change it by this much so here's this increase of G but if that increase in G changes interest rates and causes these guys to go down the guy doesn't become this guy what does he do he doesn't shift to this guy he's down alright because Gees gone up but consumption or investment I've also come down a little bit. In 2025 with the high marginal tax rates right in essence people wouldn't work and save and invest as much because it wouldn't be worth it and so the PPC curve is that guy in blue rather than the guy in black. There's some evidence that people respond to these incentives right because if you think about it like this think about like this somebody makes somebody who's on welfare they're getting a check % thousand dollars and you say that's bad we don't want you doing that go get a job. doesn't make any sense I'm not gonna do that I'm gonna goof off. I don't want it because they're facing a marginal tax rate that's really really high so which view is Keynesian new classical supply? I'll write the paper and win the Nobel Prize and I can go to Sweden and get the nice little bitty gold medallion that I couldn't wear around my neck for the rest of life at such a such recession. alright this is the end of test three material I will.

ROUGE-1: 17.56, ROUGE-2: 16.06, ROUGE-L: 16.34
BERTScore: 62.13

==============================================
==================== [36/100] ====================
Summary:
Professor: The plan for today is as follows. We're going to look at this unitary time evolution and calculate this operator u, given the Hamiltonian. Then we will look at the Heisenberg picture of quantum mechanics. The Schrodinger operators acquire time dependence, which makes the relation between classical mechanics and quantum mechanics more obvious. So we'll discuss that. We'll find the heisenberg equations of motion and solve them for a particular case today. And we'll go through three cases. the right. It cannot be to the right of U0 though, because this is a matrix, a constant matrix that we've put in here as a possible thing for boundary condition. So so far we've taken this derivative, and then i's cancel, the h bar cancels, and you get H. But this whole thing is, again, U. So try this. And it works. So having this solution we can write, for example, that U of t t0 is going to be e to the minus iHt over h bar. A little time dependence is an idea, the sign to make it possible for you to solve the equation. So you could have Hamiltonians that are time dependent, but still have a simplifying virtue. So if you have a magnetic field that is fixed in one direction but change in time, you can have a situation where your Hamiltonian is time dependent. And you will discuss such case because it's interesting. But later on as we do nuclear magnetic resonance, we will have the more interesting case in which the Hamiltonian does not commute. Time dependent Hamiltonian that actually commutes. U of t t0 is given by a natural extension of what we had before. R dot depends on H, R is an integral of H as well, but the H at different times commute anyway, so this must be true. There's no place where you can get a contribution, because R dot is like an H, and here's anintegral of H. But here is the claim R dot commutes with R. Claim R dot and R commute. something that makes sense. U of t and t0. I'll write the answer and explain how it looks, and then you will see that it's OK. It's interesting. But it probably is not the most practical way you can solve this problem. There's an acronym for this thing. T it's called the time ordered exponential. This operator does something to the exponential function. So it's a nice thing that you can write it, and you can prove things about it. But when you have a practical problem, generally that's not the way you solve it. But in terms of completeness, it's kind of pretty in that you go from the exponential to the time ordered exponential. And I think you'll see more of this in 806. So that's basically our solution for H and for the unitary operator U. And what we're going to do now is turn to the Heisenberg picture of quantum mechanics. Yes, questions? AUDIENCE: Why does R dot [INAUDIBLE]? PROFESSOR: Because that's really a property of integrals. solutions? PROFESSOR: Yes, pretty much. Because at the end of the day, this is a first order matrix differential equation. So it's a collection of first order differential equations for every element of a matrix. It's pretty much the same as you have before. If you know the operator at any time, initial time, with the differential equation you know it at a little bit time later. So the operator is completely determined if you knows it initially and the differential equations. So I think it's completely analogous. The Heisenberg picture of quantum mechanics is not something that you necessarily invent from the beginning. It all begins by considering a Schrodinger operator As hat, which is s is forSchrodinger. The time dependent matrix elements of the operator As or the matrix element of this time dependent operator between the time equals 0 states. This operator is sufficiently important that this operator is called the Heisenburg version of the operators s. Has time dependence, and it's defined by this equation. It is obtained by acting with a unitary operator. whole thing is the same. So this is something very useful and we'll need it. One more comment, expectation values. Comment number four on expectation values, which is something you've already-- it's sort of the way we began the discussion and wanted to make sure it's clear. So we say that the Schrodinger expectation value is equal to the Heisenberg expectation value. We right it in the bottom, but we mean the top equation. And we use it that way. difficult sometimes. So what we try to do in order to simplify that is find an equation that is satisfied by the Heisenberg operator, a time derivative equation. We want to calculate ih bar d dt of the He Eisenberg operator. And so what do we get? Well, we have several things. Remember, the Schrodinger operator can have a bit of time dependence. So let's take the time derivative of all this. So you would have three terms. ihBar dU dagger dt As U plus U dagger As dU dt plus-- with an ihbar-- U dagger ih Bar dAs minus dt and U.

ROUGE-1: 19.74, ROUGE-2: 19.02, ROUGE-L: 19.50
BERTScore: 66.19

==============================================
==================== [37/100] ====================
Summary:
Professor Steven Smith: I want to talk today about my favorite part of the Second Discourse, a book that never grows old. Last time, I focused on a famous passage in which Rousseau claims it was the establishment of private property that was the true formation of civil society. But in fact, that's not really true. Rousseau understands that even for institutions like property and civil society to be possible there must be huge and important developments that go on or take place even prior to this, moral and psychological transformations of human beings. David Frum: In Rousseau's Second Discourse, amour-propre is the chief villain. He says the French term is untranslatable in English but is related to pride, vanity, conceit. Frum says Rousseau distinguishes it from amour de soi-meme, a sort of self-love. He asks: How could pride have arisen in a state of nature that is solitary, poor, nasty, brutish, and short?Frum: Rousseau speculates about this hypothetical history and speculates that the passion of vanity was born. Rousseau: Government is a con game that the rich play upon the poor. Political power simply helps to legitimize economic inequalities, he says. In society we only live through the opinions of others, through the gaze of others. The book offers no positive answer to the problem of civilization but only hints at the best possible solutions. Perhaps the closest approximation to primitive society is the state of primitive society, says Rousseau, in a letter to the City of Geneva, which prefaces the book in a sense, the book lauded by Geneva.

ROUGE-1: 9.34, ROUGE-2: 7.81, ROUGE-L: 8.49
BERTScore: 66.61

==============================================
==================== [38/100] ====================
Summary:
So I think I just spend like five minutes, just briefly review on the backpropagation last time. So I didn't have time to explain this figure, which I think probably would be useful as a high level summary of what's happening. Like this is the forward path. This is how you define network and the loss function. So you start with some example x, and then you have some-- I guess, this is a matrix vector multiplication module, and you take x inner product multiplied with w and b. And then get some activation, the pre-activation. And you get some post activation. Some of the practical viewpoint of ML, like how do you relate to your model, what you have to do in this whole process, right? So like you start with the data process, and then you need to tune the model. So we're going to discuss how do we make sure your model can also generalize to unseen test examples. And we are going to talk about some of the new phenomena people have found in deep learning, which is a little bit different from the classical understanding. Training loss or sometimes, it's called training error. Sometimes, it is called training cost. So they all mean the similar type of concepts. For example, if you care about the square loss, then the training loss would just be this. And other loss could be cross-entropy loss. It could be like MLE, the maximum likelihood estimator. So this is basically, so far, what we have focused on in the last few weeks. So how do you get a training loss? So there are many ways to optimize it. In some sense, you care about two quantities. You care about the training loss and the gap. You want both of these two to be small. This one is something you can control in some sense. But this one is harder to control because you don't-- because you cannot say, I'm going to find a theta, such as l theta is small. And the point of this lecture is to discuss in what cases you can somewhat know this is not too big. So this is a typical situation of overfitting. The bias is going to be a decreasing function as the model complexity. The mechanism is that if you change the model. complexity to make it more complex, then your variance will be. bigger and bias will be smaller. And your sum of these two functions, which is a test error, will be something like this. So this is kind of the quick overview of what we're going to discuss. So any questions so far? Why is the bias [INAUDIBLE]. Why is bias this crazy? Oh, squared, I mean. Oh, this is just because it's kind of a unit thing. Some people call the bias square bias, actually, in some literature. It's just how do you choose the right unit. And sometimes, I think, I guess this quadratic. Sometimes, they are called this h star xi. Just for the sake of terminology, sometimes, they call this the ground truth. But of course, you don't know it. You want to try to recover it. So suppose you-- OK. Maybe I'll set up just really quick. So I'm going to have some training examples. And these training examples are something like yi is equals close to aquadratic function. The bias is basically like it's saying that the reason why-- I don't know exactly why people call it bias in the very first time. The thing is that you are imposing a linear structure, but the true data is not linear. So it doesn't matter how many data you see, as long you impose this, you just insist that I just believe that this thing is linear. Because this is the wrong belief about the relationship between y and x. So that's the typical situation, where you have a large bias. A fifth-degree polynomial can go up and down so many times, several times. So the higher the degree is the more times you can go. So in this case, the training error is literally 0. So I guess, this is expected. And the thing is that this is overfitting. So what's the problem here? Why it's overfitting? So why the test is not good? It's fit to the spurious patterns in the small and noise data. So this is because you don't have enough data. model tries to explain all of this small perturbations, small noise. And because it overexpressed the small noise, at loss, it kind of like didn't pay enough attention to the more important stuff. So whatever patterns you see in four data points, like you can explain it. If your model is specific to the spurious patterns, that means that if you redraw, you are going to-- you're going to learn the new spurious patterns. And you aregoing to have a different model. that you draw the same number of samples with similar ground truth-- the same ground truth and the solution. But just their randomness are different. You are using different noise. That's exactly what I'm going to talk about next. Sorry. One moment before that. OK. So basically, OK, just to summarize here, if you redraw all the examples and you find that a large variation between-- so suppose, you have a-- so you so you have -- so you call this-- to fit a fifth-degree polynomial. What happens will be that this is probably not entirely obvious-- OK. One obvious thing is that you probably wouldn't do anything like crazy as this right. So what you really will fit, like if you minimize the error on the training data with this so many training examples, then what you will get is probably something like this. Maybe there are still some small fluctuations. It's not like necessarily matching exactly the ground truth, but you have a small fluctuation. This is kind of more like a quadratic. minima [INAUDIBLE] So the question is that another possibility is that a failure mode is that you just couldn't find this degree 5 polynomial because some optimization issue. Even though there exist one, that is very good that fits the data, but you couldn'tfind it. That's probably not true for degree fifth polynomials for this one toy example. But it could be possible for some other cases, where the model does exist, butyou can find it. same distribution. From the same distribution. Yeah. So like if you collect more data from-- yeah. So if you don't know the ground truth, so how do you know that you are having a large bias? You cannot really exactly know. When you don’t know ground truth. So when we don't knows the groundtruth, you cannot really exact like-- let me think. Let me think about what I can do to mitigate that. So all of these are so far are for analysis purpose. the ground truth, I think you cannot exactly compute the bias. Because the definition of the bias, actually, requires you to sample a lot of data. So typically, what you do is you say, you fit the data on the training set. And you see you're underfitting. Underfitting means you have a large training error. And that's when you start to believe that you've got a large bias. For overfitting the graph that's right behind you, the bias square [INAUDIBLE] The third one is the sum of them. This is the test error. of bias and variance first change, did you use different type of model [INAUDIBLE] So I think this figure, so this is the-- OK. You ask a very good question. For example, suppose you, for this data set, probably, the best thing is to use quadratic. Quadratic has small enough bias because it is, in principle, expressive enough to express our data. So that's probably the best solution. And if you're going to, it is cubic, then maybe the sweet spot is achieved at cubic, maybe. In the last four years, last four or five years, people have realized that if you increase your model number of parameters even more, at some point, you will see that it will be like this. This is the second descent of the test error. This phenomenon is called double descent. It's about after people have studied this very carefully, I would talk about some of the explanations. But before that, let me also give me another related phenomenon, which is also calleddouble descent, but it's data wise. if you believe in that, then you should say that OK, the test should look at this. And it should continue to decrease as you have more and more data. But it turns out that, actually, in many cases, what happens is that the test error will look like this, or increase, at some point, and it will decrease again. Sometimes, it does increase again a little bit, but often, not much. And sometimes, it just keeps decreasing. AndSometimes, it plateaus. this function. This was like this has been for a while. This phenomenon. This one, I think, is also-- actually, the paper that first systematically discussed this is like 2020. About that peak, when was that discovered? The peak? Yeah. This is discovered in the same paper, the peak. It's not monotone. The fact that there exists a peak was also discovered right, essentially. Yeah. I think at least I would say, at least, it's only until 2020 that most people start to realize this. people really care about it. Even within linear models, you can still change the complexity, just to clarify that. Most of this theoretical study, I think, are for linear models. And they are pretty precise these days. And I'm going to try to kind of roughly summarize the intuition from the study of this double descent. So I think the first thing to realize is that this peak, so you can argue what is the most exciting or surprising thing about this graph. But let's first talk about a peak, this peak in the middle. stochastic gradient descent for linear models. So the existing algorithms underperform dramatically when it's close to d. So both these two peaks are basically like this. So in some sense, if you use the norm as the complexity, actually, these peaks have large complexity. But I guess what I'm trying to say is that the number of parameters also is not necessarily the right measure for the complexity of a model. But there is no universal answer to that question, but there is a way to describe it. happens that for mathematical reasons, I think l2 norm behaves really nice. It seems to relate to a lot of fundamental properties. And actually, if you-- and you can test this hypothesis in some sense. So you can say that OK, I'm saying here the existing algorithm underperforms. But if you have a new algorithm, that's regularized, suppose you recognize the norm. Then you're going to see something like this. So here, it's just saying that at least for this case, it sounds like norm seems to be a slightly better complex measurement. There is no peak, but why there's no ascent? In many cases, you don't have ascent. And the explanation is that n is much, much bigger than d-- sorry, the number of parameters is muchbigger than d. And actually, for example, another question is when number of parameter is bigger than number of data points, sometimes, you are thinking this is the-- you have too many degree of freedom to fit all the specifics of the data set. But actually, empirically, you do work pretty well. points. So the thing is that even though it sounds like you are supposed to overfit, but actually, the norm is small. The reason is that somehow, there is some implicit regularization effect, which makes the norm small. And that's something I'm going to discuss, I think, more next time. So that's why the norm here is very big. But the normhere is small? But the reason is because your optimization algorithm has some implicit encouragement to make the normsmall, which is not used.

ROUGE-1: 27.29, ROUGE-2: 26.28, ROUGE-L: 25.83
BERTScore: 71.31

==============================================
==================== [39/100] ====================
Summary:
Vladimir Ilyich Ulyanov, AKA Lenin, helped overthrow the Russian tsar Nicholas II in 1917. He founded the Soviet Union, one of the worst dictatorships of the 20th century. But was he a hero who toppled an oppressive tyranny or a villain who replaced it with another? It's time to put Lenin on the stand in History vs. Lenin, says Alexander Nekrassov, the author of the novel "The Death and Life of Vladimir Lenin" how would I have sounded?" We can never be sure how things could've unfolded if different people were in power or different decisions were made, but to avoid the mistakes of the past, we must always be willing to put historical figures on trial. We must never forget that history is not an exact copy of the present, but it can be a guide to the future. It can be used to learn from the mistakes made in the past and to make better decisions in the present. It is never too late to put a historical figure on trial for their actions.

ROUGE-1: 26.44, ROUGE-2: 18.07, ROUGE-L: 16.46
BERTScore: 59.46

==============================================
==================== [40/100] ====================
Summary:
The goal of this course is to give you the tools to interpret complicated phenomena. The vibrational problem is a way of understanding internuclear interactions-- nuclear motions. When we go to molecular orbital theory, we take what we know about atoms, and build a minimally-complex interpretive picture, which is sort of a framework for understanding complicated molecular interactions. Your support will help MIT OpenCourseWare continue to offer high-quality educational resources for free. To make a donation or view additional materials from hundreds of MIT courses, visit MIT OpencourseWare at ocw.mit.edu. every pair of electrons, not just two. In helium we just dealt with two, and that wasn't so bad. But when we deal with n electrons, what we are going to discover is that in order to anti-symmetrize the wave function, we have to write a determinantal wave function. When you expand an n by n determinant, you get n factorial times. And when you calculate matrix elements, you haven factorial squared integrals. The relationship between the effective quantum number and the ionization energy of a state then provides a hydrogen-atom-based structural model for everything you can observe. Structure is static. Dynamics is magical, and special, and hard. But if you understand structure in a way which is not the exact eigenstates, you can understand how one observable relates to another. And so the main obstacle to being able to do important stuff is to understand the matrix elements of operators that we care about, like transition moments, spin orbit, Zeeman effect. able to evaluate these matrix elements is the permutation requirement. And it turns out that there is a really simple way of dealing with the requirements for electron permutation, and that is to write the wave function as a determinant of one-electron orbitals. A determinant has three really important properties. One, it changes its sign when you permute any two columns. And three, if you have two identical columns or rows, it's 0. That's really fantastic. And that is the Pauli exclusion principle-- not what you learned in high school. MIT professor in physics. He invented these things in 1929. So basically what Slater did is showed, yeah, you can do the necessary algebra to deal with any atom. And there are two ways of doing this. One is the truth, and one is the fit model. Now the truth is really boring, because you lose all the insights. And a fit model also tells you what are the import the actors. And maybe they're in costume,maybe they're not, but we can deal with them. P permutation operator, operating on any two-electron function, has to make P 2, 1, which has to be equal to minus. So there's only two possible eigenvalues. You can have minus 1 or plus 1. And the minus 1 corresponds to fermions, things that have half-integer spin, like electrons. The plus 1 correspond to things with integer spin, such as photons, and vibrons, and other things. And actually, it's harder to construct a symmetric function than an anti-symmetric function.  spin orbitals are just the combination of the name of the orbital with whether the spin is up or down. And the reason for this is it's easier to do the algebra. There are rules that are easily described. For every kind of orbital, an orbital that is a scalar, that doesn't depend on quantum numbers, that has a selection rule delta SO of 0; and for something like a one-electron operator, that's a delta spin orbital of 1 and 0. The algebra for each is something you work out, and then you know how to do it. Sally Kohn: We want to be able to do something like what you did in chemistry in high school. She says we want to know the trends of things, and we know how to interpret them. Kohn says we have to use a different notation, J tilde minus K tilde, for the ground state of helium 1s squared. She explains how to remove the tilde from the notation, and how to get the right sign for the 1 over rij Hamiltonian, which is a Hamiltonian. Hund's rules is all about, of all of the states that belong to a particular configuration, which one is the lowest? One-- which one, not the second lowest. And why do we care? Because in statistical mechanics everything is dominated by the lowest energy state. So you want to know what are L, S, and J for the lowestenergy state of a configuration. And it's the one with the maximum L. And then the last step is, what is lowest J for that LS state? And that's kind of cute. L minus S, absolute value. If N is greater than 3, you have the lowest being-- the highest possible value of J is equal to L plus S. When you have a half-filled shell, the lowest state is usually an S state with maximum spin. So Hund's rules tell you how to identify, without knowing beans, what is the lowest energy state, and it's never wrong. Well, maybe sometimes wrong, but that's because of one of my things. Why is that? Well, the 4s sees the larger charge is less shielded. So it goes in. Then when you go from potassium to calcium, you put another electron in this. And that's true. Now you take an electron-- I'm cooking my own goose. If you take one of these electrons away-- this is not the way I wanted it to come out-- you find yourself in a 3d state. 3D penetrates a little bit under 4s. I can't explain it in a way that's going to make sense.

ROUGE-1: 29.60, ROUGE-2: 27.78, ROUGE-L: 27.10
BERTScore: 73.89

==============================================
==================== [41/100] ====================
Summary:
As a nurse we will transfuse a patient who is low on red blood cells with new blood cells via a venous access of some type. Red blood cells are very vital for our survival and how our body works so in other words our body can't function very well without them. When is a patient transfuse well this really depends depends on what's going on with the patient their vital signs how are they tolerating that low blood level and recent guidelines by the American Association of blood banks recommends transfusing blood when hemoglobin levels fall to 7 to 8 grams per deciliter. Nursing lecture exams but you need to know it for the job okay the first thing before a patient is even transfused is a lot of prep work that is super important and essential because our prep work helps prevent transfusion reactions. Most hospitals require that you're a registered nurse in order to transfuse the blood so again follow your Hospital protocol with that so let's say you got an order for patient to be transfused with two units of packed red blood cells what's the very first thing that's going to be done. You'll want to get informed consent tell the patient what they're going to be receiving assess their understanding of it also this is a good time to ask about their allergies and if they have received any blood transfusions in the past. If you're giving a patient who is in fluid overload or congestive heart failure has renal failure and but they really need blood you need to be looking at that because they may be at risk for circulatory overload. You can access that and take a quiz that can test you on that but just a quick review who is the universal donor who can donate to all types. You typically want an 18-gauge or larger IV site. It takes anywhere between two to four hours for a unit of blood to transfuse. You use special tubing which is called Y tubing with an inline filter which helps filter some of those substances out of the blood before it actually goes to the patient. You never use any met other medications or any other fluids only saline because dextrose and red blood cells don't get along it can cause them to clump up together so only 0.9% normal saline. breath headache backache or nausea and vomiting and if this happens you'll immediately want to stop the transfusion. You want to minimize the amount of blood that the patient's going to receive in case they do have a possible transfusion reaction we can turn that blood off. We want to stay with them during the first 15 minutes because that's when most transfusion reactions occur. We're gonna be watching their bottle signs throughout so you're going to get bottle signs and again this is depending on your hospital protocol. If your patient says I have a backache all of a sudden or I'm having chest pain or my head is hurting that's a red flag c-4 chills t4 tachycardia especially if it's really increased from baseline I for increased respirations same thing with that increase from baseline. Watch the earring closely and then in for nausea GI issues like diarrhea then when the transfusions done your patients tolerated it well you'll want to flush that remaining blood out of that line with that saline. it in the back of your mind so the first thing what you want to do is stop the transfusion. You want to note mentally what time this occurred what time you stopped it because you'll be documenting this later on. The physician depending on what type of reaction they suspect the patient's having or how severe it is they may order some medication so it varies some things they can orders like corticosteroids which is going to suppress that immune response along with fluids helping flush out that free hemoglobin that's in the body. diuretics also some labs are going to be ordered they want to look at those claudine levels because remember if this is hemolytic type because a lot of times they don't know what type of reaction this patient is having. You'll be collecting urine urine on them looking for the free hemoglobin that's came from those red blood cells I have lysis and whenever you are disconnecting your tubing over here do not throw it away don't throw any of it away because you'll be sending that one with the leftover blood and any other documentation to the blood bank.

ROUGE-1: 29.37, ROUGE-2: 28.61, ROUGE-L: 28.49
BERTScore: 64.23

==============================================
==================== [42/100] ====================
Summary:
The unit came out of the Manhattan Project. The idea of the scientist was to confuse potential spies towards what cross-sections for nuclear processes are. One barn is a cross-section where it's really, really hard to miss. In this context, also, the shed was introduced. This is not very popular today anymore. It turns out that this idea of confusing the readers of papers or of discussions turned into a new standard. So we continue the discussion in 8.701 on units. In particle physics and in nuclear physics, use a system called natural units. This system is based on fundamental concepts of quantum mechanics and special relativity. The idea here is that we replace kilogram, meters, and seconds by h-bar, which is a unit of action in quantum mechanics, c, the speed of light, and GeV, where GeV is a typical approximate mass of a proton. When you talk about relativistic equations, E equals m c squared and all those things, m, E, and also the momentum have the same unit. equal to 0.197 GeV femtometers. On top of this, it's useful to use Heaviside-Lorentz units and combine them with some measurable units we discussed. So this is also very convenient, to have this kind of convention. So we'll use those natural units as we go through class. In some examples, we will use SI, in others, use natural units. This will always be clear from the problem we're looking at. So you should already know the answer from previous discussions in the lecture.

ROUGE-1: 50.29, ROUGE-2: 48.47, ROUGE-L: 46.65
BERTScore: 69.20

==============================================
==================== [43/100] ====================
Summary:
Jeffrey Grossman: We're going to talk about energy storage. He says the planet is kind of a storage device for that thing over there. Most energy storage needs are going to be around a year or less, he says. Grossman says electricity is more and more important in our lives. We'll go into batteries and then talk about the chemistry of batteries and throw in a couple of, why this matters, he writes..com/jeffrey-grossman/energy-storage-technique. would be like, I've got energy storage technology and it's got this much density, this much power-- well, then it's going to last if I use it at that power for 41 days and so forth. That sounds like a good thing, right, so shouldn't we be pumping water up hills all over the place? Well, obviously, if you think about it, you don't do that in your phone. You don't evenDo that in a town. Why? Because pumped hydro is extraordinarily limited. It's very low energy density, right. The relationship between electricity and chemical reactions is called electrochemistry. Electrochemical energy storage isn't just one type of thing, it's not just one battery. It's this broader definition, but we're going to talk about batteries today. We're getting to the point where we're electrifying everything. We need electricity almost everywhere, even in your combustion car. You need electricity a lot more than you used to, than 20 years ago, than even a few years ago. The second law of thermodynamics makes you pay a penalty of energy if you convert it into heat. energy storage is so appealing, because you don't really pay a penalty. I can go back and forth without the second law hurting me. Entropy is about accessibility to states. It's about how many states you have to be in. Electric cars, for example, have a very high efficiency depending on the car, but upwards of 90% in terms of converting that electrical energy back and back. These are the things that matter in batteries, but so many other things matter, and this is why this is a complicated problem. change not going from your computer to your phone but from your car to a grid. And so the needs are all going to be different, and then which ones of these things you care about is going to depend on what chemistry you think about. It all comes down to the chemistry as we're going to see. It's all about the chemistry. And just to put a few numbers down, the cell phone has a few watts of power. Light bulb is 10 to 100. A AAA battery has 1.5 watt hours of storage. Needed to drive a car 200 miles, 100 kilowatt hours. In the 1800s, a scientist tried to make a connection between things that move and electricity. He hooked up a frog to a lightning rod and the frog went crazy. He deduced that the motion of a frog, and all motion of all living creatures, generates electricity. The story of Frankenstein was written because they went around and electrocuted things and showed that they moved, and often they weren't alive, and so Mary Shelley, I think, saw that. And I don't think they ever used humans, but still, people's imaginations and so forth. There's over a billion people who can't read at night because there's no access to electricity. $25 billion liters of kerosene are used to meet the basic lighting needs in a lot of these places. You've got to store energy to make these things actually useful, right, and I really like this program so I wanted to make that one of my, why this matters. And we'll go big on the next one, but let's get to the chemistry. It's Volta's two different metals. lots of metals. That's what he did. And he showed that the frog moved with most of them, so it's the metals. So let's take these two classic metals and show what happens. You've got copper and you've got zinc. Now, in this case, I'm just going to have a zinc. This is the zinc piece of metal, and this is a piece of zinc. And I put it in a solution of copper ions. And what happens is there is a transfer of electrons, right. The Nobel Prize was given to the development of lithium ions as a storage technology. A lithium battery works much the same way in the sense that you have the separator here. That's the electrolyte. But now it's rechargeable. All the chemistry is the same. You're shuttling ions back and forth. You've got that external circuit. And there's a whole bunch of chemistry. So what has exploded in the last-- well, that's a bad word for batteries because that is a danger with lithium batteries. The growth in solar and wind has been incredible over the last 10, 20 years. The problem, as I think I've shown you, is the variability. And so here's the question. How do you fix this? You got to store it. There's no way to use renewables at large scales unless you store it, and we're at essentially the tipping point.go in everything now, all the way up to the grid. And that was my last Why This Matters in the last 2 and 1/2 minutes. This is the grid now. But as they add more and more of this renewable to the grid, its price goes down when it's available. In fact, it's now not economical for them to add more PV unless they can add it with storage. You can't solve renewables at the scale of the grid without storage. And no option exists today. It might be batteries. It may be batteries, but there's nothing that actually does it today at the scales that we need. So that's a real challenge for chemistry.

ROUGE-1: 22.45, ROUGE-2: 21.14, ROUGE-L: 21.27
BERTScore: 65.05

==============================================
==================== [44/100] ====================
Summary:
Last time we talked about some of the kind of the bigger questions in deep learning theory. And today, we are going to start talking about the optimization perspective in deeplearning for two lectures. The main focus is to analyze what the functions you are optimizing look like so that you can use some trivial or some standard optimization algorithm for it. The bigger question we are trying to address here is that why many optimization algorithm are designed for convex functions. But why they can still work for nonconvex functions? In calculus, x is a local min of the function f if there exists an open neighborhood-- let's call it n-- around x such that, in this neighborhood n, the function value is at least fx. If x is the local min, it means that the gradient of fx is 0. The gradient square of the Hessian of f is PSD. So these are necessary conditions for being a local minimum, but not vice versa. So why? I guess a simple example here, I cannot-- it's easy to come up is that you just say maybe-- actually, I'm not taking the simplest one. execute our prime. So this is what will happen. So here is a condition called strict set of conditions. And if you certify the condition, then you remove those pathological cases which requires high order derivatives. So-- sorry. Strict-saddle condition-- so in some sense, I guess I'm not sure whether this makes sense before I define it. But generally, you are basically saying that you want to rule out this kind of somewhat subtle possible candidate of local minimum. So every point, whether it's a local minimum or not, can be told from examine only the first order gradient. is the definition. So we say f is alpha, beta, gamma strict-saddle if, for every x in RD, it satisfies one of the following. So if you can satisfy number one here, you cannot be a local minimum. You can have a point with gradient 0 and Hessian is PSD, right, it doesn't satisfy one, two. That's the pathological case. You have to be close to a real local minimum, right? So that's what this strict-Saddle condition is saying. exactly the thing that we did for the strict-saddle. But if you think about it, it's basically the same statement. Anyway-- so cool. So we are basically done with the first part, so about identifying the subset of functions that are easy to optimize. And next, we are going to show some examples where these kind of properties can be proved rigorously for machine learning situations. But these examples are pretty simple. They are not deep learning. So these are still roughly the best that people can do in some sense. The best rank one approximation is basically the Eigendecomposition or the singular value decomposition of the matrix here. Just for simplicity, let's also assume this matrix M is symmetric. And this becomes a nonconvex objective function because you have a quadratic term here. And our goal is to show that, even though it's non Convex, all local minimum of this g are global minimum under the assumptions that we have mentioned, so like rank one, PSD. So how do we prove this? So as you can imagine, the proof is pretty simple. The plan is very simple. You first find out all stationary point, the first order stationary points. And then you prove that they are all global minimum. So basically, it's just more or less like we solve all of these equations and see what are the possible local minimum you can have, right? So let's firstly use the stationary points, a gradient condition. So gradient of g of x, I'm not going to give a detailed calculation here. This is equal to minus this times x. The Hessian is in dimension d by d because you have d parameters. Sometimes your parameters is a matrix, and the Hessian becomes a fourth order tensor. And it's kind of very complex to be even just written down to just write down the Hessians. So here is a kind of a very useful trick and which actually also has some fundamental reasons that this is useful. So the useful thing is that, if you look at the quadratic form regarding the. Hessian and you transpose Hessian v or v in the part that was Hessian. times v, this is the quadRatic form related to Hessian, and this is much easier to compute. point and is x is not global min then moving in v1 direction-- so moving inv1 direction wouldn't change our function very much. Because you have stationary point, that means your point is flat. And that's why it's not a local minimum. So that's basically the gist of the analysis. OK. so now, let's talk about matrix completion, which is kind of like an upgraded version of PCA. And as I said, this is actually a pretty important question in machine learning. So let me define the question first, and then I can briefly talk about why people care about it. In a matrix, the columns are indexed by the users. Amazon wants to understand what each user's preference is, want to know that each user likes which item, right? So the Amazon has an incentive to just fill in the entire table. So that's why you have to recover all the rest of the entries to serve the users better in the future. And the most used method to solve this is basically nonconvex optimization to find this ground truth matrix M using the fact that you have a low rank structure. it's unlikely it can work. So basically, that is saying that p is bigger than roughly 1 over d. And speaking of the objective functions, this is actually a pretty commonly used method in practice. So you just say I'm going to minimize this function that's called fx, which is defined to be that basically you have a parameterization called xx transpose. And you want to say this matrix actually faced all my observations, right? So you are taking a sum over all possible observed entries because these are the only cases you know what the entries are. convex transition methods and so and so forth. However, those methods often have stronger guarantees. For example, they have tighter sample complexity bounds. In practice, just because the convex transition takes too long time, people actually are using objective functions or methods like this. And that's why it's also practically relevant to analyze these kind of objective functions because they are, indeed, used in practice. All right, so our main goal is to prove that this objective function has no local minimum, all local minimum are global. also have strict-saddle conditions. So you can also prove that. It's just that I didn't include it just for the sake of simplicity. All right, so I guess so the proof is obviously too long to cover in 1 minute. So I guess I'll leave it to the next lecture. I can take some questions if anybody has any questions. Otherwise, I think we are good today. OK, there's a question. So are there any network models where these are known to be hold? The answer is no especially if you look for a global property, like globally, all local minimum are global. assume that the input are linearly separable, then there is a proof for this. And there are a bunch of other cases where you can have some partial results. Next week, in the next lecture, maybe the second half of next lecture,. I'm also going to give another result, which is somewhat more general. It applies to many different architectures, but it has other kind of constraints. First of all, it doesn't really show exactly these kind of landscape properties. It shows that these kinds of properties holds for a region, for a special region in the parameter space.

ROUGE-1: 24.08, ROUGE-2: 23.50, ROUGE-L: 24.01
BERTScore: 67.14

==============================================
==================== [45/100] ====================
Summary:
Iceland is one of the world's most active volcanic hotspots crafer has erupted 30 times in a thousand years and last blue in the 1980s. Scientists are now preparing to drill into it the is to learn more about how volcanoes behave so that we can better predict eruptions and also tap into a super hot source of energy volcanoes can be spectacular but they're also devastating around the world millions of people live close to them here in Iceland. Researchers here hope their work will change that helping to save lives and money while also pioneering a form of volcano power.

ROUGE-1: 23.42, ROUGE-2: 22.80, ROUGE-L: 23.42
BERTScore: 66.24

==============================================
==================== [46/100] ====================
Summary:
Taxes are one of three different kinds of tax we're gonna have aggressive so with our regressive tax as people earn more income base they are attacked a smaller amount of smaller percentage of the rounding. It's not the dollar amount it's gonna be obvious that everyone is must and a larger dollar amount in taxes their income goes up it's nothing down it's the percent of their income that you're spending that's what determines put the tax is regressive and what we see here is taxes. 17,000 Orlando will go to top Bill and Hillary will both oh eight thousand each so our total BAM attacks if we put this way 19,000 if we tax the household this goes back to this idea we discussed the horizontal language all right we want to treat people that are in similar circumstances. If we tax them as a family they're both a 20-3 if we Tax them individually they're paying 16. If they're married and they're not they'repaying 16,000 these people are paying 23,000. That's the American penalize people for being married. was in this year spec's 383 three point eight trillion dollars when we spent almost three point nine those caught 329 Troy and we had parameters seeds we have revenue three point three trouble nice we have seven under six hundred billion dollars and we can engage in types of policies here too. We're under the change and your demand as we can have learn called spam canary fiscal policies. We can have contractionary order restrictions physical follows so if our expansionary fiscal policy here we're seeing the decrease in taxes or an increase in government spending and if you have a deficit the deficit is going to get larger. on the progressive income tax does the exact same thing because if people's income is $50,000 right and then for some reason it falls to say $20,000 if you have progressive taxes they're going to pay their tax burden their average tax rate is going to automatic. When you've got the expansion apart you're going from time here t2 to t3 people theoretically don't need food stamps and Welfare right so spending on these guys is automatically reduced. You're automatically seeing reductions in government spending automatically.

ROUGE-1: 23.17, ROUGE-2: 22.67, ROUGE-L: 23.17
BERTScore: 66.57

==============================================
==================== [47/100] ====================
Summary:
If I were to repeat this 1,000 times, so every one of those 1,.000 times they collect 124 data points, then in average, the number I should get should be close to the true parameter that I'm looking for. And so what I want is to have a small bias, hopefully a 0 bias. If this thing is 0, then we see that the estimator is unbiased. So this is definitely a property that we are going to be looking for in an estimator, trying to find them to be unbiased. But we'll see that it's actually maybe not enough. for sum of independent random variables, now it's time to wake up. So we have the variance of something that looks like 1 over n, the sum from i equal 1 to n of Xi. And so if I'm interested in the quadratic risk-- and again, I should just say risk, because this is the only risk we're going to be actually looking at. Yeah. This parenthesis should really stop here. I really wanted to putquadratic in parenthesis. So the risk of this guy is what? Well, it's the expectation of x bar n minus theta squared. the order in which I draw those curves. All right. So let's find-- I'm going to give you three candidate estimators, so-- estimators for theta. So the first one is definitely Xn bar. That will be a good candidate estimator. The second one is going to be 0.5, because after all, why should I bother if it's actually going to. be-- right? So for example, if I ask you to predict the score of some candidate in some election, then since you know it's going. to be very close to 0. 5, you might as well just throw0.5 and you're not far from reality. The bias is 0 and the variance is equal to theta, 1 minus theta divided by n. The second number is better for the other guy, so I will definitely go for this guy compared to this guy. The bias is actually-- just for simplicity, I can think of it as being X1 bar, the average of itself. And I have the variance that's actually n times smaller when I use my n observations than when I don't. So this guy is gone. The risk is really telling you how much fluctuations I have around average around the theta. The bias doesn't count in the risk. So if you're sure-- let's say you're talking about presidential election, you know that those things are going to be really close. Maybe you're actually better by predicting 0.5 if you know it's not going to go too far. But that's for one observation. But if I look at the risk of Xn, all I'm doing is just crushing this curve down to 0. can be large. Or you have 0 bias-- you have a bias, but the variance is 0. So you can actually have this trade-off and you can find things that are in the entire range in general. Those things are actually-- those trade-offs between bias and variance are usually much better illustrated if we're talking about multivariate parameters. In this class, it's mostly one-dimensional parameter estimation, so it's going to be a little harder to illustrate that. But if you do, for example, non-parametric estimation, that's all you do. what a confidence interval is. And so we fixed a statistical model for n observations, X1 to Xn. The parameter theta here is one-dimensional. Theta is a subset of the real line, and that's why I talk about intervals. A confidence interval of level 1 minus alpha-- so we refer to the quality of a confidence intervals is actually called it's level. It takes value 1 minusalpha for some positive alpha. The closer to 1 it is, the better the confidence interval. The probability that I contains theta is at least 1 minus alpha. So it better be close to 1, because it's really telling you that whatever random variable I'm giving you, my error bars are actually covering the right theta. If you want this to hold for every n, you actually need to use things such as Hoeffding's inequality that we described at some point. So here is the example that we had. So the sample average, Xn bar, is a strongly consistent estimator of p. also to some standard Gaussian. We've seen that when we saw Slutsky as an example. And so those two things-- actually, just because I'm taking the limit and I'm only caring about the asymptotic confidence level, I can actually just plug in consistent quantities in there, such as Xn bar where I don't have a p. And that gives me another confidence interval. So this by now, hopefully after doing it three times, you should really, really be comfortable with just creating this confidence intervals. is no. The only thing you're losing is the rate of convergence of the central limit theorem. Of course, the price you pay is that your confidence interval is wider than it would be if you were to use Slutsky for this particular problem. So it depends on how comfortable and how critical it is for you to put valid error bars. If they're valid in the asymptotics, then maybe you're actually going to go with Slutky so it actually gives you slightly narrower confidence intervals. how critical it is for you to output valid error bounds or if they're really just here to be indicative of the precision of the estimator you gave from a more qualitative perspective. What is the slope of the function 1 over square root of X1 minus X around the value you're interested in? And so if this function is super-sharp, then small fluctuations of Xn bar around this expectation are going to lead to really high fluctuations of thefunction itself. Its derivative really is what matters. When we do maximum likelihood estimation, likelihood is the function, so we need to maximize a function. And if I give you a function, you need to know how to maximize this function. Sometimes, you have closed-form solutions. You can take the derivative and set it equal to 0 and solve it. But sometimes, you actually need to resort to algorithms to do that. And we'll briefly touch upon it, but this is definitely not the focus of this class. OK. So we'll do a little bit of reminders on those things. When you compute probabilities on one distribution, you should have the same probability on the other distribution pretty much. So what we can do is say, well, now I have two candidate distributions. So if theta hat leads to a candidate distribution, and this is the true theta star, it leads to the true distribution according to which my data was drawn. That's my candidate. As a statistician, I'm supposed to come up with a good candidate. And what I want is that when I'm computing probabilities for this guy, I know what the probabilities for the other guys are. The probability that X is equal to little x is 0 for a continuous random variable, for all possible X's. There's just none of them that actually gets weight. So what we have to do is to describe the fact that it's in some little region. So I have this density, such as the Gaussian one. And the probability of X falling to some subset of the real line A is simply the integral of the density on this set. That's the famous area under the curve thing. The total variation has some properties. So let's keep on the board the definition that involves, say, the densities. So think Gaussian in your mind. And you have two Gaussians, one with mean theta and one withmean theta prime. And I'm looking at the total variation between those two guys. So if I look at P theta minus-- sorry. TV between P thena and P thea prime, this is equal to 1/2 of the integral between f theta, f thena prime. The total variation equal to 0 implies that P theta is equal to P theTA prime. If this thing being small implied that P. theta could be all over the place, that would not help very much. The fact that you need two definitions of the [INAUDIBLE],, is it something obvious or is it complete? PHILIPPE RIGOLLET: I'll do it for you now. Let's just prove that those two things are actually giving me the same definition. PhilipPE RIGOLLET: The set A star is the set of X's such that f of X is larger than g of X. That's the set on which the difference is going to be positive or negative, he says. He shows that if he takes any other A in this integral than this guy A star, it's actually got to decrease its value. Rigollet: The first one has to be larger, because this thing is actually equal to a non-negative number. inequality. And so that means that we have this actual total variation distance between probability distributions. And here is now a statistical strategy to implement our goal. Remember, our goal was to spit out a theta hat, which was close to P theta star. So hopefully, we were trying to minimize the total variationdistance between P thena hat and P thea star. But here, one of the arguments is not known, so we need to estimate it. But it's very unclear how you would build this estimator of TV. The total variation is telling you how far in the worst case the two probabilities can be. This is really the intrinsic notion of closeness between probabilities. The KL divergence is non-negative. Who knows the Jensen's inequality here? That should be a subset of the people who raised their hand when I asked what a convex function is. And actually, if you pay attention, we're actually really throwing out everything else. So they're not symmetric. But it's non- negative and it's 0 if and only if the distributions are the same. of X. I'm just shifting the plot of my function up and down, but the minimizer stays exactly where it is. Every time it's a translation on the y-axis of all these guys. And the value that I translated by depends on theta star. So we'll just keep going on this property next time. And we'll see how from here we can move on to-- the likelihood is actually going to come out of this formula. Thanks. Back to the page you came from.

ROUGE-1: 25.10, ROUGE-2: 24.32, ROUGE-L: 24.25
BERTScore: 68.68

==============================================
==================== [48/100] ====================
Summary:
Markus Klute: I'll explain how the course is structured, how you participate and how you'll be graded. To get started, this class will be taught in an inverted classroom or flipped classroom setting. It's going to be online-only so we will not have in-person opportunities to discuss. You can log in at any time and we'll discuss whatever you want to talk about. For that let's contribute to the Doodle poll, which I posed during the first time of the first class. The course evaluation or your evaluation in this course will be made up 50% out of homework. The paper presentation I was talking about will have 20 points. And then there's going to be two short oral exams. The great divide-- the great divide or the grade divide-- at 85% between A and B, 70% between B and C, 60% between C and D, and below 50% earns you an F. I don't think anybody can get into this environment as long as they participate.

ROUGE-1: 45.07, ROUGE-2: 42.11, ROUGE-L: 44.81
BERTScore: 71.31

==============================================
==================== [49/100] ====================
Summary:
In this problem, we are given a joint PDF for x and y. And then we are asked to compute the variance of x plus y. We're told we should compute this variance by using something called the law of total variance. So first, I want to focus on this term-- the conditional expectation of X plus y conditioned on x. So this is actually just x plus the expectation of y given x. And because it's uniformly distributed and because expectation acts like center of mass, we know that the expectation should be the midpoint.  memorizing formulas might seem like cheating, but there's a few important ones you should know. And it will help you sort of become faster at doing computations. And that's important, especially if you guys take the exams. So that's it. See you next time. We'll be back in a few days with a new episode of "The Daily Discussion" to talk about the week's top news stories. Back to the page you came from. Click here for the next episode of The Daily Discussion.

ROUGE-1: 18.96, ROUGE-2: 16.11, ROUGE-L: 15.88
BERTScore: 65.28

==============================================
==================== [50/100] ====================
Summary:
In the second half, we'll talk about multisequence alignment. This is a generalization of the two-dimensional array that we had before. Let's go beyond triple, but to a very simple dinucleotide alignment. And we will say that this is the optimal multiple alignment. We want to generalize the kind of algorithms we've been using. And again, this is again, the MIT OpenCourseWare team's work. We hope to see you in the next half of the course. will be a recursive algorithm where the score of a two-character string is defined in terms of the maximum of various shorter strings. The number of different cases we have here-- before, it was 3 for a global alignment, which was k, being the number of sequences, was 2. Now k is3 for a three-way comparison. And all possible subsets is 2 to the k minus 1, in this case, so it's 7. You can see the first one is no insertions or deletions. The next three are two insertion or deletion in the three different ways that can happen. The time complexity is have to do 2 to the k comparisons per node. The larger k is, the more you can explore. If you know where the band should start and how wide it should be, you can essentially prune off many of the nodes without really losing any optimality. There are very good reasons for inferring structure or function without experiments, just from sequence. And the two that we'll illustrate are not guaranteed to be optimal, but on the other hand, they don't necessarily require arbitrary pruning. in the next couple of slides is a tree alignment, as illustrated by ClustalW. By the way, pruning is illustrated by a program called MSA, which is short for multisequence alignment. And we'll show a star alignment. Later on into the transcriptome part of the course, we will talk about the Gibbs algorithm. So let's walk through ClustAlW, and then a star algorithm. Here's progressive multiple alignment. So here, we have five sequences instead of four, but it's the same thing. You do all pairwise similarities. And so you can construct a tree. give a score. These scores are the scores that would have come out at the end of that traceback in the pairwise alignments. And so we'll use S1 as the focus of the star geometry. And we'll get to the Gibbs sampling later. But in general when you have a hard problem, where you can't comprehensively go through the entire space, what you do is sample it. You say, let's try a few things, and try to randomly sample it, and maybe even develop locally. by having this storing, this pairwise or multi sequence in a matrix-- so that's actually you've done a trade-off where you've taken up computer memory in order to save time. If you're willing to sacrifice a little accuracy or a little comprehensiveness, then you can save even more time or memory. Now we want to use motifs, which is the sort of thing that you get out of local alignments, to find genes. And we're going to use the motifs and the finding genes as a way of introducing a particular motif. cell type and the rare [INAUDIBLE],, rare messenger RNA within a cell type. GEORGE CHURCH: Let's talk about the sizes of proteins. How is it that it precipitously drops off at 100 amino acids? Why are there so few proteins that are short? And there are slightly more short proteins in Mycoplasma? Any guesses why they're so few? Why does it drop off at 10,000 of amino acids for the largest proteins? When we get to proteomics, we'll talk about ways that you can empirically, by mass spectrometry and so forth, find those small proteins. The 23 sRNA encodes this pentapeptide, which confers erythromycin resistance at low levels in wild type. It is not a mutant kind of peptide. It's the normal pentapepticide. This has to be sensing the translation process itself, asking whether the transfer RNAs are charged up with amino acids enough that you're getting efficient translation. If you want to know whether you need to make tryptophan, phenylalanine, or histidine, you ask whether there's enough of it around to do translation. you're not, then you'll pause here. That ribosome will hesitate, waiting for the right transfer RNA. And as it hesitates, this RNA changes. It's folding. And a series of events results in-- if it's hesitating, then it wants to make the biosynthetic genes downstream to make more of the amino acids. So the tRNA has to be charged up. So you get this nice, little feedback loop that the hesitation causes a change in RNA, which causes change of transcription. be A, C, G, or T. These are four different sequences, real start sites, that we've aligned, either manually or by computer. This is dead easy to do the alignment, but the interpretation here is the position upstream of the start codon doesn't matter. And so this is the weight matrix or position-sensitive substitution matrix. But it's not the most precise way of representing this. It's position sensitive, but we've lost the higher order correlations between positions. And you can find these motifs that have great biological significance. Bayes' theorem says that the probability that the model given the sequence is equal to the probability of the sequence given the model. We're going to be doing-- of the various applications, we had recognition discrimination and database search. And we're talking about a particular sequence, where we can have randomness at the mononucleotide level rather than the level of the Markov chain. And I want to give you some biological rationale for why you can have nonrandomness at every order of a chain. Gs with them. Many organisms repair-- well a T near a T in the presence of ultraviolet light will get mutated to something else. And so you'll lose that particular dinucleotide out of the 16 possible dinucleotides. Every place you've got a methyl CG turns into a TG, and you tend to lose the CGs, unless they're not methylated. And similarly, you can have rare codons. And hence, these turn into rare triplets. And we'll get to that. The triplet bias, documented here that this 10 times lower frequency of ATG than of some of the other arginine codons. We've said that CGs are underrepresented in the genome as a whole, and they're over-represented in promoters. This particular transition of what's the probability of getting a G given a C in the 5-prime position-- this is one of those conditional probabilities. This is a Bayesian that we had set up a couple of slides back. And so this particular arrow going from a C to a G is represented by this probability. There's 16 possible transitions, including four homopolymers, AA, TT, CC, GG, and 12 transitions of the other dinucleotides. We've got CG islands where the CGs have been protected from methylation, and hence, protected from mutations. And then outside are the ocean, which are not protected. And you want to know where the island begins and ends because that helps you know where regulatory factors are. And so this Markov model that you have has to be different for whether you're in an island or not.

ROUGE-1: 29.05, ROUGE-2: 27.62, ROUGE-L: 27.27
BERTScore: 67.48

==============================================
==================== [51/100] ====================
Summary:
in this module of tools and equipment we're going to explore cookware and storage wear. There are five basic metals that are used in cookware aluminum copper stainless steel cast iron and carbon steel aluminum pots and pans are the ubiquitous pans that you'll find in most every kitchen. Additional cooking materials include things such as non-stick coatings like teflon terracotta which is used in cooking things like tangents glass such as corning ware enamel wear which is also used in things like crock pots. interchangeably in many ways but believe it or not though there is a difference between these two does it make a difference in your cooking if you use one instead of the other. A saute pan or satwa is one with the straight sides and has a larger surface area which makes it ideal for tasks like searing meat or reducing a pan sauce. A sawtooth or skillet has a slope side and is used mainly in sauteing the slope sides providing the ample and perfect angle for flipping your food or as it's referred to as saute or to jump a rondeau is a favorite staple in a chef's arsenal and should be in any home cooks as well. Polycarbonate and stainless steel containers called hotel pans are designed to be used to store small amounts of ingredients on a refrigerated make table such as a sandwich station. They come in sizes ranging from 4 6 8 12 18 and 22 quarts and they often come with matching or size lids to accommodate them. They wash well by either hand or dishwashing and can stand up to years of usage other food storage options include what is often referred to by polycarbonate's trade name lexan or by the company trade name cambro. from full size hotel pans two-thirds pans half pans third pans fourth pans six pans and ninth pans and various other sizes each size is available in six six inch four inch and two inch deep models the name of the pan refers to how many of each would it take to equal a full size pan for example the nine it takes nine of the one-ninth pans or ninth pans to make one full-size hotel pan here you can see various different configurations depending on the usage needs many of the configurations require the use of a spanner bar to prevent the pans from falling in. called a third pan because you can fit three to a hotel pan now there are also half hotel pans which are simply just half the size of a hotel pans still coming in a two inch four inch and six inch depth. Large sheet trays which can be used for any number of things roasting reheating baking whatever you want to use them for. sip top bags often referred by their trade name ziploc are an indispensable storage device for kitchens they allow the storage of dry goods and other small quantity items. Not all cookie medals are identical cheap cookware often translates into cheaply made and burnt product. specialty cooking surfaces allow for more versatility in the cooking process and different results. The right size storage bin is essential why store two quarts of stock in a 22 quart container it simply doesn't do the right job and it takes up too much room in your coolers. You can never have enough hotel pans no matter what size you have this presentation will attempt to give an overview of the different small wares and pieces of equipment that you'll experience in a restaurant environment.

ROUGE-1: 31.42, ROUGE-2: 30.62, ROUGE-L: 30.55
BERTScore: 58.18

==============================================
==================== [52/100] ====================
Summary:
Charles Brockden Brown: The Enlightenment is a critique of a kind of impoverished view of mind or consciousness that over-relies on reason. We started to look at it a little bit with Barlow's [assumed spelling] Raven poem with Poe. The idea that in fact there is something wrong with reason as a faculty, and this can take a couple of forms. One is that all of those nice ideals that come along with the enlightenment may in fact be parasitic precisely on the kinds of negative types of thinking that they are designed to get rid of. self as author which requires him to create a self as character. In so doing he's able as author to pick out the defective font [phonetic] of type those errors, and either talk about the ways in which he was actually able to correct them in real life in his real life, or, saying I wish now that I would correct that, I can point that out as an error. It's a sort of textural do-over as he rewrites his life. The Passion of the Christ is set in Philadelphia in 1787, the same time as the Constitution is being framed a hop, skip, and a jump away. Brown is an interesting character because he is one of the first US writers to try to take advantage of first American copyright law which was passed in 1790. The copyright law does something interesting; it makes a profession of authorship possible, because it suggests that writing is property, and if you can write, you can be a writer.  copyright law doesn't protect US writers from being pirated abroad. It does nothing to prevent US publishers from publishing pirated editions of well-known books already circulating in the continent. So that seems like a bad thing for the aspiring US author. On the other hand remember what I said before, there is no patronage system in place in the early republic; therefore, what is emerging is a kind of marketplace for writing. Without that market, a young American author wouldn't be able to sell his writings and sustain himself as a writer. In 1798 Brown publishes his first novel Wieland: or, The Transformation: An American Tale. He says that his aim isn't merely to please the idle and the thoughtless, but actually he says to engage those who study and reflect. In Edgar Huntly there's another kind of scientific fact that is played with right? Sleepwalking. And you can see there is a certain kind of haste in the plotting of Edgar Huntley. There are red herrings, there are loose ends that are not exactly tied up, and that might be the result of the fact that it was the third book that he wrote in that year. happy, many, many pages. Has anybody read Clarissa or an abridged Clarissa? Clarissa's a seduction narrative right, in which a middle class heroine is menaced and finally seduced and raped by an aristocrat. And you might say right away in the early English epistolary novel there's the worry about what it is we're writing and reading. One solution early on was well, it would be realistic if these were letters that you were getting to read. Of course, the idea that people would spend that much time writing and then reading, would leave hardly any time for living. There's a worry that's probably a worry of Puritanism that there's something wrong with novels because they're fiction. And what is fiction, if not the opposite of truth? Fiction means lies, right? Why would you read this? If you do it's got to be only a guilty pleasure. There can be nothing improving by it. So fiction, as opposed to fact, fiction somehow has falsehood. Therefore early novels try to insist on their basis in fact. As if there were something that would then could be morally improving for that. Brown is trying to find a form of the novel that can be used to instill virtue and all of these other qualities. He's trying to wrest the novel away from its association with some kind of damaging femininity. In part because he wants to say that you know to accept that kind of separations of fear does neither men nor women credit. So if we could get away from this idea that somehow there's something feminized about the novel, it's good for both sexes. One last quote, this comes from the Reverend Samuel Miller. He says that if it were possible he would wholly prohibit the reading of novels. The Gothic novel is a forum that in the English context is probably pioneered by this guy, Sir Horace Walpole who wrote a novel that was called The Castle of Otranto that was published in 1764 on Christmas Eve and subtitled, A Gothic Story. Now Gothic, you know is a form of architecture I suppose, first and foremost. It's a kind of you know the pointy churches after the Romanesque arches that are more rounded. Gothic is more pointy. But it becomes a pejorative term by this time that is, that takes on the medieval association. of a trifle nevertheless has a role to play because it frees up these resources of fancy. Walpole had a decided predilection for what may be called the Gothic style, a term which he contributed not a little to rescue from the bad fame into which it had fallen. But it's interesting that the novel, the Gothic novel itself as a form doesn't take off in 1764. It has to wait about 25 years in and around the French Revolution and then it really takes off when a woman named Ann Radcliffe writes her Gothic novels. like that. There appear to be manifestations of the supernatural. She often finds herself in the various kinds of settings crumbling castles, dungeons, graveyards, darkened churches. Usually she almost escapes her persecutors, then they catch her. They are trying to get her out of greed or lust or both. And usually there's a Theodore-like protector who's kind of chasing after trying to save the day. Usually, they catch up, they don't catch up. In the end, all of the ghosts and supernatural manifestations that have tyrannized her are shown to be fakes. Gothic is almost always in English a kind of political anti-Catholic, anti-Italian sort of strain and you know anti-French I guess as well. In The Monk you might say is really horrific Gothic. In it the ghosts are real. It is blasphemous. And, this is a French an illustration from a French translation of The Monk. The story goes something like this. Juan Ambrosio is the abbot of the capechins [assumed spelling] and he's presented as a. kind of admired preacher. Elvira who is a noble lady who is one of his, you know in his parish. He's her confessor. With the aid of the satanic Matilda who conjures up the devil also for added backup, Ambrosia gets into Antonia's room and is about to rape her. So he kills Elvira by smothering her with a pillow, gives Antonia a sleeping potion, and throws her in the dungeon. She cries out so he kills her. At the same time there's another woman who is there named Agnes. She's been imprisoned for other reasons down there and she actually gives birth to a baby down in the [inaudible]. There's a kind of weird conflation of supernatural demonic activity, sexuality, violent and disgusting death. Ann Radcliffe makes a distinction between what she calls, let me see if I have it here, the distinction between terror and horror. Terror would therefore be linked to the sublime and the faculty of imagination. Her Gothic she would've offered was a form of terror. Matthew Lewis's you know disgusto Gothic is actually making fun of that. It's doing what might be called the anti-sublime. think there's a cultural reason why Gothic takes off, not when Walpole writes, but when Radcliffe and Lewis do. All those democratic ideals, liberty, equality, fraternity, turn into the horror of the terror, and that's a terror that is a horror. And then finally the Napoleonic era which is the re-institution of anti-democratic principles. So, the experience, the cultural experience of the French Revolution would suggest that this regime of reason is not all that it is cracked up to be. All of this you might say is the context for Charles Brockden Brown's Edgar Huntly. The author lays out what we might think of as the project of American Gothic. He says America has opened new views to the naturalist and politician but it has seldom furnished scenes to the moral painter. He's presenting this. Puerile superstition and exploded manners, Gothic castles and chimaeras are the materials usually employed for this end right? If you're not writing seduction obviously you're writing Gothic. But here in the United States we have an opportunity. The incidents of Indian hostility and the perils of the western wilderness are far more suitable. at the second, second to last paragraph on the page. But it suddenly occurred to me for what purpose shall I prosecute this search? What benefit am I to reap from this discovery? How shall I demean myself when the criminal, all right the one who murdered Waldegrave, is detected? I was not insensible at that moment of the impulses of vengeance. But they were transient. I detested the sanguinary resolutions that I had once formed. Right and that's a form of [inaudible] right, for talking about being bloody minded or being revengeful. Yet I was fearful of the effect of my hasty rage. insane. And he says oh curiosity if vicious if undisciplined by reason. Okay, so we can keep curiosity in check and disciplined if we use our reason. But how does that square with what he says in the next paragraph? Curiosity, like virtue, is its own reward. Knowledge is a value for its own sake and pleasure is a next to the acquisition without regard to anything beyond. It is precious even when disconnected with moral inducements and heartfelt sympathies. But the knowledge which I sought by its union with these was calculated to excite the most complex and fiery sentiment in my bosom. to your knowing cares, to the deep and incurable despair that haunts you to which your waking thoughts are [inaudible] from which sleep cannot secure you. I know the enormity of your crime, but I know not your inducements. Whatever you were I see the consequences with regard to yourself. I see proofs of that remorse which must ever be attendant on guilt. That is enough. Why should the efforts of our misdeeds be inexhaustible? Why should we be debarred from comforter? An opportunity of repairing our errors may be demanded from the rulers of our destiny. Clithero portrays Edgar here not as a redeeming confessor, but as an agent of perdition, of damnation. And if we look at what happens in these 4 chapters right, Chapters 4 through 8, which his Clithero's story, we see that he actually has had nothing to do with the death of Waldegrave. And the story reinforces the novel's picture of identity as confusing and inconstant right? Think about the motif's in there. There's that strange co-partnership of being that Mrs. Loramer has with her brother Arthur Wyatt. the hand and by which force could be exerted. Some spring therefore secretly existed which might forever elude the senses, but on which the hand by being moved over in all the directions might accidentally light. The process was effectual. A touch, casually applied at an angle drove back a bolt and a spring at the same time was sent in action by which the lid was raised above half an inch. No event could have been supposed more fortuitous, by chance than this. No measure that I could adopt enabled me to place the lid in the same situation in which I had found it. The opening of Chapter 13 suggests that Edgar may be feeling guilty too because you might say he's torn by conflicting allegiances to his dead friend Waldegrave and to Mary. Third sentence however was banished from my waking thoughts and occurred in an incongruous form to my dreams. Chapter 13, the chapter that follows cements our understanding of the perils between Clithero and Edgar. It gives us our first glimpse into Edgar's personal history. It comes fairly soon after Clit hero's account of himself and so it invites us to compare the two. Edgar's experience in the woods is key to the story. Brown maps the woods onto Edgar's mind. In exploring the woods, Edgar starts exploring facets of his own mind that have remained hidden from view. Brown: "Famine and blindness and death and savage enemies never fail to be conjured up by the silence and darkness of the night. I cannot dissipate them by any efforts of reason" "My cowardice requires the perpetual consolation of light. My heart droops when I mark the decline of the sun"

ROUGE-1: 29.98, ROUGE-2: 28.85, ROUGE-L: 28.94
BERTScore: 62.47

==============================================
==================== [53/100] ====================
Summary:
Markus Klute: We're starting a new chapter in which we look at tests and implications of special relativity. One experimental test is stellar aberration, which we discussed can be explained by special relativity and by velocity addition. We can look in particle physics at the decay of a pion into two photons. And those pions, they can have a lot of energy, for example, with a beta of 0.999 times the speed of light. And still the photons are of a very high energy. One of the hypotheses you could have is that the photon actually is a massive particle. This would directly modify Coulomb's laws, which are tested experimentally. Another class of experiment is where we look directly at time dilation, for example, in the decay of the muon. In all of this experimentation and experimental verification, it's important to understand the importance of uncertainties in the scientific process overall, he says. "Remember, when one has the historic perspective on science, one often forgets that a specific measurement comes with an uncertainty," he adds. bias. And he conducted the experiment at the time Einstein was proposing his theory. The bias really came from the model of an electron. And, as experiments, they were inconsistent with Einstein. So he said Einstein is wrong. Einstein and Lorentz are wrong. Planck looked at this and said maybe, but Einstein's conclusion immediately by looking at this was, no, this cannot be. And it took a little bit of time to tee up those experiments until 1940. We now know this was wrong, but scientific process happens in scientific environment.

ROUGE-1: 47.33, ROUGE-2: 45.28, ROUGE-L: 46.98
BERTScore: 68.54

==============================================
==================== [54/100] ====================
Summary:
In the colonial era, most American women of European descent lived lives much like those of their European counterparts: They were legally and socially subservient to men. Lower and working class women were actually more equal to men of their own classes, but only because they were, like, equally poor. In general, throughout world history, the higher the social class, the greater the restrictions on women. High class women have traditionally had the lowest mortality rates, which is one of the benefits of you know doors and extra lifeboats and whatnot. expected to marry and have kids rather than, like, pursue a career. Under the legal principle of “coverture” actually husbands held authority over the person, property and choices of their wives. Since women weren’t permitted to own property and property ownership was a precondition for voting, they were totally shut out of the political process. Citizens of the new Republic were therefore definitionally male, but women did still improve their status via the ideology of ‘Republican Motherhood’ domesticity decreed that a woman’s place was in the home, so rather than making stuff, the job of women was to enable their husbands to make stuff. “Woman is to win everything by peace and love; by making herself so much respected, esteemed and loved, that to yield to her opinions and to gratify her wishes, will be the free-will offering of the heart.” “The moment woman begins to feel the promptings of ambition, or the thirst for power, her aegis of defense is gone. All the sacred protection of religion, all the generous promptings. of chivalry, all of the poetry of romantic gallantry, depend upon woman�'s retaining her place as dependent and defenseless, and making no claims, and maintaining no right” The idea of true equality between men and women was so radical that almost no one embraced it. The truth is, most American women had no chance to work for profit outside their houses, so many women found work outside traditional spheres in reform movements. Reform movements were open to women partly because if women were supposed to be the moral center of the home, they could also claim to be moral conscience of the nation. Many of the most famous advocates for legally prohibiting the sale of alcohol in the US were women, like Carry Nation. 19th century. Women gave many temperance lectures featuring horror stories of men who, rather than seeking refuge from the harsh competition of the market economy, found solace at the bottom of a glass or at the end of a beer hose. Now don't get me wrong: Prohibition was a disaster, because 1. Freedom, and 2. It’s the only time we had to amend the constitution to be like, “Just kidding about that other amendment,” but it’S worth remembering that back then people drank WAY more than we do now. Many women were also important contributors to the anti-slavery movement, although they tended to have more subordinate roles. The most visible manifestation of it was the issue of woman’s suffrage, raised most eloquently at the Seneca Falls Convention of 1848. The movement for women's rights was primarily a middle-class or even upper class effort. It was an international movement. Often American feminists travelled abroad to find allies, prefiguring the later transatlantic movement of other advocates for social justice like Florence Kelley and W.B DuBois. American women changed the world for better and for worse, just as great men do. And along the way, they made “the woman question’ part of the movement for social reform in the United States. In doing so, American women chipped away at the idea that a woman’s place must be in the home. That might not have been a presidential election or a war, but it is still bringing real change to our real lives on a daily basis. I’ll see you next week.

ROUGE-1: 39.60, ROUGE-2: 38.12, ROUGE-L: 38.88
BERTScore: 67.42

==============================================
==================== [55/100] ====================
Summary:
ae houseman 998-999 uh very intellectual individual um with these two authors today um we're really kind of bridging the gap between the victorian and the modern. We're really transitioning and that's why these are the last authors that we'll focus on in this time period some of their philosophy some of the writing styles and things that motivated them. Most of his poetry especially towards the end if you look on the right there the grief and poetry era you know especially the last of his poems. Houseman can tap into that pain and that darkness and help you know other people and and help spread emotion through and through. To an athlete dying young is an article that i'm going to have you take a look at later. Look at what he says about that individual and how they you know can celebrate and reflect on that individual. The time you won your town the race we cheered you through the marketplace man and boy stood cheering by and home we brought you shoulder high today the road all runners come shoulder high we bring you home and set you at your threshold down. Early though the laurel grows it withers quicker than the rose eyes the shady knight has shut cannot see the record cut and silence sounds no worse than cheers after earth has stopped the ears now you will not swell the route of lads that wore their honors out runners whom renown outran and the name died before the man so set before its echoes fade the fleet foot on the sill of shade and hold to the low lintel up the still defended challenge cup and round that early laureled head will flock to gaze the strengthless dead. heart away give pearls away and rubies but keep your fancy free but i was one in twenty no use to talk to me when i was 1 and 20. Don't give your hard way don't fall in love you can spend money on them you can give them all this stuff but what ultimately don't you want to give them at this age don't give them your heart. "I know what's best i know what i'm doing you're not listening to the wise man or wise woman in that case" he had heard mention that okay so now he is crushed okay he's been defeated to some degree because when i was 1 and 20 i heard him say again all the heart out of the bosom was never given in vain to paid with size of plenty and sold for endless rue sorrow remorse in the footnote there. So giving it out okay it's you're going to be getting some pain and you're opening yourself up uh for a lot of sorrow and remorse and he gets struggled so you can see this kind of as the lover's lament which we haven't really done much of since the sonnets.

ROUGE-1: 45.60, ROUGE-2: 44.04, ROUGE-L: 45.42
BERTScore: 64.55

==============================================
==================== [56/100] ====================
Summary:
In quantum mechanics, the spin of a particle with a vector is quantized, and in terms of its length and its components. You find that it's square root of f times s plus 1 in units of h-bar. The components, and along any axis, actually-- and in this case here, the d-axis-- have eigenvalues, and they are listed here. And we find that there is 2s plus 1 possible values. But the question-- it's an obvious question-- which axis is a sensible choice for this problem? physical state of particles, which axis are the right ones to choose-- or, sensible? There's no right and wrong in this discussion. If you look at the orbital momentum of a particle, that's given by r cross p, where p is the momentum vector of the particle. So now, if you're looking at the total momentum, we have to look add the angular momentum and the spin of the particles together. This, then, immediately gets us to a new definition, then of helicity. the same connection. The pion here is spin 0, and if you discuss this in the rest frame of the pion, the electron and the positron fly off in opposite directions, which means that the spin doesn't in align to 0. So that's why this is highly suppressed. It's not 0, because you can find-- you can put [INAUDIBLE] into a rest frame, where you're just basically looking at both particles from one side that was coming to you, and in that case, it's allowed.

ROUGE-1: 54.23, ROUGE-2: 52.50, ROUGE-L: 54.03
BERTScore: 76.24

==============================================
==================== [57/100] ====================
Summary:
In the 5th Century BC Athens was a direct democracy that encouraged wide participation through the principle of ho boulomenos, or anyone who wishes. This meant that any of its approximately 30,000 eligible citizens could attend the ecclesia, a general assembly meeting several times a month. The Athenian system also relied on a 500 member governing council called the Boule, to set the agenda and evaluate proposals. Some ancient philosophers, including Plato, disparaged this form of democracy as being anarchic and run by fools. poll, all examples of how the democratic principle behind sortition still survives today. Polls show that the principle of sortition is still very much alive and well in the United States today. The majority of Americans support sortition, with the majority of people in the U.S. voting in favor of it by a wide margin in the last presidential election in 2008. The most popular type of vote in the 2008 election was for the Democratic Party, followed by the Republican Party and the Libertarian Party. The Democratic Party won the popular vote.

ROUGE-1: 36.82, ROUGE-2: 24.94, ROUGE-L: 25.12
BERTScore: 62.20

==============================================
==================== [58/100] ====================
Summary:
Last time, we covered Fourier series and the Fourier transform. Today, we're going to talk about the convolution theorem, noise and filtering Shannon-Nyquist sampling theorem and spectral estimation. Next, we'll move on to spectrograms and an important idea of windowing and tapering, time bandwidth product, and some more advanced filtering methods. The series of three lectures will continue on Thursday and Friday in New York and Washington, D.C. and on Friday in Los Angeles. as a function of time. At some frequency, here, 20 hertz. We compute the Fourier transform of that and plot that. So that's what this looks like. Here is a cosine at 20Hertz. And you can see that what you see is the real part as a. function of frequency. It has two peaks, one at plus 20Herz. And the imaginary part is 0. So any questions about that? Feel like I didn't say that quite as clearly as I could have? OK. we plot power in log base 10. A difference of an order of magnitude in two peaks corresponds to a unit called a bel, b-e-l. More commonly used unit is called decibels, which are 10 decibel per bel. So decibles are given by 10 times the logbase 10 of the power of the square magnitude of the Fourier transform. Does that make sense? Good question. All right, any questions about this and what the meaning of decibela is? The faster something moves in time, the more stretched out the frequencies are. We're going to talk about the Fourier transform a Gaussian noise and this power spectrum of Gaussian Noise. We'll talk about how to do spectral estimation and we'll end up on the Shannon-Nyquist theorem and zero padding. And there may be, if there's time at the end, a little trick for removing the line noise from signals. OK? All right, so that was just a brief review of what we covered last time. that particle can be computed as the Fourier transform of the wave function. So if the particle is more lo-- [AUDIO OUT] in space, then if you compute the Fouriers transform of that wave function, it's more dispersed in momentum. OK, so the uncertainty in momentum is larger. So this concept of time bandwidth product in the physical world is what gives us the Heisenberg uncertainty principle. It's very cool. Actually, before I go on, you can see that in this case, the Fouriester transform of this function is the same function. a convolution. The convolution theorem tells us that the Fourier transform of y is just the product of the Fouriers transform of g and the Fouriester transform of x. Michael FEE: I'm going to walk you through how you derive that. The derivation is kind of cute, and I enjoyed it. But it's also really powerful. OK, so let me show you what you can do with that. We're going to calculate transform of a Gaussian, some window centered around 0 in time-- this is a function of time. The Fourier transform of a Gaussian is the product of a sine wave and a cosine function. You can figure it out very simply, because you know the Fourier transforms of aGaussian. There are many, many examples of interesting and useful functions in the time domain that you can intuitively understand just by having this idea. It's very powerful. All right, let's talk about Gaussian noise and the power spectrum of noise. And we're going to eventually bring all these things back together. wanted to show you what the autocorrelation function of this looks like, which I think we saw before. So if you look at the distribution of all the samples, it just gives you a distribution that it has the shape of a Gaussian. And the standard deviation of that Gaussian is 1. Now, what if you plot the correlation between the value of value of this function at time t and time t plus 1? So they're completely uncorrelated with each other. On average, if you take many different signals, many copies of this, and calculate the power spectrum and average them all altogether, it's going to be flat. But for any given piece of noisy signal, the power Spectrum is very noisy. We're going to spend a lot of time addressing how you solve that problem in a principled way. In the next lecture, we'll talk about how to make a good estimate of the spectrum of a signal by breaking it up into little pieces. a noisy signal that has a little bit of underlying sine wave in it, we talked about in class, if you take the autocorrelation of that function, you get a delta function and then some little wiggles. So there are ways of pulling periodic signals, periodic structure out of noisy signals. But it turns out that this method of spectral estimation did the most powerful way to do it. So using these methods, you can pull tiny signals out of noise at a very bad signal to noise ratio. So it's a very powerful method. of noise. In order to estimate what the spectrum of noise looks like, you have to take many examples of that and average them together. And when you do that, what you find is that the power spectrum of Gaussian noise is a constant. It's flat. The power spectrum, really, you should think about it properly as a power spectral density. So there is a certain amount of power at different frequencies in this signal. And forGaussian noise, that power spectraldensity is flat. It’s constant as a function of frequency. low pass, by convolving a signal with a kernel. The kernel for a high-pass filter is a delta function that reproduces the function. filtering in the frequency domain is multiplying the Fourier transform of a [AUDIO OUT] times the Fouriers transform of the kernel. Power spectrum of the filtered signal is just the power spectrum of your original signal times the power Spectrum of the kernels. So convolving our original blue signal with this green Gaussian kernel smooths 0 at high frequencies of blue signal and 1 at low frequencies. with each other? If we look at for the red signal, y of i and y ofi plus 1, what does that look like? They become correlated with each other, right? Because each value of the smooth signal is some sum over the blue points. So if you look at the autocorrelation of the original Gaussian noise, it has a delta function at zero. And the width to that autoc orrelation function tells you the time [AUDIO OUT] this signal was smoothed. Any signal that has discrete components and frequencies is periodic in time. Discretely sampled in time means that the Fourier transform is periodic. If the sampling rate is less than twice the bandwidth, what happens? That means delta t is too big. These copies of the spectrum are too close to [AUDIO OUT] and they overlap. That overlap is called aliasing-- a-l-i-a-s-I-n-g. You can perfectly reconstruct the signal, even though I didn't look there. Matlab has built into it the ability to do zero-padding right in the FFT function. You can also sample the signal in the time domain and then add a bunch of zeros to it before you Fourier transform. And that gives you finer samples in the frequency domain. And I'll show you in more detail how to do this after we talk about tapering. See something at the wrong frequency? That's an example of aliasing. OK? OK, let's actually just stop there. I feel like we covered a lot of stuff today.

ROUGE-1: 24.47, ROUGE-2: 23.26, ROUGE-L: 23.33
BERTScore: 69.71

==============================================
==================== [59/100] ====================
Summary:
 Physics 8.01 is MIT's introductory course in classical mechanics for first year undergraduates. The science of classical mechanics establishes an important principle of cause and effect, wherein the changes in a bodys motion arise from the application of physical forces. In many high school level physics courses, mechanics is taught as a set of formulas to memorize for a series of standard situations. Here, you will learn fundamental principles that you can use to apply to unfamiliar situations and analyze them rather than just situations that you studied previously. level problem sets. Developing a command of mechanics is a powerful tool for understanding the world around us. Welcome to 8.01, the latest version of the Java programming language. We hope to see you in the next edition of Java 8, which is scheduled to be released later this year. For more information, visit the official Java 8 website or follow us on Twitter at @joshjenson and @jenniferjenson. For the latest Java 8 news, visit our news page.

ROUGE-1: 52.80, ROUGE-2: 42.17, ROUGE-L: 44.80
BERTScore: 70.39

==============================================
==================== [60/100] ====================
Summary:
An independent set in a graph is a subset of vertices with no two adjacent. The proof of this theorem applies the probabilistic method. The expected size of this set I is equal to the sum over vertices V in G of the probability that this V lies in I. This is what happens on average, and therefore, there must be some ordering in the set I, says YUFEI ZHAO. It will be impossible to have two vertices in I that are adjacent to each other. Every n-vertex. graph G contains a clique on at least this many vertices. Turan's theorem gives us some bound on the maximum number of edges that a graph can have if it doesn't have a large clique. When n is not divisible by r, you need to do a similar construction, and this bound can be improved slightly, but not by too much. And this proof that we just saw is a beautiful illustration of probabilistic graph theory that allows us to prove this wonderful result.

ROUGE-1: 22.83, ROUGE-2: 20.90, ROUGE-L: 21.00
BERTScore: 63.72

==============================================
==================== [61/100] ====================
Summary:
Professor: Today I've got quite a lot of stuff to go through. So hopefully we'll get to the actual game playing, but if not, don't worry, you'll actually get to play the games. The games that we'll end up playing today are games from last year. A lot of them are actually the final semester-- the final project, which means they are not answering the same question that you're trying to answer with your very first project. But it should give you a sense of what the scope of this class actually is. Last week, a couple of you played the LEGO game, Block Party. you just wanted to make your car game, for instance? You can. That's no reason why you couldn't. What if you want to do a live action game where you actually move around with your body, that sort of thing? That's totally doable. I really, really hesitate not-- try not to put any live digital components in your game. By actually writing code, then the scope of this project just went through what I expected you to do. And the possibility of failure is-- go back please. a game? AUDIENCE: (COLLECTIVELY) The players. PROFESSOR: The players, right, not the designers, not you. But you when you actually play someone else's game. Then you're the most important person. If that person's experience is problematic or exciting or engaged or outright hostile to other players, or something like that, that's making it the way that they want to take it. And then you as the designer are are going to run through a number of different challenges trying to give them an experience that you're trying to create. A professor explains what it means to make a decision in a game that can be meaningful. He explains that if the outcome of the decision hasn't changed, it wouldn't be terribly meaningful. The professor also explains that a game's mechanic, the roll die, is a weird thing because it's something a player does but a player didn't decide how to do that. He says that in games like Yahtzee, you can still change the game state and the fact who wins or whatever happens. Candy Land was invented to keep kids from getting polio from each other, professor says. Professor: With polio, you can't do much of anything. There's a huge inversion from the get out and get some exercise. It's a serious game. It has health benefits. Let's play in the land of Candy. If you don't let the player know what changed the game state, is it that meaningful a decision anymore? You did something. Some numbers changed inside the system. But it's good to affect how things go out later. If you expect players to get very into your game, this is the stuff that you can leave with because they will understand it and figure it out on their own. In Dominion, oftentimes you can draw cards that don't help you just pointlessly causing a reshuffle. In Starcraft, you can't really see what your opponent is doing. You don't end up with a good mental model of how the game works. And so even though you make meaningful decisions, you don't know what the meaning is. When I do this one attack. And it's like, yes, because you saved the random number seed. So that is actually a problem because people have this concept about how the mechanics of what does 90% probability of success mean? So this brings me to the second reading, which is Don Norman's first chapter in the design of everyday things. The book used to be called The Psychology of Everyday Things, which had a nice little acronym him of POET. But then people had trouble finding the book because he was looking for design books. make a change and you iterate on it. In a board game, maybe in a card game,maybe in a visual game, that's telling me visibly, right now, that this might be the way I get to do that. It's depending on the design of the game. But visibility has something to do with the intent of the player of the user in Norman's case. But he's not talking about games. He's talking about the Psychology of Everyday Things. And what does visibility do in a syst-- in a design? It helps people understand the qualities of what they're doing. design of everything. And what the system can actually do, the actual operations of the system. So there are affordances and there are constraints. These are both words that are introduced in that reading. I think affordances is introduced in this reading. What's an example of an affordance? What about this thing tells you that you can sit on it? AUDIENCE: It seems sturdy, and it's got a place for your butt. PROFESSOR: OK, there's a little bit of cultural familiarity with other chairs that you've seen. handle on a door? PROFESSOR: What does a handle on adoor allow you to do? AUDIENCE: It's like-- it's a place for your hand. It's hand-shaped. If you hold your hand out in a natural way, you grab-- looking at the text. The design with the ground on the bottom is a bad idea because if something starts falling out that are too exposed to it, it will not be the ground one. So it would be better to flip it. British plugs, actually, have the pin usually on top and-- AUDIENCE: The British just have the eyes [INAUDIBLE] pin. PROFESSOR: When it's the other way around, it's actually a little more stable. You could stick your finger in a British pin, which means they have to design all kinds of protection mechanisms, plastic springs, and things like that. So they talk about materials like wood and glass, right? Glass is for looking through. Wood is for smashing. And wood is possibly for writing. about. It has this thing. Actually, what you do with this thing? AUDIENCE: Ring it. PROFESSOR: Slap? [BELL RING] Yeah, that's what it does. OK, all right, so now that you've seen what this thing does. What is one of the things that this-- not in the rule book completely. If you have this in your game, what is this thing good for? Audience: Getting attention, yeah. It's loud. You could use it to annoy people. comes with a bunch of cards. Ooh, whoa. What happened to these cards? Good Lord. OK, now the paint could rubbed off or something that. So don't worry too much about the text and the graphics. But just look at the card. What do cards allow you to do? What are the affordances of cards? Hm? You can hold a couple of them in your hand at once, OK. Because there's two sides. There's a side that you can put no useful information on, right? Carcassonne is a board game that involves moving pieces around a board. The pieces are called meeples and have a back that is not colored. The back of the board has a design on it that can be used to hide cards from other people. The board also has tiles that are identical with a back, which tells us about these tiles, something that we already know about cards. The fact that you've got a hidden back gives you quite a lot of different possibilities in the game. a whole bunch of ways that you can help people with these sorts of mappings. I guess Donald Norman would describe a lot of these as spatial, metaphors to use. Spatial mostly to describe things like driving in a car and you turn the wheel specifically toward the top of the wheel to the left. There are sort of bodily metaphors as well. Things that are high are either supposed to be good and happy, things that are low when you're feeling depressed. And you can't pick yourself off the ground, making you sad or bad. The tiles are designed in such a way that it makes it hard for you to hold on to do. You're not really supposed to have more than one tile at a time in Carcassonne. The cards are not very useful to you when you've got them face down. You are supposed to draw one, figure out where it goes, put it down. But if it's a large number, maybe you could remember what you had face down if it was a small number. of cards, like in Pit, you really want then held face up, facing you. When they're face down, they're not interesting to you. These are things-- this is another way that the visibility of the system can help you with those mappings. When it comes to Carcassonne, there is actually a deep, deep problem with this game despite how popular it is. It's a math intensive problem. It does largely map on to how many of the meeples that it's giving you. you have of your own color on large patches of things. That's worth something. And then that's the feedback that you get back from games when it's Pit-- the feedback you get when you hit the bell, right? There's a huge ding sound when you try to fit a piece that doesn't fit into something else. And there's a couple of board games where you have to put pieces together, and that gives you an idea of maybe those two pieces don't go together. your objectives from the player that knows the information. That's a big difference in Scotland. Donald Norman ends his very first chapter on why is it so hard to actually make something right? Anyone remember? Or anyone thinking of-- AUDIENCE: Doesn't the designer never really communicate one-to-one with the user? They're communicating through the object that either was designed or being used? PROFESSOR: That is definitely true. It is a second way to do things. It's a good game. order problem. You're designing something that then becomes this manifestation that somebody else uses. Market forces push you to add things, to do additive design in order to distinguish yourself from the competition. And that naturally leads to complexity in interface. So iteration is the reason why design starts off as being very clunky. But it can eventually become something that works well, communicates well, or something that people can learn and maybe even enjoy in the case of the games. And it's interesting because he described a phone that can only be used with two buttons, right? what he's talking about only, I doubt that actually imagined that this was something to be possible at the time when he wrote it. That was 1980. And cell phones obviously have gone through a lot of criticism. But it is it has been successful through a number of different reasons. But what I want you to think about now is actually the process of prototyping. Prototyping might be something we'll leave up to Wednesday. So that there'll be a little more time for you to work in your teams. to get all the Carcassonne bits back. So and then we'll pick this up in about five minutes. We'll be back with the rest of the story in a few minutes. Back to Mail Online home. back to the page you came from. Click here to read the full transcript of this article. Back To the pageyou came from, click here to Read the Full Transcript of this Article. Backto the pageYou came from the page You were from, Click Here to Read The Full Transcript.

ROUGE-1: 32.18, ROUGE-2: 30.02, ROUGE-L: 29.47
BERTScore: 73.24

==============================================
==================== [62/100] ====================
Summary:
The two-level problem is one that's exactly solved. It's one of our favorite exactly solved problems, although it doesn't seem to have any physical relevance. So we can take a two by two Hamiltonian and exactly diagonalize it. And that's done using a unitary-- or actually, in the case that we looked at, orthogonal-- transformation. And so it's a rotation in state space. And the rotation angle is an explicit function of the parameters in the Hamiltonian. by two and does transformations. And it keeps doing that until the off diagonal matrix element is small. So that means that it doesn't matter how big the problem is. You just have to have a computer that's patient or fast and it will crank out the results. Now you need to know how to use the t matrices, or the t dagger matrix. These things enable you to solve basically any problem in time independent quantum mechanics and a lot of problems in time dependent quantum mechanics. So there is another way, and that's going to solve an n-level problem. We're not allowed to determine the wave function by any experiment. But we are able to observe the energy levels and properties of the Hamiltonian. And from that, we can calculate everything, everything that we could possibly observe, including things we didn't observe. So it's really powerful. It's a way of taking the totality of observations that you're going to make and saying, yes, I have looked inside this molecule. And I've determined everything that I'm allowed to. determine. you the tools to be able to take any arbitrary spectrum and extract from it the crucial information. In the last lecture, we talked a little bit about matrix mechanics, and that involved linear algebra. And there is notation. And the notation is unfamiliar, and you have to learn how to use it. And so we have for example-- the analogy to the Schrodinger equation in matrix language-- where we have the Hamiltonian, an eigen vector, and this eigenvalue. And our friend is this unitary transformation where t dagger is equal to t inverse. the most common, doable problem, and it's also something that one does in experiments. You can set up a problem so that, with a short pulse, you prepare the system at t equals 0 in something that's not an eigenstate. With perturbation theory, it doesn't matter that the computer could solve for the t and t dagger matrices. But there's no insight. You just get the numbers. I like to say that spectroscopy is not about creating archival tables of observed transitions. It's about understanding how things work. bunch of problems-- I'll go over here-- that are related to the particle in a box. And so one of the things that you might do is round off the corners because physical systems don't have discontinuities. And we have to work out the matrix elements of the Hamiltonian that correspond to these extra things and then figure out what to do to get the eigenvalues and eigenfunctions. So the particle-in-a-box is a whole family of problems that you could solve using perturbation theory. n atoms, there's 3n degrees of freedom. There's 3 translations and 3 rotations. And so that leaves 3n minus 6. And all of that is vibrations. So we have many normal modes, and it's not too surprising that, if you stretch one normal mode, it'll affect the frequency of another. So perturbation theory is really valuable for polyatomic molecules. And that's the bulk of the examples that I worked in the non-lecture notes for this lecture. a mind numbing, formal derivation. So we start out with this rotary equation. And we say, well, let us expand the Hamiltonian. And let's put a little thing here. This is an exactly solved problem. And what you're going to do is now use the psi n 0 and en 0 to do everything else. And so that means we can deal with all of the interactions among the non-degenerate levels in one fell swoop. Now we can write this as a linear combination of the zero order wave functions. When we do that, we end up with this formula. energy levels that we're sampling in our experiment. So the molecule more or less tells you how to focus on the part that you care about and to get rid of the stuff that is of no trouble whatsoever. And it just contaminates the wave functions a little bit. And if you wanted to know what that contamination is, you could deal with it. So this is the tool that you can use to solve, basically, any problem involving molecules with a potential like a harmonic oscillator at the bottom. levels are that some of the energy levels for the two independent oscillators and the wave functions are the products. So these then turn out to be the zero order states that you use to evaluate all the integrals here. And we have these and A and A dagger operators, which are enabling us to-- we have the operator for coordinate x is proportional to A plus A dagger. So we like these things because we don't have to do any integrals. They're all done for you. Q2 cubed, and those terms usually are not important because they mostly are dealt with under here. Now there's an interesting other thing. The first-order correction to the energy is equal to a diagonal matrix element of the correction term to the Hamiltonian. All of the second-order terms involve squares of matrix elements. The second- order terms you don't know the signs. If there is a first- order correction, you get the sine. And sometimes you know you want to know: is the perturbation like that or the perturbing like that? Is there a barrier or an extra barrier? Non-degenerate perturbation theory works when the energy denominator is large compared to the coupling matrix element. And accidents occur when you have, say, a situation where omega 1 is approximately equal to 2 omega 2. And so you want to really know how to do these sorts of things. So you want the algebra that combines all this horrible stuff according to selection rule that leads to simplification of the formulas. And then once you've got everything sorted according toselection rule, then you can calculate what happens. isn't just blowing smoke. This happens an amazing number of times because stretches are higher frequency than bends. And it's very common for the bending modes to be roughly half or one third the frequency of a stretching mode. And so you get a resonance. So this is special because now it's violating the fundamental approximation of non-degenerate perturbation theory. But it's a two-level interaction. And these resonances have names. There is a Fermi and there is a Darling-Dennison. and extra lines. This is called a spectroscopic perturbation. And so you can learn about some of these coupling terms because, instead of hiding in the forest of these small corrections, you get a big effect. For big molecules, you have a resonance with a dense manifold of levels. All of these levels share, say, the character of the dark state. and so they can all interact with this guy. Now you can get the information you want from this because the width of this thing is related to the number of dark states and their average coupling matrix element. There is this dichotomy between small molecules where the vibrational density of states is always smaller until you get to really high energy. And bigger molecules-- now they're not very big for this sort of thing. And there is the question.about that later. So I think there's a pretty good place to stop because what I try to do is to show you, yes, it can be really complicated. But it's something that you can do, and you can project out the coupling constants that you want. names that any educated physical chemist will know to say, oh, that's the Bixon-Jortner. And they're still alive. They're still doing beautiful stuff. But anyway, that is all I want to say today. I will do details on one mode and Morse oscillator in other sorts of things next time. Back to Mail Online home. back to the page you came from. Back To the pageyou came from, Back to thepage you came From. Back into the page.

ROUGE-1: 40.54, ROUGE-2: 39.21, ROUGE-L: 37.49
BERTScore: 68.05

==============================================
==================== [63/100] ====================
Summary:
If certain amount of output can be produced by given amount of inputs, then the same combination of input puts can also be used to produce less amount ofOutput. The key word is feasibility technology represents the feasibilities that the combination of inputs and outputs, that can be achieved in this world even the current level of technology fine. The second is no free lunch. At least 1 input is required to produce some output, or more than you know more than 1 kind of output. Third is non reversibility, what it means is that a production process cannot be reversed. in the process and labor hours spent in the process back. So, in that sense all the production and processes are irreversible. If we have reversibility in the production process, then it would violate the laws of thermodynamics. To produce 1 kg of rice ok, you need either let us say 100 grams of fertilizer or 50 liters of water. To write a software code 1 software code of course, we will have to define what do we mean by 1Software code. It requires either 2 man hour, and this is also not a good example. 200 grams of fertilizer. 2 and 25 liters of water of course, we need land, but that is fixed those are fixed, we are not talking about it only these 2 are variables. So, what I am saying here let me write it in this term this is 100 grams and this is 50 liter and, here it is 200 grams and 25 liter. Student: Should not, has be 2 like 1kg of rice gives 1 and 2. No not give this is 1 kg of price is output. If it is technologically feasible to produce let us say y amount of output. And it is also technologically possible to produce y 1 amount ofOutput. If this is the combination it is feasible to combine them in this way and get some output. If we decrease all the inputs in the same proportion, then output will also decrease. This is example of convexity; let us talk about additivity first. What is additivity? Let us say that a production plan y means, that it gives certain combination of input and output that is feasible. 1 unit of input 2. So, 100 units we can produce by replicating this process 100 times. What we are using basically 150 units of both the input. And what is happening there, we can say 100 units of output we can obtain using t not 1 comma 2 plus 100 minus t not 2 comma 1 and this is where additivity kicks in, we are able to add these to up. Now we will use the divisibility to get the same result for each number. property, we will say that on average if we are using you know 100 times like this on average, we want to produce 1 kg and divisibility is allowed, then what will happened if we divide it by 100, what we get t not by 100. And this will give us 1 kg or 1 unit of output. So, basically t 1 comma 2 1 minus t 2 comma 1 will given us again 1 units of output and this exhibits convexity. The combination is all given here, y is taking care of not only inputs but also output.

ROUGE-1: 32.59, ROUGE-2: 30.92, ROUGE-L: 29.33
BERTScore: 68.12

==============================================
==================== [64/100] ====================
Summary:
The Great Wall began as multiple walls of rammed earth built by individual feudal states during the Chunqiu period to protect against nomadic raiders north of China. Under the Han Dynasty, the wall grew longer still, reaching 3700 miles, and spanning from Dunhuang to the Bohai Sea. After the Ming dynasty gained control in 1368, they began to refortify and further consolidate the wall using bricks and stones from local kilns. The wall was formidable but not invincible. Both Genghis and his son Khublai Khan managed to surmount the wall during the Mongol invasion of the 13th Century. main body and expanding this remarkable monument to human achievement. Main body is made up of three parts: the head, the torso, and the legs. The main body of the main body is the most important part of the body. The second part is the lower body, which includes the legs, the arms, the legs and the feet. The third part is made of the lower torso, which is the largest body of its kind. The last part is called the lower legs, which are made of marble and marble.

ROUGE-1: 36.14, ROUGE-2: 27.54, ROUGE-L: 26.49
BERTScore: 66.09

==============================================
==================== [65/100] ====================
Summary:
Peter Solovits: I got a call a year ago from the National Academy of Science, Engineering, and Medicine. He says the body is made up of old people with lots of gray hair who have done something important enough to get elected to these academies. SolovITS: They convened a meeting to talk about the set of topics that I've listed here. The issue of using litigation to target scientists who have opinions that you don't like, for example, is one of the topics. tenor of the times. So the group of us that talked about AI and decision making, I was a little bit surprised by the focus because Hank really is a law school professor at Stanford. Cherise Burdee is at something called the Pretrial Justice Institute, and her issue is a legal one which is that there are now a lot of companies that have software that predict, if you get bail while you're awaiting trial, are you likely to skip bail or not? And so that that was our panel, and we each gave a brief talk and then had a very interesting discussion. interesting to me. He said, no, he wouldn't want people like that, which kind of shocked me. And so we quizzed him a little bit on why, and he said, well, because he views the role of the judge not to be an expert but to be a judge. To be a balancer of arguments on both sides of an issue. And he was afraid that if he had a clerk who had a strong technical background, that person would have strong technical opinions which would bias his decision. Algorithmic technologies may minimize harms that are the products of human judgment. But the use of technology to determine whose liberty is deprived raises significant concerns about transparency and interpretability. The algorithms are developed privately by private companies which will not tell you what their algorithm is. And so you have no idea what's going on in that box other than by looking at its decisions. The state Supreme Court ruled against a particular person in Wisconsin, saying that knowledge of the algorithm's output was a sufficient level of transparency in order to not violate his rights. When I was an undergraduate at Caltech, the Caltech faculty decided that they wanted to include student members of all the faculty committees. In those days, Caltech only took about 220, 230 students a year. So one day, one of the professors said here's what we ought to do. We ought to take the 230 people that we've just offered admission to and we should reject them all and take the next 230 people, and then see whether the faculty notices.look like they're a better bet. Peter Zolovits: What do you mean by fairness? What characteristics would you like to have an algorithm have that judges you for some particular purpose? Irene: It's impossible to pin down sort of, at least might in my opinion, one specific definition, but for the pre-trial success rate for example, I think having the error rates be similar across populations, across the covariants you might care about, is a good start. And you'll see later Irene-- where's Irene? Right there. People who are similar should be treated similarly, says Peter Zolovits. But defining that function is a challenge, he says. He shows an example of a bias in the genetics used to determine whether somebody is at risk for cardiomyopathy, hypertrophic cardaomyopathy. "It's another of the classic sort of notions of fairness," he says, "that puts a lot of weight on the distance function" "You obviously don't want to use the sensitive characteristics, the forbidden characteristics in order to decide similarity" In the US, there were tests of this sort done, but the problem was that a lot of African and African-American populations turned out to have this genetic variant frequently without developing this terrible disease. And it was only after years when people noticed that these people who were supposed to die genetically weren't dying that they said, maybe we misunderstood something. And what they misunderstood was that the population that was used to develop the model was a European ancestry population and not an African ancestry population. So you go, well, we must have learned that lesson. descent do not improve the prediction of osteoporotic fracture and bone mineral density in Chinese populations. So it's the same story. Different disease, the consequence is probably less dire because being told that you're going to break your bones when you're old is not as bad as being told your heart's going to stop working. But there we have it. OK, so technically, where does bias come from? Well, I mentioned the standard sources, but here is an interesting analysis. This comes from Constantine Aliferis. the data, randomizing, essentially, the relationships in the data. And then you get a curve of performance of those models, and if yours lies outside the 95% confidence interval, then you have a P equal 0.05 result that this model is not random. So that's the typical way of going about this. Now, you might say, but isn't discrimination the very reason we do machine learning? Not Discrimination in the legal sense, but discrimination in the sense of separating different populations. Until 1967, it was illegal for an African-American and a white to marry each other in Virginia. Trevor Noah, if you know him from The Daily Show, wrote a book called Born a Crime. He had to pretend to be-- his mother was his caretaker rather than his mother in order to be able to go out in public. So here are some of the legally recognized protected classes, race, color, sex, religion, national origin, citizenship, age, pregnancy, familial status, disability, veteran status. is something not right, that there is some sort of discrimination. Now, the problem is, how do you defend yourself against, for example, a disparate impact argument? Well, you say, in order to be disparate impact that's illegal, it has to be unjustified or avoidable. So the question, of course, is can we change our hiring policies or whatever policies we're using to achieve the same goals, but with less of a disparity in the impact? So that's the challenge. and you can't square that circle easily. Well, there's a lot of discrimination that keeps persisting. There's plenty of evidence in the literature. And one of the problems is that, for example, take an issue like the disparity between different races or different ethnicities. It turns out that we don't have a nicely balanced set where the number of. people of European descent is equal to the number. of people of African-American, or Hispanic, or Asian, or whatever population you choose. descent. There are three criteria that appear in the literature. One of them is the notion of independence of the scoring function from sensitive attributes. Another notion is separation of score and the sensitive attribute given the outcome. And then sufficiency is the inverse of that. It says that given the scoring. function, the outcome is independent of the protected attribute. So that says, can we build a fair scoring function that separates the outcome from the protected. attribute? So here's some detail on those. And by the way, this relates to the 4/5 rule. because they have better outcomes. Or alternatively-- well, of course, it could be caused by malice also. There's also a technical problem, which is it's possible that the category, the group is a perfect predictor of the outcome. They can't be independent of each other. Now, how do you achieve independence? Well, there are a number of different techniques. One of them is-- there's this article by Zemel about learning fair representations, and what it says is you create a new world representation, Z. the protected attribute, but is as independent as possible. And usually, there are knobs in these learning algorithms, and depending on how you turn the knob, you can affect whether you're going to get a better classifier that's more discriminatory or a worse classifiers that's less discriminatory. So you can do that in pre-processing. You can do some kind of incorporating in the loss function a dependence notion or an independence notion. And again, there's a knob where you can say, how much do I want to emphasize misclassifications for the protected attribute? does in other populations, and the FDA has actually approved the marketing of that drug to those subpopulations. And if you think about the personalized medicine idea, which we've talked about earlier. The populations that we're interested in becomes smaller and smaller until it may just be you. And so there might be a drug that works for you and not for anybody else in the class. But it's exactly the right drug for you, and we may get to the point where that will happen and where we can build such drugs. to reduce the errors in all groups. So that issue about randomly choosing members of the minority group doesn't work here because that would suppress the ROC curve to the point where there would be no feasible region that you would like. So for example, if it's a coin flip, then you'd have the diagonal line and the only feasible region would be below that diagonal, no matter how good the predictor was for the other class. And then the final criterion is sufficiency, which flips R and Y. So it says that the regressor or the predictive variable can depend on the protected class, but the protectedclass is separated from the outcome. degree of calibration will give you a good approximation to this notion of sufficiency. These guys in the tutorial also point out that some data sets actually lead to good calibration without even trying very hard. Now, there's a terrible piece of news, which is that you can prove, as they do in this tutorial, that it's not possible to jointly achieve any pair of these conditions. So you have three reasonable technical notions of what fairness means, and they're incompatible with each other except in some trivial cases. A paper based on Irene's work says that gender influences whether you're a programmer or not. It also says that visiting Pinterest is slightly more common among women than men. The paper says that despite the fact that you have these two scenarios, it could well turn out that the numerical data, the statistics from which you estimate these models are absolutely identical. And so from a purely observational viewpoint, you can't tell which of these styles of model is correct or which version of fairness your data can support. work. We were really interested in socioeconomic status, but we didn't have that in the database, but the type of insurance you have correlates pretty well with whether you're rich or poor. So we wanted to see not the coded data, but whether the things that nurses and doctors said about you as you were in the hospital were predictive of readmission, of 30-day readmission. So yeah, this is what I just said. So the question we were asking is, is there bias based on race, gender, and insurance type? We used LDA, standard topic modeling framework. And the topics, as usual, include some garbage, but also include a lot of recognizably useful topics. And so what we found is that, for example, white patients have more topics that are enriched for anxiety and chronic pain, whereas black, Hispanic, and Asian patients had higher topic enrichment for psychosis. And we were speculating on how this relates to sort of known data about underdiagnosis of COPD in women. So again, stereotypes of what's most common in these different groups. The error rates on a zero-one loss metric are much lower for men than they are for women. We have much tighter ability to predict outcomes for private insurance patients than for public insurance patients with a huge gap in the confidence intervals. This indicates that there is, in fact, a racial bias in the data that we have and in the models that we're building. The last thing I want to talk about is some work of Willie's, so I'm taking the risk of speaking before the people who actually did the work here. but that's the case. The eICU data set we've mentioned, it's a larger, but less detailed data set, also of intensive care patients. And there, we see, again, a separation of mechanical ventilation duration roughly comparable to what we saw in the MIMIC data set. So these are consistent with each other. On the other hand, if you look at the use of vasopressors, blacks versus whites, at the P equal 0.12 level, you say, well, there's a little bit of evidence, but not strong enough to reach any conclusions. The author says black patients are more likely to be distrustful of the medical system than white patients. He says it's not race, but something that correlates with race because blacks are more distrustful. The author says mistrust is not just a proxy for severity, but a reflection of the fact that they're sicker. The book is being presented at conferences across the country. It's a very rich area for both technical and technical work for trying to understand what the problem is, he says. fairness popping up at different universities. University of Pennsylvania has the science of Data ethics, and I've mentioned already this fairness in machine learning class at Berkeley. This is, in fact, one of the topics we've talked about. I'm on a committee that is planning the activities of the new Schwarzman College of Computing. The college obviously hasn't started yet, so we don't have anything other than this lecture and a few other things like that in the works, but the plan is there to expand more in this area.

ROUGE-1: 38.51, ROUGE-2: 36.49, ROUGE-L: 36.75
BERTScore: 68.81

==============================================
==================== [66/100] ====================
Summary:
MIT OpenCourseWare offers high quality educational resources for free. To make a donation, or to view additional materials from hundreds of MIT courses, visit MIT OpenCourse Ware at ocw.mit.edu. The next time I'll be lecturing to you will be a week from Friday. The homework following the quiz will be posted and so you guys can get started on that. That homework involves a COMSOL problem which might take some extra time, since it's the first time you're doing it. So I would suggest you at least give it a try early. a diffusion term, a convection term, and a reaction term. This is part of the Navier-Stokes equations for the whole reactive flow system. And what's special about the partial differential equations is that, in this case, this partial derivative is respect to time, holding all the spatial coordinates fixed. Now, you can change coordinates however you want. Just really watch out that you carefully follow the rules about what you hold fixed, when you change things. Otherwise you can cause all kinds of craziness. methods. So all those methods basically look the same. You just get equations that have more unknowns in them. In practice, there's a big problem if the number of unknowns gets to be so large. Once you get up to a million unknowns, we're starting to talk serious math, now. So you have all those kinds of problems that we had in the ODE case, and it's just sort of amplified by the fact that we have to worry about numerical stability. A very small spark can make a really big difference in a system like that. If you get numerical noise at some point, that might be enough to kick it over from, say, not igniting at all to having a detonation, which is a completely different solution. A small change in one cell, for example, could be areally big difference. So this is not just in combustion problems you have these kinds of instabilities. All kinds of problems have this kind of thing. In a hyperbolic system, information-- there's regions of the domain that you can make a change here and it doesn't make any effect on some domain part over there. In an elliptic problem, you introduce some numerical noise and as you solve it, the numerical noise kind of goes away. And then another kind of problem that we get into a lot is like this one. This is called parabolic. And typically in the cases that are parabolic, we have time as part of the problem. a flow in a pipe. And you have a nice, fast velocity flow down the pipe. What's happening downwind is very much affected by what happened upwind, but not vice versa. So it has a directionality, even though I might do it as a steady state problem and not have time in it at all. If you slow the velocity down, then the diffusion will start to fight the velocity. And so then, you'll have the same kind of issues. In fact, you may even try to solve it by just starting it at the upstream end and competing stuff. If you look in the textbook at figures 6.7, 6.8, it's kind of a very famous problem. If you make the local Peclet number too large, actually anywhere bigger than 2, and you try to solve this equation, what you get is oscillations, unphysical oscillations. So if you take your delta z too big, then it's a terrible approximation to do the solution. And if you look at the analytical solution, you can see that what we're doing is taking a differential and turning it into a finite solution. before. There's a similar thing and it might be relevant to the COMSOL problem that Kristyn showed you on Monday, is you can-- In that case, there was a region of the boundary here that had some concentration c naught, and then there was, over here, the concentration was zero. Here, it's some number, 17, whatever number it was. And if you think of how to model what's going on, one way to look at it is, I have a diffusive flux coming in from the drug patch, diffusing the drug into the flow. with this size right here. Is that right? So you could compute what the steady state concentration would be if it's just coming in and flowing out. This is the finite volume view of this kind of problem. And so you can write the equations that way, instead. And what you really care about are what the velocity flows and the diffusive fluxes are on these boundaries around the point. So you don't try to compute things right at the point, you compute around it. or every state variable at every state point at all times. And the problem is, my computer can't handle infinite number of unknowns. In fact, it's even going to have trouble with a 100 millionunknowns. And so, I have to figure out what to do. So how would I solve this normally, is I would take the Jacobian of this and I would do, I'd take an initial guess and I'd update it and have theJacobian times my change from the initial guess to the equal negative. 16th numbers in this matrix are zero. So we don't have to represent them. But there still might be quite a few. Probably the diagonal events are non-zero, so that's like 10 to the 8th of them right there. So you're not going to just use Gaussian elimination. So what do you have to do? You want to find a method. They're called direct methods to solve this kind of problem. And the direct methods are ones that you don't actually store the gigantic matrix. compute delta y. Now, this whole approach is sort of based on Newton-Raphson. We know that doesn't have the greatest radius of convergence. So how are we going to get a good initial guess if you have a 100 million unknowns? So what methods do we know? So one idea is you could do things like homotopy, stuff like that. If you could somehow take your PDE system, get rid of the nonlinear terms. You're really good at solving linear systems of PDEs. Maybe you could coax it over. for the derivatives. Or I did colocation or something to convert this into algebraic equations. And now, I'm going to time march this and I just-- This is very long. It's a 100 million. So how are we going totime march that? Well there's kind of two schools of thought for this. One school of thought is, if I can use an explicit method, an explicit ODE solver, those are pretty nice. Because all I have to do is evaluate f and then I can march along and I never have to save any matrices. too big. The biggest thing you have to store are the vectors. Now, this is really good if an explicit solver can solve it. And also, if you want a time accurate solution. But suppose we're trying to really solve the steady state problem over here. Then we really don't care about the time accuracy. In the end, we're only going to report our steady state solution. That's all we cared about. So in those cases, you might instead want to use-- So this is time accurate. set up is right, so you can use it. But if you can, it can be pretty good. All right. See you on Friday. Back to Mail Online home. back to the page you came from. Click here for the latest from CNN.com. Back To the pageYou came from the page You came from: Back to the Page You Came From: Back To The Page You Were Originally From: The page you were originally from: The Page you were initially from: the Page you started from.

ROUGE-1: 27.35, ROUGE-2: 25.92, ROUGE-L: 25.88
BERTScore: 68.79

==============================================
==================== [67/100] ====================
Summary:
elizabeth the first is considered one of the greatest rulers in england's history if not the greatest for women. She never married can you blame her i mean look at what she grew up with around her dad and all of those moms and stepmoms she never had any kids which that causes a problem come her end time okay where we have i don't have an heir so she eventually does set on a cousin up in scotland which is the country on the same island of england. She names him as the heir so when elizabeth died king james of scotlands comes down and he is now the king of englands and the king  of scotland it's like he united two kingdoms. know how to read latin or french you were stuck by solely getting your material this is what the bible says solely from your clergy solely from the church so you were kind of a mindless robot with regards to comprehending the the word of god or whatever the bible said on your own. Now that something is in english and if you can read english you are able to read it over and over get your own theories your own arguments and make your own decisions. So you were less mindless page 414 is a really nice page that if you need to come back to review the significance of this or you can't remember um it's pretty good you can see down at the bottom. the da vinci code is not the biggest circulated book in the world. i'm like i i can't speak to don quixote so um that's something i might just have to look up because that's interesting to me. oh no you know don Quixote or something i'm just like i don't know what that is. that's an interesting thing to think about. i may have to do some research on that. i might have to find out more about it.

ROUGE-1: 66.40, ROUGE-2: 59.98, ROUGE-L: 59.07
BERTScore: 75.63

==============================================
==================== [68/100] ====================
Summary:
The average velocity depends on the time interval t to t plus delta t while the person has displaced a certain amount of vector delta r. So, as a vector, we have delta x over delta t i hat. This component here is what we call the component of the average velocity. And, again as before, this component can be positive, zero, or negative depending on the sine of delta x. And now what we want to do is consider what happens in the limit as delta t becomes smaller and smaller. And that will enable us to introduce our concept of instantaneous velocity.

ROUGE-1: 55.17, ROUGE-2: 53.33, ROUGE-L: 55.17
BERTScore: 82.08

==============================================
==================== [69/100] ====================
Summary:
Last week we looked at the we first saw our first magic formula which you've probably forgotten because it was a wonderful weekend. The Machinery I provided you is powerful enough to solve pretty much any kinematics problem because it's based purely on Geometry angular velocity is a kind of a madeup concept that just simplifies the math. The rotating Theta Dot multiplied by you know the Le lever arm right gives you a velocity but you can see how it kind of slaughtered into place so we introduced the angle of velocity like this.  angular velocity is the angular of velocity frame B with respect to frame a right okay so we saw that for the first time now a little bit on the low of angular velocity that you've seen in previous classes but we we Plunge in anymore um the first thing is can of Point have an angular velocity now can a point have a velocity yes can a frame have a Velocity kind of no because if a frame's rotating some point different points will have different velocities so yeah a frame can have aVelocity but doesn't really mean much. Sam: If I rotate about one axis a certain amount 30° and then rotate about another axis 30° right if I reverse the sequence they don't add up but if I rotate. about oneaxis like a tenth of a degree and then rotated about another. axis a ten of adegree and then I reverse. the sequence actually come pretty close all right so it turns out that infini dismal angles do add up so while in general rotation about different angles right it doesn't different axes doesn't adding up infini. dismal rotations do. the final you know in terms of that initial frame get it and W so we introduced the the velocity and then the next thing we did last last in the last class was I revealed to you this magic formula I said if you have a vector let's call it n and you want to take its derivative with respect to the frame a now if you the way we did it was we would rewrite the basis vectors of frame a right interms of A1 A2 a lot of math lot of derivatives that's how we didIt uh but and actually it was Andrew asked he said isn't there a nice way to kind of simplify the derivatives here it is. Let's assume I have a vector let's call it n and that's that it's given to us for some reason naturally. Let's assume n is equal to sum U1 which is not a constant time B1 + U2 * B2 now what I'm trying to prove is this formula how would I go about doing it someone wake up everyone's asleep wake up wake up yeah let's Express exactly we're going to express B. We're looking for a d by DT of N and look n is expressed in terms of B1 and B2 that's really not nice. here so will you please expand it for me what does that come out to be this is the left hand side of that formula right and we'll do the right hand side separately and show that the two are the same I'm trying to prove it to you right so I'm not you know I'm doing just very boring stuff but we need to do it just establish it. U1 do cosine Theta right minus U Theta do sin Theta and Below what do I get mhm Theta one here this should be a Theta dot right yeah okay so these are the terms. Let's just imagine I'm doing it right I'm standing at this and spinning a basketball on my finger the room the world the Earth is a frame the basketball is another frame the kind of the U you know you guys need to be like Neo in Matrix have you seen that movie right it's very very important if you understand Matrix algebra you need to see Matrix all right so go see Matrix and you know how Neil kind of you know he when he's totally you know after Lawrence fishburns you know kind of trained him Etc he goes down and he can see he stops seeing buildings and he sees these characters you know flowing instead of the building right he sees right through all the video game right so similarly so what others see is a is a basketball spinning what you see once you're done with the scores is you will see an embedded V embedded basis frame. can are you following me here can you see it right anyone not see it I'll it's basically all I'm saying is that here's me this is my finger right here's the basketball okay my hand here're the basketball it's spinning what you guys should see is these embedded vectors basis vectors sticking out so you'll only see kind of the surface you know sticking out right and then you see these two rotating like that right but that guy stationary. So in your mind there's an angle of velocity that that basketball has with respect to the ground got it now if I was spinning it this way which I couldn't. yeah okay using um frames uh using angle of velocity so the problem was we have the ground, the spider and the Frisbee. We characterize this situation with three parameters okay and the parameters are the positions of the center of the frisbee and the distance of the spider. We're using three parameters but in the end the SP spider is only going to have two degrees of freedom right but that's okay because when you get the velocity of theSpider it'll have only two components when get the acceleration only have two components so it'll work itself out. can kind of do it that way but the simple way to do it is to simp well you know it doesn't matter we can just write this as B take the whole term d by DT of lb1 plus what quick hm I'm hearing a hesitant murmur not the yeah I was mumbling something a moment ago I'll tell you what I mean what I was trying to say uh cross lb1 what is a Omega B so we can rewrite a Omega b as Theta dot B3 and so this whole thing comes down to I'm going to write the answer here to save a Blackboard. We're going to derive a very general formula for something moving on something that's moving get it no seriously it's going to be a general formula. If one of you invents a new term we'll name it after you okay and so far I've used these words coris and all that I kind of called them out without telling you a lot more about them right so what we'regoing to do now is take a break in the following way I'm going to show you some videos showing you the coris force when I'm when you're done with these videos um you will a totally understand what the cholis not effect sorry not force cholis acceleration is. The Coriolis effect is a phenomenon that occurs when an object is deflected by the Earth's rotation. This video shows what happens when four kids sit on a merry go round and then someone starts spinning the Mero around. The path of the ball appears straight to someone sitting on it but to an observer on the rotating Earth the path of an object appears to curve to the left. This exemplifies the Corola Force. The videos are the work of two dudes who went to a high school and they put a bilard a pool table on top of the mound and they did some experiments. totally into it so they don't stop but the real the funny thing is that the kids actually can't see all this because they're below the level of the pool table so they're like jumping off you know okay so let me explain what's going on here so it turns out and then we're going to do the super magic formula in about a minute right. With respect to someone standing in space and not rotating with the planet earth right so if you're standing above the North Pole and you're looking down at the Earth rotating it's like a mer around. Water rotates in One Direction or not in the northern hemisphere another Direction on the southern hemisphere is that right is that valid yes or no why not yeah it's turns out that in that scale in fact there is a Coriolis effect but the slight deviation angle is so slight it's like a fractions of a degree per meter at that scale that it doesn't build up enough to really impact the direction in which the vortex takes place. The vortex is created far more by asymmetries in the sink or by rotation that was created earlier that can of remained right. Theta Dot and an l dot term get it then there is a deflection in the B2 Direction in the way we've written it now I'll give you the general formula and you know B2 what it is ETC it become more clear any questions about coris by the way in the last class I told you this anecdote I said that artilleries artillery guns right in the 1700s 1800s um they would have to compensate for directionality because of the coris effect it turns out that that is true. What is the force that the astronaut feels you know or what force must the astronaut exert in order to move in a straight line with respect to a spaceship or a space shuttle or something? That's the question we seek to answer. What we're going to do is basically generalize the Frisbee problem. We will Define as usual and you'll find this all repetitive because really it's all the same stuff over and over again just getting more and more compacted with more insights kind of teased out of it. around Etc they use this method they use the symbology a b and all that right very few undergraduate courses on the planet uh learn teach the mechanisms that you've learned and understood right this is it that's all there is to on the kinematic side now Kan also has a way of doing Dynamics which I won't tell you I'll do the Newtonian approach uh but you certainly have it okay so the problem we seek to solve now is a your frame a a I'm actually trying to derive a general formula that captures what we did for the frisbe you have a frame B. Andrew: I'm going to write out the super magic formula actually this is it okay the super Magic formula is this and I'll name the terms and we'll finish it next time a acceleration of Q is equal to the acceleration of Point P which is the fix point plus b a Q Plus. A is kind of my guardian angel if I make typos or he catches me a just a PhD student is getting his PhD in U in wiress sensing oh oh sorry sorry sorry Simone I'll be out in a minute one more excuse huh just one minute sorry thanks Andrew next time set your watch 3 minutes fast like mine. yeah sorry and plus this thing okay we'll end now because Professor Socrates needs to take over this is it this is correct acceleration of P acceleration of Q with respect to B Oiler coris centripedal and you know why it doesn't go to zero because I'm not taking a Omega B cross a omeg B. I'm taking a omega B cross this whole term which is at 90° to a OmegaB got it got it? Yeah sorry and Plus this thingOkay we'llend now because professor Socrates need to takeover.

ROUGE-1: 34.24, ROUGE-2: 32.97, ROUGE-L: 33.01
BERTScore: 70.59

==============================================
==================== [70/100] ====================
Summary:
Al shalot is the CEO of GT Sports. He is a member of the Olympic commission the international Olympic Comm. He has been advising in the background for a while now. He says it makes absolutely sense to pay respect and celebrate gaming by having a dedicated event. It's part of the agenda strategic agenda 2020 plus 5 to connect with the audience the youth audience where they are and to leverage gaming and Esports to do that so it was strategically cited and we're just part of a discussion about how to implement it. The Asian Games officially added League of Legend and other games on their list. The local authorities in in Saudi are very passionate and very supportive for eorts. PC always brought the idea to bring a property and events to a country to create some bridges. There's been a lot of criticism about Saudi Arabia as a host. Some LGBT players uh women saying that they don't feel like they'd be welcome. Are you expecting to have those kind of conversations with some of the national Olympic committees and at the very least is that something that came up? to use this platform to to be open to anyone I think that's the way they describe it um so I look forward to it uh what I would say is like as a team as a as as players as athletes we want them to to play in the the best tournaments the best teams and therefore want everybody to participate so you mentioned the Esports World Cup there uh it feels like we've got lots of big new announcements lots of tournaments are we in danger of having too many things going on I don't think so. The first edition of the Olympic Games will be held in LA in 2028. There will be a dedicated tournament for women and a full mix tournament for men. The Olympic Games it took them 120 years to get to that point Paris Olympics this summer will be 50/50 50% of the athletes will be women 50% will be male. There's a commitment to a for full par and that's super exciting. I'm very positive and affirmative that this will make a huge step forward and help the whole industry to to get better. "Everybody wants to see what a dennish team will do against Corin I I would love to see it too you know so um I've seen a lot of players making the the guess or what the team could look like so his aspiration to be part of is is clearly part of the discussion," he says. "You follow a bit like the conversation about but uh rocket league and OverWatch back in the days at the at the World Cup for for their game and and and League of Legend also often have this conversation about creating the Euro Cup or World Cup"

ROUGE-1: 22.98, ROUGE-2: 21.79, ROUGE-L: 20.76
BERTScore: 62.62

==============================================
==================== [71/100] ====================
Summary:
"The door is unlockable" means it can be unlocked. But the door is broken, "It's unlockable," means it cannot be locked. There's a suffix that changes verbs into adjectives. There are two "un-"s. "Un-" means something like undo the effects of, or change something so that it is no longer in the state that it would have been if the verb had applied to it, or something like that."I have this desire to put an extra oomph on "un-," if I mean the thing on the right. "untie" means "take something and do things to it such that it is no longer in the state that it was in after you have tied it" "Un-" is a reversitive meaning. "Un-" combines with adjectives and makes adjectives that mean more or less "not (adjective) " Unfamiliar, "unfortunate," "fortunate" are all "un-"s that mean not the adjective, whatever it is. "Undo" is hard to do, because it's hard to combine with. "Unlockable" is ambiguous because there are two "un-"s. "Un-" can combine with either a verb or an adjective. " unlockable" consists of three morphemes, a prefix, a root, and a suffix. The order in which these words are put together is also important to think of as being useful to think about in a bunch of sentences or more or less "unlockable," as we're going to see in the next few episodes of "This is Life With Steve" Theory of syntax divides sentences into three kinds. There are sentences that you've heard a zillion times before, like "We're going to class" And on the other hand, sentences you have possibly never heard anyone say, but that are fine. "Stop entertaining that hypothesis. Make it go home. It's not a good hypothesis. It won't do you any good," Richards says. "We have this distinction between grammatical sentences that are ungrammatical that isn't just a list of all the sentences that aren't grammatical" Sally Kohn: We can distinguish grammaticality, even in sentences that don't mean anything. She says we can have English sentences that consist of two adjectives modifying a noun and then there's a verb, and then an adverb. Kohn says we have this intuition that there are sentences that are OK and sentences that're bad. She asks: Do people have this feeling that they're meaningless, and second, that the first one is OK and the second one is bad? NORVIN RICHARDS: English speakers end sentences with prepositions every day. "Who are you talking to" where, without the "to," it doesn't have to be about redundancy, he says. "To whom" is part of this phrase that's at the beginning of the sentence, he adds. "Why does "to" have to come along, according to Mr. Richards? I don't know about you guys," he says to the audience, "but it's not my go-to way to say this" your English teacher? Yes? AUDIENCE: Is this another example because that's how it's done in Latin? NORVIN RICHARDS: Yes. Your English teacher told you to do that because Latin, actually, among many other languages, doesn't allow you to say this. You have to do this. English is quite rare in being able to doing this. Most of the languages of the world can't. Some time in the 15th, 16th century, a number of grammarians decided that English would be way cooler if it were more like Latin, and so they began declaring that it was. grammar. So prescriptive grammar is the study of rules that your teachers might have taught you in school about how to speak. So we're not going to try to improve your writing in this class, except insofar as the writing advisors can do that. So this is going to be a study of descriptive grammar and notPrescriptive. Yeah? AUDIENCE: So for that second sentence on the board, "What are you talking about?" NORVIN RICHARDS: Oh, this one? Yeah? In many languages, most languages, including Latin, you have to say, "About what are you talking?" In English, there's this distinction between the examples where leaving a preposition behind is what you prefer. "What did you leave despite?" and "Despite what did you left?" Is either of those acceptable at all? "AboutWhatAreYouTalking?" sounds fancy and snobbish, but still right. "To whom are you Talking?" sounds Fancy and Snobbish. a fruitful area of research here. English is happy to leave prepositions at the ends of sentences. But in which cases is it happy to do the Latin thing? And in some cases, it's happier than others. Another distinction to make is between meaningless on the one hand and ungrammatical on the other. That sentence can be both meaningless and un grammatical, but it can also be meaningless and grammatical. We've drawn a distinction between prescriptive and descriptive statements. We're just going to study what English speakers do. "This is the cough, ugh, puh." That's something I said. And I'm a native speaker of English. So there are two kinds of things we could say. One would be to say, we're developing a theory of all of the kinds of sentences that Native English speakers can say. And then there are going to be other things. Flies, sudden heart attacks, and then other kinds of thing that are maybe less clear to think what to say about them. people would be like if there were no distractions and no flies, and no sudden homicides, no falling asleep in the middle of your sentences, all of that stuff. So the distinction here is competence versus performance. We're imagining a speaker who's kind of like a frictionless plane, that there are various kinds of complications, and there's no air resistance or whatever else. Does that makes sense? That's how we're going to do syntax. Similarly, "It's raining" is a possible sentence of English. begin doing some syntax? Here's the sentence, "I will find the red book." grammatical sentence. It's clear what it means, although we've just said it doesn't matter whether it means anything. We're going to look for these units. And what we'll find is that there are various phenomena that care about whether something is a unit in that tree, a single blob of structure. "The red book" is treated as a single object that syntax gets to refer to. So "find the red," for example, is not a constituent. or we're getting to modify nouns that you can't hear, however we want to talk about that. So all this slide is meant to convince you of is that "the red book" and "find the red" don't have the same status. "The red book," we want it to be a substring that has certain privileges, can be used for these various types of phenomena. So it isn't just these are phenomena that pick out three-words substrings. It's these are phenomenon that pick out certain substrings and not others of the sentence. When we put together "red" and "book," what we get has properties that are determined by the fact that it contains a noun. If there were no noun, it wouldn't have those properties. Similarly, with "the red book," the things that can go in that slot, the things That can go after "find," we just saw in the last slide, can be larger or smaller. What they all contain is a noun, so we're going to name that thing after those kinds of units. We'll give it the label "noun" just contain a preposition, like, "I will look up," where, again, you can say, "Up?" (Why will you look up?) "I said I would look up, and up I will look"-- Maybe. "The day after tomorrow" sure looks like a noun phrase. It's got a noun in it, "day," and then "the" before that, but we know that can go at the beginnings of noun phrases. So there are probably-- so the word "adverb" can be used to cover a bunch of things, including things that we don't have any other word for. "I will wake up" is one of those prepositions that combine in an interesting way with verbs in English and a lot of other Germanic languages. "Up the cats" is not a prepositional phrase. We need different structures for these verb phrases, and we will develop them. "I work in the domain in which up and down can be the same thing. Yeah. If I were a physicist, then if NASA were to hire me, the spacecraft would have all kinds of problems" "I will walk the student up to her room" is different from all of these, actually, kind of interestingly. "Up" is modifying "to her room." We want there to be a constituent "up to herRoom" "Up to her Room?" suggests that that's a constituent. "Waking up or walking up" is not an adjective, it's describing a verb. It's like the opposite of "walking up" or "walking down" "I will take you home" is like "taking you to your room" "Waking the cats up" is a little bit like painting the cats red. We want "up" to not be a preposition that's combining with the cats. "I will walk her up" as in "I walked up the stairs" seems to be three different things that we need three different structures for. "Up" is modifying "to her room," but I don't think [? so. ?] NORVIN RICHARDS: It doesn't have to, does it? No. For "walk up them" we want "up" and "them" to combine to be a propositional phrase. But for these other two, we want something else. We're going to want to circle around and try to find out what that other thing is. Does anybody else-- yeah? AUDIENCE: I was going to suggest something else [INAUDIBLE] Even though you can't say, walk the-- "walk the stairs up to your room." NORVIN RICHARDS: "I walk--" the book in the garage," what we want to do is construct a structure for this that's sensitive to all of the tests for structure that we've been developing. It's going to involve putting things together via pairwise merge and creating labels for the things that we create. So when we merged "the book" or "the garage," we're going to create something we'reGoing to give the label "noun" to. Or when we merge "in" together with "the Garage," we'll create what I've been calling a prepositional phrase. Similarly, we'll combine "find the book" with "in the garage" and we'll end up with a structure. construct syntactic trees for strings of words. It's often the case that we get ambiguities like the "unlockable" ambiguity. There is more than one way to combine things. And what we'll do is develop tests that allow us to see which way we've combined words in different ways. And we'll find cases where, depending on in what order you combine things, you get different meanings, and our tests will combine with that. All right. We will do this again on Tuesday.

ROUGE-1: 24.73, ROUGE-2: 23.08, ROUGE-L: 22.74
BERTScore: 68.83

==============================================
==================== [72/100] ====================
Summary:
William Wordsworth along with the next person we talked about Samuel Samuel Taylor Coleridge probably the two of the most influential of this particular era. William Blake from earlier is very important as well. These names that I've been telling you are people that show up on Jeopardy from time to time especially if there's a Romantic era or British literature type thing it's usually a good stab in the dark if you don't know to kind of throw out one of these names very interesting individual. such you know it's about that emotional feelings and that powerful feelings and he was one that really was pushing pushing and pushing for those to be in his writings and that why he stands out that's why you've such a visionary of the time and the building background on the next page gives a little bit more insight into that same premise so that's something to remember about this era and about Wordsworth to help out growing up his past to talk about his past. He talks about having two tragedies the loss of his parents which would be a tragedy for anybody but also the division of his family after the death of hisParents. You know throughout his upbringing even though he was away from his sister at the school he developed this passion for poetry and this observation of nature and such and we will see that played out in the piece today. He falls in love with a woman but he becomes broken so he has to take off and go back home and so for years it talks about there that he's feeling a sense of abandonment and loss for leaving the woman that he loved back in France that I can't stay and you keep the cause going any longer. The Rime of the Ancient Mariner for Coleridge was as we'll read here and for the intro for that you know Wordsworth worked with him on that it was going to be a dual credit both of them were going to get credit for it. Wordsworth had to drop out of that but some of the most famous elements of the piece still are in there and they're still a focal point so we still see his kind of finger prints on that masterpiece áwillá will take care of in the next couple days. different than the typical every line it's kind of its own little thing and so when it's read in a different way it's it might see a little Jill at times button jamming is something very easy to visually identify a once you understand oh that's in JAMA it's a piece of cake okay. It might give you a little bit of reservation here and there but it shouldn't okay what I want to talk about first is a 786 lines composed a few miles above Tintern Abbey on Abbey you know a religious building. This is probably one of the most difficult readings that we have throughout the semester I don't think it's impossible but you're going to have to you're gonna have to work a little bit ok so just kind of grab onto those moments those moments of nature and such but also as this is kind of like a monologue where he's going on all of a sudden we find out at the end that he's actually talking to somebody and based on his history can we guess who that might be and why this moment is interesting? he talks a lot I mean just describing you know leaning against the tree and looking out amongst the field on looking at the orchard with the you know it talks about the all of the which at this season with the unripe fruits are clad in one green hue. He almost has kind of a teleportation back to some degree of when he was a child okay he goes into great details about when I found through around the river like a like a row like a deer and I would play and when I was little I was having a great time. He says that the memory of the woods and colleges offered tranquil restoration to his mind and even affected him when he was not aware of the memory. While he was away thinking back to this it helped calm him and help restore him if he mentioned that when I was off in a city whether I was alone or far away or with some people you know I would think back to these place. It brought me a lot of peace it brought meA lot of comfort ok do you remember a certain trip that you took at some point in your life maybe five to ten years ago and you distinctly remember a visual image have you ever gone back to that same place and like oh yeah this is the place one of my earliest memories was I was like two or three and I went to Disneyland on California. in California and watch the train go across and was like this is the same spot I stood when I was three you know and so it's kind of a nice little Wow think about how I've changed in my life. You can think back to this this for me I didn't look back to Disneyland and think hey that was awesome but you look back at certain things in life and that brings you some calm those are you looking forward to spring break or summer are you going to some place you've gone to before. steeped in nature and such okay he mentions the river you know meandering through the the trees and through the fourth that's why looking at that picture before we started you know that river is the same river that he was looking upon and quoting and talking about here. We see these elements of nature we see remember we talked about in the introduction the child and the common man the innocence of children and such this isn't a story about or poem about children but yet through his childhood memories we are able to make that connection to those passion emotion feelings about nature. a little maybe some of the footnotes you know that's why a second exposure as always is always beneficial yeah we don't know that there's anybody with him okay anybody with them until he says my dear dear sister they're at the bottom of 790 and such let me see here. Even if he did not feel this way about the nature or understand all the things he would still be in good spirits for this day even if I didn't have those previous four pages you know the previous hundred lines of thoughts about nature. like I said we first started this particular piece talking about it you know that memory if I am dead if I'm gone think back to this time sister if you can't hear my any longer just remember what my feelings are for you. "This is one I probably say the top three or four most difficult pieces not that it's crazy hard but it's just it's a lot more difficult than reading those pulp" "My voice and my spirit will continue because you'll be experiencing all of these things and thinking in the same mind frame that I was thinking"

ROUGE-1: 46.93, ROUGE-2: 45.74, ROUGE-L: 45.69
BERTScore: 71.16

==============================================
==================== [73/100] ====================
Summary:
The goal is to understand solar cell conversion efficiency, which is the ratio of output to input energy. For most solar cells, this breaks down to the following progression, from the solar spectrum to charge collection. And we're going to be focusing on charge separation, incorporating elements of either side. The total system efficiency is the product of each individual component. If any one of these is low, the efficiency for the entire system is low. If you can see the absorption spectrum of chlorophyll, of the different types of chloropterll here, you'll see large portions of the sun's spectrum that go underutilized. on the same page and redo it this class right at the beginning because it's that important. For those who are still struggling, let's make sure that you get this sometime between now and, say, the next two weeks because this will feature prominently on the exam. So if you would not mind working directly with your partner, the person who's sitting directly next to you. Let's walk through the diode in the dark. I'll walk you through it as soon as you've done. Maybe I'll give you three minutes to complete that. a little confusing for some folks. So just to be totally clear, in a device like this one, if it were subject to illumination you would have light coming in from the side, right, either from the p side or from the n side. To transfer this into what we've seen so far with the solar cell devices facing up toward the sun, you'd have to rotate this by 90 degrees, right? We're essentially forcing current coming from this side into this side because we're using a battery. PV researchers will report a current density, in other words, a current per unit area instead of the actual current coming out of a solar cell device. At the y-intercept, there is maximum current flowing through the circuit but no power because voltage is equal to 0. The opposite happens over here at this point called Voc, which we'll call open-circuit voltage. This is the point at which the solar cell is producing the maximum amount of power, and it would be a negative number. The conversion efficiency determines the area of solar cells needed to produce a certain peak power. Many costs scale with area, including glass, encapsulants, the absorbent materials within the solar cell devices. The aluminum and racking and framing materials that go into holding the panels up in the field either on a roof or out of the field. So efficiency affects pretty much everything but the inverter and possibly some of the soft costs of the project. The material costs might end up whopping you, but it might not. pay more for a high-efficiency cell because I'm using less area, you can use this type of calculation to get to the answer quickly. This is a really back-of-the-envelope envelope engineering approach to estimating costs of a solar system. So I think this is a great place to stop. And if anybody has an idea, a fun idea, for a class project, I'd invite you to give a pitch up here at the front of class, or you're welcome to send it on an email to the class listserv.

ROUGE-1: 12.02, ROUGE-2: 11.56, ROUGE-L: 11.83
BERTScore: 61.89

==============================================
==================== [74/100] ====================
Summary:
Global chaos caused by a global outage the boss of the cyber security firm responsible has said it could be some time before all systems are back up and running thousands of flights have been cancelled Banking and health care has been affected including the NHS and some TV channels have been taken off air millions of people have been affected the problems are first reported in Australia before spreading across the world. tonight we'll be looking at exactly what happened and how it's affected patients passengers and businesses our first report is from our technology editor Zoe Kleinman. Passengers should have been on planes were forced to wait out delays so people are tired they've been handing out water the boards don't really say anything so it tells you where to go but there's no departure. Many airlines found themselves unable to use their normal systems we've had to revert back to pen and paper basically and manually check each of our customers in of course that takes longer for our customers so we've seen a good operation but it's a slower operation. from chaos at Amsterdam to planes stuck on the ground at Newar in the US cancellations and delays spread around the world Edinburgh stopped accepting incoming flights. Disruption on what was set to be the busiest day for UK flights since before the pandemic will take time to sort out Katy Austin BBC News and just a reminder you can get all the latest on the Travel turmoil online and that's at bbc.co.uk newws well for the NHS it's GP practices in England and Northern Ireland that have been most affected with doctors struggling to access their records. Pharmacy Services have also been hit here's our health editor H Pim so I am at the minute doing life is usually hectic at GP practices but a lot more so today most in England had no access to electronic patient records. saying well actually if we have lots of smaller companies doing this stuff then are we leaving ourselves more open to vulnerabilities to weaknesses to attack so it is a real dilemma I think. Nobody has seen anything of this size you know I've been covering outages now for a long time and normally they're over before they start so to see something like this have such a huge Global impact I think is really going to um make people sit up and think it certainly made people get cash out of the bank today.

ROUGE-1: 35.62, ROUGE-2: 34.83, ROUGE-L: 35.54
BERTScore: 63.48

==============================================
==================== [75/100] ====================
Summary:
In the year 2000, four-year college grads actually earned more with their entry jobs than they're earning today. Changing technology has made wages rise more at the top, but has held wages down for a lot of other jobs. A third set of factors has to do with slower economic growth, slower productivity growth and slower dynamism in the U.S. economy, says Tyler Cowen. He says there is less demand for new labor and that makes it harder for a new college graduate to get the job he or she wants. in information technology, who work well with computers and who can exploit growing global markets. When supply and demand are ruling labor markets, the people who do well are those who have an economic understanding of where is demand high, and where is supply scarce. Check out our practice questions to test your money skills. Next up, we'll show you where to find data to help you decide which career to choose. We'll also show you how to get the most out of your college degree, including how to apply your knowledge to the real world.

ROUGE-1: 24.18, ROUGE-2: 21.27, ROUGE-L: 21.38
BERTScore: 60.34

==============================================
==================== [76/100] ====================
Summary:
So, first we learned rationality, rationality axioms - completeness, reflexivity and transitivity. And when our when someone’s preference exhibit these three properties, what does it mean that he is able to completely rank all his choices all his potential choices, with also possibility, with possibility that there are more than one bundle at some ranks this is a possibility. One thing also I should add that he will be able to rank only if he has finite consumption set and also what we have learned that this will translate into a utility function. So, if these 3Axioms are satisfied you will get one utility function, in case, of finite consumption good, fine. Next we add I am not saying just continuity implies this, but continuity in addition to earlier three Axioms that we have learning, continuity. And what do we get? What we get is continuous utility function and now we do not have to impose the restriction that our consumption set is the consumption set of this individual is finite. Non-satiation would say that indifference curve has to be very thin, fine, it is clear. Convexity is not satisfied, where? In this zone, in this zone. Now tell me is it convex in this case preference of course, this is not the zone it is just poor drawing. Ajit: It is a partial, I did not define anything in the class, let us look at it. It is convex or it is not convex? Deveshvesh: Yes.

ROUGE-1: 35.20, ROUGE-2: 33.53, ROUGE-L: 34.77
BERTScore: 76.71

==============================================
==================== [77/100] ====================
Summary:
If you have a complex polyhedron, we found general unfoldings. We proved one of them. If you want an edge unfolding, that's the centuries-old open problem. For non-convex polyhedra, we know this is too much to hope for. But for general unfolding, we don't know. So today's lecture is actually mostly about these two open problems and different variations of it that we know how to solve, special cases, and changes in the model. In the case of a triangulated polygon, we usually call them ears. So cut edges until you can't anymore, so until cutting would disconnect. This means what you're left with will be sort of a tree of faces. There'll be no cycles because if there was a cycle, you could cut one of the edges, and it wouldn't fall apart. So obviously, there's a tree here. Now, trees have leaves. That's our favorite lemma lately. They have at least two leaves. Picture, or if you just have two maybe. Well, I guess-- Yeah, because then, these are both first-level ears. Or maybe just a Mickey Mouse because those are all-- Well, it probably works. But for these cases, I just need to check that I can find a facet-path. For each of those-- I guess this is still step four-- is I actually want to make cycles for these guys. We've probably seen this trick once or twice before, I think in the Mountain Valley assignment stuff. that's why you get exponential, in general. On the other hand, if your tree happens to be nice and balanced, doubling is not so bad because here you'll have constant. This will double everything below. Exponential number cuts is a lot, but it works. You can unfold every orthogonal polyhedron this way. I would love to see an implementation of this algorithm. You could only do it in a computer because you'd be splicing into all these little things, and it would fall apart. Professor: Grid unfolding is an edge unfolding of the grid. This only makes sense for orthogonal polyhedra. Do grid unfoldings always exist? Well, actually, it's not possible. The next best thing you could hope for is to refine. Professor: One thing you can do, with merely 5 by 4 refinement, is something called Manhattan Towers. This is not-- this probably is complete, but here, there, there. There are a ton of results about this. They're all partial. isn't too much doubling because there's only a single child, more or less, everywhere. But the unfolding looks something like that. Here is Manhattan Tower. So it has a connected base on the x-y plane. And I want to consider z-slices as I go up in z-coordinate. And in that case, 5 by 4 refinement is enough to unfold these things. And this is by the same authors, like a year before the general result. Let's see. I think maybe I have a movie. You can do vertex grid unfolding of any orthogonal polyhedron. You can generalize this to trees also, though, it's not totally known. As long as you have a tree of cubes with fairly long connectors in between the branch points, that works. We don't have anything where grid unfolding is definitely impossible. It's very hard to prove that there aren't unfoldings, accept by exhaustive enumeration. We've come up with lots of candidate examples, but eventually, we unfold them all. With folding, we're imagining we're given some polygon, and we'd like to make a polyhedron out of it. When is this possible? Now, the rules of the game here are different from origami. With origami, we showed from any shape, you can make anything if you scale it down. But here, I really want exact coverage. Every point here, or let's say every little patch here, should be covered by exactly one layer over here, not allowing multiple layers. This is Cauchy's rigidity theorem. If you have two convex polyhedra and they are combinatorally equivalent, they have the same way that things are connected together, and they have congruent faces, then, they are actually the same thing, the same polyhedron. If I want to make something convex, there's only one thing it could possibly make. Finding out what that one thing is is quite a challenge, but at least, we can prove that it's unique. The number of alternations is at least 4 times the number of vertices. We're going to count it in two different ways. We'll get two different answers, but we know they must end up being the same. We know because a number of alterations has to be even you can't get up to 2 k plus 1 alternations. So this is kind of obvious. If you have 2 k vertices, no more than 2 k alternations, slight, the place where we're making a little improvement is for the odd case. The average degree is 5? Slightly under 6, 4, 3, 2, 1? Let's see. Should be like 3 n minus 6 edges, so that should be 3. So most of the faces are going to have low degree. So 3 and 5 really matter, but out here, it doesn't matter so much. This is kind of a magical proof. It shouldn't be intuitive where it came from, but it's really beautiful. You'll see as it all comes together. conveniently relates vertices to faces, but it involves edges. If I look at every face and I count the number of edges, I will end up counting every edge twice, once from each side. So now, things are starting to look similar, and I want to get some cancellation going on. Use my cheat sheet here. I'm going to rewrite this formula as V equals 2 plus E minus F. OK? All I did here was decrease by-- well, because there's a half out here, I decrease each coefficient by 2. nothing surprising. So I took E, I subtracted F, just took away one of each. That's V. Now, I also know that 4V is at most, the number of alternations. Hm. I could get a formula for 4V here. 4V's going to be 8, instead of 6 and 7. We also have a plus 8. We don't know whether there any faces of degree 6 or more, so we can't rely on that, but we havePlus 8. This works as long as there's at least one face. compute the shortest paths between all pairs of vertices, something like this picture, except you don't know what it looks like in 3D. You know every edge must be a shortest path. So the edges are some subset of these guys. And so you've got lots of little convex polygons here. We know it must make a convex polyhedron. If it made two, Cauchy's rigidity theorem would tell you that they're the same. So even once you fix the gluing, you know that there's a unique convex realization.

ROUGE-1: 18.53, ROUGE-2: 17.77, ROUGE-L: 17.87
BERTScore: 67.57

==============================================
==================== [78/100] ====================
Summary:
For a particle traveling in a circle, we've seen that the velocity can be written as r d theta dt. Let's now look at rotation in an arbitrary plane. And so what we're going to do is use the right hand rule to define a direction that tells you both the direction it defines the plane and it also tells you what the positive direction of rotation is for that plane. Now, in the k hat direction, in this case, I'm going to call it in the arbitrary n hat direction.

ROUGE-1: 19.13, ROUGE-2: 18.37, ROUGE-L: 18.73
BERTScore: 73.16

==============================================
==================== [79/100] ====================
Summary:
In week four, I'm going to talk about machine translation related topics. And then in the second half of the week, we take a little bit of a break from learning more and more on neural network topics. I'll talk about final projects, but also some practical tips for building neural network systems. I'm just checking everyone's keeping up with what's happening. So first of all, assignment 3 is due today. And really today's lecture is the primary content for what you'll be using for building your assignment 4 systems. In the early 1950s, there started to be work on machine translation. It was hyped as the solution to the Cold War obsession of keeping tabs on what the Russians were doing. Claims were made that the computer would replace most human translators. But despite the hype it ran into deep trouble. The experiments did not go well. And so in retrospect, it's not very surprising that the early work did not work out very well. But as regards poetry and novels, no, I don't think we'll ever replace the translators of that type of material. using to translate. And so effectively, what you were getting were very simple rule based systems and word lookup. But that just didn't work well because human languages are much more complex than that. So if instead we had a probability over English sentences p of y, and then a probability of a French sentence given an English sentence, that people were able to make more progress. And it's not immediately obvious as to why this should be because this is sort of just a trivial rewrite with Bayes' rule. That allowed the problem to be separated into two parts which proved to be more tractable. CS228 is a statistical machine translation system. It uses a language model and a translation model to pick out the most likely why there's the translation of the sentence. It then breaks things down into pieces and explores it in what's called a decoding process. In the process, I'll go through in more detail later when we do the neural equivalent.off and see CS228 if you want to know more about that. And that's what we're not really expecting you to understand the details here. But I did then want to say a bit more about how decoding was done. In the period from about 1997 to around 2013, statistical machine translation was a huge research field. Google Translate launched in the mid 2000s. And people thought wow, this is amazing. But that was chugging along well enough. And then we got to 2014. And really with enormous suddenness, people then worked out ways of doing machine translation using a large neural network. And these large neural networks proved to be just extremely successful, and largely blew away everything that preceded it. So for the next big part of the lecture, what I'd like to tell you something about neural machine translation. model end to end on parallel sentences. And it's the entire system rather than being lots of separate components as in an old fashioned machine translation system. So these neural network architectures are called sequence to sequence models or commonly abbreviated seq2seq. And they involve two neural networks. So if we have a source sentence here, we are going to encode that sentence. And what we know about a way that we can do that, we can start at the beginning and go through a sentence and update the hidden state each time. Each step, as we break down the word by word generation, that we're conditioning not only on previous words of the target language, but also each time on our source language sentence x. Because of this, we actually know a ton more about what our sentence that we generate should be. And so in the same way that we saw last time for language models, we can work out our overall loss for the sentence doing this teacher forcing style, generate one word at a time, calculate a loss relative to the word that you should have produced. Multi-layer or stacked RNNs are more powerful. So we've got this multilayer LSTM that's going through the source sentence. And so now, at each point in time, we calculate a new hidden representation that rather than stopping there, we sort of feed it as the input into another layer. And the output of it, we feed into a third layer, and so we run that right along. And then that we use to then feed in as the initial, as theInitial hidden layer into then sort of generating translations, or for training the model. stuck with it. And you have no way to undo decisions. So if these examples have been using this sentence about, he hit me with a pie going from translating from French to English. So we'd like to be able to explore a bit more in generating our translations. And well, what could we do? Well, I sort of mentioned this before looking at the statistical empty models. Overall, what we'd want to do is find translations that maximize the probability of y given x, and at least if we know what the length of that translation is. Far too expensive. So beyond greedy decoding, the most important method that is used. And you'll see lots of places is something called beam search decoding. So beam search's idea is that you're going to keep some hypotheses to make it more likely that you'll find a good generation. So what we want to do is search for high probability hypotheses. So this is a heuristic method. It's not guaranteed to find the highest probability decoding. But at least, it gives you more of a shot than simply doing greedy decoding. an example to see how it works. So in this case, so I can fit it on a slide, the size of our beam is just 2. And the blue numbers are the scores of the prefixes. So we start off with our start symbol. And we're going to say, OK. What are the two most likely words, to generate first according to our language model? And there are the log probabilities. Then what we do next is for each of these k hypotheses, we find what are likely words to follow them? Neural translation has proven to be much, much better. It has many advantages. It gives better performance. The translations are better. In particular, they're more fluent because neural language models produce much more fluent sentences. And then the technique of optimizing all parameters of the model end to end in a single large neural network has just proved to be a really powerful idea. The models are also actually great in other ways. They actually require much less human effort to build. There's no feature engineering, there's in general, no language specific components. In 2014, the first modern attempt to build a neural network from machine translations and encoded-decoder model. Within two years' time, Google had switched to using neural machine translation for most languages. Does that mean that machine translation is solved? No. There are still lots of difficulties which people continue to work on very actively. But there are lots of problems with out of vocabulary words. And domain mismatches between the training and test data. And hopefully, you'll even get a sense of this doing assignment 4. Even our best multilayer LSTMs aren't that great of capturing sentence meaning. For languages that have lots of inflectional forms of nouns, verbs, and adjectives, these systems often get them wrong. So here's just sort of quick funny examples of the kind of things that go wrong, right? So if you asked to translate paper jam. Google Translate is deciding that this is a kind of jam just like this. And so this becomes a jam of paper. There are problems of agreement and choice. If you have many languages don't distinguish gender, the sentences are neutral between things masculine or feminine. NMT is a flagship task for NLP and deep learning. It was a place where many of the innovations of deep learning NLP were pioneered. We've got a new version of the assignment, which we hope will be interesting. But it's also a real challenge. So for assignment 4 this year, we've decided to do Cherokee English machine translation. Cherokee is an endangered Native American language that has about 2000 fluent speakers. It's an extremely low resource language. So it's just there isn't much written Cherokee data available. For languages for which there isn't much parallel data available, commonly the biggest place where you can get parallel data is from Bible translations. So this is a piece of parallel data that we can learn from. Cherokee is not a language that Google offers on Google Translate. So we can see how far we can get. But we have to be modest in our expectations because it's hard to build a very good MT system with only a fairly limited amount of data. There is a flipside, which is for you students doing the assignment. The advantage of not too much data is that your models will train relatively quickly. In the U.S. in the.1840s, the percentage of Cherokee that were literate in Cherokee written like this was actually higher than the. percentage of white people in the southeastern United States at that point in time. So we had this model of doing sequence to sequence models such as for neural machine translation. And the problem with this architecture is that we have this one hidden state, which has to encode all the information about the source sentence. So it seems like we would do better, if somehow, we could get more information from the source.

ROUGE-1: 27.06, ROUGE-2: 26.07, ROUGE-L: 25.96
BERTScore: 65.38

==============================================
==================== [80/100] ====================
Summary:
Michael Short: Radiation damage and nuclear materials are the biggest research field in the department. He explains the different material properties and what they actually mean. Short: The basic mechanism of radiation damage is like you might imagine. The strength of the material is how much stress you can put in until it starts to either plastically deform or it hits its UTS, ultimate tensile strength, where it will just fail. The stiffness is more of a response function, so it's how much does it deform in relation to how muchstress you put into it. A measure called DPA, or displacements per atom, is a simple measure of how many times has every atom left. DPA measures the number of times that each atom has moved out of its original site, but it has nothing to do with where they end up. A lot of those atoms will get knocked away and then just move right back. Even though they were displaced, and would be counted as part of the DPA,. or the radiation damage dose, the net effect on the crystal material is nothing. the number of displacements done, for each one of these reactions. Temperature. Temperature determines diffusivities. It also can change phases or crystal arrangements, like for the case of anything iron-based. The dose rate, the rate at which those neutrons come in can change the speed at which the defects cluster up. And all of the presence and density of all those different defects can be quite strongly influenced. And there's evidence for this. And it's the stuff that happens here that can tell us will our materials fail in nuclear reactors. Nuclear materials data looks something like this. Would you be bold enough to draw a trend line through a single data point? No. What about three where it doesn't actually match up with one of them? Or is there any reason why you think they made this parabolic instead of a linear line? I can draw a line that would fit between the error bars of these two right. They're not readable because they're not important. What I do want you to know is what's the quality of this data set you see? The best we can do right now is to stick them in a reactor called BOR-60 in Russia. Russia's got a fleet of sodium cooled fast reactors that can get you 25 DPA per year. And if your reactor is going to go to 500 DPA, you have to know whether or not your materials will survive. So what investor isgoing to be like, all right, here's $10 billion, but I can wait 20 years for a return on investment? No. I can. wait 20 years to start building the reactor, which means 40 years for a return on investment. Chances are, if someone's got $10 billion to give, they're going to be dead by the time they get a return. So what we really need to know is what is the full population of every single type of defect in an irradiated material? That's what I mean by damage. Did I show you guys this movie yet? The orange one? We've talked about vacancies in an abstract sense, but this is a movie of one of them actually moving about on the surface of germanium. case scenario, but that is indeed what happens in the end, and I'll show you some pictures of that actually happened. A void is nothing but a bunch of vacancies or a pocket of vacuum or gas in a material, and it all has to start with these single vacancies. As they cluster together, they reached this threshold in terms of free energy where putting a few of them together is not quite energetically favorable, but it's not so unfavorable that it never happens. The other problem too is that most materials generate helium when you irradiate them with neutrons. When you apply radiation to a material, you can just create dislocations. Normally, you would have to deform a material to create and move disLocations. You create what's called this network forest of dis Locations that makes things a lot harder to deform. The trick that we didn't talk about yet, is when dislocation from different directions collide, they get stuck. And when they can't move, you shift them away from each other and away from the source. Stiffening is an increase in the Young's Modulus, which means it takes more stress to impart the same strain. Dislocation movement is irreversible. You can't just snap it back when you relieve the stress. By making something stronger and stiffer, you make it more difficult for those dislocations to start moving. And what it ends up leading to is a strong loss in ductility. At the same time as things get stronger and stronger, they tend to get much less ductile. Most of Kazakhstan's radioactive materials came from a reactor that shut down in 1999 on one side of Kazakhstan. So they hired the cheapest truck drivers to go on the bumpy roads. Only they knew that all of the metal that all those guys thought was going to be ductile like metal was more brittle than glass. And any sort of bump would cause just complete shattering of this metal and catastrophic release of radioactive material. And luckily, there were no accidents on this trip. It took them 13 days because they went slow. In nuclear reactors, dislocations move on the easiest planes. This means that something will deform a little bit before it just shatters like those channel boxes from the Russian reactor. What actually causes this embrittlement? Well, there's a few things. If you don't push too hard, you can actually see these perfectly symmetrical slip planes at 45 degree angles to the axis of compression, and you know every one of these pillars you see this happening. This is what you want to happen to nuclear materials because you're really trying to balance this between slip and fracture. When you make something colder, it tends to be more brittle. This is the property that people worry about for reactor pressure vessels. The way you test ductile-brittle transition temperature is what's called a Charpy impact test. It's probably the highest tech, lowest tech test I've seen. You simply hit things with a hammer. And you can actually measure by turning this dial and letting it turn as it moves through the material, you can see how much energy was absorbed by the material. Temperature curves, is this not just this part that you're worried about, it's that part. So even at high temperatures, things get less ductile. It's a combination of temperature and number of defects. If either one of these criteria fails, if you become too brittle at low temperature, or your total ductility at high temperature goes down too much, that's the end of life of your reactor vessel. This is one of the biggest problems in life extensions of light water reactors. Michael Short: Can you make Charpy coupon or coupons that are similar to the status of the ones most recently taken out of the vessel, and then just put them in? Short: That's what they're doing. So they're taking these Charpy coupons, which this is bigger than actual size. They break them, so let's say this region's all garbage. Then they cut a little miniCharpy coupons out of last piece, and they're putting those back in. You've just probably recreated a year's worth of licensing work and ideas and in a class. There's a way to know how many defects there actually were in a material. It's called differential scanning color imagery. We're doing this process on nanograms of material and seeing if you can irradiate something and measure its stored energy. If you could, you could take a tiny little razor blade, take out the smallest sliver of the vessel-- smaller than a grain of sand. Not enough to cause a crack-- enough to measure its storage energy. And I want to show you guys what this process looks like. Snipes are real. You pretty much have to be British to know it, because they hunt them there for sport, and apparently, they're delicious. If you can shoot that bird with a gun, you are an expert marksman and deserve the delicious and tiny treat that you've then blown apart with your bullet. So you can you know rain bird dust on whatever meal you've already prepared. Most of the defects that cause these reductions in material properties are too small to see, even in. A memo between Eugene Wigner and Leo Szilard said that radiation stores energy by neutron collisions. When you irradiate something, we predict that it stores about 2% of its energy in radiation defense. The problem is that DSC induces a lot of artifacts in the signal that we couldn't separate from the noise. So our solution was to go nano. To use a nano a DSC, or nano differential scanning calorimeter, that can heat about 10,000 times faster than a traditional DSC. The DSC chip is made of aluminum. We irradiate the whole thing, and then finally put it in the nanocalorimeter. What happens slowed down by a factor of 1,000. That whole thing just went from zero to 450c a millisecond. That little pulse of heat actually released some of the defect energy. We were able to see very clearly, the first time we heated the sample, this extra area corresponds to some sort of energy release. We then heated that same sample a whole bunch of times, and made sure that it was always the same, which meant we had a relaxed material. The heating is so fast that the defects don't even have time to find each other, annihilate, and release their stored energy. So we need to repeat the experiments at some lower temperatures, see what the peaks are, but if you go too low, you end up getting a lot of noise in your signal. So there's going to be some sweet spot that we haven't yet found in order to see this stored energy, he says. "After doing simulations for a year that probably no one believes until you do an experiment, including me," he adds. "For now, it actually shows some sort of a trend" we keep our reactors running in about two hours. I think that's the most compact introduction to nuclear materials I can possibly give you. So any questions on what you've seen today? AUDIENCE: Is that roughly the trend you would expect? MICHAEL SHORT: Yes, I would expect an up. That's the best I can say. As far as is it actually a line? Is it a curve? I am not as brave or stupid as some of the other folks that will draw an arbitrary shaped line through a single data point. it will in the material, and will react with all the other defects nearby, decimating the population of that defect and slightly depressing that of the others. As you go higher, and higher in temperature, the slower and slower defects start to move. If you heat the vessel too much, it's no longer a code stamp vessel. Pretty tricky spot that we're in, huh? But we're trying to science our way out of it. It's a couple minutes after. I don't want to keep you longer, but I'll open on Thursday with a little story about how mass attenuation coefficients can get you out of apartheid South Africa.

ROUGE-1: 32.58, ROUGE-2: 30.71, ROUGE-L: 30.97
BERTScore: 61.24

==============================================
==================== [81/100] ====================
Summary:
According to the Chinese zodiac, it's your shēngxiào, meaning the animal assigned to your birth year. Of the many myths explaining these animal signs and their arrangement, the most enduring one is that of the Great Race. As the story goes, Yù Dì, or Jade Emperor, Ruler of the Heavens, wanted to devise a way to measure time. The first twelve animals to make it across the river would earn a spot on the zodiac calendar in the order they arrived. The traditional Chinese calendar is made up of two overlapping systems. The animals of the zodiac are associated with what's called the Twelve Earthly Branches. Another system, the Ten Heavenly Stems, is linked with the five elements of metal, xīn, wood, mù, water, shuǐ, fire, huǒ, and earth. Each year is associated with one of the animals in this order, with the cycle starting over every 60 years. reflect their communities. So if you consult the Vietnamese zodiac, you may discover that you're a cat, not a rabbit. If you're in Thailand, a mythical snake called a Naga replaces the dragon. So whether or not you place stock in what the zodiac says about you as an individual, it certainly reveals much about the culture it comes from. The zodiacs of Vietnam, Thailand and the U.S. can be found at www.zodiac.com.

ROUGE-1: 49.02, ROUGE-2: 45.41, ROUGE-L: 44.01
BERTScore: 64.83

==============================================
==================== [82/100] ====================
Summary:
Professor: In almost all cases when you address atoms, you do two photon courses because a photon is scattered. In reality, it is a two photon process and not two single photon processes. If you would not do any rotating wave approximation, how many terms would we get? Audience: Eight. PROFESSOR: Eight? I think it's multiplicative because we have four processes involving one-- oops. I wanted to say 16, 4 times 4. But now I would say in the first step, which is to the intermediate level, four at frequency, which makes eight. just tell you what is special about two photons. I focus now on a situation, and that's the most common situation in the laboratory, where there is a near-resonant intermediate state. If this intermediate state is resonant with omega 1, then we only want to consider now the process where the first step to the intermediate step is driven by the field e1. So therefore, we have now a transition rate, which is Fermi's Golden Rule. This is the delta function. I called it the function f because I want to discuss the spectral profile a minute. photon is emitted in a stimulated way and not absorbed. And therefore, our detuning is the detuning from the Raman resonance. If you're unlucky and don't choose your lasers wisely, the two laser photons could get you high up into an electronically excited state. And this may have some detrimental effect, depending what you want to do. But in general, I would say if you have more than one process, there is no interesting interference term. You just get two different rates. They're not leading to the same final state. coherence. We can say the photon omega 2 cannot be absorbed by the initial state. omega 1 mixes in with a certain probability, the state k into the ground state. And from this admixture, we can absorb omega 2. The real state k is somewhere else. And so it looks like, actually, now a two-level system, where we go from the dashed line-- called the virtual state-- to the final state b. But in this kind of picture I just suggested, I start with a dress state a. But what is relevant is only kind of this admixtures. Two-photon processes are the most important in the field, says MIT physicist. Raman process involves one photon stimulated, one photon spontaneous. When we simply want to change the momentum of an atom, we are not changing the internal state of the atom, he says. "We are going back to the same state, but it may have a different internal state," he adds. "It may be important for a lot of research within the CUA research group," he says of MIT's accelerator program. cycling transition. Well, when the atom goes up and then emits a photon on a cycling transition, there is recoil involved. So actually what you're doing is on the cycling transition,. You cycle it through many, many spontaneous Raman processes. So this is the correct description of resonant fluorescence and Rayleigh scattering. Any questions? Good. I've just mentioned that when we do Reyleigh scattering, we have to consider that the photons have momentum, and this takes us to another state. well, you give the atom an infinite mass by tightly connecting it to the laboratory. And this is what tight confinement in an ion or atom trap does. But now, we are kind of going beyond this restriction. We are now saying, OK, now we allow the atom to move. And we can deal with that by simply saying when the atom has a velocity v, we can transform-- we can just use the Galilean transformation and say OK, the physics is the same. However, the atom, due to its velocity, sees a slightly different frequency. Two-photon absorption or Raman processes are like single-ph photon transitions. If you arrange for the two photons to be absorbed from opposite directions, we reach the situation where the Doppler shift is really zero. So this is one of the handful of methods of practical importance for eliminating the first order Dopple effect. But once you have completely eliminated first order, you are then limited by the second order effect, which comes from relativistically speaking, the clock ticks differently. line, which is sufficiently sharp, sufficiently narrow, and also insensitive to magnetic fields and electric fields. This is the case for hydrogen. And, actually, with some advance in the numeric calculation of wave functions and all that, it may also be possible to do it with helium. But this sort of tells you what the choices if you want to test fundamental physics or determine fundamental constants. In all atoms other than helium and hydrogen, you would be limited by the infeasibility of many electron calculations. If you allow in your head any picture of first absorption then emission, that's where all the wrong answers come from. Photon in, photon out-- should be described together, unless you have some form of de-coherence which decouples the two processes. But that's a wonderful opening to our next chapter, namely coherence. To what extent-- we just discussed one aspect of it and we come back to this in this chapter-- to what extent is the photon a two-photon? Coherence is an important technique and important tool for measurements. Coherence exists if there is a well-defined phase. It can be two amplitudes describing two different atoms, or it can be the amplitudes of two states within the same atom. When we observe an interference, that means we obesrve-- and this is how we read out of a coherence. This is actually intricately related to the fact that we can obtain more precise information about the energy levels of systems. KR: There is a certain randomness in spontaneous emission when we go to the laboratory and look at the spontaneously emitted photons. KR: What is really the information-- the phase information-- which we have in a photon, which has been spontaneously emitted? KR: Is it always the fundamental reason or is there no fundamental reason? It's only a kind of reason of ignoring information, taking a partial trace?KR: If you ignore the position where the atom has scattered the light-- if atoms scatter light and they are wavelengths apart, then you have optical path length. driven by the operator-- the dipole operator, e-- and e, the electric field, has the phase of the laser beam. After the laser pulse is over, the photonic part of the wave function is a vacuum. We have no photon in our cavity. And now we wait and we allow spontaneous emission. And spontaneous emission is nothing else than the time evolution with the operator I just discussed with you. It's a completely coherent Rabi oscillation. I just allow half a cycle to evolve. And the result of that is, well, nothing happened to this part. the atom has been perfectly matched. Perfectly mapped onto the photon field. And the one thing we have to discuss on Wednesday is the whole of the phase phi. Phi-- we started out by phi being the phase of the laser. And if the laser is in a coherent state, I will talk about on Wednesday, in a homodyne measurement we can measure phi to any accuracy you want. And this is what I referred to as the fundamental limit of spontaneous emission because we have not lost any coherence.

ROUGE-1: 22.71, ROUGE-2: 21.57, ROUGE-L: 21.99
BERTScore: 64.02

==============================================
==================== [83/100] ====================
Summary:
Lecture eight is about deep learning software. This is a super exciting topic because it changes a lot every year. As usual, a couple administrative notes before we dive into the material. Project proposals for your course projects were due on Tuesday. Another reminder is that assignment two has been out for a while. That's going to be due next week, a week from today, Thursday. And again, when working on assignment two, remember to stop your Google Cloud instances when you're not working to try to preserve your credits. The midterm will be in class on Tuesday, five nine. It'll be sort of pen and paper working through different kinds of, slightly more theoretical questions to check your understanding of the material that we've covered so far. Last time we talked about fancier optimization algorithms for deep learning models including SGD Momentum, Nesterov, RMSProp and Adam. And we saw that these relatively small tweaks on top of vanilla SGD, are relatively easy to implement but can make your networks converge a bit faster. We also talked about regularization, especially dropout. Deep learning uses GPUs, but we weren't really too explicit up to this point about what exactly these things are and why one might be better than another for different tasks. Today we're going to shift gears a little bit and talk about some of the nuts and bolts about writing software and how the hardware works. And we'll talk about several of the major deep learning frameworks that are out there in use these days. We'll also talk about what the software looks like that you actually use to train these things in practice. how it works and what are the basic ideas even if you're not writing it yourself. So if you want to look at kind of CPU GPU performance in practice, I did some benchmarks last summer comparing a decent Intel CPU against a bunch of different GPUs that were sort of near top of the line at that time. For things like VGG 16 and 19, ResNets, various ResNet, then you typically see something like a 65 to 75 times speed up when running the exact same computation on a top-of-the-line GPU. overview of these things. So, remember that in the last several lectures we've hammered this idea of computational graphs in sort of over and over. That whenever you're doing deep learning, you want to think about building some computational graph that computes whatever function. Remember that these graph structures can get pretty complex in the case of a big neural net, now there's many different layers, many different activations. Many different weights spread all around in a pretty complex graph. And as you move to things like neural turing machines then you can get these really crazy computational graphs that you can't even really draw because they're so big and messy. TensorFlow and PyTorch let you write code in the forward pass that looks very similar to Numpy, but lets you run it on the GPU and lets you automatically compute gradients. The other nice thing about TensorFlow is you can really just, like with one line you can switch all this computation between CPU and GPU. And then again, you can just usePyTorch to compute gradient, all your gradients, with just one line. So if you just look at these three examples, these three code snippets, the Numpy side, the Tensor Flow side, and the Pytorch side, all of these kind of just kind of dive in and talk a little bit about what's going on. TensorFlow gives you a bunch of convenience functions that compute these common neural network things for you. So in this case we can use. tf.losses.mean_squared_error and it just does the L2 loss for us so we don't have to compute it ourself in. terms of basic tensor operations. Once you get to like convolutions and batch normalizations and other types of layers this kind of basic way of working could be a little bit unwieldy. TensorFlow is an open-sourceensorFlow framework. It's used by Google to train neural networks. The code example shows how to use TensorFlow to train a neural network. It uses the xavier initializer object to set up an initialization strategy for the data and labels. It also sets up variables for those with the right shapes that are kind of inside the graph but a little bit hidden from us. And in fact if you run this code, it converges much faster than the previous one because the initialization is better. There's like a lot of different higher level libraries that people build on top of TensorFlow. When we're working with neural networks we have this concept of layers and weights and some layers have weights associated with them, and we typically think at a slightly higher level of abstraction than this raw computational graph. So that's what these various packages are trying to help you out and let you work at this higher layer of abstraction. Another very popular package that you may have seen before is Keras. There's another framework also from Google, but not shipping with Tensor Flow called Pretty Tensor that does the same sort of thing. different where we're actually building up this new computational graph, this new fresh thing on every forward pass. That's called a dynamic computational graph. For kind of simple cases, with kind of feed forward neural networks, it doesn't really make a huge difference, the code ends up kind of similarly. But I do want to talk a bit about some of the implications of static versus dynamic. And what are the tradeoffs of those two? So one kind of nice idea with static graphs is that because we're kind of building up one computational graph once, and then reusing it many times, the framework might have the opportunity to go in and do optimizations on that graph. In PyTorch because we're using dynamic graphs, it's super simple. Your code kind of looks exactly like you would expect, exactly what you would do in Numpy. Now in TensorFlow the situations is a little bit more complicated because we build the graph once. So that means that any kind of control flow operators that you want to have need to be not Python controlFlow operators, you need to use some kind of magic, special tensor flow operations to do control flow. So one option is recurrent networks. So you can see that for image captioning we use a recurrent network which operates over sequences of different lengths. Caffe 2 is the successor to Caffe which is from Facebook. It uses static graphs kind of similar to TensorFlow. Caffe has a Python interface but it's not super well documented. Google's kind of trying to build one framework to rule them all that maybe works for every possible scenario for deep learning. If you want dynamic graphs, you're maybe out of luck with PyTorch and Caffe 2. Tensorflow is a pretty safe bet for just about any project that you want to start new.

ROUGE-1: 14.50, ROUGE-2: 13.74, ROUGE-L: 13.37
BERTScore: 68.16

==============================================
==================== [84/100] ====================
Summary:
This is part 3 in our series on distributed word representations. We're going to be talking about vector comparison methods. To try to make this discussion pretty intuitive, I'm going to ground things in this running example. On the left, I have a very small vector space model. We have three words, A, B, and C. And you can imagine that we've measured two dimensions, dx and dy. And then you can see graphically that B and C are pretty close together. And A is kind of lonely down here in the middle. corner, the infrequent one. Let's start with Euclidean distance, very common notion of distance in these spaces and quite intuitive. We can measure the distance between vectors u and v if they share the same dimension n by just calculating the sum of the squared element wide differences, absolute differences, and then taking the square root of that. So here we have our vector space depicted graphically, A, B, and C, and Euclidan distances measuring the length of these lines. As a stepping stone toward cosine distance, which will behave quite differently, let's talk about length normalization. There is valuable information in raw frequency. If we abstract away from it, some other information might come to the surface. But we also might lose that important frequency information in distorting the space in that way. And it can be difficult to balance these competing pressures. Finally, I'll just close with some code snippets. We're going to start to massage and stretch and bend our vector space models. And we will see much better results for these neighbor functions and everything else as we go through that material.

ROUGE-1: 23.92, ROUGE-2: 23.26, ROUGE-L: 23.92
BERTScore: 75.95

==============================================
==================== [85/100] ====================
Summary:
liyan blake a little bit about him he's kind of out there some of his contemporaries thought he was insane. William Wordsworth said that there is something in the madness of this man which interests me more than the sanity of Lord Byron and Walter Scott. He was an individual who you know was into art and was into eventually you know religion and such became a big part of his life but his as an artist you know he was a very prolific poet but he was also big-time into painting. He can kind of do that same thing but also a new type of art form kind of relief fetching. fetching into a metal which would be a long long time and so he fetches out these scenes he edges in his poems into it and so it takes a lot of time a very time-consuming as it says that's why on that page it talks about how it can be the most some of the most valuable pieces of literary history that that have survived. He kind of really pushes the boundaries on and really became the form the more bear of it and a front-runner so very interesting individual. and your parents don't know just throw out William Blake you might be right okay very very famous individual so we'll be covering in the next couple days the lamb by William Blake. Little lamb who made thee thus thou know whomade thee gave the life and bid thee feed by the stream and o or the mead gave the clothing of delight softest clothing woolly bright gave these such a tender voice making all the vales rejoice. I'll tell thee little lamb I'llTell thee he is call it by thy name for he calls himself a lamb. ultimately about God made you little lamb God madeYou he gave you these things. He gave you your clothes he gaveYou that little voice that you have. He is called by thy name so he calls himself a lamb so Jesus God religion okay you are created by those things and you have been given those things he is meek in his mild he became a little child I a child and thou a lamb we are called by his name so even though I'm a man and you're a Lamb we are we are the same okay now could the lamb be a symbol for something else could it. the field but somebody that is passionate about their religion and their faith you know the nature and the common man the shepherd person of the flock or even a child in a sense of children so it's a very short one and it's one that I believe is pretty easy to to comprehend and understand okay. "I believe it is one of the easiest to understand and understand," he says. "It's one of those things that is very, very easy to understand. It's very simple to understand"

ROUGE-1: 55.59, ROUGE-2: 52.00, ROUGE-L: 52.05
BERTScore: 70.95

==============================================
==================== [86/100] ====================
Summary:
Cú Chulainn, hero of Ulster, stood at the ford at Cooley, ready to face an entire army singlehandedly. The army in question belonged to Queen Meadhbh of Connaught. Enraged at her husband’s possession of a white bull of awesome strength, she had set out to capture the fabled brown bull of Ulster at any cost. Cú Chulpainn invoked the sacred rite of single combat in order to fight the intruders one by one. broken heart, leaving behind a land that would remain ravaged by Meadhbh’s war for years to come. “I will never forget you,” he wrote to his son. ‘I will always love you.’ “You are my son,’ he replied, “and I will always be your son.” “And I will never leave you, my son. I will forever love you’, he said, ‘even though I’m no longer your son’.

ROUGE-1: 26.13, ROUGE-2: 22.31, ROUGE-L: 23.54
BERTScore: 64.46

==============================================
==================== [87/100] ====================
Summary:
The Underworld gets a bad rap, but it's actually a lovely place to "live" It boasts historic charm and eccentric neighbors with eternal ties to the area. The community even has its own guard dog, Cerberus. Elysium is the Underworld’s exclusive VIP section. Here, you'll join the ranks of royalty and heroes. The Underworld also features four other waterways: Acheron, the river of woe; Cocytus, river. of wailing; Lethe, river of oblivion; and Phlegethon,River of fire. Tartarus is reserved for a select few who some might call the greatest sinners of all time. When Tantalus reaches for food, the branches grow taller, and when he stoops to quench his thirst, the water recedes. At their father’s order, the Danaids beheaded their husbands on their wedding night. From here, you can see the hill where Sisyphus pushes his boulder day after day, only for it to roll back. Finally, our last stop on the tour is one of our loveliest vistas. down again— all for trying to cheat death. As you can see, Achilles, the Underworld is full of exciting amenities. Here, you don't have to worry about brutal wars or painful cycles of revenge. You can finally just put your feet up and relax. It's a great place to take a break from your daily life. It also has a great view of the city of Athens, which is one of the most beautiful cities in the world. It is also a great spot to take some time off from your regular life and recharge your batteries.

ROUGE-1: 51.83, ROUGE-2: 43.32, ROUGE-L: 38.71
BERTScore: 63.68

==============================================
==================== [88/100] ====================
Summary:
Professor Martin: Fluorescence is the emission of light not associated with heat. Luminescence is the general term; fluorescence is a little bit more specific. There are different types of luminescence, and you'll get to see some of those varieties. Professor Martin will give you one that has even more imaging in it-- you'll sort of really get to understand where these pretty magical reagents come from, he says. He'll also talk about how protein engineering was done to make these macromolecules. Bioluminescence is a biological reaction that causes luminescence. The first thing to learn about fluorescence is how to spell fluorescence. It actually is fluor, F-L-U-O-R-E-S. The important part is the first part. So make it look like you know what you're talking about and spell it correctly. It's one of those-- there's two or three amazingly amazing things that happen in the ocean at night, says the professor. common typos in people's slides. One of them is spelling fluorescence wrong and the other one is spelling complement wrong. fluorescence is a key point. It's the absorption of light energy by a molecule. Once that molecule has absorbed light, there's a very transient period until the molecule lets out energy in the form of light. And then we'll talk about the application of antibody reagents with respect to fluorescence in the electromagnetic spectrum. And we're going to look at some fluorescent dyes in a little bit more detail in a minute. DNA ethidium bromide is a dye that can get into cells. We need to be careful of it because if it gets absorbed through our skin, it could get into our cells. And it could interfere with replication and other cellular processes. Because of this, there's been quite a revolution in the work done with DNA binding agents that bind a little differently and are way less toxic. The new dyes are known as DAPI and HOECHST, H-O-E-C-H-S-T. cyan blue. So that's at a shorter wavelength from the ethidium bromide. We know intercalation is perpendicular to the axis of the DNA. So where, looking at this, do you think these bind? A while ago when I was talking about the structure of DNA, I like to think of DNA as having two grooves, two places where things combine to it. And so these compounds would be known to bind in the minor groove. And in fact, they bind in particular regions of DNA where there's AT, not GC. Professor Martin will focus on the technological side of antibodies. antibodies are agents of the human adaptive immune system. They have been exploited intensively to study biology. In two or three lectures time, Professor Martin will talk about the nuts and bolts of the immune cells. And how it mounts a response to disease and other features. The lectures will be held at the University of California, San Diego, in the evenings and in the weekends. For more information on the lectures, visit the U.S. Department of Health and Human Services website. The adaptive immune system is an amazing system where you can do combinatorial biology and basically recognize any target entity you're interested in. There are a bunch of different cells that are produced from the pluripotent hematopoietic cells. But what we're going to focus right in on are the B cells. These are the cells of the immune system that produce soluble antibodies. And when you challenge a B cell population with a foreign entity, the B cell populations will go into gear to produce antibodies that very specifically recognize that foreign target.

ROUGE-1: 16.43, ROUGE-2: 14.59, ROUGE-L: 14.61
BERTScore: 64.67

==============================================
==================== [89/100] ====================
Summary:
 RAFAEL JARAMILLO: Today we're going to discuss the many D's of thermodynamics. We'll talk about lowercase d, lowercase Greek d, and uppercase Greek D. He'll draw a 3D visualization of how the entropy for a given phase might vary with pressure and temperature at a fixed composition. Jaramillo: The most common transformations that we talk about are transformations that take place at constant pressure and constant temperature, isobaric and isothermal transformations. is a function of those independent variables that measures the transformation quantity for the transformation between two different phases. For example, in the case of water, the quantity of water in the solution is the ratio of the water volume to the volume of the solution. For the example, the total water volume is the sum of the volumes of the solutions of the two phases. The total water content is the product of the proportions of the different water volumes. For more information, see the Wikipedia article on water.

ROUGE-1: 24.98, ROUGE-2: 18.59, ROUGE-L: 17.48
BERTScore: 64.24

==============================================
==================== [90/100] ====================
Summary:
In this video I'm going to be going over lung oscilation specifically the sites of where you osculate. We're going to go over some audio clips of normal breath sounds and where you should hear them at CU that's a big thing. We'll also go over abnormal breath sounds. In the next video I'll be performing an assessment on a patient and show you how to listen with your stethoscope to these sides. I'll also give you some landmarks to make your job easier so you'll know what intercostal space correlates with which lobe of the lung. patient's breathing in and breathing out so that's why you're hearing it on inspiration and expiration now it can sound similar to a parac cardial friction rub how do you tell the difference um if you are wanting to know is this the lungs or is this this the heart just listen have the patient hold their breath. If you can still hear that harsh grading sound it's the heart because they're holding their breath their lungs aren't moving so you've rolled out the lungs. That is lung occultation and normal breast sounds versus abnormal breast sounds.

ROUGE-1: 11.88, ROUGE-2: 10.97, ROUGE-L: 10.68
BERTScore: 69.75

==============================================
==================== [91/100] ====================
Summary:
Professor Donald Kagan: Sparta is the most important of the early poleis. He says the only decisions made were questions of whether to go to war, whether to make peace. Kagan says when a question was put to the Spartans, they responded by shouting and banging on their shields. The only people who spoke at those assemblies were the kings, the gerousia, or a group of people I haven't mentioned to you yet, the ephors, the five ephor. Aristotle tells us that beginning in the early seventh century, the date he gives us, we are introduced to a new thing, magistrates are chosen from the aristocracy to do various jobs in the city. In Athens, the magistrates were called archons. It means, in the most technical sense, rulers, but it means really important magistrates in the state. Nobody in Athens holds office at this time, or at any time, as far as I can tell, for more than a year. The only thing in town that has continuity is the Areopagus. begins to engage in commerce to a greater degree than before. This leads to new wealth and new class distinctions, which are now based not on birth but on wealth. We hear new terms, not all of them new, a couple of themnew that come into the picture. We are talking about people of the hoplite class. At the bottom of the barrel we hear about people called thetes. They've always been around, they are the poor; they don't own land. They live at the mercy of chance; they work for other people. Another theory is that they were indeed named that, because they were hoplites. It hardly matters which of the stories you prefer or whether you choose both; we're talking about the same people. This new class of independent family farmer has arrived in Athens, and as in other states is not satisfied with his position in the state. We will come back to this story when we talk about Solon, but think about these changes as happening, as the next change that I want to tell you about occurs. opponents that he was defeated. The leader of the resistance was the family known as the Alcmaeonidae. They went up there, locked up Cylon and his supporters in the Acropolis, in a temple. They cut the cord and killed the Cylonians. That put an end to the Cylonian conspiracy but it brought something to AlcMAeonidae as well, a curse. They were declared accursed and driven from the city. Later on we will hear they're back again and they're very important. But the curse continues to be attached to the family. that there are the kinds of discontents that we have been talking about which find the leader in the form of a man who is an outstanding figure for some reason, who is willing to try to establish a tyranny. That it fails, I think, is an indication that the same forces haven't reached the power in Athens that they had reached in Megara, Corinth, Sicyon, and places like that. It's a warning about troubles ahead and I'll turn to those troubles in the next hour.

ROUGE-1: 10.54, ROUGE-2: 9.94, ROUGE-L: 10.31
BERTScore: 58.84

==============================================
==================== [92/100] ====================
Summary:
Ka-Yen: "I walked into MIT not knowing a single thing about nuclear energy" "I was like, I wish someone could have told me these things" "We're just kind of like-- it's a refresher. A couple of fun facts" "You guys are going to be starting up full cycle on Friday with really cool topics like stopping power" "There won't be a lot of crazy intense math because we just want to give you guys a break" "It's a very brief history in a nutshell" Most of this development happened between 1939 and 1945. In 1951, the first nuclear reactor to produce electricity was the experimental breeder reactor. The first nuclear powered submarine, the USS Nautilus, was launched in 1954. But the real heyday of nuclear was between 1960 to 1975, when Westinghouse created the first 250 megawatt electric nuclear power plant. Unfortunately, all good things have to come to an end, though. From 1975 to 2002, you can see a massive decline in nuclear energy. China, India, and South Korea are the main players in this game. Nuclear creates 75 times less carbon emission than coal does, and 35 times less than natural gas. One of the good things about nuclear is that it can serve as a good baseload source of energy. Other alternative forms of energy might be better for the environment, it might be safer, and things like that, but it's not really able to do this. So this is why we kind of want to replace coal and natural gas with nuclear. About 21% of the reactors that are located and working in the United States are boiling water reactors. Light water reactors, or LWRs, are mostly broken up into two subcategories: boiling water reactor and pressurized water reactor. So how you guys can think about reactors is that honestly they're just kind of glorified steam turbines. So it's a really, really simple mechanism and we can walk through that right now. About what I've mentioned? Awesome. So now we'll talk a little bit about reactor types. BWRs comprise about 21% of the reactors in the United States. They are functionally essentially the same, and it's just slightly more complicated. With BWRs there is a higher chance of leaking radioactive material into the environment. The next kind of reactor that falls under the light water reactor category is the pressurized water reactors. PWRs are actually more important, if you will, than B WRs. They have two separate loops with the nuclear fuel being more isolated from the environment, which mitigated violent fission. Heavy water reactors use deuterium oxide instead of light water. Deuterium is cheaper than light water, but it also has a higher water cost. Breeder reactors are essentially the same thing as light water reactors. They were most popular between like the '50s and the '60s-ish in the very beginning of creating nuclear reactors and they're a really cool idea, but they're not very practical. They have to change out their fuel more often, and that means the fuel gets spent more quickly. Fissionable material is material that is willing to undergo fission with a thermal neutron. fertile material is just material that absorbs a neutron and then is able to become a piece of fissile material. Breeder reactors are adding extra chunks of uranium 238 and thorium 232 into the reactor. They're able to work at a higher fuel efficiency because they don't need to add as much fissionable materials as you would for a normal light water reactor. And then we're going to move on to the next generation of reactors. The main reason why we're a little bit hesitant to start using more nuclear power is because of safety issues. None of us can argue that nuclear is like 100% safe. It actually does have some dangers associated with it. After a nuclear accident you can see a pretty steep decline in the amount of nuclear reactors that are being commissioned. And again Fukushima, once again, with the number of reactors being commissioned after the accident just declines dramatically. The main things that are holding us back is just social, economic, and therefore like government hesitance to use nuclear power. The Three Mile Island reactor underwent a core meltdown in 1979. Chernobyl exploded in 1986 because the control rods were displaced too much, causing the first explosion. To this day, we can't pinpoint the cause of the second explosion at Chernobyl, but it could have been like helium building up or a helium-filled building up, or just a building up of helium or helium or something like that that blew the whole reaction apart. But there's a second explosion that blew just this entire core apart that kind of stunk. Next reactor accident that we were alive for, which is cool, was Fukushima Daiichi. This is a very similar problem, as you can see that in all these instances of the reactor incidents, it's just kind of like the fuel core was misbehaving and we weren't able to get enough coolant water to it. So following the earthquake, these coolant pumps broke. They're like, oh, that's OK. What we can do is we have backup generators to continue running the pumps. We're all good. Reactor accidents are actually pretty rare. There's three main accidents that have happened. But because these are the things that people get ingrained into their mind, people think that nuclear reactors are incredibly dangerous. That's why we have this social hesitance, which is why we aren't able to get enough government funding. And that's why nuclear power isn't more of a thing. Makes sense? Yeah. So what in the world do we do with it? So first of all, the main thing in nuclear waste is spent fuel. After undergoing a bunch of fissions, these uranium particles get transformed. The main issue is what do we do with all this material? So this material that comes out is pretty radioactive and it's also incredibly hot, so it can be dangerous if someone decides to come and eat it. The primary way of disposing of the spent fuel is putting it into spent fuel pools. This is an OK solution, except for the fact that, again, we just have way too much spent fuel to be able to do this. The next solution was something called dry cask storage. Yucca Mountain is located in Nevada. People in Nevada weren't happy about this. There was a lot of opposition. Because of the social opposition there was government opposition and many loopholes we had to jump through. They also realized that it wasn't as geologically sound as they had hoped. There's a lot more groundwater running through and seeping through Yucca Mountain than they thought there would be. So there's a huge debacle. Basically the costs are rising, nothing much was happening. And then 2011, under the Obama Administration, he just called it quits. The economics of nuclear power is actually a really complicated topic and it changes depending on who you talk to. If you look at this chart, yellow is nuclear power, the gray is coal, and the blue is the natural gas. But basically, anyone you. talk to, you can see that nuclear is not nearly as economic of a source of electricity generation as. any other of these ones I mentioned. Unless you talk. to UK. UK thinks it's OK. But everyone else is saying that it's not as money efficient. other forms of electricity. People buy the electricity that's cheapest, not necessarily the Electricity that's best for our grandchildren or something like that. Yeah, so that's why nuclear power isn't more of a thing. So do you guys have any questions about anything I mentioned? If you guys are interested about any of these topics, I recommend going to NRC.gov. They have a lot of really cool information. That's basically where I got the majority of my information for the slide show. It might just be skewed a little bit pro nuclear.

ROUGE-1: 25.72, ROUGE-2: 23.33, ROUGE-L: 22.31
BERTScore: 61.24

==============================================
==================== [93/100] ====================
Summary:
Professor Steven Smith: I want to talk today about Aristotle's discovery of America. He says Aristotle discovered the American Constitution 1,500 or 2,000 years before it was written. Smith: Aristotle's proposal for a mixture of oligarchy and democracy seems, in many ways, to anticipate James Madison's call for a government where powers must be separated. But Aristotle's mixed constitution differs from ours still in certain important respects, he says. He doesn't so much insist, as you will see in your reading, on the actual separation of functions of government. Aristotle's Politics is a book about the kind of knowledge requisite for political regimes. The great-souled person is the ideal recipient of this form of education. The gentleman may lack the speculative intelligence of a Socrates. But he will possess practical rationality, of practical judgment necessary for the administration of affairs, he says. The book was developed without any explicit reference to Aristotle, writes David Frum, in Berlin, Germany, in 17th century.. Anyone ever heard of Isaiah Berlin, the English political philosopher? the back door. On Friday, let me just remind you, Il Principe. We'll study Machiavelli. On this very partisan note I conclude. On Thursday, we'll study the life of the Italian statesman and play a game of chess with him. We're going to see how well he can play the game of poker. I'm looking forward to it. I hope you'll join us for the game on Friday night at 8 p.m. ET on CNN.

ROUGE-1: 8.33, ROUGE-2: 6.36, ROUGE-L: 6.46
BERTScore: 57.15

==============================================
==================== [94/100] ====================
Summary:
SNLI is the Stanford Natural Language Inference Corpus-- MultiNLI, and Adversarial NLI. The premises are all image captions from the image Flickr30K data set. All the hypotheses were written by crowdworkers. Only 74% of the examples are sentences that is S-rooted in their syntactic parses. The overall Fleiss kappa measured interannotator agreement was 0.7, which is a high rate of agreement. The data set has over 550,000 training examples. crowdworker had to come up with three sentences. One definitely correct -- that's an entailment case. One may be correct-- that is our gloss on neutral. And one definitely incorrect, which is our Gloss on contradiction. So you can see here that there's an attempt to use informal language connecting with informal reasoning, common sense reasoning in the prompt. And then those get translated into our three labels for the task. And here are some examples from the validated set. And I think they're sort of interesting, because you get high rates of agreement, but you do find some examples that have a lot of uncertainty about them. SNLI was a competition between deep learning systems and humans. SNLI is based on premise and hypothesis that probably describe a different photo. MultiNLI is a kind of successor to SNLI. The train premises, in this case, are going to be much more diverse. They're drawn from five genres-- fiction; government reports, and letters and things; the Slate website; the Switchboard corpus; which is people interacting over the phone; and Berlitz travel guides. And then interestingly, they have additional genres just for dev and test. MultiLNI is an interesting early example of being adversarial and enforcing our systems to grapple with new domains and new genres. MulitiNLI is another large data set, slightly smaller than SNLI. They did the same kind of validation, and that gives us our estimates of human performance. And then finally, adversarial NLIs, kind of a response to that dynamic that looks like we're making lots of progress. But we might worry that our systems are benefiting from idiosyncrasies and artifacts in the data sets. in the abstract, but rather with the goal of fooling state-of-the-art models. That's the adversarial part of this project. And this is a direct response to this feeling that results in findings for SNLI and MultiNLI, while impressive, might be overstating the extent to which we've made progress on the underlying task of common sense reasoning. Adversarial NLI is exciting because it's given rise to a whole movement around creating adversarial datasets. And that's represented by this open-source project, Dynabench. SNLI and MultiNLI into Turkish. XNLI is a bunch of assessment data sets that is dev-test splits for more than a dozen languages. Those are human-created translations that could be used to benchmark multilingual NLI systems. So there's a wide world of tasks you can explore, and I think that makes NLI a really exciting space in which to develop original systems, and projects, and so forth. And those could be interesting for seeing how well a model can grapple with variation that comes in very specific and technical domains.

ROUGE-1: 33.71, ROUGE-2: 31.58, ROUGE-L: 31.12
BERTScore: 63.99

==============================================
==================== [95/100] ====================
Summary:
In a perfectly competitive market, the firm is a price-taker. No matter how many units they produce, they're just going to be able to get that same market price. Their marginal revenue curve will essentially just be a horizontal line like this. But in an imperfectly competitive market there's some differentiation amongst the various players who are competing, and so their market price is a function of quantity. The price that they get in the market is higher than the marginal cost and the marginal revenue at that point. more because it doesn't make sense from a marginal revenue point of view. This gap, the difference between the price and the marginal cost at this rational quantity for this firm in an imperfectly competitive market to produce, economists would refer to this as an inefficiency, inefficiency. Folks are willing to pay more than that marginal cost, but you still have no motivation to produce more. Because if you produce more, even though the price is higher, your marginal revenue is going to be below themarginal cost.

ROUGE-1: 39.47, ROUGE-2: 37.57, ROUGE-L: 39.47
BERTScore: 69.00

==============================================
==================== [96/100] ====================
Summary:
Professor: Today we're going to talk about a variation, although related to generic rigidity. The idea is we think about rigidity-- does a linkage move or not-- to the first order. If the motion exists, surely you can take this derivative and evaluate it. You need some smoothness. But if there's a way to get started moving, that doesn't actually mean you could actually move. The notion will, stated in terms of motions-- you can think about this in the absence of a motion. with motions, we were supposed to preserve edge lengths. In the book, we take the derivatives. We work out all the details. But I'll just tell you what the condition is, because ultimately all we need is this condition. I'm just defining an infinitesimal motion to be a velocity vector for every vertex, such that this property holds. This is a dot product between two vectors. If you have two vectors, a and b, you take their dot product, this is just something like this. general, we want this thing to equal that thing. If you rearrange terms, that's the same thing as this property. I just take this one minus this one, and set it equal to zero, and you get that. OK, so this is some intuition. Maybe it makes sense, maybe not. In the end, this is all I care about. And it's useful to realize that it's a first derivative of a motion, because what that implies is that if there's a motion-- a real, honest to goodness motion-- then there's an infinitesimal motion. This is the idea of a rigidity matrix. So we have a whole bunch of these constraints, one for every edge. And whenever you have a bunch of linear constraints, you can write them as R-- some big matrix-- times the velocity. And what we care about, what we're trying to solve for, is d. We want to know how many-- this is basically the number of degrees of freedom in infinitesimal land, which is not quite reality, but it's close. so important, because it's so easy algorithmically. Things become a lot easier. Let me draw you one example just to show what happens at non-generic points. So these two points are pinned relative to each other. And then this thing is taut, so it can't move. But to the first order, it can move. And it's allowed to move straight up or down-- anything perpendicular to this segment. That's always true. And if you perturb this example, infinitesimal rigidity will give you the right answer. reverse holds. If there's a motion, there's always an infinitesimal motion. If you're infiniteimally rigid, you're always rigid. In the ideal setting, we're going to have d n columns-- so let's say 2 times n columns in two dimensions. And we're supposed to have 2n minus 3 rows. So it's close, but in general, they should differ by about this much. So they're almost square, actually. So maybe go over here. I don't need the rigidity matrix any more. Tensegrity is a generalization of a linkage and where we allow three kinds of edges. It's a space of all possible configurations where these three things hold-- give you desired lengths for each of these things. A path through that space is going to be a motion, just like before. The only way to move the square or in fact in any generic configuration is to move it in any convex or convex direction. The other one will get shorter, but one of them will get longer. dot product, and I claim this is really measuring-- it's a number, and it's measuring the sign's change in length of that edge. So if C of v minus C of w dot product with d of v plus d of w is greater than or equal to 0, that's the property I want for struts. That says it gets longer or stays the same to the first order. Now of course, in reality it might get slightly shorter, but not in the first derivative. All you need to know is it there are fast algorithms to solve this, also. In a rigid structure, the edges can prevent a motion. This is the notion of being in equilibrium. Let me draw a picture. Here's a rigid linkage. I'm not going to say yet which edges are struts and cables. But assuming I've got this right-- and I didn't. I got it backwards. All right, now, it would be nice to say if you have an equilibrium stress and only rigid if this is true. But that's not true. This goes back to 1981, 1981. There's been a lot of excitement about origami tesselations since the mid '90s. Chris Palmer revitalized them, and then tons of people are looking at them. And there's this cool algorithm for building them. So you take some tessellation like these squares, and triangles, and hexagons-- some collection of polygons. And then you twist them a little bit, all the same amount. Then you just connect the dots. But when does it work? There is a new result from the summer that it works for some choice of angles. So I had some flexibility-- how much do I shrink and turn? But this will work for some choice of turn and shrink amount, if and only if your graph or your initial tessellation is a spiderweb. So that's kind of fun-- a little bit origami out of linkages and tensegrities. All right, then we move on to another characterization of stresses, which is polyhedral liftings. A polyhedral lifting is going to work if I have a non-crossing configuration. Because I forbid crossings, I end up decomposing space. In general, you're going to have a lot of non-triangles. Those should all remain planar. One thing you can do-- there's some freedom here. If you have any lifting or, for example, you could not lift them at all. Set all the z-coordinates to zero. That's fine. You could also just lift everything onto some other plane, and generally have a rigid motion of freedom. You can't translate around, but you have one two-degree two-rotational freedoms, I think.  Maxwell proved this, or claimed it in 1864. Theorem is there's a one-to-one correspondence between the equilibrium stresses and polyhedral liftings. So far we've been allowing crossings, except for this very last theorem. Now we begin this section where I forbid crossings between the edges, just like what that picture was. Prevent this, and this is a constraint on the configuration space now. I'm going to use basically everything I proved today to prove a big theorem-- the Carpenter's Rule Theorem. configurations where no two edges cross. This is a smaller version of our old configuration space. It's still defined by polynomial inequalities. Two edges either share a vertex or not. And that's it. But they're not allowed to touch anywhere else. And now what we care about is something called a locked linkage. And a linkage is locked if its configuration space is disconnected. If I have some configuration here, I can move to any other configuration here. And same over here, but there's no way to get from this configuration to that configuration. If we have a linkage, and say we take a configuration of that linkage, let's say it's of maximum degree 2, there's a motion that straightens out. So if I can straighten all the paths, I can definitely get from anywhere to anywhere. There's one catch which I didn't say here. I need to add outermost. When you have nesting like this, you're in trouble. This guy is not going to get straightened out. It could be super long. It may not have room to straighten out inside that convex chamber. Tensegrities-- struts-- will force things to be expansive. If initially you're non-crossing, you will be non-Crossing forever. This lets you not worry about crossings. You just have to worry about distances. We use just, is this thing rigid or flexible at all? And that's the power of all this rigidity stuff, just telling whether something can move a little bit, and then just doing that over and over again, you can actually go all the way, and get to straight or convex. to show that it's flexible. But it's rigid if and only if this is true. This will actually always be true-- the corresponding linkage is rigid. If I turn them all into bars, this thing's not moving at all. So fact, this second condition doesn't really matter. What matters is the stresses. Now stresses always exist. But I claim actually pretty much all the stresses have to be 0, at least on the cables and the struts, which is what I care about here. Carpenter Rule: In four dimensions, you can't get locked in things like valleys and other things like that. If everything's a valley, something's going to go wrong at the top picture, because you've got to have mountains, like, turn around. The only way the whole thing can't be flat is if your polygon is already convex, and then we're done. The Carpenter Rule: Next, we're going to talk about the other situations, and what happens with degree 3. still buy them at the hardware store. That's it. You can't buy them online. You have to buy them from the store. They are not available online. They can be bought at the store, but you have to pay for them in cash. They're not available on the internet. You must buy them in the store to get them. You cannot buy them on the Internet. They must be bought from the hardware stores. That is it. They have to be bought in the stores.

ROUGE-1: 26.52, ROUGE-2: 24.76, ROUGE-L: 24.74
BERTScore: 65.61

==============================================
==================== [97/100] ====================
Summary:
Chess has been known as a tool of military strategy, a metaphor for human affairs, and a benchmark of genius. The game was originally known as chaturanga– a Sanskrit word for "four divisions" With its spread to Sassanid Persia, it acquired its current name and terminology– "chess," derived from "shah," meaning king, and “checkmate” from “shah mat," or “the king is helpless” After the 7th century Islamic conquest of Persia, chess was introduced to the Arab world. Deep Blue’s triumph over Garry Kasparov in 1997 was the first time a machine had defeated a sitting champion. Today, chess software is capable of consistently defeating the best human players. But just like the game they’ve mastered, these machines are products of human ingenuity. And perhaps that same ingenuity will guide us out of this apparent checkmate, as it did in the case of chess greats such as Benoît Mitterrand and Toulouse-Laffitte.

ROUGE-1: 35.34, ROUGE-2: 32.53, ROUGE-L: 32.93
BERTScore: 65.62

==============================================
==================== [98/100] ====================
Summary:
presenting okay share your screen that's what I'm doing oh you are okay hopefully it is yeah stop sharing yeah you should be sharing my screen under your camera until I can decide if I click on slideshow this is still show my camera uh it does I guess I can minimize it do screen sharing are you recording tooYeah great baseball back yeah I mean it's my first time giving my lecture so I'm as good as I can be do you want that cheers I mean I have to write something to hear me okay okay you know that it works. this week will probably go live tomorrow uh not quite sure yet but you'll try to get it up as soon as possible so I guess like without further Ado let's Jump Right In. I'm gonna go somewhat into detail into what representation learning is and I think this should sort of cap out the last few weeks of deep learning um and probably give you a more comprehensive understanding of what deep learning actually is doing. So I guess before we jump into deep learning let's talk a bit about shallow learning. might not be something that you can so let's say that you're working on a problem of predicting the price of a house from a house so your X can be a house you can really put that into your model. Different kinds of problems will have different kinds of feature extractors so if your data is arranged in say a table you could maybe say that each row is a single house and each column is one of the features. This process of choosing the right features can get really complicated really fast and this is also a compromise solution in the sense that you are learning the weights of your model but you're still hand programming the feature extractor yourself. what if you combine them and that's exactly what a neural network is a model that combines with feature extraction and output prediction and it learns everything from the data so hopefully it just sort of gives you some context as to why deep learning has been sort of taking off and classical machine learning is not as used and areas like Vision anymore because deep learning allows you to sort of automate this entire process from end to end and in a sense what you're really doing is you're learning a representation of your input right so your features are a way to represent what that input looks like. When we train a model from scratch it takes a lot of time compute and training data so to just to give you an idea of how much data is often necessary to train a pretty good model even just a few thousand examples is often not nearly enough. But luckily huge models have already been trained before so the sort of question is can we leverage them in some way and the answer is yes absolutely so you might have heard of what we've called pre-trained models and many of these models are frequently used all the time. to embeddings to something called a latent space so we often prefer to work with lower dimensional data. A common task is transforming them in from high dimensional data into lower dimensional instructors so we can capture this through something called latent space of features. One way is to learn and embedding as part of our neural network for our Target task so this sort of allows us to get in a bedding that's nicely customized for our particular task but it may take longer than training the embedding separately. as a 70 784 dimensional Vector plus 25 to 784. so what you can do is you can train a model that would classify um what digit the image contains. Once this model has been trained this the weights must have learned something meaningful right. So this means that we could just take some of these layers in the middle take the output of that as a representation of this 20 dimensional 28 by 28 dimensional image okay and so here's sort of an example of some of the results from training networks from scratch versus applying some sort of transfer learning. major advantages of pre-trained networks so a lot of the time pre- trained networks are trained on very very large data sets. We can also pre-compute and store our embeddings instead of using the original High dimensional data. We will transition to the next part of the lecture which is going to be on self-supervised free training. We'll go into some details and examples of this in action but especially um I know this is not an NLP course we talk more about CV but transforming learning is especially huge in NLP. 16b I think you might have seen regression in those classes um there are other examples of object detection segmentation we will discuss those in the coming weeks. Even without any labels we can still learn something meaningful about the structure of the raw data how many have you guys taken 16b before so yeah you may have you might recall something called PCA from the class of principle component analysis it's actually one of the most common unsupervised learning algorithms out there because if you remember correctly you just input some data Matrix into that algorithm. Unsupervised representation learning is a technique that can be used to learn representations from large data sets. Large data sets can be helpful for learning more generalizable representations. The data sets are usually not labeled you can you could like scrap text or image or images from the web but you can't really label them automatically. We want to see if we could use unsupervised learning techniques to the sum level data sets and learn representations using that and this is also appealing because labeling in general is a very time consuming and tedious process say that you want to label a billion images. part or property of the input from any observed or unhidden part of theinput. This information can be hidden across time or space. We will go over some examples soon and actually we will have an entire lecture dedicated to self-supervised learning for Envision in a few weeks so we'll double deeper in that lecture Cube. In computer vision this can be something like you learn the representations from some pretext tasks and you'll use those representations for image classification or object detection or semantic segmentation or whatever for NLP this could be text classification machine translation document summarization question. Great training is a very broad topic and self-supervised learning algorithms can be applied to different domains so I guess we can delve into examples now. The hope is that if the model can learn to predict the original order like what patch goes in the let's say the top left corner versus the top right corner it is trying to learn something meaningful about the image so it's not just focusing on say all level features anymore but it's also trying to understand what's going on in the image it's going to learn that an image can be made up of different parts. this can be done is you could predict a word from its surrounding context so say if you have the sentence the dog with the man you could try to predict the word bed from dog and dog because a dog should kind of imply that the word bit is associated with it so this approach is called a continuous bag of words model. There are many other ways to train virtual back models one example is um skip Ram so instead of predicting aword from a context you instead predict the context from a word so you like kind of like flip the model upside down. the slide is it turns out that the current state of the art for CV is actually very similar to Burke so that's a teaser for the lecture that we discuss Advanced Techniques and as software CV. There is a homework for this entire cluster which is the high Crush notebook that should be due next Tuesday even though this lecture doesn't have homework I mentioned before that there will be a lecture on on Advanced SSL for CV and that will have a homework so to work on that homework this slide deck should be up on the website again if you feel free to do that that is it for today a second pause.

ROUGE-1: 28.49, ROUGE-2: 27.64, ROUGE-L: 28.02
BERTScore: 72.59

==============================================
==================== [99/100] ====================
Summary:
Third lecture on Foundation mulative AI. We're going to cover chat GPT. Next time we'll talk about stable diffusion image generation. We'll end with the lecture on AI ethics and regulation as well as a panel. It was a really daring bad not obvious at all at the time that this would actually work out. So should be be a lot of fun and just to quickly go through our course schedule as well a little bit right so today is January 16 uh and next time we will talk about emerging Foundation models basically Foundation models generative AI. GPT relies on a lot of tricks and Engineering insights and breakthroughs that we're not going to cover. We apply this self-supervised learning where we learn without label data so we can get as much data as we want because there's no human being in the loop. What we get from this you know by learning from observation and learning from the data directly is a very contextual and relational understanding of meaning. We'll talk about something that's extremely engineering heavy in you know chat GPT. be useful for you without getting into all the engineering details but of course in real life those engineering details really really matters and are very very hard to get right. I think that again like we talked about a little bit of a theme here right is that the why this new AI is so powerful is because it doesn't Force things to comply to Simple Rules right it kind of abandons our ability to understand and compress what we're seeing and deals with that chaos directly. I took this quote from a general from the 18 and 1700s and he says that P Theory which sets itself in opposition to the mind. We're going to create scores or predictions for all words in the L like in the human you know vocabulary in the English vocabulary that sounds extremely expensive and it is quite expensive and so I have different tricks to make this work. Only one word would be correct but it gives a lot of feedback as well because there's a lot. of information knowing which ones are not correct and and how we're going. to do this is that we'regoing to create these U probability scores meaning that these are just non- negative numbers and they all sum to one. The CTP is a very simple approach but it's it's a certain scale that's that's never been seen before and really that's what a big part of open eyes is. Just training the final model cost around $5 million just in in Compu electricity bills right that's how huge and much compute they spent on this. The first version it was using 175 billion parameters and just training the the the final models was using around $4 million to $6 million a year and that's not including all the iteration. bet and and the research there is like well you know we've been doing this language modeling for quite some time trying to understand Language by predicting the next word based on previous words and we're using it for certain things but I mean know very few thought and were convinced that if you just scale this up big enough it will become a multitask Sol and show humanlike Intelligence and that this actually really will work. People talk a little bit of this emerging abilities because also it's not linear right like you start adding and putting putting more and more compute and parameters and like oh it's still not very useful but then at some point just start being like extremely useful so it was it was a huge kind of leap of faith as well for open eye to say like well we're just going to go all in. Transformer part is extremely extremely important so there's a debate a little bit what was the most influential part of making uh CHP and large language model possible uh Transformer is definitely a significant part of it and and I'll let you judge for yourself but uh I think it's less important than the actual modeling perspective that we've we've come up with. In order to understand the Transformer we're going to start to thinking about how we can process sequences so text is just sequence of words. is that for every step here that's label with the same uh digit you know they can all be done in parallel. This is key because in in deep learning we use this uh um computer is called gpus. If we can make multiple steps into single step in parallel this is a single cost. We want to run things in parallel as much as possible so here basically you know this be a cost of four because all these different numbers can be run in parallel uh so this would just be acost of four and then of course processing this whole sequence will be a costs of nine. these are extremely extremely popular and a version of them called uh lstm long short-term memory networks um it performs really really well and some people say it performs you know almost better than Transformers a lot of times but they just take them longer to train because we're going to realize why it one a point but they work really really good and also notice here somehow that uh this was very very intuitive for researchers to say like well text we read text from left to right we process words one at a time and therefore our models should to to to tble to learn effectively. when to the Target so we kind of we're we're not going to enforce this quential structure we're just going to directly let the information flow from the previous word to the current word Etc right what we've seen so far and then if in the third step as well we also just go to let information flow directly and have a direct connection to the previous words and not force things to happen sequentially um and the important thing to notice here right before when we were processing things sequentially you had to wait for the previous step to finish to do the next step right but here you don't because here everything is processed independently. in parallel it's the same step and this is true for all of these steps yes this sense when we compute a output distribution over all the words later like as a prediction we talked about. In a recuit network things are processed one at a time so the the model can figure out that you know Financial comes after the because it sees the first and then Financial but in a Transformer you know in the below here like if the only thing you see is the word and they're all F to you know. be efficient and work well so now we're just going to look at the F last part which is the chat part um so you know you you you train this model now that you call DPT 3.5 or something and now you want to turn turn into chat GPT right so we have a uh a really uh good model basically we've done 99% of all the work that's that's required and a lot of people still kind of debate how important this last step is but open is it does a difference. Just goes off a little bit like here for example when you say you know I went to the financial and then just you know some small error happens and it goes off the road to restaurant like somehow you know they started seeing that okay now it's basically go Haywire because it went off and it's in a different space than it's been trained on. So somehow we want to be able to say like well if you find yourself you know alittle bit off the the path you should be ableto find your way back to be as as robust as possible to any kind of use case. is about like how do you figure out what actually helps you reach your goal and optimizing your score function even if it's delayed um okay so another thing in doing this that's very very important it's exploration versus exploitation so let's say now basically that our model has seen these two different cases and have received two feedbacks right. In one of these you will got a pretty good score and in one uh you know in the lower here you got apretty bad score so then basically that means that we just exploit the information that we've seen so far and do the best. gratification actually leads to very non- GRE and and independent robustness so these are the consequence of applying reinforcement learning where you only get feedback at the very end so there's less you know supervision right you're more on your own and you have to deal with an uncertainty of not having constant feedback. Okay so we've solved our problems uh we used human I mean we actually paid human beings for label data which is not maybe that goes against our principles here but but we still did it because open AI is Rich so they paid people to label things. so to summarize what's the big fuss well just predict the next word based on previous words that's basically it. Transformers allows us to leverage more data and train quickly because we can paralyze paralyze all these steps in during training and uh then uh when we've done this we have a really really sophisticated model. We spent 99% of our time in computer on on this preaching this Transformer to predict thenext word basedon previous words then we can adjust things to make it a little bit more nice to interact with if you're a human being. and self Suess learning care more about both aspects somehow. Next time we will talk about uh we'll do a similar Deep dive into stable diffusion there will be self supervised learning and Foundation mod an AI but I think it's going to be slightly more conceptually interesting um so should be a lot of fun and yes please go to the website for more information Etc and if you have any questions feel free yeah can I something can I assume that probability on that after there changes based on the subject of the totally yes yes yes great question okay. it's going to be able to collapse and create distribution that's much more targeted uh so is if it doesn't have data run a context or about you and your interest as a person it it won't be can to tailor to you right they cannot create Magic out of thin air it can only do the best of The Prompt and knowledge has so far. Al that's what's so fascinating with self suus learning because now it's the building block that makes all other Technologies in AI actually fruitful. The Transformer inspired by research about how we our kids learn right how the brain works. word mask it and try to predict it based on the surrounding words um yeah the answer to that is we used to do that way and predict that and it works better uh but due to engineering you can Bas basically kind Transformer you can maybe this can be like a homework for you but if you look at Transformer Works uh if you mask a word then you will like then you can only um okay I if you do this you know if only predict the last word based on previous words. each Target by itself but if you mask then you maybe mask like 15% of the words and then it's only like that's only 50% then in terms of getting that feedback uh so it just in end of the day it means that uh when you try this empirically doing it aut agressively and this trick to be able to use the each you know each word is a target itself just leads to better performance at this task of generating export based on previous words uh so yeah so it's I guess it's engineering empirical okay. you like ask a factual question and they give us something that's wrong and they're confident about it maybe that's bad so like I think what we're starting to see is that they um are very humanik even it's in its mistake they're like well they're biased and they have stereotypes around things uh they also like we do I mean they suffer from wishful thinking and some type of imagination where they rather be you know make you happy than being completely truthful uh so these are things that you then somehow have to balance uh yeah and then so that's like some problems immediately arrived. Video is a next level in terms of compute what it needs because they have tons of videos a frame is very expensive because it's a high dimensional picture or image but clearly you can learn a lot about the World by looking at videos right. You can even sort to understand how human beings work even better because you can see people being upset or sad or happy whatever right in in a video and start picking these cues up and you can connect the vision part to the text part and get a multimodality model that's able to do both.

ROUGE-1: 32.65, ROUGE-2: 31.68, ROUGE-L: 31.54
BERTScore: 67.18

==============================================
==================== [100/100] ====================
Summary:
The EM algorithm is an unsupervised version of k-means GMM. In GMM, you guess randomly an assignment of every point to the cluster, the probability. You guess where they're probabilistically linked, that is, what's the probability of these points belong to cluster one, this point belongs to cluster two. And then once you have that, you then solve some estimation problem that looks like a traditional supervised learning. The decomposition is quite important. And we're going to try and kind of abstract that away. This is convexity and Jensen. So this is a classical inequality. And what I want to show you is that Jensen's inequality really is like conveXity in another guise, OK? And it's a key result, so i want to go through it. And then we almost certainly will not have time for this today, but I combine the notes, and we'll go continue to gothrough them on Wednesday. We'll go through what we call factor analysis, OK?" And factor analysis is another model. The reason I want it to be different is it's different than GMMs, so it occupies a different space, and it will kind of force you to look at the kind of decisions you're making, right? What are you modeling here? A set is convex if for any a and b element of omega, the line between them is in omega, OK? So what does that mean? So let's draw the picture first, and we'll draw the math. Here's the convex set. Now, no matter how I pick the points-- and clearly, I should really only have to worry about picking on the edge. So if I pick a point here and I picked b here, yep, the straight line is in the set, but that doesn't prove it's convex. If f is twice differentiable and, for all x, f double prime of x is greater than 0, then f is convex, OK? So this says these functions really are bowl-shaped, right? Second derivative being positive means that they have this kind of positive curvature that looks like the U's. Their first dimension-- first derivative goes up and down, but they're kind of always trending. That first derivative is always getting more positive. That's what it means by bowl- shaped. out a Taylor series for this. f double prime-- see the a, a minus z square. OK? And this a to a is just something in a, b. So maybe you remember this from your Taylor series. All I'm saying is I can write f of a at some point f of z plus some first derivative information. And then I'm using the second derivative remainder. This isn't super important for your conceptual understanding, by the way. Like, this is just to show that you can do what you want to do here, that this makes sense to you. Jensen's inequality says that the expected value of f of x is greater than f of theexpected value of x so long as f is convex. This inequality holds, and that's going to allow me to build a lower bound for my function, and I'm going to hillclimb using it. We actually don't want to use a convex function here because we're maximizing likelihood, and this is just notational pain, right? Like, if we were-- maybe we should have minimized unlikelihood, but this is where we are. conceptualize it is we solve for some hidden parameter. We solve, and that gives us an entire family of possible solutions. Let me draw the picture after I give you the formal set, OK? Oops. So EM algorithm has max likelihood-- I'll actually put MLE. All right, now, we're working with latent variable models. P(x; theta), says the function factors this way. Looks like a sum over z, where z is our hidden or latent variable. The algorithm is called GM, which is a supervised supervised learning algorithm. It works by taking an initial guess and mapping it to an easy-to-optimize function. The algorithm is then used to create a new curve, Lt of theta, based on that point. Jensen: "It's kind of a surrogate that I'm not overestimating my progress, and it's tight. So if I did happen to have the actual optimal value, it would meet at that point" Jensen's inequality can be used to turn a discrete function into a lower bound that works no matter how you pick it up. The key holds for any Q satisfying star, OK? No matter how I pick the probability distribution, this chain of reasoning goes through. So we have this character-- copy-- in here. This can also be written as an expected value, where z is distributed like Q of this weird-looking quantity. This is just symbol pushing. There is nothing deep going on. But it's important symbol pushing because it means Jensen's applies. Each data point is going to get its own different Q, which is the log of how likely this thing is, OK? And we picked those for each i. So as long as this term is a constant-- that is, it doesn't depend on z-- I'm in business, all right? So what that means is I want to pick Q such that log P of x, z; theta over Q(z) equals C. So what we've defined here is called the Evidence-based Lower BOund, or the ELBO. sequence that is monotonically increasing or nondecreasing, OK? So it's possible that it would grind to a halt. But eventually, it has to be strict. And so to derive a counterexample, you would just find a likelihood function that had those two bumps. And what it will do is it will gradually hillclimb. And this is actually not great. Like, it can't go back downhill, right? It's got to just continue to go up. If it gets locked inside one of those bumps, it's kind of toast. entire curve because we wanted to optimize it. So it wasn't enough to find a point in a lower bound. We needed to find the whole thing that was underneath it so we could run our argmax step. It's a lot of notation because we're abstracting out a huge number of things that we're doing. But in the end, it's not so bad, right? You take the Q(i)'s, set the thetas, do a descent on them, or ascent in this case. see if the loss of the likelihood function is not changing too much. What does too much depend on? Depends on your data, depends on the problem. Sometimes if you have a huge amount of data, and you're averaging over billions of examples, you want to get to machine precision and 10 to the minus 16. And so that's the way you decide when to do it. This just says that it's not going to oscillate wildly. It's a very weak statement I'm making. sense? Yeah, yeah. Thank you for the question. Why is this tight? Which one? Oh, it's tight because we went through this small piece here, which was that if we selected it as a constant in this particular way, then actually this line was no longer an inequality but was actually exact equality. And it depended, though-- that selection of Q depends on theta and x. [INAUDIBLE] Awesome. Great question. Yeah, and that's just making sure that the picture in your head is exactly right. EM is very general. You can instantiate it, right? So what does it mean here? Now, what actually happened here when we wanted to understand-- what was the probability? This says, the probability that i, the i-th component, belongs in j given what we've observed about x(i) and what we know about the cluster shapes and their frequencies. Now, remember, if we just looked at this, and these two distributions were-- or the phis were equal-- then we would say, oh, it's probably much more likely it belongs to this function, cluster 1. phi 2 was hugely bigger than phi 1, right-- a billion points came from the second source, and only one point came from first source. We'd probably say that it's more likely that it would go to this, right? It would certainly boost its probability. So to automate this, this is Bayes' rule. It just weighs those two probabilities and tells us what should happen. That's it. We ran through exactly those calculations last time. All right, let's take a look at the M-step now. P z(i) j-- to compute that, remember, we expanded it by Bayes' rule. We had, if you knew you were in a cluster, how likely is the data point? And then we had a term that said, howlikely is the cluster? And those were the two functions that we put in and broke down. It's exactly the same. You've got it perfectly. Yeah. All right, so let me write out this monstrosity just because it will be potentially-- it has been in the past educational. Who knows if it's educational in the future? equal to the sum over j-- because now I'm summing over the cluster centers, right? The z(i) notation was still very abstract-- wj(i), which was summing. over this part here, log-- and help us all, 1 over 2 pi-- this is a covariance, 1/2. This is the exp of Oh, I decided to write this in four general things. OK, I see why. Transpose sigma inverse x( i) mu j times phi j. All right, let me scoot. Sigma j is full rank. Because it's full rank, we can pull it out and it's linear, and it doesn't change anything. We get here sigma j inverse times sum, which is an unfortunate collision. i equals 1 to n w(i)j x(i), minus mu j equals 0, OK? But then because this is full. rank, the only way that this thing is 0 is if it's identically 0, right? If this were non-full rank-- sorry, the j is in the wrong spot. In calculus, phi j is constrained, and I just want to remind you of something that you probably learned in high school or freshman year in calculus. If you haven't seen this before, this will trip you up in some way. What you do is you introduce this thing called Lagrange multipliers. And it says, if you're going off in a direction that would not change any of their values, that's OK. And I'll just post a one-page write-up for you. the same j's. No, but [INAUDIBLE]. Which line? Just says phi j i. I see. I was talking while I was saying. Thank you for the clarification. Apologies for that. Yes, it's this constraint here. Sorry, this is the constraint that was in our head. Yeah, and it just makes a mysterious reappearance here, all right? All right, awesome. OK, so what is the message that I want you to take away from this? Two things. First message is GMM is an EM algorithm. MLE for the entire quarter on those properties. Then we introduced a ton of typos to keep you on your toes. In the next class, as I said, what we're going to see is this notion of factor analysis. And that is going to tell us how to apply EM to a different kind of setting, which, at first glance, will look kind of impossible to do without a latent variable model. And I think that's all I want to say. Thanks so much for your time and attention.

ROUGE-1: 23.26, ROUGE-2: 22.24, ROUGE-L: 22.31
BERTScore: 63.96

==============================================
