James W. SWAN: I hope everybody saw the correction to a typo in homework 1 that was posted on Stellar last night and sent out to you. The TAs gave a hint that would have let you solve the problem as written. But that's more difficult than what we had intended for you guys. So maybe you'll see the distinction between those things and understand why one version of the problem is much easier than another. So we've got two lectures left discussing linear algebra before we move on to other topics. because you didn't do pivoting-- you'd like to do pivoted in order to minimize the numerical error. Or you need to reorder inorder to minimize fill-in. As an example, I've solved a research problem where there was something like 40 million equations and unknowns. If you reorder those equations, then you can solve via Gaussian elimination pretty readily. But if you don't, well-- my PC had-- I don't know-- like, 192 gigabytes of RAM. The elimination on that matrix will fill the memory of that PC up in 20 minutes. And you'll be stuck. model I showed you last time works. If the chip is in a particular cell, then at the next level, there's a 50/50 chance that I'll go to the left or I'll going to the right. So the probability that I'm in a certain cell at level i is this Pi plus one. And there's some sparse matrix A which spreads that probability out. It splits it into my neighbors 50/ 50. And we'll see the simulation that tells us how probable it is to find the Plinko chip. Yes? Eigenvectors of a matrix are special vectors that are stretched on multiplication by the matrix. They're transformed. But they're only transformed into a stretched form of whatever they were before. For a real N-by-N matrix, there will be eigenvector and eigenvalues, which are the amount of stretch. Finding eigenvector-eigenvalue pairs involves solving N equations. We don't know how to solve non-linear equations yet. So we're kind of-- might seem like we're in a rough spot, but I'll show you that we're not. Eigenvalues and eigenvectors seem like special sorts of solutions associated with a matrix. If we understood them, then we can do a transformation. Here are some examples of the diagonal elements of a diagonal matrix. See if you can work out the eigenvalues of that matrix. Anyone want to guess what they are? Try to make sure you can do it on your own. If you couldn't do this, that's OK, but you should try to make your own practice on this. James Swan: The eigenvalues of a rate matrix are going to tell us something about how different rate processes evolve in time. Eigenvalues could be real or complex, just like the roots of a real-valued polynomial, he says. The determinant of a matrix is the product of the eigen values, Swan says. He says they tell us the rate at which different transformations between materials occur in a reaction. The characteristic polynomorphism of the rate matrix looks like this, Swanson says. Eigenvalues can be interpreted in terms of physical processes. This quadratic solution here has some eigenvalue. It's not unique, right? It's got some constant out in front of it. So add the first row or subtract the second row. And then we'll compare. This will just be a quick test of understanding. Are you guys able to do this? Sort, maybe, maybe an answer, or an answer for the eigen value. Is that too fast? Are you OK? No. James W. Swan: Try this example out. See if you can work through the details of it. I think it's useful to be able to do these sorts of things quickly. Here's a matrix. It's not a very good matrix. But it's all 0's. So what are its eigenvalues? It's just 0, right? And they're 0. That eigenvalue has algebraic multiplicity 2. Can you give me the eigenvectors of this matrix? Knowing what those eigenvectors are requires solving systems of equations, anyway. But in principle, I can do this sort of transformation to do. We haven't talked about how it's done in the computer. These are ways you could do it by hand. There's an alternative way of doing it that's beyond the scope of this class called-- it's called the Lanczos algorithm. It's what's referred to as a Krylov subspace method, that sort of iterative method where you take products of your matrix with certain vectors. There are many times when there's not a complete set of eigenvectors. And then the matrix can't be diagonalized in this way. So there's an almost diagonal form that you can transform into called the Jordan normal form. There are other transformations that one can do, like called, for example, Schur decomposition, which is a transformation into an upper triangular form for this matrix. And we'll talk next time about the singular value decomposition. All right. Have a great weekend. See you on Monday.