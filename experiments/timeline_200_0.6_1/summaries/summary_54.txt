foreign I'm really excited especially for this lecture which is a very special lecture on robust and trustworthy deep learning by one of our sponsors of this amazing course themus AI. Theyus AI is a startup actually locally based here in Cambridge our mission is to design advance and deploy the future of AI and trustworthy AI specifically. I'm especially excited about today's lecture because I co-founded Themis right here at MIT right here in this very building in fact this all stemmed from really the incredible scientific innovation and advances that we created right here. Sadhana sadhana is a machine learning scientist at Themis Ai. She's also the lead TA of this course intro to deep learning at MIT. Her research focuses specifically on how we can build very modular and flexible methods for AI and building what we call a safe and trustworthy Ai. Sadhana will be teaching us more about specifically the bias and the uncertainty of AI algorithms which are really two key or critical components towards achieving this Mission. She'll be talking to you all about robust and trustworthy deep learning on. behalf of Themis so over the past decade we've seen some tremendous growth in artificial intelligence across safety critical domains in the Spheres of autonomy and Robotics. We now have models that can make critical decisions about things like self-driving at a second's notice and these are Paving the way for fully autonomous vehicles and robots. That's not where this stops in theSpheres of medicine and Healthcare robots are now equipped to conduct life-saving surgery. We have algorithms that generate predictions for critical drugs that may cure diseases that we previously thought were incurable. rooms is this these are some headlines about the failures of AI from the last few years alone in addition to these incredible advances we've also seen catastrophic failures. Every single one of the safety critical domains I just mentioned these problems range from crashing autonomous vehicles to healthcare algorithms that don't actually work for everyone even though they're deployed out in the real world. At a first glance this seems really demoralizing if these are all of the things wrong with artificial intelligence how are we ever going to achieve that vision of having our AI integrated into the fabric of our daily lives. A lot of the problems in modern day AI are the result of a combination of unmitigated bias and uncertainty. In this lecture we're going to focus on investigating the root causes of all of these problems these two big challenges to robust deep learning. We'll also talk about solutions for them that can improve the robustness and safety of these algorithms for everyone. Themis is innovating in these areas in order to bring new algorithms in this space to Industries around the world and we'll talk about the ramifications for this for real world AI. a lot of clinical data sets where they often contain fewer examples of diseased patients than healthy patients because it's much easier to acquire data for healthy patients than their disease counterparts. We also have selection bias at the data portion of the AI lifecycle think about Apple's series voice recognition algorithm this model is trained largely on Flawless American English but it's deployed across the real world to be able to recognize voices with accents from all over the world. These biases can be propagated towards models training Cycles themselves which is what we'll focus on in the second half of this lecture. I have a model that I trained on the past 20 years of data and then I deploy it into the real world in 2023 this model will probably do fine because the data input distribution is quite similar to data in the training distribution. What would happen to this model in 2033 it's it probably would not work as well because the distribution that the data is coming from would shift significantly across this decade. If we don't continue to update our models with this input stream of data we're going to have Obsolete and incorrect predictions. Commercial facial detection systems are everywhere you actually played around with some of them in lab two when you trained your vae on a facial detection data set. The biases in these models were only uncovered once an independent study actually constructed a data set that is specifically designed to uncover these sorts of biases by balancing across race and gender. There are other ways that data sets can be biased that we haven't yet talked about so so far we've assumed a pretty key assumption in our data set which is that the number of faces in ourData is the exact same as theNumber of non-faces in our Data set. accurate class boundary between the two classes so how can we mitigate this this is a really big problem and it's very common across a lot of different types of machine learning tasks and data sets. The first way that we can try to mitigate class imbalance is using sample re-weighting which is when instead of uniformly sampling from our data set we instead sample at a rate that is inversely proportional to the incidence of a class in the data set. The second way we can mitigate class and balance is through loss re-weightsing. to dbias a model so what we want is a way to learn the features of this data set and then automatically determine that samples with the highest feature bias and the samples with lowest feature bias we've already learned a method of doing this in the generative modeling lecture. Now we'll walk through step by step a de-biasing algorithm that automatically uses the latent features learned by a variational autoencoder to under sample and oversample from regions in our data set um before I start I want to point out that this debiasing model is actually the foundation of themis's work this work comes out of a paper that we published a few years ago. good representation of what a face actually is so now that we have our latent structure we can use it to calculate a distribution of the inputs across every latent variable and we can estimate a probability distribution depending on that's based on the features of every item in this data set. This allows us to train in a fair and unbiased manner to dig in a little bit more into the math behind how this resampling works this approach basically approximates the latent space via a joint histogram over the individual latent variables. data point x will be based on the latent space of X such that it is the inverse of the joint approximated distribution. As Alpha increases this probability will tend to the uniform distribution and if Alpha decreases we tend to de-bias more strongly. This gives us the final weight of the sample in our data set that we can calculate on the Fly and use it to adaptively resample while training. Once we apply these this debiasing we have pretty remarkable results. Keep this algorithm in mind because you're going to need it for the lab 3 competition which I'll talk more about towards the end of this. lecture so so far we've been focusing mainly on facial recognition systems and a couple of other systems as canonical examples of bias however bias is actually far more widespread in machine learning consider the example of autonomous driving many data sets are comprised mainly of cars driving down straight and sunny roads in really good weather conditions with very high visibility. In some specific cases you're going to face adverse weather bad um bad visibility near Collision scenarios and these are actually the samples that are the most important for the model to learn. an extremely famous paper a couple years ago showed that if you put terms that imply female or women into a large language model powered job search engine you're going to get roles such as artists or things in the humanities. If you help input similar things but of the male counterpart you'll end up with roles for scientists and engineers so this type of bias also occurs regardless of the task at hand for a specific model. Finally let's talk about Healthcare recommendation algorithms these recommendation algorithms tend to amplify racial biases. uses that UL will also be developing today and for the next part of the lecture we'll focus on uncertainty or when a model does not know the answer. We'll talk about why uncertainty is important and how we can estimate it and also the applications of uncertainty estimation. To start with what is uncertainty and why is it necessary to compute let's look at the following example this is a binary classifier that is trained on images of cats and dogs for every single input it will output a probability distribution over these two classes. Uncertainty estimation is useful for scenarios like this this is an example of a Tesla car driving behind a horse-drawn buggy which are very common in some parts of the United States. The exact same problem that resulted in that video has also resulted in numerous autonomous car crashes so let's go through why something like this might have happened. There are multiple different types of uncertainty in neural networks which may cause incidents like the ones that we just saw we'll go through a simple example that illustrates the two main type of uncertainty that we'll focus on. of a regression task the input here x is some real number and we want it to Output f of x which is should be ideally X cubed so right away you might notice that there are some issues in this data set assume the red points in this image are your training samples so the boxed area of this image shows data points in our data set where we have really high noise. If we queried the model for a prediction in this part of in this region of the data set we should not really expect to see an accurate result. names to the types of uncertainty that we just talked about the blue area or the area of high data uncertainty is known as aliatoric uncertainty. The green areas of this just the green boxes that we talked about which were Model uncertainty are known as epistemic uncertainty. This cannot be learned directly from the data however it can be reduced by adding more data into our systems into these regions okay so first let's go through alliatoric uncertainty so the goal of out estimating alliatorIC uncertainty is to learn a set of variances that correspond to the input. model we have the same output size that predicts a variance for every output so the reason why we do this is that we expect that areas in our data set with high data and certainty are going to have higher variance. The crucial thing to remember here is that this variance is not constant it depends on the value of x so now that we have this model we have an extra layer attached to it in addition to predicting y hat we also predict a sigma squared how do we train this model our current loss function does not take into account variance at any point. a data set called cityscapes and the inputs are RGB images of scenes the labels are pixel wise annotations of this entire image of which label every pixel belongs to and the outputs try to mimic the labels they're also predicted pixel wise masks. Why would we expect that this data set has high natural alliatoric uncertainty and which parts of this dataSet do you think would have aliatoric uncertainty? And that's exactly what we see if you train a model to predict aliatoic uncertainty. Even if your pixels are like one row off or one column off that introduces noise into the model the model can still learn in the face of this noise. very little variance in the um the logits or the outputs that we're predicting. However if a model has never seen a specific input before or that input is very hard to learn all of these models should predict slightly different answers and the variance of them should be higher than if they were predicting a similar input so creating an ensemble of networks is quite similar it's quite simple you start out with defining the number of ensembles you want and then you fit them all on the same training data and training data. epistemic uncertainty so both of the methods we talked about just now involve sampling and sampling is expensive ensembling is very expensive but even if you have a pretty large model um having or introducing Dropout layers and calling 24 word passes might also be something that's pretty infeasible. At Themis we're dedicated to developing Innovative methods of estimating epistemic uncertainty that don't rely on things like sampling so that they're more generalizable and they're usable by more Industries and people. training The Ensemble and calling multiple ensembles on the same input we received multiple predictions and we calculated that variance. We've gone through two major challenges for robust steep learning we've talked about bias which is what happens when models are skewed by sensitive feature inputs and uncertainty which is when we can measure a level of confidence of a certain model. We'll talk about how themus uses these Concepts to build products that transform models to make them more risk aware and how we're changing the AI landscape in terms of safe and trustworthy AI. i's product called capsa which is a model agnostic framework for risk estimation so capsa is an open source Library you all will actually use it in your lab today. It transforms models so that they're risk aware so this is a typical training pipeline you've seen this many times in the course by now we have our data we have the model and it's fed into the training algorithm and we get a trained model at the end that outputs a prediction for every input. With capsa what we can do is by adding a single line into any training workflow we can turn this model into a risk-aware variant that essentially calculates biases uncertainty and label noise. then further analyze and so this is the one line that I've been talking about um after you build your model you can just create a wrapper or you can call a wrapper that capsa has a an extensive library of. Capsa wraps models for every uncertainty metric that we want to estimate we can apply and create the minimal model modifications as necessary while preserving the initial architecture and predictive capabilities. This could be adding a new layer in the case of a variational autoencoder this could be creating and training the decoder and calculating the Reconstruction loss on the Fly. Themis is unlocking the key to deploy deep learning models safely across fields. We can now answer a lot of the questions that the headlines were raising earlier which is when should a human take control of an autonomous vehicle. What types of data are underrepresented in commercial autonomous driving pipelines. We now have educated answers to these questions due to products that Themis is developing and in spheres such as medicine and health care we can now answers questions such as when is a model uncertain about a life-threatening diagnosis. compete in the competition which the details are described in the lab but basically it's about analyzing this data set creating risk-aware models that mitigate bias and uncertainty in the specific training pipeline. At Themis our goal is to design advance and deploy a trustworthy AI across Industries and around the world. We're hiring for the upcoming summer and for full-time roles so if you're interested please send an email to careers themesai.io or apply by submitting your resume to the Deep learning resume drop and we'll see those resumes and get back to you.