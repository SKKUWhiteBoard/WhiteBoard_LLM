The last lecture of the winter term course on probablistic estimation techniques. This lecture will focus on front ends and how to determine if a constraint is likely to be a correct one. We are still interested on adding as few false positives as possible and so this this lecture will look at how to avoid to add roam constraints to the data. We will also look at some of the techniques which can deal with outliers in the data Association so if we have from constraints they are technique that are able to deal with that. is related to the front end and what my goal is for today I will introduce three kind of small front end systems on a very abstract level just giving you the idea on how they work with different sensors. In the second part of this talk today I would like to stress on what kind of conditions should such a constraint fulfill or the the metrics of the local environment fulfill in order to let's say I would say ensure to be out layer free but to reduce the probability that this is actually an outlier mesh. when the robot observes the same part of the environment and so far we always assume these constraints are given so these errors here we assume to be given and today we would like to look at the case how do actually generate those arrows. The two main components of the slam system are the back end and the front end. The back end optimize the graph and returns a new node positions back to the frontend. The front end uses this information together with the new centre information to generate new constraints. to begin by matching observations so we have different observations depending on what platform that can be whatever stereo camera or laser rangefinder or different types of sensory modalities. For every sensor of course there's a different way of obtaining those constraints and those constraints may take into account what the sensor actually sees how kind of unique is the data that the sensor generates for specific area. If you only rely on laser range data and the corridors all look exactly the same from the Rays range data point of view then we may take that differently into account. what we assumptions we make about our observations this tarz can be very hard or not that hard and what typical approaches are is if we go for laser range data is then scan matching. Other approaches use features for example we had those the Victoria Park where trees have been extracted in this case from the laser rangeData. The trends of trees and every trunk of tree was seen as being similar to each other. If we can match them well I'm gonna say okay this part of the environment I see at the moment looks very similar to the part of. the environment that I have seen so far in the past and therefore it is quite likely that we are at the same place. one feature or one landmark and the robot map those landmarks by checking for say fitting circles into the laser range data and whenever it found a good circle say that's likely to be a chunk of a tree and use it as a landmark the third class of approaches uses feature descriptors most popular ones are for example sift and surf. These are two features which you can extract from image data and which kind of describe the local surrounding of the place where this descriptor is computed. You can match surprisingly large databases of images using those descriptors. moment and that's my sensor range I can compute where are those other pulses so in this case B 1 and B 2 just two examples could be more obviously and then I can also estimate what is the uncertainty of those poses B 1 & B 2 relative to a do that by eliminating the note a from my linear system and then inverting the resulting Hessian and looking to the main diagonal blocks this gives me the uncertainty here indicated by these dashed lines. Based on this information I know I can never have an estimate of given my current pose where's b1 and b2 together with The Associated uncertainties certainty estimates. area of my scanner and say okay say if the robot was sitting standing over here that's the area it may have observed then I can simply check is the current sensor range is there an overlap between the current sends a range and the possible observation that I obtained from b1 on b2 and if I found places for which this holds then this is the case for b1. In contrast is that if I look to be - I can say okay the uncertainty of b2 extended with the visibility range of my of my sensor by no means overlaps with a so it's extremely unlikely that a can match b2. was here indicated with a which can where I could I can obtain by inverting the hessian the problem is in practice this is a pretty expensive operation. So you actually want to try to avoid inverting this larger matrix you can do an approximation what actually most system in practice do to do that more efficiently. This is a thing by they say okay we simply ignore the loop closures for the moment just for estimating the uncertainty here and you just do what is also called Dijkstra expansion so we expend propagate the uncertainties through the graph. I reach all the posts I'm interested in looking into but this does is ignores the loop closures so the uncertainty estimates are too big. You can still argue that okay uncertainty estimates I get are toobig but I can compute this extremely efficient. I may inspect a few places too much but I should get all the places which I need to inspect in order to make sure I find the course with whatever 95 percent probability. This is what is done just as a side note. What is done in practice to what inverting this matrix age there so far. a side note and what we do then we just check which areas overlap and if there's an overlap in the area between the current scan of the robot and the sense range of the Robot. At that point I say okay this may be a match this may not be amatch okay next question is how do I determine is there a matter or not. This strongly depends on the late data laser data or the on the data and here is one example of a very simplistic front end which will try to identify constraints which uses iterative closest point. match so how could we do that so for the first thing we do we estimate the uncertainty of the other poses relative to the current pose of the robot so that's what we discussed so far given behind that we take those posters which are in the air we just made just a sample poses in in in those areas so just read we draw horses so if I go back to my example I take a okay I select okay B is a good is aGood potential match so just sample random locations here close to B and what I then do from every of those sample points I apply skin matching. just see okay how well I'll do those observations align or how well does my current observation align with the local map I've built so far let's cook looking into the sum of the squared distances between those the corresponding end points those end points which I regarded as corresponding points and if this is above a certain threshold I accept the match. That's a very simplistic technique of course you can do much better but this is kind of the basis the basics of most laser-based front ends today. into account but in the end the cordilla Mallis boils down to that again you can extend to improve all those aspects here but this kind of the basic decision based on a range data are two places or given an initial guess what's the transformation of the locations from which those two scans have been taken okay where do you see problems here that this were perfectly aware would you see the failure points for this approach. If your customer and I've proposed you this solution what would be maybe your argument against that would you buy that this approach which I propose you good. ICP is a combination of the iterative closest point algorithm and the initialization so ICP depends on the initial guess and it just finds one solution and may be the right solution but as you pointed out in your example it might be the wrong solution. P is sensitive to the initial guessing and as a result of that we may end up in a local minima so in something which looks like a match it may not be the best solution. If you kind of it's an absolutely correct observation this can happens where would you identify this problem in this list of approaches. but in reality is not a match other things we may identify is how do i sample possible locations where the platform can be if this is kind of the uncertainty is a very large area I may need to sample a lot of different poses in order to do that efficiently. How do I find actually even a good initial guess and so these are typical problems that those approaches has or ICP is sensitive to the initial guess we have local minima we may have an inefficient sampling strategy. Just accepting something based on a threshold as always something which some one might dislike. showing you three different examples of systems that we have built here in Freiburg. Some of the mapping techniques we developed here have been used to at least tested on that car so this is a pioneer a two robot which has a two d-day the rangefinder sitting here and sits on a pencil unit so it moves always like this song so it's called a nodding laser and this way generates 3d data you get 3d information about the scene and then it tries to build sorry a 3d map of the environment using this technique. assume the robot is standing still when doing it skins maybe it's driving while it's taking its games driving may make it a little bit more complicated if you have not a good odometry estimate. While driving you don't observe the same part of the environment and this makes it hard to make any incremental alignment. Based on our local 3d scans we can actually build a map we can build a local 3D map of the scene this this map can be either a point cloud accumulated over multiple pulses multiple scans it can also may also be represented by a 3d grid structure. then we can do is we can take those two local maps and try to align those twoLocal maps it's typically Nanban so depending on how many scans you integrate either you could take these skins. If it's just kind of one 3d scan you would although in practice it's a number of 2d scans for matching. Sometimes it's easier to match full maps like versus individual scans this depends on the data that you have and how many ambiguities you may find your environment so if you have a local blast slightly bigger view so you really match a map against the map that may be that. have less local minima so we match those Maps and get a six degree of freedom constraint in this case XY that your patron roll so six dimensional constraint. Then we can accumulate all constraints and then do a graph optimization and obtain a local map so this also a parts of this is building 79 building 51 52 the Mensa building and of course some parts here the green area which robot hasn't seen how does it align those two scans of these are two examples of two scanned 3d scans and which have been taken from different positions. say classification or segmentation of the environment like and if I wall think that stick out and first met results against each other and only in the end it does the alignment of all points because you're less likely to end up in a local minimize so if you let's take the tree the pole and the walls and match them first you if you can separate those part of the skin reliably he typically are less Likely to end in aLocal Minima and then you get this kind of alignment so this the iterative procedure nice based on ICP which align those scans. darkest stripes these are simply small alignment errors of these individual maps they can they can see kind of small steps over here here that was also probably an alignment error which simply leads to her step which was let's say bigger than and all five centimeters in the ground and therefore it's classified as not reversible anymore. Everything is red over here but so what do you see here for example other the bikes which are parked over here and the individual trees here something it's like to have gone wrong or maybe there was some copy. It has a 3d scanner in here on top so that's a valid I'm 3D scanner which is rotating and generate 3d point cards at a very high frequency. You can use a very similar technical basically exactly the same technique in order to align different pulses of the wiegel here. If your initial estimate is better if your laser data is a high quality laser data then you get an example like this you have seen this picture already this is an example of how to do it. is a parking lot or a 3d model of a parking lots where yellow again means drivable areas and red means non drivable area and then you can actually use this this map over here in order to localize the vehicle. This was actually work of China that he built or he realized autonomous parking using this map representation which was built here for that vehicle in that parking lot well it's actually the picture of a Parking garage. It's a trajectory of the car doing the mapping process so stop it from here stop from here and build a map and this was this in this example. done with the grid for 20 by 20 centimeter grid cells and this by lining those grid cells you can actually get maps off and say this quality that's something you can expect to get with this technique. System which was flying on the prototype for a helicopter so never made it to the blimp in the end this was just a self assemble stereo camera system with two webcams assembled in a stereo setup and a small IMU there's there's a lot more to come on this project. an initial inertial measurement unit and one of the advantage of this system is it gives you also the gravity vector this quite accurately at a high frequency. If you know the gravity back door you can eliminate already two of the six dimensions from your state space. The roll angle and the pitch angle can be determined but just by knowing the gravity so you have only one angular component and XY that that you need to estimate and therefore adding this IMU to the tourist error system is is highly advantageous. I'm interested to really get so it's kind of a webcam off 2008 I would say it's about 2007 yeah these times this was actually the work of past times DITA his master thesis when he did that. These are the the piles you can see these releases of hires in our building these are the positive front of our building and the grass area so if you take the stereo camera pointed downwards and walk over the green these are images that you get and if you get if you see this image view you may already at it guess that it's maybe hard if you travel over grass for extended periods of time. Surf features provide a local description of the scene of a small noise a scene of the small part of the image. Alice is computing their computer kind of from a local window around them doing some local operations and returning typically a 128 dimensional vector which is a local descriptions of the area over there. In reality typically is in two step process first so-called key point detector is executed which tries to find could have let's say call it stable areas in the image so if you have two similar images of the same scene. Based on the position of my stereo camera I try to build a local model of the surrounding so what we want to estimate is the X Y that and three angles your roll pitch and yaw so if you have this your camera looking forward this is roll this is pitch and this is your and so as I said before by adding this IMU to the to the stereo camera and the in this case the the camera is looking downward to me of the IMU on top we know the gravity vector and so this eliminates the roll directionally. order to estimate the relative pose of the camera so in practice looked like this so this was the camera pointing downwards they say this these are images which and the red dots are positions for which those features have been computed these are my features in the map let's say this is the current image that our observe saleh bin this looks like this the transform it over here and then you can take this image and add it to you map or add a constraint between the position where this camera image has been take the current camera image and the images have been taken at previous points in time. image and you have a database of all the sort of features that you have seen so far in the past what is the first thing you do is you try to make a nearest neighbor query in the descriptor space to try to find the best matching descriptors over your map or in those areas which are in line with the credit unit in the beginning with its A's and B's which overlap. This cue can typically do that very efficiently with was a KD tree it's a data structure which allows you to search in log n time with a number of data points. the best 100 features which match my current the current feature I'm considering I made and can do that for multiple features in my current image this gives me already a pretty good idea where I might be ok. We then sort those matches according to their quality simply start from the top so we start with the best match it matches we say ok let's take the first pair and with the first other matching pair in the other image and it's compute where the camera pulses should be given the under the assumption that this pair is correct and then we take all other features that we have seen project them into the from one image from projecting to the other. is a procedure which is very very similar to Rancic it's actually a variant of good sake I think which was used here but this is just you you sample a few parameters that you need in order to compute the solution and then use all other informations to evaluate this solution. Then you try that multiple times and see how often do we find a consistent match. That's the way this works this technique is used in three different ways in this approach the first one is for for visual odometry so there is no wheel encoder on the camera. an estimate of where the relative poses are so what you can do is you canDo what's called a visual odometry so based you inspect the images and consecutive frames estimate the positions of features and then estimate the movement of the camera based on the feature that you see and the 3d location of the feature is exactly in the same way. This how we can actually generate odometry information although we don't have a physical odometer information from wheel encoders the thing technique how you can use this is actually for matching your current observations against a small part of the environment so it's good it's kind of a localization step so the robot its reach traversing. an existing part of the environment has a good estimate where it is that is what we refer to as localization and the last part which is loop clothing so given I kind of I don't know where I am some a large uncertainty and I you can use this approach to see how well do the features that I see at the moment mattress features have seen in the past and try to find an alignment for this this is a good alignment you may accept that or you may try this for a couple of consecutive frames that not just kind of one bad image screws up everything. is a stereo camera kind of on a fishing rod so that it's kind of simulating a flying platform which we didn't had at that time and these are they this is kind of this is the current camera image that you see the feature that were extracted. What you see here is a map that the system builds on the fly and that's how that looks like so walking over the green and the back of our building so today this is all butters and trees over there so it's quite a while ago right. the tiles because you get better features to match a few seconds he will return to a place where he has been before so you can see actually he approaches it's actually in reality here. There's a there's a substantial mismatch there no features that are matched at the moment. As soon as he goes back to its starting location you will see that it finds some of the features which are consistent. Some constraints will be added to that graph and even found constraints along the grass because in no way he was walking and tries to walk along the same area again. Weather system real localized is a blue one because it's moving back on the director so you can apply the optimization. Get out this trajectory with the corresponding images and feature locations that the system has observed can then overlay that with our building that was actually Google Maps image of 2007 so the quality was bad today the quality is much better. The rough estimate of this trajectory actually is similar to where he was actually walking you can actually evaluate that even better so the same experiment he can do indoor where you put in known markers in the scene or markers had known positions again. of taking the three positions of those features and doing kind of a mapping beat the textured image on those on the 3d points it was kind of one and a half by five row by ten meter where different markers were placed on the guard which were measured and this was measurement tapes or not the presely perfect high accurate measurement but let's say up to a centimeter. Then you can actually revisit the same place in this way close the loop and then estimate where are the marker positions in between the map and reality. of the Reaper this was one of those boxes was just taking the camera images and projecting it to the or mapping it to 3d point structure. We have the 3d post information only for those feature points so we don't have that for the overall image you can actually overlay that so as x61 out by 10 meters and you could actually see that the estimate so this is a ground truth information the green lines and this is the mean and uncertainty that was estimated so this scene seems to be a somewhat consistent estimate you may see a small bias it's not centered around zero. in the end God used here was exactly the same approach but only a single camera and the SONA which was measuring the depth information and so this was a blend. The tas of the blimp was always to hover on top of this location which was here marked by the book i think somewhere over here or here and so it's always trying to hover and whenever it hovered this is the hovering location someone took it and throw it away so robot was going somewhere else building a map of the scene. Tasker builds a map online and use the map in order to make navigation decisions of where it should actually go. It's an online process which requires us to built the map to update the map and always come up with an consistent estimate of the map. Tasker then generates steering commands which always guide the platform back to the desired location in this case again it should hover here at one location okay these are two problems with this we discussed or we said how can we get around them. The platform is actually a little bit tricky for the platform to always hover or people walking by and yeah and then someone again pushing the platform away. solve that so the ICP is sensitive to the initial guess so one thing you can do is try to find arrange things into Maps instead of single scans this helps or we can separate the local perceptions into some parts let's take this wall so there's obstacle that you stick out and try to match them first we are less likely to end up in a local minima but of course there's no guarantee for for doing that. If you have descriptors like feature descriptors it can actually help you to find good estimates where you can be so you don't have to try all camera polls and see if the camera poses match. talk which I Neff was more over more whatever like wait overview about how different approaches work was not going to too many details. The second part of the talk today I would like to talk about ambiguities in the environment and what are good ways for dealing with them. How can we actually even though we have environments with ambiguity build accurate maps consistent maps of the environment so they are are so or the main assumption here is not we simply ignore all n big you T's and say the environment has no ambiguity. the place a which is the current view of the robot let's say local view and a place B say it's actually place here looks pretty similar to this place over here so our a and B the same place in the ICP based matching approach would say okay let's sample some positions here and then try if you can find a match and actually we will find amatch here. This will mesh this structure here quite well for an ICPbased alignment technique but we could always argue is this the same places what's the problem. be the same place but there might be something else which looks exactly than in this place a here so it may not be a good idea to add this constraint unless we have seen all this part over here. In order to make this constraint view maybe it's better to first explore all this scene over here before we can make this data Association. This is something which is called global ambiguity or something so there may be different places where the system can be which are just which are which do not intersect with the place I'm currently considering. that you want to make sure so they're two ways you can do that the first thing is try to close your loops as early as possible so don't let the uncertainty grow so much and the area smaller it's more likely did you find a match. The other thing is you could simply cover the whole area so that the answer the ellipsis one day and once you found one where it matches then the uncertainty of all other through decrease in this helps the system but so we're not doing here the active approach of Explorer and how to explore in order to build a good map. or not so maybe a bit imprecise from what I talked before so there here is a global ambiguity which is something we don't want to have and they are they example if the uncertainty is small and I have this Metro they say okay that looks good that's kind of what's called global sufficiency so the opposite side so this is a globally sufficient match so there's no other place in the uncertainty area of that node where the system can fit in these are the things I am interested in finding exactly those it's all situation. this structure here you have the corridor and it's a kind of the part of the doors or they whatever small pillar which holds the door in torch to its Agha you can see here. There are multiple hypotheses how it can match inside and they overlap therefore it's local the other ones non-overlapping its global. This is this is our overlapping matches is global so I don't know how this a fits in here so does this guy over here fits this one this one or this one so either here here or here simply something I it's. hard for me to you to to identify and this is also called what's called the picket fence problem so good offense you seem you don't know which part of the fence matches to what you see so far it's a very very long repetitive structure. These are things where you also don't want to add a constraint the curses simply do not know is this is this locally ambiguous or not. What you could do is you could use the max mixture approach and say simply it'm a multi-modal constraint that I may add I'm either here here or here. same author I've been all since ethanol in this group who develop this approach and later on max mixtures so this would be one nice application for mac semesters but assuming we don't have max mixture we have to treat those things separately okay and what can you we can actually do two tests the first one is a global sufficiency test so we want to say there is no possible disjoint match in the uncertainty lives it means a cannot be completely somewhere else it's not possibly that a can be somewhere come at a completely different place. to white or I want to make sure that both constraints are both conditions actually hold and the approach that every Doulton and his team proposed we're saying okay we have all slam back and it gives me an estimate into the prior the current estimate of the of the graph that I have and I do a it does a post poll scan measuring very similar to what you've seen so far. Based on the scan matching we can actually do a topological grouping so pulses which are nearby which are in the same part of the environment I just grouped together it's kind of can see this is a small local map. of doing that and then it does two things the first thing is it tries it tries to find within those groups of those topologically grouped nodes. The second thing is to find consistent constrain so how many constrains I can find in there which are consistent among each other if there some say either I'm one meter to the left or I'm two meters to the right there's an indicator for this picket fence problem we can be like in the corridor it can be either here a media for word or two media for vocal three media forward but nothing in between. is to trying to identify this situation and the second thing is kind of a global ambiguity test which is much simpler which basically takes into account what's the area that I know given the uncertainty lips that I have and is there another local area of what I've seen which would fit in there and still will allow for match. With this with getting rid of those two doing these two tests we can actually eliminate a very very large number of false positive constraints so that's actually one of the state-of-the-art systems which were used in slam front-ends. found based on the sense observations and say this is it seems to be okay or this seems to. be a wrong constraint okay so this is a criterion 1a the second criterion for the for the local local ambiguity and global sufficiency so or local and ambiguity andglobal sufficiency it'll be so these two tests I have okay we would like to go through these three steps over here the first one is the topological grouping which is easy to be done so I just takes my post graph and I just take okay which poses are nearby and then I try to match all of them. good fit maybe yeah doesn't sound too bad just add them to kind of a temporary constraint list and this is shown here in red so this one can Michigan this pot this post this post is against this opposes both this post and this guy again. Some of them will be likely to be right some of them are verylikely to be wrong. The first thing we do is we want to test for local unambiguous so we take one of those groups and check okay is here do they is here the risk for picket fence problem how can we do that? are above a certain threshold I keep the other thigh removed it would optimize this map. I would get this situation over here you can see here they're kind of walls which are dublicate it so it's definitely an imprecise map of the environment. If I however find only those which are the right ones which it leads matchings over here if you get a consistent map of. the environment and based because of this structure over here. You can see this structure in this structure and he actually structure may be seen again as similar. The key trick in here is we have a large number of constraints pairwise constraints between nodes. We want to find we want to check in to how many consistent subgroups are there. If I can assign a kind of a group ID to every constraint and the goal is that among one group within one group they all consistent with each other that's would be the perfect thing. If not I'm happy if there's a local ambiguity I may say okay it's better to not let a constraint okay locally consistent matches how do we actually get those locally consistent matching that's one of the key questions here. Within a group they all agree and this is one of those situations where I can see all which is an indicator for this picket fence program okay how does it look like. Once we're the first time with it the place I just saw here in red and the second time in the robot visited the place here shown in blue and I get those matching constraints in this case so H I and it's J isn't just two hypotheses of four constraints how that can look like and the idea is to say okay in order to check which are consistent I need to check if they kind of transform the environment in the same way. these are those add edges which result from odometry or incremental scan matching if I start from this node over here I can take my little madama tree constrain to go here. I take my constraint HJ to jump into the second trajectory for the point in time when I visited the place a second time. If I have this kind of loop of constraints if they are all perfect and agree and consistent I should add up at an identity transformation if I concatenate all of them of course I'm kind of starting at pose one in the first time series. time one at the second visit go to time two of the second with it and then go back to time one in the first with it. Only if those constrains agree with each other to end together with the odometry information that was collected I will end up at an identity transformation. So the trick is now to use all those pairs of constraints and see which one end up with with an identity in which was the biggest group of consistent transformation that I can find in here you look a bit skeptical of course. and how accurate can you actually align your scans so of course Moodle oh that yeah okay so I have whatever a number of those hypotheses what I can do is I can actually build up my matrix a I J where this simply depends how consistent are the hypothesis using the hype of this I and a hypothesis J together with the odometry. So this is this is IJ so I just cannot make this walk around in my graph and say how close am I with respect to the identity. If I'm closer than you say that's pretty good if I'm far away from identity as they are something has gone wrong here. given those two constraints and these are welders which are which are sitting here so as they are high values if they say it's very likely as I'm at the identity that's a bad thing which can happen to me or I'm getting a really low score close to zero which basically means I'm far away from identity we may use just a Gaussian about how far I am away you're away from the from the identity so what you end up you have a matrix with those values in here and some values are have high. and if you don't understand what the matrix means it will be hard so every entry of this matrix IJ tell us how well do hypothesis hypothesis J agree with each other just looking to this this pair of it's just a pairwise consistency mention the small if they're small Wireless in there I mean they don't agree they are high values and Daisy but they agree that may be good again this so far hasn't helped me to identify these groups or if they identify if they are different groups just says which pairs are consistent with each each other. I have a 1 here at the field I it means that the assumption that H I is correct and if there's a zero of it if AJ is incorrect or it's not correct so I get effect every vector consists of zero. Once it is one hypothesis about the consistency of my matches if I have in there one in there it's pretty good. If all of them are zero which means it's completely incorrect it's just one so the goal is just to find this vector and then later on find a way and how can we determined what V this vector be all right. correct if there's a zero in there it's incorrect and now comes a trick we combine this indicator vector with my matrix. If let's say I and J are one the rest is zero it will directly take out the corresponding value of my matrix a for the consistency of I andJ. If they are all of them are one at the sum of the elements of the matrix if only two elements are one and all of their their are one this gives me the sums of all consistent hypothesis according to the indicator vector. I'll just get this is the corresponding element out and then I divide it simply by V transpose times V so the scalar product of this vector V which is simply the number of hypotheses which are correct according to this vector so it is just an average of course it's consists of zero and once they're the same vector so whenever both elements are one I just add plus one so that's kind of the average pairwise consistency so now given an indicator vector of V I can compute this quantity here and this gives me a score. once once in this vector and I will get a high score if I have ever two groups in there I have I get one. I get scores among the groups but not between each other so we get and I divide by again a large number of apostasy so we getting a small value so I have this function just high values for both elements and low values for bad hypotheses so what can I do again treated as an optimization problem I try to find the vector B which maximizes this fraction that's exactly what is done. this constraint that is an np-hard problem this is a corresponding densest subgraph problem. sort of find the best v actually need to try out all possible solutions which is something which for a large number of constraints simply doesn't work out therefore the ID years okay III know how to compute it but it's to computationally demanding to do that. Let's see if you're trying to find the approximation out of that and actually find a pretty good approximation for that by saying okay I simply don't treat my vector V as discreet I said I just allow continuous variables. problem I should come up with a very similar solution there's no guarantee that this is the case so it is definitely an approximation is a different problem that I solve if I go from discrete value from this credo from zero and once for binary variables to continuous variables but the assumption that I'm not too far away. ok how do I maximize this function i compute the first derivative I set the first derivatives to zero they don't want to go into the details how the derivative is obtained but it turns out solving this equation is equivalent to solving the eigenvalue problem. times V so in order to to solve this problem I simply need to solve an eigen vector eigenvalue problem. We then look into the and so the the vectors V that maximizes this equation. The original equation is kind of the maximum consistent subset or the average maximum cost is some subset. We can use the SVD that we discussed for example with the diagonalization in whatever a few months ago when we when we discussed this I don't know in which context we did that. and if I have multiple solutions for that get MA multiple. If I haveMultiple solutions for this eigenvalue problem this is simply they are multiple maxima in my in my problem. The larger the eigen values are the better the score so there's a proof that i get a perfect combination I get a couple of eigen vectors with current putting eigenvalues the larger the  eigen value the higher the score and the  eigen vector allows me to say okay which which of the constraints are switch on and off. is a low value low valueLow value is a score of 0 or 1. The first solution corresponds to 1 V 1 so the indicator of 1 V1 which gives me the score Lambda 1 which is the highest score I get this is kind of the best solution that I have and all other solutions are much worse in performance. If I have three different solutions which give me mall as the same consistency score they could four and this is exactly the identical for one of this picket-fence problem I'm working on. have different consistent subsets of constraints which might be don't agree with each other so the only thing I need to do is if I want to say is it a picket fence it's a pickets fence here. If you have more than one solution I can just say okay what the ratio between the largest eigenvalue and the second largest eigenevalue. If this is a value which is let's say 1 or between 1 and 2's again yeah this is very likely to be aPicket fence prom. that might seem a topological grouping is done in a good in a fair manner and the includes all the relevant constraints but under this assumption that's exactly what I get out here so what I do is I take compute the first eigen value in the second eigenvalue and I compare them. If the solution 1 is locally unambiguous that means there is no picket-fence problem where is the high probability of course still may be the case I made a decision but that's my assumption here. I need to discretize V 1 to 0 and once in order to find my activation it's 1 point in here typically the eigen vector is normalized between 0 and 1. I would just round the vector be born I would end up with probably having 0s everywhere but that is easily solved because I just multiply this by a scalar. I just need to find the constants constant which make which maximizes this function over here so just a kind of 1d search problem in order to do the disguise ation of course if I compute the eigen vector or eigen value problem the eigevectors are normalized and I don't need it normalized I really want toDo the discretization so what I do is I just know I'm somewhere along one vector as lies my solution. what V is it's just kind of V times a constant C and just to determine this constant so that this expression is maximized but V is already given so that's that can be easily done okay so now I solved the local ambiguity problem or at least I'm able in a situation we can say there is a picket fence problem here or not. The second question is so this is one potential match is there some area around this area a within this ellipse so they would fit a second time in there so something I haven't seen for example well I see enough white areas that able to fit in there. second time this is the case I mean say I can't be sure that this is a concern it could be a constraint but I'm not sure that it really is a constraint because there's simply an area in the uncertainty on the relevant uncertainty area which I haven't seen so far. What we would in theory need to do we need to really check this area I can see if we can fit it in here an approximation for that is just compute the. size of the ellipse on this circle and theEllipse over here. in here is larger than the largest eigenvector eigenvalue of this guy. There may be possibility to squeeze it in there just by just comparing the eigenvalues along the dominant excess of those ellipses. I can actually do that again if a very very narrow matches this may veryVery narrow corridors this approximation may lead to a failure. If I find then if I find the picket fence promising we say I just abort and say don't add a constraint if it passes the tests. do the global ambiguity test or global sufficiency test so it's a globally sufficient order they say a global ambiguity if there's number you D don't add anything and otherwise I have a loop over which I can say with a high likelihood it's globally consistent and there's no local and acuity. That's a way actually most a lot of different slam system works also the SEM system that we use in here you've seen this video already where the system we have the robot mapping our campus over here. is a picket fence problem see that there is a local and B an ambiguous situation on a global scale and if not it adds the constraints and in this way builds a map of the environment of this kind of basically the front end hears and reimplementation or an implementation of the method of ethanol snow to make this test over here. We found that as very successful techniques in order to solve the same problem and not add false positives constraints or add only a very very small number of false positive constraints okay so to conclude this talk today I know that was kind of compared the front ends to the back ends. strongly depends on the sensors that I'm using and I need to the better I can exploit the individual properties of my sensor the better it is. The approach of this global ambiguity tests and the local ambiguity tests are important things that a good front that should consider in order to avoid adding false positives constraints. This is done with a single graph partitioning approach this was kind of the techniques there the technical - Olson which uses this and yeah again so regarding the the position uncertainty of the platform the higher the uncertainty is in an area. Ambiguity or not it's kind of if the uncertainty lips of speakers need to seen all that area to make this is no that's the only place where I actually can match and about the special clustering technique for the original paper where you find all the information here's the work back in also recognizing places using spectral clustered local matches. This is exactly the approach that I presented here and actually a couple of the slides that I use in here or of the images material at least comes from a tin Olsen.