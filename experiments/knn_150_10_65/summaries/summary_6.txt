foreign and I should turn off the zoom background blur Ry options like this oh it does show up uh yeah there's like speaker notes on your screen but there's be careful because I accidentally just put something else in the first longer okay. I apologize that was kind of a rough introduction that was uh that was me making a couple of last minute edits that probably hurt more than they helped so I want to just apologize. I just think I was just too ready to go I usually uh yeah as our slides were they and put which is the product describing to replace the names. review um convolutions and and the architecture of a CNN to make this more clear and put it put it into perspective how it relates to just standard um dense neural networks I think it's fine. When you have images and a CNN you can simply just do convolutions instead of your normal matrix multiplication. If you want to decrease the size of this volume because it can get quite unwieldy you have pooling layers. There's only five convolutional layers and the next slide should have an updated drawing that's hopefully a lot easier to understand. and we also have a bias term that gets added to the output of moving each window on each location of our input we refer to it as a volume simply because it sort of looks like a cube. If you have a whole bunch of different filters you're going to end up stacking up all of the different outputs from taking each one of your different filters and running it over your input. The big the big thing I really want to get across is that you treat it just like another layer um you're just gonna stack a bunch of them. Resnets are the only architecture that you really need to take away from here which is going to be resnet we'll get to that. The motivation behind most of these architectures start off with a bunch of data a full dimension image with all the features that you have in whatever your input is. Most of these architecture one in their respective year um vgg in 2014 I think Inception net in 2015 a lot of really cool advancements. We want to be able to have our model learn higher order feature maps and by that I mean low level features and edges. like color spaces um how edges lead to other forms and things like that yeah this is basically about what like what I was talking about um there's a lot of space that we want to have and we want  to uh go through this architecture um so yeah this this is hopefully a lot easier to understand than the previous drawing and it's saying the same thing. So you have um a this is a 5x5 image with three channels um similar to what Jake is drawing here actually um this is your original image in like RGB or something it's passed in through to a convolutional layer it's Max pooled. soft Max will scale more logarithmically um and it'll give you a final like probability map um are there any questions on Alex now actually before uh we move on yeah what's upYeah so uh if you don't specify a certain type of padding valid padding is going to be applied to make sure that as you're sliding your kernel across an image uh you're left with the same dimension is there anything you want to add Jake or no that's I I should have mentioned adding two yeah. The motivation behind this is that again we want to learn low level features in earlier stages of the classifier. Vanishing gradients is a common problem as you add a bunch of layers stacked together and that the learning signal or the gradient computation becomes extremely weak the model struggles to learn. The solution is make it easier to learn at least the identity so keep information from previous stages into future computation that's the key motivation behind residuals so yeah this is what I was talking about earlier um which is some identity that you want to keep in mind. Problems that come with that that Inception and resonance um try to fix and that is adding residuals yeah uh what do you mean by branched uh yeah the classifier is at the end uh by this uh again there's there's a drawing for it but essentially you have a multi-headed yeah so you're you're taking whatever your uh input is in a certain step and applying it to the output of another step. This maintains kind of a about it's a backwards way of maintaining a residual value. residual um that's being computed. A 34 layer residual will have jumps between every two. This is a process known as bottlenecking versus if it was after every layer. Adding residuals will increase the time to convergence because you're increasing the number of backwards considering computations that you have so if you're if your bottleneck isn't as big your time to converge will be smaller. It matches Inception of D3 accuracy just by using depth and point wise convolutions and combining. event like a low dimensional projection s yeah yeah this is like probably like really important thing for today but like this idea of like why it'd be important to sort of be able to learn the identity like it's sort of a weird thing um are there any questions or comments or concerns about that yes yeah for sure right so like if you have a dent snail Network like like let's just ignore convolutions right now if you like a dense neural network trivially you have the identity Matrix which is just ones along the diagonal and it spits out the exact same thing that it took in. step in your network if you just multiply the partial derivatives of all of those steps you can find the the derivative of your loss with respect to a given parameter just with the chain rule. If all of these different things that you're multiplying are even a little bit smaller than one immediately like at a certain point at acertain number of multiplications your partial derivative uh your chain rule that you've gotten as a result of many many multiplications just gets sent straight to zero. That's just not helpful. shouldn't produce accuracy when in reality like if this is scaled it can yeah awesome group wise uh your Dimensions so yeah this is a really good point um so if your layer is a convolution um the dimension can change which is why often this is result like kind of viewed as f of x plus W of X where W is a transformation that you do on X two to make it the same Dimension exactly. To make sure your Matrix addition stays the same like like basically you have like the X number of layers and things like during the state. at the end it's very easy to overfit to the data that I have provided uh for training um so this kind of prevents that. This comes more into play also in Mobile nuts and uh the efficient that's that will be talked about as well. There any questions on the previous kind of topics all right all right that was kind of the meat of this lecture uh but mobile nuts are very cool in that you're you're using depth wise convolutions and point-wise convolutions to reduce the number of computations. connected layer passing it through relu and with a fully connected layer you can also expand this back to whatever Dimension you originally had um rescaling according to the layer output is also not as computationally intensive. I think the slides are pretty good and compressed in a very visual way uh the remainder of the piano more impressive art it's actually the other way. I I hope the main takeaways are that you'll like understand those rules and that you you see that we've like adding all of these different sort of like tools to your tool belt now. so these are some things that these models wanted to optimize over time accuracy performance and model size um model size is something that has a trade-off if you get too big you lose out on other metrics like accuracy. performance is something directly corresponds to depthwise convolutions and mobile nuts for Edge Computing and things like that. You want to drastically reduce the number of computations that you want to do yep that is basically everything for today thank you guys for coming oh and there will also be a quiz.