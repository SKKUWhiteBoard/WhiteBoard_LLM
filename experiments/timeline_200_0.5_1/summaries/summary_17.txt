Machine learning is about how to acquire a model and acquire the parameters of a model, from data and experience. In the next few lectures, we're going to work through a sequence of different takes on machine learning that are going to highlight different subsets of the big ideas on this topic. We'll see today exactly how data kind of goes through the mill and gets turned into a model. And we'll see more in-depth examples and more structured examples of these kinds of problems later on when we talk about applications. The boundary between what is actually spam, unsolicited commercial email, and emails you just don't want can be a fuzzy boundary. Some people just click on an email they wish people hadn't sent them, even if it's like from their mom. Machine learning is going to do some amount of work, but something has to power this. There has to be something about those first emails that's going to give you the clues that something's fishy here. And so, what kinds of features can you include? Defining these features is a big part of deploying machine learning systems. Machine learning can be used to make predictions about spam or ham. The training set for machine learning is very noisy, so it can be hard to label. Machine learning is not perfect, and some inputs are just really, really hard, and they're going to look like this and we're just not all even going to agree on what that's supposed to be. The goal is to be able to predict labels of new images that are not the ones we've already seen, OK? So that's actually subtle, but it's super-important. representations that if the thing gets tilted or it's a little bit lighter. It's not the exact pixels being on that we care about. But the pixels are something we could use. We could look at how many connected components of ink there are. What's the aspect ratio? How many loops are there? It's increasingly the case, especially for problems like this, that we feed in low-level features like pixels, and higher level features like edges. We'll talk about that in a couple weeks when we talk about narrow nets. some account activity and you want to red flag accounts that are suspicious. Automatic essay grading, auto grading, this can be a machine learning problem. Customer service email routing. You'd like to automate the routing of that. Review sentiment. Here's a bunch of reviews of my product. Which ones are good and which ones are bad? Have they gotten better in the past 10 days since the new announcement? And so on. You can do that with classification. You gotta do that before you can do things like translation. In model-based classification, rather than directly learning from errors that you make in the world from experience, think like reinforcement learning model-free. The Naive Bayes assumption is an extremely radical assumption as far as probabilistic models go, but for classification, it turns out to be really effective and it goes something like this. You build a model where the output label and the input features are random variables. There's going to be some connections between them, and maybe some other variables too that might help you build a better model. 1 to 0 is equally likely. If you're looking at lots of round numbers, maybe it's 0. You can think about why that might be. So these come from data. And this actually underscores the point that depending on the data, depending on where you are, it depends on what you are looking at. And that's what we're trying to figure out here in this article. We want to know what you think is most common in real data. Or are they all equally common? So 0 might be common. How you collect the data, it can actually shape the distributions that you are imagining are going to exist at test time. In addition to the prior probability of each label, we can compute things like, what is the probability that pixel 3 comma 1 is on, given each class? This isn't a distribution over on or off. These are just the probability of that pixel for each class. And it's going to be some number. So for example, the pixel in that position might be pretty likely for the number six, but pretty rare for thenumber one. be weighed, and that's what's going on here in the conditional model. It's actually very common when you're multiplying probabilities to just add log probabilities instead. In the end, when you want to turn it into probabilities, you do need to sum them. And summing the logs won't do that. You need to do a sort of log sum, which one way to do that is to convert them back to probabilities by taking exponentials. That's actually not the way you would do it. You would sort of shift them by their minimum or their maximum as appropriate. word depends on the class and also the previous word. In general, it will be a little more accurate, but at a cost of having significantly more complicated conditional probabilities to estimate. How much more accurate will depend on to what degree the bag of words assumption is dangerous. If you're looking for a class which is not kind of strongly connected to the actual ordering, Naive Bayes is really good. Otherwise, you add other correlations like this to fix it. OK? Other questions? Good questions. Machine learning theory is based on trying to say something precise about the connection between what's going on in your data and this future used to which you're going to put the classifier to. The main worry is that in picking the parameters of your model, you do a really good job of capturing that training data, but it doesn't generalize. This is like you download all the exams from past years, and you optimize. You learn all those answers, and then you go to the final exam and it's totally, totally different questions that look nothing like those. You go through an experimentation cycle, it looks something like this. Get your model and learning already, and then you're going to learn the parameters. parameters are things like what's the probability of pixel 73 for the number eight? Then there's hyper-parameters, like, do I want to have features for the lowercased version of the words in case I've seen the word, but never uppercased? Right? These are questions about, is this or this orthis going to work better? You always know you're training data. The question is, do you generalize? This can happen to your classifier too, so you always want to test your performance on data that was not used to train it. And there can be a slow leak of your test data into your training data if you're not careful. So you try not to peek at the test set, and that's another reason why I say don't test your classifiers on the test data.to see that today. we have held-out data, which gives you something you can peek at. I ran 20 experiments. How did they go? Am I doing well? Is this thing good enough to release? You need to have some metric, and there's a lot of possible metrics. An easy one is accuracy. For how many of these emails did I make the correct decision? Fraction of instances predicted correctly, but actually, that's actually not a great metric for spam detection. Any ideas why? What's wrong with accuracy? cost-- of different kinds of mistakes may not be the same. And so accuracy isn't always what you want. What you really want, is you want a utility here. You want to know what was my utility, and you should have different costs for these things. There are also cases like machine translation, where you're always going to be a little bit off, a little word here or there, but there's a difference between being completely off and a tiny bit off. And again, we're going talk a lot today and next time about over-fitting and generalization. Spam detection is, in some ways, a very poor example of a canonical classification problem. The problem here is not that you're test accuracy is low, but your training accuracy was also low because you didn't learn anything. We'll investigate these things formally in a few lectures. I had a really good question during the break, which I want to answer for everybody, which is, couldn't you just defeat this Naive Bayes spam classifier by pasting the word Gary 100 times to the end of your offer to lose weight while you sleep? Spam is being generated by people who are trying to defeat spam filters. Spammers are going to double down on what's working. And so if you have features that are like, did the same email get sent to a lot of different people? What do spammers do? They're going to start modifying that email in some templated way. Now you have some feature that detects templates. Now there's sort of an arms race here. and so in that sense, in a sense, there's a spam arms race. over time, spam classification doesn't actually look like a standard classification problem because it's adversarial. OK, so in these images, you want to fit the hat right. You don't want it to be too small, because if you over-fit, you're not going to be able to generalize. Here's an example of this tradeoff. In general, we're going to do discrete classification. But for this example, let's imagine the thing we're trying to do is to fit a curve to this data. the data points of the squared distance or something. So what is my constant approximation to this? Does anybody want to hazard a guess? Let's call it five. OK, did I fit something about this data? Yeah, I felt something about the data. I fit basically it's mean. Did I capture the major trends? No. I would call this under-fitting. All right, let's try again. Let's fit a linear function. OK. It's close, right? It's a better fit than the constant function. Notice that when I went to linear function, the space of hypotheses grew. Instead of just lines, now it's like lines with slopes and intercepts. In Naive Bayes probabilistic models, over-fitting usually shows up as zeros in your probability table. For other methods, it's actually going to show up in totally other ways. We need to smooth or regularize our estimates, and we could take that polynomial and limit the degree of the polynomials. We already know one kind of over-fits to limit that, so let's just look like what it would look like to just illustrate it. you shrink a hypothesis space, you fit less. Using it too much, you under-fit. So let's take a look at the distribution of a random variable, just to sort of show why we need to do these kinds of things. We can do elicitation, right? You can ask a human. You can go to a doctor and say, hey, I'm building a classifier. What fraction of people with meningitis will present with a fever? And a doctor can give you a guess. You could also do that empirically. The maximum likelihood estimate, or relative frequency estimate, is a way to estimate the likelihood of an outcome. The more samples you draw, the more accurate your estimate will be. But in practice, you need some smoothing to prevent things like zeros in these estimates. This is actually due to a philosopher who kind of worried about things like things like how do I estimate the probability the sun will rise in the morning? Every morning it's risen, so far so far. Every morning the sun's risen. So I need some way of incorporating that into my estimate. example, I can go into my spam, and instead of computing odds ratios on the maximum likelihood-- or empirical relative frequency estimates-- I can instead do some smoothing. And suddenly things that only occurred once, they don't percolate to the top, because they haven't occurred enough to overwhelm that flat prior that I'm associating them with. So this is the top of the odds ratios for ham on the left, and favoring spam on the right. Some of these maybe make sense. Like, there it is. Free is probably in there somewhere. If you see money, that's a good sign that it's spam. In general, your model is going to make errors. The k that's going to be most accurate on my training data is zero. That's the maximum likelihood estimate. We learn our parameters from the training data. We tune them on some different data, like some held-out data, because otherwise, you'll get crazy results. And then eventually, you're going to take the best value, do some final tests, test run. We're talking a bit more about features, because it's important for when we start to get to neural nets. In general, in general, you're going to need more features. In spam classification, we found out that it wasn't enough to just look at words. For digit recognition, you do sort of more advanced things than just looking at pixels, where you look at things like edges and loops and things like that. Try to do things that are more advanced than just pixels, like looking at loops and edges and edges, and try to look at other metadata from the ecosystem as well as just words. are invariant to rotation and scale and all of that the vision folks think about. You can add these as sources of information by just adding variables into your Naive Bayes model. We'll also talk in the next few classes about ways to add these more flexibly, and also ways to induce these. All right, I'm going to stop there for today, and as you go, please come up and grab some more candy. Thank you. Back to the page you came from.