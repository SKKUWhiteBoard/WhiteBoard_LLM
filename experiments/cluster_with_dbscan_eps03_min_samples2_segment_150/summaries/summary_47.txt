that we are going to be looking for in an estimator, trying to find them to be unbiased. But we'll see that it's actually maybe not enough. So unbiasedness should not be something you lose your sleep over. Something that's slightly better is the risk, really the quadratics risk, which is expectation of-- so if I have an estimators, theta hat, I'm going to look at the expectation of theta n minus theta squared. And so for different thetas, some estimators are better than others. for sum of independent random variables, now it's time to wake up. So we have the variance of something that looks like 1 over n, the sum from i equal 1 to n of Xi. So it's of the form variance of a constant times a random variable. We would like somehow to say that this is the sum of the variances. And in general, we are not allowed to say this, but we are because my Xi's are actually independent. And that's by independence, so this is basic probability. The bias is 0 and the variance is equal to theta, 1 minus theta divided by n. The second number is better for the other guy, so I will definitely go for this guy compared to this guy. The bias is actually-- just for simplicity, I can think of it as being X1 bar, the average of itself. And I have the variance that's actually n times smaller when I use my n observations than when I don't. So this guy is gone. want this-- so there's theta that changes here, so the distribution of the interval is actually changing with theta hopefully. And theta is changing with this guy. So regardless of the value of theta. that I'm getting, I want that the probability that it contains the. theta was actually larger than 1 minus alpha. And so in particular, if it's equal, then I can put some larger than or equal to, which guarantees my asymptotic confidence level. The total variation distance between probability measures is central to probabilistic analysis. If the total variation between theta and theta prime is small, it means that for all possible A's that you give me, then P theta of A is going to be close to P theTA prime of A. The problem is that we don't know what the total variations is to something that weDon't know. The goal was to spit out a theta hat, which was close such that P thena hat was close to theta star. So here's the strategy. Just build an estimator. PhilipPE RIGOLLET: Let me write the set A star as being the set of X's such that f of X is larger than g of X. So that's the set on which the difference is going to be positive or negative. Rigollet: This, again, is equivalent to f ofX minus g ofX is positive. Everybody agrees? So this is the set I'm interested in. So now I'm going to split my integral into two parts, in A, A star. thing over there. We want to show that if I take any other A in this integral than this guy A star, it's actually got to decrease its value. So we have this function. I'm going to call this function delta. And what we have is-- so let's say this function looks like this. Now it's the difference between two densities. It doesn't have to be non-negative. But it certainly has to integrate to 0. And so now I take this thing. And the set A star is the set over which the function delta is non- negative. So that's just the definition. this constant is 0 for my purposes, or 25 if you prefer. All right. So we'll just keep going on this property next time. And we'll see how from here we can move on to-- the likelihood is actually going to come out of this formula. Thanks. Back to Mail Online home. back to the page you came from. Back from the page where you came From. Click here to go to the show page where we'll be talking about the probability of winning the lottery. PhilipPE RIGOLLET: If I were to repeat this 1,000 times, so every one of those 1,.000 times they collect 124 data points and then I'd do it again and again, then in average, the number I should get should be close to the true parameter that I'm looking for. The fluctuations that are due to the fact that I get different samples every time should somewhat vanish. The risk of this guy is what? Well, it's the expectation of x bar n minus theta squared.