This week's lecture is about machine translation. In the second half of the week, we take a break from learning more and more on neural network topics, and talk about final projects, but also some practical tips for building neural network systems. So first of all, I'm going to introduce a new task, machine translation, which is a major use case of a new architectural technique. And then there's a crucial way that's been developed to improve sequence to sequence models. And so that's what I'll talk about in the final part of the class. In the early 1950s, there started to be work on machine translation. It was hyped as the solution to the Cold War obsession of keeping tabs on what the Russians were doing. Claims were made that the computer would replace most human translators. But despite the hype it ran into deep trouble. And this idea was largely canned. In particular, there was a famous German words we've translated. And so we explore forward in the translation process. And in the process, I'll go through in more detail later when we do the neural equivalent. When do you hope to be able to achieve this speed? I our experiments go well, then perhaps within five years or so. US government report in the mid 1960s, the ALPAC report, which basically concluded this wasn't working. Work then did revive in AI at doing rule based methods of machine translation in the 90s. But when things really became alive was once you got into the mid 90s, and when they were in the period of statistical NLP. And then the idea began, can we start with just data about translation i.e. sentences and their translations, and learn a probabilistic model that can predict the translations of fresh sentences? original piece of parallel data that allowed the decoding of Egyptian hieroglyphs. In the modern world, there are fortunately for people who build natural language processing systems quite a few places, where parallel data is produced in large quantities. And we can use that to build models. So how do we do it though? All we have is these sentences. And it's not quite obvious how to build a probabilistic model out of those. Well, as before, what we want to do is introduce an extra variable, which is an alignment variable. one translations, where one French word gets translated as several English words. You can get the reverse, where you can have several French words that get translated as one English word. So mis en application is getting translated as implemented. And you can get even more complicated ones. So here we sort of have four English words being translated as two French words. But they don't really break down and translate each other well. These things don't only happen across languages. They also happen within the language when you have different ways of saying the same thing. we have is based on the translation model. We have words or phrases that are reasonably likely translations of each German word, or sometimes a German phrase. And so then inside that, making use of this data, we're going to generate the translation piece by piece kind of like we did with our neural language models. So there's a search process. But one of the possible pieces is we could translate "er" with "he", or we could start the sentence with "are" translating the second word. So we could explore various likely possibilities. In neural machine translation we are directly calculating this conditional model probability of target language sentence given source language sentence. And so at each step, as we break down the word by word generation, that we're conditioning not only on previous words of the target language, but also each time on our sourcelanguage sentence x. So this is what the picture of a LSTM encoder-decoder neural machinetranslation system really looks like. Next week, John is going to talk about transformer based networks. They're typically much deeper. But we'll leave discussing them until we get on further.  Sequence to sequence models have been an incredibly powerful, widely used work force in neural networks for NLP. Machine translation was the first big use of them, and it's sort of the canonical use. So you can do summarization. You can think of text summarization as translating a long text into a short text. But you can use them for other things that are in no way a translation whatsoever. So the encoder will encode the previous two utterances, say. And then you will use the decoder to generate a next utterance. arc, right arc, shifts like the transition system that you used for assignment 3. Feed the input sentence to the encoder and let it output the transition sequence of our dependency parser. These models have also been applied not just to natural languages, but to other kinds of languages, including music, and also programming language code. So you can train a seq2seq system, where it reads in pseudocode in natural language, and it generates out Python code. And if you have a good enough one, it can do the assignment for you. looked at so far are already deep on one dimension then unroll horizontally over many time steps. But they've been shallow in that there's just been a single layer of recurrent structure about our sentences. We can also make them deep in the other dimension by applying multiple RNNs on top of each other. And this gives us some multilayer RNN. And having a multilayers RNN allows us the network to compute more complex representations. So simply put the lower Rnns tend to compute lower level features, and the higher RNN's should compute higher level features. Stuck with it. And you have no way to undo decisions. So we'd like to be able to explore a bit more in generating our translations. And well, what could we do? Well, I sort of mentioned this before looking at the statistical empty models. Overall, what an example to see how it works. So in this case, so I can fit it on a slide. The size of our beam is just 2. And the blue numbers are the scores of the prefixes. So these are these log probabilities of a prefix.  beam search's idea is that you're going to keep some hypotheses to make it more likely that you'll find a good generation. So that's most of the algorithm. There's one more detail, which is the stopping criterion. So in greedy decoding, we usually decode until the model produces an end token. And when it produces the end token, we say we are done. In beam search decoding, different hypotheses may produce end tokens on different time steps. So this is a heuristic method. It's not guaranteed to find the highest probability decoding. But at least, it gives you more of a shot than simply doing greedy decoding. n complete hypotheses. And then we'll look through the hypotheses that we've completed and say which is the best one of those. And that's the one we'll use. OK. So at that point, we have our list of completed hypotheses and we want to select the top one with the highest score. Well, that's exactly what we've been computing. But it turns out that we might not want to use that just so naively. Because that turns out to be a kind of a systematic problem, which is not as a theorem. But in general, longer hypotheses have lower scores. In the early 2010s, the big hope that most people had. application of deep learning and natural language processing, neural machine translation was the huge big success story. In the last few years, when we've had models like GPT2 and GPT3, and other huge neural models like BERT improving web search, it's a bit more complex. But this was the first area where there was a neural network, which was hugely better than what had preceded, and was actually solving a practical problem that lots of people in the world need. Even our best multilayer LSTMs aren't that great of capturing sentence meaning. For languages that have lots of inflectional forms of nouns, verbs, and adjectives, these systems often get them wrong. There are problems of agreement and choice. Many languages don't distinguish gender. So when that gets translated into English by Google Translate is that the English language model just kicks in and applies stereotypical biases. So if you want to help solve this problem, all of you can help by using singular they in all contexts when you're putting material online. For languages for which there isn't much parallel data available, commonly the biggest place where you can get parallel data is from Bible translations. Cherokee is not a language that Google offers on Google Translate. So we can see how far we can get. But we have to be modest in our expectations because it's hard to build a very good MT system with only a fairly limited amount of data. There is a flipside, which is for you students doing the assignment. The advantage of having not too much data is that your models will train relatively quickly. source sentence more flexibly to allow us to generate a good translation. I'll finish this off by going through the actual equations for how attention works. Next time I'll go through how attention is generated in the language of the brain and how it works in the brain of the human brain. Click here to read the rest of the article and follow me on Twitter @jenniferjames1 or on Facebook @JennyJames1. Back to the page you came from.