So I think I just spend like five minutes, just briefly review on the backpropagation last time. So I didn't have time to explain this figure, which I think probably would be useful as a high level summary of what's happening. Like this is the forward path. This is how you define network and the loss function. So you start with some example x, and then you have some-- I guess, this is a matrix vector multiplication module, and you take x inner product multiplied with w and b. And then get some activation, the pre-activation. And you get some post activation. Some of the practical viewpoint of ML, like how do you relate to your model, what you have to do in this whole process, right? So like you start with the data process, and then you need to tune the model. So we're going to discuss how do we make sure your model can also generalize to unseen test examples. And we are going to talk about some of the new phenomena people have found in deep learning, which is a little bit different from the classical understanding. Training loss or sometimes, it's called training error. Sometimes, it is called training cost. So they all mean the similar type of concepts. For example, if you care about the square loss, then the training loss would just be this. And other loss could be cross-entropy loss. It could be like MLE, the maximum likelihood estimator. So this is basically, so far, what we have focused on in the last few weeks. So how do you get a training loss? So there are many ways to optimize it. In some sense, you care about two quantities. You care about the training loss and the gap. You want both of these two to be small. This one is something you can control in some sense. But this one is harder to control because you don't-- because you cannot say, I'm going to find a theta, such as l theta is small. And the point of this lecture is to discuss in what cases you can somewhat know this is not too big. So this is a typical situation of overfitting. The bias is going to be a decreasing function as the model complexity. The mechanism is that if you change the model. complexity to make it more complex, then your variance will be. bigger and bias will be smaller. And your sum of these two functions, which is a test error, will be something like this. So this is kind of the quick overview of what we're going to discuss. So any questions so far? Why is the bias [INAUDIBLE]. Why is bias this crazy? Oh, squared, I mean. Oh, this is just because it's kind of a unit thing. Some people call the bias square bias, actually, in some literature. It's just how do you choose the right unit. And sometimes, I think, I guess this quadratic. Sometimes, they are called this h star xi. Just for the sake of terminology, sometimes, they call this the ground truth. But of course, you don't know it. You want to try to recover it. So suppose you-- OK. Maybe I'll set up just really quick. So I'm going to have some training examples. And these training examples are something like yi is equals close to aquadratic function. The bias is basically like it's saying that the reason why-- I don't know exactly why people call it bias in the very first time. The thing is that you are imposing a linear structure, but the true data is not linear. So it doesn't matter how many data you see, as long you impose this, you just insist that I just believe that this thing is linear. Because this is the wrong belief about the relationship between y and x. So that's the typical situation, where you have a large bias. A fifth-degree polynomial can go up and down so many times, several times. So the higher the degree is the more times you can go. So in this case, the training error is literally 0. So I guess, this is expected. And the thing is that this is overfitting. So what's the problem here? Why it's overfitting? So why the test is not good? It's fit to the spurious patterns in the small and noise data. So this is because you don't have enough data. model tries to explain all of this small perturbations, small noise. And because it overexpressed the small noise, at loss, it kind of like didn't pay enough attention to the more important stuff. So whatever patterns you see in four data points, like you can explain it. If your model is specific to the spurious patterns, that means that if you redraw, you are going to-- you're going to learn the new spurious patterns. And you aregoing to have a different model. that you draw the same number of samples with similar ground truth-- the same ground truth and the solution. But just their randomness are different. You are using different noise. That's exactly what I'm going to talk about next. Sorry. One moment before that. OK. So basically, OK, just to summarize here, if you redraw all the examples and you find that a large variation between-- so suppose, you have a-- so you so you have -- so you call this-- to fit a fifth-degree polynomial. What happens will be that this is probably not entirely obvious-- OK. One obvious thing is that you probably wouldn't do anything like crazy as this right. So what you really will fit, like if you minimize the error on the training data with this so many training examples, then what you will get is probably something like this. Maybe there are still some small fluctuations. It's not like necessarily matching exactly the ground truth, but you have a small fluctuation. This is kind of more like a quadratic. minima [INAUDIBLE] So the question is that another possibility is that a failure mode is that you just couldn't find this degree 5 polynomial because some optimization issue. Even though there exist one, that is very good that fits the data, but you couldn'tfind it. That's probably not true for degree fifth polynomials for this one toy example. But it could be possible for some other cases, where the model does exist, butyou can find it. same distribution. From the same distribution. Yeah. So like if you collect more data from-- yeah. So if you don't know the ground truth, so how do you know that you are having a large bias? You cannot really exactly know. When you donâ€™t know ground truth. So when we don't knows the groundtruth, you cannot really exact like-- let me think. Let me think about what I can do to mitigate that. So all of these are so far are for analysis purpose. the ground truth, I think you cannot exactly compute the bias. Because the definition of the bias, actually, requires you to sample a lot of data. So typically, what you do is you say, you fit the data on the training set. And you see you're underfitting. Underfitting means you have a large training error. And that's when you start to believe that you've got a large bias. For overfitting the graph that's right behind you, the bias square [INAUDIBLE] The third one is the sum of them. This is the test error. of bias and variance first change, did you use different type of model [INAUDIBLE] So I think this figure, so this is the-- OK. You ask a very good question. For example, suppose you, for this data set, probably, the best thing is to use quadratic. Quadratic has small enough bias because it is, in principle, expressive enough to express our data. So that's probably the best solution. And if you're going to, it is cubic, then maybe the sweet spot is achieved at cubic, maybe. In the last four years, last four or five years, people have realized that if you increase your model number of parameters even more, at some point, you will see that it will be like this. This is the second descent of the test error. This phenomenon is called double descent. It's about after people have studied this very carefully, I would talk about some of the explanations. But before that, let me also give me another related phenomenon, which is also calleddouble descent, but it's data wise. if you believe in that, then you should say that OK, the test should look at this. And it should continue to decrease as you have more and more data. But it turns out that, actually, in many cases, what happens is that the test error will look like this, or increase, at some point, and it will decrease again. Sometimes, it does increase again a little bit, but often, not much. And sometimes, it just keeps decreasing. AndSometimes, it plateaus. this function. This was like this has been for a while. This phenomenon. This one, I think, is also-- actually, the paper that first systematically discussed this is like 2020. About that peak, when was that discovered? The peak? Yeah. This is discovered in the same paper, the peak. It's not monotone. The fact that there exists a peak was also discovered right, essentially. Yeah. I think at least I would say, at least, it's only until 2020 that most people start to realize this. people really care about it. Even within linear models, you can still change the complexity, just to clarify that. Most of this theoretical study, I think, are for linear models. And they are pretty precise these days. And I'm going to try to kind of roughly summarize the intuition from the study of this double descent. So I think the first thing to realize is that this peak, so you can argue what is the most exciting or surprising thing about this graph. But let's first talk about a peak, this peak in the middle. stochastic gradient descent for linear models. So the existing algorithms underperform dramatically when it's close to d. So both these two peaks are basically like this. So in some sense, if you use the norm as the complexity, actually, these peaks have large complexity. But I guess what I'm trying to say is that the number of parameters also is not necessarily the right measure for the complexity of a model. But there is no universal answer to that question, but there is a way to describe it. happens that for mathematical reasons, I think l2 norm behaves really nice. It seems to relate to a lot of fundamental properties. And actually, if you-- and you can test this hypothesis in some sense. So you can say that OK, I'm saying here the existing algorithm underperforms. But if you have a new algorithm, that's regularized, suppose you recognize the norm. Then you're going to see something like this. So here, it's just saying that at least for this case, it sounds like norm seems to be a slightly better complex measurement. There is no peak, but why there's no ascent? In many cases, you don't have ascent. And the explanation is that n is much, much bigger than d-- sorry, the number of parameters is muchbigger than d. And actually, for example, another question is when number of parameter is bigger than number of data points, sometimes, you are thinking this is the-- you have too many degree of freedom to fit all the specifics of the data set. But actually, empirically, you do work pretty well. points. So the thing is that even though it sounds like you are supposed to overfit, but actually, the norm is small. The reason is that somehow, there is some implicit regularization effect, which makes the norm small. And that's something I'm going to discuss, I think, more next time. So that's why the norm here is very big. But the normhere is small? But the reason is because your optimization algorithm has some implicit encouragement to make the normsmall, which is not used.