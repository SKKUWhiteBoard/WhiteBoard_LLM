Machine learning is about how to acquire a model and acquire the parameters of a model, from data and experience. In the next few lectures, we're going to work through a sequence of different takes on machine learning that are going to highlight different subsets of the big ideas on this topic. We'll see today exactly how data kind of goes through the mill and gets turned into a model. And we'll see more in-depth examples and more structured examples of these kinds of problems later on when we talk about applications. There's tons of classification tasks. It's probably the most widely-used application of machine learning. Classification, you're given inputs, you predict labels, or classes, y. Medical diagnosis could be classifications. Fraud detection could be, think about your credit card company. Automatic essay grading, auto grading, this can be a machine learning problem.. Review sentiment. Here's a bunch of reviews of my product. Which ones are good? which ones are bad? Have they gotten better in the past 10 days since the new announcement? And so on. In model-based classification, rather than directly learning from errors that you make in the world from experience, think like reinforcement learning model-free. The Naive Bayes assumption is an extremely radical assumption as far as probabilistic models go, but for classification, it turns out to be really effective and it goes something like this. You build a model where the output label and the input features are random variables. There's going to be some connections between them, and maybe some other variables too that might help you build a better model. word depends on the class and also the previous word. This Is a better model of language. If you started, if you did prediction in this, and you cranked out a pretend document, it would look significantly more like a real email than if you just did a bag of words. Will it be more accurate for classification? It really depends. In general, it will be a little more accurate, but at a cost of having significantly more complicated conditional probabilities to estimate. All right. Let's take a break now. We are now into the machine learning section which means we are done with the spooky Halloween time ghost-busting section. But I have candy. Machine learning theory is based on trying to say something precise about the connection between data and training. The main worry is that you over-fit. The principle of empirical risk minimization goes something like this. We would like to find the model, classifier, whatever, that does the best-- whatever the best means-- on our true test distribution. We don't actually know that true distribution. So how do you make progress? Well, what you do know is you have a training set. And you can't pick the parameters which are going to do best. over time, spam classification doesn't actually look like a standard classification problem because it's adversarial. Here's an example of this tradeoff. In general, we're going to do discrete classification. But for this example, let's imagine the thing we're trying to do is to fit a curve to this data. So it's about-- you can always fit your data more. It's about fitting your data to the point where the patterns that you are capturing are ones which generalize to test. in a corner where there's no number. This is an example of over-fitting, because this probability versus this probability, that is about the idiosyncrasies of the samples I have in my data. What do you think, in my training data for ham versus spam, things with the highest odds ratio for ham would be? These are things that are significantly more likely for ham than for spam. Words like Gary, except when I look at my data, it's actually a mess. It turns out, there are a bunch of words in this data which occur in spam once, and in ham zero. of over-fitting, where the exact details of which sample points you drew when you collected your data get captured in a way that doesn't generalize. To do better, we need to smooth, or regularize, our estimates. In Naive Bayes probabilistic models, over- fitting usually shows up as sampling variance. For other methods, it's going to show up in totally other ways. The exact mechanics of over-fits are going to vary from model to model. The maximum likelihood estimate, or relative frequency estimate, is a way to estimate the likelihood of an outcome. The more samples you draw, the more accurate your estimate will be. But in practice, you need some smoothing to prevent things like zeros in these estimates. This is actually due to a philosopher who kind of worried about things like things like how do I estimate the probability the sun will rise in the morning? Every morning it's risen, so far so far. Every morning the sun's risen. So I need some way of incorporating that into my estimate. In a real classification problem, you have to smooth if you're going to use Naive Bayes. This is the top of the odds ratios for ham on the left, and favoring spam on the right. If you see money, that's a good sign that it's spam. There are some things that indicate ham. This looks like general English text. What is going on there? Helvetica vs. Verdana. If I crank down k, I fit more, and so I now have a dial which can trade off the amount of fitting against generalization. In general, your model is going to make errors. In spam classification, we found out that it wasn't enough to just look at words, you've got to look at other sort of metadata from the ecosystem. For digit recognition, you do sort of more advanced things than just looking at pixels, where you look at things like edges and loops and things like that. You can add these as sources of information by just adding variables into your Naive Bayes model, but we'll also talk in the next few classes about ways to add these more flexibly.