homework two is out now. This weekend sessions will be having some more background on deep learning. We're also gonna be reaching, uh, releasing by the end of tomorrow, what the default projects will be for this class. Um, and those proposals will be due, um, very soon, er, in a little over a week. The assignments, [inaudible] are they limited to TensorFlow? asked the question if, if the assignment is limited toTensorFlow. I'm pretty sure that everything relies that you're using Tensor Flow. Last week, we were discussing value function approximation. Today we're gonna start to talk about other forms of value function approximations using deep neural networks. We're mostly not gonna talk so much about enormous action spaces, but we are gonna think a lot about really large state spaces. And so, this could be things like a laser range finder for our robot, which told us how far away the walls were in all 180 degree directions. And the key thing was that we don't know what the true value of a policy is. So, the two ways we talked about last time was inspired by the same. The number of data points you need tends to scale with the dimension. So, one alternative is to use sort of a really, really rich function approximator class. These actually have a lot stronger convergence results compared to linear value function approxIMators. Um, and everyone just [inaudible] name first please to stop me. Yeah. [LAUGHTER] So, can you repeat again why the exponential behavior happening? Yeah. A lot of these sort of kernel based approximators or non-parametric. Deep neural networks are very flexible representations. They can be guaranteed to be averagers which is a linear value function approximator. These approximators are guaranteed to converge compared to a lot of other ones. But they're not gonna scale very well and in practice you don't tend to see them, though there's some really cool work by my colleague, Finale Doshi-Velez, over at Harvard who's thinking about using these for things like health care applications and how do you sort of generalized from related patients. Going to push them into some function and then output something which is probably gonna be also a vector. Then you're gonna push that into another function, and throw in some more weights. I'm gonna do that a whole bunch of times, and then at the very end of that you can output some y which you could think of as being like our Q. Then, we can output that to some loss function j. These are- happen a lot in unsupervised learning like predicting whether or not something is a cat or not. your weights. When I first learned about deep neural networks, you have to do this by hand. But I think one of the major major innovations that's happened over there, you know, roughly what? Like last 5 to 8 years is that there's auto differentiation. So, that now, um, you don't have to derive all of these, uh, gradients by hand instead, you can just write down your network parameters and then you have software like TensorFlow to do all of the differentiation for you. and as usual we need a loss function at the end. Typically, we use mean squared error. You could also use log likelihood but we need something that- that we can differentiate how close we are achieving that target in order to update our weights. [NOISE] Yeah? Name first. So, this ReLU function is not differentiable, right? It's ended up being a lot more popular than sigmoid recently, though I feel like it [OVERLAPPING]. It's not differentable at one point? Yes. But I don't see how gradient [inaudible] is gonna work on the part where it's flat. The universal function approximator property is stating that that will not occur for, um, uh, deep neural network if it is sufficiently rich. Another benefit is that potentially you can use exponentially less nodes or parameters compared to using a shallow net which means not as many of those compositions to represent the same function. Then the final thing is that you can learn the parameters using stochastic gradient descent. All right. We're now gonna talk a little bit about convolutional neural networks. They're used very extensively in computer vision. In 1994, we had TD backgammon which used Deep Neural Networks. And then we had the results that were kind of happening around like 1995 to maybe like 1998, which said that, "Function approximation plus offline off policy control, plus bootstrapping can be bad, can fail to converge" So I think it sort of really changed the story of how people are perceiving using, um, this sort of complicated function approximation, meters, and RL. And so, perhaps it was natural that, like, around in like 2014, DeepMind and DeepMind combined them and had some really amazing successes with Atari. where they changed things to be more on policy or, or which we know can be much more stable. Um, ah, they are doing deep learning in, in this case, Deep-Q Learning. And so it can still be very unstable, but they're gonna do something about how they do with the frequency of updates to the networks. We'll see how it works here. Anyone else? Okay, cool. So, we'll- we'll see an example for breakout shortly, um, of what they did. Dennis and David, both had sort of a joint startup on, um, video games, a long time ago. So, their idea was, we'd really like to be able to use this type of function approximator to do Atari. They picked Atari in part.as well as in our derivative. And whether we're gonna see is, is uh, an alternative to that. Okay. Can anybody think of an example where maybe an Atari game, I don't know how many people played Atari. In this case, there's 80 joystick button positions, um, may or may not need to use all of them in particular game. And the reward can be the change in score. Now notice that that can be very helpful or may be it, depends on the game. So in some games it takes you a really, really long time to get to anywhere where your score can possibly change. In other cases, you're gonna win a reward a lot. And so that's gonna be much easier to learn what to do. approximators act. So they're, they're representing the Q function. They're going to minimize the mean squared lost by stochastic gradient descent. Uh, but we know that this can diverge with value function approximators. And what are the two of the problems for this? Well one is that, uh, there is this or the correlation between samples which means that if you have s, a, r, s prime, a prime, r prime, double prime. standard approach, just uses a data point. In the simplest way of TD Learning or Q-learning, you use that once and you throw it away. So, even though we're treating the target as a scalar, the weights will get updated the next round which means our target value changes. And so, you can sort of propagate this information and essentially the main idea, is just that we're gonna use data more than once, um, and that's often very helpful. And we'll look at that more in a minute. use in that value of S prime for several rounds. So, instead of always update- taking whatever the most recent one is, we're just gonna fix it for awhile. Because this, in general, is an approximation of the oracle of V star. What this is saying is, don't do that, keep the weights fixed that used to compute VS prime for a little while. And that just makes the target, the sort of the thing that you're trying to minimize your loss with respect to, more stable. In terms of stability, it helps because you're basically reducing the noise in your target. If you think of this as a supervised learning problem, we have an input x and output y. The challenge in RL is that our y is changing, if you make it that you're- so your y is not changing, it's much easier to fit. This is also sort of reducing how quickly you propagate information. So, you might misestimate the value of a state because you haven't updated it with new information. Do you every update the- Minus at all, or is that [inaudible] Great question. This is really just about stability and that's- that's true for the experience replay too. Experience replay is just kinda propagate information more- more effectively and, um, this is just gonna make it more stable. Uh, so these aren't sort of unique using deep neural network. I think they were just more worried about the stability with these really complicated function approximators. Of what the agent is doing. So, remember the agent's just learning from pixels here how to do this. Um, and the beginning of its learning sort of this policy. You can see it's not making- doing the right thing very much, um, and that over time as it gets more episodes it starting to learn to make better decisions. So this is really cool that sort of it could discover things that maybe are strategies that people take a little while to learn when they're first learning the game as well. In general, replay is hugely important and it just gives us a much better way to use the data. So, we've done some work, um, using a Bayesian last layer, using like Bayesian linear regression which is useful for uncertainty. Other people have just done linear regression where the idea is you- you sort of, um,. uh, deep neural network up to a certain point and then you do, uh, kind of direct linear regression to fit exactly what the weights are at the final layer. That can be much more efficient, but you still have a complicated representation. Doubled DQN is kind of like double Q learning, which we covered very briefly at the end of a couple of classes ago. The idea was that we are going to maintain two different Q networks. This is to try to separate how we pick our action versus our estimate of the value of that action to deal with this sort of maximization bias issue. The second thing is prioritized replay. So, in Mars Rover we had this really small domain, we are talking about tabular setting through just seven states. And we're talking about a policy that just always took action a1 which turned out to mostly go left. Vote if you think it matters which ones you pick, in terms of the value function you get out. Back-propagate from the information you're already [NOISE] have on step one to step two. So, if you pick backup three, so what's backup three? It is, S2, A1, 0, S1. So if you do the backup, that's, zero, plus gamma V of S prime. And this is one. So that means now you're gonna get to backup and so now your V of. S2 is gonna be equal to one. The order in which you did, do it. If you had done S3, a1, 0, S2, your S3 wouldn't have changed. Order can make a big difference. The number, of, um, updates you need to do until your value function converges to the right thing can be exponentially smaller, if you update carefully. It's very computationally, expensive or impossible in some cases to figure out exactly what that uh, that oracle ordering should be. But it does illustrate that we might wanna be careful about the order that we do it and- so, their, intuition, for this, was, let's try to prioritize a tuple for replay according to its DQN error. It could be super tempting to try to start, by like implementing Q learning directly on the ATARI. Highly encourage you to first go through, sort of the order of the assignment and like, do the linear case, make sure your Q learning is totally working. Even with the smaller games, like Pong which we're working on, um, it is enormously time consuming. It's way better to make sure that you know your Q Learning method is working, before you wait, 12 hours to see whether or not, oh it didn't learn anything on Pogge.