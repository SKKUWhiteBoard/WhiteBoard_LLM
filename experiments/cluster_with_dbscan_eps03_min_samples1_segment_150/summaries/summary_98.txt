presenting okay share your screen that's what I'm doing oh you are okay hopefully it is yeah stop sharing yeah you should be sharing my screen under your camera until I can decide if I click on slideshow this is still show my camera uh it does I guess I can minimize it do screen sharing are you recording too yeah great baseball back yeah I mean it's my first time giving my lecture so I'm as good as I can be do you want that cheers I mean I have to write something to hear me okay okay you know that it works. a kid but I have like admins responsible because I don't need a sections foreign [Music] thank you okay let's see what's going to happen what are we supposed to be in the cereal and we are both presenting our place it's not happening. homework one is going to be due next Tuesday so probably start that if you haven't already um the quiz for the week is due on Tuesday. It's almost 7 10. maybe like maybe two minutes at least yeah yeah we will get started in a few minutes. this week will probably go live tomorrow uh not quite sure yet but you'll try to get it up as soon as possible so I guess like without further Ado let's Jump Right In. I'm gonna go somewhat into detail into what representation learning is and I think this should sort of cap out the last few weeks of deep learning um and probably give you a more comprehensive understanding of what deep learning actually is doing. So I guess before we jump into deep learning let's talk a bit about shallow learning so say you have the classical machine learning problem and the way it is set up is you have some input X and you extract the features from this input. X and you pass it into some model that is going to be parametrized by some data to get an output y. Keep in mind that this sort of theta is. going to contain all of your learned parameters so if you have a neural network this would contain all. of your weights biases any other things that you might want to learn if this is say a regression model this would just contain the weights and maybe a bias term if it's if if one is there. might not be something that you can so let's say that you're working on a problem of predicting the price of a house from a house so your X can be a house you can really put that into your model right you would have to extract some information about the house. This information can be things like the number of rooms in the house uh the size of the house how old it is that can be categorical variables like does it have a pool back here or a basement any of that. Once you extract these relevant features you can get your output y or you canget a prediction Y. close to the true level as possible so just to sort of recap uh the machine learning pipeline you start with an input X you extract all the relevant features from it um and then you push those into a machine learning algorithm should get an output Y and you sort of optimize based on that so I guess now we defined this feature extractor or something that we need to we this is something thatWe need to Define right so and you might imagine that different kinds of problems will have different kind of feature extractors so if your data is arranged in say a table. row directly right or you can also maybe take a column depending on however the data is arranged in this tabular format but what if your input is something complex like it can be text audio images right how do you extract all the relevant features from such a complex input? So since this is a CV class I'm going to go over the CV example and turns out that there are special feature extractors for images so this is sort of what classical machine classical CV look like you people would come up with all of these like different kinds of feature Extractors. be an svm with learned weights for example now something you have to notice is that this feature extractor is something that you. have to program yourself this is not something that's being learned right now it's something that. you come up with yourself based on your intuition about the problem whatever you think might be the most relevant features for this problem right. If you want if you have a task that has to do with the colors in an image this sort of these features won't really do anything because this gives you edge information right. You have to define a different feature extracter does that make sense. really fast and this is also kind of a compromise solution in the sense that you are learning the weights of your model but you're still hand programming the feature extractor yourself. As of right now only the second half is automatic the learning of the weights we are still defining the features ourselves. This is sort of where deep learning comes in so deep learning says that hey we don't need to hand program feature extractors we can all we can learn those as well. We can learn the entire pipeline from feature extraction to um training and you could just need to pass in this like raw image input and it will spit out an output. of the pipeline so one example of this is you can use a something called a convolutional layer again don't worry about what a con layer is that is something that we will teach you guys next week but it is a neural network layer that can extract features from an image. This extractor has parameters that can be learned sensor of something like something like hog which is sort of a very stationary in the sense that it doesn't really change. You can learn this feature extractor and you can then pass these features into a learned algorithm so in in Post-its in both steps of this process you're learning something right. what if you combine them and that's exactly what a neural network is. Deep learning allows you to sort of automate this entire process from end to end. In a sense what you're really doing is you're learning a representation of your input right so your features are a way to represent what that input looks like because a model doesn't know what an image is a model will know what the features of that image are because that's what it's receiving right so in a sensewhat deep learning is doing is doing. it's allowing you to learn good abstract representations from the data itself and without having to manually do anything. The main idea is to sort of like relinquish all control to the model and let it learn whatever it needs to learn for whatever task it is trying to solve. A deep neural network is basically learned representations that are stacked on top of each other and these representations are going to be kind of hierarchical so if you look at the the the same data you can see the same representations. age right over here we have a neural network that whose goal is to sort of predict something about a car. As you go deeper down the network is trying to become more and more abstract. depth refines representations you start with course information like edges and you go all the way down to like find information like this mental model of a car so I know that this was a good exercise for me. I guess the thing I wants you I want you to take away from this is that depthRefines representations. lot of information does anyone have any questions about any of the any of this uh in case nudge I'm gonna pass the my country Verona who will talk about transfer learning okay yeah so now that we've sort of gone over an overview of what representation learning is we'll talk a little bit about what transfer learning is and what the benefits might be. When we train a model from scratch which we don't usually do a lot of the times uh it takes a lots of time compute and training data so to just to give you an idea of how much data is often necessary to train a pretty good model even just a few thousand examples is often not nearly enough. but luckily huge models have already been trained before so the sort of question is can we leverage them in some way and the answer is yes absolutely. Many of these pre-trained models are frequently used all the time and so we can take a look at how and why we might want to use them. If we train a model from scratch our model parameters or our weights are randomly initialized in the beginning and then we update them gradually through an optimization algorithm such as stochastic radio descent or something like atom. Without transfer learning we might have to train these two models separately from scratch. earlier is very costly in terms of time compute and data so the idea is can we do better and so when we learn something we often find in general you know just like us ourselves what we can use what we've learned already and apply them our skills and our knowledge to other domains right so just to give a little example let's say we have a convolutional neural network trained for a single or a single user. We can apply the same idea to our neural networks so as an example. simple computer vision task like a cat versus dog classification or some sort of object detection and again no worries if a convolution is sort of familiar we'll cover convolutions and CNN's in much greater depth very soon. So for example um each of these models should be able to for example capture how low-level features as Arya mentioned such as the general shape the edges the patterns and the colors in the lower layer so the more earlier layers of our neural network. Then once we've moved on and we've gone towards the later layers they should be can to capture some higher level features like the abstractions of the cats versus the dogs people's faces or some of the major objects. at the end we reach to a close to final level of the attraction that we fought and just for a slightly more concrete example let's say we train a residual neural network on the imagenet data set. We want to figure out what aspects of this network is already shared among the other ones and we want to keep that shared information and then basically use that kept information for our other models. So how might we actually go about this um and it's a little bit of an example. So in other words like how do we actually transfer or do transfer learning um does anyone have any ideas perhaps of how we might want to keep certain information from a previous pre-trained model and transfer that knowledge yeah that's basically the right idea what about you you're just copy the weights Maybe yeah so very similar um one of the more common ideas is that well basically our neural networks are just stacks of layers so the idea is can we keep certain layers and basically use that on our next model so there's an idea called freezing. so we can add and train the later layers to basically customize them in a way that sort of satisfies the tasks that we want to perform on uh for the second task. The layer weights of a trained model are not changed when they are reused in a downstream task and by freezing layers you might imagine we're not really modifying the weights or parameters and so that backward pass that we talked about earlier is what we're going to use to train the layers. So we generally want to keep those General shapes textures patterns but our abstractions might be a bit different depending on the type of tasks. in the past few lectures can be basically avoided so the speed of our model increases by a lot by doing this. When you're trying to freeze certain layers be careful of where you're freezing your your model so if you freeze layers too early on um that's basically sort of useless like this can lead to pretty inaccurate predictions because you're not really understanding the low-level edges for example. A CNN model with several several layers that's trained on a pretty large image data set like imagenet can be reused by removing the last few layers. than just the unfrozen layers um so essentially we sort ofinitial our new neural network to the pre-trained network the weights instead of initializing them at random. So again this speeds up our training process as the parameters of the neural network is taken from thepre-trained parameters. Okay so um one other question that you might have is how do wesort of decide between whether we want to freeze or fine tune um so there are a few main factors that you Might want to consider but the two main important ones are the size of your new data set. um and the similarity of the new data set to the original data set so for example in the case one where you have um a small small data set or a lot a large data set. Since it's larger we have more confidence that we won't overfit if we fine-tune. On the other hand if we have a smaller data set even though it's similar to theOriginal model it's not a good idea sometimes to fine tune because you can definitely overfit. And then in the third case in which you have like a smallsmall data set and then it's pretty different from the first task um. you can probably um fix some of the initial layers but you don't want to really fine tune and then at the very end you have a very large second data set but it's very different from the first task so you can probably just change it from scratch. In practice it's mostly beneficial toinitial your weights from the pre-trained patient model okay so um before we sort of get into embeddings um just some practical advice so just a few other things to keep in mind when you're performing transfer learning. In terms of neural networks embeddings are pretty important so they are often described as lower dimensional learns continuous Vector representations of discrete variables. Embeddings are useful because they can reduce the dimensionality of your categorical variables for example and meaningfully represent them in the transform space. In this photo example they've decided to use teasney for thedimensionality reduction so taking the embedding Vector dimensions and mapping them to a 2d space in this case um plotted they also plotted the embeddeddings and then they color coded based on the genre of the book. to embeddings to something called a latent space so we often prefer to work with lower dimensional data. High dimensional data is embedded in a lower dimensional space latent space and a lot of the time latent space is sort of interchangeable with like an embedding space. One of the reasons that high dimensional data or high dimensional spaces can be bad is because if the data is naturally a high dimensional the high dimensional space is bad for the embedding. So what we offer is what we often call the curse of dimensionality. lower dimensional structure it's going to be very sparsely spread out in your latent space as you can imagine um in high dimensional spaces so a lot of the times you want to be careful about yeah I don't know you can sort of draw something yeah but you don't need to work with the entire 3D space. This sort of goes back to the idea of of course dimensionality because you can see that all of these points occupy a very small region. This is an example that I came up with so say that you have the 3D. space but your data is along this line now yourData is basically has a one-dimensional structure right. of space in 3D right so the idea is that if we can directly work with the significant somehow find a way to represent this line using just one variable instead of three and so here's let's say the XYZ coordinates that's going to be better because like a high dimensional data can be very complex we want to avoid that as much as possible thanks okay. One way is to learn and embedding as part of our neural network for our Target task so this sort of allows us to get in a bedding that's nicely customized for our particular task but it may take longer than training the embedding separately. for other tasks again embeddings they're sort of just like what we talked about earlier the broad ideas that we're trying to um represent our data in meaningful ways. So here's sort of an example um honestly it'ssort of like I don't know we talk about we don't really quite talk a lot about soft soft Max but um you can sort of see how there's different ways like we have um one hot Target probabilities. So um I didn't do this do my run this part what's up the target Boston oh yeah um oh so this example. as a 70 784 dimensional Vector plus 25 to 784. You train this using a soft Max loss because soft Max is the loss that is used for classification. Once you train this model you can take um can you see my cursor you can once this model has been trained these three like set up neurons in the middle of the model can then be used to identify a digit in an image. It's basically because it's a loss function don't worry too much about it so the idea is that. used as an embedding for this mnist image so once this model has been trained this the weights must have learned something meaningful right so this means that we could just take some of these layers in the middle take the output of that as a representation of this 20 dimensional 28 by 28 dimensional image okay. Here's sort of an example of some of the results from training networks from scratch versus applying some sort of transfer learning um from a paper so in this example the authors compared pre-changed convolutional neural networks for audio classification using transfer learning and they found that the retrained models with transfer learning applied actually achieved better accuracy classification accuracy than retraining the network from scratch. major advantages of pre-trained networks so a lot of the time pre- trained networks are trained on very very large data sets and oftentimes again more data means better representations. Pre-compute and store our embeddings instead of using the original High dimensional data which can again save aA lot of time and Storage okay and so here's just a very very broad summary so without transfer learning we're basically trying to learn two separate tasks and train our model separately um but without transferlearning we'reBasically trying to learning two separate task and trainour model separately. then with transfer learning we basically apply the knowledge that we've learned from one pre-trained network and apply that to the second task and so we've talked about two main techniques for doing that freezing some layers and fine-tuning our our Patriot Network. So very broad once again you basically just apply your knowledge um and try to transfer that to another task instead of relearning everything. We'll go into some details and examples of this in action but especially um I know this is not an NLP course we talk more about CV but transforming learning is especially huge in NLP. of words once you've pre-trained you can understand syntax so you can imagine in tasks for natural language processing it takes a lot of time if you want to retrain an entire network so using pre- trained networks and using transfer learning is a really really good idea. Not only does that apply to NLP it applies to almost other domains every other domain xcp yeah okay so we will transition to the next part of the lecture which is going to be on self-supervised free training. notion of a loss that sort of Compares how far apart this prediction is from the ground truth label y right and your goal is to optimize the network such that this error decreases and your model is trying to output something that is very close to the actual labels y . so in a sense your training process is receiving supervision from the labels your super your labels are guiding what the model must learn and some examples of and and this whole process is called supervised learning like the name suggests. Some examples of supervised learning can be your typical classification problem like the one that we just showed where you classify digits. 16b I think you might have seen regression in those classes um there are other examples of object detection segmentation we will discuss those in the coming weeks. Even without any labels we can still learn something meaningful about the structure of the raw data how many have you guys taken 16b before so yeah you may have you might recall something called PCA from the class of principle component analysis it's actually one of the most common unsupervised learning algorithms out there. If you remember correctly you just input some data Matrix into that algorithm and it splits out those principles common inductors. any labels into that algorithm you just feed in the data Matrix. If you remember from the 16v car one thing that you did was to you you took the audio signals from the words that you would pass to the car and you would cluster them together again. There was there were no labeling involved you just took each audio signal um projected it down to two dimensions and clustered them with other points so yeah it turns out that dimensional eruption with PCA clustering etc etc are common examples of unsupervised learning. draw like decision boundaries based on that but even if you don't have any labels the model can still learn that okay these points are dripping up together they're forming clusters and this is still like meaningful information that themodel can learn. So yeah hopefully it's sort of this picture makes clear the difference between unsupervised and supervised learning so the examples that we have discussed so far when we were going about transfer learning was supervised pre-training so we take these large models that were trained on say something like imagenet and usually these models like a resnet are trained for the image net classification task. Large data sets are helpful for learning more generalizable representations. English Wikipedia has hundreds of millions of text tokens in that but you can find like a trillion tokens on the internet of text right so if you try to harness all of this information and maybe you can learn better representations, this goes beyond CV as well. Since this is classification is a supervised learning problem you have a label associated with each of theeach of the images in the data now we mentioned before that large data sets can be helpful. using that and turns out this is a pretty good idea also but it turns out that these large data sets are usually not labeled you can you could like scrap text or image or images from the web but you can't really label them automatically right so a lot of the time you're working with unlabeled data. We want to see if we could use unsupervised learning techniques to the sum level data sets and learn representations using that and this is also appealing because labeling in general is a very time consuming and tedious process. Gathering good labels is simply a very very hard process and so that the question that research has asked is if it could do unsupervised representation learning and indeed we can. So the valency provides representation learning is done is to play to something called self-supervised learning. The name suggests that the data is receiving supervision from itself you still don't have any labels with the in the data set. But what you can do is you can create labels from those images and train in a supervised manner. part or property of the input from any observed or unhidden part of the Input. This information can be hidden across time or space. We will go over some examples soon and actually we will have an entire lecture dedicated to self-supervised learning for um Envision in a few weeks so we'll double deeper in that lecture Cube so again some more terminology before before I go on to examples. In a discussion of transfer learning we have been referring to two different tasks as task one and task two which is you know not a very descriptive name. name so the task on which you train the representations you know what what we have been referring as to cast one for so long is also called a pretext task. The task that the representations are transferred down to are also known as a downstream task now different domains we have different kinds of Downstream tasks. In computer vision this can be something like you learn the representations from some pretext tasks and you'll use those representations for image classification or object detection or semantic segmentation or whatever for NLP this could be somethinglike text classification machine translation document summarization question. means but yeah just wanted to show that great training is a very broad topic and self-supervised learning algorithms can be applied to different domains. I guess we can delve into examples now so this was a paper that was published in 2017 and it was called jigsaw what the authors do is they take an image they take some part of the image and divide it into a three by three grid so you get nine patches and what they do is Shuffle the patches around and ask the model to predict the original order. anymore but it's also trying to understand what's going on in the image it's going to learn that an image can be made up of different parts and those parts are going to be related to each other. There are some more technical details on the slides um I won't go into those but something that the authors actually did I actually go mention this is when they sample the patches and they divide it into a grid instead of taking the grid directly they actually generate each patch a bit. a very easy way to like cheat this process right so which is why you might see a non-perfect written image a any questions about this task before I move on yeah so what they did is they actually take they took the representations from this pre-text task and tested it on classification and protection um Downstream tasks. It turned out that this was sort of the best pre- text task at the time and it actually and then it actually sort of bridged the gap between supervised and self-supervised learning in a sense. and I think they just took the thing that created that as like a frozen feature extractor representations that way another task is something called broadnet uh it's sort of a similar idea but instead of predicting let's say a shuffle order what you do instead is you take an image you you rotate it by some number of degrees and which is selected from zero 90 degrees 180 or 270 degrees. You ask the model to predict the rotation angle from the from this rotated image and the hope is that. branch is still going to be the same in each image so the hope is that the model can learn some of that information is that a single image might have different kinds of objects and it might have to learn to focus on something like the object's orientation location pose type etc etc. Instead of just focusing on again low level details I think this sort of next example makes it more clearer so on the left hand side you have a model that was trained in a fully supervised manner and when you look at what is focusing on on a given image it's looking at a single part at at one time. On the right hand sideyou have this model using the rotation prediction task and instead of focusing on a single parts it's looked at multiple Parts at the same time. if you look at this image on the bottom right it's looking at the eye of the cat and sort of it's not at the same time to see if there's like a relationship between those two and in the image of a dog it's. looking at both the body of the dog and the face at the. same time. It's trying to discern some sort of relationship between the two so maybe this is a way to like qualitatively show that this also provides learning algorithms are trying to learn something much more Beyond supervised training turns out that this example is not constrained to just CV you can also do SSL with NLP. this can be done is you could predict a word from its surrounding context so say if you have the sentence the dog with the man you could try to predict the word bed from dog and dog because a dog should kind of imply that the word bit is associated with it so this approach is called a continuous bag of words model. There are many other ways to train virtual back models one example is skip Ram so instead of predicting aword from a context you instead predict the context from a word so you like kind of like flip the model upside down.  SSL can also be applied to audio it's a very broad sort of paradigm and I think the currency of the art and audio classification is Wave 2 Vector Q which I think came out a few years ago okay so what if we go back to this idea of where to work we are predicting a single word from some surrounding context right. What if you predict a word from the entire sentence that it is a part of and as I think I think you might imagine that this white this might work better because the sentence will give you more context than just like those two surrounding words. you can try to predict multiple words from a sentence and what when what happens is that you call these like multiple words a masked word and your goal is to predict the masked word from the rest of the sentence. There's a very famous model in an NLP called bird which takes us to the next level uh the bird is something called a Transformer model you don't need to know what Transformers are yet we'll have a lecture on that later in the course. Verge Texan these sentences that have like Mass birds and it tries to predict what those Mass birds are. words are it actually goes a step further it actually takes in two sentences instead of one and it protects the best words for both of them and at the same time it also predicts the order of photosynthesis. So it learns a word level and a sentence level embedding now bird was a huge success in MLP and I think bird is sort of what kick-started the interest in um self-supervised learning back in CV because this this idea of SSL and CV kind of like died down a bit in 2015 2016 2017. the slide is it turns out that the current state of the art for CV is actually very similar to Burke so that's a teaser for the lecture that we discuss Advanced Techniques and as software CV. I just want to point out that this specific lecture doesn't have any homework but there is a homework for this entire cluster which is the high Crush notebook that should be due next Tuesday even though this lecture doesn’t have homework I mentioned before that there will be a lecture on on Advanced SSL for CV. and that will have a homework so if you ever need to review uh the topics from this lecture so to work on that homework this slide deck should be up on the website again if you feel free to do that. That is it for today a second pause. Back to the page you came from. The slide deck for this lecture is now available on our website again. Click through the slide deck to see the rest of the lecture. The lecture will be available again on the site later this week.