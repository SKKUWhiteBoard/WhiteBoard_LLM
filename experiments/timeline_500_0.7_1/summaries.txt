==================== [1/100] ====================
Summary:
John ESSIGMANN: I measure my blood sugar at different times during the day. Gluconeogenesis technically means new synthesis of glucose from non-carbohydrate precursors. The medicine I take is called Metformin. It has a number of targets, but one of them is one of the enzymes, called PEPCK, Pyruvate Carboxykinase, that's in the gluconeogenic pathway. The liver provides a constant stream of glucose to organs that absolutely require it, like our brain.

ROUGE-1: 28.97, ROUGE-2: 26.69, ROUGE-L: 28.97
BERTScore: 70.94

==============================================
==================== [2/100] ====================
Summary:
In this lecture, we introduce and develop the concept of independence between events. If I tell you that a certain event A has occurred, this will generally change the probability of some other event B. In such a case, we say that events A and B are independent. We will then proceed to define the independence of a collection of more than two events. Finally, we will close with an application in reliability analysis and with a nice puzzle that will serve as a word of caution about putting together probabilistic models.

ROUGE-1: 62.59, ROUGE-2: 60.96, ROUGE-L: 62.59
BERTScore: 82.29

==============================================
==================== [3/100] ====================
Summary:
In today's video i'll show you the importance of de-gassing your bread dough as it's fermenting. No matter how gentle you try to handle your dough you will always de-gas it if only a little bit in today's comparison video we'll make 4 breads they will be made from the same dough but they will all be treated differently. The first one of the four breads will be left alone from the beginning of fermentation until it's baked. The final one will be folded shaped and degassed three times and we won't be fermenting them for the same amount of time. dough is left undisturbed it will result in a crumb with large holes surrounded by denser dough that's exactly what we're seeing here on the bread that wasn't de-gassed at all. De-gassing pops those air pockets so the love that got the final shaping has a more even crumb and it rolls higher of course resulting in softer texture. As we keep going the breads become progressively softer and lighter and larger and as i said earlier we're not trying to prove what is better or worse.

ROUGE-1: 19.99, ROUGE-2: 19.52, ROUGE-L: 19.99
BERTScore: 68.96

==============================================
==================== [4/100] ====================
Summary:
Of the nearly 11,000 amendments proposed in the centuries since, only 27 have succeeded. The founders of the United States were trying to create a unified country from thirteen different colonies. For an amendment to even be proposed, it must receive a two-thirds vote of approval in both houses of Congress. To actually change the Constitution, the amendment must be ratified by three-quarters of all states. Americans today are the most politically polarized since the Civil War, making it nearly impossible to reach a broad consensus. Court justices are unelected and serve for life once appointed, this is far from the most democratic option. Interestingly, the founders themselves may have foreseen this problem early on. In a letter to James Madison, Thomas Jefferson wrote that laws should expire every 19 years rather than having to be changed or repealed. Although he believed that the basic principles of the Constitution would endure, he stressed that the Earth belongs to the living, and not to the dead. He wrote that every political process is full of obstacles that distort the will of the people.

ROUGE-1: 46.04, ROUGE-2: 43.88, ROUGE-L: 42.26
BERTScore: 64.38

==============================================
==================== [5/100] ====================
Summary:
50 years ago, John McCarthy and Marvin Minsky coined the term artificial intelligence. Progress has been made, especially in the last 20 years. I think it's a golden age for intelligent applications, if people want to make a lot of money with useful things. But this is kind of engineering. Interesting one, but engineering. And we are working on it. And so, this was the people that we put together from different labs, from neuroscience, from computer science, from cognitive science, and from a number of institutions in the US. still very far from understanding how people can answer questions about images. This is one of the main focus in the center, really. We want to have a system that does it in the same way as our brain does it. And we want to compare your model, our system, with measurements on the brain of people, or monkeys, also during the same task. So that's what we call Turing plus, plus questions. And part of the rationale about it is, this is kind of a more philosophical discussion. looking at the face. And make models of what's going on. And, of course, we want these models to respect the neural data, ideally the MRI data. And do the job of recognizing faces as well as human do. So we are getting there. I'm not saying we have the answers, but we have at least models that can be tested at all these different levels. So that's kind of the ideal situation, from the point of view of what we want to do in the center.

ROUGE-1: 36.85, ROUGE-2: 35.13, ROUGE-L: 32.77
BERTScore: 62.15

==============================================
==================== [6/100] ====================
Summary:
Chef Todd Moore shares with you the seven basic skills that I think everyone should have to cook food consistently in the kitchen and be proud of the results. The chef test highlights the skills that everyone should possess if they want to learn to cook anything at any time and be confident that it'll always come out great. If you already have all seven of these skills and cooking techniques great you can work for me on the other hand if you only have one or two ofthese skills that's still fantastic. a cook that uses a thermometer and this is a cook that passes this test not one that is obviously more cooked than the other to be well done they all look similar on the outside and they're different internal temperatures on the inside if you can Master this you are a master griller most people just burn stuff you fail this test. If you can't tell which steak is rare medium well done if you're taking them off the grill guest atam the chef that can that can't control direct to Source conductive heat would create waste in my restaurant.

ROUGE-1: 15.54, ROUGE-2: 15.01, ROUGE-L: 13.98
BERTScore: 61.38

==============================================
==================== [7/100] ====================
Summary:
as a nurse you want to be familiar with heart blocks and in this review i'm going to be talking about third degree heart blocks also known as a complete heart block. This type of heart block is the worst of all blocks and the reason is occurring is because electrical signal from the atria isn't making it to the ventricles. The person could be born with it so it could be congenital or the person has severe heart disease or they have a myocardial infarction or they're taking some type of medication. could progress to death so what you want to do is activate that emergency response system and this will get a team in the room to help you now some treatment that can be given to that patient is that atropine can be administered to help that heart pump more efficiently or the patient could be connected to a temporary pacemaker which will again get that heart beating correctly so we can maintain cardiac output. Eventually the patient will need a permanent pacemaker implanted okay so that wraps up this review over third degree heart block.

ROUGE-1: 45.71, ROUGE-2: 44.63, ROUGE-L: 45.71
BERTScore: 67.01

==============================================
==================== [8/100] ====================
Summary:
The 24th lecture of 188, before last one. The idea behind these two lectures is to look at advanced applications, where we have covered a good amount of the material in the ideas behind those applications. We will not quiz you on these application lectures, on the final exam, or anything like that. It's more meant to give you more perspective rather than extra study materials for the final. So far, I've looked at foundational methods for search, for acting adversarial environments, for learning to do new things, and for dealing with uncertainty, noisy information. DeepMind's AlphaGo can predict who will win from a certain situation in Go. The game is much harder to solve than chess or Tic-Tac-Toe. The branching factor in Go is much larger than in chess. A neural network can be trained to evaluate the value of a position. This gives you a lot of data that can be used to decide who is likely to win in a given situation. The first thing being learned is a policy network, which is deciding which moves to play against. AlphaGo Zero has no prior knowledge. It starts at a pretty, actually, negative elo rating. Loses all the time initially. It's playing itself, but then gets tested against a set of players. After 21 days, it goes past where AlphaGo Master, was which was an improved version of AlphaGo Lee Sedol. And then it was still creeping up after 40 days. Once you reach that level, essentially, there's no further to go, because you solved the game. even longer. It could be that by using human knowledge, you're in some kind of based enough attraction. I don't know if that would be the case or not, but that's a possibility. It also depends on how much randomness you have in your exploration. If you have enough randomness, then initialization will have much less effect than if you have limited randomness. How good can this system get? Is it even possible to learn good Go players by just playing against yourself? That's something people did not have an answer to until this experiment was run. The MDP has four control channels, two in each joystick. A collective is the action for the main rotor collective page. It's the average angle of attack as the blade goes through the air, which modulates how much vertical thrust you generate. The tail rotor has a variable pitch also, and that pitch allows you to modulate how much thrust you get from the tail rotor. You cannot directly ask it to fly forward or sideways. If you want to fly. forward, you've got to rotate nose-down and then you can accelerate vertically. In the RL lecture, we saw this example of a helicopter reliably hovering, which is a very hard problem. Upside down is harder. How do you keep yourself upside down? Well, that main rotor can have a negative angle of attack. If you're flying upside down, it keeps you up in the air. And it's actually more efficient because when you pull in air, you're accelerated. And vertically, the helicopter frame will now be partially forward and partially up, and up compensates for gravity. We use something called dynamic time warping to align two trajectories. Then we re-infer, through probabilistic inference, what the hidden state might be. Keep repeating this till we reach some fixed point, and that will be our target for our helicopter to fly. And then we can run reinforcement learning in simulation to find a good controller and run it on the real helicopter. The controller we learn in simulation is still a little optimistic about really following that path. We can now penalize our award, penalized for deviating from the target. able to look ahead only two seconds, rather than needing to look further. A value function tells us, OK, how good is it to end up here? We also have a reward at each time tick. The algorithm's only this big, so it's pretty fast for something of this size. The fastest we flew this helicopter was close to 55 miles per hour, so almost highway speeds. And ending an inverted hover. So with this methodology, it was possible to fly this helicopter at the level of the best human pilots. would be there, shutting it off if it started doing something weird. So essentially, he would shut it off whenever it starts tilting itself, so it lands on some pretty wide landing gear so it's more stable. But that was the only human input required. He was able to have it learn to hover reliably. We did not push that further to flying those maneuvers. There is some work. If you look at Woody, Woody was shutting things off. Then recently at OpenEye, there's been some work on robots learning to do back flips. And that was kind of one step further. gets a little more power. Maybe a question related to that is, how much power does this thing actually have? This helicopter had inverted slide, where it has more power, 3 Gs. So it can generate three times the power of gravity. Of course, you need to generate one of them to even stay up in the air, but it still had two G's left to do other things with. And regular flight had about 2.5 G's maximum acceleration. OK, let's take a short break here. And after the break,let's do legged locomotion and manipulation. In 2008, a professor and his students tried to teach a robot to walk and drive. The robot fell a lot, which is indicative of how hard it is to do walking with robots. In 2015, there was the Doppler Robotics Challenge, which was held in Pomona, just east of Los Angeles. Now, what's changed recently in the past few years is that through advances in learning, it's been possible to map from raw sensory information to controls, says Andrew Keen. autonomous car drive a desert race. It's a time trial type race. There's no other cars that you have to overtake, but you're kind of on your own and try to do it as fast as possible, 150 mils off road. Well, it's on a road, but it's this kind of road that is not like a regular road. So it's pretty hard to distinguish road from non-road, and if you steer off the road, you might lose your car if you go down some kind of ravine. Four cars finished the 150-mile Berkeley autonomous car race in 2005. What goes onto the cars? There is IMU, like right on a helicopter, a lot of computers. Lasers, where you shoot out laser beams. Cameras, radar, control screen, steering motor. How do you decide with path to follow? Often, your sensor readings will tell you if there might be obstacles or not.. A camera will be better at that than a LIDAR. Somebody needs to tell you what is road, what is not road. The devil is really in the details, in the long tail of special events that can happen when you're driving. In urban environments, there's even more need to recognize, not just road versus not road. A lot of progress has been made this is video from 2013. This was before deep neural networks were heavily used for this kind of thing. It's only getting better to recognize what's in scenes, thanks toDeep neural networks. Instead of classifying into which categories in the image, you would classify each pixel, as to what is in each pixel. That way, you get a semantic segmentation. is between 10 and negative 2 and 10 negative 3 per 1,000 miles of human driving. In green is the Google slash [? wave ?] mode disengagement. It's when the driver decides they want to take control because they don't trust the autonomous system right now to avoid an accident. And we see that it's going down how often that needs to happen, but still a bit removed from where humans are at. Where does this data come from? If you test in California, you have to report this data to the DMV.

ROUGE-1: 21.54, ROUGE-2: 20.29, ROUGE-L: 20.21
BERTScore: 62.08

==============================================
==================== [9/100] ====================
Summary:
Price of good 1 has gone up substitution, income these are the effects and overall. When P 1 goes up subs because of substitution effect x 1 will. Come down. And for income also because of income effect it will come down. While inferior goods substitution effect quantity demanded would come down while income effect is. Up. So, theoretically speaking what is happening? The price of good1 is increasing and if these two are true then the quantity demanded ofgood 1 is increasing such good are called Giffen good. Can you give me an example its very very difficult to find Giffan good in real life why?

ROUGE-1: 21.14, ROUGE-2: 20.37, ROUGE-L: 21.14
BERTScore: 74.68

==============================================
==================== [10/100] ====================
Summary:
which we are to for next week and here and I'll talk about that actually week from today because it's callay on Monday so it will get behind but if we get any further behind I'll have to make it up sometime the but if you don't have that you be copies available upstairs is it c um come to my office okay to dany's office he doesn't have copies of it there that's six six upstairs in the first floor immediately after CL and then on the next Friday I will talk about the parts in the in the inquiry that were assigned and also a that essay of of the oral concept. The English Constitution was at that time widely regarded this a sort of accepted view on all sides it's a hex Constitution that this problem as I say arose during the exclusion crisis of 1679 and 81. In the English case of course it's the crown and Parliament so we're only actually dealing with two agents in this case and neither is supreme so we want to say that they are coordinate powers for example the legislation that cannot be enacted without the crowns of consent and the crown must consent to all directives or laws that Parliament enacts. L's view was that Charles II then had violated his trust and it altered the form of the Constitution and therefore the government was dissolved against to power of the Comm brought into play. Mark says nothing about the details of of this process that is how does he Enis that this constition power is actually to take place he doesn't give any institutional account of how he supposes actually might be done laon is also vague on this he says something has the idea that it would begin in the county courts and they organized there. view came out too late in 166 restoration what I did was to see the importance of this view formulate in a very brief and readable clear manner I think uh and it is I think reasonable account of the basis of sovereignty in a constitutional machine. I think that what happen since losty is that one has found ways to express what appear to be elements of or features of cons digent power EG by by having a procedure for Constitutional Amendments and and how they are done one then has an Institutional procedure where you might say exercising this constition power. and who does not have the right to vote and what the conditions on it are maybe we want to free it from that of course what I'm thinking of eventually um is that that that would be one motivation for introducing an idea like the origal position that say it's a way of conceiving how con power might be exercised. No point in criticizing someone for something they didn't intend to do okay well I think it's stop so remember there's no class here Monday so the next class here will be next Friday.

ROUGE-1: 15.34, ROUGE-2: 15.03, ROUGE-L: 15.26
BERTScore: 67.97

==============================================
==================== [11/100] ====================
Summary:
David Kaiser: particle cosmology is a new subfield within physics. He says it studies the smallest units of matter, the fundamental forces and elementary constituents of matter. Kaiser: The field is doing pretty well these days by other measures. Its annual budget just within the U.S. is on the order of $1 billion a year, roughly, he says. It is really a booming, booming subject of study, Kaiser says. The field literally didn't even exist 45 years ago, he adds. In the '60s and '70s, scientists began to ask questions about the origin and impact of mass. The question was often framed in terms of what was called Mach's principle. Mach was a 19th century polymath whose work helped directly inspire Albert Einstein. In a moment, we'll take a look at some of the broader institutional shifts that were also coming to be very dramatic, and some of them quite unexpected, that also helped propel this new merger of fields. And then we'll zoom back in to see what were some examples of the kinds of research questions that now seemed obvious or natural to ask for members of this new hybrid area. and '60s. There was quite a different set of conversations happening around the same time. This had nothing to do, at least on the surface, with Mach's principle or even with Einstein's general theory of relativity. The challenge became very clear very quickly. These nuclear forces are self-evidently of short range, unlike gravitation or electromagnetism, which, in principle, can extend arbitrarily long distances. The idea was could have finite range nuclear forces if these force-carrying particles had a very large mass. The question of mass turns out to have been on many specialists minds in the 1950s and '60s, but as embedded in quite different-sounding conversations. The Brans-Dicke theory of gravity was put forward in 1961 by, at the time, a very young Carl Brans. Their idea was to try to go back to this notion of Mach's principle and more thoroughly account for that within a quantitative theory ofgravity. They wanted to modify Einstein's general theory of relativity in a very specific way. In Einstein's theory, the unit strength of gravity is set by some universal constant, the same constant G. Brans and Dicke were wondering was, what if that unit strength is actually not a constant? What if the strength ofgravity could vary across time and space? So one way to represent that variation was to say that this unitstrength of gravity, Newton's so-called constant, actually could vary because it was actually a dynamical field. And they put in very cleverly this extra dimensionless constant, a fudge factor, that they labeled by the Greek letter omega. Their version would depart from the ordinary behavior from general relativity. The idea is that this new form of matter, this new field phi, is extended throughout all of the universe through every nook and cranny of space. All of matter interacts with phi. It's almost like a new ether, you might say. And then the fact that phi is extended everywhere and is interacting universally is what they thought would take into account Mach's idea that the local inertial effects, the effects of this strength of local strength of gravity. At the time, a young Carl Brans and his advisor Robert Dicke was a new form of matter. They'll label it by the Greek letter phi. It extends to all of space, and everything else interacts with it. Could you have nuclear forces mediated by the exchange of particles? Could those particles themselves be very massive? So they don't go very far-- so a short-range force-- and yet, still be respecting the very symmetries for which people had invented those particles in the first place. terms by hand. You'd leave those particles massless. However, you'd add in a new additional form of matter even beyond those force-carrying particles. It's what's now called the Higgs field. That separate field isn't responsible for the nuclear forces. But it's responsible for giving everything else the masses that we measure, including force- carrying particles. When the scalar field gets anchored to some energy-minimizing value, that changes the interactions of all the fields that coupled to it. An induced mass coming from this spontaneous symmetry breaking. So now, again, you can have your symmetries and your short range. You can have his cake and eat it too. That was a lovely idea being introduced right around the same time as Brans-Dicke also as one way to try to get to this question of why do objects have mass. So these two communities saw very different things even though they use the same Greek letter phi. They were embedded in different kinds of conversations. A paper is considered technically renowned if it accumulates at least 500 citations in the scientific literature. Each of these papers-- the Brans-Dicke paper and the Higgs papers-- became technically renowned within fewer than 20 years. These papers were setting their own fields on fire. And yet, there's almost no overlap. These two communities were quite separated during this 20-year span. So you can see more than 500 each. In fact, it's 1,083 distinct papers doing the citing if you add up all the ones between these two plots. over 2% of the author pool-- cited both Brans-Dicke and Higgs usually in separate papers, but actually cited them in any of their work, again, over that first 20-year period. These are just ways to say that these two subfields really were not strongly interacting. In 1979, two separate theorists working independently of each other actually suggested that the two fields might be literally the same, not just comparable or worth considering side by side. How people assess them or what they thought they were good for was changing over time. more than just the letter that they chose. There was a lot of what we might have considered similarities. And yet, the two sets of ideas really were treated so separately. By the end of the '70s, things did not look very good for Brans-Dicke gravity experimentally. In fact, Einstein's theory was not highly favored, and that's why everyone else started paying attention on the gravity side of the field. So we might wonder, well, was it changes in data? Did experiments force a new evaluation? asymptotic freedom. And actually, it's the reason why our friend and colleague here at MIT, Frank Wilczek, received the Nobel Prize. So this was work introduced by Frank and his then advisor David Gross, and independently by a different very young grad student at the time, David Politzer. And what they found was that the strength of the strong nuclear force, that QCD force that we talked about quite a bit at the end of last class session, that the force actually decreases with the energy scale. the 16 GeV, rather than 10 to the 3 GeV. So this became a natural reason-- this is the main argument-- for particle theorists to ask about things like a very high-energy cosmology. And the phrase that was often used at the time, a gendered term, was a cosmology would provide the so-called poor man's accelerator, the poor person's accelerator. And so this becomes the main reason that's usually given, the main kind of cause for why these two previously quite separated fields of study were somehow merged. In the '60s and '70s, physicists saw a dramatic change in the kind of infrastructure for the discipline. High-energy particle physics was the field that had the most to lose or that lost the most during this reversal of fortune. The US budget for that subfield fell in half in just four years. It's not coincidental that these young theoretical physicists in particle theory had the hardest time when the trouble came, says David Frum, professor of physics at the University of California, Los Angeles. start showing up regularly on the general exams for physicists across all fields, all specialties. So now you have more and more students in their PhDs responsible to have learned something in neighboring subfields. And you see a market response as well. You see a flood of new graduate-level textbooks on general relativity on gravitation and cosmology-- twice as many published in the 1970s versus the 1960s. And in fact, even of those 1970s books, the vast majority came really in the later '70s, in the wake of these pedagogical reforms. a continuous unitary symmetry, which is like saying you could rotate the electron field by any continuum amount, and the equations remain unchanged. The photon only has to mop up a relatively simple symmetry, the U1 gauge symmetry. Whereas SU2 was what I was pointing to when I was referring to the weak nuclear force. That's a more complicated symmetry structure. And that's the symmetry group that these force-carrying particles-- the W and the Z particles-- are invented to enforce. In 1979, Anthony Zee and Lee Smolin introduced a whole new model where they didn't only cite Brans-Dicke and Higgs, they literally united them. They wanted to ask why gravity appears to be so weak compared to all the other forces. So the idea was that this local strength of gravity, Newton's constant, which goes 1 over this phi squared, would get anchored to a very small value when phi gets stuck at a relatively large value. And so when this dynamical field, this field phi, skitters around the universe, it eventually reaches some kind of lowest equilibrium state. setting the inverse gravitational field strength. So why is gravity so weak? They suggested maybe it's because it's arising from some broken symmetry. Much like the Higgs-Goldstone mechanism, the field is dynamical, but it's getting stuck. Only in the broken-symmetry phase do we experience a phenomena that we are used to. So gravity gets stuck being weak because its local strength is arising through the Brans-Dicke field getting accurate in a symmetry-breaking potential. '75. So he entered roughly 10 years after Zee had. He was roughly ten years younger. Both at Harvard, as it turns out, both at the same school, but 10 years apart. So unlike Zee, who focused pretty exclusively on particle theory as a graduate student, Smolin was actually, from the start, combining the two fields, both in the courses he took and eventually with his advising team for his thesis and for his dissertation itself. It was actually that second version that becomes more and more common. Few physicists today think Brans-Dicke theory of gravity best describes our Universe. Interest in the field grew even as it was getting experimentally less and less favored. One of the leading contenders for our understanding of the very early universe does exactly the kinds of things that Smolin and Zee had been doing, trying to unify the Brans and Higgs field. The idea that we're picking single theories and that they replace each other, I think, just misses this fine structure. Lee Smolin and his generation just was importantly different from the very excellent training that people like Tony Zee had had even just a mere 10 years earlier. And in turn, these new folks, especially people like Mike Turner and Rocky Kolb, went on to become real institution builders in their own right. So in fact, they were accelerates. Not only had they been trained to think carefully at this new interface, they helped really accelerate the trend. So it's gone from really, really just never done for nearly 20 years to now remarkable if people even question it. the hall from Alan's. And by a quirk of the old building 6, we had the same key. A single key would open the whole hallway. Couldn't get rid of it now. And one time, my parents were visiting. And I basically broke into Alan's office. They couldn't believe me when I described what it was like to try to work with this person. So I actually broke into his office to show them the safety violation, fire code violation, horror show that was the den of entropy. So that's a true story.

ROUGE-1: 26.38, ROUGE-2: 25.10, ROUGE-L: 25.30
BERTScore: 59.11

==============================================
==================== [12/100] ====================
Summary:
Frida Ghitis: Ferdinand Magellan may have been the first person to actually circumnavigate the globe. She says Spain and Portugal had their eyes on the same prize: trade routes to the Spice Islands. When a Portuguese defector claimed that a westward route existed, King Charles made him captain of a Spanish armada, she says. Ghitis says Magellan's legacy lingers: galaxies and space programs named after him, and he was celebrated in Spain. of the "Victoria" sailed into harbor in southern Spain in September 1522.

ROUGE-1: 17.48, ROUGE-2: 13.88, ROUGE-L: 12.94
BERTScore: 60.28

==============================================
==================== [13/100] ====================
Summary:
Ani was a real person, a scribe from the Egyptian city of Thebes who lived in the 13th century BCE. Ani's epic journey begins with his death. His body is mummified by a team of priests who remove every organ except the heart, the seat of emotion, memory, and intelligence. It's then stuffed with a salt called natron and wrapped in resin-soaked linen. The wrappings are woven with charms for protection and topped with a heart scarab amulet that will prove important later on. The Papyrus of Ani has been in the British Museum since 1888. Only Ani, if anyone, knows what really happened after his death. But thanks to his Book of the Dead, we can imagine him happily tending his crops for all eternity. In the endless and lush field of reeds, Ani meets his deceased parents. Here, there is no sadness, pain, or anger, but there is work to be done. Like everyone else, he must cultivate a plot of land, which he does with a Shabti doll.

ROUGE-1: 43.45, ROUGE-2: 41.05, ROUGE-L: 36.20
BERTScore: 66.82

==============================================
==================== [14/100] ====================
Summary:
Judy Hoyt: We're going to discuss the accurate control and placement of active dopant regions through a process called dopant diffusion. The placement of those regions determines many of the so-called short channel characteristics of MOSFETs that we'll talk about. The doping of other materials, not just the silicon itself, but of the polysilicon gate affects things like gate depletion and limits how well the gate voltage controls the channel potential, Hoyt says. "We really need to understand the placement of these atoms," she says. have a cube or a chunk of material-- we're usually measuring the resistance of a thin sheet in the near surface region. And that sheet generally has dimensions of L length and width that are much larger than its thickness. If we have a shallow junction, we can calculate its resistance as follows, again, using the same formula at the top of the page. It's the resistivity in ohm centimeters times the length now divided by the area. So that's a simple way and a convenient way of calculating resistance of various structures in semiconductor devices. In general, as we scale devices, the channel length becomes shorter and the channel resistance goes down. We similarly need to scale down these extrinsic resistances of the source drain and the extension regions. To reduce those resistances, we would like to increase, in general, the junction depth xj. However, there's a problem that if we make the junctions deeper, it will make it easier for voltages at the drain to affect the current flow in the channel. So this results in a fundamental design tradeoff for MOSFETs. The threshold voltage is an important parameter as far as the device and circuit designers are concerned. In the short-channel case, that threshold voltage equation has to be modified to a certain extent. And it's essentially because charge sharing-- that some of the charge under the gate is balanced by the charge from the source-drain regions. The L prime is actually shorter than the actual gate length. So there's this tendency as we scale devices for the threshold voltage itself to drop or to roll off. Dopants are typically introduced at their solid solubility limit. At 1,000 degrees, you can get something like 3 to 4 times 10 to the 20 electrons per cubic centimeter by introducing arsenic into the lattice. If you introduce more arsenic than that, it may still be below the solidsolubility, but you won't get any more electrons. It's not electrically active. It may not precipitate until you get up into the 10 to 21 range. That turned out to be a bit of a mystery to people for a number of years. And it was subject to a lot of discussions in the literature. When the concentration gradient goes to 0, essentially, the dopant or the atoms are uniformly distributed, say, in the solid, and the flow would stop according to Fick's first law. In this case, the proportionality constant is called the diffusivity D. It has units of length squared per time or centimeter squared per second. D is related to the atomic hop rate or the jump frequency over some kind of energy barrier. And in the silicon lattice, by symmetry D is isotropic. In semiconductor processing, linear scales for dopants are not all that useful because, in fact, we often care about how the dopant falls off over many, many orders of magnitude of concentration. So we typically like to use a semi-log type plot. OK, so we have one solution for one case. Let's go on to slide number 28 and talk about the second case, which is a fixed dose Q, just like we talk about, constant in time. But now we're diffusing near a surface. And the third case, essentially, that we can solve analytically is called the case of an infinite source. dopant at 1,000 degrees is less than about about 7 10 to the 18th. You can use this constant diffusion coefficient. Boron, again, the only really available p-type dopant, is relatively fast. It can be up to a factor of 10 or 20 or 30 faster diffusion coefficient than the slower diffusers such as arsenic or antimony. So this gives you a rough idea of, when we talk about doping diffusion, what we're going to have to worry about a little bit more would be boron. of cases where there are analytic solutions. We talked about the diffusion of a Gaussian profile with a fixed dose. We apply this diffusion to a constant surface concentration. And finally -- we talk about the diffusion of a complementary error function, which we apply for a constant level of surface concentration. The diffusion of this error function can be applied to a fixed level of surface concentration, or a constant level of substance on the surface, for example.

ROUGE-1: 16.13, ROUGE-2: 15.29, ROUGE-L: 15.25
BERTScore: 67.68

==============================================
==================== [15/100] ====================
Summary:
Atas model shows what happens if aggregate demand increases and firms respond to this by saying we want to make more output. As people demand these higher wages shortening our supply curve decreases basically all the way back to essentially where it was before. The price of tuition to Missouri State goes down but my wage is fixed under contract right and so they're gonna have to pay me the Saints of tuition declines and my input prices stay the same that's bad for them and bad for all of the firm's too. The Keynesian model in this framework its alphabet soup right that's where it's gonna be trust me I've been doing this for 20 years questions on the AAAS wanna yes so that's what we're going to talk about when we talk about fiscal policy and monetary policy. Changes in taxes and changes in government spending are also pretty obvious right if your taxes go up and the government doesn't give you more goods and services for that you have less stuff. If taxes go down and thegovernment doesn't change the amount of services that they give then you have more money in your pocket. to a at some point in time you will Keynes comes along and says famously and the long run we're all dead in other words yes eventually we'll get back to a but why wait why not go ahead and do something change aggregate demand and get you here to see. When people say yeah that sounds pretty good let's do that and so people did that and then they said hmm the Keynesian model has problems too it doesn't work perfectly either so that's why we have these changes in aggregate demand.

ROUGE-1: 14.08, ROUGE-2: 13.57, ROUGE-L: 14.08
BERTScore: 60.08

==============================================
==================== [16/100] ====================
Summary:
As a nurse you play an important role in teaching the parents about car seat safety and this education actually starts at birth before the child even goes home from the hospital in their first car ride. In this lecture we're going to concentrate on the main concepts that you need to know as a nurse and for exams first let's talk about the four types of car safety restraints that you can use in a motor vehicle. The back seat of the car is actually the safest place for a child 12 and under. level not one inch below it okay so if you would like more free quiz questions you can access the link below and don't forget to access the other videos in this pediatric nursing series. Thank you so much for watching and have a great day. Back to the page you came from. Follow us on Twitter @CNNNursing and @cnnnursing on Facebook and @CNNHealthNursery on Instagram. For more pediatric nursing videos, visit CNN.com/nursery.

ROUGE-1: 11.41, ROUGE-2: 9.86, ROUGE-L: 9.85
BERTScore: 61.10

==============================================
==================== [17/100] ====================
Summary:
Machine learning is about how to acquire a model from data and experience. In the next few lectures, we're going to work through a sequence of different takes on machine learning that are going to highlight different subsets of the big ideas on this topic. We'll start with model-based classification, and, as an example of that, we'll work through some details of how the Naive Bayes models work. And we'll have a couple running examples, such as that spam classifier that pulls out all the emails you don't want from your email. In the real world, getting the right kind of data is often one of the hardest parts of building and deploying a machine learning system. We'll see today exactly how data kind of goes through the mill and gets turned into a model. Also see how something like spam classification starts to give you a little bit of a window into how other natural language tasks work. And we'll see more in-depth examples and more structured examples of these kinds of problems later on when we talk about applications. is going to be unique. It's going to have to be at least one pixel off of something else you've seen. There's a lot of inputs that are really noisy, and you're training set, they might be hard, expensive to label, because they're noisy. And then at test time, you're going to make mistakes because machine learning is not perfect. We can look at other kinds of patterns. We could look at how many connected components of ink there are. What's the aspect ratio? How many loops are there? machine learning methods get better at doing that. We'll talk about that in a couple weeks when we talk about narrow nets. There's tons of classification tasks. It's probably the most widely-used application of machine learning. In model-based classification, rather than directly learning from errors that you make in the world from experience, instead we're going to learn by building a model from our data, and then doing inference in that model to make predictions. After today we'll look at the model-free methods. The Naive Bayes assumption is an extremely radical assumption as far as probabilistic models go, but for classification, it turns out to be really effective. The model itself might look something like this, where the class is the cause, and it independently causes each of those features. And that means when you go to make a prediction, it decomposes into a product of a bunch of different feature conditional probabilities, and we'll see examples of that and unpack the inference for that. In a Naive Bayes model, each class 1 to 0 is equally likely. In a real collection of data, 10% of the examples are 1 and 10% are 2. In addition to the prior probability of each label, we can compute things like, what is the probability that pixel 3 comma 1 is on, given each class? This isn't a distribution over on or off. These are just the probabilities-- what I'm showing here-- just the probability of that pixel for each class. And it's going to be some number. The standard model for text is to say that the features are the words and that the random variables are the word at each position. This is called a bag of words model. In the standard case, each feature gets its own distribution. Here, we assume the features. are all identically distributed, but there are multiple copies of that feature. for the different positions. And this is nice, because when the document is longer or shorter, you don't really need to change your model. Or, it is the first time. In this example, we're going to compute the probability of spam along with all of the words. And we're also going to multiply that by the number of words in the corpus. And in spam, the most likely word is the. Somewhere far down that list is the word free. What do you think is the most frequent word in spam? I heard free. I heard the. What else? Any other guesses? You laugh, but here's the answer. What's going on here? I thought free meant spam. All right, so where do these tables come from? be weighed, and that's what's going on here in the conditional model. If you started, if you did prediction in this, and you started with the class and you cranked out a pretend document, it would look significantly more like a real email than if you just did a bag of words. However, will it be more accurate for classification? It really depends. In general, it will be a little more accurate, but at a cost of having significantly more complicated conditional probabilities to estimate. We are now into the machine learning section which means we are done with the spooky Halloween time ghost-busting section. But I have candy. During the break, everybody get up and come grab candy if you would like. This gets applause? All right. Come up and grab some, please. I'm not allowed to take it home. [NO SPEECH] All right, we're going to get started again. Let's talk about training and testing. The principle of machine learning is something called empirical risk minimization. But really, the reason we're doing this is not just for Naive Bayes, or probabilistic model estimation in general. The main worry is that you over-fit. You can have plenty of training data, but you can sort of learn in a way that fails to generalize. And then, you can have tons of data drawn from the wrong distribution. How do we limit that? Mechanically, we limit the complexity of your hypothesis. We penalize sort of overly-specific models in various kinds of ways, and we'll see some examples of that even today. How can you do badly on the final exam? There's just no way to have sort of seen the whole space. In practice, there's usually other little shards of the data that you're going to want to have. So, for example, one common one is held-out data. We'll see today and in future lectures what that's for. You don't want to test your classifiers on theData that was used to train them. It's like if you go back through, and you try those same practice exams, you're like, wow I really know this stuff now. But it's like, no. This was your training data. you're training data. The question is, do you generalize? This can happen to your classifier too, so you always want to test your performance on data that was not used to train it. You need to have some metric, and there's a lot of possible metrics. An easy one is accuracy. For how many of these emails did I make the correct decision? Fraction of instances predicted correctly, but actually, that's actually not a great metric for spam detection. Any ideas why? What's wrong with accuracy? Spam detection is, in some ways, a very poor example of a canonical classification problem. The problem here is not that you're test accuracy is low, but your training accuracy was also low because you didn't learn anything. Spam is being generated by people who are trying to defeat spam filters. And so in that sense, over time, spam classification doesn't actually look like a standard classification problem because it's adversarial. OK. Any questions before we talk about generalization and over-fitting. OK, so in these images, you want to fit the hat right. the thing we're trying to do is to fit a curve to this data. So you say, what is the fit? Is the fit getting as close as possible to the last dot? So what is my constant approximation to this? Does anybody want to hazard a guess? Let's call it five. OK, did I fit something about this data? Yeah, I felt something about the data. I fit basically it's mean. Did I capture the major trends? No. All right, let's try again. Let's fit a linear function. It's close, right? It's a better fit than the constant function. Notice that when I went to linear function, the space of hypotheses grew. Instead of just lines, now it's like lines with slopes and intercepts. In Naive Bayes probabilistic models, over-fitting usually shows up as zeros in your probability table. In other methods, it's actually going to show up in totally other ways. So let's figure out some ways to do that, to just illustrate what it would like to look like. We could take that polynomial and limit the degree of the polynomials. Using it under a hypothesis, you can also shrink the hypothesis space, so you can fit less under it. We already know one kind of over-fit to limit that. The maximum likelihood estimate, or relative frequency estimate, is used in machine learning. It says, OK, the probabilities are just the counts in the training data. For each probability I assign to red, and one minus that goes to blue, I can compute the probability of D. This is something you could try writing out for yourself. Of all of those probabilities, the one that matches the frequency of the data is the one. that maximizes the probability. of theData. But in practice, you need some smoothing. CS281A is an open-source computer program. It can be used to test computer models. CS281A uses a Bayes rule to find the parameters which maximize the product of this, which is what we were doing before. But there's this extra term, p of theta, which says, if I want to know what parameter or what probability is most likely, I need to weigh the likelihood of the data against how likely I think that parameter is in the first place. This is actually, due to Laplace, hundreds of years ago now, who's a philosopher who kind of worried about things. In a real classification problem, you have to smooth if you're going to use Naive Bayes. Instead of computing odds ratios on the maximum likelihood, I can instead do some smoothing and see after that smoothing, what has the biggest odds ratio? And suddenly things that only occurred once, they don't percolate to the top, because they haven't occurred enough to overwhelm that flat prior that I'm associating them with. So this is the top of the odds ratios for ham on the left, and favoring spam on the right. Some of these maybe make sense. Like, there it is. probably in there somewhere. If you see money, that's a good sign that it's spam. Or capital order, or credit, presumably credit card, I don't know. There are some things that indicate ham. This looks like general English text. What is going on there? Helvetica vs. Verdana. This reflects the default fonts that were in use at this time across different platforms. And so one of the things you find in machine learning is, you know what you think the features are going to be. But you might be wrong. In general, you're going to need more features. In spam classification, we found out that it wasn't enough to just look at words, you've got to look at other sort of metadata from the ecosystem. For digit recognition, you do sort of more advanced things than just looking at pixels, where you look at things like edges and loops and things like that. You can add these as sources of information by just adding variables into your Naive Bayes model, but we'll also talk in the next few classes about ways to add these more flexibly.

ROUGE-1: 23.73, ROUGE-2: 22.74, ROUGE-L: 21.93
BERTScore: 65.56

==============================================
==================== [18/100] ====================
Summary:
in this video we're gonna talk about how a country can gain from exporting goods or services through international trade. We're gonna look at how consumer surplus producer surplus and total surplus are going to change when we introduced the idea of trade in allowing Chile's copper manufacturer producers to trade on the global market. The world price of copper is five thousand four hundred and forty dollars a ton. Because the world price is higher than the price in Chile Chile will export copper. There is a shift of some of the consumer surplus is going to go to the producer surplus.

ROUGE-1: 14.64, ROUGE-2: 13.69, ROUGE-L: 13.40
BERTScore: 67.04

==============================================
==================== [19/100] ====================
Summary:
Thiazide tells us that this medication works in the early part of the distal convoluted tubule that's found within this nephron. This transporter is called the sodium chloride co-transporter and it is considered a cyanide sensitive transporter so hence why this drug works so well. While loop diuretics are a lot more effective than a thiazide diuretic but the thiazid does provide a nice diuresis effect. They're less effective in patients who have a compromised GFR a go merrill ER filtration rate. channels are influenced by aldosterone and we've learned that ald testosterone causes the body to keep water and sodium in exchange for potassium. Patients may have to take potassium supplements you'll want to be watching that potassium level for sure especially if they're on other medications which we'll talk about in a moment. educate them about foods that are rich in potassium and they want to definitely keep those in their diet those are diuretics can also alter the blood sugar and this is important to know for your patients. about thiazide diuretics can be used in the treatment of renal calculi which are those renal stones that are composed of calcium now let's wrap up this review and let's talk about nursing responsibilities the side effects and education pieces for the patient who may be taking a thigh-high diuretic. As a nurse you can watch out for is you can look at their vital signs how is their blood pressure if it's hypotensive low that's the salts less than 90 that probably means that we've removed a little bit too much fluid volume from their blood.

ROUGE-1: 14.59, ROUGE-2: 14.03, ROUGE-L: 14.59
BERTScore: 64.06

==============================================
==================== [20/100] ====================
Summary:
Future John Green tells you that in a stunning turn of events the 2020 presidential election will be won by - Harry Styles. We’re going to change the constitution to make it possible. Because… that’s how much we love Harry Styles in 2020. John Green: The Wall Street Wamboozle, the Financial Fartstorm. The Major Recession of 2008 - 2012. Crash Course U.S. History, Sundays at 10pm on CrashCourse.com and 11pm on CNN. The thinking was that the interest people paid on their mortgages would supply the underlying value of the security. When the mortgages turned bad, these securities became toxic assets. The Bush Administration tried to stop the damage by getting Congress to pass the Troubled Assets Relief Program, or TARP, which was basically a $700 billion bailout for banks like Citigroup and Bank of America. Most of the banks that received a rescue from the taxpayers didn’t help the homeowners facing foreclosure, and despite receiving millions of federal dollars, AIG continued to pay huge bonuses to its top executives. In 2008 Obama’s election seemed a political watershed and not just because he was the first African American president. He appealed to young people and minorities, and he harnessed the power of social media to communicate with supporters, and get out the vote. So Obama promised to change the culture of Washington. He would end partisan squabbling…. sorry. I guess the author of the Mystery Document, if I’m wrong I get shocked. No more shocks. The getting shocked part of my life has come to an end. Obama wanted a foreign policy based on diplomacy, he wanted to reduce inequality and increase access to health care. He also wanted to end the wars in Iraq and Afghanistan and, as critics mocked, reverse global warming. So how has he done? Not bad either. For instance he launched diplomatic outreach to the Muslim world, but a lot of this was more rhetoric than action. And he did keep some of his campaign promises, for instance he signed into law the Lily Ledbetter Fair Pay Act, which made it easier for women to sue. depends on who you ask. Among 9 large studies, 6 found that the stimulus did have a positive effect on growth and employment, 3 found that it had little or no effect, and economists are equally divided. The stimulus is estimated to have saved about 3 million jobs, but it also increased the deficit quite a bit. Liberal economists see America’s current 7% unemployment rate as evidence that the Keynesian policies should have gone further, while conservatives say that the Stimulus exploded the federal deficit and debt. The 111th congress was one of the least productive in American history. Unwillingness to compromise precipitated a series of mini-fiscal crises over things like the budget and raising the debt ceiling. The Tea Party is right that the founding fathers would be astonished by the extend of the American government and the extent to which it’s involved in the lives of Americans. We have to ask ourselves again, “What does freedom really mean?” Can you be free when you live in poverty or when you’re one injury away from bankruptcy? Can you being free when the government can go to a secret court to read your text messages? Crash Course World History has been on the air for two years. The show celebrates two successful years of teaching history. This has been one of the great professional joys of my life and I’m so grateful to everyone that has helped make the show and everyone who has watched it. Thank you again for watching, and as we say in my hometown, “Don’t forget to be awesome.” You can find a full list of your reading for Crash Course Literature in the doobly-doo.rolling.

ROUGE-1: 33.13, ROUGE-2: 30.92, ROUGE-L: 30.49
BERTScore: 58.73

==============================================
==================== [21/100] ====================
Summary:
Simple graphs don't have direction. They just correspond to a mutual connection, which is symmetric. An edge just has two endpoints that are in V, and we don't distinguish the endpoints. When two vertices are connected by an edge, they are said to be adjacent. The degree of a vertices is simply the number of incident edges, the number. of edges, that touch it, and the number for which its an end. point. It's a basic concept in graph theory. We're going to make a little bit of in this video segment. The handshaking lemma says that the sum of the degrees summed over all the vertices is equal to twice the number of edges. So that's the cardinality symbol. Absolute value of a set means the size of the set. E here is finite. Twice the number. of edges isequal to the sum over all. the verticing of the degree of the vertice. And the proof is trivial, but let's make something of this. You might wonder why it's called the handsh shaking lemma. That will emerge in some problems that we're going to have you do. Study: Men have 30% more partners than women, according to the US Department of Health. David Frum: "We're going to come up with a very elementary graph theoretic argument that says that this is complete nonsense" He says there is a fixed relationship between the average number of partners of men, the average degree of the M vertices and the average degrees of the F vertices. Frum says the answer seems to be that people are lying about the number of women and men.

ROUGE-1: 23.81, ROUGE-2: 21.90, ROUGE-L: 21.88
BERTScore: 61.70

==============================================
==================== [22/100] ====================
Summary:
Robotics is a really cool and important direction for the future. I really believe that in the future we will have AI assistance whether they are embodied or not to act as our guardian angels. These agents will help us with cognitive and physical work. With AI we will see such a wide breadth of applications for instance these technologies have the potential to reduce and eliminate car accidents. We have three types of learning and you have seen different aspects of these methodologies throughout the course we have supervised learning, unsupervised learning and reinforcement learning. The most popular Benchmark for measuring the accuracy of image classification is imagenet. Machine learning is very powerful for building systems for building robots but as robots it's important to keep in mind the scope of machine learning in the context of robots. It's quite easy to take a stop sign and perturb it in such a way that you can't tell with a naked eye that there is a difference between the two images with all of the images and all with all the images. We employ thousands of people to label the objects that we have. to do given input reinforcement learning is causing a huge revolution in robotics and so um and and why is that. Reinforcement learning is concerned with how intelligent agents ought to take action in an environment in order to maximize the notion of a cumulative reward. In order to get the simulation to drive a real robot we actually need to think about the Dynamics of the robot and so in other words we have to take into account what the vehicle looks like what are its kinematics and its Dynamics and so here is a vehicle that is running the policy learned in simulation. In 1986 German engineer Ernst Dickman started thinking about how he could turn his van into an autonomous vehicle and so he put computers and cameras on the van and began running tests on an empty section of the German Autobahn which had not been open for for public driving. Computer processing improved from one frame per 10 minutes to 100 frames per second and this has been a game changer for autonomous cars. Today we have very effective and Deployable solutions for robot cars that move safely in Easy environments where there aren't many static nor moving obstacles. of agent to agent interaction and and so the the Vista simulator has been recently open sourced you can get the code from vista.csel.mit.edu and a lot of people are already using the system. What we get from Vista are is the ability to simulate different physical sensing modalities that means including 2D cameras 3D lidar event cameras and so forth and then you get the possibility to to simulate  environmental situations and perturbations you can simulate weather, lighting and different types of roads. vehicle that is executing the learning based control and here's Alexander with his vehicle that was trained using data from Urban driving and now he's driving to the soccer field. The decision engine has about a hundred thousand neurons and about a half a million parameters and I will challenge you to figure out if there are any patterns that associate the state of neurons with the behavior of the vehicle. The attention of the solution is extremely focused as compared to other models like CNN or ctrnn or lstm which are much more noisy. do some computation at work inside the intelligent boardroom the temperature could get adjusted automatically by monitoring people's comfort and gesture and just-in-time Holograms could be used to make the virtual world much more much more realistic much more connected. There's the garbage ban the garbage bin that takes itself out and after a good day when it's time for a bedtime story you can begin to enter the story and control the flow and begin to interact with the characters in the story. These are some possibilities for the kind of future that machine learning artificial intelligence and robots are enabling.

ROUGE-1: 14.41, ROUGE-2: 13.75, ROUGE-L: 13.87
BERTScore: 60.42

==============================================
==================== [23/100] ====================
Summary:
hair up grab an apron let's go now with the new chefs in placelet's go Tatiana okay Gordon knows it's critical that Tatiana is familiar with the inner workings of the kitchen. I just can't wait to get to work and prove to my mom and my sister that I can do this I think the message is clear nobody scared to walk through that door and get their hands dirty in that kitchen no we're not tomorrow is a big day let me tell you I need everyone on their game good night guys get some sleep thanks. ramsy you're my boy dude what the [ __ ] have you done stop no no no listen [__ ] donkey Danielle come here I want you to stop but if the restaurant goes your hous is going go wow um and how much money did you put in um probably like 1,000 bucks probably like 2,000 dollars.ramsy: "I'm sorry. I'm so sorry. It's just the way it is. I don't know what I'm going to do. I just want to go home"

ROUGE-1: 47.58, ROUGE-2: 42.20, ROUGE-L: 40.26
BERTScore: 62.69

==============================================
==================== [24/100] ====================
Summary:
HONG LIU: Last time, we talked about this IR-UV connection between the bulk and the boundary. Now let's talk about some further aspects of the duality. HONG LIu: Once you realize there's such relation, since the two sides are completely different objects, so the game is that you really have to do lots of guess work. Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free. To make a donation or to view additional materials from hundreds of MIT courses, visit MIT Open courseWare at ocw.mit.edu. HONG LIU: So here on this side, there is a conformal symmetry which we explained before because this is a four dimensional theory. On the right hand side, we still have some SO 5, S5. So this is more like a space time symmetry. In N equals 4 super Yang-Mills theory, we discussed last time there are six scalar fields. You can rotate them each other. So essentially, you have eight supercharge of the [INAUDIBLE]. So this all together is 32 real superchargers. In super gravity, the supersymmetry is local. The isometry is a subgroup of diffeomorphism. For each global symmetry in the field theory side, there's a corresponding local symmetry on the gravity side. The Yang-Mills theory is all global symmetries on the global side and not on the local side. But the level is even though space time fluctuates, but the AdS5 times S5 specifies the asymptotic geometry of the space time. HONG LIU: Conformal symmetry of the component of the field is the Minkowski space. So on this side, you have isometry of AdS, on the other side you have conformal symmetry. Even for the local supersymmetric transformation in the gravity side, there's also a part which does not vanish at infinity. And that's the part corresponding to the global supersymmetry on the field theory side. The story actually works more general. This is the statement for the N equal to 4 super Yang-Mills theory. SO(6) gauge symmetry. And some fermions indeed are charged on this SO(6), so it's just more general. So whatever things, anytime if you have some global symmetry here should be mapped to some gauge symmetry here. So now let's move to the matching of parameters. And again, first N equals to 4 super Yang-Mills theory. And then this is type IIB on AdS5 times S5. So the N is the gauge group N. Do you have any other questions? expanding 1 over N squared. So as we said before, we often do dimensional reduction on S5. Then G5 divided by R cubed, again only related to N given by pi divided by 2N squared. These relations are often useful in the future. So now let's look at [INAUDIBLE] limits of this relation. So let's first look at the classical gravity limit on the gravity side. So essentially, we are dealing with quantum field theory in curved space time. The decoupling of the string effect requires on the field theory side the strong coupling. If you look at just those [INAUDIBLE] diagrams, of course you don't see a space time interpretation because they are just planar topology. The strong coupling, then the diagram with many, many vertices will dominate. And then the most dominated diagrams are those diagrams with not a lot of vertices. And they essentially are going to continue to limits. So this will tell you that the strong. coupling limit is described by classical gravity. a general correspondence between some conformal field theory and some AdS gravity theory. HONG LIU: If the two theories have to be the same, then you should be able to map their spectrum. For example, in the boundary, if there is some scalar operator, there must be a corresponding scalar field on the gravity side. And similarly, if you have some symmetric tensor, then this must also be related to some symmetry tensor. Yes? AUDIENCE: So we proved that the super Yang-Mills theory really lives on the boundary of the AdS? HONGLIU: No. We did not prove that. This is just a postulate. you can imagine that this is the boundary, this relation is related to the bulk and the boundary. And this is a postulate based on that fact. In the field theory side, there's no massless spin to particles. They map to some field in the gravity side in the five dimensions. Then this maps to some five dimensional fields. And that is a nontrivial prediction from thinking the boundary and the bulk relation. Now you can generalize it. You can argue from the symmetry point of view also, but that language is more direct. In the '80s, people have worked out the supergravity spectrum precisely on this space. So now, as an immediate check, you can now just open your old papers. And then you can immediately see they actually map to certain representation of operators on the N equal to 4 super Yang-Mills theory. So if there are other symmetries, then everything should match. But on this side, so this appears as isometry on the S5. But when you dimensionally reduce on S5, then in AdS5 then there will be a pure gauge field corresponding to each symmetry generated [INAUDIBLE] a few minutes. So now, given this mapping, any operator is due to a bulk field. Then you can ask some immediate questions. For example, the quantum numbers of these operators will map to the quantum number of the bulk fields. And that's something I said you can check their symmetries. So for local operator on the field theory side, we can immediately ask questions related to operators on this side. And ask the story about the field on the gravity side, and ask what's happened. We can start developing the relations. HONG LIU: If field theory has a gauge field, then I can write O as trace F squared. Then this corresponding to changing the coupling for trace F square. So immediate question is, what does this operation-- so in the field theory point of view, you can always do this operation. So here, I write partial AdS means the boundary of AdS. So this is the value of Ad S, value of phi at the boundary. So expectation value essentially can be associated with the boundary value of the field. HONG LIU: We have established a connection between the Yang-Mills theory coupling with the value of phi at the boundary of AdS. If we deform the Lagrangian, say, change this coupling, deform the boundary by changing the coupling, which corresponds to you add some delta G. And this corresponding to you essentially change the boundary value of dilaton. So now, this example gives us the answer to this question. In real life, what you will do from this example, you will say, ah, this must be the case. Then you will start to find examples to check it. And we will describe it later. N equals 4 super Yang-Mills theory and this type IIB gravity. It's that any conserved curve in the boundary theory must be equal to some gauge field in the gravity side. So now I'm going to use this argument to make that a little bit more natural. So let's first consider this. So suppose I have a conserved current, J, J. I've a J mu. And then according to this identification, the A mu must be A mu-- we should be able to identify it as the boundary value of some bulk field. T mu mu to the boundary Lagrangian. And this is the source to the stress tensor. When we consider the information, we always consider the source is small. But now, we can argue this thing, mass corresponding to boundary value of the metric in the gravity side for the following reason. So this is a very general statement valid for any correspondence between the gravity and the field theory. If you have a dynamical metric, then you have gravity. So you can say, if any field theory is due to a theory of one higher dimension, that theory must involve gravity-- nothing about quantum gravity.

ROUGE-1: 27.49, ROUGE-2: 26.37, ROUGE-L: 25.72
BERTScore: 68.59

==============================================
==================== [25/100] ====================
Summary:
Bolek Wyslouch: How do you convert a given physical system with all the forces, et cetera, into some sort of fixed form, fixed type of notation? We will discuss various interesting-- even though the system is very simple, just two masses, a spring, a little bit of gravity on top of that. The way they behave could be extremely complex, but it can be understood in terms of very simple systematic way of looking things through normal modes and normal frequencies. into a set of equations. So we have a mass, m, hanging from some sort of fixed support, another mass here, same mass for simplicity. We connect them with a string, and we know everything about this system. The spring is initially at its rest position such that when the two pendula are hanging vertically, the spring is relaxed. But if you move it away from verticality, theSpring either compresses or stretches. We assume that this is an ideal system, highly idealized. for mass 1 in our coupled system. And I could say most of the terms have to do with a motion of mass 1 itself. Mass 1 is its own pendulum. And mass 1 is feeling the effect of the spring force. So we can repeat exactly the same calculation focusing on mass 2. And then the equation which you will get will be very similar. So where m x-acceleration is equal to minus k plus mg over l times x1 plus k times x2. This is what makes those pendula coupled. me do everything. Let's write down everything in the matrix form, because it turns out that linear matrices are very useful for that. So let's introduce to them and show vector, which consists of x1 and x2. So we will be monitoring the change of this x2 as a function of time. We will introduce a force matrix k, which is equal to k plus mg over l minus k here. And then we need a third matrix, mass matrix, which simply says that masses are mass of first object is m and the other one is also m. oscillates at same frequency, both x1 and x2, undergoing motion of the same frequency. So I propose that-- so of course, we use the usual trick that anytime we have a solution in complex variables, we can always get back to real things by taking a real part. So let's plug this into our matrix calculation. And what you see here is that -- so what do we have? So this is the term, which is second time the derivative vector X. And because vector-- or vector Z really. or they go opposite. So to make it more general, I have to give some multiplicative factor there. So if I do everything, I end up with x, the mode 1 will in general have some sort of overall constant C. And the mode number 2 will be C2 cosine omega 2 times t plus phi 2 times 1 minus 1. So the superposition of x1 plus x2 gives you the most general combination of possible motion. And to describe it in specifically-- defined for a specific configuration, you will have to determine the values of alphas and phis. The motion is simply those cosines which are kind of adding up to give you this impression of rather a complicated motion. If you plug it into those equations, if you plug t equals 0, phi is equal to 0, et cetera, you will see that it works. So each of those objects effectively feels the effects of omega 1 and omega 2, but in a slightly different way. That's why their relative motions are different. And we will have time-- since on the computer, you can make things perfect. Let's do it. The magenta is normal mode number 2. And blue and the red are the actual pendula. And the motion of blue and red is simply a linear sum of the two. And this is exactly what-- this is the computer simulation that shows you that one of them is going up, the other one down, et cetera. This is for the certain combination of initial conditions. I could go change initial conditions in my program and have a different behavior. But whatever happens, I would be able to-- it will always be a combination of thetwo motions. BOLESLAW WYSLOUCH: I could put it with me some spaceship, and go to a place where the gravity is different, right? Why not? So what would happen? So if gravity changes, then basically what will happen is both this term and that term will change. So let's try to see what happens on the Moon. It's a little bit not completely clear what's going on, but you see, actually the motion is kind of strange. It looks kind of messy, doesn't it? equations. Let's look at these equations here. This is mass 1 and mass 2. So I can rewrite those solutions a little bit different. And what you get is x1-- x1 of t is equal to minus x0 sine of omega 1 plus omega 2 divided by 2 times sine omega 1 minus omega 2 divide by t. So again, we did zero physics here. We just rewrote the simple trigonometric equations. But what you see is something interesting here. There is-- we have those two frequencies which are playing a role. like this. OK? So there are in fact two-- when you look at this picture, you can see two frequencies. One which is clear the oscillation of the-- high-frequency oscillation. But there's also this kind of overarching frequency of much smaller frequency, and this is what corresponds to a difference of two things. So we see this here. We see it on the pendula. But now what we are going to do is we're going to try to hear it, right? So this is a demonstration which maybe it works, maybe not. BOLESLAW WYSLOUCH: We were able to set up the system, put in some of the matrix equation, kind of solved it, found two frequencies, et cetera. There is one more-- one additional trick, which you can do to describe the motion of a coupled pendula. And that is, in a sense, force mathematically, force the normal modes from sort of early on, to instead of, so far, when we talked about pendula, we describe their motion in terms of motion of number 1. what is coming out. So if I add and subtract the two original equations of motion, which I don't know if I have them somewhere, and you can look back, then you end up having those crossed terms drop out. And you have one, which has only this coefficient, the other one which has that coefficient. And this immediately-- and it looks-- if I now write it in terms of normal coordinates, then I have that m u1 double dot is equal to simply minus mg over l, u1, and m u2 double dot. All right? So no, the determinants needed no matrices, no nothing. We just added and subtracted the two equations, and things magically separated.

ROUGE-1: 21.50, ROUGE-2: 21.01, ROUGE-L: 21.47
BERTScore: 61.71

==============================================
==================== [26/100] ====================
Summary:
GILBERT STRANG: This is the second of the three basic partial differential equations. We had Laplace's equation, that was-- time was not there. Now time comes into the heat equation. We have a time derivative, and two-- matching with two space derivatives. So I have my function. My solution depends on t and on x, and I hope I can separate those two parts. This is exactly like the way we solved the ordinary systems of differential equations: We pulled out an e to the lambda t. Fourier series tells us how to find these Bk's. We're talking about a partial differential equation. We have numbers depending on time and decaying rapidly, and something depending on x. So at time 1, if I drew a picture, suppose the heat is, the temperature starts out through the whole bar at 1. But with this kind of time decay, a little later in time, it's going to be something like that. It'll be way down at the ends, pretty low in the middle. So that's what solutions to the heat equation look like.

ROUGE-1: 25.41, ROUGE-2: 24.63, ROUGE-L: 24.46
BERTScore: 70.25

==============================================
==================== [27/100] ====================
Summary:
James W. SWAN: I hope everybody saw the correction to a typo in homework 1 that was posted on Stellar last night and sent out to you. The TAs gave a hint that would have let you solve the problem as written. But that's more difficult than what we had intended for you guys. So maybe you'll see the distinction between those things and understand why one version of the problem is much easier than another. So we've got two lectures left discussing linear algebra before we move on to other topics. because you didn't do pivoting-- you'd like to do pivoted in order to minimize the numerical error. Or you need to reorder inorder to minimize fill-in. As an example, I've solved a research problem where there was something like 40 million equations and unknowns. If you reorder those equations, then you can solve via Gaussian elimination pretty readily. But if you don't, well-- my PC had-- I don't know-- like, 192 gigabytes of RAM. The elimination on that matrix will fill the memory of that PC up in 20 minutes. And you'll be stuck. model I showed you last time works. If the chip is in a particular cell, then at the next level, there's a 50/50 chance that I'll go to the left or I'll going to the right. So the probability that I'm in a certain cell at level i is this Pi plus one. And there's some sparse matrix A which spreads that probability out. It splits it into my neighbors 50/ 50. And we'll see the simulation that tells us how probable it is to find the Plinko chip. Yes? Eigenvectors of a matrix are special vectors that are stretched on multiplication by the matrix. They're transformed. But they're only transformed into a stretched form of whatever they were before. For a real N-by-N matrix, there will be eigenvector and eigenvalues, which are the amount of stretch. Finding eigenvector-eigenvalue pairs involves solving N equations. We don't know how to solve non-linear equations yet. So we're kind of-- might seem like we're in a rough spot, but I'll show you that we're not. Eigenvalues and eigenvectors seem like special sorts of solutions associated with a matrix. If we understood them, then we can do a transformation. Here are some examples of the diagonal elements of a diagonal matrix. See if you can work out the eigenvalues of that matrix. Anyone want to guess what they are? Try to make sure you can do it on your own. If you couldn't do this, that's OK, but you should try to make your own practice on this. James Swan: The eigenvalues of a rate matrix are going to tell us something about how different rate processes evolve in time. Eigenvalues could be real or complex, just like the roots of a real-valued polynomial, he says. The determinant of a matrix is the product of the eigen values, Swan says. He says they tell us the rate at which different transformations between materials occur in a reaction. The characteristic polynomorphism of the rate matrix looks like this, Swanson says. Eigenvalues can be interpreted in terms of physical processes. This quadratic solution here has some eigenvalue. It's not unique, right? It's got some constant out in front of it. So add the first row or subtract the second row. And then we'll compare. This will just be a quick test of understanding. Are you guys able to do this? Sort, maybe, maybe an answer, or an answer for the eigen value. Is that too fast? Are you OK? No. James W. Swan: Try this example out. See if you can work through the details of it. I think it's useful to be able to do these sorts of things quickly. Here's a matrix. It's not a very good matrix. But it's all 0's. So what are its eigenvalues? It's just 0, right? And they're 0. That eigenvalue has algebraic multiplicity 2. Can you give me the eigenvectors of this matrix? Knowing what those eigenvectors are requires solving systems of equations, anyway. But in principle, I can do this sort of transformation to do. We haven't talked about how it's done in the computer. These are ways you could do it by hand. There's an alternative way of doing it that's beyond the scope of this class called-- it's called the Lanczos algorithm. It's what's referred to as a Krylov subspace method, that sort of iterative method where you take products of your matrix with certain vectors. There are many times when there's not a complete set of eigenvectors. And then the matrix can't be diagonalized in this way. So there's an almost diagonal form that you can transform into called the Jordan normal form. There are other transformations that one can do, like called, for example, Schur decomposition, which is a transformation into an upper triangular form for this matrix. And we'll talk next time about the singular value decomposition. All right. Have a great weekend. See you on Monday.

ROUGE-1: 20.72, ROUGE-2: 19.11, ROUGE-L: 19.29
BERTScore: 63.21

==============================================
==================== [28/100] ====================
Summary:
A random variable is a number that's produced by a random process. The number of alpha particles detected by a Geiger counter in a second is believed to be a random number. Number of faulty pixels in a monitor is also produced from an unpredictable randomness in the manufacturing process. And if I flip coins then the number of heads in a given number of flips-- let's say I flip a coin n times -- will be another rather standard random variable.. Another one is simply a [? 0-1 ?] valued random variable where it signals 1 if all 3 coins match in what they come up with, and 0 if they don't match. a convenient use of random variables is to use them to define various kinds of events. C is greater than or equal to 1 when there is at least 1 head. M greater than 0 means all the coins match. This is an obscure way of describing the event all heads, and it has a course probability 1/8. Now we come to the formal definition. A random variable is simply a function that maps outcomes in the sample space to numbers. It's a total function. Usually this would be a real valued random variable. A lot of the event properties carry right over to random variables. A bunch of random variables are mutually independent if the events that they define are all mutually independent. In order to explain it I've got to set up the idea of an indicator variable, which itself is a very important concept. If I have an event A, I can package A into a random variable, just like the match random variable was really packaging the event that the coins matched into a [? 0-1 ?] valued variable. this can have value 0 and 1. If R is independent of S then R is really independent of any information at all that you have about S. And of course the notion of k-way independence carries right over from the event case. If I have k random-- if I have a bunch of random variables, a large number much more than k, they're k- way independent if every set of k of them are mutually independent. The notion that there's an odd number of heads is simply the mod 2 sum of the Hi's. hold in circumstances where pairwise does hold. So this is good to know. We'll be making use of it in an application later when we look at sampling and the law of large numbers. Back to Mail Online home.back to the page you came from. Back To the pageyou came from, back to the pages you came From. Back then, the page was: "The first page of the first week of the second year of the Second World War. The first page was the first day of World War II. The second was the second day of the First World War."

ROUGE-1: 30.11, ROUGE-2: 27.25, ROUGE-L: 26.97
BERTScore: 63.85

==============================================
==================== [29/100] ====================
Summary:
SAR Gowan as a ruthless bloodthirsty Warrior um we really don't see him in that regard. Throughout the story of seral when we again come back to what a knight truly is what they believe their practices their nobility you know there there's the PowerPoint on Lut where it talks about their their pentagram their four or five things well must be five it's pentag the five things that you know that the way that they live their life you know being good to women and being chorous and all that stuff. along uh we're going to uh go through this it'll take a little bit of time and uh we'll talk about it upon completion but it's a sir Gowan or G and the Green Knight back to the beginning we start to get those archetypes um page uh 175 or so we come right at the beginning with the Green Knights there the physical description does he come full out armor for B for battle no he simply comes with his ax on his horse and everything that he has is green and so on. own land and look for no further trial so if after the deal I'll tell you where you can find me I will give you my name there are no tricks here. If for some reason I am unable to or I choose not to tell you myname then the agreement's done and you don't have to come and look after me okay. It's kind of like those things where be careful what you wish for that type of thing make sure you understand exactly what agreement what contract you're getting into the night from the wife of bath. will be able to pick it up and still live and survive and so oh crap this is not going to turn out well. The Knight tells him you have a year to find me you can find me. The wife really kind of starts to come on to him over time okay real flirty she's she's a temptress okay very seductive uh she gives him a kiss what does he do when the husband comes home that day cuz the husband's going to share his his food share his everything with him as long as GNE you share with. me and so he gets kissed what does he do to the husband when the husband gets home kisses the husband I'm sharing honor this is what I said I would do so I'm going to do it she attempts to give him things you know jewels and everything and he he rejects that stuff she pretty much throws herself upon him and he rejects it because he is a good guy. She eventually does give him a green girdle um a green corset a a sash some sort of clothing that he can wear under his stuff and he accepts it okay. thing and so he hears it you know happening and um so he screams out and the and the um and the Green Knight comes he says by there said one on the bank above his head and you shall swiftly receive what I once swore to give you. The Knight addresses him they have a little back and forth and he gets ready to hit him and it's one of those kind of like you've had friends before probably have come up to you and go like that. He chastises him and says do it again I will not flinch go on game. "The Green Knight pushed her along hey try to seduce him see what happens we're testing him to see what's uh if he's truly a good uh a good person or a coward as it turns out" "Their Vice and villainy are virtues undoing um uh where I you finally see that now I am faulty and false and found fear fearful always in the train of treachery and untruth go woe and shame" "Can we really call him a villain probably not but yet isn't he that antagonist early on?" good person away with the forbidden fruit you know something similar we could maybe connect uh down the road but it's very similar to to those so um so go and the green. I hopefully you enjoyed it um really story-wise we're done with knights uh. It's a very similar story to the one we did with the knights. It was a very different story but it was very similar. I hope you enjoyed the book. I'm glad you read it. I'll see you next week.

ROUGE-1: 28.92, ROUGE-2: 27.47, ROUGE-L: 27.49
BERTScore: 63.24

==============================================
==================== [30/100] ====================
Summary:
YouTube boy Robert teaches you everything in the kitchen. We have blueberries strawberries Kiwis red beets yellow beets pineapples right in fruit and cucumber this is the equipment you'll need for today you don't need one blender sheet trays mason jars parchment paper aluminum foil a spice grinder a bowl with a strainer and a plastic spatula all right so what you're gonna want to do first is cook off your beets we're gonna soften them up I said that when we blend them. with salt so they could be added to savory dishes and that's how you please have you made it this far thank you for watching if you liked the video then go ahead and like the video give it a thumbs up. Don't don't worry about the finger just give it an thumbs up and I'll see you next time [Music]  "I'll See You Next time" is a weekly video series on CNN iReport.com. Visit CNN.com/Video each week for a new video.

ROUGE-1: 43.06, ROUGE-2: 38.75, ROUGE-L: 39.43
BERTScore: 63.06

==============================================
==================== [31/100] ====================
Summary:
The six Vital Signs are pain oxygen saturation temperature heart rate respirations and blood pressure. The patient is rated on a scale of zero to 10 with zero being no pain at all and 10 being the worst pain they've ever had. If they do have pain ask them the quality what does it feel like and where it is at. A normal temperature is about 97° fit to 99° F okay and take the thermometer out and read it and his temperature is 98.2 and then clean it properly per Hospital protocol. I'm going to take his oxygen saturation every system has different ways of how they measure it um different devices this is a little portable device and what you do is you put this on the nail bed of the finger it has some red lights in there and those red lights read through the nailbed the oxygen saturation a normal oxygen saturation O2 sat as you may hear and the hospital setting is 95% to 100% so let's see what his is. We are going to count the heart rate and respirations generally I like to do this together. I count that for 30 seconds if it's regular and then the next 30 seconds I count the respirations. may have to learn how to do a manual one now my previous video and a card should be popping up I go over the two-step method if that's how you're being instructed. In this video we're going to go over  the one step blood pressure of how to obtain it manually so what we'regoing to do we are going to palpate the brachial artery this is in the bend of the arm and make sure you ask the patient which arm you can take their blood pressure in. Mark now we're listening for whenever it stops and whenever it stopped that's our diastolic okay it stopped right at 65 so his blood pressure is 114 over 65 so that is how you check bottle signs now whenever you're done remember to let the patient know what their bottle signs were and um do hand hygiene and clean your equipment before you go to the next patient so be sure to check out all my other videos on nursing skills and thank you so much for watching and for all your support.

ROUGE-1: 39.03, ROUGE-2: 37.52, ROUGE-L: 38.74
BERTScore: 65.14

==============================================
==================== [32/100] ====================
Summary:
The grenade algorithm uses a minimal set of three-point correspondences to solve the camera pose estimation problem. The disadvantage of this particular formulation is that it ends up with a four degree polynomial which means that it could give up to a total of four possible solutions. The solution that gives the minimal reprojection error would be considered as the correct solution. The question becomes whether can we find the solution directly from any four point correspondences or more than four point Correspondences such that the solution is unique. equation in terms of s1 and we will find that if we do this it will end up to be a 4 degree polynomial in the unknown of sOne. A better solution here is proposed by uh kwan and lan in the paper published in active tipami in the year 1999 so what they suggest in this particular paper is that for n equals to four that means that it's a four-point algorithm. They propose a linear four point algorithm where we have seen earlier on that since we have six polynomials. have a rank of uh three this is the same as solving ax equals to zero where now a here has arank of three the rank of a is at max going to be equals to uh just three so but we have a vector so x here is actually a vector of uh 5 by 1 which means that this has to live in a five-dimensional subspace. A system of homogeneous linear equations is under constraint but what we can do here is that we can take the last two columns that corresponds to b4 and v5 in the right singular. where we can solve for since we know that x equals to s1 squared we can solved for the final depth by taking the square root of x. Once s1 is solved we can back substitute s1 into the polynomial equation of f i j s i and s j equals to zero. We can then solve for the other unknown depth and finally we will get all the depth after we have gotten all the unknown depths we can do the same thing to to apply absolute orientation to recover the camera pose as in the grenade algorithm. The epmp algorithm mitigates the problem of the linear endpoint algorithm that was shown earlier on by quan and lun that was published in the year 1999 which has cubic complexity in the order of the number of points. The objective is actually to find out the relative transformation between the camera frame and the world frame. We'll first look at the case where these four control points are non-coplanar control points this means that these four Control points will not lie on a plane. We will then use the same relation for the 3d points in the camera coordinates. Every entry here every row here will give us one constraint one equation so all together we have four equations and four unknowns which is in terms of the alpha over here and we can solve for alpha 1 alpha 2 alpha 3 alpha 4 for every single point. The only set of unknowns that remain would be the control points in the camera frame denoted by cj uh to the superscript of c where j here equals to uh 1 2 3 and 4. Since each point correspondence gives us two independent equations this simply means that we need to stack them up into a matrix to solve for the 12 unknowns in total. the the points so just now we say that we are solving for beta 1 beta 2 and so on using the constraints the fixed distance between any two uh control points now we are reduced to only three constraints from the six constraints that we have earlier on. Once we know this we will be able to solve for the rotation and translation between this two sets of 3d points they are actually the same 3D points that is defined in two different reference frames. The complexity is a linear in terms of the number of points which is much easier to compute. forms a plane then this is also a degenerate case and that's the end of today's lecture thank you mx equals to zero. Mx is the letter that starts with the letter "m" and ends in "x" mx is also the letters that start with the letters "n" and "o" mX is the word that begins with "n", and ends with the word "n". mx means "one" and n is the number of a person or object.

ROUGE-1: 16.45, ROUGE-2: 15.31, ROUGE-L: 15.09
BERTScore: 69.40

==============================================
==================== [33/100] ====================
Summary:
Markus Klute: If there is a time dilation effect due to gravitational fields, then there's also a redshift which is of gravitational fields. He asks you to estimate the magnitude of this effect. Klute says the speed of light is pretty fast, 3 times 10 to the 9 meter per second. And this distance is only 22 and 1/2 meters, so we find that this is a tiny, tiny,tiny effect, he says. But nevertheless, experimentalists at Harvard tested this effect in the 1950s and '60s.

ROUGE-1: 39.72, ROUGE-2: 36.58, ROUGE-L: 39.72
BERTScore: 71.50

==============================================
==================== [34/100] ====================
Summary:
During the semester we have a few recitation instructors they help with the students during the recitation section. During those sections your the the instructor will solve a similar problem like what is actually covered during the same during the lecture and that give the students another chance to look at more example and to get for media will get used to the calculation which we carry how for the first time during the the lecture. We did not record the Recitation sections during the fall semester in 2016 on the other hand we included problem-solving videos from Professor with Busha.

ROUGE-1: 67.77, ROUGE-2: 66.22, ROUGE-L: 67.77
BERTScore: 85.45

==============================================
==================== [35/100] ====================
Summary:
In this video, we're going to compute some useful quantities for the exponential random variable. The CDF of x is the probability that X is less than or equal to little x. We use the standard formula, which is minus infinity to infinity t times fx of t dt. For this, if you evaluate the balance, 0 makes this 0, and 0 to get 1 over. And so the expectation is 1 over lambda, and so the variance is part c, so OK, so far so good. The limit as x goes to infinity-- the exponential will beat x squared. We're asked for the PDF of z, which is the max of x1, x2, and x2. The probability of the max being less than or equal to z is actually also the probability of each of these random variables individually being less. than orequal to z. So these two events are equivalent and this is true. By independence we can break this up. And we get-- all CDFs of the exponential and they all have this form.

ROUGE-1: 16.52, ROUGE-2: 15.03, ROUGE-L: 16.24
BERTScore: 68.43

==============================================
==================== [36/100] ====================
Summary:
Professor: Can you explain the physical significance of the crystal momentum? Professor: It's the thing that commutes with p or with x by i h bar. Professor: All the intuition you have about momentum, you can translate into intuition about the spatial variation of the phase of the wave function. The momentum is shifted by each h bar k, p naught plus h bar p l. The information about the momentum can be encoded in these spatial variations of thePhase of the Wave Function. Crystal momentum is defined from beginning, from the following property. If we have a potential v of x, which is invariant under shifting, by one lattice spacing, by some l, v of X, then this tells us that the energy operator is. invariant if we shift by l. And from this fact, we deduced via block or a la block, that the wave functions are really the energy eigenfunctions, can be written in the form e cubed is equal to e to the i q x, u of x. In describing a particle, we should use Cartesian coordinates, or should we use Spherical coordinates? Well it can't possibly matter. And so you'd better make sure in any description of your system that changing your coordinates doesn't change your results. And here, that's exactly what's going on. And that's the difference. The difference is that when you fold them up, you're imposing this periodicity and you're labeling the eigenfunctions by q and the overall number of k phases that you're subtracting off. the imaginary part, h bar over 2 m i. Well, hbar over m times the imaginary part of SI complex conjugate derivative, with respect to x, which is the current, in the x direction of SI. And we need this to be imaginary, or we will get no current. You show this in a problem set, if you have a pure, real wave function, for example. A single real exponential, that's decaying, as on the wrong side of a barrier. There isn't a single energy for a given value of q. In fact, the set of energy eigenvalue is as many as there are integers. So to specify a state, I don't just have to specify q NAUGHT, I also have to. specify N. And when you unfold this into the parabola picture, remember where these came from. These came from these curves. Came from shifting over. And the higher up you go, the more you had to shift over. E as a function of k is always going to look like that, but k is not a well-- so what is k? K is just defined as h bar squared, k squared upon 2 m is equal to e. So this doesn't tell you anything about which states you're at. It just that given an e, there's some quantity that could define k. This is a definition of k, in terms of e. What this diagram is telling you is which e's are allowed. The question is could you like re-explain how imperfections and a lattice leads to actual conduction? we just give up on quantum mechanics and say it totally failed? And so this is a totally reasonable question, and I want to emphasize something important to you. That model led to a prediction, which is that if you put a capacitor plate across a perfect crystal, then you would get no current flowing across. And that is manifestly what happens with copper. But the experimentalist comes back to you and says look dude. That is a ridiculous model because the copper isn't in fact perfect, it's messy. Professor: We know that it's true, but we want to see it. We want to feel it, so various people around the world are working on making a truly beautiful demonstration of this bit of physics. The question is really a question about quantum computation, which we'll come to next week. But, the basic question is how robust is this. And the answer is it's not robust at all. But which you can tell because everything in the real world has enough impurity that it conducts. situation, it depends on the system. And exactly how it depends is something that is an active area of research. So we know that these block oscillations are true. We see them in all sorts of different systems that are analogous. But it's one that turns out to be surprisingly difficult to tease apart. So don't throw away the model. Observe that you've modeled the wrong system. If you find a system that fits your-- that is-- that shares the assumptions of your model, that's when you ask did it work. down here in the ground state, and looking back at that band, we know that the band for that ground state looks like this. So, here it is. There's our electron. It's sitting in the lowest energy eigenstate. Is it moving? Well, it's in a stationary state. Is the expectation value of the position changing in time? No. It has some meaningful, well defined time variation of its position expectation value. In order to induce the current, I must put the electron into a higher energy state and in a particular superposition of higher energy states. In order for the light to scatter off the crystal, you must have electrons in superposition states so that they can have a dipole and absorb and radiate that energy. In order for that to happen, the light has to excite an electron across the gap. It has to give it this macroscopic amount of energy. So we have these two materials which one has the larger band gap? Diamond, because it's transparent. At in the visible. The band gap, delta e of diamond is much larger than the band gap for copper. Diamonds have to have a band gap. It must be such that when you fill up all the electrons you need for it to be neutral, there is a gap to the next energy states. You can't have a partially filled band if each band comes from a bouncy, in a single well, and each well comes with an integer number of electrons. You just-- you're stuck. In 3D, this isn't such a big deal, because those splittings are tiny. But in 1D they can't. In three dimensions, the gaps are not the same. That they do not remain constant. There's one that changes, one that doesn't. And then this guy has five. One, two, three, four, five. The story changes in a dramatic way. The deformation is where you have a sphere and you stick out your arm. So it's no longer symmetric top. So here we can have states crossing. And there's no nodes here in three dimensions. Because there's nothing preventing states from different-- in different multiplates from having the same energy. When we filled those first two bands, well, there is the first-- so the first band is now filled. And back here , we had an insulator because we had filled bands separated by gap. The gap between the filled band and the next available band. This is actually called a band insulator. Because there are other ways of being an insulators. And this is why Matt wasfing at me. So the answer is that there isn't one answer to that. But what we need is the non-zero band. It's just not a generic thing from what we've done. And it's really not for spin-less systems. On the other hand, accidental small gaps. Easy. That happens. That certainly happens. So that brings me to the last thing I wanted to talk about before getting to entanglement. So what happens to a system which is neither conductors nor insulators? They are reasonably good conductors and reasonably bad insulators. But they're not perfect. And these materials are called semiconductors. small number. And e to the minus of a small number is close to 1. So at high temperature, you're very likely to excite electrons up here. This is called a semiconductor. And there are notes on the Stellar. web page that discuss in a little more detail what I just went through and show. you how you build a transistor out of a semiconducting. OK. So that's it for band gaps. And I want to move on to the remainder, the last topic of our course. Which is going to be entanglement and quantum computation. The probability of finding the particle at point A is given by chi a squared. And similarly, the probability that we find the second particle at b is this thing norm squared. But we also studied the symmetric configuration, which was equal to 1 over root phi, root 2. This should be a correct description of many experiments, this should be totally awesome. But it's deeply disconcerting, because I could've taken these particles, put them in this entangled state, and sent one particle off to a distant planet. And my sister measures this second particle and determines what state it's in. yet another moment of serious discomfort. How can something here dramatically change the state, the configuration, the initial configuration, of a particle arbitrarily far away? Isn't that deeply concerning? And if you think about relativity, this should be all the more deeply disconcerting. So there was a person that roughly this time, a little earlier, who was troubled by this problem. And his name was Einstein. And so one of the things that's kind of amazing is that he created a thought experiment which we're going to study in detail next week. The physicist Bell had a lovely way to describe entanglement. Bell used to say, if you saw Bartelstein and you could only see one leg and that sock was pink, you knew to a certainty that the other sock was not pink. If you have a coin and you cut it in half down the-- so you've got two coin shape disks. You cut the disk in half, not-- and you have one side that's the head. And they're separated. And one gambler tries to cheat the gambling establishment by tossing in his half coin. Einstein, Podolsky, and Rosen argued that the EPR paradox suggested that quantum mechanics was incomplete. They said that if you can perform a measurement, you know that quantity absolutely. But you can't do the-- so on the one hand, quantum mechanics says you can’t know physical reality to this level of precision. And on the other hand, the fact that you can do that measurement violates the relativistic picture of reality. But the question is whether or not the framework of quantum mechanics is somehow unsatisfactory in any formal sense. In 1905, Einstein publishes The Quantum Theory Of Light. By 1935, 10 years later, he's still a relatively young man. He nominated Schrodinger and Heisenberg for Nobel Prizes twice. He was aesthetically incapable of pursuing this new physics in the ways that were possible under the time that I would leave Physics. But he wasn't stupid. He had the intellectual capacity to leave the time with the possibilities of the new physics. And that's what you would do if you were Albert Einstein.

ROUGE-1: 23.70, ROUGE-2: 22.74, ROUGE-L: 22.25
BERTScore: 61.92

==============================================
==================== [37/100] ====================
Summary:
First up the proper way to chop fresh herbs to get maximum flavor chopping herb the secret is to chop them not bruised them now basil this is a soft herb so treat it with some respect when people go mad chopping herbs all the goodness comes out on the board. Tip to get the flesh out of a Kiwi this is simply cut the fruit in half and scoop out with a teaspoon try it it really works I absolutely love these peppers now they have the most amazing sweet delicious flavor with a really nice crunchy.  texture and the most exciting thing about the peppers is that they're just as delicious raw or cooked to identify the perfect pepper must be smooth and firm and not a wrinkle inside. A great tip to check if a pineapple is ripe is to pull a leaf out from the top if it comes away easily it's ripe and ready for slicing a great tip for getting meat or fish to cook faster is to score it which allows the heat to penetrate quicker this also allows mayonnaise to be absorbed more deeply.

ROUGE-1: 38.88, ROUGE-2: 38.35, ROUGE-L: 38.88
BERTScore: 63.37

==============================================
==================== [38/100] ====================
Summary:
hey everyone it's sth register nurse rn.com and in this video we're going to be going over our weekly inlex practice question. Let's see what our question says a patient who has a health history of uncontrolled hypertension coronary artery disease and diabetes militis is prescribed to take propanolol. The question asks which statement by the patient is correct about this medication propano wal that they're Going to be taking. The answer is that the patient should take the medication every morning with grapefruit juice and monitor their blood glucose level closely.

ROUGE-1: 12.51, ROUGE-2: 10.83, ROUGE-L: 12.12
BERTScore: 65.52

==============================================
==================== [39/100] ====================
Summary:
This is part 2 of a guide to clinical reasoning or how to create an accurate differential diagnosis from a patient's presentation. The patient is a 75 year old woman presenting with epigastric pain for four hours. She had moderate nausea it has refused to attempt to eat or drink anything since the pains onset because she is concerned that it will concert a vomit which she has not yet done. She denies changes in her bowel habits shortness of breath chest pain changes her skin or eye color. Her past medical history is notable for hypertension and diabetes she has had no surgeries. The patient is an elderly woman a piercer stated aged in moderate discomfort secondary to abdominal pain. Her husband is concerned about contamination from a new well that was drilled on their property for drinking water one week ago. The patient is a 75 year old woman with abdominal pain for four hours it is epigastric well localized progressed over 45 minutes is constant with no exacerbating or alleviating factors and associated with nausea from the rest of her history. She has hypertension diabetes moderate alcohol use smoking and is drinking water from a newly drilled well. well on exam she is MA in moderate distress she has a borderline temperature tachycardia and tachypnea. She has a non distended abdomen that was soft with no rebound severe epigastric tenderness and guaiac negative. The key test results are a white blood cell count to 15 normal basic metabolic panel mildly increased indirect bilirubin modestly elevated lipase normal troponin and CK and an EKG with only sinus tachycardsia. The epigastrium would obviously be the most critical anatomic region to include organs that physically lie directly underneath it. Pancreas and small bowel diseases of the stomach which caused acute abdominal pain are many but most commonly are gastritis and peptic ulcer disease in the pancreas. The major structures are of course the liver and gallbladder along with other components of the extra hepatic biliary system. The spleen can cause pain with either a splenic infarct or asplenic abscess. pain from a pulmonary embolism can be referred to the abdomen although it more typically is to the right or left upper quadrants and not the epigastric now that we have a framework we move on to the final and hardest step applying the key features to that framework so here is our framework once again and here are our key features how does one start this process the brute force method would simply be to take each diagnosis listed one at a time and review each individual key feature to decide if it impacted the probability of the diagnosis and what that impact was. weight on the lack of positional component to the symptom how about food poisoning and gastroenteritis and so we are on the same page most American doctors use the term food poisoning to refer to consumption of food terminated with preformed bacterial toxins such as that from staph aureus or bacillus cereus we use gastroEnteritis to refer. to a condition when the patient has actually contracted an active infectious disease of the gut. in developed countries is usually viral occasionally bacterial and only rarely protozoan keep in mind that this distinction between the two diagnoses may be different in your geographic region. particularly right upper quadrant tenderness along with elevated alkaline phosphatase although it's not enough to make the diagnosis likely enough to seriously consider at this point acute cholecystitis has been described as a cause of elevated lipase. The patient has no elevation of direct or conjugated bilirubin that is no evidence of jaundice the patient is simply too sick appearing for this to be biliary colic plus as it's very name implies the pain from this usually waxes and wanes instead of being constant. step four and are subsequently using now so even though we did not use the guaiac negative stool here in step five it was still an important element of the presentation and definitely worth mentioning. The last unused key feature is the most interesting it's the history of the newly drilled well depending on where you are in your training this probably did not occur to you but the combination of acute abdominal pain nausea and a possible contamination of well water is all consistent with heavy-metal poisoning specifically arsenic and lead. The most likely diagnosis for this patient is acute pancreatitis. that will increase the likelihood of. establishing the correct diagnosis sooner in outpatient hospital course you. That will. increase the chance of establishing the right diagnosis sooner. that will increase. the likelihood that you will be seen by a doctor sooner in hospital. that you. will be more likely to be admitted to hospital for treatment. that. will increase your likelihood of being seen by doctors sooner in the hospital. and that. you will have a better chance of being treated. of being diagnosed with a condition that is more serious.

ROUGE-1: 30.43, ROUGE-2: 28.14, ROUGE-L: 28.28
BERTScore: 62.25

==============================================
==================== [40/100] ====================
Summary:
Danqi Chen is one of the foremost researchers in question answering. She is the professor at the Princeton University. Danqi once upon a time was the head TA of CS224N. She's quite familiar with the context of this class. So today I'm very happy to introduce to you some of the fundamentals in this field, as well as on cutting edge and state of the art topics. So here's my plan for this lecture. I'm going to spend the most of this lecture focused on one type of question answering problems called reading comprehension. like 10-ish minutes to talk about a more practical, and in my opinion, more exciting problem called open domain question answering. Question answering, or, let's say QA in short, is one of the earliest NLP tasks, and the early systems can even date back to the 1960s. And, the question and answer has enabled a lot of really useful real world applications. For example, today if you just put your question in a search engine like Google, you can actually click on the correct answer, which is actually our concise answer. Question answering is probably one of those fields that we have seen the most remarkable progress in the last couple of years driven by deep learning. So in this lecture, I will be mostly focusing on the text based, or textual question answering problems. Another class, bigger class of the question Answer problems is called visual question answering. So if you have interest in these type of problems, I encourage you to check out those problems, but I'm not going to dig into these problems today. So next, I'm going to start with a part 2, reading comprehension. Reading comprehension has been viewed as a very important test bed for evaluating how well computer systems understand human language. This is really just similar to how we humans actually test the reading comprehension test to evaluate how well we actually understand one language. So this is also the way that we actually post questions to test the machine's language understanding ability. It actually has been formally stated back in 1977 by Wendy Lehnert in her dissertation. She says that, since questions can be devised to query any aspect of text comprehension, the ability to answer questions is the strongest possible demonstration of understanding. is actually called a semantic role labeling. So basically try to-- given one sentence, given one word, "finish" trying to figure out who did what to whom and when and where. So by converting all these kind of semantic role relations, we can also just apply the reading comprehension problem and give you the correct answer. So this is actually a very interesting perspective, that reading comprehension can be actually very universally useful to many other tasks. So next, I'm going to introduce this Stanford Question Answering Dataset called SQuAD. Stanford, so it's called Stanford Question Answering Dataset. Today, after four or five years now, so SQuAD still remains the most popular reading comprehension data set. So it's actually very clearly a high quality dataset, but is also not a very difficult dataset. So today, basically the SQuad dataset itself has been almost solved, and the state-of-the-art already exceeds estimated human performance. Danqi, one question you might answer is, so if you can do other tasks like named entity recognition or relation extraction by sticking something on top of BERT. method work better and by how much? That's an interesting question. So the kind of state-of-the-art AER systems still have time to just train or sequence tagger on top of the word. So I want to draw some connections between the machine translation problem and the reading comprehension problem because really share a lot of similarities. So next, I'm going to talk about how to build neural models for reading comprehension, and in particular, how we can build a model to solve the Stanford Question Answering Dataset. BiDAF stands for Bidirectional Attention Flow for Machine Comprehension. It was proposed by Minjoon Seo and other folks in 2017. It remains one of the most popular reading comprehension models and achieved a very good performance at that time, at least on the SQuAD data set. So next, I'm going to just dissect this model layer by layer and talk about what this layer is actually doing and how we can really build this model from the bottom layer to the top layer. the gi's input and the output will be on the mi, which is another 2H dimensional vector for each context word in the passage. OK. So the final is the output layers. So final output layers are basically just two classifiers just trying to predict the start and end positions. And they also have another classifier to predictions the end position of the answer. OK, so this model is actually achieved-- like on SQuAD data set, it achieved a 77.3 F1 score. They found that both attentions in two directions are actually important. If you remove the context-to-query attention, the performance will drop to 67.7 F1 score. And then if you remove this part, it will drop a 4-point F 1 score. So basically this theory tells us that these kind of attention scores can actually capture those negative scores pretty well. OK, so next, I'm going to talk about BERT, how to use the BERT model to solve this problem. BERT is basically a deep bidirectional transformer encoder pre-trained on large amounts of text. And it is trained on the two training objectives, including masked language modeling and the next sentence prediction. BERT models are pre-trained while BiDAF models only builtd on top of the GloVe vectors. Pre-training basically can just change everything and it gives a very large boost in performance. Even if you use a stronger pre-training models or modern, like a-- stronger models than the BERT models, they can even lead to better performance on SQuAD. And then finally, if you see even the latest pre- trained language models, including the XLNet or RoBERTa or Albert, these models can give you another like 3, 4 F1 score. is another paper that actually just came out in 2020. So there has to be a lot of evidence showing the similar things. So today we compute a very good reading comprehension data set on the individual data sets. But these systems trained on one dataset basically cannot really generalize to other datasets. And all the other numbers in this table basically shows that if you train one system on one datasets and then evaluate on another dataset, the performance will drop quite a lot. So it basically really cannot generalize from one dataset to another dataset. very useful in the practical applications. So the term here open domains is just in contrast to closed domains that deal with questions under a specific domain. The idea is that let's take a question-- OK, so here, the article is trying to answer questions using a very large collection of documents such as the Wikipedia. So we can just decompose this problem into, as I just mentioned, in our retrieval and the reader component. And this reader model is basically trying to read through all the documents that this retrieval return and try to find out the correct answer. Danqi can stay for a bit to answer questions, but not forever. Because she doesn't have a Stanford login, we're going to do questions inside Zoom. If you'd like to ask a question, if you use the raise hand button, we can promote you so that you appear in the regular Zoom window and can just ask questions and see each other. There are now four people who've been promoted. OK. So this is my answer, is that if we can leverage a very large and very powerful pre-trained language model, there is a possibility that we can actually do the question answering well with only a small number of examples. Most existing question answering datasets or reading comprehension datasets have been collected from Mechanical Turk. So it is very difficult to avoid some kind of artifact though, like a simple clues or superficial clues. So that's the reason that more specialized models that have been trained very well in one data set, it's very easy to pick up these kind of clues, and is very hard to generalize this kind of thing to another data set. Natural Questions was a dataset that Google put out about a year and a half ago maybe where they were actually taking real questions from Google search logs. The goal of this project is trying to ingest all the phrases in the Wikipedia. So these conversations are built using the training set of the question answering datasets. By using end to end, we apply this encoder to all the phrase, like 60 billion phrases in this region. So the model is definitely able to generalize from theTraining set to all of the Wikipedia phrases. And then we can use text as the representation, can actually generalize well for the unseen questions. It doesn't have to have seen the phrase then. Is this what you were asking? I see, OK. In context learning can help models to be more robust with respect to different domains. Do you think that in order to solve NLP in the sense that you can perform on par with humans on all NLP tasks it's sufficient to only interact with text data? Or do you think we'll eventually need the sort of experiences and common sense that we get only from seeing and viewing the world and having a set of interactions that we as humans have? Yeah. I mean, common sense is a very difficult-- even in the context question answering. The next question is from Danqi, who was one of the co-organizers of the EfficientQA task. Danqi is too modest to mention that she was also a member of the DensePhrases project. Her question is how concerned should we be about potential biases into these record labels or how we evaluate them, or is that just more of a concern for more open ended questions? Yeah, I'm not sure if I really have the answer, but I also want to quickly mention that quantization has been definitely a very useful technique to make the model smaller. yeah, I guess I'm just a little worried about who comes up with the test cases? Who determines what the right answer is? I mean, we will have more discussion of toxicity and bias coming up very soon, including actually Thursday's lecture as well as a later lecture, not specifically about QA though. OK. Next person is-- Thank you for the lecture. Yeah, my question is also related to the open domain question answering. So I was just wondering how much of the learning side of domain sort of generalization or domain alignment techniques can be combined with language level, like question answering? The key difference between the generative model and the extractive model is that for generative models, you can actually leverage more input passage together and do the generation. So for this model, there isn't any retrieval. So you can always find the answer from the question, right? So this model really has you relying on all the parameters you memorized, all the questions you've answered. And then generative Models are you're remembering the whole question and you try to retrieve the memory when you answer the question. The model is very large, like 11 billion parameters. So the parameters are basically trying to memorize a lot of information that has been.information. So by just taking this input, it has to just rely on the parameters to infer this answer. So it's actually very hard to-- yeah, it's a definite balance between memory and the generalization from it. All right, thanks. Do you want to call it a night or do you want one more question? Either way, yeah.

ROUGE-1: 21.63, ROUGE-2: 20.87, ROUGE-L: 20.29
BERTScore: 61.30

==============================================
==================== [41/100] ====================
Summary:
 homework two is out now. What the default projects will be, uh, for this class. Um, and you guys will get to pick whether or not you wanna do your own construction project or the default project. And those proposals will be due, um, very soon, er, in a little over a week. Are there any other questions that people have right now? Yeah. The assignments, [inaudible] are they limited to TensorFlow? asked the question if, if the assignment is limited toTensorFlow. I'm pretty sure that everything relies that you're using Tensor Flow. Last week we discussed value function approximation, particularly linear value function approximations. Today we're gonna start to talk about other forms of valuefunction approximation in particular, um, uh, using deep neural networks. We're mostly not gonna talk so much about enormous action spaces, but we are gonna think a lot about really large state spaces. And so, instead of having a table to represent our value functions, we were gonna use this generic function approximation where we have a W now, which are some parameters. Deep neural networks represent the Q function. linear value function is often really works very well if you're the right set of features. But there are all sorts of implications about whether or not we're even gonna be able to write down the true p- um, value function. So, one alternative that we didn't talk so much about last time is to use sort of a really, really rich function approximator class. The problem is, um, that the number of data points you need tends to scale with the dimension. The chain rule is used to try to do stochastic gradient descent. It means that you can take our output signal and then propagate that back in terms of updating all of your weights. So, you can use the chain rule to propagate all of this the- the gradient of, um, your loss function with respect to w, all the way back down all of these different compositions. The common choices are either linear so you can think of hn is equal to whn minus one. If it's nonlinear, we often call this an activation function. These sort of non-linear activation functions can be things like sigmoid functions or ReLU. thing is that, um, if you have at least one hidden layer, Um, if we have a sufficient number of nodes. nodes you can think of as a- if you're not familiar with this is basically just sort of a sufficiently complicated, uh, combination of features and functions. Um, this is a universal function approximators which means that you can represent any function with the deep neural network. So, that's really nice. We're not gonna have any capacity problems if we use a sufficiently expressive function approximation. instead of saying, "I'm going to have totally different parameters each taking in all of the pixels." I'm gonna end up having sort of local parameters that are identical and then I apply them to different parts of the image to try to extract, for example, features. So, the point of doing this is gonna be trying to extracting features that we think are gonna be useful for either predicting things like whether or not a face isn't an image or that are gonna help us in terms of understanding what the Q function should be. Deep learning can be used to detect different features in an image. It can do this by applying different weights to different parts of the image. You can also do this for multiple different types of features. There's a really nice, um, discussion of this that goes into more depth from 231-n, which some of you guys might've taken. Um, and there's a nice animation where they show, okay, imagine you have your input, you can think of this as an image, and then you could apply these different filters. as pooling layers. They are often used as a way to sort of down-sample the image. They can do things like max pooling to detect whether or not a particular feature is present, um, or take averages or other ways to kind of just down, ah, and compress the, the information that you got it in. In general, as soon as we start doing this function approximation even with the linear function approximator, then you can start to have this, uh, challenging triad, which often means that we're not guaranteed to converge.  RL is using deep function approximators to do Atari games. Atari games are often hard for people to learn. In these games, you typically need to have velocity. So, because you need velocity, you need more than just the current image. So what they chose to do is, you'd need to use four previous frames. So this at least at least, they're gonna use a particular input state. The state is just gonna be the full image. The action is the equivalent of actions of what you could normally do in the game. allows you to catch for a velocity and position, observe the balls and things like that. It's not always sufficient. Can anybody think of an example where maybe an Atari game, I don't know how many people played Atari. Um, uh, that might not be sufficient for the last four images still might not being sufficient or the type of game where it may not be. Yeah. Microbes exactly right. So like things like Montezuma's Revenge, things we often have to get like a key and then you have to grab that key. So you have. to sort of remember that you have it in order to make the right decision. approximators act. And the nice thing is that, I think this is actually required by nature. So you can play around with this. So how did they do it? Well, they're gonna do value function approximators. They're going to minimize the mean squared lost by stochastic gradient descent. Uh, but we know that this can diverge with value function approximation. And what are the two of the problems for this? Well one is that there is this or the correlation between samples. considered one update to take a tuple and update the weight. It's like one stochastic gradient descent update. Now notice here because your Q function will be changing over time. So, even though we're treating the target as a scalar, the weights will get updated the next round which means our target value changes. So this is nice because basically it means that you reuse your data instead of just using each data point once, you can reuse it and that can be helpful. learn to model, a dynamics model, and then the planning for that which is pretty cool. But we don't wanna do that all the time because there's a computation trade-off and particularly here because we're in games. There's a directtrade-off between computation and getting more experience. Can we use something similar to like exploitation versus exploration. Um, essentially like with random probability just decide to re-flag [inaudible]. The question is about how would we choose between, like, getting new data and how much to replay. that it does is it has fixed Q targets. So, what does that mean? Um, so to improve stability, and what we mean by stability here is that we don't want our weights to explode and go to infinity which we saw could happen in linear value function. Um, we're gonna fix the target weights that are used in the target calculation for multiple updates. We're gonna have, um, we still have our single network but we're just gonna maintain two different sets of weights for that network. In terms of stability, it helps because you're basically reducing the noise in your target. If you think of this as a supervised learning problem, we have an input x and output y. The challenge in RL is that our y is changing, if you make it that you're- so your y is not changing, it's much easier to fit. Dian's question is whether or not, um, we ever update the- Minus at all, or is that [inaudible]. Great question. w minus, yes we do. We pu- can periodically update w minus as well. So, in a fixed schedule, say every 50 or, you know, every n episodes or every n steps, you, um, update sort of like every- and you would set w minus dw. William? Uh, we notice, like, for w, there are better initializations than just like zero, uh, if you take into account, I guess like the mean and variance. Uh, would you initialize w minus just two w or is there like an even better initialization for w minus? It stores the transition in this sort of replay buffer, a replay memory, um, use sample random mini-batches from D. So, normally sample in mini-batch instead of a single one. You do your gradient descent given those. Um, you compute Q learning using these old targets and you optimize the mean squared error between the Q network and Q learning targets, use stochastic gradient descent, and something I did not mention on here is that we're typically doing E-greedy exploration. a lot, like, it wasn't sure of its movements like it moved around places often, like [OVERLAPPING]. Yeah. From the agent's perspective, particularly if there's a cost to moving, then it may just be kind of babbling, uh, and doing exploration just to see what works. So, there's been a lot of interest in these sort of games on the bottom end of the tail which often known as those hard exploration games. We'll probably talk- uh, we'll talk a lot more about exploration later on in the course. Replay is hugely important and it just gives us a much better way to use the data. Using that fixed Q here means you seem like a fixed target. You do replay and suddenly you're at 241. Okay, so throwing away each data point what- after you use it once is not a very good thing to do. Um, and then if you combine replay and fixed Q you do get an improvement over that but, uh, it's really that you get this huge increase, um, at least in break out in some of the other games by doing replay. Doubled DQN is kind of like double Q learning, which we covered very briefly at the end of a couple of classes ago. The idea was that we are going to maintain two different Q networks. This is to try to separate how we pick our action versus our estimate of the value of that action to deal with this sort of maximization bias issue. The second thing is prioritized replay. It turns out that it gives you a huge benefit in many, many cases for the Atari games. So, uh, this is something that's generally very useful to do. back to the Mars Rover example. So, let's say you get to choose two replay backups to do. Vote if you think it matters which ones you pick, in terms of the value function you get out. If you pick backup three, so what's backup three? It is, S2, A1, 0, S1. So that means now you're gonna get to backup and so now your V of S2 is gonna be equal to one. So you've got to back-propagate from the information you're already [NOISE] have on step one to step two. The number, of, um, updates you need to do until your value function converges to the right thing can be exponentially smaller, if you update carefully and you, you could have an oracle tells you exactly what tuple the sample. But you can't do that. You're not gonna spend all this. It's very computationally, expensive or impossible in some cases to figure out exactly what that uh, that oracle ordering should be. But it does illustrate that we, we might wanna be careful about the order that we do it and- so, their, intuition, for this, was, let's try to prioritize a tuples according to its DQN error.

ROUGE-1: 23.06, ROUGE-2: 22.18, ROUGE-L: 21.82
BERTScore: 65.80

==============================================
==================== [42/100] ====================
Summary:
 salt makes up a tiny part of any bread though which has a huge effect on it and most bread is made with salt nowadays. salt has a tightening effect on the gluten it strengthens the dough and makes it more cohesive as yeast consumes the sugars in the dough. salt helps with controlling fermentation it draws moisture through the cell walls of yeast in a process called osmosis. salt can help with preserving the color and flavor of flour unbleached flour has carotenoid pigments which give the crumb of our bread the creamy color and a wheaty aroma. it's rising more slowly whilst the one on the left is already collapsing. the one in the right is still pushing on. what did you think of this experiment did you learn something new let me know down in the comments see more videos like this one click right here that's all i have for you today thank you for watching i'll see you in the next one. i'll be back with a new video in a week or so. I'll let you know what it's about.

ROUGE-1: 16.29, ROUGE-2: 14.67, ROUGE-L: 14.40
BERTScore: 58.13

==============================================
==================== [43/100] ====================
Summary:
In this section, we're going to talk about the relativistic Doppler effect. We make good use of our space-time diagrams, which we discussed earlier. The question is how is this being observed by an observer which is moving with a relative velocity v with respect to the source? So we have to apply Lorentz transformation. And we find then-- this is a little bit of an algebra exercise here-- that the period now is given by 1 plus beta over 1 minus beta square root of that times tau.

ROUGE-1: 32.62, ROUGE-2: 31.29, ROUGE-L: 32.62
BERTScore: 72.64

==============================================
==================== [44/100] ====================
Summary:
Hollywood Legend Will Smith owns a team in the new E1 racing series that aims to prove the potential of electric power in the Marine industry. The nine teams have the same boat but they're working out how to push the tech and try to get ahead of the competition. The boats can reach 50 knots that's around 93 km per hour so how do they reach those speeds? The key bit here is getting up on the thin bits of the foil and staying above the water to have the speed that's right. attracted some big name investors despite only being in his first year. There are challenges ahead can it keep those celebrity backers and can it build a big audience for this high-tech reing. I think the ground workor for this Championship is amazing you know the names behind it now are incredible. I've heard some good rumors of teams coming in for next year as well so I Think the format is really exciting I think a lot of people are really interested in excited about the new technology and also the sustainability message.

ROUGE-1: 20.42, ROUGE-2: 19.60, ROUGE-L: 18.31
BERTScore: 60.69

==============================================
==================== [45/100] ====================
Summary:
Marginal rate of substitution of good 1 with respect to good 2 is infinity. When we are increasing the amount of 1 good to bring what will happen using this, if we use the monotonicity. What we are talking about is an indifference curve of a person who exhibit, whose preference exhibits convexity. To get 1 good you will have to give up the other good, both what we are assuming that both these items are good, means they give certain satisfaction or certain you know utility to the person. good 1, how much the other person is willing to give up, the another good, but we have to measure in terms of per unit of good 1. So, that is why in that case MRS is going to be, let us say in other word, let's see if we just do it mathematically, the original bundle is x y. Change is x plus delta x and y plus delta y, and what would be the slope, what we are certain about that.

ROUGE-1: 24.44, ROUGE-2: 23.62, ROUGE-L: 23.16
BERTScore: 63.36

==============================================
==================== [46/100] ====================
Summary:
in this video we're going to discuss what externalities are in economics. An externality is when you do something that affects the well-being or the good of another person or a company but you're neither harmed or rewarded for what you did to that person so the externalities can be positive they can be negative. A negative exTERNality is where you've harmed someone you've done something to somehow impose a cost on someone or some some company or something and you haven't reimbursed that person. it's as a tangent it's actually helping some other person or people and so but those people aren't turning around and compensating you for it right so you're doing something good uh you're you're helping yourself but it has this side benefit of it helps other people as well. If you were actually paid if people said hey I really like what you did you might be more likely to get a flu shot. If your neighbor is trying to sell their house they have a for sale sign up they might appreciate if you went out and really did a great job maintaining your home. where you have a negative externality like pollution or something like that it would be overs supplied relative to what is socially efficient or optimal. Where you have an over-supply of something like pollution, for example, you would have it over supplied in a way that would be socially efficient and optimal. For example, if you have pollution, it would have to be over supplied to avoid it becoming a problem. This would be a way to reduce the amount of pollution or other negative externalities in the environment.

ROUGE-1: 39.46, ROUGE-2: 33.79, ROUGE-L: 32.45
BERTScore: 66.51

==============================================
==================== [47/100] ====================
Summary:
Political philosophy is the oldest of the social sciences. It can boast a wealth of heavy hitters from Plato and Aristotle to Machiavelli, Hobbes, Hegel, Tocqueville, Nietzsche. The study of the great books or great thinkers of the past can easily degenerate into a kind of antiquarianism, into a sort of pedantry, says Professor Steven Smith. But these works provide us with the most basic questions that continue to guide our field, he says. "The ideas of John Maynard Keynes, both economists and political philosophers, when they are right, are more powerful than is commonly understood," says Smith. A regime indicates a form of government, whether it is ruled by the one, a few, the many, or as more common, some mixture of these three ruling powers. Regimes are necessarily partisan, that is to say they instill certain loyalties and passions in the same way that one may feel partisanship to the New York Yankees or the Boston Red Sox. Fierce loyalty, partisanship: it is inseparable from the character of regime politics. A regime is more than simply a set of formal structures and institutions, okay? It consists of the entire way of life, the moral and religious practices, habits, customs, and sentiments that make a people what they are. unnecessary or redundant. It would wither away. Political philosophy exists and only exists in that... call it "zone of indeterminacy" between the "is" and the "ought," between the actual and the ideal. This is why political philosophy is always and necessarily a potentially disturbing undertaking. Those who embark on the quest for knowledge of the best regime may not return the same people that they were before. But there is some compensation for this, I think. The study of political philosophy may be the highest tribute we pay to love.

ROUGE-1: 13.15, ROUGE-2: 12.10, ROUGE-L: 12.89
BERTScore: 61.27

==============================================
==================== [48/100] ====================
Summary:
Ahern: I have never had an exam where I had fewer questions. There were maybe 10 questions I got on the exam and that was for a class of this side. I find most students are honest. I've only had a handful of situations where in this class, where I've had dishonesty as an issue. The only students I've ever had with serious dishonesty issues actually were in smaller classes interestingly enough. But it has happened here and I do have taped evidence when it does happen. So that's why I do it. Ahern: How can you have two apparent Kcats? and the answer is it depends on how you calculate the concentration of the enzyme. If you take Vmax and divide it by the total concentration of enzyme, you will see a reduced Kcat compared to the uninhibited enzyme. When the exams go back, there will be a key outside my door. Ahern: Have you ever offered alternate formats like putting out a 5 part exam and saying choose the 4 you want to attack? be large and I can do what you recommend there. My concern is, well, what I always see is there's a bias. Everybody decides to do this question over here. And it seems to me that when I have that bias, it suggests that all of the questions aren't equal in difficulty. But I have considered it and I have done it on some occasions. And I won't rule it out. I always reserve that as an option, but I've shied away from it for that reason. Anybody hate the exam? You can say it. Ahern: I hope everybody got how I start my lecture. I don't live consciously, I just blurt it out. Thank you for your feedback. That's not a lot of feedback, but I do appreciate feedback and I'm always happy to listen to what you have to say about exam formats. And I do take suggestions. The suggestion about having other possible choices is one, as I said, I've done and I won't rule out. But other thoughts or feedback, I'm open to 'em, very much appreciate that. a few minutes talking about today. Using genetic techniques today, it's very easy to alter the genetic code for any of these proteases and change which amino acid is presence at any given place. Doing that, researchers have changed, for example, a serine residue of 221, which is the serine, gives it its name, to an alanine. Or changing the histidine position 64 to anAlanine or changing aspartic acid at position 32 to analine. And when they do that, and they compare the activity, so this is the log of Kcat. Neil Ahern: Are there studies done on [inaudible] such as this where you can replace the serine, histidine, or aspartic acid with something other than alanine and perhaps increase the function? Student: Yes, sir. Neil: His question is what if I mean if I go from serine to alanin, at least they look similar, but they're not chemically similar. What if I change theserine to a threonine for example? Threonine also has an hydroxyl group. The question is, would I see activity? and might it even be better? Ahern: There are other types of proteases that behave very much like S1, well like serine proteases. One of these classes is known as cysteine proteased. Ahern: This class of protease is essentially identical to that of theserine protease, at least for our level of understanding. He says the aspartyl proteases, at first glance, look somewhat different. But those similarities aren't all the way through like we see with the cystine prote enzymes, he says. One of these is an enzyme we've been talking about some already, that's the carbonic anhydrase. Carbonic anHydrase can catalyze the conversion of a million molecules of substrate into product per second, per enzyme. The basic mechanisms are the same. We created a nucleophile, the nucleophile attacks the carbonyl group, the peptide bond breaks, and the pieces go their way. This business of creating nucleophiles is not unique to proteases. There are other enzymes that use nucleophile and generation of nucleophile in their catalytic mechanisms. Most enzymes have a fairly narrow pH range where they work that's ideal. Most of our body tissues are at a pH of 7 to 7.4. At pH 9, this enzyme is far more active than it is at pH 7, indicating that a very, very important step is the removal of that proton. The faster the nucleophile can form, the faster the enzyme is going to be. If the enzyme structure is stable at pH 10, the enzyme will be better at 10 than at 7. Because it's easier to make that nucleophile at 10.  restriction enzymes are enzymes that many of you have used if you've ever worked in a lab that does DNA work. Restriction enzymes catalyze the breakage of DNA double strains and specific nucleotide sequences. A restriction enzyme is a protein. That protein grabs a hold of DNA. So many proteins will grab hold ofDNA. DNA. Saw before. So you really hope, I hope you're starting to see the themes now. Restrictions enzymes are also called restriction endonucleases, I'll take either one, that's fine. is a negatively charged molecule, we would expect that to grab a hold of DNA, perhaps positive or neutral, we certainly wouldn't expect the protein to be negatively charged because it wouldn't interact very well with DNA. The enzyme grabs a hold the DNA molecule and what it does is it literally slides down the DNA molecules. When the enzyme is bound to that proper site, the enzyme goes "oh, whoa!" and it bends. The DNA molecule is physically bent at that point. Now that bending turns out to be critical for the catalytic action.

ROUGE-1: 22.74, ROUGE-2: 21.51, ROUGE-L: 21.20
BERTScore: 64.18

==============================================
==================== [49/100] ====================
Summary:
AA and I are going to show you how to make vanilla extract so easy so yummy so good for all the things that you would use it for cakes whatever really really good isn't it. All you want for this is about 1 oz or roughly 30 G of vanilla beans so they'll be you know long Dobby whacker things and just cut them up into small pieces. Store this in a dry cool place and just as I said just shake once a day for at least a month preferably 2 to 3 months.

ROUGE-1: 32.65, ROUGE-2: 32.08, ROUGE-L: 30.61
BERTScore: 66.41

==============================================
==================== [50/100] ====================
Summary:
RAFAEL JARAMILLO: All right, let's talk about intermediate phases and line compounds. So I want you to recall intermediate phase in a three-phase system. And I'm going to recall it visually, and we're going to remember what the free-energy composition diagram looked like in such a case. All possible common tangents are going to converge at the same point. That point is x of B equals n. That's that one composition that we find in nature. The magnesium nickel system has a number of different phases. At high temperature, this Laves phase develops some width. It can be made as a solid solution with a very, very narrow range of solid solubility. But when you drop down to low temperature, both of these intermediate phases appear as line compounds. These are very distinct structures, and they're only occurring at very distinct compositions. Even though this is an extended solid, it's extended with fixed stoichiometry. And there are examples of these compounds forming an infinity of compounds. agencies around the world have developed in the last century. There are certain silicon-based bronzes that have been developed for that application. Silicon's not an intermediate phase, but it is a line compound. The solid silicon phase appears to have no equilibrium solubility of copper. Doping semiconductors is why we're able to talk to each other over Zoom. The fact that you can dope some metals into silicon is as important as it gets. It's not just an academic point. Without doping, there is no semiconductor devices. be in the parts per billion. But they're rarely above parts per trillion. So on the last P set, you're going to do some problems around doping semiconductors. So I do want to point that out. There's always a solution. What about this one? This is gallium arsenic system. And so gallium arsenide and alloys, thereof-- which we don't show here-- are the basis for all optoelectronics and photonic technology. But likely, some part of the data between me and you is carried by fiber optic. field of power electronics. Silicon carbide is one of the leading candidates. If the idea of a power substation is a thing of the past, it will be due to silicon carbide and similar high-power electronics that are being developed today. This last one, this is a big old mess. This is the titanium-sulfur system. It has sulfur, which is a liquid below 500 C and melts at 20, and titanium, which doesn't melt till 1,670. RAFAEL JARAMILLO: I don't need solution models because nature doesn't form solutions. So what do I need instead of solution models? AUDIENCE: Just the taut rope, like, the lines. RAFAEL: Yeah. All I need is a number to represent the free-energy change on forming silicon carbide. I just need that point. And similarly, carbon from carbon doesn't take any energy to form. So it's simplified. It's simplified a lot. we have these composition variables. We're familiar with that. The equilibrium condition, the equilibrium condition dG equals 0 satisfied by common tangents. Now let's imagine two line compounds. B3A2 and B4A3. How did I come up with that? Well, I sketched an imaginary phase diagram, and then I had to follow through on my sketch. And the point is not the complicated thing. It's really, I have some two-phase region down here at low temperature. RAFAEL JARAMILLO: I want to introduce this and get this in your minds. Let's imagine reacting metal M with 1 mole of oxygen to form an oxide. So zM plus O2 gas reacting to form MzO2. What's z? How do I determine z? Anybody? Does anyone know some oxides? Name for me a common oxide that you know. And these oxides are line compounds. That z is not a variable. We're going to use this property of being line compounds in Wednesday's lecture.

ROUGE-1: 23.27, ROUGE-2: 21.86, ROUGE-L: 22.47
BERTScore: 63.38

==============================================
==================== [51/100] ====================
Summary:
A random variable can take different numerical values depending on the outcome of the experiment. Some of the possible numerical values of a random variable will be more likely than others. We will describe these relative likelihoods in terms of the so-called probability mass function, or PMF. The PMF is also sometimes called the probability law or the probability distribution of a discrete random variable. We use a subscript, X, to indicate which random variable we're talking about. And it has a probability. And in our case this probability is equal to 1/2.

ROUGE-1: 12.48, ROUGE-2: 12.10, ROUGE-L: 12.22
BERTScore: 72.99

==============================================
==================== [52/100] ====================
Summary:
Protein three-dimensional structure and its implications for the binding of small molecules such as drugs. How can we use these basic motifs to recognize other macromolecules, other proteins and nucleic acids? We have these motifs that we could find, weight matrices for them by aligning lots of sequences. Now instead of aligning sequences, let's see what we can do by mutating both the protein part and the nucleic acid part. And then we know from the three dimensional structure that it interacts mainly with the middle nucleotides. sequence that you're interested in is present. And what you do is quantitate the fluorescence of the zinc finger protein indirectly by the binding of the covalently-attached phage to the antibodies, which are fluorescently labeled by [INAUDIBLE] fluorescence. But how you relate that to the binding constant we had in the previous slide is the subject of this slide number eight. Now we call this the apparent equilibrium association constant because these experiments, just like many binding in living cells is not at equilibrium. groove of the DNA. And the reason the textbook is wrong, first of all, it emphasizes the non-helical part of the zinc finger. In contrast, the textbook where it does the sharp right hand turn and in some way poorly schematized there, it goes coaxial to DNA. That's not what happens. The helices are more or less direct extensions down from the dimerization region of the protein maintaining almost perpendicular to the DNA axis. But again, so on the left is the three tandem repeats, and on the right is a dyad axis. into two major classes. Class 1 is the single letter amino acid code [? CEL, ?] so forth, cysteine, glutamate, and so on. Class 2 is structurally dissimilar to Class 1, but they are similar within the class. You can arrange to make a new amino acid by carving the pocket the amino acid recognizes and grafting on the appropriate nucleotides. You have to have feedback, synchrony, so on that you can basically program the almost digital nucleic acid world inside the cell. not inside the cell. But the whole complex gets internalized. Still, topologically, it's as if it were outside the cell when it's inside this little vesicle. It has to get through that membrane. But now the pH change that happens when this vesicles goes in the cell, part of the natural cell biological processes causes some act. The seven-mer complex of proteins does yet another conformational change and turns into this hairy beast that allows the lethal factor to get into your cell and kill it. of six angstroms of difference between the predicted structure and what it actually is by the more precise methods up higher on this. As you get to 1 Angstrom or better in your accuracy, as you can get from NMR and X-ray crystallography, you now are in a position to study catalytic mechanism and design and improve ligands, such as drugs. There may be a day where we can do this all from ab initio prediction or modeling at very great distances. But for now, modeling atvery short, say, 80% to 90% amino acid similarity is important. letter is the new amino acids. So for example, D67N means a [? sparcade ?] at position 67 and wild type changes to an asparagine. And that causes a drug resistance in the HIV, with unfortunate consequences for the patient. We can take-- now, making mutations in polymerases is not entirely of negative consequences. And I'm going to show you a really beautiful example where a DNA polymerase, you want to change it so that it can now handle what would normally be an inhibitor. There are ways that you can program, and conditional, proteins. You can regulate under what conditions the protein is expressed or not or active or not with an entire domain, or with single nucleotide polymorphisms. Another way is by modulating the activity of the proteins from the outside with drugs or drug-like molecules and chemical kinetics. In the case of the zinc finger, we made an altered specificity. We made new zinc fingers with bind to completely new trinucleotides. With the DNA polymerase, by changing one amino acid, we could make it now accept almost four logs better. increases the risk of Alzheimer's, and probably increases cardiovascular fitness through ApoE refers to its involvement in cholesterol metabolism and transport. The ancestral form of this, for example, found in chimpanzees at nearly 100%, is this arginine 112, instead of what's now common in human populations was cysteine 112. One explanation for that might be that our nutritional standards have changed. We now eat a lot more fatty things. And so maybe this was something that was-- this bad allele, E4, was good in chimpanzees that have different diets or lifespans. very important thing to lose. If you introduce a proline into what would normally be an alpha helix, this is something where knowledge of the three-dimensional structure would say, oh, that proline, this a priori, without any knowledge of conservation, could be a huge change. Multi-sequence profiles are a good way of looking at the conservation. That's a way of prioritizing single-nucleotide polymorphisms that might have impact on pharmacogenomics or disease in general. Protein synthesis is a completely synthetic way of getting short peptides. You can think of these as drug-like molecules. These are naturally related-- they can be analogs of nucleic acids and proteins. And we'll talk about opportunities for making these analogs. A large class of pharmaceuticals, including most of the antibiotics, are made by a fairly small class of organisms, streptomyces in certain plants. And this process by which it's made in more detail, is very akin to fatty acid synthesis. to a template-independent polymerase, [? thermotransferase, ?] which will extend a few nucleotides of completely random nucleic acid sequence. This is one of the examples in biology where you generate sequence de novo. We're just beginning to talk seriously about protein interaction assays. In next class, we'll talk about ways of getting direct information on protein through cross-linking and mass spectrometry. and indirectly setting up these reporter assays, where you take advantage of the binding properties of known proteins. In structural genomics, we look at how to select targets for binding drugs or solving the structures of proteins. We have hundreds of proteins for which we have three-dimensional structures, and from some of them, we have information on what ligands they bind. But these are other criteria that are sometimes used in the field for target selection. So this is an example of a strategy where you use a little bit of prior knowledge, which can be empirical or it could be purely computational, about how to limit your library. If they are homologous to previous interesting targets, then that puts them high on the short list. If they are conserved and you knock it out, then you might expect that to be lethal. If you want to limit the action of your therapy to the surface in order to, say, reduce the cross-reaction with internal molecules, you can sometimes restrict yourself to surface-acceptable proteins. A very large class of drugs is aimed at surface-accessible membrane proteins. And so very often, those are prioritized high. is kind of the big race or bake-off between the different methods. But unfortunately, over decades, it's still hovering around 77% for secondary structure and about 25% for ab initio three-dimensional structure. Then even if you have the three- dimensional structure at adequate accuracy, getting the ligand specificity is problematic. We'll pick up this thread right after a break and carry on to actually how we get the three dimensional structure, whether it's predictive or experimental, and the computational tasks.

ROUGE-1: 25.83, ROUGE-2: 24.88, ROUGE-L: 25.09
BERTScore: 57.71

==============================================
==================== [53/100] ====================
Summary:
More than half of large US firms plan to use AI within the next year to automate tasks that were previously done by staff in a bid to cut costs boost profits and make their work more productive. The New York Times reports that generative AI could automate activities equivalent to 300 million full-time jobs around the world. open ai's chief executive that's Sam mman says governments will need to assume the bulk of responsibility in supporting workers AI labor market disruptions and the question will employees just end up training AI systems only to them be replaced by them. We're going to focus on the impact of AI on the world workplace and how some are now fighting back around the world and the UK. There's also now a concern that intelligent chatbots could replace roles that have traditionally relied on a more human touch things like customer service or call center center. We're also going to speak to one reporter who's been researching the threat posed to workers and how they are now fight back around world. We'll also be looking at how we can make AI valuable for everybody and not just the usual people who profit from technology. And we're going back to AI decoded to see if we can teach AI how to do our own jobs. A Goldman Sachs estimate that AI could automate the equivalent of up to 300 million full-time jobs. Some workers are feeling the effects of that already and they're starting to ask who is going to protect us. What entities are stepping in to take responsibility for retraining or providing opportunities for workers whose jobs might be entirely eliminated by AI technology and call center workers in particular are are clamoring to get answers to that question. We'll talk about the case study because you've met with the worker there particularly who was subject to that. A lot of workers want to feel that their voices are part of the process in deciding how AI is going to be used. Popular sentiment toward unions in America right now is at the highest point it's been in decades. You're seeing Union drives at companies like Starbucks and Amazon many companies that hadn't previously been unionized. People are hoping that this kind of momentary shift of power toward unions will turn into a more long-term and sustained power moment for unions. Could we see that that cultural change happening not just in the US but even more widely across the globe? bringing AI to the table and one last quick question a lot of the times when we talk about Job losses as a result of AI it's like in the pyramid we're looking at lower level jobs or maybe middle level jobs could we flip it on its head and say could we actually replace CEOs or Executives and actually end up having AI run companies and just get rid of those High expensive salaries altogether absolutely I think when chat GPT emerged people sort of had to throw out the door any preconceived notions they had about what jobs were most at risk because of automation.

ROUGE-1: 23.54, ROUGE-2: 22.04, ROUGE-L: 20.52
BERTScore: 63.02

==============================================
==================== [54/100] ====================
Summary:
foreign I'm really excited especially for this lecture which is a very special lecture on robust and trustworthy deep learning by one of our sponsors of this amazing course themus AI. Themis AI is a startup actually locally based here in Cambridge our mission is to design advance and deploy the future of AI and trustworthy AI specifically. I'm especially excited about today's lecture because I co-founded Themis right here at MIT right here in this very building in fact this all stemmed from really the incredible scientific innovation and advances that we created right here. Sadhana is a machine learning scientist here at Themis Ai and the lead TA of the course this year. She'll be teaching us more about specifically the bias and the uncertainty Realms of AI algorithms. Sadhana: Over the past decade we've seen some tremendous growth in artificial intelligence across safety critical domains in the Spheres of autonomy and Robotics. But there's another question that we need to ask which is where are these models in real life a lot of these Technologies were innovated five ten years ago but you and I don't see them in our daily lives. Bias is what happens when machine learning models do better on some demographics than others. Uncertainty is when models don't know when they can or can't be trusted. These are the two big challenges to robust deep learning we'll also talk about solutions for them that can improve the robustness and safety of all of these algorithms for everyone. We'll talk about how to do this and methods for mitigation of this bias algorithmically and how Themis is innovating in these areas to bring new algorithms in this space to Industries around the world. Facial detection systems exhibit very clear selection bias and evaluation bias. The biases in these models were only uncovered once an independent study actually constructed a data set that is specifically designed to uncover these sorts of biases by balancing across race and gender. There are other ways that data sets can be biased that we haven't yet talked about so so far we've assumed a pretty key assumption in our data set which is that the number of faces in ourData set is the exact same as theNumber of non-faces in our Data set. let's first start by training a vae on this data set the Z shown here in this diagram ends up being our latent space and the latent space automatically captures features that were important for classification. Now that we have our latent structure we can use it to calculate a distribution of the inputs across every latent variable. We can estimate a probability distribution depending on that's based on the features of every item in thisData set. Then we can over sample denser or sparser areas of this dataSet and under sample from denser areas of the data set. data point x will be based on the latent space of X such that it is the inverse of the joint approximated distribution we have a parameter Alpha here which is a divising parameter and as Alpha increases this probability will tend to the uniform distribution and if Alpha decreases we tend to de-bias more strongly. This gives us the final weight of the sample in our data set that we can calculate on the Fly and use it to adaptively resample while training. Once we apply these this debiasing we have pretty remarkable results this is the original graph that shows the accuracy gap between the darker Mills and the lighter Mills. tend to amplify racial biases a paper from a couple years ago showed that black patients need to be significantly sicker than their white counterparts to get the same level of care and that's because of inherent bias in the data set of this model. In all of these examples we can use the above algorithmic bias mitigation method to try and solve these problems and more so we just went through how to mitigate some forms of bias in artificial intelligence and where these Solutions may be applied and we talked about a foundational algorithm that Themis uses that UL will also be developing today. of a regression task the input here x is some real number and we want it to Output f of x which is should be ideally X cubed so right away you might notice that there are some issues in this data set assume the red points in this image are your training samples so the boxed area of this image shows data points in our data set where we have really high noise. If we queried the model for a prediction in this part of in this region of the data set we should not really expect to see an accurate result. through aliatoric uncertainty so the goal of out estimating alliatoric uncertainty is to learn a set of variances that correspond to the input keep in mind that we are not looking at a data distribution and we are as humans are not estimating the variance we're training the model to do this task. The crucial thing to remember here is that this variance is not constant it depends on the value of x. We can determine how accurately the sigma and the Y that we're predicting parametrize the distribution that is our input. a data set called cityscapes and the inputs are RGB images of scenes the labels are pixel wise annotations of this entire image of which label every pixel belongs to and the outputs try to mimic the labels they're also predicted pixel wise masks. Why would we expect that this data set has high natural alliatoric uncertainty and which parts of this dataSet do you think would have aliatoric uncertainty. Even if your pixels are like one row off or one column off that introduces noise into the model the model can still learn in the face of this noise but it does exist and it can't be reduced. Training an ensemble of networks is really compute expensive even if your model is not very large. By introducing some method of Randomness or stochasticity into our networks we're able to estimate epistemic uncertainty. At Themis we're dedicated to developing Innovative methods of estimating epistemic uncertainty that don't rely on things like sampling so that they're more generalizable and they're usable by more Industries and people. Themis view learning as an evidence-based process so if you remember from earlier we were training The Ensemble and calling multiple ensembles on the same input we received. epistemic uncertainty let's go back to our real world example let's say the again the input is the same as before it's a RGB image of some scene in a city and the output is a pixel level mask of what every pixel in this image belongs to. We can use bias and uncertainty to mitigate risk in every part of the AI life cycle. analyzing the data before a model is even trained on any data we can analyze the bias that is present in this data set and tell the creators whether or not they should add more samples.

ROUGE-1: 23.57, ROUGE-2: 22.69, ROUGE-L: 23.40
BERTScore: 64.30

==============================================
==================== [55/100] ====================
Summary:
HONG LIU: So if you want to compute, say, some scattering amplitude from alpha to beta-- so alpha's some initial state and beta's some final state. Say alpha consists of momentum p1 and PN-- or pm, and beta, say momentum p m plus 1 and pn. And then you can get this scattering amplitude just by taking your momentum-space correlation function, OK, for the n points. So this is obtained by doing a Fourier transform. So any questions on this? Yes? AUDIENCE: So can you explain again why this diagram, like, you have one branch and then there's a loop? that sign in the initial state come from. And the same thing with the final state, so for the final. state, then you need to look at the phi x acting on the left to this, and then you do the. Fourier transform. So this explains the sign. It's just from whether you act on the initial. state or act on final state. Good? So-- yes? AUDIENCE: Right, so time-ordering of x1 to xn right here? HONG LIU: Yeah. It-- of course, when you derive that, it matters, but here, for this argument, it doesn't matter. we truncate the external propagator, and then they don't matter at all, OK, and it's all included in this diagram. So this Z essentially just captures all these different corrections. And now you have truncated them, and so you don't need to worry about them. So Z also has an expansion, just 1 plus order lambda, et cetera, so the leading order, so Z does not contribute. OK, you can just set it to 1, and when you go to the higher order, then the Z can make a contribution. HONG LIU: This is the self-interact-- yeah, just when the-- when you have an interacting theory, so the particle can interact with itself. When the particle propagates, it actually can interacts with the virtual particle. And so that, this kind of interaction, will affect the property of the propagation but can have the most effect by prefactor, but actually can change the mass, too. In principle, now in principle you can treat any interacting theory. So now let's discuss how to describe fermions. Klein-Gordon equation, if you interpret it as a wave equation, suffers some-- suffers from some difficulties. So Dirac proceeded trying to correct those difficulties, to overcome those difficulties. It turns out that the Dirac equation solved the first problem, OK, but didn't really solve the second problem. So nowadays, we interpret this this is the-- gives the field theory for-- OK, so of course, Dirac didn't know this, so essentially, he discovered this beautiful theory for the wrong motivation. Go to a different frame. OK, yeah, that's what we mean by-- just when you go to adifferent Lorentz frame, the equation, the form of the equation looks the same. Just different observers in different laboratory, they see the same equation, OK? OK, so but for this to be Lorent Z Covariant, remember, LorentZ transformation transform t to x, so immediately, you conclude that H must be first-order in spatial derivatives. OK. So the only-- then the most general way you can write it is alpha minus i. some people, you come up with this idea, you will not imagine this will work. OK, so we will see how to make this work. And so now if you want H to be Hermitian, and then you can immediately conclude-- so that's why I put the minus i thing here, is the alpha and the beta. So m will just be some constant, OK, and here is a matrix. And then, so then he reasoned that for this equation, if we want this equation to be Lorentz covariant, then at least it should have the relativistic plane wave as its solution. The Klein-Gordon equation has the following form. So now we want it to be given by minus partial x square psi plus m square psi. OK, and m partial psi partial xi. Then you have beta square msquare psi, and then you have cross term. So cross term now has the form beta alpha i plus alpha i beta. But now remember, beta and alpha, they are not constant. They are matrices, OK, so they don't necessarily commute. So you have to be careful about the orders. HONG LIU: To satisfy them needs at least a 4-by-4 matrix, so n has to be 4. And so for example, beta can be 0, 1, 1,. OK, this one here is a two- by-two matrix, OK, and alpha i equal to 0, 0, minus sigma i. So you can check both of them satisfy those conditions. Yes? AUDIENCE: Sorry, so when you have alpha times grad psi, is it-- do you act the grad on each element of psi and then multiply by alpha? Or do you-- like, what's the order of operations? HONG LIU: I say this was really genius, because just nobody could have thought of this. OK, so this is a new object, so we call it spinor. Later we will see that this describes spin-half particles, so that's why we call them spinors. OK. So we will take-- so we will denote it as psi alpha. So alpha equal to 1, 2, 3, 4, and we-- for the moment, let's just take the most general situation. HONG LIU: If you know how to do the two halves, then you can generalize. So one half, essentially, you can-- yeah, based on onehalf, you could generalize it. So I divided these 4-by-4 matrices into four 2-by.2 blocks, and then I specify each block. And then this becomes the following equation, then the equation has the following form, gamma mu partial mu minus m psi equal to 0. And so I know this is annoying, but yeah, but this is just a fact of life. most simplest in terms of notation anyway, so that's the convention we use. So now, again, this is a matrix equation. Now let me just write it in the component form, OK, so this is in the components form. So from these two solutions of alpha and beta, we can easily work out what is the gamma and the gamma 0. And then we can write down for those two gammas, so we can generate a certain transformation. Do you know how to generate these representations? Yes? HONG LIU: I mentioned there are infinite number of such solutions, and this is just two of them. Yeah, so this is a new space, and so that's called-- this is called spinor space, yeah. Yes? AUDIENCE: How do we know that-- are these the only two representations? HONG LIu: Oh, no, no,. no. We will talk about that. OK? Good? So now let me make some remarks. OK, so in this case, the original equation just becomes partial t equal to minus i alpha i partial partial xi psi. positive definite classically. You will-- later, you will see why, OK? And this is very similar to the derivation of such occurrence in the case of the just non-relativistic Schrodinger equation because this has the same structure. And then the third point is related to the question many of you may have. So we said, what's the meaning of all these different solutions for alpha and beta or for gammas? So as I mentioned, you can have infinite number of solutions. because one is going to change the basis. So you can show any matrices which satisfy that equation, they're all related by similarity transformation. They're just corresponding to a change of basis. Different forms of the gamma, they may be useful for different purposes. OK, so it depends on which regime, sometimes you use different gamma matrices. So now having introduced the Dirac equation and then the structure of theDirac equation, but still we haven't showed that the DirAC equation is covariant. frame, partial prime square-- OK, so this means in the prime coordinates-- minus m square and phi prime evaluated at the x prime, there must be a Lorentz frame. OK, trivially, you could do that just by definitions. So now we want to show that the Dirac equation has the same property, OK, and that is much more nontrivial. Again, it's really ingenious, ingenious, yeah, but we see, actually, it works.

ROUGE-1: 27.48, ROUGE-2: 26.38, ROUGE-L: 26.11
BERTScore: 68.85

==============================================
==================== [56/100] ====================
Summary:
The US Supreme Court is the highest federal court in the United States. The job is for life, barring resignation, retirement, or removal from the court by impeachment. So far, six justices have been foreign-born, at least one never graduated from high school, and another was only 32 years old when he joined the bench. Most presidents nominate individuals who broadly share their ideological view, so a president with a liberal ideology will tend to appoint liberals to the court. Most rejections have happened when the Senate majority has been a different political party than the president. A US Supreme Court justice is expected to be, in the words of Irving R. Kaufman, "a paragon of virtue, an intellectual Titan, and an administrative wizard" Of course, not every member of the Court turns out to be an exemplar of justice. Each leaves behind a legacy of decisions and opinions to be debated and dissected by the ultimate judges, time and history.have held the position, not one has yet been removed from office as a result of an impeachment. One of their roles is to protect the fundamental rights of all Americans.

ROUGE-1: 47.13, ROUGE-2: 45.04, ROUGE-L: 36.39
BERTScore: 61.99

==============================================
==================== [57/100] ====================
Summary:
Jake Xia: This is the second time we are having this class. We had it last year in a smaller version. That was for six units of a credit, and we had it once a week. And mostly practitioners from the industry, from Morgan Stanley, talking about examples how math is applied in modern finance. And so we got some good response last year. So, with the support of the math department, we decided to expand this class to be 12 units of credit and have twice a week in this classroom. are in finance and business major? Just one. And how many of you are a math major? Most of you. A few engineering majors? A few. Great, because last year we had quite a few, so I want to specifically tell you that you're very welcome to attend the classes here. So anyway, today-- VASILY STRELA: And all of us got MIT emails. We all have MIT emails, which are listed on the website. Peter and Choongbum and Vasily's classes. So I'm going to talk about vega. On his first day at Morgan Stanley he asked a question about volatility. His desk quant look at him, said-- this is supposed to be options trading desk, so he look at me puzzled. So instead of answering my question, he handed over a training manual for new employees and new analysts. He opened the training manual and looked it through. He actually found his answer. At Morgan Stanley this is not called vega, it's called kappa. So now, I remember to call it kappa, which is actually a Greek letter. I did that in the last 20 years. So the point I'm trying to tell you is, before you dive into any details of mathematics or any concept in finance in this class, just bear in mind, this is a field developed in. the last mostly 30 years, or even shorter. And what you really need to ask questions is-- it's not really is it right or wrong in mathematics, is it wrong in physics? So, how the concepts are established and defined and verified. has money to lend out, someone needs to borrow money. Loan is really a private agreement between two counterparties or multiple counterparties. When you securitize them, they become bonds. Commodities, actually, you know. Metal, energy, agriculture products are traded, mostly in the futures format and some in physical format, meaning you take deliveries. And the real estate, you're buying and sell houses. And further of all of these, you heard probably a lot about the derivative products, that started with swaps, options. Banks typically organized by institutional business and asset management. Fixed income, which trade the debt and the derivative products. Equity, trade stocks and the derivatives products. Investment Banking Division, which really covers corporate finance, raising capital, listing a stock, IPO, and merger and acquisition, and advisory. Outside banks, other players, basically, the asset managers, are obviously a very big force in the financial markets. The question a lot of people ask is, is this a zero sum game? I'm sure you've heard this many times. brokers. So, brokers don't really take principal risks. If you want to buy something or sell something, if I'm a broker, I don't make you a price. I go to the market makers. I actually put two people together, matchmaking, make that trade happen, so, I earn the commission. Mutual funds, who actually manage public investors' money, typically in the long-only format. Long means you buy something. Insurance companies has large asset. They need to generate a return. stronger, you just leave it. But if you think it will trade weaker, so you may want to hedge it, meaning you want to sell euro and buy US dollars. And so that's the hedging type. The second type, as I mentioned, is a market maker. So, market maker also takes principal risk, but the main source of profit is really to earn the bid offer. And the third type is really the proprietary trader, the risk taker. They need to focus on generating return and control the risk. Risk management, nowadays, becomes pretty widespread responsibility. It's not just the corporate treasury's responsibility. Even if you are not a finance guy, you work in a corporate, you just do you import, export, or building a factory, you have to know, actually, what the exposure is. Let's talk about market making. If it's a simple transparent product, everybody pretty much knows where the price is. But if it's not transparent, so what do you do? So, if instead of asking you where Apple is, probably you're going to tell me $495 today. that liquidity, and then takes the risk. They manage the book by balancing those Greeks, which I mentioned earlier. Gamma is really the change of the portfolio. Take the derivative to the delta, or to the underlying spot. Delta is the first order. So gamma, now you have curvature or convexity coming in. And theta is really-- nothing changes in the market. Nothing changes in your position. How your trading book is carrying or bleeding away money. And on top of that, what are the tail risks? What are the events that can actually get you into big trouble? So people use value at risk. There are many examples mathematical relationship which gives you the arbitrage opportunity. Fundamental analysis, you're really trying to understand what's going on in the world. And there are special situations. Some companies are going through particular difficulties, assets are priced very cheaply. So, math is very useful in risk management, which I will give you more-- which is very much a very interesting and challenging area. It's easy to observe a stock in the market, but when it comes to more complex products, they just take one step forward on the complexity, which is the option. Risk management is not a purely mathematical question, but yet, math plays a very important role to quantify how much exposure you have. Trading is really all about how do you risk manage, have the discipline, and how to manage your losses. A lot of people with math background, or in general, people are looking for the so-called holy grail trading strategies. The robotrader, a robotic trader, is a dream. It has its place or its use, but it's a fast evolving market. You have to constantly either upgrade your research and adjust your strategies. If your bank account balance is $800, your choice will be very different from someone has $100,000 in his bank account. Market cycles are typically very long, but people tend to have short memories. How do you really build models? Is the market really efficient? What part is efficient? How do we really apply those theories in our day-to-day risk management or trading activities? So learn the math, learn the finance first, but keep those questions along the way when you are learning. VasILY STRELA: [INAUDIBLE] mentioned that, Apple trades, that now it's $494.4 Yeah, just a couple of [inaUDIBLE]. Well, first of all, no offense to people who were [INAudIBLE], but I just wanted to give an example of [INA UDIBLE]. AUDIENCE: [inaudIBLE]. VASILY STREELA: --because he was working in our group, and it just will give you a little bit of an idea what we will be talking about. what we had, we had the noisy observation of broker data and it was coming out at different non-uniform times. So, we decided to use Kalman filter and to study how it can predict. And that's one of the nice graphs [INAUDIBLE] produced, which again, we will use this strategy and the Kalman filters which he constructed in our e-trading platform in Moscow. Just to remind, the website is fully functional. We will be posting a lot of materials there. Probably most lectures will be published there.

ROUGE-1: 28.88, ROUGE-2: 27.40, ROUGE-L: 27.61
BERTScore: 63.52

==============================================
==================== [58/100] ====================
Summary:
Climate change is changing us from the inside out in the UK one in four adults and one in 10 children experience mental illness and there's growing evidence that dealing with a changing climate is adding to that burden. In 2022 we had the hottest year on record where daytime temperatures soed over 40° for the first time in history the past hour or so we've had the UK Met Office issuing its first ever red warning for extreme heat after the 2022 heat wave. Charles and a team of researchers set out to study how the extreme heat affected people's well-being over half of the people they spoke to experienced negative impacts on their mental health. people use that as a way to cope with stress so this is very beneficial social connection is a really big one as well. We are not separate from our environment we are connected not just to the world around us but of course to one another and it and it is only in working with one another that we're going to be able to move forward [Music]"It's a very exciting time for us. We're looking forward to it," says singer-songwriter. "It's going to give us a lot of energy. It's a really exciting time"

ROUGE-1: 32.60, ROUGE-2: 29.33, ROUGE-L: 28.68
BERTScore: 65.42

==============================================
==================== [59/100] ====================
Summary:
Adam Martin: How do you go from something you're interested in learning about an organism to actually identifying genes and mechanisms that are important for that? Martin: The two heroes of today's lecture will be the roundworm, Caenorhabditis elegans, and the fruit fly, Drosophila melanogaster. He also highlights a couple important vertebrate-model organisms-- the zebrafish and the mouse. Martin: We'll talk about yeast a lot more later, when we start talking about the regulation of cell division and the cell cycle. Many models in mice that mimic cancer have been useful for elucidating mechanisms of cancer. We are related to each of these model organisms through evolution because we all arose from a common ancestor. Most of them are fairly small, and they're easy to house large numbers of them in a lab. They're often cheap to house in the lab and work with. Also, they develop fast, and especially when we consider genetics, the rate-limiting step in genetics research is the time it takes from conception of an organism to the time that that organism can reproduce sexually. are involved in a specific process, but you want to identify them. So in a forward genetic screen, you're looking for a phenotype that you would expect if you affected a certain process, if you disrupt a process. So the goal in genetics is to identify a mutation that alters a gene function that gives you a phenotype. And rather than taking a gun and shooting members of the orchestra, in genetics, you try to identify mutations. And these mutations could be spontaneous mutations, meaning you didn't do anything to induce it, but they just appear as a variant. Adam Martin: Mutagenizing an organism isn't specific to genes. You're just inducing random mutations across the genome of the individual. In model organisms, we can actually find these types of mutations, he says. Martin: You don't know which are the ones that you want until you look at their phenotypes. "Wingless" mutant defined a gene that has a homologous gene in humans, Martin says. But obviously, we don't have wings, so this gene didn't get discovered in humans. Drosophila larvae has segments that alternate between smooth cuticle and hairy cuticle. Because there's a lot of hairlike projections here, it reminded the researchers of a hedgehog, and so this mutant became known as "hedgehog" The hedgehog gene was the founding member of an entire signaling pathway that plays important roles in human development and also, human cancer. There are now a number of drugs that are being developed to target the hedgehog pathway, and one was approved back in 2012 for use in treating basal-cell carcinoma. Robert Horvitz's lab identified a pathway of genes that were involved in cell death. Researchers in the C. elegans field started isolating these cell-death-abnormal mutants. They then mutagenized these ced-1 mutants, and they are homozygous, meaning they are in this case, hermaphrodites. And so they're essentially looking for mutants in this animal that will affect the death process that will be present in the remainder of the generations. Adam Martin: circadian rhythm is a behavior. We are awake during certain parts of the day and are asleep at night. If you're hidden from the light-dark cycle, you continue this cycle for some amount of time. Martin: There's something intrinsic in our system such that we want to exist on this 24-hour wake-sleep cycle. He says the Nobel Prize-winning researchers identified a gene called "period" that is associated with familial advanced-phase syndrome, which is a sleep disorder.

ROUGE-1: 18.42, ROUGE-2: 16.95, ROUGE-L: 16.17
BERTScore: 63.85

==============================================
==================== [60/100] ====================
Summary:
In this chapter we will discuss two applications, one price control and second taxation, so right. Sir, does this slope of this graph denote anything price demand upon, some price upon some quantity? So, wait little later we will talk about that that topic, right now we are just talking about movement and shifts, the direction of movement. We are not talking about the slope. So, what is price control what do we mean by price control? Price control is how we can regulate the prices of the goods in the market. In India we have pagadi system and it is more prevalent in Bombay because in Bombay, we have more we have stringent rent control laws. Pagadi is something that security deposit that land the house owner or the apartment owners they take from the person who would like to rent their house. So, to compensate they charge pagadi in name of security fee and this is in a way when they say that it is a security fee it is not illegal. Now, we can also think about the black market, the rise of black market or in other word illegal trade at prohibited prices. The farmers like people who are putting in more effort, but not getting the proper returns of their effort that is why. It is to incentivize the production for a few like for wheat or if we talk if there is a new production of potato and if we put price floor on potato. So, it will incentivize production and they can serve the market better. But for this example that at least everyday news we hear that so much of grain is rotting in government depots. Sir, that is also one more reason now that we do not have a proper distribution system. my point. That if aim is to serve poors why cannot government buy these products at the market price and sell it as you have already said that we do not have proper distribution system. So, what we are doing basically is that we are buying at the minimum support price and leaving grains to rot in open. But I am not saying that price control does not have advantage that when we talk about the welfare economics towards the end we will talk about it. But what I am saying definitely that there are definitely better ways to cater to the welfare of public.

ROUGE-1: 22.75, ROUGE-2: 21.98, ROUGE-L: 22.64
BERTScore: 68.20

==============================================
==================== [61/100] ====================
Summary:
The size of the ribosome is similar to the size of a rhinovirus. The structure of the mature messenger RNA is known as the 5 prime cap. There are many features in this part of the sequence that are very important for translation. They contribute to the efficiency of translation. It's part of what's known as a ribosomesome binding site. It is a large entity in the cell. And, when you do look at electron micrographs of cells, you can see these dark dots. They're big enough to see. The mature transcript, you don't translate the whole thing. A lot of this stuff is structural, functional for other reasons that contribute to the success of translation. Exonucleases might chew up enough. And they may end up chewing up your transcript, but that probably suggests that the messenger has been around too long, and it's time for it to retire to a better life, OK? So remember the poly-A tail. And this, once again, plays other functional roles with respect to being recognized as a transcript and being helped to get out of the nucleus. ones that are common to everybody, all right? So, obviously, when you look at the language of bases, one base-- if the language translated directly one base to one amino acid, we could only encode for amino acids. So it's finally deduced that three bases encoded each amino acid. That would give us 64 possible words in the language that needs to be translated. That's a lot more than we need. We only need 20 for the encoded amino acids, so 64 possibilities. We need a few more things anyway. have the ideas, but, really, the definition by doing a process known as cell-free translation where they could very carefully add components to understand how the code, the genetic code, was formulated. So that's the work that Khorana and others did. And then, later on, things started to get-- you know, these are decades of work I want to point out to you. The ribosomes were discovered. That was a decade later, the sort of details of the structure, but not the structure itself. And it was really exciting in the 2000s when Ramakrishnan, Steitz, and Yonath solved the structure of the prokaryotic ribosome. A lot goes on with the rest of the structure. It's a very important structure in the mechanisms of protein translation and synthesis. You might see it in this globular form. And I pointed out the anticodon loop. The place where the amino acid gets linked is also called the acceptor stem. And, up here, I show you that linkage. And you should-- yes? AUDIENCE: I was just going to ask, I see how the anticodons are specialized. How does the 3 prime end of the tRNA know which protein is bound? The genetic code gives you the identities of what are known as the codons, which is how we designate the triplet of nucleotides. The code is the absolute-- the sort of Rosetta Stone for translating messenger RNA to amino acid sequence using codons. The genetic code has variable bulges and shapes associated with the synthetase enzymes that I'll introduce in a minute that it recognizes, all great questions. That stuff in between has variables. That's what the genetic code is all about. The genetic code is the code that's going to be embedded within the messenger RNA. In some organisms, you start with different amino acids, but the most common start is the codon for methionine. Methionine is fairly rare. There may only be one or two more in the protein. There are several stop codons. Some are more predominant in some organisms than other. Some organisms might prefer two or three of the degenerate codons, and others may prefer a couple of the others. Cartoon form. To attach an amino acid to the 3 prime end of the transfer RNA, you have an amino Acid residue-- we're just going to go R here-- carboxylic acid, amine. And what we do is we need adenosine triphosphate to activate this chemistry. So you attach through an ester to the amino acid from the 3prime end of these transfer RNA. That's what's done. The ATP makes this chemistry feasible, but there's one more player here. And that's the enzyme that brings them all together, which is an aminoacyl-tRNA synthetase. The small and large subunit is the messenger. In orange-- well, that's kind of a burnt orange-- is a sneaky little bit of the messenger, and in yellow are the transfer RNAs. And there's one more unit on here that I won't describe too much. It's a protein factor that helps all the processes occur. Generally, it's thought to help the loaded tRNA come to the ribosome, get it in place, and then go away. So it's some of these extra helper proteins that are involved. OK, so let's build a protein. The structure of the ribosome shows that nucleic acids are catalyzing a reaction. The energy is actually not provided by ATP, it's provided by GTP. ATP is important is in loading the amino acids onto the transfer RNAs. This occurs at about a rate of 20 amino acids per second, meaning you're reading about 60 bases per second. That's far faster than the rate of replication, which is far faster. It's a fascinating field, but it's a bit beyond the scope of this article. have the genetic code. Just be familiar with reading it and making sure you could pick out which amino acid might be incorporated in response to which particular codon. When proteins are made on the ribosome, they have a bit of a choice. They can get made and fold beautifully into active proteins. Occasionally, proteins misfold. Maybe the rate of synthesis is too fast, or the environment isn't right. So there will need to be mechanisms whereby proteins get degraded if they're not folded properly, but that's the story for another day. an error in the DNA that then causes anerror in the messenger RNA. Nonsense mutations are not so bad because you probably just truncate the protein. Missense mutations are where you put in the wrong amino acids. And those are the ones where you end up, in a lot of cases, with inherited diseases. And I just want to remind you of the situation in hemoglobin when we had a missense mutation, and we incorporated a valine instead of a glutamic acid, which put a drastic change in the protein that caused sickle cell anemia. would be degraded. The missense mutations are the more serious ones because you end up with a full length protein that might have a mistake in it. Am I being clear enough to everyone? Yeah? Good. OK, I am going to tell you that I'm handing over the baton to my colleague, Professor Martin. He'll take over on Monday. And these will be the lectures that will occur. I think this field is fascinating. Once you get used to the mechanics of it, it's really cool to think of how you go from DNA to RNA to folded proteins.

ROUGE-1: 29.89, ROUGE-2: 28.46, ROUGE-L: 26.78
BERTScore: 61.24

==============================================
==================== [62/100] ====================
Summary:
Professor Donald Kagan: We are living in the early years of a polis sometime in the eighth century B.C. The date that's sort of typical for the general phenomenon of colonization coming out of the mother cities of Greece is 750 roughly. But in fact, the earliest date according to Greek tradition, if my memory is correct, was something like 773 where the Greeks date the foundation of what they thought was the earliest colony they ever established -- a place that they called Pithaecusa. The Greeks were terrified of the sea for very good reason. The vast overwhelming majority of people needed to farm land, in order to stay alive. One answer, and it's the one that is most widely believed among Greek scholars, is that the growth of population that we have mentioned in connection with the rise of the polis is still working. The Greeks were ancestor worshippers. They took special care of the dead and they thought that the way you buried people and so on was terribly important. When you leave the place you were born, you leave your ancestors as well. Land hunger is a key explanation, I think, is supported by the fact that wherever we find a polis, whatever other characteristics it has, and they vary. Some of the places where they settled leave us puzzled, and have left the ancients puzzled. One of my favorite examples is the colony on the south shore of the Bosporus, which is called Chalcedon. It's right opposite Constantinople--that doesn't exist, Istanbul. Winston Churchill never, never conceded that it was Istanbul; he called it Constantinople till the day he died. Colony is a Latin word ultimately for colonia and the Roman colonies were, first of all, garrisons that they planted in land they had conquered. The Greek word for this is, apoikia, and most literally it would mean a home away, an away home and that's what they're making. They are establishing for themselves a household, a home someplace away from where they started. They were the antecedents of the Serfs, which we will see later on in medieval history in Europe. of eminence, and yet unlikely to be part of the sort of dominant faction in that city, because otherwise why would they leave? Anyway, the Greeks had a name for this individual. He was called an oikistes; he is the found of the colony. So, now he has decided to do it and he's gained recognition from the town council, let's say, and he can go forward. Now, he has to have an idea. He can't just say, I think I'd like to found a colony. What is more typical is that he thinks, I would like to take and have found a colonies on the southeastern coast of Sicily. Why? Because he knows something about it. came up, where the priestess sat, where all of this came up. Well, archaeologists investigated this carefully, and the French School of Archaeology late in the nineteenth century dug everything up and concluded this was baloney. There were no gases coming up from any of this stuff, and so everybody believed for the next century. Then a young man who once sat in one of the chairs--not in this room maybe, but in which you're sitting, John Hale of the Yale Class of 1973, who is now an archaeologist at the University of Louisville. He decided to investigate this and he took with him a fine geologist from Wesleyan just to go to the place there at Delphi and to see whether it could be true that such gases did come out. They discovered evidence that, in my judgment, but I don't think really anybody doubts it anymore, that totally confirmed the Greek story. Greeks and barbarians, and everybody came to Delphi to consult the oracle. People used to bribe the priests, in order to get moved up on the front on line. The oracle probably gained its fame for being very good precisely at answering this question. The question would have been what will happen if I go and try to settle a colony at the place which I will call Syracuse. The answer would come and the priests would give a response that would be essentially straight. gotten the permission from your city to go forward, and you go to the Delphic Oracle. Next thing, you got to go home and you have to write up what amounts to a charter of foundation for the city. Recruiting is tremendously important because you need to have a certain number of settlers to make the settlement viable. So however many that is, that is what you try to recruit and you recruit typically at a time when it's easy to get people together so you can tell them the story. Syracuse is an independent polis, autonomous, self-governing, whatever regime it wants. It is not a subject of anybody, not Corinth or anybody else. The most typical, the usual, everything else is an exception is that there are friendly relations between the mother city and the apoikia, but keep in mind that they are always independent. In the Peloponnesian War, Syracuse finds itself besieged by the Athenians. They go to Corinth asking the Corinthians to please help us. The Corinthians send very little, send a couple of ships and a general, which turn out to be tremendously important. about it, because Thucydides tells us the details of it. When Potidaea got into trouble with Athens, and found itself besieged, Corinth sent a real army to go in there and fight. At the other end of the spectrum it's again Corinth and they have a colony up in the northwest called Corcyra. The first relationship between them is a navel battle, and thereafter we hear of them quarrelling and fighting with each other just about at least once a century right on down. Kagan: The question is who gave permission for a colony to go in the mother city. The best guess and that's the only thing we have. These would have been aristocratic republics at this stage of the game. Corinth always needs that kind of stuff, so we sell you our wheat, you sell us your pottery, you selling good wine that we can't grow yet and maybe never will be able to grow in our neighborhood, so on and so forth. Everybody--all of this is voluntary on both sides of every agreement. coast of Asia Minor on the west, and even around on the bottom and to some degree on the north, and on the islands in the Aegean. So, there has been a Greek--what's the word I want? There is an expansion of the Greek world already by the tenth century, and these folks are now settled down. I might point out that the way the Greeks did their immigration into Asia Minor actually had a pattern so that you can go from north to south and you will find some consistency. as powerful as it used to be, far from it. It has been conquered by now by other peoples. If you're talking about the year 750 or something like that it's in the hands of the Assyrians. So that is not territory that you can build colonies; you've got powerful empires to deal with. There is one exception. In the sixth century, the Greeks settle a single colony in the Delta of the Nile of Egypt at a place called Naucratis. It is a trading post and it's there by permission of and under the protection of the King of Egypt. In the period we're talking about there are no Romans that you have to worry about. So, south of Rome there is a tremendous colonizing of southern Italy. Greek cities are all over the place. Lots of these towns sent colonists up north into the Dardanelles. Some of the same cities send out colonies to Sicily, so that for the real colonizing states there was no limit to where they would send people who wanted to go in those areas. It is also interesting to notice who does not colonize at this early period and the answer is all the most famous cities of Greece in the Classical Period. they never lived before and their presence has a real impact of a different degree in every place. I would say that typically their impact was greater in the west and the north than it was in the east and the south. It is inconceivable the Greeks could have developed a civilization that they did without contact with these eastern civilizations and learned a great deal from them. But what is undeniably true is that the Greeks learned very important and valuable things, and adapted what they learned through their own way of life and produced something really quite new. did not have great urban cultures and civilizations, long traditions of learning and so on. No, they weren't like that, the Greeks were ahead of them and it's evident that they borrowed stuff from the Greeks in every element of life, although it didn't shape their lives in a potent, fundamental way. Of course, another tremendously important consequence of colonization was the growth of commerce, of trade for the reasons I've already given you, but beyond that the Greeks of course, now had access to food stuffs and other things out in where they settled. is seen to be a tremendously valuable safety valve to the Americans, first as colonists, and then as independent people. Americans didn't have the kind of terrible class warfare and the terrible warfare within cities that the Europeans had experienced throughout most of their history. I mean, fundamentally, Kansas is a colony in a certain Greek sense, all of these places are. So, that's part of the story of why America had the very lucky early history that it had. And that is the proper introduction to the next topic, which I'll discuss next year. No not--it seems like a year, but it's next Tuesday actually.

ROUGE-1: 29.44, ROUGE-2: 28.57, ROUGE-L: 28.25
BERTScore: 62.69

==============================================
==================== [63/100] ====================
Summary:
Instructor: We are asked, which of the following correctly identifies the areas of consumer surplus, producer surplus, tax revenue, and deadweight loss in this market after the tax? So pause this video, have a go at it. Even if you struggle with it it will make your brain more attuned to when we work through it together. All right, now let's work through this together. And I just want to sort of understand what's going on here before I even try to answer their questions.

ROUGE-1: 16.31, ROUGE-2: 16.15, ROUGE-L: 16.31
BERTScore: 70.34

==============================================
==================== [64/100] ====================
Summary:
Sir Gawain, nephew of King Arthur, was invited to a party at Camelot. A towering knight riding an emerald steed burst into the room and proposed a game. The Green Knight declared he would allow the bravest warrior present to attack him with his own axe. If they could strike him down, they would win his powerful weapon. However, the knight would be allowed to return that blow in one year and one day. Gawain tried to forget this bizarre vision, but despite the strangeness of the knight’s game, he was determined to act honorably.

ROUGE-1: 24.43, ROUGE-2: 21.68, ROUGE-L: 22.14
BERTScore: 69.18

==============================================
==================== [65/100] ====================
Summary:
well welcome back everybody to uh the last lecture 162. this is kind of a a special lecture um i did get some requests for more information about distributed storage and quantum computing and so i think we're going to do that. i want to make sure that we talk through the chord algorithm since that's a i think relatively simple thing to understand and is uh very cool and applied pretty much everywhere. If you remember we started talking about key value stores with this notion of a distributed hash table where what i've got in yellow here is really the key value um table that we might think about on one node. there's performance problems and uh scalability issues where we would like to increase the size of the system by just sort of adding more nodes down at the bottom here and um so far we haven't really talked about how to even make that work okay. So today i want to tell you about the cord uh algorithm which has been turned into storage systems of many sorts including those used by amazon et cetera okay facebook so um before we get there i wanted to remind you of this notion of recursive versus iterative lookups so um here's an example of a recursive lookup which is like routing. clean way to distribute them throughout the system without having to know pretty much all of the nodes that are participating so this seems like a strong ask when you think about it if there's hundreds or thousands or millions of servers down here and we have to somehow um consistent with consistent hashing figure out which node to go to without going through a master directory and such that all these nodes don't know about each other. So this is basically going to be a mechanism to divide our space up and we'll talk you through that in the next slide and then i'm going to show you how the chord algorithm lets you get by with only knowing essentially a logarithmic number of nodes. been talking with clients presumably what i know is i know one of the nodes in the system and if you remember this ring has nothing to do with locality that node could be i don't know 8 or 15 or something okay and so what i need is i need this new node needs to know one gateway node i'm going to say 15 just for the sake of argument and what are we going to do to join well we're going to send a join message to the node we happen to know about okay and what's interesting about this algorithm is it's going to figure out who is responsible for storing key 50. will tell you that there are subsequent versions of cord which uh when you're doing this routing and you have a lot of options here see how we have many places we could go. One of the things we can do with chord is we can use chord to store locations of data rather than the data. We can actually take locality into effect to some extent in chord and um and make our routing less like bouncing back and forth across the planet randomly and more like working our way physically toward the thing that we're interested in. have a service guarantee that says we'll get a response within 300 milliseconds uh for say 99.9 percent of the requests okay and so that's part of the way that the chord algorithms are adapted in a read real cloud service. Security is kind of dealing with actions of a knowledgeable attacker who's really trying to cause harm and we want to make sure that they can't really screw us up. quantum computing as well so we can i know there was some of you asked some questions about that so i'm going to leave this topic unless there's more questions okay. about by using new techniques and the distinction between protection and security i think is an important one because protection is the set of mechanisms that we talk about in this class. Security is basically using those mechanisms to prevent misuse of resources so for instance virtual memory is a mechanism that can be used for protection security policy would be making sure that when we use virtual memory we don't let malicious processes or different processes owned by different people use the same memory and have a potential for screwing each other up. that a user who's making changes to the system is really who they claim to be data integrity is making sure that the data hasn't changed okay so that's important confidentiality is makingSure that theData is read only by authorized users so that often involves encryption of some sort. Non-repudiation is a surprisingly important thing that people don't often talk about which is that if one sender makes a statement and they uh send a message or whatever they can't later claim that well i didn't really send out somebody malicious did. hash function is one where you take data and you run it through a hash function and you get a bunch of bits out of it. If you change the data even slightly you end up with a good hash function with something that essentially roughly half of the bits change. What makes this secure is that it's not possible for somebody to come up with another source that matches the hash function. So we can use hashes to prove later that you know after the transmission has happened that the data is authentic. it wasn't uh you know if the integrity wasn't high you know it was basically didn't match then we could know that that firmware is probably bogus and we shouldn't be using it okay now the downside of course of everything we've talked about is both sides share the same key and so if you leak the key then you got problems okay and furthermore you have to somehow share the key so that requires you to go in a dark alley and you know hand the key over and so this seems like only part of the solution and um the interesting thing about that is this idea of public private key pairs. protect everything okay and so if you think about the internet of things really uh one way to look at theinternet of things is that we have a whole bunch of devices and compute elements all over the world and it's really a graph of services that we want to connect. So distributed storage is everywhere every arrow represents communication we've got storage everywhere. We want to make sure that the data can only be written byauthorized parties and only read by authorized parties okay and these secure enclaves are a topic for another day as well. Data centric vision is one that i've adopted in uh my research group is one in which we think about shipping containers full of data. Inside the data capsule is a bunch of transactions that are hashed so remember those hashes we talked about and signed where we uh we use a private key to sign a hash over something. As a result of these data capsules this gives us a cryptographically secure way of moving data around to the edge to the cloud and back again in a way that nobody can fake out. is a is a good one to be talking about um what we know is the following the metadata is uh among other things the public key of an owner hashed okay and so all of these signatures have to be signed by the owner and anybody can verify that um the data that's in here was put in there by the right owner okay so that gives us integrity and providence it means that we can know that none of the data in here could have been put there by an adversary so that's the first um thing that we know and the second thing is of course we can put arbitrary encryption on top of this as well to make it private. The vision here really is of pretty much everybody using data capsules everywhere okay and if you can get that to happen then you potentially have a very interesting scenario here. Part of what we're doing is we're working with roboticists and machine learning folks to put their data and their models for grasping and so on inside of data capsules and as a result they can reside securely in the edge in say your robots or whatever in a way that can't be breached okay. This is really targeted at secure edge infrastructure in addition to the cloud so these data capsules can move back and forth. class but uh what you find out is for instance back in chemistry if you remember from chemistry you had the orbitals right and so only electrons were only allowed in certain rings or spheres actually around atoms. Because it's quantum mechanics we can do the second thing which is superposition and this is having uh a bit which is both a zero and a one in certain fraction of uh between the two. If you're willing to allow things to not be always a one or always a zero what you can do is you can just start doing quantum computing. Those are particles like protons or electrons have this intrinsic spin and so now i got one and zero or up and down okay and a representation called the heisenberg representation looks at this messy physical situation like this which is either a zero or a one in these brackets. What you see here is actually a superposition of zeroness and oneness together okay now you know i realize this looks a little weird we don't normally get a wave function notation in 162. But the thing that's very interesting about this is that this is a description of a combination of zERONess and Oneness where the probabilities can be adjusted anywhere. you do a bunch of computing on it such that the probabilities are kept and you measure okay and the way it looks is that you take uh let's say you put an input with all possible combinations of the input input of the inputs being equal values all possible probabilities it looks like you're doing computation on all possible values at once but then when you measure you pick up exactly one and that's the answer you get okay. If you don't do anything very interesting here this is going to look like you randomly picked some input and computed on it so basically what we're talking about here looks like a random computation. r is odd we got to repeat if r is even then we can say well i know because x to the r is equivalent to one mod n. If we could somehow figure this out what r makes this equiv equation satisfied and we could do that quickly then um we win. That's something that uh you can't do easily classically but with a quantum computer what we can do and unfortunately i guess i don't have time to do this because we're running out of time but i can set up a situation where my input to my algorithm is all the possible k's. actually investigated if you were to build uh that factoring algorithm and you could do it as quantum circuits that could run on a quantum computer what would that look like. We actually investigated ways of optimizing that and we could actually look at performance of different options for the shortest factoring algorithms. So we built a cad tool to do that so i i don't know i think it's a pretty interesting area right now and there's a lot of interest in it all right so um sorry i kept you guys way over but this is the last lecture i figured if anybody was interested we talked about key value stores.

ROUGE-1: 21.95, ROUGE-2: 21.45, ROUGE-L: 21.54
BERTScore: 69.97

==============================================
==================== [66/100] ====================
Summary:
Early astrologists did not recognize the lymphatics so they thought there are only three system there is a branch of portal or this branch of puerto bean. Originally doctors thought that every corner these three things are present and they call it portal triad. But later on of course the new it is not portal Triad it is portal triads. The liver is really working is that right that blood is again there are two input system hepatic arterial input and what was this portal venous input. coming here what is this cystic duct and now all this together is called common bile duct it is coming behind the do denim of course come into pancreas. Common bile and pancreatic duct come together and open here in Tudor denim at the employ of weatr here pancreatic juices and bile will come down other right again the relationship right and left the Patek duck and - what is the common hepatic duct meeting with the sister making what isThis.

ROUGE-1: 25.21, ROUGE-2: 24.18, ROUGE-L: 25.06
BERTScore: 61.83

==============================================
==================== [67/100] ====================
Summary:
So in the next portion of today's lecture we're going to talk about how we can modify the policy gradient calculation to reduce its variance. In this way we'll obtain a version of the policy gradients that can be used as a practical reinforcement learning algorithm. The first trick that we'll start with is going to exploit a property that is always true in our universe which is causality causality says that the policy at time t prime can't affect the reward at another time step t if t is less than t prime. Policy gradients are a way to formalize trial and error learning as a grain ascend procedure. We can derive the expression for the optimal baseline to minimize variance of a policy gradient. The optimal baseline is not used very much in practical policy grading algorithms but it's perhaps instructive to understand some of the mathematical tools that go to studying variants so that's what we're going to do in the next portion the next part will go through a mathematical calculation where we'll actually derive the equation for optimal baseline. variance we just uh sorry we often don't use the optimal baseline we typically just use the expected reward but if you want the optimal baseline this is how you would get it all right so to review what we've covered so far we talked about the high variance of policy gradients algorithms. We talked about how we can lower that variance by exploiting the fact that present actions don't affect past rewards and we talked about how we can use baselines which are also unbiased.

ROUGE-1: 19.11, ROUGE-2: 17.83, ROUGE-L: 18.43
BERTScore: 67.84

==============================================
==================== [68/100] ====================
Summary:
Today we're gonna talk a little about learning in the setting of games. Can you still be optimal if you reveal your strategy? It's actually not the size that matters. It's the type of strategy that you play that matters, so just to give you an idea. We'll talk about this in a lot of details towards the end of the class. Today is not Thursday. Tomorrow. For a second, I thought it's Thursday. Um, all right, let's co- Tomorrow. know what to play." But th- this has an unintuitive answer that we are gonna talk about towards the end of the lecture. So just more of a motivating example. Don't think about it too hard. All right. So, so let's do a quick review of games. We were playing for the agent, uh, and the agent was trying to maximize their utility. So we had this minimax tree and based on that, the utilities that are gonna pop up are minus 50, 1 and minus 5. of this weak estimate of your value is going to work well and give you an idea of what to do next. So, so instead of the usual recurrence, what we decided to add this D here, um, this D right here which is the depth that un- until which we are exploring. And then we decrease the value of depth, uh, after an agent and opponent plays. When depth is equal to 0, we just call an evaluation function. So intuitively if you're playing chess, for example, you might think a few steps ahead. how learning is applied to these game settings. And specifically the way we are using learning for these game. settings is to just get a better sense of what this evaluation function should be from some data. And, and that kind of introduces to this, this, um, temporal difference learning which we're gonna discuss in a second. It's very similar to Q-learning. Uh, and then towards the end of the class, we will talk about simultaneous games and non-zero-sum games. can actually, like, roll two dice and based on the outcome of your dice, you move your pieces various, various amounts to, to various columns. Uh, there are a bunch of rules. So your goal is to get all your pieces off the board. But if you have only one piece and your opponent gets on top of you, they can push you to the bar and you have to start again. So, so what are some features that you think might be useful? Remember the learning lecture? Yes. So, so that was my model. And now, the question is where do I get data? Like where and because if I'm doing learning, I got to get data from somewhere. So, so one idea that we can use here is we can try to generate data based on our current policy pi agent or pi opponent. And then from these episodes, we want to learn. These episodes look like state action reward states and then they keep going until we get a full episode. One thing to notice here is, is the reward is going to be 0 throughout the episode until the very end of- end of the game. as a function of w, okay? All right. So, so what do we try to do usually, like when you are trying to do learning? We have prediction, we have a target, what do I do? Minimize the- your error. So I'm gonna write one-half of prediction, minus target squared, this is my squared error. I want to minimize that. So now I have the gradient. What algorithm should I use? I can use gradient descent. All right, so I'm going to update my w. How do we update it? between the two? Yeah, so this is very similar to Q learning. There are very minor differences that you'll talk about actually at the end of this section, comparing it to Qlearning. All right. So, so I wanna go over an example, it's kind of like a tedious example but I think it helps going over that and kind of seeing why it works. Especially in the case that the reward is just equal to 0 like throughout an episode. So it kinda feels funny to use this algorithm and make it work but it work. So I want to just go over like one example of this. do these specific things that you would wanna do or these differentiating factors about it. So, so picking features, it's an art, right, so. [LAUGHTER] All right. So lemme, leMme move forward cause we have a bunch of things coming up. All right so, so this was just an example of TD learning but this is the update that you have kind of already seen. And then a lot of you have pointed out that this is, this is similar to Q-learning already, right? The idea of learning in games is old. People have been using it. In the case of Backgammon, um, this was around '90s when Tesauro came up with, with an algorithm to solve the game. And then more recently we have been looking at the game of Go. So in 2016, we had AlphaGo, uh, which was using a lot of expert knowledge in addition to ideas from a Monte Carlo tree search and then, in 2017, we have AlphaGo Zero, which wasn't using even expert knowledge. between our prediction and our target and try to minimize that error and, and find better W's as we go through. So, um, all right so that was learning and,. and games. Uh, so now I wanna spend a little bit of time talking about, uh, other variations of games. So the setting where we take our games to simultaneous games from turn-based. And then, theSetting where we go from zero-sum to non-zero-sum, okay? All right. This value function, uh, over, um, over our state here. Now, we have this value function that is- do we- we shall use here, I'll just use here. That is again from the perspective of agent A. So, so I'm trying to like get good things for A. In this case it's not at the end [inaudible] ? Uh, yeah. And then this is like a one-step game too, right? So like you're just playing and then you see what you get. So whatever the agent A gets, agent B gets negative of that. stochastic policies. So, so pure strategies are just actions a's. And then you can have things that are called mixed strategies and they are probabilities of, of choosing action a, okay? All right. So here is an example. So if, if you say, well, I'm gonna show you 1, I's gonna always show you1. Then the- if you can, you can write that strategy as a pure strategy. With probability 0 show you 2. I could also come up with a mixed strategy. You could only pull it out to like you're in the si- simultaneous game, you could just bring chance in. time show one, half the time show, show two. And then the question is, what is the value of, of these two policies? How do we compute that? [NOISE] Well, I'm gonna use my payoff matrix, right? So, so 1 times 1 over 2 times the value that we get at 1, 1, which is equal to 2. Ah, so you might be interested in looking at what happens in repeated games. So that opens up a whole set of new questions that you're not discussing in this class. think minimax. So agent B should be min- minimizing this. agent A should be maximizing this. That's, that's what we wanna do. But with the challenge here is we are playing simultaneously, so we can't really use the minimax tree. So, so I'm going to limit myself to pure strategies. So right now I will just consider a setting- very limited setting and see what happens. So then player B is going first, player A is minimizing and then player a is maximizing. If you are playing a mixed strategy, even if you reveal your best mixed strategy at the beginning, it doesn't matter if you're going first or second. For every simultaneous two-player zero-sum game, with a finite number of actions, the order of players doesn'tmatter. So remember, if you play mixed strategy,. your opponent is going to play pure strategy because this is like this the first point that we had before it. All right? So this is kind of the third thing that we just learned, which is von Neumann's Theorem. With probability p, like, if we're doing like ordering, like one of the two answers might- will come out, [inaudible] it'll be either one or two. So, uh, the thing is these two end up being equal. So no matter what your opponent does, like you're gonna get the best thing that you can do. So in expectation when- you're saying when you are choosing p? Yes, so I'm treating p as a variable that I'm deciding, right? The key idea here is revealing your optimal mixed strategy does not hurt you which is kind of a cool idea. The proof of that is interesting. If you're interested in looking at the notes, you can use linear programming here. The reason, kind of the intuition behind it is, is if you're playing mixed strategy, the next person has to play pure strategy and you have n possible options for that pure strategy. So that creates n constraints that you are putting in for your optimization. You end up with a single optimization with n constraints. the pay off matrix. Is that A or B? So, uh, so you have two players A and B. Each one of you have an option. You can either testify or you can refuse to testify. So, so what that, that means is if you look at the, the value function from perspective of player A at the Nash equilibrium at Pi star A and Pi star B is going to be greater than or equal to value of, of any other policy Pi A if you fix Pi B. There's a huge literature around different types of games, uh, in game theory and economics. If you're interested in that, take classes. And yeah, there are other type of games still like Security Games and or resource allocation games that have some characteristics that are similar to things we've talked about. And with that, I'll see you guys next time. All right. So summary so far is we have talked about simultaneous zero-sum games. We talked about this von Neumann's minimax theorem, er, which has like multiple minimax strategies and a single game value.

ROUGE-1: 21.67, ROUGE-2: 21.14, ROUGE-L: 20.58
BERTScore: 65.53

==============================================
==================== [69/100] ====================
Summary:
The famous example was posed by Comte de Buffon back in the 18th century. It marks the beginning of a subject that is known as the subject of geometric probability. The problem is pretty simple. We take a needle that has a certain length-- l-- and we throw it at random on the plane. So the needle might fall this way, so that it doesn't cross any line, or it might end up crossing one of the lines. But we will make the assumption that the length of the needle is less than the distance between the two-- between two adjacent lines. The model is going to involve two random variables defined the way we discussed it just now. What is the range of these random variables? Since we took x to be the distance from the nearest line, this means that x is somewhere between 0 and d over 2. How about theta? So the needle makes two angles with the part of the line. It's this angle, and the complimentary one. Which one do we take? Well, we use a convention that theta is defined as the acute angle that the direction of the needle is making with the lines. much more streamlined. There's not going to be any choices. We just need to consider the event of interest, express it in terms of the random variables that we have in our hands, and then use the probability model to calculate the probability of this particular event. When will the needle intersect the nearest line? This will depend on the following. We have an intersection if and only if the vertical extent-- which is this vertical green segment-- is larger than the distance x. Or equivalently, if x is less than the Vertical extent.  Monte Carlo method uses simulation to evaluate experimentally the value, in this case, of the constant pi. There are many applications in engineering and in physics where certain quantities are hard to calculate, but they can be calculated using a trick of this kind by simulation. Consider the unit cube in n dimensions, which is an object that has unit volume. Inside that unit cube, there is a complicated subset which is described maybe by some very complicated formulas. The description of the subset is so complicated that using integration, multiple integrals, and calculus is practically impossible. What you can do is to start throwing at random points inside that cube. unit cube. So you throw points. Some fault inside. Some fall outside. You count the frequency with which the points happen to be inside your set. And as long as you're throwing the points uniformly over the cube, then the probability of your complicated set is going to be the volume of that set. You estimate the probability by counting the frequencies with which you get points in that set, and so, by using these observed frequencies, you can estimate the volume. It turns out that these days, physicists and many engineers use methods of this kind quite often and in many important applications.

ROUGE-1: 38.61, ROUGE-2: 37.21, ROUGE-L: 38.53
BERTScore: 70.02

==============================================
==================== [70/100] ====================
Summary:
Professor: We're describing interaction between the electromagnetic field and atoms. Professor: I want to show you what are the tools to treat those infinity source divergences in a consistent and a systematic way. "I can reproduce this result by going to infinite order in perturbation theory," he says. "Who of you have actually seen those kind of diagrammatic tricks and summation? A few, OK. So it's maybe nice to see it again. But for those who haven't seen it, welcome to the magic of diagrams" at the end of the class on Monday, I told you, well, let's simplify things. Let's get rid of those temporal integrations and multiple integrals by simply doing a Fourier transform. And so therefore, we introduced the Fourier. transform, or the Laplace transform, of the time evolution operator. And this iterative equation where we get the nth order by plugging the n minus first order on the right hand side, this turns now into a simpler algebraic. iterative equations for the Fouriers transform. Z is the energy. And it is the initial energy. If it's a ground state and a resonant photon, we have a problem. The other parts are simple. They don't have any divergences. They are not resonant and such. So what we want to do is now, in some way, give special treatment, factor out the problematic terms. And for the easy part, which has no divergence, we can make any kind of approximation we want. But the resonant part, this needs special attention. N equals 3, so we have to start in state b. We can go from here to there with any combination of states you want. But one thing is not allowed-- to involve the stateb. And everything else other than the state b has already a symbol. It is the square symbol. So this is the exact representation for n equals 3. And the contribution to the resolvent G, the Fourier transform of the time evolution operator, is-- well, we have factored out three occurrences of the state B. started out with U, which we couldn't calculate with Fourier transform. We had G, whichWe've expressed G in R, which of course we cannot calculate exactly. But there is an importance. We have made progress for the following reasons. Namely, those resonant terms, which appears in the time evolution whenever the system goes back to the state b, is now fully accounted for. We've given them special treatment. And therefore, and this is the main result, the exposition which now is the non-trivial expression, the function of the kernel, has no divergences. light scattering. I just go now and apply to an excited atomic state. So the state we are interested in is the atomic state b and no photons. And the property of the atomic. state is obtained when we know the function Gb of Z. And this is the matrix element between state B0B0 and the time. evolution operator. So we are calculating, of course, the Fourier transform of the time evolution of the state b by the. Fourier. transform through the resolvent G. The real part is this matrix element squared, but double sum. But what we use is the principle part of it, which is well defined in the theory of complex functions. So the imaginary part gets us Fermi's golden rule. And the real part has actually-- remember when we discussed the AC Stark shift. And such AC Stark shifts which appear as self energies, as energy shifts created by the state, this is nothing else than the famous Lamb shift. But remember, we worked so hard with diagrams to make sure that the triangle-- first the square, and then the triangle. we calculate here-- has no resonant structure at the energy Eb. So therefore, we can neglect the energy dependence of that and simply replace the argument E by the energy we are interested in, namely energies close to Eb. This replace, neglect E and set, or replace the dependence by E, by taking the value at Eb, this corresponds to the Markov approximation. And that means in the temporal domain that we have a delta function. And we obtained, as promised, the imaginary part, which we can approximate by Fermi's golden rule. And you should now-- well, this is what we may have expected. Prof. Robert Schloss: " exponential decay is a simple approximation. It works very well. But at very early times, it will break down. Because then, the energy dependence matters" "We have to include now radiative shifts and an imaginary part for the decay to the time evolution" "The critical part is really the propagation of the state b, which is problematic" "If you're off-onant, you don't have a black line shift in the line widths" many modes. It emits photons and reabsorbs them. And you can often neglect that in the simple description of your experiment. But if you take certain expressions seriously, they would have divergences. And that's what we discussed without this infinite number of processes which happen. And yes, what we really need for a number of phenomenon in AMO physics for laser cooling, light forces, and much more are the optical Bloch equations. And so in this section, what I want to address at the most fundamental limit is, what is the step where we go from reversible equation, unitary time evolution, to something which is called relaxation, which is dissipative, where entropy is increased. at vacuum Robi oscillation and a few really neat things. But what happens is that the system is an open quantum system. You can have spontaneous emission. And if your mirrors are not 100.00% reflectivity, some light leaks out. So this is our system. And of course, if you write down the total Hamiltonian and do the time evolution, something will come out which, in general, is very complicated, very entangled. The atom is entangled with a photon which was emitted to the right side. And the recall of the photons push the atom. Einstein's rate equations describe the evolution of an atomic system by coupling it to the environment. But can you engineer the environment in such a way that it does something really fancy to your system? Well, you can dream of it. But you dreams are restricted by the mathematical structure of all possible master equations in the world. The environment can only do for you what can come from all possible Hamiltonians. The time evolution as a Hamiltonian, if you now bring in the environment, cannot be simply included by adding a term. display that, that we do not get any form of damping without at least the fundamental quantum noise. So what we need is we need a description of the quantum noise, which comes from coupling to the environment. The tool which we use for that is the density matrix. The density matrix can be written as a probabilistic sum over states. This will actually play a major role. We will make certain models for damping. And it's really beautiful. On Monday, I will give you the beam splitter model for the optical Bloch equation.

ROUGE-1: 24.61, ROUGE-2: 23.72, ROUGE-L: 24.21
BERTScore: 62.68

==============================================
==================== [71/100] ====================
Summary:
Inflationary cosmology is a relatively new subfield that's known as cosmic inflation. It's a framework for trying to understand the evolution of our universe over a huge expanse of time. It uses tools at the interface, not just of Einstein's general theory of relativity, but also ideas about particle physics and high energy phenomena. The asterisks are to remind you there's a set of strictly optional lecture notes on the Canvas site which go into a little bit more detail of some of these parts from the lecture. century, for 100 years or even more, that when astronomers turn their telescopes to the sky and look at many different length scales. Matter is not uniformly smushed out in space. There is actually teeming pockets of activity separated by large voids where very little matter or energy is located. It turns out that ordinary gravity-- even Newtonian gravity, let alone Einstein's fancier version that we looked at in class, general theory of relativity-- that these gravitational frameworks are sufficient to help us make sense of this hierarchy of scales. more and more uneven over time. So a challenge for astronomers for a century or so has been to try to make that account more precise and more quantitative and compare it with more and more kinds of observations. And there have been two main conceptual ingredients, especially as we'll come to in the more recent versions of this in the era of particle cosmology. One of the sets of tools, not surprisingly, is some theory of gravity. And the other main ingredient especially, as I say, refined in recent years with insights from high energy nuclear and particle physics. satisfy Einstein's equation. They took three particularly simple forms. Depending on the amount of stuff, depending on the distribution of matter and energy. If you had more than some critical value, a. critical value that came from the equations themselves above that, an overdense region. space itself would warp back onto itself like a closed sphere. You'd have a hyperbolic solution or an open geometry with negative curvature. And so you can have these global features, for example, on a positively curved geometrical surface. for an infinite expanse of time. But other colleagues showed at least it was consistent with his own equations to have universes that would change over time, that could either expand or contract. That was actually a prediction made by some of these colleagues even before some empirical evidence began to come in starting in the late 1920s. Hubble found this remarkable trend that the further away from us a given galaxy was, the faster it tended to be moving away from me further still. So you can actually then work backwards and say for how long has our observable universe been stretching? When did this stretching or expanding phase begin? originally Belgian. He studied briefly in Cambridge, England with one of the first converge to general relativity, Arthur Eddington. Then he came to MIT to finish his PhD and then was finding many of these solutions to Einstein's field equations even before Einstein did. And, in fact, Einstein came thinking he must be wrong, and then Lemaitre kept being right. So they became very nice colleagues. But Einstein started off by always being frustrated that Lem Haitre found solutions that Einstein found abhorrent or disgusting. George Gamow was advising two younger physicists, Robert Herman and Ralph Alpher. They realized that if the universe was very hot and dense at early times, then the conditions in which these elementary particles would find themselves should be quite different than what we find commonly around ourselves today. In a series of really quite ahead of its time farsighted work starting in the late 1940s, this trio and a small number of other colleagues around the world began trying to fill in this picture. And in fact, it soon became known simply as the Big Bang model. At early times in cosmic history, the universe should've been opaque. You literally wouldn't have been able to see anything because the mean free path of any given photon would be very, very short. Light can't propagate in a charged plasma because it's always bouncing between these very nearby free electric charges. At that moment, the average energy per photon or per elementary particle would fall so that you could actually form stable atoms of hydrogen. After that time, when the temperature has fallen below about 10,000 degrees Kelvin, photons are free. And then they can now travel large distances. energy continues to redshift. They lose energy as the universe continues to expand. So the energy of those photons would've started at the equivalent of around 10,000 degrees Kelvin and now today would be much, much,much lower than that. The universe has been expanding and draining that average energy per particle over time [CLEARS THROAT] so that today the universe should be filled with this remnant glow. This is all work that they predict around 1948, '49, '50, Gamov, Herman, and Alpher. 20 years later, two radial physicists working at Bell Labs, Robert Wilson and Arno Penzias, were using a new horn antenna sensitive to radial microwave and radial band frequencies. This should've been among the most precise instruments available on the planet for that band of the spectrum. And they couldn't get rid of a residual hum. This residual hum in your receiver consistent with an energy of about three degrees above zero, three degrees Kelvin, was really the leftover photons, that remnant hot radiation from the Big Bang. The idea was the whole universe is filled in the early times with very high energy particles that are at early, early times too high energy to form stable, electrically neutral atoms. And so from every part of space, from every single direction in the sky, those photons began to move freely at this single moment in time. And it's like sitting in a bathtub full of these photons. And they're just losing their energy as the overall size of space continues to grow. So the photons aren't coming from that direction of sky the way we think of with point-like sources. There's a galaxy there, a quasar, a particular bright star in our neighborhood. Astronomer Edwin Hubble was actually pretty lucky. He measured a much quicker average rate of expansion than what we have mostly settled on today. The picture was enough to get a small number of people to pay attention to Georges Lemaitre's otherwise quite obscure mathematical solutions. But it was Hubble's data that got the biggest splash, that made the biggest impact on the community at the time. But regardless of the data, we're just immersed in it trying to measure it as we flow through. whether we agree with the number he inferred of the actual rate, it seemed pretty clear to many people at the time that this was consistent with an actual overall expansion, with the change over time. Friedmann's and especially the follow up work by Georges Lemaitre, that made those mathematical solutions look much, much more curious and interesting than they had prior to Hubble's data. So this is a, I think, safe to say, remarkably successful set of ideas that eventually becomes called the Big Bang model. These convenient coordinates. Then you have to be a little more clever to adopt your clock to make things, again, actually really simple. This is actually called conformal time. That might remind you of our beloved friends, the 19th century Cambridge Wranglers. At least my beloved friends. We looked briefly at the Wrangler stuff at these conformal mappings. We're really doing a Wrangler-ish thing here, very similar idea, to adopt coordinates for the time, for the rate at which we think clocks should tick. Robert Dicke introduced this conundrum in 1969, so soon after the discovery of the cosmic microwave background radiation. He goes back to what I mentioned briefly before, that according to Einstein's equations, you can have these very simple geometries. If omega is larger than 1, you have more stuff for volume. You have a positive curvature. You expect it's open or hyperbolic geometry. So far so good. Then Dickes plugged this quantity into Einstein's own equations. And so what Dicske demonstrated is that this solution that looks like the Goldilocks solution is actually an unstable solution. universe should generically become more and more different from flat over time. If a universe started out being close to but not identically equal to flat at early times, it should look nothing like spatially flat at later times. Depending on the sign, it could either become more. and more like a hyperbolic saddle or more. like a closed sphere. That became known as a flatness problem. That was introduced by Bob Dicke in 1969. He introduced the next big real conundrum for the Big Bang model. We receive a remarkably uniform signal on the sky today from opposite sides. But those photons were emitted at a finite age when the universe was only a short portion of its current age. So the horizon distance was actually a factor of 100 shorter than the smoothness scale across which we receive remarkably uniform information. And how could that be? If this portion of the sky never had a chance to become in any kind of physical equilibrium or even exchange a single tweet, to have absolutely no information about what the average conditions are in this part of the universe? The Big Bang model had some amazing successes but some pretty stubborn quandaries as well. We can still ask about the behavior of some randomly drawn sphere even if the global shape of space might not be spherical. Any questions on the shortcomings of the Big Bang as people began articulating them throughout the '60s and '70s? Feel free to jump in or use the chat or either way. And again, there's more on the quantitative details of that in that optional primer you can find on the Canvas site. Alan Guth was studying high energy physics and therefore not gravitational cosmology. He accidentally heard some talks about some of his early work in gravitation. He was not originally asking questions about the cosmos, but he was haphazardly encountering some of those questions, again, very much like Tony Zee around the same time. What Alan was interested in was in things like spontaneous symmetry breaking and the Higgs mechanism, which was all the rage for a lot of particle theorists in the early and mid '70s by then. Alan Guth's notebook is now on display in the Adler Planetarium in Chicago. He realized that this kind of feature could actually lead to a cosmologically distinct kind of evolution. If the energy density, the stuff per volume, remains constant, then very counterintuitively, you have a runaway growth in the size of space. The stretch function, the scale factor going back just to Einstein's equations, will grow exponentially quickly, will have a period of accelerated expansion during which the universe won't just get bigger. Alan Guth was worried about exotic features from these Higgs fields that can get twisted up in some topological shape. He was really just wondering what happens if the universe gets stuck even temporarily such that the matter that dominates it, fills it, can't release or relax its potential energy arbitrarily quickly. That's called a metastable state. And if you go back to Einstein's equations exactly in the form that he began learning from Bob Dicke from that series of lectures, then you have these very different solutions for the average size of space. shape for the potential energy function just when you think about these fields like a Higgs-like field in the early universe. Now you'd see this expression, the deviation of the universe from spatially flat. That deviation should rapidly fall to 0. The universe today should look indistinguishable from a flat universe because the difference from flatness was driven to 0 dynamically. By having even a very brief phase of exponentially rapid accelerating expansion, you drive the universe towards a flat shape rather than having it flow away from aflat shape. we thought there was an origin to all of time, this Big Bang surface, at tau equals 0. And if you add up the time between tau and when those photons begin to travel freely, there was only a fixed horizon distance. Well, if inflation happened, there should've been a very brief period before what had previously been called the Big Bang. So we're adding more real estate along our time axis. We're unfurling a little bit extra time that hadn't been taken into account in the standard Big Bang model. distance and conformal time. You can see that as you trace backwards from today, instead of going back to saying the universe at early times should've been on the order of 1 meter, you say at those early times, the universe was actually exponentially tinier than you had thought. It grew exponentially quickly to map onto where we see today. And so the universe could very easily have been in a kind of equilibrium or at least a causally self-connected state. So during this tiny blink of an eye, the cosmos grew exponentially. why there's a primordial inhomogeneity. And you also have a reason why it's on the right length scales. It's going to seed galaxy formation, not mess around with your atoms like a Lamb shift because you have matter in an early quantum state as the universe is stretching exponentially. So you can go back to now a much more modern picture of a very tiny lumpiness captured in that microwave background radiation. This is from the Planck satellite team. It’s exaggerating with false color imaging the slight one part in 100,000 offsets between the regions of sky that are slightly higher energy photons in the CMB and slightly lower energy photons. solid green line is the generic prediction from the simplest models of inflation, what's the pattern of bumps and wiggles on the sky you should see today. The red dots are the actual observations from Planck team. And in many cases, the error bars are expanded so we can see them with our naked eye. This is such a precise set of measurements that, in fact, sometimes we have to make the error Bars larger. So now not only do we know do we live in a universe that is indistinguishable from flat as inflation suggests we should, but the actual pattern of those wiggled matches predictions to, again, better than a percent level accuracy. Inflation arises from types of matter in interactions that we now know exist that are heart and soul of this particle cosmology community. Cosmic inflation addresses several of these long standing conundra about the standard Big Bang model. It makes specific predictions for what we should see on the sky today, including very minute statistical predictions for things like the cosmic microwave background radiation. So why is the universe lumpy? Why is this cascade of scales? Because space time is wiggly, and matter is jiggly. why the universe is so messy is actually because Alan's been generating the mess in his own office, and it's expanded to cosmic scales. So if you want to study that part of today's lecture, it's probably the most important lesson, you'll ever take away. And I'll be glad to stay a bit longer if people have questions. Again, I'm sorry for running late. Feel free to drop off if you need. Any questions on that? The photos in Alan's office are on Canvas.

ROUGE-1: 31.12, ROUGE-2: 30.02, ROUGE-L: 29.71
BERTScore: 61.04

==============================================
==================== [72/100] ====================
Summary:
The Peloponnesian War was fought in 431 BC. After the war, Spartan power had grown to an unprecedented degree. For the first time there were lots of Spartans, who had lots of money. The Spartans had choices that they could take. They could either stay in the Pelop onnesus, or they could contest it in their power to control the entire Greek world in the east. Or they could have some control of the Aegean and the Hellespont. fear that the same fate they had visited upon some states that had defied them. In both places, the Athenians killed all the adult males on the island when they had finally put an end to the siege and sold the women and children into slavery. The Athenians had every reason to fear that that might be what happened to them. Instead, with Lysander very much in charge, they placed in power a small group of oligarchic Athenians just as he had the same kind of people in the rest of the empire. should be no democracy in Athens. It was an easy point of view to arrive at in 404. People who were not friendly to the democracy could simply point to the fact that the democracy had just lost this great war. And it was exactly the kind of idiotic idea that a democracy would come up with so that democracy itself was seen to be inherently wicked. The Greeks thought a division into two kinds was the right kind, the most important kind, a division between the high and the low, between the good and the bad. Critias, in any case, was determined that Athens in the future would not be a democracy. In fact, it looks like he was very much taken--again, this is typical, with the virtues of Sparta, because Sparta had won the war. Lysander agreed to the idea of making the Thirty compose of twenty men who were Critias' men, very extreme oligarchs, but allowing Theramenes, an Athenian general, who was very clearly not an old fashioned democrat. Spanish Armada was heading for England trying to gain control of the island for the Pope and Catholicism and one thing and another. A great wind came up and it blew the ships out of their path and wrecked many of them. So, from that day forward there sprang up the legend in England of the Protestant Wind, which had come along to save the new English faith against the forces of the Pope. Well, if they can invent a Protestant Wind I think it's okay for me to speak about the democratic snow that fell on Phyle. Lysander wanted to send a big army to restore the oligarchs to put his own people back in power. Sparta wanted to deprive Lysander of his power and influence and restore a more normal situation in Sparta. The Spartans did vote to send an army in there to deal with Thrasybulus, but they did not put Lysander at the head of the army or even one of his people. Instead King Pausanias was sent out to do the job. They worked out an agreement whereby a moderate group of ten would be chosen in Athens. A Roman historian of the first century B.C. wrote the following about Thrasybulus: "If excellence were to be weighed by itself, apart from luck, I believe I would rank this man first of all" A few years before 180 A.D., Pausanias the great travel writer of antiquity, wrote his guide to the famous and historic places of ancient Greece. "His is the first grave and after it comes that of Pericles," he says, "in every way the greatest of all famous Athenians"

ROUGE-1: 12.12, ROUGE-2: 11.34, ROUGE-L: 11.71
BERTScore: 56.72

==============================================
==================== [73/100] ====================
Summary:
The coherent state of the electromagnetic field has a simple definition, simple but subtle. It's an eigenstate of the annihilation operator, and it has a complex eigenvalue alpha. The person who popularized those states was Glauber, and he got amply rewarded for that. The coherent state has a time evolution. It moves in a circle. And this is really the phasor of the electric field associated with it. So coherent state, the alpha value is directly related to an electric field. That's why this state is closely related to the classical limit. In quantum theory, alpha is like the best description of an electromagnetic field. We ask, What is the probability that the electric field is alpha in the complex description? So that's what those quasi-probabilities are-- Q of alpha. And we immediately looked at some examples, of course, of probabilities. We showed that the vacuum state is sort of an area. It's a Gaussian. And the area is-- it's an area on the order of 1 or 1/2. In quantum mechanics, number and phase are complimentary. If the number of photons is fixed, you know nothing about the phase. The energy is sharp of a number state, since the energy is e squared. But what you get is also something blurred on the order of unity. And the time dependence is very easy. After all, we're dealing with an harmonic oscillator. In this plane, the quantum state is just rotating circle, a rotation with omega. And indeed we showed that when we apply the time evolution operator, it moves with e to the minus i omega t. And therefore everything rotates in a clockwise way. which is less fuzzy than the coherent state. So this fuzziness here is the intrinsic uncertainty of quantum physics. But then we will immediately start with non-classical states. And that is, well, if this area is determined by Heisenberg's uncertainty relation, what can be maybe deform the circle into an ellipse, and these are three states of light. That's what you're going to do in the second half of the class. But before I do that, I want to be a little bit more accurate about quasi-probabilities. the coherent state is now not this Gaussian. It doesn't have thisGaussian distribution as a course of probability. It's what you want-- what maybe some of you wanted to see-- oh, by the way, it's a delta function. The probability of the coherent state alpha has a delta. function peak at alpha, which is sort of nice. And the number state is not a ring of a finite radius. You would naively expect the energy is sharp. The square root of the energy's electric field, shouldn't it be sharp? And indeed, it issharp. on the level of whether something is a delta function or has widths unity. But if you map out something on a bigger scale, they are all related to each other. For P, it is-- the P operator is a dagger minus a. If we act with a on alpha, we get alpha, because alpha-- the coherent state-- is an eigenstate of alpha. Now, with P squared and Q squared, you have to use one or two more steps to get rid of the products. But ultimately, you can express space all that just in powers of alpha, alpha squared, alpha star. In quantum mechanics, the g2 function is not necessarily larger than 1, it can be smaller than 1. It's only possible if you have a truly non-classical state. The only way how you can measure the intensity of light is with a photomultiplier. This suggests that experiments where we look at the correlation function where we determine the expectation value should be defined as a dagger, a dagger squared value of absorbing 1 photon, which is a dagger. In many many textbooks, many many, many, textbooks would start with this expression. single mode-- and what is a single mode? It's just a sine wave. And nothing happens as a function of time. It's constant. Coherence time is the time for 2 modes to get out of phase. But if you have 1 mode, there is no coherence time. And when you find, for classical light, that the g2 function. has a peak which decays with the time, it is the. time for modes to getting out ofphase. But in a singlemode picture, this is absent. G2 for a single mode of the electromagnetic field is nothing else than a function. When you know what is an average and n squared average, you know your g2 function. There is another quantity, which we often use to characterize the fluctuations in the photon number. And this is called the Fano factor. If you use-- kind of pluck together -- kind of put them together, you get g2 of 0. For the coherent state-- remember, this is as close as we can come quantum mechanically to the ideal of a pure electromagnetic wave. This means it's Poissonian. the results we have obtained, it has a Fano factor of n-bar. So this is super-Poissonian. If the occupation number n is large, you have fluctuations which are much, much larger than Poissonian fluctuation. The g2 function, which classically cannot go below 1, is now n minus 1 over n. It is smaller than 1. The biggest violation for g2 is to go to minus 1 for the case of a single photon state. me first address one misconception. Coherent states, as I've just shown you, are very classical. They've always a g2 function of 1. And attenuation is not changing it. So a coherent state with an expectation value of 1 photon is not a single photon. But of course, there are ways how you can get single photons. And this is, well, you start with single atoms. If you have a single atom in the excited state, it can emit only one photon. one photon. Usually, we don't have the tools to prepare a single photon. We can prepare single atoms. And then we can make sure that single atoms create single photons. It's a little bit a way that we cannot control the bullets which are fired. But we can control the guns. And we make sure each gun can emit exactly one bullet. So that's a way how we can create non-classical states of light. Yes. So let's now look for those single photons at the quasi-probability distribution. Professor: "You have to prepare single atoms, which is difficult" Professor: "With n atoms, they have a super-radiant factor n with n atoms" "It's a very rich field. You have a rich field of possibilities," he says. "Spontaneous emission, pretty much by definition, goes into all spatial modes" "We don't have perfect single photon sources," he adds. "But we have a very, very high probability of finding this photon" The Hanbury Brown Twiss experiment was the first experiment which really looked at g2 functions correlations. It was the beginning of quantum optics and modern experiments with light. The classical limit is always a limit of high intensity, so at any given time, you have a ton of photons. If you put a light bulb into a cavity or couple the light from a light bulbs into a fiber, the light becomes spatially a single mode. That's the only way how you can distinguish a lightbulb from a laser beam. In an open system, you couple the system to a light source, which is always replenishing your experiment. But in those situations, we actually do not have the way-- we have actually a beam. And this requires a little bit different description. In the beam splitter, the other port of the beam Splitter should do some stuff, right? PROFESSOR: Yes. Even if you do not put in any light, we put in the vacuum state. Any question? I know I have to stop, but. The way we can distinguish from a thermal state and a coherent state is through the g2. Colin says there are actually more different light sources that just the laser and the thermal light source. There are LEDs or semiconductor devices, which provide photons with interesting statistical properties. OK. We have to stop. Colin. I'll see you on Wednesday. Back to the page you came from."Could we do most of experiments if we just had thermal sources that were very single mode, so to speak?"

ROUGE-1: 24.93, ROUGE-2: 23.52, ROUGE-L: 23.33
BERTScore: 64.14

==============================================
==================== [74/100] ====================
Summary:
This lesson will first dive into some signal Theory and then move on into things that we're more familiar with things like deconvolutions and using Transformers for next note prediction. The first thing we want to talk about is how can we sample and quantize a continuous time signal. The next thing we're going to kind oftalk about is changing forms right if if we're playing a song or playing a piece of music. And finally how how we can kind of generate sounds using these models. The analog to digital converter uses something called the sample and hold circuit. The digital analog converter is kind of the opposite except here we use a low pass filter. How many bits do we want to use to approximate a certain signal and the level of quantization here correlates very directly to the dynamic range of the signal that you're quantizing. How much time do you have to process this if this is something where for example a lot of musicians they want to sample their voice and Pitch it up very fast. is less than double of the highest frequency present aliasing will happen. This asserts that you need at least two samples per period. aliasing is the byproduct of poor sampling. A lower wave resolution will result in a modified output signal as compared to the original input that we're trying to process. There's a lot of literature about um aliasing effects including spatial illnessing. The aliasing phenomenon is incredibly interesting this happens both visually and auditorily. It's a very prevalent problem in any problem. that can tie in to reconstructing signals right um we we are also going to talk about um using deconvolutions. We talked about the unit architecture very thoroughly used for image segmentation we talked about it in our survey of uh computer vision techniques. This is something that uh that is kind of going to come back in terms of how we can reconstruct signals so the inner product and projections are an application of inner products where one vector can be projected onto another Vector and you can you can kind of see what the projection is based on. is that as our our basis differs as long as these are orthogonal vectors and they span the complete basis of a certain Vector we're able to reconstruct this Vector. The idea behind the the last two sections here was to give you motivation for for how signals work and how kind of classical reconstruction can occur using math that we're all familiar with. The next step here we want to use deep learning for reconstruction right where we are are reconstructing a low quality audio to high resolution audio. Transformers can help increase our sample of training data and generalize key scales and beats throughout a data set. A single song can be transformed into 12 songs of different Keys. The more data you have the better your model will be and the more generalizability you have in your Transformer the better it'll perform. Transformers will far outperform classical methods of of both computer vision and natural language processing. It's easier for machines to predict keys without flats and Sharps which has you know similar to what humans do. are the original Pachelbel's Canon um as you can see this does deviate a bit but honestly it sounds pretty good. The Transformer model is able to do this next note next sequence prediction pretty pretty well. So yeah there's a a lot to do in this field um a lot of really cool things happening um and yeah I hope you guys learned something about uh about generative audio today and are inspired to kind of give some of these things a try yourself. thank you guys for tuning in have a good one.

ROUGE-1: 15.34, ROUGE-2: 14.33, ROUGE-L: 14.47
BERTScore: 64.54

==============================================
==================== [75/100] ====================
Summary:
The amygdala is closely connected to the basal forebrain. In discussions of aging in human pathologies, you always hear about the basalForebrain. The amygdala primarily projects to the more caudal one here. This is where they overlap. That's the bed nucleus of the stria terminalis, which we mentioned last time. And I've indicated in blue there the acetylcholine containing neurons that you see in the medial septum. You see them in this diagonal band of cells. And then the basal nucleus at the bottom here. Schizophrenics have larger brain ventricles, an indication of early damage. Schizophrenics are the hardest to treat and often are in mental hospitals for most of their adult life. The earlier the lesion in these other systems, at least, the greater the plasticity, that is, more sprouting, more chances of regeneration, and so forth. But here we're talking about sprouting. These are the connections I'm talking about. In green there I show the very widespread projections of the catecholamines. Early in evolution, there was no dorsal striatum. It was a link between the olfactory, [INAUDIBLE],, and motor control. And we also know the outputs of that region go to hypothalamus and subthalamus. They influence the endocrine system and motivational states by these projections. They also have some connections in the midbrain, where they can influence the stacking patterns, especially locomotion. If the prefrontal cortex is functioning abnormally because of sprouting of these axons, then binding to the receptors will move it more towards the normal. The subthalamic nucleus excites both segments of the globus pallidus and the nigra. So it's unique in that way. And it's critical for the balance of this system. So anything that goes wrong with it can cause major problems with movement. For people to see how knowing these connections explains some of the disorders. So we'll leave it here. But just note, if there's just a subthalamic nucleus-- here's the Nigra down here. So these are the two main satellites of the corpus striatum.

ROUGE-1: 12.11, ROUGE-2: 11.33, ROUGE-L: 10.51
BERTScore: 63.37

==============================================
==================== [76/100] ====================
Summary:
Learn how the solar cell device converts sunlight, the input energy, to some usable output energy, which is in the form of electricity, typically, from a solar panel. Learn how to minimize the amount of light reflected or not absorbed into maximizing amount of life that's actually absorbed. Learn about the duality of light, or how to think about light as a particle, or alternatively, as a quantified particle. Use the weekly Newsquiz to test your knowledge of stories you saw on MIT OpenCourseWare. The peak of the solar spectrum at 550 nanometers, somewhere around 2.3 eV. When we start looking at the wavelength dependence of absorption inside of a material, you can have, for example, in the visible range, a decreasing depth of penetration of the light with increasing energy. With x-rays, it's the exact opposite. It's because you're dealing with different types of electrons and the material. The real component of the refractive index is material-specific property. We don't have to dive too deeply into that for the purposes of the class. the interaction of light inside of a medium, inside ofa material. And we use that information to calculate engineering relevant parameters such as reflectance of light off of a surface. So I have an equation here that describes the reflectance. Let me dive a little deeper into it and try to understand what exactly that equation is telling me. So this is a method for you to gain a foothold in this new area of understanding the refractive index of a material based on something you've already seen before. If I add a coating, for instance, to a window that increases the reflectivity, then the amount of light that is able to escape from the inside to my eyes decreases. With normal incident light, there is a beautiful symmetry involved. So just the same way that I'm losing the ability to see inside, the folks inside are also losing the able to see out. So it's important to think about these processes, both in terms of their reflectance as a percentage and the magnitudes of the light involved. very simple yet very powerful formulation that describes not only the interaction of light with the solar cell material but also light through the atmosphere, light the water, many other forms of optical absorption. And for that, I'd like to call Joe up for a quick demo that will allow us to actually plot out Beer-Lambert's Law. What we're going to be doing is taking many sheets of material. This is just some polyethylene material, a little bit discolored. And we'll be inserting these panes of plastic in the middle. And as we increase the thickness of the plastic, applying good pressure in between to minimize the reflectance. another factor of 2. Why not? There's kind of this sense that it should be exponential. What don't we add some more filter in front, and we'll see what exactly this comes out to be. OK, so we notice that we have some exponential character to be decay of the intensity of the transmitted light through a medium. And the amount that's absorbed is following another trend, which is just 1 minus that. So it's the amount of light that'sabsorbed is following a curve looks something like that. by some sort of scattering intensity within the medium. The sigma here can refer to a variety of processes. That can refer. to absorption events that result in the generation of free charge. The alpha, on the other hand, is not a geometric parameter. It's an intrinsic material parameter. So if we increase the total thickness, we're going to decrease the total amount of light coming through via that exponential function. That general equation is the same one that drives the reduction of light intensity as it travels through the atmosphere. It doesn't really matter what sort of absorption process is happening inside of a material for us to calculate the amount of transmitted light. for our atmosphere than it was for these little polyethylene sheets. Because the nature of the scattering and absorption processes are very different for the atmosphere. Oftentimes we're operating in a wavelength regime of light wherein free charge is excited. But we can also keep increasing that the wavelength of light, say, out to 10 microns. And that can excite free carriers within the material-- carriers that are already excited, essentially excited them further, without generating any new free carriers inside of our material. We're going to calculate the thickness necessary to absorb 90% of the incoming light at 550 nanometers. Did anybody manage to walk all the way through that calculation? Did anybody get any other numbers for gallium arsenide? Is the thickness needed to absorb the same amount of light going to be greater or smaller? For silicon, crystalline silicon that is, with an optical absorption coefficient and order of magnitude less than galliam arsenide, is the thickness required to absorb that much light? At 800 nanometers wavelength light, the optical absorption coefficient is dropped by about an order of magnitude relative to the peak of the solar spectrum. Most of these solar cells that you see of crystalline silicon are on the order of 100 microns. The record efficiency of gallium arsenide solar cell is a few hundred nanometers thick. What could you do to your solar cell device to increase the total amount of light absorbed inside of it? Let's do something much more simple. Put reflective coating on the back. 10% of the light that didn't make it, that's going to get reflected back. So the very simplest thing we can do on the front surface is to texturize our front surface. Texturization increases the total amount of light that gets reflected, 1 minus 0.9 squared as opposed to 1 minus 1.9 to the 1. In one bounce, rather, and two trips, two optical path links through the material. And so the term optical path length is a very important term here, because the Optical path length does not have to be the thickness of the material itself. A texturized front surface increases the.probability that light will enter the device. And what it also does-- this is a secondary benefit-- is it increases the path length of the incoming light. Snell's Law is the product of the refractive index and sine of that angle, the angle relative to the surface normal. So a simple way to think about this is when the light goes from a low index of refraction medium to a high index ofrefraction medium, light bends toward or away from the normal. most often, depending on the angle. You have what is called total internal reflection, which is this case right over here. So that's one of the reasons why you see this white spacing, the white colored material, in between the cells, is that the light gets reflected off of there. Even if the panel looks black, there are some really aesthetically pleasing solar panels out there that look completely black. They may still have white back skin, but the glass is just very good at absorbing that light and preventing it from escaping. of these two do you think is which? Why don't you turn to your neighbor quickly and chat about it without peeking at your lecture notes. Think about what would happen to the reflectivity of that front surface of the water. So if we go to a refractive index material of minus 1.3, will we change thereflectivity at all? It depends, but the answers here are shown, for this particular system. It would require sitting down and walking through the equations, but in essence right here, you're really affecting the angle at which light is coming out of the pool. Lambertian scatter is a very loosely used term in the solar industry. It is wrong by the book, but nevertheless, it's one of these things that live on in our industry. The back skins of our solar modules can quite often be Lambertian scatters. And we have a certain amount of light that comes off at some angle that will get trapped by a total internal reflection inside of a modules. And so these scattering centers off the backs of the rear sides of cells would operate more or less in the following manner. You'd have incoming light. Let's ignore front surface texturing for now. You can also texture the bus bars. The bus bars are these little metal wires right here that are collecting the charge from each of the solar cells. The light bounced here on a textured bus bar, bounced off of the glass more or less around here halfway, and then got a second chance to enter the cell over here. Obviously some of it is reflecting off so we can see it. But a lot of it's going in. optical path length by a factor 50, relative to the thickness of your material. If you have an organic material, which has a refractive index typically of around maximum 2, then that would be squared, 16. You can increase probably in the order of 20 the optical path length inside of the material through texturization. So that's a useful parameter to keep in your mind. Let me touch upon a few other forms of trapping light. We've so far just assumed that light behaves like a continuous wave, doesn't interfere with anything. Now we're going to discuss some anti-reflection effects. The absorber is the material, our photovoltaic material, the ones absorbing the sunlight and ultimately going to be generating the charge. So we want to ensure good light trapping inside it. There are fancier ways of light management as well that don't involve light trapping necessarily but light manipulation or even semiconductor manipulation. If we can eliminate the longer wavelength stuff out here, which is heat, performance of most solar cell suffers when they get hot. And so if we manage to do spectral up converting or reflect that long wavelength light away from our device, we can improve performance. surface of a material, let's say right here, then you can cause each node, each point within your material, to lag by an increasing amount, so that your wave front now bends. And that will cause the light, essentially, if you trace through the points of maximum intensity, say the pink, you'll see that the light is bent. And so it's really exciting. There's stuff coming up every day on light trapping and light management. Mostly it's for photonic devices. But they can be transferred over into solar cells as well.

ROUGE-1: 24.88, ROUGE-2: 23.69, ROUGE-L: 23.53
BERTScore: 61.16

==============================================
==================== [77/100] ====================
Summary:
Professor Steven Smith: I want to look at two sets of issues. One is Locke's theory of the constitutional state, particularly focusing on the role of the executive, vis-a-vis the legislative branch of government. The other is thinking about Locke and the American regime and the current state of political philosophy, modern contemporary American political philosophy. Smith: Locke doesn't endorse necessarily one particular form of government from any other. He is an advocate of what we have come to call limited government, of constitutional government. The many faces of modernity are working themselves out. We are but a moment in the kind of comprehensive self-dissatisfaction that is modernity. A return to Lockeanism, in many ways, is not so much a cure for the pathologies ofmodernity. I would suggest that those pathologies are themselves already rooted in the pathologically of Locke. I will end on that sober note and encourage you to take Rousseau's advice about loving one's country seriously on Tuesday.

ROUGE-1: 5.69, ROUGE-2: 5.42, ROUGE-L: 5.52
BERTScore: 65.39

==============================================
==================== [78/100] ====================
Summary:
JACK HARE: Let's do a little recap on electron cyclotron emission. We expect to have multiple different peaks, even from a single particle. These peaks are going to be occurring at frequencies. These frequencies depend only on the magnetic field. And so, if you see some emission at some certain frequency, then you know that it's been emitted by a region of plasma which has this magnetic field, he says. He says the exact shape will depend on exactly how big these two terms are with respect to each other. optically thick regime. So that is the region of plasma that's emitting is emitting as a black body. And that means the black body spectrum I of nu, is equal to T nu squared upon c squared. And so, for example, we might have a region down here corresponding to lowish frequencies at low magnetic fields. We might have another region where the frequencies correspond to the center of the plasma. And at each of these points, if we measure the intensity, we then know straight away what the temperature is corresponding to that frequency. what we want to do is measure very small temperature fluctuations. And that 1% is actually extremely hard to measure. And this is because the noise is just too high on these systems. But there are some clever tricks that we play where we use correlations. And I'll talk now about what exactly these correlations are and how they provide us with information that allows us to get a signal out despite the overwhelming [INAUDIBLE]. So our setup here is borrowed from ASDEX Upgrade. And, in effect, I referred to Alex Creely's PhD thesis, which you can find online if you want more information. We're not trying to measure the temperature profile throughout the entire plasma. We want to measure it inside some very small region. So the size of our turbulence of R turb, is on the order of 100 microns. That's the width of a human hair. We're zoomed in on only quite a small frequency range. Each channel samples a non-overlapping region in frequency. And, therefore, in real space, because, again, we've got this very strong link between our magnetic field and our spatial position. The idea here is that there was some region over which, in the transverse direction perpendicular to your collection volume, you have a very narrow scale. You can actually collect from a very small region on the order of 100 microns. And if it collimated that beam, that would mean there'd be a focus point at some distance f away-- if this lens has a focal length f, then it will diverge afterwards. So, because of reciprocity, that means that, as opposed to launching rays this way and seeing where they focus, we have rays coming from this. The technique is incredibly powerful because it's enabled people to measure, again, delta on the order of 1%. Someone called it nominative determinism, and they've done it on 1%. And it's used in a variety of applications, but this is an example of what these angle brackets are doing. There are actually lots of different ways of doing this, but there are a few different ways to do correlations, and I'm not going to go into them, but I will give you a citation at the moment. we have positioned these two volumes, which are producing frequencies omega 1 and omega 2. And we think that that distance is smaller than the size of our turbulent eddy. The temperature should be the same going up and down. And if you do this correlation and you get out nothing, that probably means your volumes are too far apart because there is-- this T correlation would just go T1 T2. And there's no good reason to believe those temperature fluctuations are correlated, because they'll be part of a different turbulent Eddy. Bremsstrahlung is a term used to describe a type of radiation in a nuclear reactor. It involves electrons being deflected and breaking and emitting photons. There are lots of different ways of doing this, and Hutchinson lists a few of them. What's remarkable about all of these approaches is they all give the same answer with a very slightly different coefficient. But, in some sense, although it's important to get the exact coefficient, it doesn't matter exactly which one of these techniques you use. from the point of view of this course, it makes no difference. Yeah, I think it's kind of remarkable that it doesn't make any difference. So, again, if you want the full treatment, go have a look in Hutchinson. And there's also a long treatment in Jackson of this same problem. I'm just going to quote some results. I kind of already spoiled it now. It's here. For the Maxwellian average, because we can have all sorts of different distribution functions, but our plasma tends towards a Maxwellian. The bremsstrahlung is the irreducible minimum amount of emission from your plasma. Cyclotron emission is a very specific frequency. This is everywhere inside your plasma at all frequencies, like a black body kind of spectrum here. That goes down to very low frequencies and goes up to very high frequencies. We're going to talk about lots of other effects which produce emissivity which is higher than the brems stahlung. But, often, when we're doing power balance calculations, we will just use this. emits. And then the wave has to ask itself, well, what sort of wave am I? And, in reality, there'll be some emission in O mode, some emission. in X mode. And the exact coupling between those will be related to the polarization of the bremsstrahlung. And so, that means for any reasonable distance of plasma that I'm looking through, that will be absorbed, and it will become black body. There's another effect, which is in Hutchinson's book, which I haven't covered here. But it's in there if you're interested. Jack Hare: I don't know if people are using it as a diagnostic. I've not heard of someone using it. But synchrotron light is used as a source of X-rays for diagnosing many other things. So it's interesting in its own right. But I'm not interested in [INAUDIBLE] This is a diagnostics course. JACK HARE: Any questions online while we pause? We're going to do free-bound radiation or recombination radiation. where we have a range of different discrete energy levels that the electrons can occupy. These energy levels are labeled by the principal quantum number n. And the energies of these levels are given by this unit, Ry, which is the Rydberg z squared of our ion over n squared. Up here, infinity, this is ionization. If your electron gets this much energy, it becomes free again. And so, what we're going to see in our spectrum is that this is only allowed if the electron energy fulfills this equation. I understand. So I was just wondering, because it's not the condition that there are photons that are released with greater than 12 mv squared the kinetic energy. It's the photon energy that's-- yeah, maybe we're doing this backwards in some sense. If you see a photon being emitted with more than 1/2mv squared, then the electron has over-emitted. And the only way it could have done that is if it fell down into one of these principles. The energy levels are in different places inside here. And the energy levels shift by the ion charge here. One use of this is a diagnostic called bolometry. It cares not at all about the detailed spectrum of what the emission is. It just wants to know how much power is being radiated by the plasma. So this is what bolometry is trying to measure. And a really simple way to do it is the integral d nu of j of nu. Any other questions? Let's keep going. do bolometry is we have some radiation coming out of the plasma. We apply some voltage, V0, over this resistor. And we also put another resistor that is shielded from the plasma that we call R. Effectively, what we're trying to do is measure this volt-- this resistance M of T. And the voltage that we measure, VR, is equal to whatever voltage we used across all of this system R divided by M. And, from that temperature, we can make an estimate of the radiation power incident upon it. setup is your entire bolometer is going to heat up. And so, what you want to know is not just how hot they are, but howHot they are relative to the vacuum vessel. So, this is why we have this system where we're trying to measure a very small change between R1 and R2, which would be identical, and M1 and M2. And it's this small quantity here, delta T, that's due to the radiated power that we're really trying toMeasure. have a thick block some distance in front of it so it can't see the plasma. It's the heat transport kappa grad T that gives us the time constant for thermal conduction through the substrate from the absorber to the resistor. The larger tau is the slower our measurement of the radiated power is going to be. And if tau gets very large, because we've got a very thick substrate here, or it doesn't have very good heat transport, then we're going to have a very poor time resolution.

ROUGE-1: 25.20, ROUGE-2: 24.12, ROUGE-L: 23.54
BERTScore: 64.99

==============================================
==================== [79/100] ====================
Summary:
So, last time we were starting to talk about the sort of the general overview of what reinforcement learning involves. We introduced the notion [NOISE] of a model, a value, and a policy. Can anybody remember off the top of their head what a model was in the context of reinforcement learning? So what we're gonna do today is, sort of, um, build up for Markov Processes, up to Markov Decision Processes. And this build, I think, is sort of a nice one because it allows one to think about what happens in the cases where you might not have control over the world. from sort of a probability distribution. So, if we have a finite number of states in this case R can be represented in matrix notation which is just a vector because it's just the expected reward we get for being in each state. Right now we're still in Markov Reward Processes so there's no action. The ways you could define rewards would either be over the immediate state or state and next state. Um, but it often we think about the case where, um, an agent might be acting forever or this process might be going on forever. The definition of a return is just the discounted sum of rewards you get from the current time step to a horizon and that horizon could be infinite. If Gamma is equal to one, then that means that your future rewards are exactly as beneficial to you as the immediate rewards. If you're only using discount factors for mathematical convenience, um, if your horizon is always guaranteed to be finite, it's fine to use gammaequal to one in terms of from a perspective mathematical convenience. But in the general case, we are gonna be interested in these stochastic decision processes which means averages will be different than particularly runs. There, one could try using other participant is certainly the most common one and we'll see later why it has some really nice mathematical properties. So, if we go back to our Mars Rover here and we now have this definition of reward, um, what would be a sample return? So, let's imagine that we start off in state s_4 and then we transitioned to s_5,. s_6, s_7 and we only have four-step returns. And of course we could define this for any particular episode and these episodes generally might go through different states even if they're starting in the same initial state. The question was was if it's possible to have self-loops? Um, could it be that this is sort of circulator defined [NOISE] in this case. Is it ever actually possible for, uh, that matrix not to have an inverse or does like the property that like column sum to one or something make it not possible? It's a good question. I think it's basically never possible for this not to has an inverse. I'm trying to think whether or not that can be violated in some cases. Markov Decision Processes are the same as the Markov Reward Process except for now we have actions. So, the agent is in a state they take an action, they get immediate reward, and then they transition to the next state. And as was asked before by Camilla I think, the reward can either be a function of the immediate state, the state and action to the state action and next state for most of the rest of today we'll be using that it's the function of both theState and action. An observation you'd see something like this s, a, r, and then transition to state s' And so a Markov Decision Process is typically described as a tuple which is just the set of states, actions, rewards, dynamics, model, and discount factor. Because of the way you've defined that dynamic model, is the case that if you take a specific action that is intended for you to move to your state s', you won't fully successful move to that state? Like I guess I'm curious about why there's a- why there is a probability at all? If you have an MDP plus a policy then that immediately specifies a Markov Reward Process. So, in order to learn what is the value of a particular policy we instantiate the reward function by always picking the action that the policy would take. And then for whatever state I end up by next continuing to follow this policy. So that's what the V^pi_k-1 specifies. What would happen if the expected discounted sum of rewards we get by continuing to. follow policy from whatever state we just transitioned to. you do this computation. Just to quickly check that the Bellman equation make sense. So in this case, um, because p is a stochastic matrix, its eigenvalues are always going to be less than or equal to one. And that's back to my original question which is you seem to be using V_k without the superscript pi to evaluate it. Oh, sorry this should, yes. This should have been pi. That's just a typo. And this is just an example of how you would compute one Bellman backup. before we do this let's think about how many policies there might be. So there are seven discrete states. In this case it's the locations that the robot. There are two actions. I won't call them left and right, I'm just going to call them a_1 and a_2. Then the question is how many deterministic policies are there and is the optimal policy for MDP always unique? So kind of right we just take like one minute or say one or two minutes feel free to talk to a neighbor. Policy iteration is a technique that is generally better than enumeration. So, in policy iteration what we do is we basically keep track of a guess of what the optimal policy might be. We evaluate its value and then we try to improve it. If we can't improve it any more, um then we can halt. If you have a lot of compute, you might just want to and this might be better if you really care about wall clock and you have many many many processors. But if you don't have kind of infinite compute, it's generally more computationally efficient if you have to do this serially to do policy iteration. a Markov Reward Process. And then we do policy improvement. A state action value says well, I'm going to follow this policy pi but not right away. So, that defines the Q function and what policy improvement does is it says okay you've got a policy, you just did policy evaluation and you got a value of it. The next critical question is okay why do we do this and is this a good idea? So, when we look at this, um let's look through this stuff a little bit more. The key question of whether or not value iteration will converge is because the Bellman backup is a contraction operator. The distance between, in this case we're gonna think about it as two vectors, doesn't get bigger and can shrink after you apply this operator. So how do we prove this? Um, we prove it- for interest of time I'll show you the proof. And then the next thing we can do is we can bound and say the difference between these two value functions is diff- is, um, bounded by the maximum. between the two value functions can't be larger after you apply the Bellman operator than it was before. There has to be a unique solution. It's also good to think about whether the initialization and values impacts anything if you only care about the result after it's converged. All right. Class is basically over. There's a little bit more in the slides to talk about, um, the finite horizon case, and feel free to reach out to us on Piazza with any questions.

ROUGE-1: 18.23, ROUGE-2: 17.82, ROUGE-L: 17.28
BERTScore: 66.24

==============================================
==================== [80/100] ====================
Summary:
 angular momentum is a set of operators that provide observables, things we can measure. We showed that any component of angular momentum, be it lx, ly, or lz, commutes with l squared. For the l squared operator, we also explained that the eigenvalue of this operator should be positive. We figured out by looking at this differential equation that if we wanted single valued wave functions-- wave functions would be the same at phi, and at Phi plus 2 pi. an equation for what were eventually called Legenre polynomials. The answer is that if you have a system you want to figure out what are the properties of the states. If you can also know the angular momentum you learn more about the physics of this state. So in general, it's a most important question to try to enlarge the set of commuting observables. Leading finally to what is initially called a complete set of commutative observables, or commutators. and an electron we can reduce this system to as if we had one particle in a central potential. So that will be also very important physically. And here there is a simple observation that one can make. Is that the differential equation for p l m depends on m squared. We expect to need values of m that are positive and negative. The complex conjugate ones should be thought as having m negative. So how did people figure this out? They, in fact, figured out that if you have these polynomials you can create automatically the solutions for this equation. In general when we choose a general l, if you choose an arbitrary l, then m goes from minus l, minus l plus 1 all the way up to l. If you choose state with l equals 1, or eigenfunctions with l equal 1, there is the possibility of having m equals minus 1, 0, or 1. The spherical harmonicas are going to be those wave functions. And they have a normalization, n l m, an exponention, and all that.

ROUGE-1: 29.43, ROUGE-2: 28.16, ROUGE-L: 26.85
BERTScore: 69.08

==============================================
==================== [81/100] ====================
Summary:
In order to do that, I basically have to do the integral. So here it is. We have psi of x and t. It's integral dk phi of k e to the ikx minus omega of kt. If you want to see the distortion, you have to keep that [INAUDIBLE]. We'll do that in a week from now. And then, you say, look. There's lots of things making it look like a difficult integral, but it's not as difficult as it looks.

ROUGE-1: 15.65, ROUGE-2: 15.13, ROUGE-L: 15.65
BERTScore: 62.97

==============================================
==================== [82/100] ====================
Summary:
There are three common uses of a rotation matrix. The first is to represent an orientation. The second is to change the frame of reference of a vector. And the third is to rotate a vector or frame. To demonstrate these, I will use these three coordinate frames, representing the same space with different orientations. To help you visualize these frames in 3 dimensions, I’ll use my handy tinkertoy frame. This is the z-axis, this is the x-axis and the y-axis. If you premultiply by R, the rotation axis is interpreted as the z-axis of the frame of the first subscript, {s}. You end up with a rotated frame {c-prime}, still expressed in {s]. If you postmultiply R, you get a different rotated frame  c-double-prime. In the next video, we will learn how to represent the angular velocity of a frame. In summary, a rotation matrix has three uses: representing an orientation, changing a frame of reference of a vector or a frame, and rotating a vector.

ROUGE-1: 42.53, ROUGE-2: 40.68, ROUGE-L: 38.40
BERTScore: 74.93

==============================================
==================== [83/100] ====================
Summary:
Coded imaging is a co-design between how you capture the image and how you process the image. The concept of a position or superposition applies to all three types, shadows- or refraction- or reflection-based techniques. We'll see how-- we already have some projects that are inspired by biological vision. And we'll see it in a taxi zipping very fast, which is a clever way to take a photo. And I believe Santiago-- where's Santiago? Oh, yeah, his triangle-- the piston. The problem is that lower frequencies are actually being set to 0, which means that in this photo, these frequencies are missing altogether. And there's nothing you can do to recover those frequencies because in the Fourier domain, all you have to do is multiply each of the frequencies in the image by the amplitude of the Fouriers transform of this. So the culprit here is really this low pass filter where some of the even lower frequencies were also being nullified. And if I tried to recover from this photo there is no chance because I have already attenuated and have lost all those frequencies. box function, which is equivalent to-- when you release the shutter, opening the-- release your shutter button. Instead of keeping the shutter open for the entire duration, you open and close it in a carefully chosen binary sequence. Here, it's closed for quite some time, open for a short time. And so on. At the end, you still get just one photo. But now something magical has happened because first of all, if you look at this number one, you'll see that it's not the same as before. that's your 1010 inquiry. Instead of keeping the shutter open for the entire duration and getting a well-exposed photo, the shutter is open for only half of the time. The support for the representation of the Fourier domain of that function that you describe there is infinite, right? So you actually truncate this in order to-- RAMESH RASKAR: It's not infinite because you still have some width. But with a vector length of 52, this filter is in time. filter from space? RAMESH RASKAR: It corresponds automatically to filter in space. So your actual blur in the image may not be exactly 52 pixels. It might be 10 pixels or it could be 100 pixels. And you're saying that it also depends on how far the object is in space because faster-moving objects. So whatever is moving has to move at a constant speed. If we did 100 milliseconds, it picks up speed, then your assumption that the 52-length vector will map to some stretched or shrunk version of 52. point spread function for your time of flight. So that's the same concept here. You just want to call leading the world, take a picture, and see how it works. And this whole field of order dimension is basically engineering of the point spread function. So we're going to engineer activity of the camera. So in this particular case, a point that was moving created a blur like this. And by engineering the time point spread. function, it stops looking a bit like that. It's going to look like fashion [? wise. ?] The goal of coded imaging is to come up with clever mechanisms so that we can capture light. The circuit is very, very simple. You just take the hot shoe of the flash, and it triggers. When you lose the shutter, it triggers the circuit. And then you just cycle through the code that you care about. What can we do for defocus blur that is for motion blur? What can you do fordefocus blur? We, again, want to engineer the point spread function. An alligator came out of the water, and he lost his balance, and the boat flipped upside down. He managed to flip back in. But it completely damaged his camera that was with him, and it just wouldn't work. So he just took out his lens, which is a standard Canon lens. And he said, let's open it all the way. It had all the mud in it and so on. And then he just showed me this thing as is. It's amazing the way it's structured, right? and create one single center of projection for normal cameras. For professional lenses, that's not true. For normal cameras, you have the central projection. But again, conceptually assume that all the rays are going through that point because you can replace this whole thing by one single lens in a [INAUDIBLE]. So finding that plane is actually a tricky problem. But in retrospect, it's very easy. If the lens makers are putting everything there, we should put a recorded aperture also in the same plane. But placing it over there, it turns out you get the same blur. around at the same time of how to make this happen. RAMESH RASKAR: So in 1D, this is what we saw, right? Its Fourier transform is flat. So there are 52 entries here, and almost all of them are the same. Now we're saying, think about the problem in 2D. And what's the Fourier transforms of this? So first, for this one, the Fouriers transform is-- as we see, it's black. And then if you take that in 2d-- so how is the code? I'll give a hint. the values will be constant. So if we're placing a broadband code, certainly we have an opportunity to recover all the information. In communication theory, everything is [INAUDIBLE].. We think about carrier frequencies of radio stations in frequencies. And convolution, deconvolution-- much easier to think in frequency domain. Although all the analysis in the frequency domain, at the end, the solution is very easy-- just flutter the shutter or just put a coded aperture. Extremely simple solution to achieve that. that, or just a software. There are methods you can employ. You need to find this 7-by-7 pattern or even the previous case, the 52 pattern. Take a Fourier transform to see if it's flat. If it's not flat, you go to the next one. So 2 the 52 is pretty challenging. But even if you use a cluster, it's still a pretty big number. So you can start with some code and do a gradient descent and so on. good solutions for 2D. But for 1D, there are some really good solutions to come up with that. For 2D, for certain dimensions, they call it one more 4 or three more 4 because prime numbers can be 1 or 3. And there are certain sequences that are beautiful mathematical properties, of which sequences could have broadband properties and which may not. So it turns out you cannot really use the broadband code here either to give you the best result. But the traditional code's called MURA code, M-U-R-A. In astronomy, you have circular convolution because they use either two mirror tiles and one sensor or one mirror tile and two sensors. If you tile aperture, you'll get really horrible frequency response, unfortunately, because if you put two tiles, that means certain frequencies are lost. In this photo, those frequencies are not lost because all the frequencies are preserved. But that's because our eyes are not very good at thinking about what the original image could be, given either this one or the previous one. Ramesh Raskar: Coded imaging is elegant and beautiful and sometimes complicated. He says there are many ways of engineering the point spread function. RaskAR: For any continuous code, there is a corresponding binary code that will do an equally good job. "It's just one of those things. It's like we are sick of it, so we don't want to do it, but I think it's worth trying," he says of orthogonal motion blur. The effect is very low, though, remember. The effect is extremely low. So maybe you have a pixel and get blurred by 10 pixels or [INAUDIBLE]. It's not a global effect. So this picture, maybe-- this particular diagram is misleading because it seems like this point is going to go all the way. But this is very narrow. And the colors are, for all practical purposes, that'd be the same. So we're painting the rays. We're just adding one glass. blur is only about 10 pixels, no matter where you [INAUDIBLE]. So maybe that was the matter. If you have a point of access, it's still going to create an image that's blurred 10 pixels. This is, again, very counterintuitive, where you go to make the image intentionally blurred. It's just that it's blurred everywhere. And then we also saw this one very early on, where the point spread function-- typically when something goes in and out of focus, it looks like a point. Ramesh Raskar: Compressed sensing is taking a photo and compressing it. He says the idea is to take a single photo and then recover it in a compressed way. Rasksar: If you're on 2 megapixels, then you need to take 2 million [? pics] All right? So the claim this group made at Rice University was that if I wanted a million-pixel image, I don't have to really take a million readings, he says. Rasa: I can take this picture effectively with just 10,000 pixels but recreate a million pixel image. Photography is a record of visual experience, which is great for humans, but it's not so great for computers because computers don't understand any of that. Computational photography is a very, very active field. The secret of success for film, of film photography, is that if somebody had given you this problem before the invention of film, that there is a scene-- and I want to give you a sensation of the same scene-- time shifted or space shifted. You can start with a reproduction of a photo, or you can tap into retina, V1, V2. RASKAR: But the benefit of tomography, which we studied in the last couple of lectures, is it's a very high-dimensional signal. And so usually, in a high- dimensional signal, there's lower sparsity. If you think about taking a CAT scan of your body, there are only like four or five types of materials. So compressive sensing allows you to take less measurements. But the problem is you need to actually have more information about the scene before you take the measurement. you don't know anything about the scene, you take very few measurements. Once you take its transform, some transform, it's very sparse. It can be represented in a complex place. So tomography is the same. It's 4D capture for 3D representation. OK, so I'm sorry we're not taking a break. Should we take a 30-second break before we move on to two very small topics, which is how to write a paper and wishlist for photography.

ROUGE-1: 23.99, ROUGE-2: 22.88, ROUGE-L: 22.52
BERTScore: 63.83

==============================================
==================== [84/100] ====================
Summary:
Prof: You know why I am dressed up? When I do this course and when I do the first half of the French course I do a lecture on the bourgeoisie, the middle classes. Middle class was a form of self-identity that was constructed in the way being a worker was constructed, or being a noble. When you look at me dressed like this, please try to think, knowing me a little bit as you do, why it was that it meant a lot to dress like this in the nineteenth century. In the nineteenth century one of the things that happens with the French Revolution and with Napoleon is that the middle-class values seem to be something to be emulated. In using and indeed insisting on the term "middle classes," what I'm suggesting is the enormous complexity of the middle class. There wasn't just one middle class, yet the middle classes shared some cultural values and symbols in common and when challenged by ordinary people could snap back in an extremely cohesive class-based manner. The word "bourgeois" has really more cultural connotations, maybe, than objective or social categorization. The French Revolution, and here's an important point, I guess, opened the way by removing legal blocks in very many places to the career open to talents. The bourgeoisie did anything but that. Work was part of how they believed to get ahead, and getting ahead is what they wanted to do. Of course, it was always in the nineteenth century sort of classic to poke fun at bourgeois culture, and in some cases the lack of it. But there's been an awful lot of good work done on the middle classes. class lived without passion, and were philistines, and that sort of thing. The middle class formed voluntary associations, and many of these were for extremely charitable purposes, particularly in Britain. The Society for the Protection of Cruelty to Animals, these sorts of organizations really are one of the classic examples of bourgeois voluntary associations doing good things. They also get together to hang out with each other and sort of try to gauge who has more money than the other, and they get together for social reasons in the coffeehouses of England.  religion was a fundamental part of the British middle class's view of itself. The percentage of people who went to church could be exaggerated. Religion for the middle classes has a greater role in their lives than in working class cities. How do we know who is middle-class? There was a whole lot of work done in the 1970s on what they used to call the new urban history, which is counting people up. The first real censuses do not come until the nineteenth century almost everywhere. Middle-class people had enough money to leave wills, therefore, their inventories after death. That's one of the reasons we know about the explosion of print culture, because they inventoried the books that people read. The further east you get, the smaller the middle class gets. In Russia, the estimates are about two percent of the population were middle class. And, of course, they are clustered in Moscow and in St. Petersburg, and in Kiev, now Ukraine, always Ukraine but then part of Russia. At the very top there are the great bourgeoisie, the big bourgeoisie. These are people who are big financiers. Then you've got other layers of bourgeoisie. Here we have smaller bankers, not in size but in money, industrialists, merchants, these kinds of people. Lawyers rise up rapidly in popular esteem. They have access to political power. Even if they're in Prussia, a place that's dominated by the nobles who are called the Junkers, they will still have access by virtue of their wealth toPolitical power. and usefulness. The middle class likes to see themselves as useful. You find lawyers reaching in there and, slowly, doctors. Doctors increase a self-identity and become more important in the nineteenth century. Notaries have a much bigger role in Europe than they do here. Notary know where all the goodies are. When you buy property in France, by the way, if you have a mortgage you pay twelve percent right off the top goes to the notary just for holding in his office your deed. times, as you know, in the French Revolution--;the French revolutions, and in the revolutions of 1848. These folks are here, too. People are always dumping all over them needlessly. I will give you some example. If you've ever read the great French novelist--;he was paid by the word, but Balzac is really the novelist of the bourgeoisie. When he describes Paris and the seventeen to nineteen percent of the population who are increasingly living in the western part of Paris, he describes it as a jungle. This is how the people on the bottom part of this ladder viewed the demands of the working class. They want to vote, too. What is the man doing? He's counting his money. The guy at the left here is a clerk. That's a very nineteenth-century profession, as it is for every subject. The chances are that in these bad years you're going to fall down. But yet lots of people get up and the ranks of the middle class increases everywhere in the nineteenth century, in Russia, too, everywhere. son-in-law or the would-be son- in-law. The bourgeoisie didn't kiss and hug a lot. But he's got his hand draped rather daintily on the old guy's arm. He's not about to embrace him and give him a big kiss on each cheek. One day all of this stuff will be his, if he plays his cards right. They still had arranged marriages. Love could count for something, but marriages were still essentially, less so for the middle classes than for ordinary people. This is your classic Hamburg financier's apartment. You've got a domestic servant. Domestic servants cost almost nothing. It was considered to be a way of moving up the ladder to say that you had four domestic servants instead of three. There's more than one room. You'll see in a minute there's even more than two rooms. There are lots of rooms. What the middle class wants, all those people in that triangle, they want privacy. They want their own rooms. Along with that comes the piano. The notion of childhood, childhood didn't exist for ordinary people. Nobles did not send their children to public schools or even to private schools. The idea of a children's room, of having your own room or a room shared with a sibling, was something that was just inconceivable for the majority of Europeans. The middle class wants to be seen rather like the Dutch in the seventeenth century. They wait in line to go to theatres. This is all Daumauau, the piece that you're obliged to swallow after dinner. In the last one minute thirty-five seconds that remains to me, the bourgeoisie, the middle classes, want the right to bear arms. They want to be in the national guard. The national guard might hypothetically be there in case there was an invasion of France or Germany by, I don't know, some distant place, the Fins or something most unlikely. But the main reason they wanted to join the national Guard was to be able to vote. You had to be defined as a property-owning citizen to have the rightto vote. these bourgeois panicked and start going into a house full of very ordinary people and simply shooting them all. The light lines disappear with Daumier. He did another one of these after a massacre in 1848 in Rouen and it's been lost. We don't have it. The rue Transnonain, where this happened in the center of Paris, simply disappeared. It didn't quite disappear from the collective memory of people thinking about Parisian things. In conclusion, the middle classes extremely vary. They share much. They have a common material culture. They want to vote.

ROUGE-1: 29.05, ROUGE-2: 28.10, ROUGE-L: 26.95
BERTScore: 62.33

==============================================
==================== [85/100] ====================
Summary:
Researchers have confirmed a second smaller space Rock smashed into the sea off the coast of West Africa creating a large crater during the same era. Scientists say it would have caused a tsunami at least 800 M High to tear across the Atlantic Ocean. The asteroid that's believed to have wiped them out 66 million years ago was not the only one researchers have confirmed. The discovery is exciting that it happens to be potentially close to the same time as the chicku event known to be the the main cause of the extinction event that killed the dinosaurs.

ROUGE-1: 38.76, ROUGE-2: 35.80, ROUGE-L: 31.78
BERTScore: 63.39

==============================================
==================== [86/100] ====================
Summary:
In automotive design Dynamics plays a very important part because it's not rigid body Dynamics it's a bunch of rigid bodies with springs the Springs are called starts with an S suspens suspensions right and there's a trade-off between how comfortable the ride is and how tightly the car handles. In Dynamics there two sides to it one is here's the system what is its trajectory going to be in other words how will its various degrees of freedom behave over time if you you know stretch it and let it go and it goes twang. a system that is not where you don't close the loop you just have a rocket you have you know boosters you fire it it goes where it wants to go that's an open loop system a closed loop system is you might have a GPS system some sort of you know gyroscope in it. Once you launch the missile it's ballistics ballistics is trajectory as in passive once you launch it it's on its own you know so the missile only the the uh the thrust is only for the first few minutes of its um journey and then it's ballistic ballistic means trajectory.  angular momentum concept is our stepping stone into Dynamics of rigid bodies because then you can start looking at two and three particles more easily. I won't have officers today only because you don't want to hear me Babel I'm really sick um but I'm also going to change my officers um several people suggest so the timing isn't right um so we'll talk about the end of class but I I might go to like a Monday off M um like later on a Monday or maybe Wednesday later or something like that. I'm referring to something with no Dimensions but with a finite Mass you know that someone asked me the other day and I just want to be sure to say this. Let's say that you have a particle Point Mass heading that way some direction and let's define its velocity we'll call it a v p okay so I have um two questions both of which you probably know the answers to the first is what is the angular momentum of that particle just from your memory go ahead say it aha. The MIT way is to do it exactly right using all the mechanisms we've used and guess what you're going to find a stray correction term which you can only ignore in some cases. torque is not always equal to rate of change of angular momentum there's a correction term there are conditions under which it's a rate of angular rate of changed momentum. I'm going to write torque on a particle with a tow because Little T looks like time so I write as a tow torque on an system write to the capital T. be clear this whole thing is this term and this is thisterm now let's let's write this The Next Step what is this what this m a acceleration of P or acceleration of p with respect to a hm H it's the force on particle P right cross product rqp yes now so I'm just going to put a dotted line so you know that that's what this is is this I don't think you'll disagree let me just write some draw some lines I'm trying to save space as I said there's a really badly designed classroom. This is a silly term but these are the two conditions yeah Q is fixed right now it just so happens that we often take angular momenta momenta about things like this point of this door about this point so AQ is z everything's good right or you know we're parallel so it vanishes but you know often it's not parallel so you need to be careful that's the point I want to make okay this term will crop up later uh and we'll we we'll it's a pesky term so way account for it or we get rid of it yep. she helps Congressional uh you know Congress analyze things from a physics point of view so you know when Katrina occurred someone asked if it would be possible to change the temperature in the when when a hurricane approaches to for example dissipate the temperature. She did some analysis and showed that you need something like a nuclear weapon but like you know the most the largest nuclear weapon ever conceived to even you know impact it by like 2% because the energy in a in a hurricane all right I told you I'd Babble all right let's do a problem here's a problem. that stuff let's let's examine it from a uh from a basic you know intuition point of view from what we studied so far first of all when is linear momentum conserved forget this in general linear momentum is conserved when what condition occurs no external Force right so let's study this guy this particle as it moves around does it feel an external Force let's do a free body diagram on this particle um o Point p as the particle moves around kind of intuitively which direction is it accelerating in kind of cental right.  linear momentum conservation is a vector equation which is the vector linear momentum is conserved right but if if it's not conserved in One Direction but there's no force in the other it's still conserve in the the other direction. So there is no torque on this particle at any point in time it's just a very long way to say listen things going in circles and the only force is radial if we take a cross product it's going to vanish which is why this is such a convenient formalism. would have to do one of two things I would have had to either calculate this term or calculate or make you know make my frame attach it to the truck. I've just done it in a very precise way okay so in the end there's no surprise the whole point is to show you they could be surprises but be careful any questions about this all right snap quiz in the next 3 minutes I want you to calculate for me the final velocity literally 3 minutes because I have toDo the dumbbell problem.  angular velocity increases with a square which is why it's so spectacular you know that I just want to write that okay so that's uh angular momentum conserved. Could have done the same thing with v it would just be proportional okay what that means is if I have the length of the cord the angular speed is going to double and the reason it's more interesting it could because when you watch a skater you know do the uh what's what's it called the twirl you know when figure skaters kind of rotate. momentum formulation actually came precisely from FAL to ma let's look at this guy what we did was I just defined terms the conservation yeah sure well this way this is the nice way to do it look angular momentum is simply f is equal to Ma you put an R cross product in front of the F and inFront of the ma that's kind of where it comes from right so effectively that's what I did but I just gave you a canned way toDo it instead of doing it. of the frame what I have now is two particle masses basically a dumbbell and they're attached rigidly by a massless bar massless and I scoot them across and it's rotating it's hurling through through you know across this rink. I'm going to try and identify understand the behavior in fact I'll make it even more complicated by attaching two rockets get it Rockets right to this thing. The Rockets are designed such that they always Point North all right so they always point in in the horizontal Direction in the on the Whiteboard. have to pick the center of mass but I pick thecenter of mass a lot of terms cancel out the rest from here on. A lot of everything we do is about get getting terms to cancel out. We'll do that but we'll do it without using any moment of inertia Concepts okay so let's write it out so we know that uh let's calculate so the what's the first thing we need to do we're going to write f is equal ma for both particles so first we will do some free body diagrams then we'll figure out accelerations of both particles. from a free body diagram point of view I'm not going to do it right now but very simply there are each bar applies a force on the part on the particle I'll do it later on because I want to the kinematics first so I'm breaking my own rule I'll doing the free body diagrams later on. I can actually just use the ultra super cool magic formula here directly I need you need have done that but we can do it brute force and just figure it out so um actually let's use the um the uh super cool formula and what we will get is the acceleration of particle p is equal to. Next week we'll pick up on this and then we'll generalize it and Define angular acceleration and moment of inertia and a more General sense okay so let's stop here because we are over time. We'll end up with essentially what what what we'll show is that the acceleration of the center of mass is uh related to the total force and the angular acceleration of  rigid body is related to torque and we will show it okay. We will then generalize the equations to a more general sense.

ROUGE-1: 28.58, ROUGE-2: 27.76, ROUGE-L: 27.56
BERTScore: 64.21

==============================================
==================== [87/100] ====================
Summary:
Differentiable programming is closely related to deep learning. It allows you to build up an increasingly more sophisticated model without losing track of what's going on. The FeedForward function takes in an input vector x and produces an output vector which could be of a different dimensionality. And the way to interpret what people are doing is performing one step of processing. In particular what that processing is, is taking this input vector, multiplying it by a matrix, adding a bias term and applying an activation function. Convolutional neural networks are a refinement of a fully connected neural network. ConvNets have two basic building blocks. The way that Conv takes an image and the image is going to be represented as a volume which is a collection of matrices, one for each channel, red, green, blue. Each matrix has the same dimensionality as the image, height by width. The convolutional network can also be applied to text or sequences which are 1D or videos which are [INAUDIBLE].. Conv is going to compute this volume is via a sequence of filters, and intuitively what it's going to do is try to detect local patterns with [AUDIO OUT] MaxPool takes an input volume and then it produces a smaller output volume. And with these two functions along with FeedForward, now we can define AlexNet which was the seminal CNN from 2012 that won the ImageNet competition and really transformed [INAUDIBLE] So in one line I have AlexNet. Now let's turn our attention to natural language processing. In NLP, words are discrete objects and neural networks speak vectors. So whenever you're doing NLP with neural nets, you first have to embed words, or more generally, tokens. So we're going to define an EmbedToken function that takes a word or a token x and maps it into a vector. And all this function is going to do is it's going to look up vector in a dictionary that has a static set of vectors associated with particular tokens. But the meaning of the words and tokens depends on context. So this representation of the sentence is not going to be a particularly sophisticated one. A simple RNN works by taking an old hidden state, an input, and a new hidden state of the same dimensionality. LSTMs, or long short term memory, were developed to solve this problem. So now we have our sequenced model on RNN which produces a sequence of vectors, and the number of vectors depends on how long the input sequence is. So suppose we want to do classification, we need to somehow collapse that into a single vector. So you can intuitively think about this as summarizing the collection of vectors as one. There's three common things you can do. Transformers are a way of combining different types of networks into one. They can be used to solve problems in language modeling. They use something called self attention, which means that the query is actually going to output the input vectors. So if self attention takes a sequence of input vectors, then it's going to stick the first vector into the query vector for y and then compute the attention, x2 and x4. So in other words, I've basically generated a sequence where all n squared of all the objects, all squared of the vectors, where I've allowed them to communicate with each other. input sequence of vectors and I spit out the corresponding set of contextualized vectors. And the intuition behind this is I'm going to apply f to x safely. So AddNorm of f of x is equal to-- I'm first going to take x and apply f. OK. So now we have enough that we can actually build up to BERT which was this complicated thing that I mentioned at the beginning. BERT is this large unsupervised pretrained model which came out in 2018 which has really kind of transformed NLP. The basic building block for generation is, I'm going to call it GenerateToken. And you take a vector x and you generate token y. And this is kind of the reverse of EmbedToken which takes a token and produces a vector. Then we can take language models and we can build on top of them to create what is known as a sequence-to-sequence model. So this is by and large how a lot of the state of the art methods for, for example, machine translation works. We're generating a translated sentence, given the input sentence.

ROUGE-1: 26.10, ROUGE-2: 24.50, ROUGE-L: 24.63
BERTScore: 63.02

==============================================
==================== [88/100] ====================
Summary:
Professor Amy Hungerford: Today it is my great privilege and pleasure to introduce Andrew Goldstone, a TF in this course. Andrew is a fourth-year student in the Ph.D. program in English, and he is writing a dissertation on the autonomy of the work of art in modernism. On the syllabus it says that I would be presenting a lecture on censorship in this slot, but that's been suppressed. Next week I'd like you to finish the novel and then read his essay, "On a Novel Entitled Lolita" Nabokov's poem Gerontion is a spoof of a poem by T.S. Eliot's "Gerontion" Nabokov calls the poem a burlesque of Eliot's modernism. Eliot says poems should be autotelic, that means they should be an end unto themselves, Nabokovsky says. Nabokova: "The novel has as its only purpose to afford aesthetic bliss" "Modernism" doesn't mean the art, it just means the art itself, he says. and literature of the early twentieth century, especially the "high art," although its roots are definitely in the nineteenth century. In English it begins with the late novels of Henry James around 1900, in poetry with Eliot and with Ezra Pound. In prose its main exemplars in English would be James Joyce, Virginia Woolf. And you should know about this movement that it had very rapid success. So, now, here's just a list for you: eight features of literary modernism that are all important to Nabokov. For Nabokov, the highest value is originality. He says this in his last Russian novel, The Gift. "Any genuinely new trend in art is a knight's move, a change of shadows, a shift that displaces the mirror," he says. "In chess the knight doesn't move in a straight line. Unlike any other piece, it skips over pieces in the way" The strategy of the Knight's move is to frustrate your expectations, to leap over the apparently important events. Is pedophilia in itself a kind of knight's move from homosexuality? Is there another form of perverted desire hiding behind the one that's in front of us? Nabokov's relationship to this modernist past is not just the burlesque that he visits on Eliot, but also this complicated attraction and dis-identification that he works on with Proust. An element of admiration is also present, and that's really part of his relationship to Joyce. Remember that he names Joyce as the greatest master of twentieth-century prose. imitate any style; at the same time, a scrupulous attention to the banality of everyday life and all its detail; yet, the constant use of a superimposed structure. Fourthly, Joyce loves puns. So does Nabokov. This is incredibly important, and there's a direct glance at that just ahead of where you read, so don't turn here. I don't want to spoil what's coming up, but on page 221 there is a reference to--don't look, don't look--to a writer named Vivian Darkbloom plagiarizing from Joyce. in the collected poems of Lord Byron. When he had written this title and drawn an ornamental line underneath, he fell into a daydream and began to draw diagrams on the cover of the book. He saw himself sitting at his table in Bray the morning after the discussion at the Christmas dinner table, trying to write a poem about Parnell. The version of this that comes up in the novel is in the midst of Humbert's diary, and the diary itself, I should say, owes a lot to Joyce. McFate is the icon of the difference between the realistic world of Joyce and the already artificial, already aestheticized world of this novel. No one was ever really named McFate. McFate is a kind of parody of real randomness. The thing that stands for randomness in this book, the thing that looks like ordinary detail, has already been arranged to give you artistic pleasure. The artificial has taken the place of the real here, and this novel really reminds you of that all the time. LZ Granderson: The novel is a response in particular to exile. He says Nabokov's exile is a denaturalized world, where everything has to be decoded. LZ: Because Humbert is a foreigner, the suburban life of mowing the lawn is transformed into something you can laugh at, something that you can apply the knight's move on the real: fate. Lz: In that afterword to this book, Nabokovsky says he had to invent America. Nabokov's novel, Lolita, was written on road trips with his wife, Vera, who drove him around the U.S. while he was writing this novel and hunting butterflies. Nabokov will say that his private tragedy is that he had to abandon his natural idiom, his untrammeled, rich, and infinitely docile Russian tongue for a second-rate brand of English. A kind of economy, a balance between the loss of one language and a particular set of techniques that comes in its place, is the source of the most appealing writing in this book. Nabokov says the American landscape is already a work of art, already part of a European memory. "Inutile loveliness" is kind of the key word of Nabokov's technique, and he says the novel has as its only purpose to provide aesthetic bliss. So, a European artist actually appears again there, with Claude Lorrain, but kind of made strange: given that knight's move, given a new twist. So--instead of familiar, incorporated into this profoundly strange, vast landscape that gets Humbert's most appealing rhetoric. violent knight's moves, like skipping past the mother's death. Somehow this is skipped past, that--the sobs in the night. There's a kind of lost paradise of European culture which he can't get back, even with this spectacular effort in English. So, that suggests that it's not all to the good; it hasn't been saved by taking up these knight's move techniques; there's still a record of damage. And you should be skeptical of it, but then you should also ask yourself whether you can really do completely without it.

ROUGE-1: 27.10, ROUGE-2: 25.03, ROUGE-L: 25.00
BERTScore: 64.12

==============================================
==================== [89/100] ====================
Summary:
NORVIN RICHARDS: Today is phonetics, which means that today we begin making funny sounds at each other. Let's see. I'm trying to remember if there's anything that I ought to announce. You remember, maybe, that problem set 1, which confusingly is your second problem set, is due on Thursday. I just figured out how to get the projector to project over there instead of in the middle so that I won't have to write everything twice running back and forth across the room. Linguists have a system for writing sounds down so that we'll all know what kind of sound we're talking about. A lot of the symbols of the International Phonetic Alphabet resemble letters of the English alphabet. The symbol for the sound at the beginning of "paint" is the letter p. As we go along, we will be seeing weirder andWeirder symbols from the International phonetic alphabet. We'll talk about other kinds of articulation that English doesn't use, but that one just doesn't exist. "R" is one of the kinds of sounds that people classically have trouble with. "z" is voiced and that "s" is voiceless. "S" and a "z," those are both alveolar sounds. But they're not the same. If you think about-- if you go back and forth between them, s, z, s,. z, S, z,. you can feel a buzzing. It's like whistling with a blade of grass, or playing a reed instrument.  phonologist: "cat" and "dog" end in sounds that differ in voicing. Is it possible to whisper "z"s and "g"s? Are they-- NORVIN RICHARDS: Yeah. So if you think about what you're doing when you're whispering, first of all, (WHISPERING) your vocal cords are not vibrating at all. That should mean that you're not making the distinction between "s," "z," or "f" or "v" In Polish, "g" becomes "k" at the end of a word. Voiced "b" becomes the voiceless version, which is "p" The way voicing works is that you've got air flowing across your vocal folds and making them vibrate. For a "b," well, the air only has so far to go, yeah? That's one reason you can't keep a " b" going for very long. But you do it for as long as you can. That's the sense in which it's voiced. The sounds that we have mainly talked about have been either stops or fricatives. There are three ways of categorizing these kinds of speech sounds-- place, and manner, and voicing. "n" is voiced-- nnnnn. And it's a stop in the sense that you are stopping the air from flowing through your mouth. For "d," the airflow is stopped at the alveolar ridge. For an "nabial nasal sound," also known as a bilabial sound, the air goes through your nose. English doesn't have a bilabial fricative, but there are languages that do. In Japanese, when people write an "f," like when they write the name of this mountain, they'll write it with an "F" In English, we have a labiodental "f" with our lower lip against our teeth. In Japan, your teeth are not involved. It is only your lips, yeah? Yeah? What would a buh bilabials sound like? Fh, fh. What would it sound like if it were voiced? There are dialects of Spanish where if you have a "g" between vowels, it'll get this kind of sound, in words like "agua" English doesn't have that, but there are languages that do, like Hmong, for example. What would a velar fricative sound like if it were voiced? Lhg. Or you can have an alveolar nasal, right? "n" You can have a glottal nasal, "ng" English has interdental fricatives-- thuh and thuh. English has alveolar stops, "t" and "d." There are languages out there that have what are called dental stops. Part of your job, if you're learning Tagalog, for example, is to learn to make dental 't's instead of alveolars. If you're thinking about linguistic "t," but let's say someone doesn't-- NORVIN RICHARDS: Have teeth? Yes. Yeah? There are places in the vocal tract that English just does not use. And yet other languages do. There are what are called retroflex sounds. So instead of a tuh, you're making a cuh-- [NON-ENGLISH]. So your tongue is curled back a little bit further than it would be for a "t" And it's making a closure, if you're made a stop, right there. So you can make stops there. You can make fricatives there, like [NONSET] or [Nonset] And you can even make nasals there. Uvulars are kind of like "k" except more so. Pharyngeals involve constriction near the pharyngeal wall. Arabic has these. The Berber languages have these. You're getting the back of your tongue to get against the back. of your vocal tract. You can also do a uvular trill-- [TRILLS]---- where you get your uvula to flap in the breeze, but not everybody does that. And then there's a retroflex lateral, too, right? uvulars, retroflexes. And for some reason, the dental stops are still red. Have to fix that. OK. Now people keep asking me about sounds that I've been carefully avoiding, so let's talk about them. There are what are called approximants. Approximants are not stops, and they're not fricatives. And they are sometimes divided into glides and liquids. And I'm hoping that nobody will ask me how you know whether something is a glide or a liquid. English has a very large number of vowels and a not-very-good system for writing them. This is one of the things that makes English spelling so difficult that we can actually have competitions where you watch people spell. If you tried to do that in Finnish, the spelling bee would just never end because every word is pronounced exactly the way it's spelled. OK. So we've done consonants. We have not done all the consonant. So what I'm going to do is show you some vowels. So we have high, mid, and low vowels. But we also have front and back vowels, like the vowel in "hoed" And we have some vowels that are rounded, like in "he" and "hoe" For "ee," where is your tongue? It's in your mouth, but where is it pointing? "Ee" And then for "ooh," where does it go? It moves, right? So you aren't just rounding your lips. English is supposed to have five vowels. It does not have five. It has 14. Why do we only have five letters for vowels? Who gave us this alphabet? The Romans, right? In Latin, there in fact are 5 vowels, which can be either long or short. English monosyllables don't end in lax vowels that are either front or high. "Flay" means to remove the skin from. But we don't have those three words at the end there. English doesn't have words that end in "ih," "uh," or "eh," with the possible exception of "meh" English monosyllables can't end in lax vowels that are either front or high. Not all speakers of English distinguish schwa from wedge. There are people who pronounce "caught" and "cot" the same. We're just about out of time. Let me just give this to you as an exercise. Anybody want to try to pronounce the first of those? this exercise next time. As we go along, I'm going to be asking you to read things in IPA. So I'll start putting IPA on the slides more and more. So start trying to familiarize yourself with it and get to where you're familiar with at least the symbols for sounds that we use in English. Do you want me to read the rest of them? I'll do some more IPA. OK, so what's the second one? STUDENT: "Sue says he's a bad egg."

ROUGE-1: 20.55, ROUGE-2: 19.31, ROUGE-L: 19.19
BERTScore: 62.98

==============================================
==================== [90/100] ====================
Summary:
Expectation is a basic question that will come up again and again when we look at random variables and probability theory. We're imagining n independent flips of a coin with bias p. The probability of heads is p. It would be biased in favor of heads if p is greater than 1/2. And we want to know how many heads are expected. So what's the expected number of heads? Well, we already know-- we've examined the binomial distribution B n,p.

ROUGE-1: 19.54, ROUGE-2: 18.43, ROUGE-L: 15.40
BERTScore: 71.67

==============================================
==================== [91/100] ====================
Summary:
Aristotle was born 384,15 years after the trial of Socrates. He was sent by his father to study at The Academy, the first university. Unlike most of you, Aristotle did not spend four years at the Platonic Academy. He remained attached to it for the next 20, until the death of Plato. He then left Athens, first for Asia Minor and then to return to his home in Macedonia. Here he established a school for the children of the Macedonian ruling class. It was here that Aristotle met and taught Phillip of Macedonia's son. story in a minute, because I think it's very revealing about Aristotle. In any way, this story helps to underscore some important differences between Plato and Aristotle. Aristotle appears from the beginning to look more like what we would think of as a political scientist. He collected constitutions, 158 of them in all, from throughout the ancient world. He was the first to give some kind of conceptual rigor to the vocabulary of political life. Above all, Aristotle's works were explicitly intended as works of political instruction, political education. Aristotle says man is, by nature, the political animal. He says participation in the life of the city is necessary for the achievement of human excellence. A person who is without a city, he says, who is apolis--without a city--must either be a beast or a god. The city is natural in that it allows human beings to achieve and perfect what he calls their telos, that is to say their end, their purpose. But there is a second sense for him, in some ways, in which he says the polis is by nature. Aristotle's Politics is considered the most antidemocratic book ever written. It offers a critique of the modern theory of freedom, which is living as one likes. He argues that freedom does not mean living as we like, but a sense of restraint and self-control. In a stunning admission, he says--listen to this--that "while nature may intend to distinguish the free from the slave, it often misses the mark," he says. "We need to avoid the temptation, in many ways understandable as it might be, to airbrush or sanitize Aristotle," he adds. Aristotle might, as a natural aristocracy? I leave you with this question to think about. Before we reject Aristotle as an antidemocratic elitist, take a look at yourselves. So are you, or you wouldn't be sitting here today. Think about that and I'll see you next week. Back to Mail Online home. back to the page you came from. Follow us on Twitter @dailymailonline and @jennifer_newton. Back To The Daily Mail home.

ROUGE-1: 15.71, ROUGE-2: 14.10, ROUGE-L: 13.63
BERTScore: 61.13

==============================================
==================== [92/100] ====================
Summary:
Mathematically, a consumer is trying to maximize his utility. And this utility maximization has to be done with respect to some constraint and the constraint the budget constraint we take P 1 x 1; P 2 x 2 should be less than or equal to I. In real life, it is possible that a person derives some satisfaction from having some money left in his pocket, but the way this problem has been framed here the person’s satisfaction depends only on his level of consumption of good 1, and good 2.

ROUGE-1: 8.97, ROUGE-2: 8.69, ROUGE-L: 8.97
BERTScore: 69.92

==============================================
==================== [93/100] ====================
Summary:
MIT OpenCourseWare is a free, online education platform. MIT OpenCourse Ware courses are free to download and use. Use the weekly Newsquiz to test your knowledge of stories you saw on CNN.com. The weekly News Quiz pits students against each other to test their knowledge of events in the news. The winner will receive a free copy of MIT Open CourseWare for their next class. Back to the page you came from. Click here to return to the newsquiz page. The Rolex Center is such an iconic building that it also serve a kind of a prestige function, to put the institution on the map in terms of it's a statement. We have a student center here at MIT. People go there to do their banking, their eating, their meeting, and so forth. So we could spend a lot of time on these, but really crisply refining and thinking about the concept is very, very important. So let me very quickly go through the refrigerator case study to show how do we transition from concept to design. deliver that value you need to design the product, the product system and the product object, and understand the operand, the thing that is being operated on or transformed by the primary value delivery process. So for food, I think we briefly talked about this before. Usually people will say, when you think about a refrigerator, keep the food cold. But if you step back and think about it in a more abstract way, it's really about preserving food or reducing the spoilage rate of the food. In order to chill, we need a chiller, and there are different types of chillers, like a cooler or refrigerator. The combination of this specific way you're going to operate the system is what we call concept. Once we have that, we can start managing complexity, decomposing function and form. Design, then, selects the actual values for those design variables, and then optimize the cooler and then the refrigerator for those values, if we look at the example of a cooler. Swiss refrigerators are much smaller than those in the U.S. The form function mapping in the refrigerator is actually much simpler than the cooler. The real complexity comes in when you look at the form form mapping. That's the decomposition of the refrigerator in terms of all the elements of form and then how they relate to each other. Next week, we'll talk about concept selection: finding systems that do the right thing and do it well, deliver value and comply with regulations, standards, and so forth. The NASA approach is basically described in the system engineering handbook in the SE engine as step 3 called logical decomposition. So the idea that we need to partition the system and then derive lower-level technical requirements. And then do functional and performance analysis to see whether you have enough detail. And if yes, then you can select that as a baseline, if not, you might have to go back to the red box, which means that architecture didn't work. We have to look for a different decomposition or different architecture. which I'm going to mention, but we're not going to do as part of the class, which is stimulants. So this is the idea that somehow people are more creative when their brain, when you put yourself into some other state. bio-inspired design would be you go in nature, or you read books about seashells and animals and you really try to understand from nature. Random inputs, provocations, challenges, and then things like alcohol, and even drugs. So I'm just telling you that there is this idea that you can stimulate creativity in these different ways. There are some rules for how to properly do brainstorming, and some of them are listed here. There's an ideal group size, and it says 5 to 10 here, but I should probably revise this to be-- what do you think? 7 plus minus 2. Participants take turns expressing thoughts, suggestions, ideas. And then there's this killer sentences you should never say during a brainstorming session, some of which are pretty funny. The idea there is produce a large amount and diversity of ideas. of thinking, these seven principles have been extracted. And then you can say, well, which of these do I feel really resonate with me? All right. Let's move to some of the structured processes for creativity. So the first one is probably the simplest and the one that's used the most. This is known as a morphological matrix. The idea there is that you try to define what are the key features, factors, or decisions that you have to make when you define a concept or an architecture. set. So now be unchained, and within the constraints that are set by the competition, come up with different concepts. And in the homework, what I ask you to do in A3 is try out at least two different techniques, a structured one and an unstructured one and then compare the results. And this will be due in two weeks, and you will have to do it in two hours. And so, in the next two weeks you will be doing your homework.

ROUGE-1: 22.31, ROUGE-2: 20.03, ROUGE-L: 19.95
BERTScore: 54.56

==============================================
==================== [94/100] ====================
Summary:
then we are going to continue with the second part of the lecture today which focuses on the problem what actually happens if the gaussian assumption that i have about my constraints doesn't hold. As you will see in some small examples having this outliers in your optimization problem is something which hurts dramatically which actually screw up your solution. Already a few outliers can lead to a environment model which is completely unusable for doing any navigation task so where the geometry of what you computed doesn't fit to the real world geometry anymore and one of the questions actually how to handle that. have also experienced and if you look to those poses over here in the poses down here how those individual structures match um if you just apply let's say scan alignment you may say this may match so maybe someone has opened the door which was closed before or here is a door now closed which was open all the other scans map actually quite well. So even as a humanism you say okay there is definitely a misalignment between the skin so they don't fit perfectly but that's something which actually can result from small changes in the environment. and in this case screwed up the measurement how can we incorporate that into the graph based slam approach um so the problem that we actually have is if we look to our um an individual constraint so the lack of an observation given the given the current configuration of the nodes was a gaussian distribution so that's what we had before. If we have this this number of constraints is the product of gaussian distributions if you compute the log likelihood this turns into a sum of the exponents exponents therefore we have these sum of squared error terms. done you know he said oh damn i can't do that what would be the ugliest trick that you can do in order to make that work even worse no that's not quite what you're going to do i mean the sum is kind of the the the bad thing what can i do with the sum instead of the sum i can i can get rid of thesum in some nice way sorry oh the integral of some to the integral actually makes our life typically worth um so that's that's going what's going to fly. The max mixture idea is actually a pretty easy idea pretty simple idea just reply funny no one has done that in robotics until recently a few years ago the first one it was actually edwin olsen and pratik um who came up with that particular with this idea of using max mixtures. It can handle both things at the same time data station errors as well as multimodal constraints. It's actually kind of a nice interesting way to to use this to deal with multi-modal constraints and even if you look to the to the runtime error if you do. actually compute these so the main changes we go to this formulation we have the scaling factor over here and we need a good way to compute the scaling Factor so how can we actually do that and there's actually closed form you can derive that under certain properties. This leads to the case that constraints which are far away from what we expect have a smaller influence on the optimization so we can actually visualize this so what we what you see over here is um the black curve is the the parabola which results from the squared arrow. Max mixture as well as for dcs is that kind of the tails of this gaussian distributions contain too few probability mass they're too close to zero. If you have constraint which introduce large errors these are these outliers this can actually screw up the optimization. If we have one outlier which is really far away from the current estimate the whole mole is tracked in this direction that's the problem that we have with a gaussian distribution. The max mixture approach is actually kind of similar to this corrupted gaussian so if you plot both of them together. here that by changing this function you can't get much better behaviors kind of deciding which function to use for the underlying optimization problem is not on it's not always an easy and easy choice so this requires some expert knowledge some good intuition on coming up with the way with one of those functions. Next week which is the last week of the term i will briefly talk about front ends and give kind of a short summary on what typical front ends exist obviously we're not going to all the details as we did that here.

ROUGE-1: 22.42, ROUGE-2: 21.96, ROUGE-L: 21.60
BERTScore: 66.58

==============================================
==================== [95/100] ====================
Summary:
John Stuart Mill is the principle expositor of neoclassical utilitarianism. The rights-utility synthesis signals that we're looking for an attempt to put together both a commitment to utilitarian efficiency that's grounded in science on the one hand, and respect for individual rights on the other hand. What you're also going to get as a by-product of today's lecture is everything you ever needed to know about neoclassicals economics in 45 minutes. All we're going to talk about today is the transition from Bentham's utilitarianism to the doctrine championed by John Stuart Mill. doing is expressing our tastes, our emotional reactions and that there is nothing more to say about ethics than that. Now, you could say this emotivist doctrine is an endpoint in a philosophical evolution that really begins in the seventeenth century. If you think about the idea of doing interpersonal comparisons of utility, and making the judgment that taking that dollar from Donald Trump and giving it to the bag lady increases her utility more than it decrease his, you're assuming that they basically all have the same kinds of utility functions. as I said, when we get to the anti-Enlightenment and, in particular, Alasdair MacIntyre's book, After Virtue. But today we're going to focus for the rest of our time on the economics of the transition from classical to neoclassical utilitarianism. And I'm going to ask you to suspend disbelief for the whole of today's lecture and just trust me, because what I'mgoing to do is I'mGoing to go into this backwards. I'm Going to look at a very different problem that the neoclassicals economists were concerned with that had nothing to do with utilitarianism, or rights. Prof: If you have a huge amount of bread the next loaf of bread is less valuable to you at the margin than the previous loaf. So that is the idea of diminishing marginal utility. The idea of indifference curves is that you want to go from P toward Q. You want to get onto, as they put in the jargon of neoclassical theory, you wantto get onto as high an indifference curve as you can possibly get, Prof says. "We don't know where Q is. It's out in the stratosphere"  neoclassical economists didn't want to do that because they were actually concerned with quite another problem. The problem they wanted to solve was to understand the behavior of markets. They wanted to be able to more precisely to predict what prices were going to be in markets. So moving from cardinal to ordinal utility is going to turn out to have huge ideological consequences, which I'm going to unpack for you towards the end of today's lecture. These indifference curves cannot cross. Can anybody tell us why? Why can't they cross? Wait for the mic. same utility even though they're different indifference curves. We're saying two is preferred to two-point-five, okay? The jargon, anybody happen to know? Yell it out. Does anybody know the jargon for this? Prof: Transitivity. The preferences are assumed to be transitive. So if you prefer A to B and B to C, you must preferred A to C. That's all that transitive means. If the different distances are taken to imply that A's happier than B disabuse yourself of that thought right away. Pareto said there's not an infinite source of utility. He said if you can anywhere into the northeast quadrant both of them are better off. On the other hand, if we went anywhere in this quadrant, southwest as it were, obviously they're both worse off. And about those two quadrants Pareto says we can say nothing at all. We can't assume with Bentham and Hume, with Sedgwick, we can't assumption that everybody's basically the same, perhaps they aren't. John Rawls: The trouble with utilitarianism is that it doesn't take seriously differences among persons. He says classical utilitarianism says, "Well, if taking all of your utility increases overall net utility then we should do that, because we don't care who has the greatest number of the greatest happiness" John Rawls' argument is half-right, he says, because the truth is that's the classical utilitarianists' doctrine. John Rawl's Theory of Justice: The Case for Justice, the Case for Liberty, and Other Essays, is published by Oxford University Press, priced £16.99. which the radical fangs of classical utilitarianism have been ripped out and it is now a doctrine that is very friendly to whatever status quo happens to be generated in a market system. So it ceases to be this radically redistributive doctrine, and in the process imports into utilitarianism a very robust, some would say, hyper-robust doctrine of individual rights. We'll see how that played out in political theory when we come to look at John Stuart Mills' harm principle next Monday.

ROUGE-1: 22.86, ROUGE-2: 21.18, ROUGE-L: 21.64
BERTScore: 67.24

==============================================
==================== [96/100] ====================
Summary:
HONG LIU: Today, we talk about chiral fermions. He says the Dirac equation requires, actually, psi to have four components. But there are two ways to reduce it, and one is called the Majorana fermion, he says. HONG LIu: You don't need four components to be able to transform under Lorentz. You just need a smaller unit, OK? He says this tells you that at least two components already can transform. The property that you can reduce to two components should exist for all choice of gamma matrices. So, now, let me tell you how to do it for the general gamma matrix. The beautiful trick to do this is to introduce the following object-- what is called gamma 5. So gamma 5 is defined to be i gamma 0, gamma 1, gamma 2, gamma 3, OK? And then you can also check the gamma 5 anticommute with any gamma matrice. So mu here is, of course, from 0 to 3. HONG LIU: Gamma 5 actually have 0 trace, OK? So, this, I will leave as an exercise for yourself, what you can do is you did before with other-- yeah, in your homework. So, now, from this properties-- now we can say the following things about the gamma 5 matrix. And, also, this is Hermitian. So its eigenvalues are all real, and gamma 5 squared equal to 1-- that means its eigenspace is either plus or minus 1. HONG LIU: By definition, this is the analog of psi L and psi R for the general choice of gamma matrices. So, indeed, you see-- so, from here, from this definition, you can check this is true, OK? This is a one-second check. OK? So, now, it's easy to check they actually transform among themselves. So you can also check the gamma 5 actually commutes with sigma mu, nu. Any questions on this? Yes, you have a question? Yes? HONG LIU: In the Dirac Lagrangian, as we discussed earlier, so we have a U1 symmetry. Psi goes to exponential i alpha psi, OK? But, now, psi L and psi R-- they are separate. So, when m equal to 0, when you don't have coupling between the two, you actually get the extra symmetry. So these are called chiral symmetries because they transform the left and the right separately. Yes? AUDIENCE: I'm a little bit confused why you can't write psi as the sum of the two projections? HONG LIu: Sorry? HONG LIU: Symmetry is one of the most important aspect of physics. He says it plays a very important role in particle physics, for example, the pions. Without the chiral symmetries, there's no pion, he says. The pions have to do with-- I will not go into detail, but they essentially come from chiral symmetry. HONG LIu: The bottom line is that the symmetry is very important in many aspects of physics, like liquid helium. HONG LIU: In physics, actually, the massless case actually gives you very much richer structure, normally, than the massive particle. In the Dirac spinor, which we have talked about so far, is four components. And then the chiral spinor we talked about-- essentially, you have two complex components. So the next one I'm going to talk about is the Majorana, in which case, I would argue, we have 4 times 1 real component. very important role in modern-day physics. Say, for example, people suspect a neutrino could be a Majorana spinner, OK? So to check whether a neutron is a spinner has been pursued by many years. And, also, in condensed matter, in quantum information, and Majorana spinor play a very important role. Because it's heuristically half electron, it has very stable topological properties, which a single electron does not have. And whether you can engineer in your condensed matter systems, Majoranaspinor then became a Holy Grail. C, of course, also relates to the spinor. And so psi M will be related to psi by C. And now, since the psi M is equal to psi M star, so that means that C star psi star should be equal to C psi, OK? So that should be the condition which you impose in the general basis. So you have to introduce this C. So if you find that the transformation between the general gamma mu and the gamma mu m, and then you can use that to find the B. I wrote down before, so if you stare at that expression, you find that gamma 0, gamma 1, and gamma 2 are imaginary, pure imaginary. And the gamma 2 is real, OK? So this pure imaginary means, when you take the star of them, you get the minus sign. So, now, in this chiral basis, these three are pure imaginary, that means B needs to anticommute with them. But this is real. It means B need to commute with this guy. Then what is B? to impose in this basis. And this is now independent of massless or massive particles? HONG LIU: Yeah, yeah, yeah. Yeah, this is-- yeah. Good? So this concludes our discussion of the Majorana spinor. Do you have any questions on this? Yes? AUDIENCE: So is the orthogonal component of this Majorana species-- like, possible-- in the chiral one, like, psi L, and then you [INAUDIBLE] and then psi R. just reverse one direction or reverse two directions, OK? That seems also to be a discrete symmetry. And indeed. So if you just change the directions, say, in the x direction, that's also a discrete. And if you only change the direction in both x and the y direction, That's also an independent symmetry. But if you change-- if you do the reflection in two directions,. that's equivalent to a 90-degree rotation. And so it's part of the continuous symmetries. And now, when you change all three directions compared to change one direction, you differ only by changing two directions.

ROUGE-1: 20.99, ROUGE-2: 19.63, ROUGE-L: 19.56
BERTScore: 69.42

==============================================
==================== [97/100] ====================
Summary:
The best-case scenario for expansionary fiscal policy is when there are lots of underemployed resources in the economy. By increasing spending, the federal government can try to counteract falling aggregate demand. In one scenario, government spending doesn't have to be as large as the fall in "C," or consumption, to counteract the recession, and that's because of the multiplier effect. But, as always, shifting lines on a graph is much easier than shifting around real resources in a multi-trillion dollar economy.

ROUGE-1: 30.09, ROUGE-2: 28.77, ROUGE-L: 30.09
BERTScore: 67.40

==============================================
==================== [98/100] ====================
Summary:
hey everyone it's sarah thread sterner sorry and calm and in this video i'm gonna demonstrate how to wear and take off a mask. One common mistake that people make is that when they wear the mask they will wear it under the nose. When removing the mask it's important to remember that the front of the mask is considered contaminated. Don't forget to check out the other videos in this series on how to put on and remove a mask including how to apply a mask and how to remove it. nursing skills series series: Nursing skills series. Learn how to become a nurse in the UK by taking part in this series. Visit www.nurseryskills.org.uk for more information and to join the series on Facebook and Twitter. For more information on the series, visit nursing skills series: nursing skillsseries.com. For information on nursing skills in the U.S., visit nursingskits.com or call 1-800-273-8255 or go to NursingSkits.

ROUGE-1: 34.74, ROUGE-2: 24.30, ROUGE-L: 24.82
BERTScore: 58.04

==============================================
==================== [99/100] ====================
Summary:
In this lecture, we're going to talk about how neurons function and how researchers are able to control that function in order to modify behavior. And this is going to involve also sort of understanding how certain antidepressants, like Prozac, work. And then we'll end by talking about how researchers did this experiment to wake up the mouse. And it all starts with something that I told you about at the beginning of the semester, which is that the plasma membrane separates distinct compartments the outside of the cell from the cytoplasm. of signal known as an action potential. In order to have an electrical signal propagate, we need some sort of electrical property that the cell has that enables this. In a resting state, the cell's resting potential is negative 70 millivolts. If the cell is not getting stimulated by something like a neurotransmitter, the resting potential will be negative 70 million. Stephen suggested opening the sodium ion channels, which would depolarize the cell and make this situation less positive. And so if you open these channels, positive ions are going to flow out. restored to its resting state, OK? So it's a transient process. When we think about the neuron at higher resolution, what you're going to see is not only is it transient, but it's also a traveling wave that propagates along the entire length of the cell. And one thing that you can notice about these neurons, or the action potentials here, is that they all depolarize to the same extent. So this illustrates a key property of neurons, in that the level of activity of a neuron is not determined by the size of this action potential. In this case, it's a sodium channel. So it's going to be-- whether or not it's open depends on the presence of the ligand. So if we take a neurotransmitter like serotonin, if it's not bound to the receptor, the receptor is closed. But if serotonin binds to the receptors, it opens up the channel, which can selectively let in a type of ion. In this case,. this is an activating channel, because letting in sodium is going to depolarize the cell, OK? So this ligand receptor binding uses a ligand-gated sodium channel which starts the depolarization. insulates the plasma membrane of the axon such that-- so here is an axon. You have glial cells that are wrapped around, and it sort of forms like beads on a string. And so there are these gaps between the myelin sheath that are known as the nodes of Ranvier. And these nodes perform an important function for the neuron, because the membrane is electrically insulated. And that allows the action potential to travel about 100-fold faster along theAxon. There are different types of signals that nerve cells can send. Signals can be excitatory, meaning it will tend to depolarize the neuron. There are other signals that bind to different receptors that are inhibitory. So it's in this way a neuron is able to integrate signals coming from different neurons. And that influences whether or not it will send the signal to a downstream cell or muscle or another cell. The way that multiple neurons communicate with each other are through a type of signal known as a neurotransmitter. Neurons are a case of where luck favors the prepared. They have everything ready to go when they get word from upstream, and they're ready to send signals to the next cell. So prior to the action potential, there are vesicles filled with neurotransmitter that are docked at the plasma membrane. In order to fuse, there needs to be some signal inside the cytoplasm to tell the vesicle to fuse. That signal is increased calcium ion concentration. When you get fusion, that's exocytosis, and the serotonin is now on the outside of the cell. sucks the neurotransmitter back into the presynaptic cell such that it can then reuse it later on. So this process of reuptake highlights a very important process that's been utilized by drug companies to create antidepressants. antidepressants like Prozac and Zoloft affect this reuptakes process. And what that does is it keeps the neurotransmitters in the synaptic cleft for longer, such as it enhances the signaling. All right, now I want to end by telling you how this experiment works, where we're able to activate specific neurons in a brain and that leads to the animal sort of waking up. you to test the function of the neuron in the behavior of an organism. So, in this case, this mouse, the light is shined into its brain, and they're testing a specific type of neuron that is involved in arousal of the mouse. And it's going to wake up right now. There it goes. It woke up. You see now its muscle activity is going, OK? So you can test thefunction of specific nerve cells using this approach, and it's because you have a light-sensitive sodium channel.

ROUGE-1: 23.43, ROUGE-2: 22.49, ROUGE-L: 22.96
BERTScore: 63.09

==============================================
==================== [100/100] ====================
Summary:
In this problem, we're going to be dealing with a variation of the usual coin-flipping problem. But in this case, the bias itself of the coin is going toBe random. And we're told that the expectation of this bias is some mu and that the variance of the bias isSome sigma squared. And what we'll be asked is find a bunch of different expectations, covariances, and variances. We'll see that this problem gives us some good exercise in a few concepts, a lot of iterated expectations. xi and xj are independent. And remember, when random variables are independent, the expectation of product, you could simplify that to be the product of the expectations. So this is, in fact, exactly equal to the variance of Q, which we're told is sigma squared. And we found that for i not equal to j, the coherence of xi and xJ is exactlyequal to sigma squares. And so, because they're correlated, they can't be independent. So we'll be using these facts again later. where if i and j happen just to be the same, that it simplifies to be just the variance. And there's n of them, so we get n times the variance of each one. So what do we learn from this problem? Well, we saw that first of all, in order to find some expectations, it's very useful to use law of iterated expectations. But the trick is to figure out what you should condition on. And that's kind of an art that you learn through more practice.

ROUGE-1: 15.95, ROUGE-2: 15.36, ROUGE-L: 15.83
BERTScore: 71.76

==============================================
