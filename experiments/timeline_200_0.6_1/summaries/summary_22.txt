Robotics is a really cool and important direction for the future. I really believe that in the future we will have AI assistance whether they are embodied or not to act as our guardian angels. These agents will help us with cognitive and physical work. With AI we will see such a wide breadth of applications for instance these technologies have the potential to reduce and eliminate car accidents. We have three types of learning and you have seen different aspects of these methodologies throughout the course we have supervised learning, unsupervised learning and reinforcement learning. Boris Katz did an experiment a few years ago where they took regular objects and they put them in a different context. With this significant change in context the performance of the top performing imagenet algorithms dropped by as much as 40 to 50 percent. So keep this in mind as you think about deploying or building and deploying deep neural network Solutions. There's another thing um that is very critical for for autonomous driving and and for robots you have heard a beautiful lecture on adversarial attacks well it turns out you can attack very well. The images that get fed from the camera streams of cars are fed to the decision-making engine of the car. Machine learning is very powerful for building perception systems for robots but as we employ machine learning in the context of robots it's important to keep in mind the scope when they work when they don't work and then it'simportant to think about what my what kind of guard rails we might put in place at the decision time so that we have robust Behavior. "With all small perturbations you can turn the stop sign into a yield sign and you can imagine whatkind of chaos this would create on a on a physical Road" Reinforcement learning is concerned with how intelligent agents ought to take action in an environment in order to maximize the notion of a cumulative reward. This differs from supervised learning and not needing a labeled input or in not needing labeled input output pairs so in this example the agent has to learn to avoid fire and it's very simple it gets a negative reward if it goes to Fire and a positive reward when it gets to water and that's essentially what what this approach is like you you do trial and error and eventually the positive rewards dominate the negative rewards. years ago and so these techniques that did not do so well back then all of a sudden are creating extraordinary possibilities and capabilities in our agents now this is a simple simulation in order to get the simulation to drive a real robot we actually need to think about the Dynamics of the robot and so in other words we have to take into account what the vehicle looks like what are its kinematics and its Dynamics and so here is a vehicle that is running the policy learned in simulation so it's really cool because really we are now able to train in simulation. they get the position of the other vehicles on the track but only only so they get this position from an external localization system but they only know where the vehicles within their field of view are. I think that these these advancements in robotics are really enabling the possibility that you saw in the first Slide the possibility of creating many robots that can do many tasks and much more complicated tasks than what we see here. What I want to talk about next is the autopilot how do we take these pieces together to enable a self-driving vehicle. In 1995 a Carnegie Mellon project called nav lab built a car that was driven by a machine learning engine called Alvin and Alvin drove this car all the way from Washington DC to Los Angeles. The car was in autonomous mode for a large part of the highway driving but there was always a student there right ready to um to take control and the car did not did not drive inonomous mode when there were when it was raining or when there was a lot of congestion or when the car had to had to take exits. Now 1995 is a long time ago right I mean it's before many of you were born. This work um computers needed about 10 minutes to analyze an image can you imagine okay so how do you go from that to enabling an autonomous vehicle to drive at 90 kilometers an hour well um what they did was they they developed some very fast solutions for paring down the image to only the the the aspects that they needed to look at. They assumed that there were no obstacles in the world which made the problem much easier because all the car had to do was to stay on on the road so it's really super interesting to think about how visual processing improved from one frame per 10 minute to 100 frames per second. vehicle we deployed and in fact we had the public ride our vehicle in 2014 we have vehicles at MIT we have a lot of other groups that are developing these vehicles now before we had lidar we had sonar and nothing worked when we had Sonar. With lidar that problem went away so all of a sudden a powerful accurate sensor made a huge difference all the algorithms that were developed on Sonar and didn't work started working when the later was introduced it's really exciting um okay now when we think about autonomous driving there are several key parameters that emerge. Alexander: We have very effective and Deployable solutions for robot cars that move safely in Easy environments where there aren't many static nor moving obstacles. Many companies and research teams are deploying and developing self-driving cars. Many of these preconditions revolve around certainty in perception planning learning reasoning and execution before we can get to Robo taxi but we can have many other robot solutions that are much that that can happen today. Alexander: We can use deep learning and reinforcement learning to take us from images ofroads onto steering and throttle and what you can do with this is really great. completely different driving environments and driving situations and you don't need new parameters you can go exactly directly to what the car has to do so in other words we can learn a model to go from raw perception and here you can think of this as pixels from a camera. The other thing we feed the vehicle is noisy Street View maps so these are not the high definition maps that are usually created by autonomous driving labs and companies and so you can do this to directly infer a full continuous probability distribution over the space of all control. Vista simulator can model multiple agents multiple types of sensors and multiple type of agent to agent interaction and so the the Vista simulator has been recently open sourced you can get the code from vista.csel.mit.edu and a lot of people are already using the system. The decision engine has about a half a million parameters and I will challenge you to figure out if there are any patterns that associate the behavior of the vehicle with the state of the neurons. Peril at the same time and then have a look at the attention map so it turns out this vehicle is looking at the bushes on the road in order to make decisions. standard deep neural network and we have asked this network to solve this problem and the attention map of the network is really all over the place you can see that the network the the Deep neural network solution is very confused but check out something else the data that we collected was summertime data and now it's fall so the background is no longer green we have we don't have as many leaves on trees and so the context for this task has completely changed by comparison the the liquid network is able to focus on the task and is not confused. This kind of this kind of ability to transfer from one set of training data to completely different environments is truly transformational for the capabilities of machine learning. that yields models that generalize to unseen scenarios essentially addressing a challenge with today's neural networks that do not generalize well to unseen test scenarios because the models are so fast and and compact you can train them online you can training them on edge devices and you can really see that they are beginning to understand the tasks that they're given so you can see that we're really beginning to get at the semantics of what these systems have to do. We have one project that is looking at whether we can understand the lives of whales and so what do I mean by this? Machine learning can be used to look for the presence of language which is a major sign of intelligence. We are trying to understand the phonetics the semantics and the syntax and the discourse for whales. We have a big data set consisting of about 22 000 clicks. Using machine learning we can identify coded types. We can identify patterns for Coda exchanges and we can we can begin to really ask ourselves how is it that that that Wales exchange information and if you're interested in this problem please come see us. that model to run on edge devices or on huge devices uh you have seen that many of our Solutions are Black Box Solutions and sometimes we have brittle function we have we have easily attackable models. There is so much opportunity for developing improved machine learning using existing models and inventing new models. If we can do this we can create an exciting work world where machines will really Empower us will really augment us and and enhance us in our cognitive abilities and in our physical abilities so just imagine waking up enabled by your personal assistant that figures out the optimal time. Robots can help us with cognitive and physical work. There's the garbage ban the garbage bin that takes itself out. After a good day when it's time for a bedtime story you can begin to enter the story and control the flow and begin to interact with the characters in the story. These are some possibilities for the kind of future that machine learning artificial intelligence and robots are enabling. I'm personally very excited about this future with robots helping us with Cognitive and physical Work but this future is really dependent on very important new advancements that will come from all of you. Thank you very much and uh come come work with us.