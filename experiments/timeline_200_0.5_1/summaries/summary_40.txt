Today is the day that you have to have done the mid-quarter survey by. Hundreds of people have, but if you haven't, this is your last chance to get the half-point for that. Final project proposals are due. We really encourage you to try and hand them in on-time or nearly on- time. And then today, delighted to have our first invited speaker. And there is a reaction paragraph talking about something that the speaker talks about. There is also assignment 5, which we're giving you one extra day for. Danqi Chen is one of the foremost researchers in question answering. She was the head TA of CS224N once upon a time. So she's quite familiar with the context of this class. Here's my plan for this lecture. So first, I would give a brief introduction of what is question answering, and what kind of problems that people are studying today. Then I'm going to spend the most of this lecture focused on one type of question answering problems called reading comprehension. And at the end of the lecture, I'm hoping to spend hopefully like 10-ish minutes to talk about a more practical, and in my opinion, more exciting problem called open domain question Answer. able to handle more complex questions like how-to questions. People actually really like to ask questions on these digital assistants. Ask a question is actually the second most used case. Only ranks after listening to music and before the check the weather and set up. The best way to prevent illness is to avoid being exposed to this virus. And to help prevent the spread of COVID-19, you can do the following. If you just click this link and read through the article. So this is also one type of the question answering problems. Question answering is probably one of those fields that we have seen the most remarkable progress in the last couple of years driven by deep learning. In this lecture, I will be mostly focusing on the text based, or textual question answering problems. And another class, bigger class of the question Answer problems is called visual question answering. So this problem basically requires both understanding of the questions and also images, and is actually a very active field between the computer vision and NLP. So if you have interest in these type of problems, I encourage you to check out those problems, but I'm not going to go into them. dig into these problems today. So next, I'm going to start with a part 2, reading comprehension. I just want to quickly check if there are any quick questions I can answer before I start us on part 2. OK. So let's talk about the reading comprehension then. So reading comprehension is a basic problem that we want to comprehend a passage of text and answer questions about the content. So here is one example. So basically to answer this question, so you need to find this sentence, like, in 1861, Tesla attended this school where he studied. he studied German, arithmetic, and religion, and only German is a language. So the answer to this question should be German. OK, here is another example, OK? Another passage of text. And the question is, which linguistic minority is larger, Hindi or Malayalam I think 5 seconds. OK. So next I'm going to talk a little bit so why do we care about this problem? So why do you care about the reading comprehension problem? It has actually many useful real-world practical applications. Reading comprehension has been also viewed as a very important test bed for evaluating how well computer systems understand human language. So this is really just similar to how we humans actually test the reading comprehension test to evaluate how well we actually understand one language. And also there is another interesting and important reason that reading comprehension is important. So in recent few years, some researchers actually found that, OK, well, there are many other NLP tasks. So we also reduce them to a reading comprehension problem. Stanford Question Answering Dataset is a supervised reading comprehension dataset. It consists of 100K annotated passage question and answer triples. Each answer is a short segment of text, or we called it span in the passage. And this kind of a large scale supervised dataset are also very key ingredient for training the effective neural models for reading comprehension. But also just to caveat, that this is also a limitation. Because not all questions have the answers in this way. But today, so this dataset, I forgot to say that. This dataset was collected in 2016 by several researchers at Stanford. Stanford, so it's called Stanford Question Answering Dataset. Today, after four or five years now, so SQuAD still remains the most popular reading comprehension data set. So it's actually very clearly a high quality dataset, but is also not a very difficult dataset. So today, basically the SQuad dataset itself has been almost solved, and the state-of-the-art already exceeds estimated human performance. So we can see that there is an exact match score between the predicted answer and any of the gold answers. the F1 score would be taking the max. Danqi, one question you might answer is, so if you can do other tasks like named entity recognition or relation extraction by sticking something on top of BERT and fine tuning for it or do it as a question answering, does one or the other method work better and by how much? That's an interesting question. So I haven't really seen the-- OK. So there has been some claim, OK, that all the tasks can be converted into question answering task. But I'm not sure if there is really a very fair comparison. answer to that. So next, I'm going to talk about how to build neural models for reading comprehension, and in particular, how we can build a model to solve the Stanford Question Answering Dataset. So the input of this problem is let's take a context or paragraph. And also we take our question, Q. And the question consists of n tokens q1 to qN. And because the answer has these constraints, the answer must be a section of text in the passage. We are going to predict a start and then end. just not with us using the both directions. In the bottom right, we sum over i, so why does the i remain in bi? Is that correct or is that a typo there? This is another typo. So the output of gi will actually range from the 1 to N, which is the number of the context words. There are lots of questions about this. What is the rationale for the expression of the gi? How does one come up with such an expression? OK, OK, I'm sorry. The attention layer is basically modeling the interactions between the query and context. The query-to-context attention is trying to measure the importance of these context words with respect to some question words. The final output layers are basically just two classifiers just trying to predict the start and end positions. And the whole BiDAF model can be just trained in an online training set, in terms of the log likelihood of the gold answer and the end position of the answer. It's basically taking the product of these two probabilities, but it is the sum of the two negative probabilities. the models are actually a very similar ballpark. So numbers range from the highest number here, 79.8 until after the ELMo was introduced, the numbers have actually improved quite a bit. Each model actually improved the previous model by one point or two points. And now here is our attention visualization to show that how this smorgasbord of attention actually can capture the similarity between the question words and the context words. So it show the actual question word here. And each column's matrix basically indicates the attention score, the similarity score that has been learned by this model. BERT is a deep bidirectional transformer encoder pre-trained on large amounts of text. It is trained on the two training objectives, including masked language modeling and the next sentence prediction. The BERTbase has 110 million parameters and the BERT-large model has 330 million parameters. If you just take this BERT model, and by just optimizing all the parameters together, it can give you a very high performance. And even if you use a stronger pre-training models, they can even lead to better performance on SQuAD. pre-training has been so important. So next I will quickly talk about-- OK, a question here is that can we actually even design better pre-training objectives for reading comprehension or question answering? And the answer is actually yes. So this is actually a work I did with Mandar Joshi and other folks one year ago called SpanBERT. So for SQuAD and other a lot of extractable reading comprehension datasets, the goal is trying to predict the answer span from the passage of the question. a lot of evidence showing that the current systems still perform poorly on adversarial examples or the examples from out of domain distributions. So today we compute a very good reading comprehension data set on the individual data sets. But these systems trained on one dataset basically cannot really generalize to other datasets. So I have 10 minutes left. Chris, is there any question I should answer at this point here? I think you can go on. OK. So in the last 10 minutes, I'm going to give you a very, very brief introduction of what is open-domain question answering and what we have been trying to do. a large collection of documents. So one example is just taking the whole Wikipedia, which has five million articles. And we're going to return the answer for any open-domain questions. So this problem, there isn't any single passage, so we have to answer questions against a very largeCollection of documents or even the whole web documents. This is actually a much more challenging and also more practical problem. So if you look at the example of Google example I showed at the beginning, so these techniques will be very useful in the practical applications. Today, because she doesn't have a Stanford login, we're going to do questions inside Zoom. So if you'd like to ask a question, if you use the raise hand button, we can promote you. And if you hang around and don't leave the Zoom for more than a few minutes, maybe we'll just promote everybody who's still there into people in the regular Zoom for some bits of discussion. There are now four people who've been promoted. OK. Should I read those questions? Should I look at the chat or? No. Most existing question answering datasets or reading comprehension datasets have been collected from Mechanical Turk. So it is very difficult to avoid some kind of artifact though, like a simple clues or superficial clues-- let's say not superficial but some simple clues that are for the machines to pick up. So that's the reason that more specialized models that have been trained very well in one data set, it's very hard to generalize this kind of thing to another data set. So if you do the remnant questions in your train, dev, and test set, there's a debate of just debate of it's inevitable that it's going to overlap. in the training set that is not really generalization, right? Yeah, but this is more on the open-domain setting not in the really [INAUDIBLE] here. Do you want to ask a question? Yes. So you mentioned in the last part of the presentation that the reader model may not be necessary and you presented the DensePhrases which also work well on CPUs. So do we know how well it performs on the question and answering datasets compared to other models including BERT and those on computer of course. The goal of this project is trying to ingest all the phrases in the Wikipedia. So these conversations are built using the training set of the question answering datasets. So if we use say a different data set that does not present the information using the structure presented in Wikipedias, this model may not work as well as. what do you mean by structure represent? So say if we lean more towards structures like the passages we see in standardized tests where the answers to the question may not be in close proximity to where the information was first introduced. while asking a question if you want. So next person is [AUDIO OUT]. All right. Thank you for taking the time to teach this. My question is kind of quick. So you mentioned work, they brought up a set of relatively simple questions that show how brittle or poor the current models can be, right? I'm curious if that-- yeah, exactly. Did that kind of change the community to improve how to evaluate the models? Because they're actually doing pretty poorly on some of those. Next question is about the future of NLP. Do you think that in order to solve NLP in the sense that you can perform on par with humans on all NLP tasks it's sufficient to only interact with text data? Or do you think we'll eventually need the sort of experiences and common sense that. you get only from seeing and viewing the world and having a set of interactions that we as humans have? Yeah. I mean, common sense is a very difficult-- even in the context question answering. So all these things need to be resolved. encountered this paper called Learnable Quantizers, which essentially learns baseless representation for the quantizers jointly with the leads of the network. And while this would be extremely effective if you were to just like, say, train from scratch, I was just sort of curious, do you think there is some way to do this say a pre-trained BERT model or something like that? I had a few ideas with like beam search for instance, but I don't see a very clear way of doing that. Next question is from Danqi, who was one of the co-organizers of the EfficientQA task. Danqi: How concerned should we be about potential encoding sort of biases into these record labels or how we evaluate them, or is that just more of a concern for more open ended questions? John: I'm not sure I have a good answer to that. Some people do a de-biasing of the pre-trained language models, all these things are very important. yeah, I guess I'm just a little worried about who comes up with the test cases? Who determines what the right answer is? I mean, we will have more discussion of toxicity and bias coming up very soon, including actually Thursday's lecture as well as a later lecture, not specifically about QA though. OK. Next person is-- Thank you for the lecture. Yeah, my question is also related to the open domain question answering. So there is some very specific designs like domain server alignments and efficient level disentanglement techniques that has shown some interesting performance on other tasks. definitely an interesting point. At least for the work that I have seen so far, it all applied or operated at a very simple sentence classification task. I feel like QA is a more structured task and also handles longer bar sequences. Yeah, so I don't know if it works unless people have tried that. OK then we've got-- and maybe we should call this the last question. Hi, I'm just wondering what is the intrinsic difference between solving question answering with generative models like T5 versus encoders like BERT. The key difference between the generative model and the extractive model is that for generative models, you can actually leverage more input passage together and do the generation. So for this model, there isn't any retrieval. So this model really has you relying on all the parameters you memorized. So by just taking this input, it has to just rely on the parameters to infer this answer. So it's actually very hard to-- yeah, it's a definite balance between memory and the generalization from it, yeah.