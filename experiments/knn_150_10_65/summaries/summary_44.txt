Last time, we talked about some of the kind of the bigger questions in deep learning theory. Today, we are going to start talking about the optimization perspective in deeplearning for two lectures. The main focus is to analyze what the functions you are optimizing look like so that you can use some trivial or some standard optimization algorithm for it. The bigger question we are trying to address here is that why many optimization algorithm. are designed for convex functions. But why they can still work for nonconvex functions? In most of the cases, people observe that nonconvex functions in machine learning can be optimized pretty well by gradient descent or stochastic gradient descent. And we are trying to understand why we can optimize reasonably well. And maybe just before talking about more details, let's first quickly review kind of like what gradient descent is just in case. So suppose g theta is the loss function. And the algorithm is just something like sets 0 is some initialization. And you have something like theta t plus 1 is equals to thetaT minus eta times the gradient of g of theta T. This is gradient decent. There is some caveat about whether you can even converge to a local minimum. This is actually somewhat nuanced. So I'm going to formalize this converging to aLocal minimum. But I'm not going to prove any of the theorems here. So the next part is the convergence to localminimum. We're going to show some very, actually, simple cases where we can prove this. And then, basically, I know this is probably not making that much sense for all of you if you're not familiar with how you prove NP-hardness. It's not like, if the gradient is 0 and the Hessian is PSD, then you are local min, right? So why? I guess a simple example here, I cannot-- it's easy to come up is that you just say maybe-- actually, I'm not taking the simplest one just for some reason because I'm going to use this again. So let's say suppose you have f of x1, x2, which is something like x1 squared plus x2 cubed, OK? And in this case, x1,. x2 is 0. The origin satisfies the gradient 0 and satisfies the Hessians. But actually, it's not a local minimum, as you can see. NP-hard. But actually, finding a local minimum is also NP-hard, right? So we have to consider these kind of pathological cases, which makes things harder. So the way to go beyond it is that there is a way to also remove some of the pathological cases as well so that you can find a local Minimum in polynomial time. So far as can converge to a local min with epsilon error in Euclidean distance in time poly d-- d is dimension-- 1 over alpha, 1 over beta,. 1 over gamma, and 1 over epsilon. I think I wrote a book chapter about this kind of optimization thing for our book. So I can send that to the person who take the Scribe notes. And that probably help you to have some references. But the materials are not exactly the same as the book, so you still have to do the Scribes kind of from scratch in some sense. OK, cool. The third strict-saddle condition sounds hard to check. So you cannot check. There is no way you can check whether empirically your function satisfies this condition. But I think you can prove that, if you are just given an arbitrary function, differentiable functions, you should then be able to check whether it satisfies strict-Saddle. somewhat big, almost kind of larger than 0. So then, actually, it's close to a global minimum of the function f, right? So this condition is just a slight different way to say that you have all local minimum global and strict-saddle together, all right? And then I know this condition. Then optimizers-- again, the same set of optimizers which can converge to local minimum. All right, so it's not a big deal. Many optimizers can convergence to aglobal min of f up to, say, delta-error and Euclidean distance in time poly 1 over delta, 1 over tau 0, and d. do linearized network, there is a little bit more things to do beyond that. And the second example I'm going to give is matrix completion. This is an important machine learning question by itself as well, right? So before deep learning, this was one of the most important topic maybe in machine learning, like especially if you think about nonlinear cases. So we're going to talk about that. OK, cool. I guess let's talk about PCA first. So I guess I'll maybe more precisely say matrix factorization. So I'm just trying to find M. Let's find a vector x such that M minus xx transpose in Frobenius norm is the smallest. And this becomes a nonconvex objective function because you have a quadratic term here. And our goal is to show that, even though it's non Convex, all local minimum of this g are global minimum under the assumptions that we have mentioned. So how do we prove this? So as you can imagine, the proof is pretty simple. You first find out all stationary point, the first order stationary points. And then you findout all local minimum, and you prove that they are all global minimum. So basically, it's just more or less like we solve all of these equations and see what are the possible local minimum you can have, right? So let's firstly use the stationary points, a gradient condition. So gradient of x is 0. And what is the gradient of g of x,right? So this is equal to minus this times x. So basically, you have to-- maybe one way to think about this is that you first find out the unit eigen vector. And then if you have unique eigenvectors, so suppose, let's say-- let minimum because that one is also a global minimum. So how do we do this? And also, we don't necessarily want to assume all the eigenvalues are distinct. So let's compute Hessian, right? So we need to use the Hessian. methodology also applies here when you talk about the Hessian. So in this case, from this quadratic form you can figure out what the corresponding matrix is. But for many other cases, actually, it's very hard to write out that matrix of the Hessians. So pretty much you'll still be looking at the different specific quadratics form. So we have to have this quadRatic form. And we know that the Hessia is larger than 0 is equivalent to that, for every v. In the past, one reason why people care about this question is that it has this relationship understand what each user's preference is, want to know that each user likes which item, right? So the Amazon has an incentive to just fill in the entire table. So that's why this problem was important. And it's still kind of important these days, but I guess there are many already existing methods to solve this. And the most used method is basically nonconvex optimization to find this ground truth matrix M. the theorem is that suppose p is something like poly mu and log d over d epsilon. Recall that we are in a regime that p is roughly 1 over d. And this is a poly factor in mu and also poly log in d, OK? And then we assume the incoherence. And then our local of f are when we are-- so actually, you can prove that they are all exactly global minimum. But for the moment, we only prove that the error will be exactly 0. The answer is no especially if you look for a global property, like globally, all local minimum are global. I don't think we have any proofs for any real neural network models. I guess there is a proof for linearized network models, like all the activations are linear. If you have more than two layers, you don't have strict-saddle conditions. You have a lot of [INAUDIBLE] points. Otherwise, I think we are good today.take some questions if anybody has any questions. assume that the input are linearly separable, then there is a proof for this. And there are a bunch of other cases where you can have some partial results. Next week, in the next lecture, maybe the second half of next lecture,. I'm also going to give another result, which is somewhat more general. It applies to many different architectures, but it has other kind of constraints. First of all, it doesn't really show exactly these kind of landscape properties. It shows that these kinds of properties holds for a region, for a special region in the parameter space.