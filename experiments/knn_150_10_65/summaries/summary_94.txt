The Stanford Natural Language Inference Corpus is a big data set. It has over 550,000 training examples and 10,000 examples for dev and test sets. The premises are all image captions from the image Flickr30KData set. The train premises, in this case, are going to be much more diverse and drawn from five genres-- fiction, government reports, letters, travel guides, and phone and Berlitz things. And then, interestingly, the mismatched condition is what they call the "9/11 Report" unanimous gold label. And we rate the overall human level of agreement at about 91.2% for the gold labels. 92.6% is the traditional measure of human performance here. For MultiNLI, the test set is available incredibly productive. How are we doing on MulitiNLI? So again, we're going to have our score over here and on the x-axis, time. And since it's on Kaggle, we can look at lots more systems. crowdworker had to come up with three sentences. One may be correct-- that is our gloss on neutral. And one definitely incorrect, which is our Gloss on contradiction. And here are some examples from the validated set. And I think they're sort of interesting, because you get high rates of agreement, but you do find some examples that have a lot of uncertainty about them. And that might be a hallmark, actually, of NLI problems. And Adversarial NLI is exciting because it's given rise to a whole movement around creating adversarial datasets. The train set is a mix of cases where the model's.pair is independently validated. So in this way, we're kind of guaranteed to get a lot of examples that are very hard for whatever model we have in the loop in this process. And so what we're hoping is that as we progress through these rounds, these examples are going to get harder and harder in virtue of the fact that the model is trained on more data and is getting better as a result of seeing all these adversarial examples.