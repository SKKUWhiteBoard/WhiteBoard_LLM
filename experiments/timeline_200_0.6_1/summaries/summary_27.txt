MIT OpenCourseWare continues to offer high-quality educational resources for free. To make a donation or to view additional materials from hundreds of MIT courses, visit MIT OpenCourse Ware at ocw.mit.edu. The following content is provided under a Creative Commons license. Your support will help MIT Open courseWare continue to offer free, high- quality educational resources in the U.S. and around the world. For more information on MIT Open CourseWare, visit opencourseware.org. maybe you'll see the distinction between those things and understand why one version of the problem is much easier than another. But we try to respond as quickly as possible when we notice a typo like that so that we can set you guys on the right course. So we've got two lectures left discussing linear algebra before we move on to other topics. We're still going to talk about transformations of matrices. We looked at one type of transformation we could utilize for solving systems of equations. Today, we'll look at another one, the eigenvalue decomposition. This course moves at a pretty quick pace. We don't want anyone to get left behind. Speaking of getting left behind, we ran out of time a little bit at the end of lecture on Wednesday. That's OK. There were a lot of good questions that came up during class. And one topic that we didn't get to discuss is formal systems for doing reordering in systems of equations. We saw that reordering is important. In fact, it's essential for solving certain problems via Gaussian elimination. be stuck. It won't proceed after that. So it's the difference between getting a solution and writing a publication about the research problem you're interested in and not. So how do you do reordering? Well, we use a process called permutation. There's a certain class of matrix called a permutation matrix that can swap rows or columns. So if I want to swap columns, I multiply my matrix from the right, IP transpose. If I swap the rows and then I swap them back, I get back what I had before. This is a form of preconditioning. It's always done via Gaussian elimination if we want an exact solution. You're studying one of them in your homework assignment now, where you know the matrix is banded with some bandwidth. So you don't do elimination on an entire full matrix. You do it on a sparse matrix whose structure you understand. We discussed sparse matrices and a little bit about reordering and now permutation. I feel like my diffusion example last time wasn't especially clear. So let me give a different example of diffusion. seen The Price Is Right? This is a game where you drop a chip into a board with pegs in it. It's a model of diffusion. The Plinko chip falls from level to level. It can go left or it can go right with equal probability. So the probability that I'm in a particular cell at level i is this Pi plus one. And there's some sparse matrix A which spreads that probability out. It splits it into my neighbors 50/50. And we'll see the simulation that tells us how probable it is to find the Plinka chip in a certain column. values for a couple of elements of this matrix. But this is a sparse matrix. It has a sparse structure. It models a diffusion problem, just like we saw before. Most of physics is local, like this, right? I just need to know what's going on with my neighbors. And I spread the probability out. I get this nice diffusion problem. Here's something to notice. This probability distribution always seems to flatten out. It becomes uniform. It turns out there are even special distributions for which A times A times that distribution is equal to that distribution. For a real N-by-N matrix, there will be eigenvectors and eigenvalues. They're special vectors that are stretched on multiplication by the matrix. The amount of stretch, however, is unique. It's associated with that direction. And that describes the eigenvector-eigenvalue pair. But because there's N equations for N plus 1 unknowns, that means they're not unique. We don't know what an eigen vector is uniquely. We can only prescribe its direction. But we'll find them in a second. a matrix is also the sum of the eigenvalues. These can sometimes come in handy-- not often, but sometimes. Here's an example I talked about before-- so a series of chemical reactions. We want to know how the concentrations of A, B, C, and D vary as a function of time. And our conservation equation for material is here. This is a rate matrix. We'd like to understand what the characteristic polynomial of that is. The eigen values of that matrix are going to tell us something about how different rate processes evolve. The characteristic polynomial looks like this. What are the eigenvalues of the rate matrix? What is this eigenvalue 0 correspond to? What's that? OK. Physically, it's a rate process with 0 rate, steady state. What physical process does that represent? It's something evolving in time now, James Swan says. He asks the audience to guess what 0 is and what it means. The audience's guess is that it means 0 is a solution. It's 0. Minus k1 is another solution. The eigenvalues can be interpreted in terms of physical processes. This quadratic solution here has some eigenvalue. I don't know what it is. But it involves k2, k3, k4. And this is a typo. It should be k5. And so that says something about the interconversion between B, C, and D. Is that too fast? Do you want to write some more on this slide before I go on, or are you OK? Are there any questions about this? No. We want to know the eigenvector of the rate matrix having eigenvalue 0. This should correspond to the steady state solution of our ordinary differential equation. Can you do that? Can you find this eigen vector? Try it out with your neighbor. See if you can do it. And then we'll compare results. Are you guys able to do this? Sort of, maybe? Here's the answer, or an answer, for the eigenector. It's not unique, right? It's got some constant out in front of it. James W. Swan: Try this example out. See if you can work through the details of it. I think it's useful to be able to do these sorts of things quickly. Here's a matrix. It's not a very good matrix. But it's all 0's. So what are its eigenvalues? It's just 0, right? And they're 0. That eigenvalue has algebraic multiplicity 2. Can you give me the eigenvectors of this matrix? Knowing what those eigenvectors are requires solving systems of equations. If I know the eigenvalues in A, then I can easily diagonalize my system of equations, right? So this is a useful sort of transformation to do. We haven't talked about how it's done in the computer. These are ways you could do it by hand. There's an alternative way of doing it that's beyond the scope of this class called-- it's called the Lanczos algorithm. And it's what's referred to as a Krylov subspace method. in a lot of cases. You can prove-- I might ask you to show this some time-- that the eigenvectors of a symmetric matrix are orthogonal. They're also useful when analyzing systems of ordinary differential equations. So here, I've got a differential equation, a vector x dot. So the time derivative of x is equal to A times x. And if I substitute my eigendecomposition-- so W lambda W inverse-- and I define a new unknown y instead of x, then I can diagonalize that system of equations. There are many times when there's not a complete set of eigenvectors. And then the matrix can't be diagonalized in this way. So there's an almost diagonal form that you can transform into called the Jordan normal form. There are other transformations that one can do, like called, for example, Schur decomposition, which is a transformation into an upper. And you'll find out that this same sort of analysis can be quite useful in nonlinear systems of nonlinear equations. triangular form for this matrix. We'll talk next time about the singular value decomposition, which is another sort of transformation one can do when we don't have these complete sets of eigenvectors. You'll get a chance to practice these things on your next two homework assignments, actually. So it'll come up in a couple of different circumstances. I would really encourage you to try to solve some of these example problems that were in here. Solving by hand can be useful.