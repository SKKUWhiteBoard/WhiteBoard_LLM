==================== [1/100] ====================
Summary:
CASEY RODRIGUEZ: Theorems: A sequence converges to x if and only if the limit as n goes to infinity of the absolute value of xn minus x goes to 0. squeeze theorem: If you have three sequences-- a sub n, b sub n and x sub n-- so that x subN is in between a subn and b sub N, then the sequence x sub N converges. Limsup and liminf: The limit of aSub n as goes to Infinity is the limit of the sub n that exists. So we're going to show that a monotone sequence is increasing. Binomial theorem says that [? for all ?] [? N, the ?] natural numbers, x, y in R, x plus y raised to the n is equal to the sum from k equals 0 to n of n. If p is positive, then limit as n goes to infinity of p to the 1 over n equals 1. And the third is-- it's just that a certain limit exists-- limit asN goes to Infinity of n to 1 over N equals 1, OK? we've only defined what it means to take a real number to an integer power, but-- and n-th roots. So using that, we can then define how totake a real numbers to a rational power. All of that is just to say that what we've done up to now-- these things actually do make sense. You don't need the exponential and the logarithm to make sense of a positive real number. So in summary, the liminf of 1/n is the limit as n goes to infinity of the inf of this set. Bolzano: We're looking at now the set 1 over k, where n is-- where k is bigger than or equal to n. So 1/n plus 1-- that's always smaller than 1/ n, and so is 1/N plus 2, andSo on and so on. So as I move to the next entry, things are getting smaller and smaller. And in fact, this sequence just here now, written as-- thinking of this as a new-- so this is not a sequence-- this is a set. So we've seen sequences that don't necessarily converge, like minus 1 to the n. So we're now moving on to the topic of limsup and liminf of a sequence. Now, not are going to be certain limits, so it's not clear that they exist at all to begin with. But we'll show that they always do exist. We define limsup x sub n. And sometimes I'll write n goes to infinity underneath. Sometimes I'll just write limsup or just have an n underneath it. on hold just for a second, and now prove a very simple theorem that, if A and B are subsets of real numbers, A, B, both not equal to the empty set, and is A subset of B-- so also need to be bounded. So the sup of a smaller set is smaller than the Sup of the bigger set. And that inequality reverses for infs. The inf of the smallerSet is bigger than or equal to. the inf of a biggerSet. So the limsup of minus 1 to the n is 1. Now, if I change all these sups to infs, then the inf of this set is going to be the inf. And therefore, we also get-- OK? So the lim sup is 1 and the liminf is minus 1 for this set. That's just supposed to be a squiggly line, not necessarily looking like sigma. OK, so there's one sequence. How about our next favorite sequence, x sub n equals 1/n? to these two numbers, which may or may not be the same. So the reason this is so powerful and so strong is that it-- to get your hands on something, it doesn't require you to show something as strong as showing there is a sequence converging to that. So quite often, you can think in terms of variational problems, where you want to show that a minimum of something always exists or a maximum ofsomething always exists. Well, what you can try and do is take a sequence of guys that you stick into your-- so this is a general nonsense that youstick into your machine or function that spits out output. And these outputs are approaching the maximum or approaching the minimum. And what you'd like to say is that there does, in fact, exist an element that you can stick intoYour machine and

ROUGE-1: 16.92, ROUGE-2: 16.17, ROUGE-L: 15.47
BERTScore: 71.75

==============================================
==================== [2/100] ====================
Summary:
The configuration of a particle is given by, or described by, a wave function. In 3D, the wave function would be a function of all three positions x, y and z. All reasonable, or non stupid, functions psi of x are equally reasonable as wave functions. There's no primacy in wave functions or in states. Some wave functions are more equal than others. Probability is basically 0. It's nice and smooth. It converges to 0 infinity. The superposition principle. If there are two possible configurations the system can be in, which in quantum mechanics means two different wave functions that could describe the system. Professor: All reasonable functions are features, I'm going to talk about it as something with large momentum. In a quantum mechanical system, something with short wavelength is something that carries large momentum, he says. Professor: Being able to have a Fourier transform where you don't have arbitrarily high momentum modes is going to turn to be related to the derivative being continuous. The uncertainty in x is equal to the expectation value of x squared minus the expected value of X quantity squared, Professor says. Professor: Any function can be expressed as a superposition of states with definite momentum. He says a state with definite position, x0, can be written as a 1 over 2pi integral dk. Professor: To calculate the expected value of momentum do you need to transform the-- PROFESSOR: Excellent question. OK, so the question is, how do we do the same thing for momentum? If you want to find it, dx squared is the probability of finding it in this domain. question I want to ask you. Is that a superposition? Yeah. I mean every vector can be written as the sum of other vectors. And it can be done in an infinite number of ways, right? So there's no such thing as a state which is not asuperposition. Every vector is a superpositions of other vector. There's no God given basis for the universe. We look out in the universe in the Hubble deep field, and you don't see somewhere in theHubble deep field an arrow going x. Professor: To construct or detect an arbitrarily small feature, you need arbitrarily large momentum modes. Professor: In order to reproduce that as a superposition of states with definite momentum, I need arbitrarily high wavelength modes. He says it's really about the range, not just the number, of a feature, and that you need an arbitrarily large wavelength to detect it. The professor says he wants to encourage people to think to conflate distance and momentum because he wants it to be something that becomes intuitive to you. of sines and cosines, do you ever get a discontinuity? No. Do you get something whose derivative is discontinuous? No, you don't. So it's going to take an infinite number of sine and cosine to reproduce that little kink at the edge. So how would you ever reproduce a thing with a discontinued thing? Well, you'd need some infinite sum of sines & cosines where there's some technicality about the infinite limit being singular. Quantum mechanics professor: Average age is the sum over all possible ages of the number of people with that age times the age divided by the total number. Professor: The average need not be an observable value. Average value of the square of ages is, well, I'm going to do exactly the same thing. It's just a squared, right? 14 squared, 15 squared, 16 square, 16 squares, 16 squared. And so that's all I've written here. But notice that I can write this in a nice way. That's just the probability that any given person has a particular age. Is a squared equal to the expected value of a squared? No, in general no, not necessarily. How do we define the width of a distribution? This is going to be like our uncertainty. How happy are you today? Well, I'm not sure. How unsure are you? well, that should give us a precise measure. What does it mean if the average of a is 0? It means it's centered at 0. So if the standard deviation is 0, one then the distribution has no width, right? in this fashion or this fashion. And the notation for this is delta a squared. Different probability distributions are going to give me different delta a's. And one answer is, indeed, the uncertainty relation works out quite nicely. But then I think important to say here is that there are many ways you could construct quantities. There is no God given reason why this had to be the right thing. I can say more, but I don't want to take the time to do it, so ask in office hours. In quantum mechanics, momentum is represented by a specific operator h bar upon I. The next lecture will be about the evolution of the physics of the real world. The first 18 lectures of notes are posted on the web page, and so they'll be posted on this page as soon as they're posted. In the next lecture, we'll talk about the Noether's theorem, named after the mathematician who three principles today, and we've let ourselves to some sort of association between the momentum and the derivative. algebra, looked at the problem and was like I don't even know what it means in classical mechanics. So she went back to classical mechanics and, from first principles, came up with a good definition of momentum, which turns out to underlie the modern idea of conserved quantities and symmetries. Noether tells us the following statement, to every symmetry-- and I should say continuous symmetry-- to every symmetry is associated a conserved quantity. OK? So in particular, what do I mean by symmetry? Well, for example, translations. x goes to x plus some length l. This could be done for arbitrary length l, but these are translations. These are conservation of momentum. Less trivial is conservation of energy. This is a series that you should recognize, a particular Taylor series for a particular function. It's a Taylor expansion for the AUDIENCE: Exponential. e to the minus L derivative with respect to x f of x. Which is kind of awesome. So let's just check to make sure that this makes sense from dimensional grounds. So that's a derivative withrespect to x as units of 1 over 6. But I'm going to write this in the following suggestive way. This is equal to 1 times f of X minus L. Professor: We're going to do this all the time in quantum mechanics. We'll talk about it in more detail, but we're always going to define it in this fashion as a formal power series. Can you transform operators from one space to another? Professor: Oh, you totally can. But we'll come back to that when we talk about operators next time. But from this what is a derivative with respect to x mean? What does a derivative of x do? Well a derivative is a function of length, so this is dimensionless, so we can exponentiate it. eyes, of foot, of eye, of brow. I see the antique pens do but express the beauty that you master now. So are all their praises but prophecies of this, our time. All you prefiguring. But though they had but diving eyes, they had not skill enough you're worth to sing. Your praise shall still find room, even in the eyes of all posterity. So no judgment arise till you yourself judgment arise. You live in this and dwell in lover's eyes.

ROUGE-1: 20.13, ROUGE-2: 18.60, ROUGE-L: 15.44
BERTScore: 61.84

==============================================
==================== [3/100] ====================
Summary:
the Lord Byron George Gordon Lord Byron very interesting very outgoing very flamboyant personality he stood out. The literary celebrity down at the bottom had spoken that he was a you know kind of born into not necessarily nobility like a monarch or anything but he was born into his title he didn't do anything to achieve it okay Hinda and he was an individual that you know probably was a bit of a celebrity to some degree he had the money he was wanting it he it said that he got in trouble because he had a lot of love affairs going on. in dark blue ocean roll ten thousand fleets sweep over the in vain men marks the earth with ruin his control stops with the shore upon the watery plain the wrecks are all thy deed nor doth remain a shadow of man's ravage save his own. I have loved the ocean and my joy of youthful sports was on thy breasts to be born like thy bubbles onward from a boy I want and with thy breakers they to me were a delight. I was as it were a child of thee and trusted to thy billows far and near and laid my hand upon thy mane. essence and a storm can just mess that thing all up if there's a hurricane coming that ship heads the other way tries to get around it okay because it would turn it into like a toy in a bathtub. The 100-year anniversary is coming up here next month of the sinking maybe not the there rerelease in the movie and I'm accessed off 3ds so that's just for you a little bit of extra stuff for today but the apostrophe on page 845 um.

ROUGE-1: 22.25, ROUGE-2: 21.82, ROUGE-L: 22.25
BERTScore: 60.63

==============================================
==================== [4/100] ====================
Summary:
When you do not have unlimited amount of everything you will have to make a choice, what to produce? In what quantity you should produce a particular good. Allocation is nothing but assignment, allotment, share, but in economics we are more technical about this particular term allocation. So, allocation here means solving these three fundamental or basic questions of economics, and the first question is what to production? The second is how to produce and the third is for whom to produce. Depending on who makes these decisions we have different form of economy, who makes this decisions. but after 1991 after economic liberalization we are slowly moving towards market based economy, but we are not yet there. One concept that I would like to emphasize here is laissez faire, this is a French word it means leaving it alone. This is an extreme version of market economy where private parties are absolutely free from government interaction, intervention such as tariffs, tax, regulations. So, individuals transact without any interference of the government. But still I would say that we have mixed economy.

ROUGE-1: 32.53, ROUGE-2: 30.44, ROUGE-L: 30.38
BERTScore: 64.94

==============================================
==================== [5/100] ====================
Summary:
Albert Meyer: Random variables are an absolutely fundamental concept in probability theory. He says in a game called the bigger number game, two teams try to pick the larger number. Meyerer: Team 2 picks one of the pieces of paper and turns it over and looks at the number on it. And then, based on what that number is, they make a decision, stick with the number they have or switch to the other unknown number on the face down piece of paper, he says.

ROUGE-1: 6.86, ROUGE-2: 6.02, ROUGE-L: 6.52
BERTScore: 64.60

==============================================
==================== [6/100] ====================
Summary:
foreign and I should turn off the zoom background blur Ry options like this oh it does show up uh yeah there's like speaker notes on your screen but there's be careful because I accidentally just put something else in the first longer okay. I apologize that was kind of a rough introduction that was uh that was me making a couple of last minute edits that probably hurt more than they helped so I want to just apologize. I just think I was just too ready to go I usually uh yeah as our slides were they and put which is the product describing to replace the names. review um convolutions and and the architecture of a CNN to make this more clear and put it put it into perspective how it relates to just standard um dense neural networks I think it's fine um so we talked I think I think most people felt okay about um the actual mechanics of doing a convolution um and I just wanted to sort of clarify that when we do a convolutions operation we treat it like a layer like with standard dense neural network. There were more questions on on this on the sort of mechanics of like what does CNN is and what it what it looks like mechanically. and we also have a bias term that gets added to the output of moving each window on each location of our input we refer to it as a volume simply because it sort of looks like a cube. If you don't want to do a striving convolution a very simple way to do it is to just look at individual little squares and just take the max in this whole area in each one of these areas and just spit that out. If there are one by one convolutions can actually be used as a form of padding and dimensionality addition and reduction. a little timeline here starting from uh alexnet and moving forward. Laneet is something that was created quite a while ago. Alexnet is like the first kind of visible Improvement in this field. We want higher accuracy and a simpler architecture and these are two things that Inception Nets are able to give us in some form um but residents bring to another level as well. We're only about like halfway through the network with this network with it's super easy to learn. We don't need the rest of these layers they're just like okay we're only like okay they're in the way to make a good classification. soft Max will scale more logarithmically um and it'll give you a final like probability map um are there any questions on Alex now actually before uh we move on yeah what's up yeah so uh if you don't specify a certain type of padding valid padding is going to be applied to make sure that as you're sliding your kernel across an image you're left with the same dimension is there anything you want to add Jake or no that's I I should have mentioned adding two yeah I mean you did a good job we basically just had a bunch of zeros on the outside. discrimination in lower stages um increase the gradient signal that gets propagated back and provide additional regularization. Vanishing gradients is a common problem as you add a bunch of layers stacked together and that the learning signal or the gradient computation becomes extremely weak the model struggles to learn. The depth of the matrix multiplication that you're doing without like by losing form of the identity is the reason that stacking a lot of layers doesn't result in like better performance or strictly better performance even like equivalent performance. relief activation function um as your X goes through a weight layer the function is applied you go through another weight layer this f of x kind of encompasses that process this is the function that you've applied to X now your output is whatever f of X is the motivation behind residuals is that after your f ofx has been applied you add X back into your network. Adding residuals will increase the time to convergence because you're increasing the number of backwards considering computations that you have. A 34 layer residual will have jumps between every two layers. event like a low dimensional projection s yeah yeah this is like probably like really important thing for today but like this idea of like why it'd be important to sort of be able to learn the identity like it's sort of a weird thing um are there any questions or comments or concerns about that yes yeah for sure right so like if you have a dent snail Network like like let's just ignore convolutions right now if you like a dense neural network trivially you have the identity Matrix which is just ones along the diagonal and it spits out the exact same thing that it took in. generally work I'm running a thin one by one by three layer here so this is 64 times three it's 192. and this is what is being multiplied by the 256 and added to our previous product which is 74 times 64. so these two are being added together to end up with your your final computation for how many I guess multiplication parameters you have other questions about this yeah. So mobilenet has a lot fewer parameters which results in a lot faster convergence time um and it matches Inception of D3 accuracy just by using depth and point wise convolutions and combining. so these are some things that these models wanted to optimize over time accuracy performance and model size um model size is something that has a trade-off if you get too big you lose out on other metrics like accuracy. performance is something directly corresponds to depthwise convolutions and mobile nuts for Edge Computing and things like that. You want to drastically reduce the number of computations that you want to do yep that is basically everything for today thank you guys for coming oh and there will also be a quiz.

ROUGE-1: 22.56, ROUGE-2: 21.75, ROUGE-L: 19.53
BERTScore: 70.25

==============================================
==================== [7/100] ====================
Summary:
The principles of baking show us how to make bread with and without a bereavement. We'll make three breads one with the brief mint and two without one of the breads without the bereavement will be bulk fermented in the fridge for 24 hours. The third one will be made with a brief mint which will be left for 12 hours to ferment then it'll be mixed into the main dough. The breads will then take the same amount of time to make but why would you add the extra step of making a briefment when cold fermenting?

ROUGE-1: 13.99, ROUGE-2: 12.17, ROUGE-L: 12.58
BERTScore: 67.22

==============================================
==================== [8/100] ====================
Summary:
Bayard Rustin was the chief organizer of the 1963 March on Washington for Jobs and Freedom. Rustin grew up in a Quaker household, and began peacefully protesting racial segregation in high school. He was jailed in 1944 as a conscientious objector to World War II. In the 1980s, he publicly came out as gay, and was instrumental in drawing attention to the AIDS crisis until his death in 1987. In 2013, fifty years after the March On Washington, President Barack Obama posthumously awarded him the Presidential Medal of Freedom. are or who we love.” “I’m so proud of you,” she says. “You’re so beautiful.’ ““I love you, too,’ I say. I love you so much.“” I’ll always love you. ”I will never forget you. ””“We’ve been through a lot. We’d rather be here than there. ’’”

ROUGE-1: 27.06, ROUGE-2: 22.61, ROUGE-L: 22.81
BERTScore: 56.37

==============================================
==================== [9/100] ====================
Summary:
Christine Hayes: The Book of Deuteronomy describes God's choice of Israel as the chosen one. She says the idea that Israel is a holy people entails obligations and responsibility. Hayes: God is repeatedly testing and correcting the Israelites until they are ready for the Promised Land. Some scholars say the texts may be understood as a kind of internal polemic against those elements of Israelite society whose practices didn't conform to the Yahweh-only policy, or YahweH-only ideals. Moses warns the Israelites not to say to yourselves, "My own power and the might of my own hand have won this wealth for me" He emphasizes, it is only because the wickedness of the Canaanites is so great that the Lord has to drive them from his land, and now he is giving you a chance. But it is conditional for you, just as it was for them. Don't fail him or he will choice. They love the person, and they make a bond with them. It does not imply anything about other people. Deuteronomy is not simply the concluding book of the Pentateuch, or the story that began in Genesis. It's also the first part of a much larger, longer literary work that runs from Deuteronomy through to the end of 2 Kings. We are going to consider today the program and the work of this so-called Deuteronomistic school. This section of the Bible is divided into two parts we refer to as the "Former Prophets" and the "Latterts" of times are some of the debates that occur on the question of dating. There is a great deal of ideological baggage that is involved in the dating of the sources. The reconstruction of the evolution of Israelite history, Israelite religion, excuse me, is really driven more by theological prejudice than it is by historical evidence. All scholars agree that the Priestly materials reach their final form in the exile or post-exilic period. So the period of the exile is the sixth century, right? obedience or disobedience to the covenant with Yahweh. And that conviction is going to color its presentation, its evaluation and its interpretation of Israel's history and her kings from Joshua right through to 2 Kings. The structure of Joshua is really somewhat simple. We can really divide it into two major parts. The first 12 chapters form a unit that conveys the invasion and conquest. There are certain important elements. in chapter 2 we have Joshua sending out spies to scout out the land. In chapter 3 we have the account of crossing the Jordan River. In the past 4000 years more wars have been fought for the possession of the tiny strip of land known as Canaan, or the land of Israel, or Palestine. In times of peace it would bring prosperity, but, of course, in times of war the land was perpetually invaded as armies would crisscross the land. Despite the fact that this is a very small piece of land, it boasts great geographical diversity. The area around the Sea is basically semi-desert. You have the semi-nomadic farmer settled in the more fertile areas. In Joshua 13:1, Joshua opens with the statement that they had not been captured. in Israel, it rises about 10,000 feet above sea level. As you move from the central area over to Jerusalem, that area is dramatically lower. By the time you get to the Sea of Galilee you are 700 feet below sea level, and the Dead Sea is nearly 1300 feet below. That is the lowest point on the earth's land surface--so this dramatic drop in just a very short geographical area. Up in the north, the river is surrounded by very lush vegetation on both sides, but there is no life 65 miles south down by the dead Sea. that are said to be destroyed by Joshua and the Israelites weren't even occupied in this period, the late Bronze Age, beginning of the Iron Age. Excavations at Jericho and Ai indicate that both of these towns were laid waste at least 200 years before the probable time of Joshua. Of 20 identifiable sites that were said to have been conquered or captured by Joshua, only two show destruction layers for this time, Hazor and Beth-el. In Judges 4 and 5, it is said that it is still a Canaanite city and Joshua failed to take it. So the conclusion one can draw from all of this is that Joshua didn't destroy Jericho. perhaps not exclusively, and adopted the national story of the Exodus as its own at some point. The Hebrew tribes, themselves, were likely still in the process of formation. But the tribal structure of Israelite society that would develop would be strengthened by the natural division of the land into these separate geographical areas. That only reinforced the tribalization of society. And these local tribes probably did assimilate elements of the local population. We've really seen already the ethnic mix of various elements reflected in religious imagery and institutions. century BCE, written by King Mesha of Moab Moab is to the southeast of the Dead Sea. In the inscription he writes, he boasts: "And the god Chemosh said to me, go, take Nebo from Israel" It is likely that such claims are hyperbolic in Moab, and it is likely they were hyperbols in Israel. But that does not lessen the shock value for a modern reader, says Andrew Keen. "War in our time is no less savage and no less brutal," he says.

ROUGE-1: 19.20, ROUGE-2: 18.15, ROUGE-L: 17.10
BERTScore: 62.61

==============================================
==================== [10/100] ====================
Summary:
Bogdan Fedeles: Today we're going to be talking about Problem 2 of Problem Set 4. We'll be discussing in detail the mechanism of HMG-CoA synthase, a key enzyme in central metabolism. The enzyme is responsible for making the five carbon building blocks from which all sterols, such as cholesterol and steroid hormones, are made. The reaction will start by forming this thioester between the acetoacetyl- CoA carbons and the cysteine in the active site of the enzyme. energy contributes to this reaction. Both of the substrates need to bind to the enzyme. In order to do that, they need to be desolvated, that is to remove all the water molecules that surround them. The binding energy is also derived by when we align the substrate in the active site of the enzyme, we align them so closely the right geometry and within a few tenths of an Angstrom so that the right orbitals overlap and allow the reaction to happen. So also the ability to align this residue so closely that also contributes to the binding energy. a look. Here is an oxygen ester that shows a proton in alpha position. And as you know, the lone pairs on this oxygen can conjugate with the carbonyl group and form certain resonance structures. So the electrons can move like this, and then we're going to have a negative charge here and a positive charge here. And this is possible because the electrons on both oxygens are found in orbitals of comparable energies. By contrast, in the case of a thioester, we have a sulfur. experimental work and evidence. The study of the effects of drugs on the human body was conducted in the 1970s and 1980s. The results of the study were published in a book called "The Effects of Drugs on the Human Body" The book was published by Oxford University Press in 1989. It was the first of its kind to be published in the U.S. and is published by Simon & Schuster, a division of Simon and Schuster Inc. in New York. The book is available in hardback and paperback.

ROUGE-1: 16.86, ROUGE-2: 14.18, ROUGE-L: 15.17
BERTScore: 61.06

==============================================
==================== [11/100] ====================
Summary:
Alberto Riva: The most important resources for finding and using biomedical information, especially information connected with the study of the human genome. He says the best resource, in his opinion, is Golden Path, a genome browser for several different organisms. Golden Path gives you the absolute of all the known elements of our genome, and maps each one of these maps to a different set of objects in the world. Riva says the most important resource, of course, is GenBank, the largest repository of DNA sequence data. Phenotypes are generalizations, too. Phenotypes are qualitative in nature. They cannot be measured exactly or precise, they cannot even be defined precisely in most cases. You always have to take into account the effect of environmental factors that, again, are very hard to describe in a quantitative way. For DNA, you just look at the sequence and you know essentially all that there is to know about DNA. For proteins, you cannot look at. the subsequence of a protein and understand just by looking at it what the protein. is going to do. Not even how it's going to be. was discovered, more or less, the same years. But at the time, nobody had any idea that there was any connection between these two things, between the DNA and inherited traits. It took over 80 years for this concept to be proven. And finally, the Human Genome Project, that was officially declared a success last year, brought us to the point where we now know the exact base pair sequence of our genome. But even if we're all human beings, there are no two human beings that are exactly the same. SNPs are the most common form of variation in our genome. They're important because for example, they can be used as genomic markers. SNPs are at a fixed location in the genome. If a SNP arises in a population, then it tends to be limited to that population. You're not going to find it in a different population, unless there is some genetic interchange between the two. So when you look at the frequency of a SNP, it's very important to specify what population you're looking at. GenBank is a database of DNA sequences that refer to a region of the genome where a gene is known to be. UniGene puts them all together in one cluster, and then tries to provide a description of why all these sequences are so similar. HomoloGene has 470,000 ortholog pairs-- so pairs of genes from different organisms that are highly similar to each other. Swift: I think it's a blast to blast, but it's also part of the blast of the human genome. LocusLink is a repository of information about genes, and it collects everything that is known about the genes. It gives you information about the sequence, itself, about the functions of the gene, links to other databases. And in the end, you get the gene is expressed, because it was latent in the protein is produced. And the transcription factors, as I was saying, don't act alone. They have to interact with the target gene, but they also interact with each other in a combinatorial fashion. And we are still doing the very early steps in the process of trying to understand how these patterns are actually structured. phenotype. What I mean is that, for example, if you have a SNP in the coding subsequence of a protein, you're going to get a protein that has an abnormal sequence. There are many diseases that are due to the fact that you have SNPs that truncate proteins. They can be used as evolutionary markers, because SNPs arise randomly during the replication, and then they are transmitted from one generation to the next. And it's very interesting to study how the gene it belongs to, notch 4, tells you that this gene is in the codes of the gene. SNPs get-- how the frequency of the SNP changes in a population. Most SNPs are deleterious. But in some cases, the SNP can also provide an advantage, if it generates something that was not present before, and that works better than the original. If a SNP is neutral, then there is no selective pressure, and it will either go away by chance, or will stay at a certain basic level of frequency. So you can study the frequency to understand if it's undergoing if the SNP was validated. these domains are overlapping. And this information comes from Swiss [INAUDIBLE] database of protein information. And so you see for example, this first domain covers almost all of the protein. AUDIENCE: So six would be the maximum number? ALBERTO RIVA: No, no, it's just that these domains can be overlapping, just because the Swiss people, they annotate the protein sequence saying OK, from here to here, we know that this happens. But-- well, well, there are some domains that cover the entire protein. is a consequence of the fact that there is a very complex machinery behind it that determines which genes are active or not, and how much, in different conditions. This is actually a system that integrates a lot of different factors that might include the following, in no particular order-- The tissue, we know very well that the set of genes that are expressed in one tissue is very different from the set that is expressed in another tissue. External signals, of course, all response to external stimuli. And it also depends on the expression state of any number of other genes. The first thing you need to do is need to be able to reliably identify which transcription factors bind to a given gene, and where, exactly, in the promoter region of the gene they bind. And transcription factorsbind to locations that are called transcription factor binding sites. They're small stretches of DNA that are recognized by the factor. If you know that two factors have to interact with each other, probably their binding sites will have to be close to each other. Or, at least, let's say if you find two binding sites that are close to Each other, there is a very high chance that they will interact. The Stanford microarray database is a repository of all the-- of a large number of micro experiments performed at Stanford. NCI60, again, from Stanford, is a famous data set that includes gene expression profiles for 60 human cancer cell lines. Other resources for gene expression are found in different PGA projects, PGA are programs for genomic applications, they are are large projects managed by the NIH. So the [? tracks ?] PGA, for example, offers 565 microarrays from mouse and rat models of sleep, infection, hypertension, pulmonary.

ROUGE-1: 18.77, ROUGE-2: 17.70, ROUGE-L: 15.34
BERTScore: 60.80

==============================================
==================== [12/100] ====================
Summary:
This week in week three, we're actually going to have some human language, and so this lecture has no partial derivative signs in it. The idea of phrase structure is to say that sentences are built out of units that progressively nest. So, we start off with words that, cat, cuddly, et cetera, and then we're gonna put them into bigger units that we call phrases. And then you can keep on combining those up into even bigger phrases, like, "The cuddlely cat by the door" what you learned about neural networks last week and the content of today, and jump straight right in to building a neural dependency parser. Um, the other thing that happens in assignment three is that, we start using a deep learning framework PyTorch. So, if you have any issues with, with that, um, well, obviously, you can send Piazza messages, come to office hours. We have under the sort of office hours page on the website, a listing of the expertise of some of the different TAs. Grammar is made of a, um, a determiner, followed by a noun. We have a system of dependency labels. All we're doing for this class is making the arrows for the arrows. And you should be able to interpret things like prepositional phrases as to what they're modifying. Dependencies are connected and whether that's right or not, you can get in touch with the tree bank to get a sense of what's going on in the tree. And if you think, "Man, this stuff is fascinating. I wanna learn all about these linguist structures," you don't need to be a linguist. In this system of dependencies I'm going to show you, we've got in as kind of, um, a modifier of crate in the large crate. The second meaning the sentence can have is, that's the man has a knife. And so, the interpretations of these sentences that you can get depend on putting different structures over the sentences in terms of who is- what is modifying what? Um, here is another one that's just like that one. Um, scientists count whales from space. to be able to do that. And one of the ways of saying, um, that's important is saying, ''What can go wrong?'' Okay. So here, is a newspaper article. Uh, ''San Jose cop kills man with knife''. Um, now, this has two meanings and the two meanings depend on, well, what you decide depends on what, you know, what modifies what? Okay. Meaning one. The cop stabs the guy. [LAUGHTER] In programming languages, an else is always construed with the closest if. But that's not how human languages are. Human languages are, um, this prepositional phrase can go with anything proceeding, and the hearer is assumed to be smart enough to work out the right one. That's where if you want to have artificial intelligence and smart computers, we then start to need to build language understanding devices who can also work on that basis. Doctor: No heart, cognitive issues.um, that are starting to turn up as in the bottom example." to potentially consider an exponential number of possible structures because, I've got this situation where for the first prepositional phrase, there were two places that could have modified. And so, if you get into this sort of combinatorics stuff the number of analyses you get when you get multiple prepositions is the sequence called the Catalan numbers. Ah, but that's still an exponential series. And it's sort of one that turns up in a lot of studies of the human brain. "There's a question of how far apart words are. Most dependencies are fairly short distance. They not all of them are. There are two ways we can do that. We're treating each arc individually, treating each word individually. And there are the correct arcs and to evaluate dependency parsers, we're simply gonna say which arcs are correct. So there's a gold arc from two to one from zero to one. She saw a subject, and there's the root of the sentence, these are the gold arcs. Um, if we're gonna generate a parse, as to what the head of each word is, what we're going to propose as the unlabeled attachment. being attached, we've now got this big verb phrase we call it, right, so that when you've sort of got most of a sentence but without any subject to it, that's sort of a verb phrase to be used for Olympic beach volleyball which might be then infinitive form. Sometimes it's in part of CPO form like being used for beach volleyball. And really, those kind of verb phrases they sort of just like, um, prepositional phrases. Whenever they appear towards the right end of sentences, they can modify various things like verbs or nouns. is, um, the results demonstrated that KaiC interacts rhythmically with SasA Ka- KaiA and KaiB. Most NLP work uses fine-grained parts of speech. So maybe we could have distributed representations, a part of speech that represent their similarity. Um, well if we're gonna do that, why not just keep on going and say the dependency labels. They also have a distributed representation. And so, we built a representation that did that. So the idea is that we have in our stack, to [inaudible] [OVERLAPPING] sort of put the words in a line and that makes it. He see, let's see the whole sentence. You draw this sort of loopy arrows above them and the other way is you sort of more represent it as a tree. So, the dependence of bills and were submitted words, the dependent of submitted and you're giving this kind of tree structure. And that's not very good if you want to parse the whole web, whereas if you have something that's linear time, that's really getting you places. Most attempts to understand the structure of human languages are essentially Dependency Grammars. Lucie Tesniere formalized the kind of version of dependency grammar that I've been showing you. Universal Dependencies is project I'm strongly involved with. It's not only about English. You can find Universal Dependency analyses of French, or German, or Finish, or Carsac, or Indonesian, um, lots of languages. Of course, there are even more languages which there aren't universal Dependencies analyses of. So, if you have a big calling to say I'm gonna build a Swahili Universal it is. In the 60s, 70s and 80s, people used to hand-engineer language features. These features were very sparse. Each of these features matches very few things. Um, they match some configurations but not others so the features tend to be incomplete. And so it turned out that actually computing these features was just expensive so that you had some configuration on your stack and the buffer. And then you wanted to know which of these feature were active for that stack and buffer configuration. So that worked well but, you know, I had different choices of when to pa- when to shift and when to reduce. And I just miraculously made the right choice at each point. Using a neural network to make the decisions of Joakim Nivre Style shift-reduce parser, we could produce something that was almost as accurate as the very best parsers available at that time. Um, they put in Beam search as I sort of mentioned. Beam search can really help. Do humans always agree on how to build this trees and if they don't, what will be the [inaudible] or agreement of humans relative to each other? Um, sad but true. Sometimes the answer to making the results better is to make it bigger, deeper and spend more time choosing the hyper-parameters. There's still room to do better. I mean, at the unlabeled attachment score, it's actually starting to get pretty good. But there certainly are cases and that includes some of the prepositional phrase attachment ambiguities. Sometimes there are multiple attachments that sort of same clause although it's not really clear which one is right even though there are lots of other circumstances where one of them is very clearly wrong. But, you know, so this actually, um, led to ah sort of a new era of sort of better parsers.

ROUGE-1: 20.10, ROUGE-2: 19.14, ROUGE-L: 15.16
BERTScore: 65.64

==============================================
==================== [13/100] ====================
Summary:
okay folks let's get started how's everybody doing pretty good Santa Claus has come to town and you know Santa does with naughty kids Hees them finals he gives them finals. He gives them very evil finals is what he does okay so look out for Santa Claus he's really a really a bad guy uh let's see let's think about a couple of things in terms of announcements and we have a couple surprises today one of which is standing in front of you with all this on and there's more surprises as well. The final exam is in this room on Monday at 9:30 a.m. so uh get here in plenty of time remember to position yourselves with seating as I said before. Heather started go for SES work almost identically Amino a TR tight changing their structure when they S1 then there are elect shs at the AC as theaction next [Applause] theide elak without ACH so one piece is bound to it the get set free has to act next toag where it started waiting for a pepti chain to go and start all again. Glycogen phosphor is U an enzyme that's regulated in several ways. It exists in two forms it exists in the glycogenosphor a form which is the form that has the phosphate on that people describe as the more active and it has the form without the phosphate known as the glycogens phosphor B that people say is less active. Def phosphorilation involves a kise or phosphatase right the r&t involve allosteric affectors okay all right okay so that's where we start now. what else did we learn what did I say about driving your Maserati to Fred Meyer you got to control them right if you turn something on and it's really powerful it's going to break things down really quickly. carrying this carrying around its own inhibitor is the most efficient way that it can turn itself off really quick. I will autograph these for you you can sell them on eBay I you know how much you might make from your knowledge here okay so knowledge is power knowledge may be money so you might meet some exciting people from having these things too. a person with McArdle disease sees ADP levels go high and then it falls meaning that the cells are catching up and making ATP now for that last chance at a CD my question to you is what's making this possible. It's something we learn during the term something we learned a very important process that allows these people who have this disease to lead a fairly normal life. It could be slightly but no that's not the right answer the it's a process we learned during this term.  heard before okay so um you guys know I like the Beatles I write a lot of stuff to Beatles music and you know the Beatles were like this but I really think they should have been like this. What succeeded The Beatles was this group called The Beggs anybody hear the bgs oh you heard the BS okay Night Fever Night Fever right okay so anyway and what succeeded the bGS was a really important group known as right the back street boy what a group huh and of course what succeeded them we know of course was um yeah low moment in music I think but if you thought you've seen low moments in music You Ain't Seen Nothing Yet. the first song that we think about is the fact that how many people in here are really sick and tired of that Sunshine there we go. The bar is very high very high okay are we going to Belt it out I can't hear you all right all right let's go with it it's called BB Wonderland Heather take us no we're all starting all right ready one Anna two mil Hall dirty and they gety he walks to and not louder [Music] started MP3's got added to my iPod some sometimes were and exams when the Cur turned out I don't think it's so my scores are too low sliding by finally there's examination on December.

ROUGE-1: 21.24, ROUGE-2: 20.66, ROUGE-L: 19.72
BERTScore: 66.01

==============================================
==================== [14/100] ====================
Summary:
JUDY HOYT: Hopefully everybody's recovered from their Thanksgiving feast. What I'm showing up here is the schedule to orient us. This is lecture 22. We'll talk about silicides, device contacts, and I've added in a new material this year on novel gate materials. And then next week we have two class periods scheduled. There'll be four speakers in each. And those will be the oral reports and the student reports given on Tuesday and Thursday. And I just want to remind people, if you're doing an oral report, you're expected to provide handouts. The contact itself is still generally considered part of the frontend because you're contacting the silicon. The advantages of aluminum has a low resistivity. It's the second lowest of all the metal candidates, copper being the lowest. And it has very good adhesion to silicon and to silicon dioxide. So it's a very stable metal, and it makes good electrical contact to heavily doped silicon, as long as the silicon is heavilyDoped. And the energy band diagram for that chip is a 19 to the 10 -6 centimeter contact. That's a four orders of magnitude difference. So you need to get this to get to 20, and you're going to have to do some math to do it. A local interconnect is a wire that connects a device to another device. In the early days, if this was the drain is shown here, where the metal is on the left and the semiconductor on the right. And a Schottky contact is governed by thermionic emission. So it's a thermionic process emitting carriers over this barrier. It's this barrier between the Fermi level in the semiconductors and the Fermani level. in the metal. The resistance of the contact itself is the contact resistance from the current flowing through and drain. The modern interconnect material is copper. A few places are still using aluminum. Copper oxidizes very readily, even at room temperature. Copper is more difficult to deposit than aluminum. But copper is the material of choice for today's interconnect. And aluminum has a finite solubility for silicon. And this is one reason why, if you have a very deep junction that's many microns deep, many people never put aluminum directly in the old silicon. It would never spike because it would never get that deep. we want to talk about that is shown here on slide 6 is called the specific contact resistivity. The resistance of a given contact in ohms is just the voltage divided by the current flowing through that contact. And you can calculate it by rho sub C, which typically has units of ohm centimeters squared or ohm micron squared. And basically what this tells you is-- and you notice you it's inversely proportional to area. So if I make a smaller contact area for a given specific contactresistivity, you're going to have a higher resistance. exponential in the applied voltage. So what we do in practice is we look at that equation and we try to get into a different regime. Instead of being in the regime where we're dominated by tunneling over this barrier by thermionic emission, rather, excuse me, over the barrier, if we make the barrier distance really narrow, really small, you can actually get quantum mechanical tunneling through the barrier. So here on slide 8, what I've shown is a tunneling contact. It's a Schottky contact in the limit of very, very high doping. The resistance that we really, when we scale the channel length, what we're really trying to scale is the channel resistance, R chan. So there are three resistances we really want to be able to think about. One is this little resistor right here, which has been the mainstay of technology for many, many years now. It has a lot of advantages. It's self-aligned. It reduces the sheet resistance of the deep source drain region. And it reduces the gate sheet resistance because you put silicide on the gate. The metal is deposited over the entire chip. It was reacted with the silicon, and then it was just etched off in a blanket etch. So this kind of a salicide process was a big breakthrough in CMOS technology, say around the 80s or so. And it also avoids the PVD damage to the gate that we showed last time that the replacement gate scheme that showed in the last time is easier than the original scheme. It's not as simple as using the old fashioned Salicide process where the thickness that you silicide on the gate was the same as the thickness you went in the source. figure 11-35. There's a reference to it in your textbook, a paper that an article that talks about it in much more detail on how it's actually made. And what you do is you have these dark regions here that are funny shaped are considered to be the N plus region that you want to contact. The dashed lines represent the metal. So that would be the metal level. So this is going to take at least three masks to powder. And these little square regions with the X's going through them are the contacts. 1 and 4 and you measure the voltage drop across that face. And then you just divide V divided by I, whatever you measure. And that's going to be equivalent to rho C divided by the area of the contact, L squared. So this is a very common way to do it. You might say, well, why do you go to the effort of having separate probes? Why don't you just measure the current and the voltage on the same probe points? And the reason you do this is because you don't want to have extra contact resistance, say, of your probes going down. You don't want excess carbon or other things at the interface because that's going to cause an increase in your contact resistance. The latest silicide that people are exploring in research and development-- and at some point will probably be in production-- is nickel silicide. Nickel silicide has the lowest silicon consumption. It can be formed at very low temperatures. It doesn't have very bad narrow line effect for silicide in the gate. Big problem with nickel is you have to be careful of your thermal budget. Industry moved a number of years ago primarily from Ti-silicide, although some people may still use it, to cobalt bisilicide. Cobalt has a little less lateral encroachment over the oxide spacer. It does not have that narrow line effect, but it gives you a slightly higher resistivity. It's a little more sensitive to surface contaminants. And at the end, I've added a few slides that show you that people are actually using silicides as metal gates. silicide. It went down and it stopped at the gate stopped. And this is the silicon channel region. It's reasonably smooth. People were concerned that the nickel might diffuse in and react with the oxide. But according to this particular temperature and time that they did, they get a reasonably smooth interface. The carriers are going to be flowing right in the silicon underneath this. So you don't want to get any nickel into that channel. This is, on slide 32, that same paper. They also did some Auger analysis. interesting thing that was also done in this paper, if you end up wanting to do research in this area, is shown here on slide 34. This is something called gate work function engineering. We hadn't really talked about it, but the work function between the metal-- I think we may have talked about threshold voltage control. The work function, which is a property of the metal material, to a certain extent. And the silicon, that determines the threshold voltage of the transistor, the voltage at which the transistor turns on. control the PT or adjust the PT a little bit. It's still a very tricky process. The reaction temperature or the thermal stability temperature is reasonably low. So you cannot take these and then take them to a backend process that is too hot. So they're very much a research. It was only published by IBM a couple of years ago. They're certainly not ready necessarily for manufacturing right now. Perhaps in the near future, but just to give you an idea of the types of things that people are concerned about. about your oral report or whatever, please get back to me. OK, thanks. About your oralReport.com. About. Your oral report. About Your Oral Report. Please get. back tome. about your oralreport or whatever. about. your oral Report or whatever,. please getback to me about.your oral Report. about Your OralReport.org. OK. Thanks, thanks, thanks for your report. You can send it to me via e-mail at jennifer@dailymail.co.uk.

ROUGE-1: 19.24, ROUGE-2: 17.98, ROUGE-L: 15.14
BERTScore: 62.64

==============================================
==================== [15/100] ====================
Summary:
The dagger algorithm aims to provide a more principled solution to the imitational and distributional Shi problem. The idea in dagger is to actually run the policy in the real world see which states it visits and ask humans to label those States. The goal is to collect data in such a way that P Pi Theta can actually learn from the data it's trained on. The basic version of dagger works like this and that's the version that you will all be implementing in your homework. It's a very simple algorithm to implement if you can get those labels. It can actually get up to fly pretty reliably through a forest dodging trees. Humans need to provide data for imitation learning which is sometimes fine but deep learning works best when the data is very plentiful so asking humans to provide huge amounts of data can be huge limitation. If the if the algorithm can collect data autonomously then we can be in that regime where deep Nets really Thrive without exorbitant amounts of human effort. One of the most exciting things we can get out of learning based control is emerging behaviors behaviors that are better than what humans would have done. In that case it's very desirable to learn autonomously. The cost function and the reward function are really the same thing they're just negatives of one another and the reason that we see both sometimes is the same kind of a cultural distinction that I alluded to before remember I mentioned that we have S a which comes from the study of dynamic programming that's where the reward comes from in optimal control. In optimal control it's it's a bit more common to deal with costs I don't know if there's a cultural commentary here well you know optimal control originated in Russia maybe it's more common in America.

ROUGE-1: 29.36, ROUGE-2: 27.48, ROUGE-L: 27.73
BERTScore: 60.12

==============================================
==================== [16/100] ====================
Summary:
Jonathan Gruber: What stops people from bingeing on everything? It's their limited resources. He says budget constraints, by setting relative prices across goods, can help with a lot of kind of decisions in life. Grubert: What about shocking the budget constraint? We're going to do a lot in this class of what we call comparative statics, Gruber says. "We're not going to tell you what to eat. That's why it's better than dieting because, once again, Adam Smith was right" earn, OK? That is there won't be any savings or borrowing,. OK? Now that is a simplifying assumption. The median American household has $400 in the bank. So this is not kind of a terrible description of the way most people live their lives in America, which is what they earn each week and what they spend each week. But that's a good question. Now let's ask about a second thing. What if your income goes up? What if prices are back to 12 and 6, but your parents decide to send you more money? the price per cookie. P is the number of pizzas, and C is theNumber of cookies. That's your budget constraint. You can essentially devote your income to some combination of pizza and cookies, but you have to consider how much they actually cost in doing that. The more you spend on one, the less you get of another. The rate at which you can trade off pizza for cookies is minus 1/2, OK? That is every additional cookie would require giving up half a slice of pizza. Weight Watchers has developed the best method of weight loss in America. They set up a budget constraint and ask you to follow it. They essentially assign point values to every good you might consume. They say, if you want to achieve a weight loss of x over y days, then you've got to limit yourself to z points. So, essentially, on lunch. you can get a 10-piece nugget, which is 12 points, apple slices, and a Diet Coke. for a total of only 13 points. Audience: How do you determine your marginal rate of transformation? How do determine your-- like say it wasn't just pizza and cookies. How would you determine that value? JONATHAN GRUBER: That's a great question, and we're going to actually answer that question next lecture very explicitly. We'll talk about why income changes differ from price changes and what are the underlying mechanisms. Yeah? You can solve virtually every consumer choice problem I'll give you, OK? That basically, the ratio of marginal utilities equals the ratio prices. point A better? Why isn't it better to have two? Maybe you just-- maybe you like cookies a lot and don't like-- or like pizza a lot. How can we say that point D is better than point A? Yeah? AUDIENCE: Why not choose point E? It's above the budget. JONATHAN GRUBER: Yeah, you can't afford it. OK, likewise, point C you wouldn't choose. Point C has the same slope as point D. In other words, the slope is minus 1/2 at point C. Jonathon Gruber asks audience to trade two pizzas for one cookie. "That's what that number means. And that is a meaningful number," he says. "You are willing to give up 2.5 slices of pizza to get one cookie" "So what should you do? Eat less pizza. Eat more cookies," he asks. "We can use that. That's not an ordinal. that's cardinal" "You're willing to trade. Yeah, say it loudly so we can hear," he adds. Jonathon Gruber: Food stamps are not actually called food stamps anymore. He says the point of SNAP isn't really with contentedness or happiness, but rather like what would be to a more sustainable life. GRUBER: If you really just care what makes people happiest, you should give them cash, OK? He says there's another reason why it might not matter? What's a way people could get around food stamps? Yeah? AUDIENCE: Buy food with food stamps and sell it. Jonathon Gruber: If we think that people won't necessarily make the right decisions for themselves, then it may be worth actually making them worse off. Gruber says the empirical evidence is that, basically, the price of our paternalism is 15%. He says in developing countries, the answer seems to be just giving people cash makes them better off, but that runs into a lot of difficulties in terms of our concerns about how people will spend it. Grubert: At this point, the evidence is sort of probably in favor of being less paternalistic and justGiving people cash. example, they have a series of evaluation programs where they've given people cash. And they find that people spend relatively little of that on drugs and alcohol, but they actually tend to spend it productively. And, in fact, they found, in developing countries, this often provides valuable resources for individuals to start businesses. So they ran experiment Uganda where a nonprofit company randomly offered a group of women $150, which is huge relative to their income. That actually effectively doubled their earnings. From that one injection of cash, it led them to actually double their annual earnings, OK? stop there. We will come back on Monday, and we'll talk about how we actually go from this stuff to the demand curves we started the class with. Back to the page you came from. back to CNN.com home. Follow us on Twitter @cnnireport and @CNNOpinion. Follow CNN Living on Facebook and Twitter. For more, go to www.cnn.com/lifestyle and www.dailymail.co.uk/lpin.

ROUGE-1: 18.54, ROUGE-2: 16.84, ROUGE-L: 14.41
BERTScore: 66.26

==============================================
==================== [17/100] ====================
Summary:
Professor Shelly Kagan: Life on the experience machine is perfect as long as you've got the right tape playing. But if something's missing from that life, there's more to the best kind of life than just having the right mental states, she says. Different theories of well-being might answer that in different ways, Professor Kagan says. But the crucial point is that it takes more to have the best life than getting the right insides, says Kagan, and that gives a whole lot of value to your life. All think accomplishment's important, but it's not as though any old accomplishment is important. We can say that there are certain things that are good above and beyond experiences. It's one thing to know your place in the universe, or to know the fundamental laws of physics. But it's another care about the overall shape of our lives, we might worry about wanting it to have the right shape overall. The unpredictability of our death adds an extra negative element. It makes it harder to plan the best way to live my life. are people who think that for everybody in every case, in every circumstance, the total is always positive. Modest container theories, that is, say there's a value to being alive, but it can in principle be outweighed. And among the bads of a valuable container theory, you have to add something more. So even if, you might say, the way my life is going in terms of its contents is bad, being alive per se might still be a good thing. But if the contents get bad enough, then you'd be better off dead. Hedonism is a version of the "neutral container theory" of the value of life. Hedonism: How well off you are, how valuable your life is is a function of the contents, the pleasure and the pain. If you're a fan of the neutral container theory, you won't have anything extra to add, because life per se is just a zero. But if you accept contents have to be horrible to outweigh it, depends on how much value you think being alive per se has. theory myself--I'm inclined to think not only that the contents of life would be bad, eventually, for all of us if we were immortal. But let me remind you that saying that does not rule out the possibility of consistently going on to say that even though it's a good thing that we die, because eventually immortality would be horrible. For all that, death could still come too soon. There could still be variability. Some people live 80 years, some people live 20 years. Because of the birthmark, everybody knows exactly how much longer they've got. It's not merely the fact that you're going to die; it's a necessary truth that we're all going to death. So we might ask, what about this inevitability of death? Does that make things worse? And here I want to distinguish between the individual question and the universal question. I myself, in different moods, get pulled in both ways. I think you can see--you can get a feel for both possible answers here. And so it is indeed a fact of our powerlessness that we are stuck with the necessities. in life is necessary, then we'd get a kind of emotional distance from it; it would no longer upset us. The story "bad to good" is the kind of story we want for ourselves, while the story "good to bad" is what we don't want. We want the bad behind us, not the bad in front of us. So, whatever the explanation is, we care about the overall shape and trajectory of our life. We have to worry then that because of the unpredictability of death that our lives may not have the ideal shape. 20s--you know 20--roughly another 60 years are going. And as you're busy calculating all this, you're walking across Chapel Street and you get hit by a truck and you die. Right? Because of unpredictability, you can't really know. And in particular, it's hard to know how to pace yourself. It's a long-term plan, which can go wrong if you get sick and die in your early 20s. Well, that's a rather dramatic example, but the same sort of thing in principle can happen to you. ask--so I'll throw the question out and we'll call it a day, start with this next time--then we have to ask, would it really be better to know? would you want the birthmark? Would you want to know exactly how much time you've got left? All Right. See you next time. Back to the page you came from. Click here for more from CNN iReport. Back To the pageyou came from, back to thepage you were from.

ROUGE-1: 19.06, ROUGE-2: 17.89, ROUGE-L: 14.71
BERTScore: 62.99

==============================================
==================== [18/100] ====================
Summary:
Michael Short: Today, we're going to measure the efficiency of a Geiger counter. We're also going to take some sort of gross count of our source plus the net count rate of our actual source. And because I don't want to count for the next seven years, we've concentrated the ashes of 50 pounds of bananas in here to boost your signal strength. Short: The idea here is that this measure is actually a measure of confidence. So the two things that you do to decrease this standard deviation are to decrease the count rate and to increase your counting time. dose and distance or measured activity and distance? Yeah, Luke. AUDIENCE: [INAUDIBLE] r cubed. MICHAEL SHORT: Close. It's, let's say, the measured activity would be proportional to 1 over r squared. Does anyone know why this formula would break down? What happens to our solid angle or our approximation for ourSolid angle is kind of the analog to regular old angle, except in 3D. So instead of looking at things in radians, this has the unit of what's called steradians. a lot. So everybody peeled the bananas, put them in the oven, baked them, separated off the tin foil, baked off as much water and sugar as possible to concentrate the potassium 40 in the banana. So when you do the banana count, we frequently take a spectrum on this with the lid closed, and we always see potassium 40. There's potassium 40 everywhere. So after we get the count of bananas, we'll take a background count. You'll want to subtract the two signals. And it's just a little tiny piece in a little piece of quartz. know. Most of the Boston area is 21. But once you leave Boston-- MICHAEL SHORT: It varies. I don't think it is where I'm-- from Swampscott. But that's kind of up on the commuter rails. You don't want to go to Swampsc Scott. At any rate, I would think that, OK, it's 21. You can buy them. It's still late-stage. it's like town-to-town. But you have to be 18 to smoke. Geiger: How long do you have to count in the smoke shop to be 95% percent sure? So let's say your count rate's 5% uncertain. How do you know that we're 95% confident of our count rate plus or minus 5% error? That's the main question for today. Does anyone know how we'd start? Anyone get to the reading today? I see some smiles. OK. We'll start from scratch, then. All right, so who here has heard of a normal distribution before? A lot of you guys. Great. Michael Short: Counting is a truly random process, and errors in the background rate and the gross rate could add together or could subtract from each other. What you actually want is to do what's called uncertainty in quadrature, where you add up the sum of the square roots of those errors. Short: Even if you count for 67 minutes at 25 counts per minute, that might not be enough to discern the activity of the smoke shop, or the source, or whatever you happen to be looking at. If you go plus or minus 1 sigma away from your true average right here, you've filled in 68% of the area under this normal distribution. Similarly, if you went plus 2 sigma or minus 2 s Sigma, it's around 95% confident. 3 sigma is getting towards 99 point-- what was the number, again-- I think it's 6.5%. And then so on, and so on. There's actually societies called 6 sigma societies. And the way that they get their name is we're so confident of things we can predict them to 6 s Sigma. let's start substituting this out. That's not mine, so we can get rid of that. So we have see C b over t b squared plus C g over t g squared. What's next? How do we relate t g and C g? Well, let's start with the easy stuff, right? What can we cancel, or square, or whatever? Just somebody yell it out. And do I have to go through the rest the math with you guys? I think, at this point, we've got it pretty much solved. activity in the smoke shop to within some confidence and some error. In New Hampshire, the background count's quite a bit higher, because there's a lot of granite deposits, and granite can be upwards of 52 parts per million radium. The reason there's copper is if you get a high energy gamma ray into some lead, that's how I measure nickel, using n p reaction. The thing I really like-- excuse me, where's my vials? I used to have some smaller ones up here. NAA is a process of measuring the neutron flux in a nuclear reactor. The neutron flux is used to calculate how much activation you'd get based on neutron flux. NAA is done in liquid nitrogen, which is a very low, fast spectrum. There's usually an energy threshold for fast reactions, like 1V or 1.5V, which we do in NAA. The place where the thermal irradiations take place is where the NAA takes place, so we don't usually have to worry about that. Michael AMES: I've got a whole bunch of little spacers if I'm counting something that's hot. For longer radiations there's a spot in the basement in the reactor where they can get these, and they send them into the irradiation location. The dangerous thing is dropping lead bricks on your feet. MICHAEL SHORT: We did just go for a solid angle too, today, today. So, cool guys, you are here to do an experiment on MIT 1. samples. But we'll figure out how many samples we'll run. MICHAEL SHORT: It's one per person. [INAUDIBLE] MICHAEL AMES: That's a lot of shorts. In pairs, right? MICHAEL ShORT: Yeah. So I'll show you how the shorts get run. So when we run your shorts, we'll running your samples and we'llrun standards, and then you can do the comparative method. Or, if you feel like it, you can doing the other method. lake sediments. Other analytical methods have gotten a lot better, and so they've kind of caught up to NAA, and you don't need a reactor to run those. The environmental side of this has kind of quieted down a lot. But it's still useful for a bunch of things. I also work in the NCORE group. So that's a lot of my time, rather than just this lab. Practical things-- let's go take a look at a couple other labs. that looks at zinc deficiencies, and fingernails and toenails will give you a good record of how much zinc you've had over the last week, or month, or whatever-- depend where you cut the nails. And so I was going to get a couple of hundred African children's toenail. That didn't happen. But I did analyze my own toen nails. Well, if you went to somebody who was a little suspicious of you, asking for toenailed is a lot easier than asking for a blood sample. Because people would give up toenailing-- it's not a big deal. This is some soil from Montana next to a mine, so it's nicely contaminated with some metals. This is my IAEA mercury and hair standard. And this is kind of what everybody uses for standards. And you just kind of have a whole collection of them. And depending on what elements you're looking for, you try to mix and match them so you cover what you want without having to run five or six of them, says Michael Ames. And so that's how I do the comparative method. The experiment we're doing is basically change reactor power by half a megawatt. With me today is Tim. To actually do this experiment, we need two licensed people in here, one at least has a senior reactor operator. Both Tim and I are both senior licenses, so we have that covered. The only way you can actually do these manipulations are if you're in my training program-- I'm the training supervisor for the facility. And the program you guys are in fits that definition. Once it reaches the power level you want to stop at, the 1 megawatt, keep driving the regulating rod in to hold it at that power level. You haven't stopped the power at this time, you've just decreased how fast it's going up. The power level will sill go up, but a much slower rate than it was before. So once again, you make an entry in a log book that says I'm going to lower ranked power to 500 kilowatts, and then this time we'll use a shim blade. The reactor is on autocontrol. When we do these manipulations, the reactor operator is going to take manual control. That'll cause an alarm to come in. And this will only happen for the first time. And that should be the only time you hear this alarm, because we'll leave it on. The last person will make an announcement that we're done with power manipulations. We'll do that at the end-- is she'll take Manual control of the reactor, and she'll answer it. where it started, the 13.42 inches out of the bottom of the core. It might not make it all the way back up to [INAUDIBLE]. FRANK WARMSLEY: It'll be close. Compensate with the reg rod if you need to. 30.8. OK. And that's the end of the exercise. We'll be back in a few minutes with the results of the test. We hope to see you on "Larry King Live" next week.

ROUGE-1: 20.29, ROUGE-2: 18.88, ROUGE-L: 15.79
BERTScore: 62.58

==============================================
==================== [19/100] ====================
Summary:
Professor: We're talking about evolutionary game theory in the case of, well, biological evolution. He says it's not that we think that the cells are engaging in any sort of weird puzzle solving. Instead, he says, they're just mutations. And the more fit individuals spread in the population, and somehow, you evolve to the same or similar solutions in the context of game theory, he adds. "From my standpoint as an experimentalist, I don't forget about the basic insights about the experimentalist," he says. In a population that is experiencing this Moran process or Moran model, constant population size N equal to 1,000. Each time that an individual divides, it has a 1 in a million probability of mutating. The probability of getting both mutations in one generation is going to be 10 to the minus 12. And then there is the question, will 0, 1 actually fix in the population before later later? And actually, there's a question of do we have to worry about clonal interference or not? a minute. But if you don't understand what's going on, it'll take you an hour. Ready? Three, two, one. OK, all right, so we do have a fair range of answers. I'd say it might be kind of something like 50-50. And that's great. It means that there should be something to talk about. So turn to a neighbor. You should be able to find somebody that disagrees with you. And if everyone around you agrees, you can maybe-- all right. So there's a group of D's and aGroup of B's here, which means that everybody-- AUDIENCE: Let's fight. Professor: If you have 998 individuals that are 0, 0 individuals, and one that's 0, 1, and you have one individual that is 1, 0. The probability for that first path would be the S for 0,1, so it's 0.02, multiplied by the probability that the other [INAUDIBLE] 1,0 would die out. So the probability of fixing it in the back direction is not 0, but it's exponentially suppressed. So this is actually, in principle, not quite answering the question that I asked, because this is a non-neutral deleterious mutation. that are present maybe in one copy. In order for this individual to fix, he has to survive stochastic extinction, which happens with the probability of 2%. And the 1, 0 individual has to go extinct, which happening 90% of the time. And so this is, indeed, answering the question that if you had one copy of each of these two mutant individuals in the population, that's the answer to what is the probability that this 0, 1 mutant would fix. But that's a slightly different question than talking about the relative probability of the first state. One is the time between the time to fix, which went as 1 over S log of NS, right? So we can ignore clonal interference if this is much larger than that. No clonal interfered corresponds to mu N log NS much less than 1. Is that right? Did I do it right? OK. So and once again, there are multiple S's, and it's easy to get kind of upset about this. But you can just use whichever S would be-- which S would you want to use to be kind of-- rate, you don't even do successive fixations. So it may be that neither state ever actually fixes, because it could be that the 1, 0 state is growing exponentially, but is a minority of the population. And it gets another mutation that allows it to go to 1, 1. So as you increase the mutation rate, you can kind of move through states. And there's a whole literature of the rate at which you cross fitness valleys. It's just important. If the fitnesses depend on composition-- this is the population composition-- then you cannot even define a fitness landscape. In game theory, the payouts that you read about in Chapter Four are kind of what would happen in that sort of situation. And from this framework, you can see that there are going to be a bunch of kind of non-trivial things that can happen, even in this regime where everything's linear. And surprisingly, surprisingly, that does not mean that it's not possible to have bi-stability, or you can have another coexistence or you have another type of coexistence. be in trouble, et cetera. So the idea of the prisoner's dilemma is that if you set up these jail sentences in the right way, then it could be the case that each individual has the incentive to confess. And you can come up with some reasonable payout structure that has that property. And we'll call this-- so this is for individual one, say and individual two. So there are different strategies you can follow. And do you guys remember from the reading slash my explanation how to read these charts? In a population, if you have genetic A's and genetic B's that are each giving birth to their own type, then you evolve to some coexistence of genotypes. Whereas in this situation over here, we have coexistence. Does not matter where you start. So long as you have some members of both A and B in the population, you'll always evolve to the same equilibrium. Whereas the mixed Nash equilibrium is a situation where you have, in principle, genetic homogeneity. So this is a single genotype that is implementing phenotypic heterogeneity. In many cases, isogenic populations of microbes can exhibit a diversity of phenotypes as a result of stochastic gene expression and bi-stability. Another question is, what is the evolution explanation for why that behavior might have evolved? Now in general, we cannot prove why something evolved, but we can make educated guesses that make experimentally testable hypotheses. And for example, in the experiment that we've been doing, we're looking at bi-modality in expression of the galactose genes in yeast.

ROUGE-1: 17.84, ROUGE-2: 16.85, ROUGE-L: 15.56
BERTScore: 61.40

==============================================
==================== [20/100] ====================
Summary:
In this video i'm going to be covering sinus tachycardia. sinus tells us that we're dealing with a heart rhythm that originates in the sa node. The sa node is really the starting point of the electrical conduction system. If this process is occurring like it should it should cause this heart to beat at about 60 to 100 beats per minute on the ecg. On the flip side let's say that this sa nodes is disease it's causing the heart to beating slowly. waves we want to make sure that there's a p wave one p wave in front of every qrs complex. We want to look at those p waves is because they tell us about the atrial rate. Then i want to check the regularity of these qrs complexes just like i did with the p wave so i'm going to go from r wave to r wave with my calipers and i'm just going to confirm that they are regular and they're regular just like with thep wave. compensate for what's going on but in the end it's going to fail and the patient'sGoing to have cardiogenic shock and you'd want to report that to the doctor. Check out the respiratory system are they having shortness of breath whenever they breathe in. Are they having chest pain that's getting worse or are they developing a cough so you'd also want toReport that as well now to help you remember those major causes of that. assessing blood levels like the thyroid level making sure they don't have hyperthyroidism looking for anemia or maybe infection.

ROUGE-1: 13.08, ROUGE-2: 12.48, ROUGE-L: 12.80
BERTScore: 62.47

==============================================
==================== [21/100] ====================
Summary:
In this problem, we're given a collection of 10 variables, x1 through x10, where each i, xi, is a uniform random variable between 0 and 1. And we'd like to develop a bound on the probability that some of the 10 variables being greater than 7 using different methods. In part A we'll be using the Markov's inequality written here. And in part B, we'll use the Chebyshev inequality, which takes into account the variance of random variable x.

ROUGE-1: 9.95, ROUGE-2: 9.61, ROUGE-L: 9.83
BERTScore: 73.92

==============================================
==================== [22/100] ====================
Summary:
So now that we've combined pulley A, string 2, platform, and washer as our system, we can now address our question. If we measure the acceleration of the person, what is the force that the person pulls the rope down with? Well, of course, that will just be the tension in the string. And with this simple system,we can now apply Newton's second law, F equals ma. And so by thinking about how to choose a system, what could be a very complicated problem, with lots of equations, is simply one equation.

ROUGE-1: 49.74, ROUGE-2: 48.95, ROUGE-L: 49.74
BERTScore: 80.28

==============================================
==================== [23/100] ====================
Summary:
In order to be successful whenever you're drawing blood or starting IVs you really have to know a couple things number one you need to know the name of the vein that you're going to use and its location along with what can that vein actually handle. Some veins can only handle about a 20 or a 22 gauge IV cannula versus some of them can handle 18 gages 16 gages. I also like to use accessories cephalic vein along with the median vein of the forearm and of course those hand mains the dorsal venous network.

ROUGE-1: 13.00, ROUGE-2: 12.62, ROUGE-L: 13.00
BERTScore: 67.49

==============================================
==================== [24/100] ====================
Summary:
When we talk about complement and substitute, we have to be clear whether we are talking about the demand side or the supply side. So, what do we mean, when do we say a good is complement in production or complement in supply? Can you give an example, first substitute think about it? Plastic chair to iron chair like. Boeing is a manufacturer of airplanes; it makes civilian airplanes as well as military aircrafts. If there is an increase in price of military aircraft, what would happen? Boeing devote more a space to military. To manufacturing, to manufacture military. aircrafts, it would go down.

ROUGE-1: 28.57, ROUGE-2: 27.27, ROUGE-L: 28.57
BERTScore: 66.31

==============================================
==================== [25/100] ====================
Summary:
Liz: Social intelligence should pervade our thinking about the mind and brain. Liz: Trying to get a coherent account of everything from your hand motions and your perception of other people's hand motions all the way to politics and sociology is daunting and, frankly, deeply unlikely. LZ: I think an incredible gift to our ability to understand the human mind also imposes a huge number of limitations on what we can discover. And so what I'm going to tell you guys is two phases of my attempt to attempt to discover what we do that we can so discover. Psychologist David Frum: How much blame do we assign to people based on our beliefs? Frum says we assign blame based on what we think we deserve, not what happened. He says we can change the meaning of knowingly murder than to unknowingly murder. Frum's work focuses on how we assign thoughts or internal mental states to people around us. The more we know about someone, the more we can blame them for their actions, he says. The less we know, the less we should blame them. false belief task or give a false belief task? How many people would like to see a false Belief task? OK, so then I'm just going to show you one. I'm almost exclusively going to talk about the first one, so how we think about what other people see, think, and know-- but not want or feel. At the end I'll come back to wanting you thinking about thoughts-- is there any sense in which that's special or different from the whole rest of the logical and cognitive capacity of your brain? false belief task looks like. This is being given to a five-year-old human child. Do you know what pirates really like? CHILD: What? REBECCA SAXE: Pirates really like cheese sandwiches. Here comes Ivan. He says, I want my cheese sandwich. And he takes this one. Uh oh-- why did he take that one? OK, and so the traditional read of what just happened there is that's a kid who gets wanting. But he doesn't get believing. stories in the scanner, and we record activity in the brain region-- here, for example, the right TPJ-- while you're reading those stories. And so what we do with that is we make arguments about selectivity and these kinds of things that we've been talking about this entire time. These experiments typically proceed in what's called now the forward or encoding direction. And that turns out in many contexts-- especially in the back half of the brain, the representation regions-- to correspond in some sense to the stimulus type. MVPA is an old, discredited theory of concepts, but nevertheless a powerful strategy in neuroscience, including in this context. The idea that we're going to look at is that populations of neurons will respond differentially to features or dimensions of our stimuli. And we're doing this using a key assumption which comes from systems neuroscience, which is that we can think of representations in terms of population codes of features or Dimensions. And by figuring out what the main features or. dimensions are of a stimuli, we can infer something about the representation underlying. the representation that this brain region participates in. fMRI that V1, for example, has an orientation map, that neurons in V1 have an orientation preference. And the answer in standard analyses is-- no, you can't, because V1 as a whole will activate to big images regardless of the orientation of the content of the image. So you need to be able to get to something more fine-grained than V1. And that's the decoding perspective that says, if we wanted to look at V1 and know is the line like this or like that, the way we would tell. Researchers used a multivariate analysis to see how different people's brains reacted to knowing harm compared to unknowing harm. The more that you represented knowing harm as different, the more you judged them as different when we asked you for moral judgment. The pattern difference in your right TPJ accounts for 35% of the variance in your moral judgment, which is pretty amazing, because that's a pretty noisy measurement of your brain and a pretty noise measurement ofyour behavior. But we'll get to the method. The amount of activity in the right TPJ is a big signal. The relative activity between one voxel and another is a tiny signal. It's superimposed on a lot of noise. But if there's anything there at all, then you'll still be able to pick up a little more similarity for pairs that are matched on the feature of interest compared to pairs that aren't matched on that feature. So that's another way that you can use this method-- hypothesize two or three orthogonal dimensions within the same stimulus set. There's something more real about it than if you knew the hypothesis before you ran the experiment. There's just this experience like, if I had the hypothesis in my head, maybe it somehow got from my head to the data. But when the data were already there and then you went back and analyzed them and the effect was hiding in the data that you'd had on your server, there's something way more real and magical about that. So anyway, because it was there in all of our old data, I just believed it.

ROUGE-1: 16.64, ROUGE-2: 15.08, ROUGE-L: 13.05
BERTScore: 61.18

==============================================
==================== [26/100] ====================
Summary:
Joanne Stubbe's lab works on the only cool enzyme in the world-- ribonucleotide reductase. It's the only way in all organisms that you make the building blocks de novo that are required for DNA biosynthesis and repair. If you inhibit this enzyme, you have no building blocks. You can't survive. So from a practical point of view, it's the target of drugs they use therapeutically in the treatment of cancer. And I think in probably not so distant future in the antibacterials because there are sufficient differences between humans and bacteria reductases. and do the same chemistry, but they have different metal cofactors depending on where they evolved. The function in all cases is to generate a radical in the active site and then the chemistry is the same in all these things. And the function of the metalcofactors in all case is to create a radical, which is the key to the chemistry in all of these cases, says Dr. Michael Bociurkiw, a professor of chemistry at the University of California, San Diego.

ROUGE-1: 38.51, ROUGE-2: 33.92, ROUGE-L: 31.74
BERTScore: 70.88

==============================================
==================== [27/100] ====================
Summary:
GILBERT STRANG: Differential equations is the big application of calculus. He says it's interesting to see what information and ideas from calculus actually get used in differential equations. Strang: You really do need to know basic derivatives. The derivative of e to the x of functions that really blow open the functions or we can deal with, he says. "I just like that the use of the fundamental theorem of calculus we need," Strang says. 'It's not everything by any means, but not all the details you learned' is e to the x. Dy dt equals y. And then the inverse function related to the exponential is the logarithm. With that special derivative of 1/x. But you know those. Secondly, out of those few specific facts, you can create the derivatives of an enormous array of functions using the key rules. Derivative is a linear operation. The product rule fg prime plus gf prime. The quotient rule. Who can remember that? And above all, the chain rule. The derivative of this-- of that chain of functions, that composite function is the derivative of f with respect to g. be nice, I just think if you plug that in, to that differential equation it's solved. OK so I want to take the derivative of that. That's my job. And that's why I do it here because it uses all the rules. OK to take that derivative, I notice the t is appearing there in the usual place, and it's also inside the integral. But this is a simple function. I can take e to the t-- I'm going to take eto the t. tells us about bending? That is delta t squared times the second derivative. One half shows in there. So this is the term that changes the tangent line, to a tangent parabola. It notices the bending at that point. So it curves up. It doesn't follow it perfectly, but as well-- much better than the other. OK. Now finally, what if we want to do even better? Well we need to take into account the third derivative and then the fourth derivative and so on. If we get all those derivatives then, all of them that means, we will be at the real one.

ROUGE-1: 34.19, ROUGE-2: 32.42, ROUGE-L: 30.68
BERTScore: 73.84

==============================================
==================== [28/100] ====================
Summary:
okay so welcome to the last lecture of this course here in this winter term and what we discussed so far in the course were mainly the so-called backends or optimization engines or probablistic estimation techniques that were running kind of in the background solving the same problem. Today I would like to give a very very of course brief short overview about front ends and one important aspect inside successful front ends on how to determine if a constraint is likely to be a correct one so we are still interested in avoiding to add roam constraints although we've learned that we there are techniques which can deal with outliers in the data Association. to begin by matching observations so we have different observations depending on what platform. Other approaches use features for example we had those the Victoria Park where trees have been extracted from the laser range data. Third class of approaches uses feature descriptors most popular ones are for example sift and surf. These are three popular techniques or sensor information that front-ends use in order to make the data Association and way we actually look into those who is very short example examples during this course today okay so you're typically in the situation we say okay the robot is currently let's say at location a and let's assume the blue circle over here is a sensor range. moment and that's my sensor range I can compute where are those other pulses so in this case B 1 and B 2 just two examples could be more obviously and then I can also estimate what is the uncertainty of those poses B 1 or B 2 relative to a do that by eliminating the note a from my linear system and then inverting the resulting Hessian and looking to the main diagonal blocks this gives me the uncertainty here indicated by these dashed lines. Based on this information I know I can never have an estimate of given my current pose where's b1 where's B2 together with The Associated uncertainties certainty estimates. I reach all the posts I'm interested in looking into but this does is ignores the loop closures so the uncertainty estimates are too big but you can still argue that okay uncertainty estimates I get are toobig but I can compute this extremely efficient and I may inspect a few places too much but I should get all the places which I need to inspect in order to make sure I find the course with whatever 95 percent probability this is what is done. What is done in practice to what inverting this matrix age there so far we really tried to explicitly invert it. ICP is sensitive to the initial guess so one thing you can do is try to find arrange things into Maps instead of single scans this helps or we can separate the local perceptions into some parts let's take this wall so there's obstacle that you stick out and try to match them first we are less likely to end up in a local minima. If you have descriptors like feature descriptors it can actually help you to find good estimates where you can be so you don't have to try all camera polls and see if the camera poses match. darkest stripes these are simply small alignment errors of these individual maps they can they can see kind of small steps over here here that was also probably an alignment error which simply leads to her step which was let's say bigger than and all five centimeters in the ground and therefore it's classified as not reversible anymore and therefore everything is red over here. You can actually use 3d data to build a map of the environment the next example this is an autonomous car is a parking lot or a 3d model of a parking lots where yellow again means drivable areas and red means non drivable area. an initial inertial measurement unit and one of the advantage of this system is it gives you also the gravity vector this quite accurately at a high frequency. If you know the gravity back door you can eliminate already two of the six dimensions from your state space because the roll angle and the pitch angle can be determined. Based on that I can buy a triangulation compute where are the points in the 3d space give me that I only need to know two features two of those descriptors with their corresponding 3d coordinates in the image in the best 100 features which match my current. God used exactly the same approach but only a single camera and the SONA which was measuring the depth information and so this was a blend. The tas of the blimp was always to hover on top of this location which was here marked by the book i think somewhere over here or here and so it's always trying to hover. Whenever it hovered this is the hovering location someone took it and throw it away so robot was going somewhere else building a map of the scene trying to find again a place which has seen before. an existing part of the environment has a good estimate where it is that is what we refer to as localization and the last part which is loop clothing so given I kind of I don't know where I am some a large uncertainty and I you can use this approach to see how well do the features that I see at the moment mattress features have seen in the past and try to find an alignment for this this is a good alignment you may accept that or you may try this for a couple of consecutive frames that not just kind of one bad image screws up everything. Tasker builds a map online and use the map in order to make navigation decisions of where it should actually go. It's an online process which requires us to built the map to update the map and always come up with an consistent estimate of the map. Tasker then generates steering commands which always guide the platform back to the desired location in this case again it should hover here at one location okay these are two problems with this we discussed or we said how can we get around them. The platform is actually a little bit tricky for the platform to always hover or people walking by and yeah and then someone again pushing the platform away. talk which I Neff was more over more whatever like wait overview about how different approaches work was not going to too many details. The second part of the talk today I would like to talk about ambiguities in the environment and what are good ways for dealing with them. How can we actually even though we have environments with ambiguity build accurate maps consistent maps of the environment so they are are so or the main assumption here is not we simply ignore all n big you T's and say the environment has no ambiguity. There are multiple hypotheses how it can match inside and they overlap therefore it's local the other ones non-overlapping its global so I don't know how this a fits in here so does this guy over here fits this one this one or this one so either here here or here simply something I it's good fit maybe yeah doesn't sound too bad just add them to kind of a temporary constraint list and this is shown here in red so this one can Michigan this pot this post this post is against this opposes both this post and this guy again these two poses some of them will be likely to be right. same author I've been all since ethanol in this group who develop this approach and later on max mixtures so this would be one nice application for mac semesters but assuming we don't have max mixture we have to treat those things separately okay and what can you we can actually do two tests the first one is a global sufficiency test so we want to say there is no possible disjoint match in the uncertainty lives it means a cannot be completely somewhere else it's not possibly that a can be somewhere come at a completely different place. Every vector consists of zero and once it is one hypothesis about the consistency of my matches. If all of them are zero which means it's completely incorrect it's just one. If I have a group where everything agrees I can add all the ones once once once in this vector and I will get a high score. I get scores among the groups but not between each other. I have this function just high values for both elements and low values for bad hypotheses. I try to find the vector B which maximizes this fraction that's exactly what is done. this constraint that is an np-hard problem this is a corresponding densest subgraph problem which is an NP- hard problem sort of find the best v actually need to try out all possible solutions which is something which for a large number of constraints simply doesn't work out therefore the ID years okay III know how to compute it but it's to computationally demanding to do that let's see if you're trying to find the approximation out of that and actually find a pretty good approximation for that by saying okay I simply don't treat my vector V as discreet I just allow continuous variables because then I can actually optimize this and then get a solution. and if I have multiple solutions for that get MA multiple. The larger the eigen values are the better the score so there's a proof that i get a perfect combination I get a couple of eigen vectors with current putting eigenvalues. If I visualize this so one situation over here so this is kind of the first eigenvector second eigen vector third force this is Lambda i if this is a kind of a high value. This is the best solution that I have and all other solutions are much worse in performance.

ROUGE-1: 22.55, ROUGE-2: 21.99, ROUGE-L: 20.05
BERTScore: 69.52

==============================================
==================== [29/100] ====================
Summary:
Marketing is about four things creating communicating delivering and exchanging value. The first thing is to identify an unmet need that consumers have. The key message in our advertising is going to talk about quality. We have to provide proof in our commercials in our print ads on our website. We need to identify we need to determine a need that is not being met in the market so it's not like what are some of the problems that they have when cooking or baking and what do you think they're going to say food sticks to the pot. for something to be of a good value it doesn't need to be a low price it could be a high price but it's a very high quality and it has a lot of benefits do you agree who could explain that further good tell us your name theresa bad um i mean would you consider that clothing yes go ahead like so like i find people saying like buying a pair of jeans and zara is expensive because there's like 65 but they last longer than buying these at all maybe unless unless. marketing is about creating communicating delivering and exchanging value the way that we communicate the value is through the brand. The brand is what's wrapped around the product so all products in a given category have the same generic functionality which is transportation. What makes one car different from another is the brand so every car is wrapped in a brand and we're going to talk more about perceptual maps where our brand is positioned in the marketplace relative to our competitors. We could look at that through market research to understand the perceptions that consumers have for our brand importantly relative to other brands. You're not going to be able to have a commercial that's going to resonate with everybody in your target audience or more specifically with your target market so he said all men so when you're showing a commercial do you think that those men that are seeing the commercial between the ages of 18 and 29 want to see somebody in the commercial who's 60 or 65 that's probably not something that's gonna resonate with them. A thousand can be statistically significant a thousand to fifteen hundred but in most categories in the united states it doesn't need to be more than that. being met so what would be a good example how about shampoo i know right but an unmet need is a shampoo that is safe for hair that's what that's dry or that that's curly oily that's perm. The way we identify the need is how guess guess guess oh you're not they're not good at guessing that's not good for exam day you got to be good guessers supply and demand right the way we're going to determine the need  is through marketing research. complete that questionnaire now if 350 million people complete the questionnaire that's called the what census a census is when a hundred percent of the population participates in the research only the government does that they're really the only one that could afford to do that. What we're trying to do is get a representative random sample from that population so it doesn't need to be 350 million how many does it need toBe 349 million 300 million 200 million what do you think a quarter of what's the number how many is that?

ROUGE-1: 32.73, ROUGE-2: 31.83, ROUGE-L: 27.05
BERTScore: 60.67

==============================================
==================== [30/100] ====================
Summary:
Bentham's principle is "the greatest happiness of the greatest number" He thinks all utility is quantifiable. If you take one unit of utility from one person and give it to another person their utility will go up and the first person's utility is going to go down. If it turns out Leonid Eichmann has a vastly superior capacity to anybody else, then we could get a huge increase in total utility. Or we could change from the status quo to a more inegalitarian society. Prof: In the first instance we say that utility is quantifiable and expressible through money. Then related to that, we can work with a doctrine of revealed preference. We can vary the price that we charge admission for the course. Anyone think there's a problem with that idea? Yeah? Student: The idea with shoes. If you're given one shoe you're going to get absolutely no utility, but if you're. given two shoes, a right and a left, then maybe you'll get more utility? Prof: Okay. If we just kept giving you lots of right shoes, there'd be a problem. just need more money to get the same amount of happiness. For Donald Trump to get more utility, you have to give him a huge amount of new money. So the more money you have, the more you will want in order to of diminishing marginal utility. So we should take the dollar from Trump and we should give it to the bag lady, and the greatest happiness of the greatest number will have increased, right? But then maybe we shouldtake another dollar, shouldn't we? I mean it worked the first time, so we shouldn't stop. When are we going to stop? Student: What about other values like integrity? If you have a little bit of integrity and you get some more. Prof: Okay, so it's a possibility. Any other examples of where this becomes problematic? I mean, think about beer. So there might be some goods like integrity that are not easily capture-able in this logic. We should put that out there, but yeah, over here? Professor Ian Shapiro: Health. It's tricky to think about redistributing health, right? Although you'll see we will come up against some pretty bizarre cases. as all that, and he wanted to temper the downward redistribution that flows from his principle. He says, Suppose but a commencement made, by the power of a government of any kind, in the design of establishing it. He's basically saying, if you want to reduce that to a bumper oil goes up, or commodities collapse, or the dollar, or this, or that. So that when it gets down to it, you're never going to get a definitive answer to the question what is the point of practical equality. that the rich will burn their crops before giving them to the poor might not be true. And even if we get to less extreme circumstances like South Africa before and after the transition, this is what we see. Ronald Reagan comes in and says, "If we cut taxes, the pie will get bigger for all and they'll be actually more revenue," and so utilitarianism says do it. And you will find, if you go back now and look at what happened during the 1980s, perfectly credible economists will line up on both sides because they cut the taxes. classical to what we're going to all neoclassical utilitarianism is a subject with which I will begin on Wednesday. See you then for the next installment of this week's Daily Discussion, which will focus on the role of utilitarianism in the development of the modern world. Back to Mail Online home. back to the page you came from. Click here to read the first installment of the Daily Discussion. Follow us on Twitter @CNNOpinion and @jennifer_newton.

ROUGE-1: 19.32, ROUGE-2: 17.72, ROUGE-L: 15.51
BERTScore: 60.01

==============================================
==================== [31/100] ====================
Summary:
Professor: "I'm going to briefly wrap up the lecture we were doing on Friday" "We'll move on to section 2.3 about amino acids, peptides, and proteins" "What do the non-covalent forces at that membrane interface may be?" "What are all the types of interactions that you might have there? Give me a minute to think about it" "Why is the structure, negative charge, positive charge, and what is the solvent with that layer?" Bonds take on a particular shape because there's not freedom of rotation around double bonds the same way there is around single bonds. Many of these bonds show free rotations, you can twist them around, there's nothing stopping that conversion. The amide, or peptide bond, is unique in that there's restricted rotation about that bond. So it's as if you've got a linear polymer, but every third bond has kind of stuck in a particular orientation, which starts to define a lot of details about protein tertiary structure. And then in some cases, proteins may be a mixture of a secondary structure elements. is a really cool video of someone doing micro-injection into eukaryotic cells. You can drop something into the cell, and then the cell closes and maintains its integrity of the barrier. So this is a very cool observation. People do this. They have to not drink too much coffee because it's quite complicated to do a lot of micro- injection, because you can really cause carnage in your cell population if you're not very dexterous. So I just want to ask a couple of questions before, give you a couple more things to think about before we close up. The amino acids that are encoded in our proteins are all what are known as alpha amino acids. The amino acids are also chiral, but you'll learn more than you ever wanted to know about chirality in 512. Proline is a little odd because its side chain is kind of in a cyclic structure, and towards the end of the class, I'll talk to you about collagen, whose structure is totally dependent on the involvement of proline. And then the last sorts of unusual amino acid is cysteine. Compact, globular structure that's functional is encoded in that primary sequence. We may not be able to tell by looking at it what it really looks like, but all the information is there in order to program the folding. The primary sequence determines the fold, and it's the fold of the protein that mandates its function. It's not simple because what you're doing is you're solving a massive energy diagram, where as you fold a structure up, you're trying to maximize all those non-covalent forces. This is a simulation. This is all computation. It's not looking at anything by spectroscopy or in solution or anything like that. What I'm going to do is just show you for a few seconds, you know, this thing's like trying to find its thermodynamic minimum, and it's actually failing pretty badly. And then that defect gets propagated into all the fibrils and results in the weakening of the bones. So I think that's a good place to stop and I'll pick up next time with hemoglobin. Collagen is the most abundant protein in the human body. It plays enormous roles. A single amino acid change in the primary sequence of collagen can destabilize the structure, so it is no longer viable. The disease type I'm going to talk to you about is a set of diseases known as collagenopathies, and the particular one is called osteogenesis imperfecta. And here's the X-ray of a baby born with brittle bones syndrome. The bones in the upper arm are all irregular because the bones are brittle, and they'll break even in utero.

ROUGE-1: 15.82, ROUGE-2: 15.06, ROUGE-L: 13.25
BERTScore: 62.78

==============================================
==================== [32/100] ====================
Summary:
Professor: angular momentum can only take one of two values. He explains how an electron's internal form of angular momentum only exists mechanically. Professor: If you send an electron through a Stern Gerlach Apparatus, it always hits one oftwo spots that's that's right? And this is how we describe the experimental fact that we describe that's an experimental fact, right? It's an intrinsic form of momentum. It's not related to a rotation. It is an intrinsic momentum. And what do you mean by that? to the very first lecture, and so, we'll do this in more detail, but I'm going to quickly tell you-- imagine take a magnet, a little, tiny bar magnet. In fact, well, imagine you take a little bar magnet with some little magnetization, and you send it through a region that has a gradient for magnetic field. If there's a gradient-- so you know that a magnet wants to anti-align with the nearby magnet, north-south wants to go to south-north. So, you can't put a force on the magnet, but if you have a gradient of a magnetic field, then one end a dipole-- one end of your magnet-- can feel a stronger effective torque then the other guy. And you can get a net force. Lapland: Imagine you have a sphere of uniform charge distribution, and you make it rotate. And that's charged, that's moving, forming a current, right? And that current generates a magnetic field along the axis of rotation. So, for a charged sphere here which is rotating with angular momentum, let's say, l, has a magnetic moment which is proportional to l. If it's rotating, it's gonna deflect. And here's the experimental results. Every electron that gets sent through bends. And it either bends up a fixed amount, or it bends down a fixed. amount. know of a fundamental particle. If super symmetry is true, then there must be a particle called a gravitino, which would be fundamental, and would have. spin 3/2, and four states, but that hasn't been observed, yet. So, we still need a reason for why the hydrogen system, quantum mechanically, is stable. We'll talk about that a little more when we talk about hydrogen, but it was observed and deduced from experiment before it was understood there was such a physical quantity. we're gonna talk about real, physical systems in three dimensions. And as we'll discover, it's basically the same as in one dimension, we just have to write down more symbols. But the content is all the same. So, the energy for this is p squared upon 2m, plus a potential, which is a function only of the radial distance. But this is gonna be equal to, from the first term, minus h bar squared-- let me just write this out-- times r dr squared r. And so, the upside here is that when we have a central potential, the potential energy is invariant under rotations, then the energy commutes with the angular momentum squared. guys cancel, right? 1 over r times dr. So, this is equal to 1 over R dr squared r. But, why is this equal to dr squared plus 2 over R times dr? And the answer is, they're operators. And so, you should ask how they act on functions. And finally, the third fact is that r must be strictly positive, so as a 1D problem, that means it can't be negative, it must have an left on the left. these guys? Absolutely, we can. However, we're using separation. We're gonna look at a single term, and then after constructing solutions with a single eigenfunction of L squared. We can then write down arbitrary superposition of them, and generate a complete basis of states. General statement about separation of variables. OK. So, here's the resulting energy eigenvalue equation. This is purely a function of r. We've removed all of the angular dependence by making this proportional to yLm. Professor: "What do you expect to be roughly the ground state energy of this system?" Professor: " Roughly minus e0.lL 1 over 2mr squared. That's the wave function. That is V" "I'm terribly sorry! I've abused the notation terribly. Let's-- Oh! This is-- Crap! Sorry. This is standard notation. And in text, when I write this by hand, the potential is a big U, and theWave function is a little u" Professor: As you come to smaller and smaller radius, holding the angular momentum fixed, your velocity-- your angular velocity-- must increase. Professor: Is hydrogen stable because of conversation of angular momentum? Audience: No. Absolutely not, right? So, first off, we're calling that a potential term just because we can. Because we've worked with definite angular momentum, OK? You should have done this in classical mechanics as well. So, this is called theangular momentum barrier. The energy is equal to 1 over 4 n squared, where n is an integer, n plus l plus 1. The wave function phi sub E0 of r theta and phi-- oh god, oh jesus, this is so much easier in [INAUDIBLE] so, phi [INAudIBLE] 0 of rtheta andphi isequal to y0m. But what must m be? 0, because m goes from plus L to minus L, 0. do I have a 0 at the origin? Is that the question? AUDIENCE: Yeah. PROFESSOR: It's true. There's nothing special about the origin, except for two things. One is that we're working in a system which has a rotational symmetry. Second, saying that little u has a 0 is not the same as saying that the wave function has a0. So, the physical thing is the probability distribution, which is the [? norm ?] squared. From this, we can build two nice quantities. We can build something with units of a radius. This is equal to 4 times the binding energy, which is also called the Rydberg constant. We've discovered the energy is, in fact, not just independent of m, but it's independent of l, too. Why? What symmetry is explaining this extra degeneracy? We'll pick that up next time. We're off by a factor of 4 from the observed binding energy of hydrogen, 13.6 eV. because solving it is a sort of involved undertaking. So, we had this differential equation-- this guy-- and we want to solve it. We first did, we did asymptotic analysis. Then we did a series approximation. And here, I'm just going to write down the answers. And the reason is, first off, this is something you should either do in recitation, or see-- go through-- on your own, but this is just the mathematics of solving a differential equation.

ROUGE-1: 16.78, ROUGE-2: 15.76, ROUGE-L: 14.00
BERTScore: 58.58

==============================================
==================== [33/100] ====================
Summary:
The 60002 course is the second half of the 600 program. The main topic is computational models. The final exam will be based upon all of the above. The lectures will be a bit faster paced than 60001. The course is really less about programming and more about dipping your toe into the exotic world of data science. It will be taught in Python, but there will be additional bits of Python as well as some comments about software engineering, how to structure your code, more emphasis in using packages. Science is moving out of the wet lab and into the computer. Increasingly, there is an increasing reliance on computation rather than traditional experimentation. We'll talk about three kinds of models-- optimization models, statistical models, and simulation models. An optimization model is a very simple thing. We start with an optimization model and then go on to other types of models, such as statistical models or simulation models, which can be used to predict the future of a given area of science. For example, a climate change model. We can build models that sort of explain how the climate has changed over the millennia. with an objective function that's either to be maximized or minimized. We then often have to layer on top of that objective function a set of constraints, sometimes empty, that we have to obey. So for, example, if I'm going from New York to Boston, I might want to find a route by car or plane or train that minimizes the total travel time. We use these things all the time. I commute to work using Waze, which essentially is solving-- not very well, I believe-- an optimization problem to minimize my time. The most obvious solution is brute force, but that's not very practical. Here's some code that uses greedy. Takes in the items, the constraint, in this case will be the weight, and just calls greedy, but with the keyfunction and prints what we have. Used greedy by value to allocate and calls testGreedy with food, maxUnits and Food.getValue. And then we have something pretty interesting. What's going on with this lambda? So here we're going to be using greedy by density to allocate, sorry, greedy by cost. We want the cheaper items to get chosen first because they have fewer calories. Python's test function testGreedy takes foods and the maximum number of units. It's going to go through and test all three greedy algorithms. When we use greedy by cost, I get 318 happiness points and a different menu, the apple, the wine, the cola, the beer, and the donut. I've lost the pizza and the burger. And here's another solution with 318, apple, wine-- yeah, all right. So I actually got the same solution, but it just found them in a different order. Lambda is used to create an anonymous function, anonymous in the sense that it has no name. Lambda does is it builds a function that evaluates that expression on those parameters and returns the result of evaluating the expression. So instead of writing def, I have inline defined a function. Here you have it or maybe you don't, because every time I switch applications Windows decides I don't want to show you the screen anyway. This really shouldn't be necessary. Why it keeps forgetting? Anyway, so here's the code.

ROUGE-1: 19.95, ROUGE-2: 17.62, ROUGE-L: 14.22
BERTScore: 57.00

==============================================
==================== [34/100] ====================
Summary:
Expectations play a huge role in economics, says Ricardo Caballero. He says they play a big role in the decision of all economic actors, including investors, consumers and firms. He gives a shortcut to think about the role of expectations in the kind of models we have already discussed. CaballERO: I want to revisit the consumption function and the investment function, now taking into account expectations and motivate how you should really think about consumption and investment in a more realistic model than we have been discussing. Milton Friedman called it the permanent income theory of consumption. What really matters to you in a consumption decision is what you expect to get on average during your lifetime. How wealthy you are will pin down more or less the consumption you have more than your current income. The very rich seldom sell assets. They borrow against those assets to fund consumption. It's easier to borrow against your human capital than your income. But even if you have no income, you will probably borrow against that wealth to the extent that you can. you're going to consume more for any given level of wealth, OK? It's temporary, but that's what it is. What about investment? That's a decision by the firm. How much physical capital? I'm talking about physical investment, real investment, not financial investment. The decision also depend on current, but particularly on expected profits. And when you think about expected profits, you need to think about interest rate as well. We put the interest rate, as I said, OK, it's more expensive to borrow if the interest rates is high. But actually it matters a lot more than just that. machines, is some sort of geometric depreciation-- so meaning, it's not deterministic. It's more or less-- machines break down occasionally, but there is certain probability that they break down. We typically call that notation in economics-- we refer to that as delta. That's the depreciation probability. So if you think in terms of expected value, if you buy a machine today, and you ask, how much of a machine I'll have next year, well, it'll be a weighted average of 0 and 1 probably. The Fed is trying to cool the economy, but the loan rates have begun to decline already. The Fed would like you not to believe that will happen. And that's a big issue. Monetary policy depends a lot on its ability to convince people that things will remain in the direction that they want. If they fail-- there was a famous episode in US monetary policy during the times of Alan Greenspan. It was known as the Greenspan conundrum. He kept hiking interest rates. But he couldn't cool off the economy. but are the variables we expect of those-- are the values of respect for those variables in the future and, again, with the same sign. So if output-- so if taxes go up today, aggregate demand will decline, and output will decline. But if I expect future taxes to go up as well, then that's going to depress aggregate demand even more. That's the type of logic I want you to develop. So this is the IS in the same space I had before-- interest rate and output, current output. Mervyn King described good monetary policy very much like Maradona's goal scored against the UK, England, in some World Cup. And he reason, the perception of fiscal deficit, was really dragging the economy down because people didn't know when there could be a financial crisis in the near future. So most of the fiscal contractions are contractionary. But there are some famous episodes of what are called expansionary fiscalcontractions, such as in Ireland in the late '80s. was expansionary. That was contractionary. But it was overwhelmed or offset, more than offset by the improvement in the outlook that you had. And that also happens with monetary policy. Countries that have high inflation problems and so on sometimes get-- and they have to go through dramatic tightenings. Yes, most of them get very short-lived recession. But sometimes they are veryshort-lived recessions because eventually the reduction of the instability caused by high and unstable inflation sort of ends up dominating any direct contractionary effect.

ROUGE-1: 18.19, ROUGE-2: 17.21, ROUGE-L: 16.35
BERTScore: 64.62

==============================================
==================== [35/100] ====================
Summary:
We have in essence three different views of fiscal policy and how well it works. Keynes says we can change aggregate demand in such a way that we can make. The crowding out problem and the crowded out problem says well when you go out and you borrow all of this money you're gonna change the demand for loanable funds. We have a political problem here which is that once you actually start spending money it becomes difficult to take money away once you start giving people something taking out money away now becomes very very difficult. irate demand it's what's changing the short-run aggregate supply right eventually this recession we've got a lot of unemployed people remember here you see unemployment is greater than the natural rate of unemployment right so in essence you have cyclical unemployment we don't want that. Eventually we'll return to this full employment level of output and we'll do so at this lower price level say pl0 or I'll just call it pl - so we know that eventually SRA s increases to S aureus - output equals YF price level equals po2. money aside for college that was the loanable funds market they're putting money in there for savings they're saving it they're the suppliers you guys are borrowing money to go to school and you're now demanders you're the ones taking money out. When the interest rate goes - I - what happens to private borrowing what is it at now do it q2 is the total that's private and public how much is private borrowing. If I borrowed 300 from you you have 300 less to spend and so this guy goes up by 300 this guy go down by 300 what happens with this guy he stays the same. Paige: There's some tax rate here between zero and 100 that makes tax revenue as large as it can possibly be. Paige: Every single Monday that I come to work I'm essentially not doing anything I do all the day's work I deal with all of the kids. If you go labor every Monday right Monday Tuesday Wednesday Thursday Friday 20 40 60 80 100 % I don't teach on Saturdays all right and so if they're taking 20% it's like they're like forget it it's not worth it.

ROUGE-1: 14.41, ROUGE-2: 13.88, ROUGE-L: 11.13
BERTScore: 64.59

==============================================
==================== [36/100] ====================
Summary:
Professor: The plan for today is as follows. We're going to look at this unitary time evolution and calculate this operator u, given the Hamiltonian. Then we will look at the Heisenberg picture of quantum mechanics. Professor: We'll find the heisenberg equations of motion and solve them for a particular case today. It's a pretty useful way of seeing things. It makes the relation between classical mechanics and quantum mechanics more obvious. So it's a very important tool. The Schrodinger equation is there. OK so now let's solve this. We'll go through three cases. Case one, h is time independent. So H of t is really H like that. So what could that be? For example, you know that the particle in a magnetic field, the spin in a Magnetic Field is minus gamma B dot the spin. And you could have a time dependent magnetic dt equals this, which is the same as this equation. Now finally, I want to discuss for a second the general case. So that's case-- there was a 1, a 2, a 3 H of T general. But it's not too much you can do. U of t t0 is going to be e to the minus iHt0 over h bar, some constant matrix. When t is equal to t0, this matrix becomes the unit matrix. U0 is the inverse of this matrix, which is nothing else but e. The Heisenberg version of this operator using the definition that anything, any operator that we have a U dagger in front, a U to the right, is the Heisenburg version of the operator. So what did we have? Here it is. ih bar d dt of A He Eisenberg of t. The magnetic field is time dependent, but its direction is not time dependent. And the Hamiltonian at different times commute because Sz commutes with itself. So if you have a magnetic field that is fixed in one direction but change in time, you can have a situation where your Hamiltonian is timedependent. And you will discuss such case because it's interesting. But later on as we do nuclear magnetic resonance, we will have the more interesting case in which the magnetic field isn't time independent. the derivative of this quantity with respect to time. But in terms of completeness, it's kind of pretty in that you go from the exponential to the time ordered exponential. And I think you'll see more of this in 806. And what we're going to do now is turn to the Heisenberg picture of quantum mechanics. Yes, questions? Audience: Why does R dot [INAUDIBLE]? PROFESSOR: Because that's really a property of integrals. of this, you will get H times that thing. So since it's a power series, you'll differentiate the first term, and you'll get the right thing. Then the second term and you will start getting everything that you need. It's reassuring that something like this success, but in general, you would want to be able to do all these integrals and to sum them up. So it's of limited usefulness. But when you have a practical problem, generally that's not the way you solve it. We will try to figure out the solution some other way. T is not an operator in the usual sense of quantum mechanics or anything like that. It's an instruction. Whenever you have an exponential of this form, the time ordered exponential is this series that we've written down. At time equals 0, the Heisenberg operators are identical they to the Schrodinger operators. And therefore Xh of t is equal to A, which is X hat cosine omega t plus B. And at t equals. 0, Ph of t becomes equal to this is 0 m omega B. U t 0 dagger 1 U t 0. But 1 doesn't matter. U dagger with U is 1. This is a 1 Schrodinger, and therefore it's the same operator. So the unit operator is the same. It just doesn't change whatsoever. OK, so that's good. But now this is something interesting also happens. Suppose you have Schrodingers operator C that is equal to the product of A with B, two Schrodingers. And then you see why this is really nice. Whole thing is the same. So this is something very useful and we'll need it. One more comment, expectation values. Comment number four on expectation values, which is something you've already-- it's sort of the way we began the discussion and wanted to make sure it's clear. So the Heisenberg operators, at this moment, are a little mysterious. They're supposed to be given by this formula, but we've seen that calculating U can be m omega squared cosine squared omega t X squared. P Heisenberg times the commutator of X and P, which is ih bar times a factor of 2. And then what do we get? The ih there and ih cancels. Well, it actually looks like an equation in classical mechanics. So how do we solve for them now? Well, you sort of have to try the kind of things that you would do classically. Take a second derivative of this equation. So d second Xh dt squared is equal to minus omega squared Xh, exactly the equation of motion of a harmonic oscillator. variables in, it just becomes identical to the Schrodinger Hamiltonian. All right, so that's all for today. I hope to see in office hours in the coming days. Be here Wednesday 12:30, maybe 12:25 would be better, and we'll see you then. [APPLAUSE] We'll be back at 12:50 on Wednesday, and be here at 1:30 on Thursday. Back to Mail Online home. back to the page you came from.

ROUGE-1: 21.17, ROUGE-2: 20.07, ROUGE-L: 17.42
BERTScore: 66.69

==============================================
==================== [37/100] ====================
Summary:
Borat: I want to talk about my favorite part of the Second Discourse, a book that never grows old. Borat: Inequality begins in a faculty or a disposition that is untranslatable into English. He says it is amour-propre, the first term I put on the board, which is the first and most durable cause of inequality. The miserable is everybody else, Borat says, and Rousseau wants us to feel how bad things are, how bad we are. Rousseau says amour-propre only arises in society and is the true cause, he believes, for our discontents. "Amour de soi-meme" is a kind of self-love, he says, that is at the root of our desire to preserve ourself. Rousseau speculates about this and, again, this is part of his hypothetical or conjectural history. The desire to be seen and respected by others is the underlying cause of our sense of justice, Rousseau says. gaze of another, and it is from that gaze, from the look or gaze of another that the passion of vanity was born. The one who sang or danced the best, the handsomest, the strongest, the most adroit or the most eloquent became the most highly regarded. From these first preferences were born vanity and contempt on the one hand and shame and envy on the other. Our own sentiment of self and existent comes entirely from the judgment, as he puts it, of those around us. Rousseau: Amour-propre and society gave rise to the state of war. Do you remember that, about the cartoons of the prophet Muhammad and the outrage and the protests, often violent, that occurred about that? To some degree, I think, Rousseau would believe the protesters over those cartoons had a point. Their views were not being respected and to which you might say a Lockean or a liberal formulation of the problem or response would be, "Well, so what?" The task of government, according to Locke or the liberal view, is to ensure the respect.proportionate to the esteem in which he held himself. Government's job is not to impose a gag order on what can and cannot be said, he says. This is a respectable, sort of liberal line of thought going from Locke to John Stuart Mill, he adds. But there is something powerful and true about what the government is supposed to do, he writes. It is to protect you from harm and provide you with the freedom to practice what religion you like, consistent with others' freedom to do so too, he argues. The Danish prime minister has refused to apologize for the cartoon. of civilization is responsible for all of our miseries. Yes, it is society's fault. It's not your fault, he wants to tell us, it's society's. Yet he also leaves us with no real apparent way out. He denies that we can, as a practical solution, return to simpler, more natural forms of political association. But how then do we resolve the problem that he left us with? And his answer to it is contained in his book, yes, called the Social Contract.

ROUGE-1: 17.72, ROUGE-2: 16.13, ROUGE-L: 14.77
BERTScore: 59.64

==============================================
==================== [38/100] ====================
Summary:
So I think I just spend like five minutes, just briefly review on the backpropagation last time. So I didn't have time to explain this figure, which I think probably would be useful as a high level summary of what's happening. This is how you define network and the loss function. So you start with some example x, and then you have some-- I guess, this is a matrix vector multiplication module, and you take x inner product multiplied with w and b. And then get some activation, the pre-activation. And you get some post activation. one of those three lemmas. If you know how to compute the derivative with respect to the output of some module, suppose this is a module, tau is the output. And now you can see what this does kind of like lemma are for. Those lemma, basically, are saying that if you know dg over the d tau, how do you computedg over da? And there's another lemma which says that ifYou know how. to compute dgover da, howDo you compute dG over dz? All of those lemma is about this kind of relationship. Some of the practical viewpoint of ML, like how do you relate to your model, what you have to do in this whole process. So far, we only talk about training. We have some examples, which we have seen when they are training data sets. And often, this is called test distribution. And then you evaluate what's the expected loss on this new test example. And oftentimes, it's not always true. The training loss is less than the test loss. So you have a discrepancy between training and test. time. We're going to discuss what will happen if you change your model complexity, and whether in what cases, you may underfit. In what cases you may overfit, and what is the best response. We are going to decompose the test into two terms, which is called bias. And technically, it's bias squared because the bias is defined as the square root of this term. So plus variance, the test error is the sum of these two. And so the question you want to answer is that if you. change the model. complexity, what's the best test error, right? In some sense, you care about two quantities. You care about the training loss and the gap. You want both of these two to be small. And typically, when l theta is big, there are two failure mode in some sense. So one of the failure mode is called failure patterns. And so overfitting, I'm going to discuss a lot about overfitting. But the first other bit is that the typical situation of like this, maybe this, and something like this. And what's the best fit? The best fit probably would change a little bit. Bias is a decreasing function as the model complexity. The bias is the best error or loss, you can get with even infinite data. If bias is large, even with infinite data, you cannot do anything. So bias is a property of the large family of linear models, right? So bias would be the best [INAUDIBLE] linear model that is that is true that is a linear model. And that error, that closeness, that is the bias, basically is the closest to the ground. In the lecture notes, actually, there are some visualizations of the real models you're going to fit. So for linear models, I guess, you can see a bunch of properties. There's a large training error, training loss or training-- let's call it loss just for consistency. And now, I'm going to show cases where the variance is the culprit to blame for. So basically, you are looking at-- you are kind of like overfitting to the spurious patterns, but instead of the big pattern. the training data set. This is your prediction for this x. And you look at the distance between the prediction and the true label. The training error is pretty big. So this is underfitting, by our definition of underfitting because the tuning is already big. Why the training is big? What's the culprit? The culprit, I would argue, is that it's just because no any linear model can fit your data. It's not just because you don't have enough data, and your to fit a fifth-degree polynomial. A fifth-degree polynomial can go up and down so many times, several times. So the higher the degree is the more times you can go. So if you have high degree polynomials, you can be more flexible. If you have not too many data, but you have very, very simple model, then it's probably still OK. So suppose you have a lot more data. And you observe a lot of more data, roughly. There's a little bit fluctuation, of course. spurious patterns are the fluctuations in some sense. And so in other words, I think you are explaining the noise instead of the ground truth. How do I formulate this? Like one way to kind of formulate this a little bit more mathematically is that you can consider to redraw the samples. So you redraw some new samples with different spurious patterns. They are spurious because they are noise. If your model is specific to the variance is caused by lack of data. And it can be mitigated if you have more data. same distribution. From the same distribution. Yeah. So like if you collect more data from-- yeah. So in some sense, you kind of like the mindset-- I'm not saying this is universally applicable to every situation, but the mindset we are in is that, for example, you have a lot of like medical images. And now, I'm asking I found out my variance is very big. So how do I mitigate that? So probably one thing is that I can just sample more data is not expressive enough. is that this test error should have this U curve. But then, people realized that this is a striking thing. So people realize that if you increase your model number of parameters even more, at some point, you will see that it will be like this. This is the second descent of the test error. That's why it's called double descent because there is a decent here, there's a descent here. So this peak is often happening when-- it's roughly equal to d. And we realized when d is kind of above n, above the number of data points, you had a peak. people really care about it. And what I mean by that is that even within linear models, you can try to change the model complexity. So what that means is that you try to decide how many features you use. So you can start with only using one feature or two features like for example, in the house price, where you can use the square foot as the single feature, or you can collect a bunch of other features. So keep adding more and more features. That means you have more. and this model. So this model seems to have less parameter than this. That's by definition. The norm is actually very big. So in some sense, if you use the norm as the complexity, actually, these peaks have large complexity. So what is the right measure for complexity? So this is a very difficult question. Like for different situations, you have different answers. But there is no universal answer. But norm could be one complex measure. And for linear model, it just happens that for mathematical reasons, I think l2 norm behaves really nice. going to discuss this more next time. So the high level thing is just that something else is driving the norm to be small. Thanks. Going to talk more about this in the next few days. Back to Mail Online home. back to the page you came from. Back To the pageYou came from: Back to thepage you camefrom. Back into the page You came from was from: The Daily Mail. Back onto the pageyou came from, the DailyMail.com page you were from.

ROUGE-1: 18.76, ROUGE-2: 17.83, ROUGE-L: 15.15
BERTScore: 70.76

==============================================
==================== [39/100] ====================
Summary:
Vladimir Ilyich Ulyanov, AKA Lenin, helped overthrow the Russian tsar Nicholas II in 1917. He founded the Soviet Union, one of the worst dictatorships of the 20th century. But was he a hero who toppled an oppressive tyranny or a villain who replaced it with another? It's time to put Lenin on the stand in History vs. Lenin, says Alexander Nekrassov, the author of the novel "The Death and Life of Vladimir Lenin" how would I have sounded?" We can never be sure how things could've unfolded if different people were in power or different decisions were made, but to avoid the mistakes of the past, we must always be willing to put historical figures on trial. We must never forget that history is not an exact copy of the present, but it can be a guide to the future. It can be used to learn from the mistakes made in the past and to make better decisions in the present. It is never too late to put a historical figure on trial for their actions.

ROUGE-1: 26.44, ROUGE-2: 18.07, ROUGE-L: 16.46
BERTScore: 59.46

==============================================
==================== [40/100] ====================
Summary:
The goal of this course is to give you the tools to interpret complicated phenomena. You don't know anything unless you have a reductionist picture of what's going on. The relationship between the effective quantum number and the ionization energy of a state then provides a hydrogen-atom-based structural model for everything you can observe. Your support will help MIT OpenCourseWare continue to offer high-quality educational resources for free. To make a donation or view additional materials from hundreds of MIT courses, visit MIT Open courseWare at ocw.mit.edu. happen. But this was something that nobody in the world was interested in because it was the breaking of the usual patterns. And so I'm going to try to give you at when you go from hydrogen to helium, you can't solve the Schrodinger equation exactly. But you can do it really well, and it just costs computer time. And if the computer is doing the work, you don't really care. Because once you've told the computer the rules, then it's off to the races. So what we know is this permutation operator, operating on any two-electron function, has to make-- OK, I'm skipping steps, and my notes are really kind of stupid sometimes. So there's only two possible eigenvalues. You can have minus 1 or plus 1. And the minus 1 corresponds to fermions, things that have half-integer spin, like electrons. The plus 1 corresponds, like photons, and vibrons, and other things. And actually, it's harder to construct a symmetric function than an anti-symmetric function. So we just have to kill this one. Stick diagrams are great, because it's easier to see on a picture, who are the actors, and have I included all of them, or have I left something out? And so now we're interested in the stick diagram for the 1s 2s configuration. And there are several kinds of 1s/2s configurations, depending on what the alpha and beta are. So there's four guys, and we can put our arrows on these things. It tells us what to do. want details. Hund: We want to know the trends of things, and we want to be able to do something like what you did in freshman chemistry on shielding. Hund: People get really excited when they discover a violation of Hund's rules. And it's just trivial. So there is this. What time is it? I have a few minutes to talk about shielding, and I will. OK, so we have a nucleus. and it has a charge of z. And so outside the nucleus, the charge is plus one, because you have a neutral atom. And then when you penetrate inside this region of dense charge, and all of the spins are generally paired, this is spherical. Hund's rules is all about, of all of the states that belong to a particular configuration, which one is the lowest? One-- which one, not the second lowest. And why do we care? Because in statistical mechanics everything is dominated by the lowest energy state. So if you can figure out what is the low energy state, you've basically got as much as most people are going to want. The highest possible value of J is equal to L plus S-- so for L, N greater than 3.

ROUGE-1: 18.43, ROUGE-2: 17.75, ROUGE-L: 14.70
BERTScore: 63.37

==============================================
==================== [41/100] ====================
Summary:
As a nurse we will transfuse a patient who is low on red blood cells with new blood cells via a venous access of some type. Red blood cells are very vital for our survival and how our body works so in other words our body can't function very well without them. Most hospitals require that you're a registered nurse in order to transfuse the blood so again follow your Hospital protocol with that. You can access the free quiz that will test you on this procedure so let's get started. matches up perfectly you're gonna look look at the patient's blood type versus the donors tie and the Rh factor. If there's a discrepancy you'll need to notify the blood bank immediately and just from personal experience this has happened with one of my patients I was doing the whole verification process with another nurse and we were looking at the blood bag. There was one letter that they had did a clerical error on so I had to send the blood and we had to go through the whole process again. If someone's having an anaphylactic reaction to something another type is febrile and this is non hemolytic so you don't have the breaking up of those red blood cells. This is where the recipients white blood cells or reacting with the donors white blood cell and this causes the body to build antibodies. You can see that increased temperature like one degree Celsius or one point eight degree in Fahrenheit from the baseline. Another transfusion reaction you can have is the GVHD the graft-versus-host disease and again like I said this is rare but it's deadly.

ROUGE-1: 11.81, ROUGE-2: 11.33, ROUGE-L: 11.65
BERTScore: 63.75

==============================================
==================== [42/100] ====================
Summary:
In particle physics and in nuclear physics, we use a system called natural units. This system is based on fundamental concepts of quantum mechanics and special relativity. In some examples, we'll use SI, in others, use natural units, such as Heaviside-Lorentz units. We'll use those natural units as we go through class. This will be a discussion in 8.701 on units, and we'll end with a question-and-answer session on the use of SI units. equals m c squared and all those things, m, E, and also the momentum have the same unit. That simplifies quite a bit. You might think that you lose information by setting fundamental constants to 1, but you actually do not, because you carry with you, in your equations, the dimension of the problem. If you want to do a quick exercise here, I invite you to calculate the charge radius of the proton, which is 4.1 over GeV, or per GeV. always be clear from the problem we're looking at. Always be clear about what you're trying to do. Always show us how you're going to solve a problem. Always. Show us how to get to the solution you're looking for. always. show us the way to get there.always. be clear of the problem you're aiming to solve. always be clear. of the goal you're seeking to achieve.always be. clear of your goal. of getting to the answer you're after.

ROUGE-1: 38.68, ROUGE-2: 29.65, ROUGE-L: 26.69
BERTScore: 56.89

==============================================
==================== [43/100] ====================
Summary:
Jeffrey Grossman: We're going to talk about energy storage. He says the planet is kind of a storage device for that thing over there. Most energy storage needs are going to be around a year or less, he says. Grossman says electricity is more and more important in our lives. We'll go into batteries and then talk about the chemistry of batteries and throw in a couple of, why this matters, he writes..com/jeffrey-grossman/energy-storage-technique. The second law of thermodynamics makes you pay a penalty of energy if you convert it into heat and then back into electricity. Most power plants at best are around 50% efficient in terms of converting the thermal energy into electrical energy. So you could do chemical reaction to heat. That's how we power most of our world today. And it's one of the reasons why electrochemical change not going from your computer to your phone but going fromyour car to your cell to a house to a grid. Entropy is not about smoothness and it's not about disorder. Entropy is about accessibility to states. A messier room does not have higher entropy, and in this case, it's just simply the rules of the algorithm. It's about how many states you have to be in. Now, that can appear to be like disorder, right, but a messier Room does not has higher entropy. I love that you guys are blown away. I have rules here about entropy. In the early 1900s, a physicist named Galvani tried to make a connection between motion and electricity. He hooked up a lightning rod to a frog, and the frog went crazy. He deduced that the motion itself is something that generates electricity. The story of Frankenstein was written because they went around and electrocuted things and showed that they moved, and often they weren't alive, and so Mary Shelley, I think, saw that. The volt is named after Volta, who created the first stable battery, the voltaic pile. How do you make a battery that doesn't heat up when it's turned on? How do you stop the heating from happening and instead take advantage of this trading of electron work? That's what a battery is. It's just a different construction of the device, but it's just as important as the design of the battery, right? We'll get to that in a few minutes, but first, let's look at a reaction that happens when you put copper sulfate and zinc in a beaker. is yes, but I'm taking 2. I'm not taking 1, I'm talking about the oxidation state of 2 plus. That's going to be harder here. It's harder to take a 4s1 in a 3d from that beautifully filled 3d shell than it is to take these 2s electrons. So what you can see is that the potential difference is all about chemistry, right. And then finally, the last kind of piece of this is that we need a reference because you can only get changes in potentials. renewables at large scales unless you store it. In fact, we're really at essentially the tipping point. If you look at-- this is Germany, now. Huge penetration-- these are these six years, seven years of adding PV. They're now at 7%. 2016 is 7%. This is how much the price of electricity was in Germany when they added. They can sell it for a lot of money. But as they add more of this renewable to the grid, its price goes down when it's available.

ROUGE-1: 13.85, ROUGE-2: 12.06, ROUGE-L: 11.60
BERTScore: 61.47

==============================================
==================== [44/100] ====================
Summary:
Last time, we talked about some of the kind of the bigger questions in deep learning theory. Today, we are going to start talking about the optimization perspective in deeplearning for two lectures. The main focus is to analyze what the functions you are optimizing look like so that you can use some trivial or some standard optimization algorithm for it. The bigger question we are trying to address here is that why many optimization algorithm are designed for convex functions. But why they can still work for nonconvex functions? So GD cannot always find local mean or global minimum, right? This is for continuous functions. So why? I guess a simple example here, I cannot-- it's easy to come up is that you just say maybe-- actually, I'm not taking the simplest one just for some reason because I'm going to use this again. So let's say suppose you have f of x1, x2, which is something like x1 squared plus x2 cubed, OK? The origin satisfies the gradient 0 and satisfies the Hessian is PSD. So these are necessary conditions for being a local minimum, but not vice versa. There is some caveat about whether you can even converge to a local minimum. We're going to show some very, actually, simple cases where we can prove this. The problem here is that, when the gradient of fx is 0 and also the Hessian is not strictly positive semidefinite, it's just a positive semidefinite. If your second order derivative is literally 0 in some direction, then a third order derivative starts to matter. So that's why local minimum is not only always a property of the first and second order derivatives. NP-hard. But actually, finding a local minimum is also NP-hard, right? So we have to consider these kind of pathological cases, which makes things harder. So the way to go beyond it is that there is a way to also remove some of the pathological cases as well so that you can find a local Minimum in polynomial time. And then we can talk about matrix completion, which is kind of like an upgraded version of PCA. And as I said, this is actually a pretty important question in machine learning. I think I wrote a book chapter about this kind of optimization thing for our book. So I can send that to the person who take the Scribe notes. And that probably help you to have some references. But the materials are not exactly the same as the book, so you still have to do the Scribes kind of from scratch in some sense. OK, cool. The third strict-saddle condition sounds hard to check. So you cannot check. There is no way you can check whether empirically your function satisfies this condition. So now, what you can do is this, right? do linearized network, there is a little bit more things to do beyond that. And the second example I'm going to give is matrix completion. This is an important machine learning question by itself as well, right? So before deep learning, this was one of the most important topic maybe in machine learning. And now, still I think it's used in the recommendation system. OK, cool. I guess let's talk about PCA first. So I guess I'll maybe more precisely say matrix factorization. so one dimension. So d is 1. Then you just have a scalar, m minus x squared squared. This is our function, g of x. And you plot this function. And there are two local minimum. And they are both global minimum because there is some symmetry here. OK, so let's talk about the proof. So how do we prove this? So as you can imagine, the proof is pretty simple. The plan is very simple. So if you apply these kind of techniques, you can get the Hessian like this. The quadratic form related to the Hessian is much easier to compute. The methodology is the following. You just iteratively expand it, Taylor expand it. And then if you have this, then this basically corresponds-- if you replace epsilon to v, and so forth. So in this case, from this quadratics form you can figure out what the corresponding matrix is. But for many other cases, actually, it's very hard to write out that matrix of the Hessians. It requires some intuition. But it also probably makes sense because the top eigenvector direction is the global minimum. top one eigenvalue-- eigen vector with the right scaling. So the second case is that x has eigen value, let's say, Lambda, which is strictly less than Lambda 1. There is no guarantee that two eigenvectors are always orthogonal because they could have the same eigen Value and they are just in the same subspace. But if they have different eigenvalues, then they have to be Orthogonal. So that's why x1 is orthogona to b1. Let M be a rank one matrix, and symmetric, and PSD just for simplicity. You can assume M equals something like zz transpose, right? And z is kind of the ground truth. And the setup is the following. So we are given random entries of M. We pick some random indices of M, and you review the corresponding entries. That's the only thing you know about M. And then the goal is to recover the rest of the entries,. More formally, so you say that there is omega, which is a subset of. the entries, subset of the indices of d. assume that the input are linearly separable, then there is a proof for this. And there are a bunch of other cases where you can have some partial results. Next week, in the next lecture, maybe the second half of next lecture,. I'm also going to give another result, which is somewhat more general. It applies to many different architectures, but it has other kind of constraints. First of all, it doesn't really show exactly these kind of landscape properties. It shows that these kinds of properties holds for a region, for a special region in the parameter space.

ROUGE-1: 18.75, ROUGE-2: 18.05, ROUGE-L: 17.61
BERTScore: 68.59

==============================================
==================== [45/100] ====================
Summary:
Iceland is one of the world's most active volcanic hotspots crafer has erupted 30 times in a thousand years and last blue in the 1980s. Scientists are now preparing to drill into it the is to learn more about how volcanoes behave so that we can better predict eruptions and also tap into a super hot source of energy volcanoes can be spectacular but they're also devastating around the world millions of people live close to them here in Iceland. Researchers here hope their work will change that helping to save lives and money while also pioneering a form of volcano power.

ROUGE-1: 23.42, ROUGE-2: 22.80, ROUGE-L: 23.42
BERTScore: 66.24

==============================================
==================== [46/100] ====================
Summary:
Taxes are one of three different kinds of tax we're gonna have aggressive so with our regressive tax as people earn more income base they are attacked a smaller amount of smaller percentage of the rounding. It's not the dollar amount it's gonna be obvious that everyone is must and a larger dollar amount in taxes their income goes up it's nothing down it's the percent of their income that you're spending that's what determines put the tax is regressive and what we see here is taxes. want to penalize people for being married does that make sense right I mean society should not encourage people to do just shack up right exactly make sense of maybe that for at least generally we've got what we're done matter so we know we don't want this that's not the answer is is that there is no answer you cannot devise but these are just kind of illustrates some of the problems that you have in developing the taxes do you want to make you wantto penalize marriage generally all right.

ROUGE-1: 11.52, ROUGE-2: 11.23, ROUGE-L: 11.46
BERTScore: 66.73

==============================================
==================== [47/100] ====================
Summary:
PhilipPE RIGOLLET: I want to have a small bias, hopefully a 0 bias. If this thing is 0, then we see that the estimator is unbiased. So am I planning-- yeah. So if I do, for example, X1, Xn, there are iid Bernoulli. And I'm going to write it theta so that we keep the same notation. Then theta hat is the average of Xi's. So what is the bias of this guy? Well, to know the bias, I just have to remove theta from the expectation. that we are going to be looking for in an estimator, trying to find them to be unbiased. But we'll see that it's actually maybe not enough. So unbiasedness should not be something you lose your sleep over. Something that's slightly better is the risk, really the quadratics risk, which is expectation of-- so if I have an estimators, I'm going to look at the expectation of theta hat n minus theta squared. And so for example, if the quadratic risk goes to 0, then that means that theta hats converges to theta in the L2 sense. for sum of independent random variables, now it's time to wake up. So we have the variance of something that looks like 1 over n, the sum from i equal 1 to n of Xi. So it's of the form variance of a constant times a random variable. But we know that the variance leaves on the square scale, so when I pull out a constant outside of the variance, it comes out with a square. We would like somehow to say that this is the sum of the random variables. Theta is the probability that I contains theta. You want 1 minus alpha to be very close to 1, because it's really telling you that whatever random variable I'm giving you, my error bars are actually covering the right theta and I want this to be true. So regardless of the value of theta that I'm getting, I want that the. probability that it contains the theta is actually larger than 1 minus. alpha. And so in particular, if it's equal, then I can put some larger than or equal to, which guarantees my asymptotic confidence level. what a confidence interval is. And so we fixed a statistical model for n observations, X1 to Xn. The parameter theta here is one-dimensional. Theta is a subset of the real line, and that's why I talk about intervals. A confidence interval of level 1 minus alpha-- so we refer to the quality of a confidence intervals is actually called it's level. The closer to 1 it is, the better the confidence interval. The goal is to estimate a true theta star, the one that generated some data. One of the properties that we wanted. Strongly consistent means that as n goes to infinity, it converges almost surely to the true parameter. That's the strong law of large number. It is consistent also, because it's strongly consistent, so it also converges in probability, which makes it consistent. We've actually computed its quadratic risk. We built a confidence interval at level 1 minus alpha. And we know that this is just the probability that the absolute value of sum not Slutsky, right? Two ways of getting rid of this. Since we only need this thing-- so this thing, as we said, is really equal. Every time I'm going to make this guy smaller and this guy larger, I'm only going to increase the probability. And so what we do is we actually just take the largest possible value for p1 minus p, which makes the interval as large as possible. And by Slutsky, we know that this is actually converging to be-- if I want this to hold for all possible A's, I have all possible events. about maximum likelihood estimation. If I give you a function, you need to know how to maximize this function. Sometimes, you have closed-form solutions. You can take the derivative and set it equal to 0 and solve it. But sometimes, you actually need to resort to algorithms to do that. And we'll briefly touch upon it, but this is definitely not the focus of this class. OK. So what are the properties? The KL divergence between P theta and P thena prime is different. In a way, what does it mean to have two distributions that are close? It means that when you compute probabilities on one distribution, you should have the same probability on the other distribution pretty much. So what we can do is say, well, now I have two candidate distributions. So if theta hat leads to a candidate distribution P theta. And this is the true theta star, it leads to the true distribution. According to which my data was drawn. That's my candidate. And so really what I want is that if I compute what it is doing for you. It's controlling the difference of probabilities you can compute on any event. The total variation distance is actually called the Kullback-Leibler divergence. It has some roots coming from information theory, which I will not delve into. The KL divergence between two probability measures is not going to be symmetric to start with. And I go from discrete to continuous using an integral. Everybody can read this. Everybody's fine with this. Is there any uncertainty about the actual definition here? So here I go straight to the definition, which is just plugging the functions into some integral and compute. So I don't bother with maxima or anything. The total variation distance-- the KL divergence, sorry, is actually an expectation of something. If this thing being small implied that P theta could be all over the place, that would not help very much. The divergence is doing a pretty good thing for us. And this is what will allow us to estimate it and basically overcome what we could not do with the total variation. The fact that you need two definitions of the [INAUDIBLE],, is it something obvious or is it complete? PHILIPPE RIGOLLET: I'll do it for you. PhilipPE RIGOLLET: The set A star is the set of X's such that f of X is larger than g of X. That's the set on which the difference is going to be positive or negative, he says. He shows that if he takes any other A in this integral than this guy A star, it's actually got to decrease its value. Rigollet: The first one has to be larger, because this thing is actually equal to a non-negative number. when I take A to be the set where it's positive. Just need to make sure that there is someplace where it is, but that's about it. So it's a distance. It's symmetric, non-negative, equal to 0, if and only if the two arguments are equal, then it satisfies the triangle compared to the convex function of the expectation of a random variable. If it's not satisfying this thing, it's called pseudo-distance or quasi- distance or just metric or nothing at all. But here, one of the arguments is not known to us, so we need to estimate it. And so here is the strategy. Just build an estimator of the total variation distance between P theta and P theTA star for all candidate theta, all possible theta in capital theta. Now, if this is a good estimate, then when I minimize it, I should get something that's close to P thena star. That's a pretty good estimation strategy. But it's very unclear how you would build this estimator. it is. And that's now where the log plays a role. If you actually pay attention, I said you can use Jensen to prove all this stuff. You could actually replace the log by any concave function. That would be f divergent. That's called an f divergence. But the log itself is a very, very specific property, which allows us to say that the log of the ratio is the ratio of the log. If I change theta, this thing is never going to change. It depends only on theta.

ROUGE-1: 19.77, ROUGE-2: 19.22, ROUGE-L: 16.31
BERTScore: 70.96

==============================================
==================== [48/100] ====================
Summary:
Markus Klute: This class will be taught in an inverted classroom or flipped classroom setting. The course evaluation or your evaluation in this course will be made up 50% out of homework. The great divide-- the great divide or the grade divide-- at 85% between A and B, 70% between B and C, 60% between C and D, and below 50% earns you an F. The grading scheme will not be worse than what I've given you here, Klute says.

ROUGE-1: 23.03, ROUGE-2: 21.64, ROUGE-L: 19.24
BERTScore: 55.97

==============================================
==================== [49/100] ====================
Summary:
In this problem, we are given a joint PDF for x and y. And then we are asked to compute the variance of x plus y. We're told we should compute this variance by using something called the law of total variance. So first, I want to focus on this term-- the conditional expectation of X plus y conditioned on x. So this is actually just x plus the expectation of y given x. And because it's uniformly distributed and because expectation acts like center of mass, we know that the expectation should be the midpoint.

ROUGE-1: 11.36, ROUGE-2: 11.14, ROUGE-L: 11.36
BERTScore: 69.68

==============================================
==================== [50/100] ====================
Summary:
In the second half of the course, we'll talk about multisequence alignment. This is a generalization of the two-dimensional array that we had before. As k grows, the space complexity grows by n to the k-th power. And we'll show a program called MSA, which is short for multispecies alignment, when we get into the transcript of the part of course that we will talk about later. And the two that we'll illustrate in the next couple of slides is a tree alignment, as illustrated by ClustalW. general is going to be 2 to the k minus 1 or about roughly 2 to k. And so the time complexity is have to do 2 to. the k comparisons per node. And the larger k is, the more you can explore. It's like doing a huge mutagenesis experiment and exploring viable mutants. So we pairwise alignments. And you get this 4 by 4 matrix. The best score is S1 with S3, which has a score of 9. And this is-- basically, we're starting to describe the method by which we construct a tree. best score for S1 with each of the others, and have S1 in red in each case, and use that as the anchor. And so then in the multialignment, you take all the indels relative to the red one, and introduce them so that it's the anchor, and so on. So those are two radically different ways. And we'll get to the Gibbs sampling later. The Gibbs sampling, just in a nutshell, is in general when you have a hard problem, where you can't comprehensively go through the entire space, what you do is sample it. cell type and the rare [INAUDIBLE],, rare messenger RNA within a cell type. Let's talk about the sizes of proteins. How is it that it precipitously drops off at 100 amino acids? Why are there so few proteins that are short? And there are slightly more short proteins in Mycoplasma? Any guesses why they're so few? Why does it drop off at100 amino acids?" STUDENT: There are more but we can't find them? GEORGE CHURCH: Right, there probably are. If you have an AT-rich genome, you're going to tend to have a lot of stop codons. You tend to run into a stop codon at random quite frequently. You need to have more codons in a row to the same motif, ATG. C would be rare because the Cs mutate into Us. You can have rare tetranucleotides if you, for example, have a methylase, the methylase is a pentanucleotide. row happen to be-- the next gene down is a histidine biosynthetic gene. And not only that, but about eight histidine genes in a row come after that. And the same thing with phenylalanines. This weird excess of tryptophan is upstream of tryPTophan biosynthesis genes. So what does this all mean? What it means, probably-- and there's actually quite a bit of experiments on this-- is that this is an excellent feedback loop, where you want to do feedback in the most relevant way. do that is now take this weight matrix, and ask for each-- we're scanning the genome, and we run into the sequence [? AAT ?] AATG. Now you want to know, how good a match is that to this weight Matrix, which was taken from either 4 sequences or 1,000 sequences? And the way you do it is for each position, you ask what was the score in the whole learning set? And now this should be a now independent test set you're trying this out on. Bayes' theorem says that the probability that the model given the sequence is equal to the probability of the sequence times the model divided by the sequence. Now let's see what all this Bayesian stuff is useful for. We're going to be doing-- of the various applications, we had recognition discrimination and database search. We'll have two models, a model that we actually have a hydrolase and the model that is randomness. We want to report all the sequences where the. probability that that sequence, given the model, is better than that sequence given a null model. George Church: The context for a dinucleotide is either an ocean or an island. Church: There are eight different states, and so we just divide 1 over 8 is a starting point, or 0.125. He says there are two possible places it can be, and they're equally probable. Church says the boundary occurs at this first CG din nucleotide. The next step is to find out if the dinucleotides can be in an ocean, an island or both. are 0s. 0s are a problem, both for the CG dinucleotide in the ocean and for the transitions between oceans and islands. The way you handle it is called pseudocounts. You basically say, what if we just missed finding that thing? We're going to add 1 to it because however big the counts are, you can always add 1. And so you can see. You can actually calculate these conditional probabilities by hand in the privacy of your home, not while the hordes are waiting to get into the room.

ROUGE-1: 21.08, ROUGE-2: 19.67, ROUGE-L: 18.57
BERTScore: 61.33

==============================================
==================== [51/100] ====================
Summary:
in this module of tools and equipment we're going to explore cookware and storage wear. There are five basic metals that are used in cookware aluminum copper stainless steel cast iron and carbon steel aluminum pots and pans are the ubiquitous pans that you'll find in most every kitchen. A rondo is an amalgam of several different metals cladware sometimes referred to as allcloud because of its brand name is using different layers in of metal including stainless steel copper and aluminum to provide the benefits of each one of those metals. Interchangeably in many ways but believe it or not though there is a difference between these two does it make a difference in your cooking if you use one instead of the other. A saute pan or satwa is one with the straight sides and has a larger surface area which makes it ideal for tasks like searing meat or reducing a pan sauce. A sawtooth or skillet has a slope side and is used mainly in sauteing the slope sides providing the ample and perfect angle for flipping your food. Ziploc bags are an indispensable storage device for kitchens they allow the storage of dry goods and other small quantity items they can even be used in sous-vide cooking. polycarbonate's trade name lexan or by the company trade name cambro consists of large food boxes they come in full and half sizes three and a half inch deep six inch deep and nine inch deep. The speed rack is essential for storage and movement within the restaurant they can fit full size and half size sheet pans readily. notice that this two inch hotel pin will fit directly into this and it'll fit snugly. This is because you'll have water underneath in your four inch hotel pan that will be simmering. You do this by using sternos and these sternos will go in those little indents on the bottom and you'll fit this whole thing into a stand. You also have perforated hotel pans which are basically hotel pans with holes in them. These can be used as steamers you can use them to drain things in a steam steamer setup.

ROUGE-1: 20.31, ROUGE-2: 19.50, ROUGE-L: 18.66
BERTScore: 58.60

==============================================
==================== [52/100] ====================
Summary:
Charles Brockden Brown: The Enlightenment is a critique of a kind of impoverished view of mind or consciousness that over-relies on reason. We started to look at it a little bit with Barlow's [assumed spelling] Raven poem with Poe. The idea that in fact there is something wrong with reason as a faculty, and this can take a couple of forms. It may be that there are real reasons that all of the vaunted ideas of The Enlightenment in fact co-exist with those things, tolerate them, maybe even require them. Franklin believes that you can identify error. He prizes appearance in a way that no good Calvinist would. He's stressing that he's doing something American. If you could read the little epigraph here it says from virtues blissful paths away, the double-tongued are sure to stray. Good is a forthright journey to still, and many paths but lead to ill. That's the way the S was originally printed at the beginning of a word. And yes, if you can actually see it, that is not an F, that's an old fashioned way of setting an S. book tells you it's set in Philadelphia in 1787, all right? The same time as the Constitution is being framed a hop, skip, and a jump away. There's a way in which we might say this is also if not a critique, then a cautionary tale you might say about the principles of enlightenment that are enshrined in The Constitution. A kind of meditation on what the dangers to the new nation might be at the moment that the new country is being formed. Now, Brown is an interesting character because he is one of the first US writers to try to take advantage of thefirst American copyright law which is passed in 1790. appears in 1798 and it's the first novel that he actually publishes, and he talked he echoes what he had said before there, and says that the novelist should be regarded as a moral painter. And he proposes, he says, to illustrate some important branches of the moral constitution of man. In Edgar Huntly there's another kind of scientific fact that is played with right? Sleepwalking. And this is Wieland: or, The Transformation, the title is still a very interesting book although if you read the subtext of the book, it suggests it's an account of American literature. In the early English epistolary novel there's the worry about what it is we're writing and reading. One solution early on was well, it would be realistic if these were letters that you were getting to read. But it's interesting that the novel, the Gothic novel itself as a form doesn't take off in 1764. It has to wait about 25 years in and around the French Revolution and then it really takes off when a woman named Ann Radcliffe writes her Gothic novels. Gothic becomes a pejorative term by this time that is, that takes on the medieval association and makes you think of you know it's so Gothic, it's medieval. Romantic writers are gonna mobilize the medieval and Gothic and dark over against The Enlightenment to show you what's wrong with The Enlightenment. The Castle of Otranto, this Gothic novel which seems like kind just too much of the actual supernatural. Classic Scooby-Doo is Ann Radcliffe. The ghosts are fake. like that. There appear to be manifestations of the supernatural. She often finds herself in the various kinds of settings crumbling castles, dungeons, graveyards, darkened churches. Usually she almost escapes her persecutors, then they catch her. They are trying to get her out of greed or lust or both. Usually there's a Theodore-like protector who's kind of chasing after trying to save the day. In the end, all of the ghosts and supernatural manifestations that have tyrannized her are shown to be fakes. It's a very popular formula and it continues to this present day. Brown: The story reinforces the novel's picture of identity as confusing and inconstant right? Think about the motif's in there. There's that strange co-partnership of being that Mrs. Loramer has with her brother Arthur Wyatt who was taken for dead, turns out to be alive. Clithero takes Clarice, who is Mrs. Lieramer's niece, and his fiancee to be Mrs. Lieramer and he almost kills her. He portrays Edgar here not as a redeeming confessor, but as an agent of perdition, of damnation. every eye was a loathsome and disgusting object. To every eye but a mothers. In vain did human feelings bid me recoil from this emblem of mortality. I was not insensible at that moment of the impulses of vengeance. But they were transient. I detested the sanguinary resolutions that I had once formed. Yet I was fearful of the effect of my hasty rage and dreaded an encounter in consequensive wish I must, I might rush into evils which no time could repair, nor penitence expiate. well as he sends Edgar out into the wilderness. But the sublime, if the wilderness is about the sublime for many romantic writers, for Edgar Huntly the wilderness becomes about something else altogether. Terror and disgust is what Agnes feels in The Monk. Ann Radcliffe makes a distinction between what she calls, let me see if I have it here, the distinction between terror and horror. So Monk Lewis' Gothic is horror. Her Gothic she would've offered was a form of terror, okay? There's grounds of hope for everybody, he says. But it also means destroying old systems of obligation. What if the total depravity of human beings is true, not because of something called the fall of human kind, but just because that's human nature? Then, we've got a big problem. All of this you might say is the context for Charles Brockden Brown's Edgar Huntly. Okay? Now, let's take a look at the first pages of this. This is the preface to the book. Be impossible to arm myself with firmness? If forbearance be the dictative wisdom can it be so deeply engraven on the mind as to defy all temptation, and be proof against the most abrupt surprise? My late experience has been of use to me. It has shown me my weakness and my strength. No caution indeed, he says, can hinder the experiment from being hazardous. Curiosity is vicious if undisciplined by reason and inconducive to benefit. Except that he's gonna be experimenting with a person who might well be a murderer. he descends into madness which is a state of sleepwalking in which Edgar finds him. So that you might say that Clithero's story in these 4 chapters act as a kind of narrative counterweight to the primacy of reason that Edgar is trying to say this larger story is about, is a dramatization of. On page 88, he describes himself as a dispassionate observer still. And later on, take a look at this on page 90, he still is unwilling to accept the evidence of what is in front of him. the hand and by which force could be exerted. Some spring therefore secretly existed which might forever elude the senses, but on which the hand by being moved over in all the directions might accidentally light. The process was effectual. A touch, casually applied at an angle drove back a bolt and a spring at the same time was sent in action by which the lid was raised above half an inch. No event could have been supposed more fortuitous, by chance than this. No measure that I could adopt enabled me to place the lid in the same situation in which I had found it.

ROUGE-1: 18.76, ROUGE-2: 18.09, ROUGE-L: 15.86
BERTScore: 62.81

==============================================
==================== [53/100] ====================
Summary:
Markus Klute: We're starting a new chapter in which we look at tests and implications of special relativity. One experimental test is stellar aberration, which we discussed can be explained by special relativity and by velocity addition. In all of this experimentation and experimental verification, it's important to understand the importance of uncertainties in the scientific process overall, Klute says. He says one needs to be open minded and scientifically open minded to study and grow to the question of whether or not Einstein's theory is correct.

ROUGE-1: 17.82, ROUGE-2: 14.03, ROUGE-L: 15.91
BERTScore: 60.23

==============================================
==================== [54/100] ====================
Summary:
In the colonial era, most American women of European descent lived lives much like those of their European counterparts: They were legally and socially subservient to men. Lower and working class women were actually more equal to men of their own classes, but only because they were, like, equally poor. As production shifted from homes to factories, it shifted away from women doing the producing. This led to the so-called “cult of domesticity,” which like most cults, I am opposed to. Many of the most famous advocates for legally prohibiting the sale of alcohol in the US were women. Women gave many temperance lectures featuring horror stories of men who, rather than seeking refuge from the harsh competition of the market economy, found solace at the bottom of a glass or at the end of a beer hose. The temperance movement made a huge difference in American life because eventually, male and female supporters of temperance realized that women would be a more powerful ally against alcohol if they could vote. in comments where you can also ask questions about today’s video that will be answered by our team of historians. Thanks for watching Crash Course and as we say in my hometown, don’t forget to be awesome. Back to the page you came from with this week's Crash Course video. Follow us on Facebook and Twitter @CrashCourse and @CNNOpinion. For more Crash Course videos, visit CNN.com/Crashcourse and follow us on Twitter @cnncrashcourse.

ROUGE-1: 15.90, ROUGE-2: 14.33, ROUGE-L: 14.46
BERTScore: 57.62

==============================================
==================== [55/100] ====================
Summary:
ae houseman 998-999 uh very intellectual individual um with these two authors today um we're really kind of bridging the gap between the victorian and the modern. We're really transitioning and that's why these are the last authors that we'll focus on in this time period some of their philosophy some of the writing styles and things that motivated them. Most of his poetry especially towards the end if you look on the right there the grief and poetry era you know especially the last of his poems. Houseman can tap into that pain and that darkness and help you know other people and and help spread emotion through and through. To an athlete dying young is an article that i'm going to have you take a look at later. Look at what he says about that individual and how they you know can celebrate and reflect on that individual. The time you won your town the race we cheered you through the marketplace man and boy stood cheering by and home we brought you shoulder high today the road all runners come shoulder high we bring you home and set you at your threshold down. where glory does not stay and early though the laurel grows it withers quicker than the rose eyes the shady knight has shut cannot see the record cut and silence sounds no worse than cheers after earth has stopped the ears now you will not swell the route of lads that wore their honors out runners whom renown outran and the name died before the man so set before its echoes fade the fleet foot on the sill of shade and hold to the low lintel up the still defended challenge cup and round that early laureled head will flock to gaze the strengthless dead and find unwithered on its curls. heart away give pearls away and rubies but keep your fancy free but i was one in twenty no use to talk to me. When i was 1 and 20 i heard him say again the heart out of the bosom was never given in vain just paid with size aplenty and sold for endless rue and i am two and twenty and oh tis truth is true okay the first few lines when i was 2 and 20 and i heard a wise man say give crowns and pounds and guineas but don't give your way. Don't fall in love you can spend money on them you can give them all this stuff but what ultimately don't you want to give them at this age? here in when i was 1 and 20 the kid kind of learns his learns his uh his issues and still 22. that's still pretty young to learn um to learn a life lesson. That's when i learned a lot about myself. That was when I was 1 to 20. I'm 22 now. I've still got a long way to go, but that's a good thing. I still have a lot to learn. I don't think i've learned all of my lessons yet, but I'm getting there.

ROUGE-1: 46.30, ROUGE-2: 42.04, ROUGE-L: 41.21
BERTScore: 64.14

==============================================
==================== [56/100] ====================
Summary:
In quantum mechanics, the spin of a particle with a vector is quantized, and in terms of its length and its components. You find that it's square root of f times s plus 1 in units of h-bar. The components, and along any axis, actually-- and in this case here, the d-axis-- have eigenvalues, and they are listed here. And we find that there is 2s plus 1 possible values, so I'll pick here, just arbitrarily, the z axis. physical state of particles, which axis are the right ones to choose-- or, sensible? There's no right and wrong in this discussion. Let me motivate this. If you look at the orbital momentum of a particle, that's given by r cross p, where p is the momentum vector of the particle. So now, if you're looking at the total momentum, we have to look add the angular momentum and the spin of the particles together. So this is a nice choice of coordinate system or of axis.

ROUGE-1: 38.96, ROUGE-2: 38.60, ROUGE-L: 38.96
BERTScore: 77.91

==============================================
==================== [57/100] ====================
Summary:
In the 5th Century BC Athens was a direct democracy that encouraged wide participation through the principle of ho boulomenos, or anyone who wishes. This meant that any of its approximately 30,000 eligible citizens could attend the ecclesia, a general assembly meeting several times a month. The Athenian system also relied on a 500 member governing council called the Boule, to set the agenda and evaluate proposals. Some ancient philosophers, including Plato, disparaged this form of democracy as being anarchic and run by fools. poll, all examples of how the democratic principle behind sortition still survives today. Polls show that the principle of sortition is still very much alive and well in the United States today. The majority of Americans support sortition, with the majority of people in the U.S. voting in favor of it by a wide margin in the last presidential election in 2008. The most popular type of vote in the 2008 election was for the Democratic Party, followed by the Republican Party and the Libertarian Party. The Democratic Party won the popular vote.

ROUGE-1: 36.82, ROUGE-2: 24.94, ROUGE-L: 25.12
BERTScore: 62.20

==============================================
==================== [58/100] ====================
Summary:
Last time, we covered Fourier series and the Fourier transform. Today, we're going to talk about the convolution theorem, noise and filtering Shannon-Nyquist sampling theorem as a function of time. All right, so that was just a brief review of what we covered last time. Now, let's take a look at another function that we've been talking about, a square wave. In this which is 2 bels, which is 20 decibels, we can see a series of peaks because it's a periodic signal. and spectral estimation. And next time, we're going to move on to spectrograms and an important idea of windowing and tapering, time bandwidth product, and some more advanced filtering methods. And we'll end up on the Shannon-Nyquist theorem and zero padding. And there may be, if there's time at the end, I'll talk about a little trick for removing the line noise from signals. All right, let me point out why spectral estimation is very powerful. So remember, we talked about how you can see, if you have noise, the power spectrum of the original signal. we plot power in log base 10. A difference of an order of magnitude in two peaks corresponds to a unit called a bel, b-e-l. Deci just means a tenth of, right? Remember those units? A factor of 10 in signal is a factor of 100 in power, in the middle. It's just imagine that you have a sine wave that gets smaller as you go away from the origin by an amount 1 over f. That's all it is. Fourier transform just stretches out. If you make the square pulse smaller, the sinc function gets broader. If I make that Gaussian pulses in time narrower, then the Gaussian in frequency gets wider. And inversely, if I make the pulse in time wider, than the Gaussian in frequency space gets narrower. This is where the Heisenberg uncertainty principle comes from, because wave functions are just-- you can think of wave functions as just functions in time. All right, so it's a very important theorem. Basically anybody's acquiring to know the Shannon-Nyquist theorem. Convolution theorem says Fourier transform of y is product of Fourier transforms of g and x. We're going to talk now about how you do filtering in the frequency domain. The kernel for a low-pass filter is a delta function that reproduces the function of a high-pass filtered version of the signal. And then we'll talk about how to filter a signal with a linear kernel in the high-frequency domain. We'll end the show with a question-and-answer segment. wanted to show you what the autocorrelation function of this looks like. So if you look at the distribution of all the samples, it just gives you a distribution that it has the shape of a Gaussian. And the standard deviation of that Gaussian is 1. Now, what if you plot the correlation between the value of value of this function at time t and time t plus 1? So they're completely uncorrelated with each other. So there's zero correlations between neighboring samples. into short pieces. extracting that little piece of signal from this longer signal is essentially the same as multiplying that long signal by a square pulse. So that process of taking a long signal and extracting out one piece of it has a name. It's called windowing. And you compute the power spectrum in each one of those windows. And again, you average them together. So using these methods, you can pull tiny signals out of noise at a very bad signal to noise ratio, where the signal is really buried in the noise. Filtering in the frequency domain means multiplying the power spectrum of your signal by a function that's low at high frequencies and big at low frequencies. So convolving our original blue signal with this green Gaussian kernel smooths the signal. It gets rid of high frequencies. Any questions about that? Well, yes-- AUDIENCE: So why is it that like-- you need to like-- of I guess when you filter a signal, either high pass or of that Gaussian? It's just another Gaussian. Gaussian, you're not adding some of the signal here that were over here. Convolving with a sinc function kind of mixes things in time. So normally you would smooth by functions that are kind of local in time, local in frequency, but not having sharp edges. So we're going to talk about how to smooth things in frequency with signals with kernels that are optimal for that job. That's Thursday. What would a high-pass filter look like in the frequency domain? signal has some bandwidth B that in order to sample that signal properly, your sampling rate needs to be greater than twice that bandwidth, 1, 2. There was recently a paper where somebody claimed to be able to get around this limit. And they were mercilessly treated in the responses to that paper. Now that's an amazing claim. Right? You have a [AUDIO OUT] time. All right, it's wiggling around. What this is saying is that I can completely ignore what's happening between those samples and make it a longer vector.

ROUGE-1: 17.62, ROUGE-2: 16.66, ROUGE-L: 14.45
BERTScore: 65.96

==============================================
==================== [59/100] ====================
Summary:
The science of classical mechanics establishes an important principle of cause and effect. Newton's Laws of Motion established the scientific principle of analyzing observed phenomenon through the use of clearly articulated mathematical models rather than through intuition. Developing a command of mechanics is a powerful tool for understanding the world around us. 8.01 assumes a strong background in high school level physics and mathematics, so a previous course in calculus is not a prerequisite. It is a rigorous and technically challenging course aimed at MIT undergraduates. In each of these lessons, you will find a series of short lightboard videos that will help you understand concepts.

ROUGE-1: 49.32, ROUGE-2: 46.33, ROUGE-L: 44.75
BERTScore: 74.77

==============================================
==================== [60/100] ====================
Summary:
In this video, we'll look at an application of the probabilistic method to graph theory. An independent set in a graph is a subset of vertices with no two adjacent. Caro-Wei's theorem says that every graph G contains a large independent set of size at least the following quantity: summing over all v among vertices G, 1 over the degree of v plus 1. We'll see an application and some ways to interpret this result in graph theory as well as the corollary of Turan's theorem. case when n is divisible by r, you need to do a similar construction, turns out to be best possible, and this bound can be improved slightly, but not by too much. Let us take the n vertices and split them into equal parts. And there are r parts. So here in this illustration r equals to 3. And let's put in all the edges between parts, but no edge within a part. So this is called the complete r partite graph. So you see that this graph here, one can do a quick calculation to see that it has exactly this number, 1 minus 1 over r times n squared over 2 edges.

ROUGE-1: 26.34, ROUGE-2: 25.22, ROUGE-L: 25.96
BERTScore: 67.42

==============================================
==================== [61/100] ====================
Summary:
Professor: The games that we'll end up playing today are games from last year. A lot of them are actually the final semester-- the final project. Professor: I want you to keep in mind that you're not constrained to building games that look exactly like those. "You when you actually play someone else's game, then you're the most important person," he says. "If that person's experience is problematic or exciting or engaged or outright hostile to other players, that's making it the way that they want to take it" I want to-- I try to cover quite a bit of it from the first reading during class itself. Was anyone here not here on Wednesday? OK, all right. So we need to make sure that your name's on the attendance sheet. And you should come and talk to me after class. And we'll make sure you get a copy of the syllabus and everything like that. Make sure that you get expectations for this class. So I hope you got-- I got it allright. If you don't let the player know what changed the game state, is it that meaningful a decision anymore? If you expect players to get very into your game, this is the stuff that you can leave with because they will understand it and figure it out on their own. There's a long term consequences, so a lot of immediate feedback, which is a term that you brought up. In Dominion, oftentimes you can draw cards that don't help you just pointlessly causing a reshuffle. Candy Land was invented to keep kids from getting polio from each other, professor says. In Mafia, all the information you're getting in the game is through other people in Mafia. In Battleship, there's again the hidden-- information that's hidden from you, but it's completely available to your opponent and vice versa. Charades and Pictionary-- usually your teammates are the ones who are guessing in Charades. In Village, we sort of did stuff, but we had-- and then afterwards, we didn't really understand. say, well, what if I tried this decision? And then there's an interesting phenomenon that goes with that in a lot of strategy games called save scrubbing. And that is where you know that the outcome of something is based on a probability. And so if you save and reload often enough, you will always be successful. But for a game whose random number seed has been saved and you fail and then you reload from the start, it means you'll always fail because that's a different concept of how the random number generator works. Don Norman's first chapter in the design of everyday things. He talks about materials like wood and glass, right? Glass is for looking through. Wood is for holding things together. And in games-- let me just bring in an example. Let's try to identify the components of-- how many of you have played put before? Really, really-- this is the box. And we're going to do a little bit more detail in about two weeks when we revisit the idea of user design. in a visual game, that's telling me visibly, right now, that this might be the way I get to do that. In a lot of strategy games, some of these things are very literal, right? So and so technology gives you this bonus. But visibility has something to do with the intent of the player of the user in Norman's case. But he's not talking about games. He's talking about the design of everything. And what the system can actually do, the actual operations of the system. what you can see of the system. So there are affordances and there are constraints. These are both words that are introduced in that reading. I think affordances is introduced in this reading. What's an example of an affordance? AUDIENCE: If you can sit on a chair? PROFESSOR: What about this thing tells you that you could sit on it? AUDience: It seems sturdy, and it's got a place for your butt. PROFessor: Yeah, it's not made of spikes. It's got at least three legs, which may help, and evenly distributed, which means that it'sNot going to tip over. face on it, which I always thought was a little bit strange about-- that might have been designed, too, actually. That might have something because if you-- this won't kill you, really. So you should put it in your home. I want to find out more about the history of the Edison plug. AUDIENCE: It's actually-- the design with the ground on the bottom is a bad idea because if something starts falling out that are too exposed to it, it will not be the ground one. Carcassonne is a game about making large patches of land. You don't know what tile you're going to draw when you start playing the game. The tiles are identical with a back, and that tells us about these tiles, something that we already know about cards. The fact that you've got a hidden back gives you quite a lot of different possibilities in the game, says Professor Peter Schmitz. The game is about mapping, so back to the idea of mapping, he says. a whole bunch of ways that you can help people with these sorts of mappings. I guess Donald Norman would describe a lot of these as spatial, metaphors to use. Spatial mostly to describe things like driving in a car and you turn the wheel to the left specifically toward the top of the wheel. And you car it's directed to turn left, that sort of thing. There are sort of bodily metaphors as well. Things that are high are either supposed to be good and happy, things that are low when you're feeling depressed. deal and trade your cards to corner the market, et cetera. When it comes to Carcassonne, there is actually a deep, deep problem with this game despite how popular it is. And that scoring is actually pretty difficult to do. It's a math intensive problem. It does largely map on to how many of the meeples that to get all the CarcASSonne bits back. So and then we'll pick this up in about five minutes. We'll be back with the next episode in about 30 minutes. be good feedback on whether your clues are getting across to your teammate. But your opponents are also usually some sort of feedback mechanism that's keeping time. If they hear you say something, they go ah, ah,. ah. And are there a few more hands or something like that? So just always remember that you can employ other players into your feedback mechanism. It doesn't always have to be your game alone. It's a game I played a long time ago called Scotland Yard. The player positions is almost never known. The detectives are working on partial information to try to corner and close this dragnet. what he's talking about only, I doubt that actually imagined that this was something to be possible at the time when he wrote it. That was 1980. And cell phones obviously have gone through a lot of criticism. But it is it has been successful through a number of different reasons. Don't discount marketing as being a big part of it. It's expensive for what it does. But then, arguably, by locking out a many things that you might want to do but maybe don't have to do, they are trying to make it easier for you not to do the wrong thing.

ROUGE-1: 22.38, ROUGE-2: 21.15, ROUGE-L: 19.15
BERTScore: 62.34

==============================================
==================== [62/100] ====================
Summary:
Learn how to use the t matrices, or the t dagger matrix, to solve quantum mechanics problems. Use the Schrodinger equation to solve for the energy levels, and we can solve for basically all of the eigenvalues and eigenvectors. Learn how to solve time independent quantum mechanics and a lot of problems in time dependent quantum mechanics. Learn about non-degenerate perturbation theory to solve an n-level problem, and that's the subject of this lecture and the next. The energy levels and properties of the Hamiltonian. We're not allowed to determine the wave function by any experiment. But we are able to observe the energy levels. The molecule more or less tells you how to focus on the part that you care about and to get rid of the stuff that is of no trouble whatsoever. This is an amazingly powerful. tool that you can use to solve, basically, any problem involving molecules with a potential like a harmonic oscillator at the bottom. But it's usable for all problems, but there's a different basis set. The columns of t dagger are the eigenvectors. They are linear combination of the basis vectors that correspond to each eigenvector. And so it's possible to show-- and I don't want to do it. I did it last time. You can show that this transformation t dagger c equals c. Do I want?-- above, above. This is telling you that the columns of T dagger are. the eigenevectors of a particular eigenvalue. It's really quite simple. But it's just this extra notation. to know J, CJ eigenstates. Once you have this, then you know how to write the time dependent wave function. And then there's the origin of life. You need two particles to come together and start to condense into a liquid. That's the beginning. Perturbation theory explains the long range interactions by which all gas phase particles attract each other weakly. So that's important too. And so you'll be able to do all of this stuff. So here we have non-degenerate perturbation Theory. And it is important too, because it is just mechanical and boring. we do is now we write the full equation and we sort it into sub equations corresponding to powers of lambda. So the lambda to the 0 equation is really easy. It's just H0 psi 0 is equal to E0 psi0. We could put n's on this. And this is the exactly solved problem. It says, yeah, you build your foundation from the Lambda to 0 equation, and it's just what you know already. And what you're going to do is use the psi n 0 and en 0 to do everything else. level V1 minus 1 V2 plus 2 V3. Because of the energy denominators, these two guys are nearly degenerate. One gets shifted up, one gets shifted down a little bit. And it might also be that this state is what we call bright and this is called dark. This is called a spectroscopic perturbation. And so you can learn about some of these coupling terms because, instead of hiding in the forest of these small corrections, you get a big effect. says, well, tough luck. You can't do spectroscopy in emission. But you can still see the absorption spectrum because then your signal is the removal of photons from your beam. So there's a huge amount of photochemistry and interesting stuff connected with a large density of states. And again, when I was a graduate student, there was a huge controversy about non-radiative transitions in medium sized molecules. And that got resolved by two gentlemen called Bixon and Jortner.

ROUGE-1: 19.68, ROUGE-2: 18.70, ROUGE-L: 16.79
BERTScore: 60.35

==============================================
==================== [63/100] ====================
Summary:
The key word is feasibility technology. It represents the feasibilities that the combination of inputs and outputs, that can be achieved in this world even the current level of technology. The second is no free lunch. At least 1 input is required to produce some output, or more than you know more than 1 kind of output. Third is non reversibility, what it means is that a production process cannot be reversed. So, if you obtain half kg of curd from 1 kg of milk, you cannot obtain 1kg of milk from half kg. To produce 1 kg of rice ok, you need either let us say 100 grams of fertilizer or 50 liters of water. We need land, but that is fixed those are fixed, we are not talking about it only these 2 are variables. On average, what we can do is that we can produce 1kg of rice here, using t by 100 and 1 minus t by100, 2 comma 1. So, on average we are saying this is what it is not always true, but I am saying on average. same proportion, then output will also decrease in the same proportion and this is possible this is feasible fine. The easier is easier way easier although they these two are not same, but the similar 1 implies the other, but not the other way around. The combination is all given here and this exhibits convexity. Here we are not concerned about the amount of output, what we feasible here, y is taking care of not only inputs but also output. It is also possible to combine lambda x 1 to lambda x 2 of course.

ROUGE-1: 17.52, ROUGE-2: 16.20, ROUGE-L: 15.68
BERTScore: 65.65

==============================================
==================== [64/100] ====================
Summary:
The Great Wall began as multiple walls of rammed earth built by individual feudal states during the Chunqiu period to protect against nomadic raiders north of China. Under the Han Dynasty, the wall grew longer still, reaching 3700 miles, and spanning from Dunhuang to the Bohai Sea. After the Ming dynasty gained control in 1368, they began to refortify and further consolidate the wall using bricks and stones from local kilns. The wall was formidable but not invincible. Both Genghis and his son Khublai Khan managed to surmount the wall during the Mongol invasion of the 13th Century. main body and expanding this remarkable monument to human achievement. Main body is made up of three parts: the head, the torso, and the legs. The main body of the main body is the most important part of the body. The second part is the lower body, which includes the legs, the arms, the legs and the feet. The third part is made of the lower torso, which is the largest body of its kind. The last part is called the lower legs, which are made of marble and marble.

ROUGE-1: 36.14, ROUGE-2: 27.54, ROUGE-L: 26.49
BERTScore: 66.09

==============================================
==================== [65/100] ====================
Summary:
Peter Solovits: I got a call from a committee of the National Academy of Science, Engineering, and Medicine. The committee is chaired by David Baltimore, who used to be an MIT professor until he went and became president of Caltech. Solovitz: They convened a meeting to talk about the set of topics that I've listed here. He says the group of us that talked about AI and decision making, I was a little bit surprised by the focus because Hank really is a law school professor. Trevor Noah: Algorithmic technologies may minimize harms that are the products of human judgment. Noah: We know that people are in fact prejudiced, and so there are prejudices by judges and by juries that play into the decisions made in the legal system. He says we should look for people with broad educations, like Trevor's father is white and his mother is African-American and his father is a Swiss guy. Trevor: What do you like about being able to defend a marriage that they were not allowed to marry six years ago? In California, the decision of whether you get bail or not is going to be made by a computer algorithm, not by a human being. The critique of these bail algorithms is based on a number of different factors. The data collection system is flawed in the same way as the judicial system itself, Peter says. Irene: If we're going to define the concept, what is fair? What characteristics would you like to have an algorithm have that judges you for some particular purpose? Yeah? When I was an undergraduate at Caltech, the Caltech faculty decided that they wanted to include student members of all the faculty committees. In those days, Caltech only took about 220, 230 students a year. So one day, one of the professors said here's what we ought to do. We ought to take the 230 people that we've just offered admission to and we should reject them all and take the next 230 people, and then see whether the faculty notices.look like they're a better bet. Peter Zolovits: People who are similar should be treated similarly. He says it's hard to define a universal notion of what it means to discriminate. Disparate treatment is about procedural fairness and equality, he says. Zolvits: There's plenty of evidence in literature that keeps persisting discrimination that keeps a lot of people out of the job. The question is can we change our hiring policies or whatever policies we're using in order to achieve the same goals, but with less of a disparity in the impact? In the US, there were tests of this sort done, but the problem was that a lot of African and African-American populations turned out to have this genetic variant frequently without developing this terrible disease. And it was only after years when people noticed that these people who were supposed to die genetically weren't dying that they said, maybe we misunderstood something. And what they misunderstood was that the population that was used to develop the model was a European ancestry population and not an African ancestry population. So you go, well, we must have learned that lesson. The study looked at how machine learning models can identify disparities in general medical and mental health. It found a racial bias in the data that we have and in the models that we're building. In psychiatry, when you look at the comparison populations, you see a fair amount of overlap. The models are not going to give us as accurate predictions, but you still see, still, a huge gap in the confidence intervals between them. The study was published in the American Medical Association's Journal of Ethics, which I didn't know existed. is choose some family of models to try to fit, and then I'm going to use some fitting technique, like stochastic gradient descent or something, that will find maybe a global optimum, but maybe not. And then there is noise. And so his observation is that if you count O as the optimal possible model over all possible model families, then the bias is essentially O minus L. The variance is like L minus A, it's the error that's due to the particular way in which you learned things. He had to pretend to be-- his mother was his caretaker rather than his mother in order to be able to go out in public, because otherwise, they would get arrested. So here are some of the legally recognized protected classes, race, color, sex, religion, national origin, citizenship, age, pregnancy, familial status, disability, veteran status, and more recently, sexual orientation. So if you want to create a stereotype, men are druggies and women are depressed, according to this data. scoring function is independent of the protected attribute. So that allows a little more wiggle room because it says that the protected. attribute can still predict something about the outcome, it's just that. you can't use it in the scoring function given the category of which. outcome category that individual belongs to. And then sufficiency is the inverse of that. It says that given the scoring. function, the outcome isIndependent of theprotected attribute. And so that says, can we build a fair scoring function that separates the outcome from the. protected attribute? Hiring is based on a good score in group A, but random in B? So for example, what if we know a lot more information about group A than we do about group B? Well, the outcomes are likely to be better for a group A rather than for group B. So it's a kind of nice graphical hack. Again, it'll be on the slides, and I urge you to check that out, but I'm not going to have time to go into it. does in other populations, and the FDA has actually approved the marketing of that drug to those subpopulations. And if you think about the personalized medicine idea, which we've talked about earlier, the populations that we're interested in becomes smaller and smaller until it may just be you. And so there might be a drug that works for you and not for anybody else in the class. But it's exactly the right drug for you, and we may get to the point where that will happen and where we can build such drugs. degree of calibration will give you a good approximation to this notion of sufficiency. These guys in the tutorial also point out that some data sets actually lead to good calibration without even trying very hard. So for example, this is the UCI census data set, and it's a binary prediction of whether somebody makes more than $50,000 a year if you have any income at all and if you're over 16 years old. It's almost exactly along the 45 degree line without having done anything particularly dramatic in order to achieve that. The probability that you visited the Grace Hopper Conference is dependent on your gender. Computer scientists are much more likely to be programmers than non-computer science majors. The optimal score is going to depend basically on whether you have a computer science degree or not. If you're a historian, you're not likely to have been interested in going to that conference. It's a really cool conference. Grace Murray Hopper invented the notion bug or the term bug and was a really famous computer scientist starting back in the 1940s. We used LDA, standard topic modeling framework. White patients have more topics that are enriched for anxiety and chronic pain. Black, Hispanic, and Asian patients had higher topic enrichment for psychosis. It's interesting. Male patients had more substance abuse problems. And so we said, what happens when you look at the different topics, how often the different topic arise in different subpopulations? And so what we found is that, for example, white patients haveMore topics enriched for Anxiety and Chronic pain. Public insurance patients often have multiple chronic conditions. Public insurance patients have atrial fibrillation, pacemakers, dialysis. Private insurance patients has higher topic enrichment values for fractures. The error rates on a zero-one loss metric are much lower for men than work here and embarrassing myself. So this is modeling mistrust in end-of-life care, and it's based on Willie's master's thesis and on some papers that came as a result of that. It could be any of a lot of different factors, but that's the case. or social differences, but to a difference in the degree of trust between the patient and their doctors? It's an interesting idea. And of course, I wouldn't be telling you about this if the answer were no. So there are red flags if you read the notes. For example, if a patient leaves the hospital against medical advice, that is a pretty good indication that they don't trust the medical system. If a person is in pain, that correlated with these mistrust measures as well.

ROUGE-1: 26.44, ROUGE-2: 24.62, ROUGE-L: 21.55
BERTScore: 61.72

==============================================
==================== [66/100] ====================
Summary:
Professor: Today we're going to talk about partial differential equations. Next time I'll be lecturing to you will be a week from Friday. homework involves a COMSOL problem which might take some extra time, since it's the first time you're doing it. There's no lecture on Monday, but you have the quiz, too, Monday night. Wednesday, next week is Veterans Day so it's a holiday. There're no classes at MIT. The homework following the quiz will be posted and so you can get started on that. an equation like this, is that whatever partials you see-- When I have this and it has a term that's like d squared, dx squared, is one of the terms in this thing. This says t, this says x. The convention is, oh boy, I better hold x fixed. And here must be, I must hold t fixed. Because there's another one in the same equation. Now, you can change coordinates however you want. Just really watch out that you carefully follow the rules about what you hold fixed. Otherwise you can cause all kinds of craziness. methods. So all those methods basically look the same. You just get equations that have more unknowns in them. And so fundamentally, there's no problem, but there's a big problem if the number of unknowns gets to be so large. 10,000 unknowns are a lot harder to solve for than 100 unknowns. And once you get up to a million unknowns, we're starting to talk serious math, now. And then in reality, we have problems like the Navier-Stokes equation. A lot of combustion problems are very sensitive to the physical initial conditions. A very small spark can make a really big difference in a system like that. Another class of PDEs where it's stable in some sense, but it causes a problem. A lot of these things, light a spark on a stove, it ends up basically how you light it. So I do a lot of work on combustion problems. So that's one idea about what to do for initial guesses. Another idea is the time marching idea. If it does, then this is a good idea. a flow in a pipe. And you have a nice, fast velocity flow down the pipe. What's happening downwind is very much affected by what happened upwind, but not vice versa. So it has a directionality, even though I might do it as a steady state problem and not have time in it at all. And so if you look at figure 6.8 in the textbook, you see you get perfectly stable solutions when you do that. But where you do this one, you get crazy, oscillatory, unphysical solutions. If you look in the textbook at figures 6.7, 6.8, it's kind of a very famous problem. If you make the local Peclet number too large, actually anywhere bigger than 2, and you try to solve this equation, what you get is oscillations, unphysical oscillations. It's crazy. It looks like the simplest equation in the world, right? It's a linear equation, so what's the problem with this? But it doesn't work. And so, then you could think, well, why doesn't it work? And there's, like, multiple ways to explain this. a diffusive flux coming in here. So basically, it's coming up and it's making a right-hand turn. And so then I could think, well, if I was going to try to figure out, what's the steady state concentration of drug in there, I could say there's a certain amount of drug entering. And it's probably, my units are screwed up so there's-- Well, maybe not. You guys will get it right. It's probably an area. 16th numbers in this matrix are zero. So we don't have to represent them. But there still might be quite a few. Probably the diagonal events are non-zero, so that's like 10 to the 8th of them right there. So you try and vary and find the delta y that makes this go to zero by minimizing it. And then this method, it turns out that the conjugate gradient, if everything works great, this is guaranteed in n iterations to go directly to the solution. iterations like this, you tend to pick up numerical problems. And so, they've worked out better methods. And there's one that people use a lot for these problems called bi cg stab. Which is biconjugate gradient stabilized. And this is the one that, I think currently, people think is the best. Though probably, there's a lot of research on this, so maybe there's even better ones now. But inside Matlab,. I think this isthe best on they have. set up is right, so you can use it. But if you can, it can be pretty good. All right. See you on Friday. Back to Mail Online home. back to the page you came from. Click here for the latest from CNN.com. Back To the pageYou came from the page You came from: Back to the Page You Came From: Back To The Page You Were Originally From: The page you were originally from: The Page you were initially from: the Page you started from.

ROUGE-1: 18.92, ROUGE-2: 17.54, ROUGE-L: 16.42
BERTScore: 65.38

==============================================
==================== [67/100] ====================
Summary:
elizabeth the first is considered one of the greatest rulers in england's history if not the greatest for women. She never married can you blame her i mean look at what she grew up with around her dad and all of those moms and stepmoms she never had any kids which that causes a problem come her end time okay where we have i don't have an heir so she eventually does set on a cousin up in scotland which is the country on the same island of england. She names him as the heir so when elizabeth died king james of scotlands comes down and he is now the king of englands and the king  of scotland it's like he united two kingdoms. know how to read latin or french you were stuck by solely getting your material this is what the bible says solely from your clergy solely from the church so you were kind of a mindless robot with regards to comprehending the the word of god or whatever the bible said on your own. Now that something is in english and if you can read english you are able to read it over and over get your own theories your own arguments and make your own decisions. So you were less mindless page 414 is a really nice page that if you need to come back to review the significance of this or you can't remember um it's pretty good you can see down at the bottom.

ROUGE-1: 55.53, ROUGE-2: 53.91, ROUGE-L: 55.53
BERTScore: 78.48

==============================================
==================== [68/100] ====================
Summary:
The average velocity depends on the time interval t to t plus delta t while the person has displaced a certain amount of vector delta r. So, as a vector, we have delta x over delta t i hat. This component here is what we call the component of the average velocity. And, again as before, this component can be positive, zero, or negative depending on the sine of delta x. And now what we want to do is consider what happens in the limit as delta t becomes smaller and smaller. And that will enable us to introduce our concept of instantaneous velocity.

ROUGE-1: 55.17, ROUGE-2: 53.33, ROUGE-L: 55.17
BERTScore: 82.08

==============================================
==================== [69/100] ====================
Summary:
This week we'll look at the super magic formula. The formula is a compaction of a lot of math. We'll use it to derive a general formula for something moving on something that's moving. The class will also work on the spider problem and the Frisbee problem. The course is free and open to the public. Use the weekly Newsquiz to test your knowledge of stories you saw on MIT Open Courseware. For more information about MIT open courseware, visit MIT opencourseware at ocw.mit.edu.  angular velocity is a kind of a madeup concept that just simplifies the math and it's kind of intuitive stuff is rotating Theta Dot multiplied by you know the Le lever arm right gives you a velocity but you can see how it kind of slaughtered into place. angles don't add up right if I change the order in which I add angles they don'tAdd up right so if I said 45° up and I'm going to trade 45° my arms up here right why is the why is this angle adding up here a has something changed over the weekend if I turn 45° anyway you know. Andrew Keen explains how angles don't add up but infinite decimal angles do add up. He says that if I just took Theta as a Theta with this vector and I said apply these two rotations well depending on the sequence in which you added the rotation vectors up you end up with a different position right but vectors don't have the problem when you add two vectors it doesn't matter what sequence you add them in right if I have two vectors whether I do this or whether I write this first and do this the net Vector is the same you get it. The final you know in terms of that initial frame get it and W so we introduced the the velocity and then the next thing we did last last in the last class was I revealed to you this magic formula I said if you have a vector let's call it n and you want to take its derivative with respect to the frame a now if you the way we did it was we would rewrite in in terms. of the basis vectors of frame a right interms of A1 A2 a lot of math lot of derivatives. but you need this correction term which is very beautiful done done okay so this was the magic formula that I introduced in the last class how would we go about proving this anybody yeah do the math how wouldWe do theMath how would We prove this here's what we'll do let's prove it let's consider a frame a by the way you notice I will always show frames with this little circle a saying that it's a frame and with a little wiggly line pointing to it I willAlways do that if I don't stop me and make sure I do it okay. here so will you please expand it for me what does that come out to be this is the left hand side of that formula right and we'll do the right hand side separately and show that the two are the same I'm trying to prove it to you right so I'm not you know I'm doing just very boring stuff but we need to do it just establish it what is d by a d by DT of U1 cosine Theta it is U1 do cosine theta right minus U Theta do sin Theta okay. let's do that so let's imagine I'm spinning a basketball on my finger I could never do that but I'm doing it right. The room the world the Earth is a frame the basketball is another frame the kind of the U you know you guys need to be like uh Neo in Matrix have you seen that movie right it's very very important if you understand Matrix algebra you need to see Matrix all right so go see Matrix and you know how Neil kind of you know he when he's totally you know after Lawrence fishburns you know kind of trained him Etc he goes down and he can see. yeah okay using um frames uh using angle of velocity so the problem was we have the ground, the spider and the Frisbee. We characterize this situation with three parameters okay and the parameters are the positions of the center of the frisbe right and the distance of the spider. How many degrees of freedom does an insect have on a plane treat is a point okay it's a treat like a DOT it's only two right but yet we characterize this situations with three. parameters. can kind of do it that way but the simple way to do it is to simp well you know it doesn't matter we can just write this as B take the whole term d by DT of lb1 plus what quick hm I'm hearing a hesitant murmur not the yeah I was mumbling something a moment ago I'll tell you what I was trying to say uh cross lb1 what is a Omega B so we can rewrite a Omega b as Theta dot B3 and so this whole thing comes down to I'm going to write the answer here to save a Blackboard. you'll notice with coris these two terms always show up in different places and they add up just and it becomes two right. If you as I said if one of you invents a new term we'll name it after you okay and and so far I've used these words coris and all that I kind of called them out without telling you a lot more about them right so what we're going to do now is take a break in the following way I'm going to show you some videos showing you the coris force when I'm when you're done with these videos um you will a totally understand what the cholis not effect sorry not force cholis acceleration is. The Coriolis effect is what happens when things want to go straight but the kid in the m go around sees something different G volume turned up to an observer above the merry go around. The path of the ball appears kind of Swing Away get it and what happens with things like hurricanes and we'll show you I'll show some videos later if we have time is so now you're you're on the North Pole right you're sitting in the North pole and this is the Earth this is North Pole okay you're looking down a particle when it's let's say there's a low pressure. direction in which the vortex takes place okay the vortex is created far more by asymmetries in the sink or by rotation that was created earlier that can of remained right okay the angle of momentum which we'll cover later on the class it stays for a long time and it influences things to go in one way or the other. So it sinks you in fact you can construct a really large sink and you know apparently you can show it but it's very hard to do it it's hard to kind of isolate environmental factors so sinks not a valid example hurricanes avalid example. that wasn't the reason the corus effect was discovered so I take it back right but there it is also true that in the in World War I when the British when a British Fleet near Faulkland ran into a German Fleet they spend they shot 1,000 shells uh taking the Coriolis effect of the northern hemisphere well they were in the Southern Hemisphere and missed and then 60 shells um they actually use the right correction and they nailed the German Fleet and they won that battle so it's certainly true that by this you know by by the time World War II came along the range of these guns was high enough that the coriolus effect is relevant.

ROUGE-1: 24.20, ROUGE-2: 22.97, ROUGE-L: 22.69
BERTScore: 63.84

==============================================
==================== [70/100] ====================
Summary:
Al shalot is the CEO of GT Sports. He is also a member of the Esports commission the international Olympic Comm. He has been advising in the background for a while now. He says the turning point for him are the Asian Games. The Olympics need to build the connection with this Olympic movement and the youth and we need to create something special to find the balance between the historical games and the new games, he says. The Olympic Games will be a dedicated event to celebrate Gamers and eorts F. moving around very much they're going to be in one location can you explain why Saudi Arabia as the host I'm not sure that from from what I heard um this is a strategic partnership with a with with a host that has a lot of capabilities. I think 70% of their constituents are passionate Gamers um and and if you're a gaming publisher or gaming team like us this is the fastest growing region of the world right now for gaming and eort so this is why finally I would just like PC always brought the idea to bring a property and events to a country to create some bridges. view to elevate the Excellence but it's not the case for all teams. There's not a lot of competition if if you don't qualify for the Mixed environment which is very difficult. I'm very excited because I think one of the direction that these events will take is 50/50. Paris Olympics this summer will be the first time in 120 years that the Olympic Games have been held in Paris. It took them 120 years to get to that point Paris Olympics will be held in the French capital this summer. similar to the summer and winter games uh there'll be something you know with a flag ceremony uh probably at the beginning. I think you know to see some of the players that we selected when they were underage that we help become the best version of them themselves as athletes being selected to represent you follow a bit like the conversation about but uh rocket league and OverWatch back in the days at the at the World Cup for for their game and and and League of Legend also often have this conversation about creating the Euro Cup or World Cup.

ROUGE-1: 18.70, ROUGE-2: 17.24, ROUGE-L: 17.14
BERTScore: 63.04

==============================================
==================== [71/100] ====================
Summary:
"Apparently This Matters" is CNN Radio's weekly, offbeat look at stories you saw on CNN.com. This week, Jarrett ponders the meaning of the word "unlockable" "Unlockable," means either it's possible to unlock it, or it is not possible to lock it," Jarrett says. "Un-" combines with verbs and makes adjectives that mean more or less "not (adjective) " " Unfortunate" means not the adjective, whatever it is. could imagine that it would, but that's not what it means. Joseph, did you have a-- AUDIENCE: Yeah, I was going to-- based on what Raquel said, does the final-- after you "un-" something, is that going to be able to be redone? So if I untie a shoelace by cutting it up, now-- NORVIN RICHARDS: Yeah. It can't be tied again. If you undo an operation on a computer, does it have to be possible to do the operation again? I don't know, maybe. You can ask questions like, "Up which stairs did John walk?" It's a strange way to ask the question, but you can say it. syntacticians have to care passionately about the difference between one sentence, which is complete gibberish, and another sentence. There's a fair amount of great syntax that's built on those kinds of distinctions. This way of talking about vocabulary has the virtue of giving us a vocabulary for those those sentences that we don't know about but that are OK. CNN's John Sutter and CNN.com's John D. Sutter take on the idea that syntax is the ability to remember things you've heard people say. They argue that the existence of the second class of sentences shows that that's hopeless. They say the problem is we're capable of distinguishing grammaticality, even in sentences that don't mean anything. The debate continues at 10 p.m. ET tonight on CNN's "Sutter and Sutter" and "CNN.com Live" In many languages, you never leave a preposition at the end of the sentence. In English, there's a distinction between the examples. "To whom are you talking?" sounds fancy and snobbish, but still right. But you can't do that in English, "About what are youtalking?" NORVIN RICHARDS: It's not a meaning thing. It's something about syntax. You want to try to understand that. The only point of these few slides has been it's possible to study syntax independently of meaning. whereas the first one, it doesn't mean anything, but you feel as though-- and this is what I'm slowing down right about here where I say it doesn's mean anything. But it obeys the rules for how words can be combined. If the words meant something else, the sentence would be fine. We can have English sentences that consist of two adjectives modifying a noun. And that certain types of words are categories that you can add more words to, even if you don't know what they like. your English teacher? Yes? AUDIENCE: Is this another example because that's how it's done in Latin? NORVIN RICHARDS: Yes. Your English teacher told you to do that because Latin, actually, among many other languages, doesn't allow you to say this. You have to do this. English is quite rare in being able to doing this. Most of the languages of the world can't. Some time in the 15th, 16th century, a number of grammarians decided that English would be way cooler if it were more like Latin. here's another useful distinction. It's sometimes called competence versus performance. Imagine that I'm standing up here talking to you and I say, "This is the--" and then I Inhale a fly. And then imagine that this experience is so traumatizing for me and also for the fly, that I just I never complete that sentence. So we're going to be talking about speakers' competence, what they would do if there were no distractions and no problems. There's also performance. That's the study of what people actually do. Journalists know that the best way to make someone look like a complete idiot is to quote them accurately. What journalists, in fact, do is to clean up all that stuff so that people sounded like they were talking in complete sentences. So we're going to develop a theory of what English speakers say, but it's going to be a theory that's divorced from reality to a certain extent. We're Going to Imagine what English Speakers Say is going to look like to you and me. "The red book," we want it to be a substring that has certain privileges, can be used for these various types of phenomena. But there is no unit that we've created as we've been putting these pairs together that consists just to "find the red" Do people see that in this tree? So there's a node in the tree, if you want. It's the one that I circled in red that consistsjust of the words "the red book" But there's no thing that I could circle that would consist just of the word "find" is that it contains a noun. "Find the red book" we're going to give that the label verb, because having a verb is the important part for that. "In the garage" is a unit, it's a constituent. It's the kind of thing syntax gets to care about. If I want to, I can topicalize "in the garage." I can say things like, " in the garage I will find the book" It's a prepositional phrase. "the day after tomorrow" sure looks like a noun phrase. It's got a noun in it, "day," and then "the" before that, but we know that can go at the beginnings of noun phrases. So there are probably-- so the word "adverb" can be used to cover a bunch of things, including things that we don't have any other word for. And so I think you might be right that this is an adverb in the sense that it modifies the verb. "Waking the cats up" is a little bit like painting the cats red. We want "up" to not be a preposition that's combining with the cats. "I will walk her up," you're absolutely right. That's the way to say that. But I think you also have to walk the student up. You can't walk up the student, unless the student is lying down and you're walking on her. But if you mean you're going to walk with the student so that the two of you are up, so we want "walk the student" up to be different.

ROUGE-1: 15.38, ROUGE-2: 14.47, ROUGE-L: 13.61
BERTScore: 60.03

==============================================
==================== [72/100] ====================
Summary:
William Wordsworth is a very famous individual along with the next person we talked about Samuel Samuel Taylor Coleridge probably the two of the most influential of this particular era. The piece that we'll read today it's heavily influenced by his past you know they say that he was a true literary pioneer he defied the conventions of his time by insisting that poetry should be should express deep feelings about everyday experiences. The next piece we read is called The Rime of the Ancient Mariner forColeridge that was as we will read here and for the intro for that. Sister I've never lost a parent um I don't know if anyone has but I would imagine that's a big gap in your life and you may be bond a little bit more with your siblings maybe she being the only female he has a sense of protection for and so being separated you can imagine the struggle that one might have in dealing with it with the loss and so he you know he longed to be with her and such and it wasn't until it said the mid-20. This is probably one of the most difficult readings that we have throughout the semester I don't think it's impossible but you're going to have to you're gonna have to work a little bit ok. Just kind of grab onto those moments those moments of nature and such but also as this is kind of like a monologue where he's going on all of a sudden we find out at the end that he's actually talking to somebody. If the rhythm gets choppy and you start to get lost refocus this is only about eight minutes so it's not a long one but focus on what he is saying about nature from time to time. different than the typical every line it's kind of its own little thing and so when it's read in a different way it's it might see a little Jill at times button jamming is something very easy to visually identify a once you understand oh that's in JAMA it's a piece of cake okay. It might give you a little bit of reservation here and there but it shouldn't okay what I want to talk about first is a 786 lines composed a few miles above Tintern Abbey on Abbey you know a religious building. a little bit you know there's one look at you know a grass I think there's no roof on it but imagine what it'd have been like to you know to walk through this when it was a practicing monastery and such what was the like with that big roof on there with the stained glass windows all still in there I mean we don't have much of this here in Fort Wayne you knowthere might be a couple bigger churches sprinkled throughout that are kind of still Gothic influence but you know things like this it's pretty pretty amazing. he talks a lot I mean just describing you know leaning against the tree and looking out amongst the field on looking at the orchard with the you know it talks about the all of the which at this season with the unripe fruits are clad in one green hue. He almost has kind of a teleportation back to some degree of when he was a child okay he goes into great details about when I found through around the river like a like a row like a deer and I would play and when I was little I was having a great time. you distinctly remember a visual image have you ever gone back to that same place and like oh yeah this is the place one of my earliest memories was I was like two or three and I went to Disneyland on California I remembered nothing throughout my life except I remember visually what it looked like and so I took some journalism students to a convention there four or five years ago and I remember standing there in the front of Disneyland in California and watch the train go across. It's kind of a nice little Wow think about how I've changed in my life think aboutHow I've grown up things that I've experienced. hitting it wasn't now we know it's not just because he wants to record all the beautiful panoramic surround it's recording my time with my sister as well okay and so to see that they can appreciate that you know a few lines in your nature never did betray the heart that loved her nature's power over the mind that seeks her out is such that it renders that mind impaired impervious to evil tongues rash judgments and the sneers of selfish men and instilling instead a cheerful faith that the world is full of blessings.

ROUGE-1: 33.54, ROUGE-2: 32.80, ROUGE-L: 29.56
BERTScore: 64.61

==============================================
==================== [73/100] ====================
Summary:
Professor: We're going to be taking this IV curve that we've so laboriously set up and understood. And now we will subject it to illumination. So that's the essence of our lecture today, the diode under illumination. And as part of today's lecture, we have some wonderful little kits over there in the corner where we'll actually be testing IV curves of solar cells. So we should have sheets that describe essentially the equivalent circuit diagram, the IV characteristics and the energy band diagram for our pn-junction in the dark. The goal is to understand solar cell conversion efficiency, which is the ratio of output to input energy. For most solar cells, this breaks down to the following progression, from the solar spectrum to charge collection. Under all other conditions of operations of the solar cell, we're putting power into the device, not getting power out of it. The total system efficiency in blue is somewhere, depending on the plant, somewhere around 1%, maybe as high as 7% or 8%, depending on very specialized plants that are experts at converting sunlight. a little confusing for some folks. So just to be totally clear, in a device like this one, if it were subject to illumination you would have light coming in from the side, right, either from the p side or from the n side. To transfer this into what we've seen so far with the solar cell devices facing up toward the sun, you'd have to rotate this by 90 degrees. The drift and diffusion currents for electrons-- electron diffusion, electron drift-- there is an abundance of electrons over here in the n-type side, and so they want to diffuse over to the p-type. In a real device, when we you have a two-dimensional device within homogeneities, the current will travel through the weakest point of that pn-junction. Wherever the barrier height is lowest, current will begin crowding through that spot. So it's a way of probing or testing the quality of your junction characteristic in the dark. OK. Let's flip our page over and now device in theDark. Here, in the forward bias conditions, we're actually forcing carriers from the n-type material into the p- type material. But under illumination, now we have all of our carriers traveling from theP-type into theN-type. And that's why our current has changed signs. let's try to imagine what will happen under illuminated conditions, and let's start out in a very simple case. We'll assume that the principle of superposition applies here. What do we think will happen if our light intensity goes down by a factor of 2? So now if the amount of sunlight falling on our solar cell drops by 1/2, what do we predict will happen based on this right here? AUDIENCE: [INAUDIBLE]. PROFESSOR: The curve will shift, the red curve willShift up by about 1/1. Professor: Open-circuit voltage is just as you would think it is. When your solar cell is in an open circuit, when you-- say you took a pair of scissors-- please don't this-- and cut the leads, there would be a bias voltage have the right load. So the two need to be matched to each other. And that's where some of the power electronics come into play. Yeah? AUDIENCE: So in the last problem set, where [INAUDIBLE], we assumed that output voltage would be [INAudIBLE] volts. power out versus power in, the power out being the maximum power point power and the power in being the illumination from the sun. The ratio of the two boxes is defined as the fill factor. The fill factor indicates the quality of your diode. If your fill factor is very poor, that means that that sun right over there is being dragged toward the origin. That means you're filling less of this maximum square box function defined by the Voc Jsc. OK. So we have defined efficiency as power out divided by power in. box is Jmp times Vmp. OK? And notice I have another box around here. I have this clear box that starts at the Voc point and the Jsc point. And now I have two rectilinear shapes, this blue one and the clear one right here, the bigger one. The bigger one has an area of Jsc times Voc. And I'm going to define a parameter called fill factor, which will be the ratio of these two areas. If this is 1, which is virtually impossible to do, but if this were 1, it would mean that these two boxes were the same size.

ROUGE-1: 16.26, ROUGE-2: 15.73, ROUGE-L: 14.05
BERTScore: 64.72

==============================================
==================== [74/100] ====================
Summary:
Global chaos caused by a global outage the boss of the cyber security firm responsible has said it could be some time before all systems are back up and running thousands of flights have been cancelled Banking and health care has been affected including the NHS and some TV channels have been taken off air millions of people have been affected the problems are first reported in Australia before spreading across the world. tonight we'll be looking at exactly what happened and how it's affected patients passengers and businesses. BBC News as we heard the firm behind the outage crowd strike has held up its hands but admitted that it'll take time for things to get back to normal.

ROUGE-1: 11.48, ROUGE-2: 11.21, ROUGE-L: 11.39
BERTScore: 63.03

==============================================
==================== [75/100] ====================
Summary:
In the year 2000, four-year college grads actually earned more with their entry jobs than they're earning today. Changing technology has made wages rise more at the top, but has held wages down for a lot of other jobs. A third set of factors has to do with slower economic growth, slower productivity growth and slower dynamism in the U.S. economy. The Great Recession starting in 2008 meant that output was declining, employment was declining and people were laid off, he says. kinds of labor. The computer enhances the productivity of the skilled laborer. Information technology makes it possible for skilled labor to sell their products around the entire world. When supply and demand are ruling labor markets, the people who do well are those who have an economic understanding of where is demand high, and where is supply scarce. Check out our practice questions to test your money skills. Next up, we'll show you where to find data to help you decide which career to choose. ♪

ROUGE-1: 23.04, ROUGE-2: 20.90, ROUGE-L: 20.33
BERTScore: 62.34

==============================================
==================== [76/100] ====================
Summary:
Rationality, rationality axioms - completeness, reflexivity and transitivity. When our when someone’s preference exhibit these three properties, what does it mean that he is able to completely rank all his choices all his potential choices, with also possibility, with possibility that there are more than one bundle at some ranks this is a possibility. One thing also I should add that he will be able to rank only if he has finite consumption set and also what we have learned that this will translate into a utility function. Monotonicity tells us that indifference curve has to be downward sloping. The other thing that we can get from monotonicity is that indifference Curve cannot be a thick curve. So, whenever we have convexity the indifference curve will look like this. If I do this let me take this zone and look at it in, at the bigger scale what will happen, what we are basically seeing is that. indifference curve is very thin, fine, it is clear. And now we have convexity plus concexity.

ROUGE-1: 26.12, ROUGE-2: 24.94, ROUGE-L: 24.91
BERTScore: 71.54

==============================================
==================== [77/100] ====================
Summary:
If you have a complex polyhedron, we found general unfoldings. We proved one of them. If you want an edge unfolding, that's the centuries-old open problem. For non-convex polyhedra, we know this is too much to hope for. Even if it's topologically convex, there's not always an edge unfolds. But for general unfolding, we don't know. So today's lecture is actually mostly about these two open problems and different variations of it that we know how to solve. Theory of the facet-path: A path that alternates between visiting faces, triangles, and vertices. Professor: It's an open problem for something like a cube where you actually have quadrilateral faces. He says the claim is facet- Paths always exist for any triangulated, connected surface. The theorem works not only for a polyhedron-- it works for any manifold, any sort of locally, two-dimensional thing, Prof. Peter Kuznetsov says. original thing, we think of this triangle as being an ear, and this triangle is an ear. I like ears because they're kind of on the boundary, on the surface, so they're just triangles that are adjacent to only one other triangle. Now, the next step is to color what are called the second-level ears. If you remove those ears, what would, then, become an ear? And I'm going to stop there, just two levels. But for the general cases, if I find that that for two ears or three ears, I can just draw that in. Even if you have 2 k vertices, no more than 2 k alternations, slight, the place where we're making a little improvement is for the odd case. At every level of the tree, you're going to double what was below you, so have a face of 2 k or 2 k plus 1 edges, then it will have, at most, 2k alternations. I'm going to try and prove an upper bound, sandwich it between, and show that, actually, the upper bound is smaller than the lower bound, and that's a contradiction. The algorithm is called non-selfintersection. The general approach is always proceed rightward in the unfolding. You can unfold every orthogonal polyhedron this way. It also works if t is on the other side of s. The algorithm could also be inset into the [INAUDIBLE] voxel, but it's not quite sure what that would look like, yet. It's going to be exponential. Exponential number cuts is a lot. But it works. face at a time, you can't do that, but if you visit faces multiple times and kind of weave around in a clever way,you can do it. So if you follow along here, I just turn right here. So now, I go down here. And that is a left turn because it's on the bottom, a little hard to think about. So I turn left here, and then, we can zoom out, and you get the unfolding. They're so much fun. It's like exploding a city. Boom! If your tree is ugly like something like this, you'll start with something nice and small down here, maybe constant number of terms. Then, you'd double again, and it will be exponential. On the other hand, if your tree happens to be nice and balanced, doubling is not so bad because here you'll have constant. This a double everything below, but there's only log n levels. So is that linear? It should be about linear. It's certainly 2 to the theta log n. My conjecture's, in general, you need to omega n refinement. So ideally, k is one, and you're not subdividing at all. But maybe, you take every rectangle, divide it in half. Maybe that's enough to then be edge unfoldable. That would be sort of a refined level two grid-like unfolding. There are a ton of results about this. They're all partial. But one thing you could do, with merely 5 by 4 refinement, is something called Manhattan Towers. That's a Manhattan Tower. And in that case, 5 by four refinement is enough to unfold these things. glue whole edges to whole edges. If you want something sphere-like, in fact, those gluings have to be non-crossing. We know it must make a convex polyhedron. If it made two, Cauchy's rigidity theorem would tell you that they're the same. Even once you fix the gluing, you know that there's a unique convex realization, and there will be a unique set of edges from the shortest paths that actually realize it. Cauchy-Steinitz rigidity theorem is often attributed to both of them. It's sort of a proof by contradiction. We want to prove uniqueness. So we're supposing, well, maybe, there's two polyhedra, p and p prime, and they're combinatory equivalent and have matching faces congruent. What I want to do is look at corresponding vertices, let's say a vertices v and p and a vertex v prime and pprime. a sphere. Think of it is as almost flat. What that would mean is there's some other way to draw this thing. Basically, there's a way to flex this linkage so that all of these angles increase and this one stays the same. How could I get a polygon where all of the angles increases and still be convex? Ain't possible. Why is it not possible? I think we've used this fact a couple lectures ago. It's not possible by something called the Cauchy Arm Lemma. And here's the thing. If you have a convex chain but open chain here. There's a missing bar. true just viewing p prime as p and p as p prime. If there's anything in there other than zeroes, there has to be at least one plus, at leastone minus. Alternation is either going from plus to minus or from minus to plus. What's interesting here-- before we were thinking of labeling the vertices of the spherical polygon, but in fact, whatever this edge does, it does the same thing local to that vertices. So really the labels are on the edges of the graph. The number of alternations is at least 4 times the number of vertices. The other natural way to count angles is by looking at the faces. Every phase has a bunch of angles that have some degree or whatever. They're really kind of the same thing. This number's at least 8 larger than this number. It could be even more larger, but at least that contradiction done. It can't be both. This works as long as there's at at least one face. And that is Cauchy's rigidity theorem. conveniently relates vertices to faces, but it involves edges. If I look at every face and I count the number of edges, I will end up counting every edge twice, once from each side. So now, things are starting to look similar, and I want to get some cancellation going on. Use my cheat sheet here. I'm going to rewrite this formula as V equals 2 plus E minus F. OK? All I did here was decrease by-- well, because there's a half out here, I decrease each coefficient by 2. nothing surprising. Now, here it gets bigger. It's going to go 10, instead of 6 and 7. We also have a plus 8. We don't know whether there any faces of degree 6 or more, so we can't rely on that. But we have plus 8, which is at most, the number of alternations. Hm. So I could get a formula for 4V here. Right? 4V is going to be 8 plus this isgoing to be like 2.

ROUGE-1: 19.94, ROUGE-2: 19.02, ROUGE-L: 16.78
BERTScore: 66.91

==============================================
==================== [78/100] ====================
Summary:
For a particle traveling in a circle, we've seen that the velocity can be written as r d theta dt. Let's now look at rotation in an arbitrary plane. And so what we're going to do is use the right hand rule to define a direction that tells you both the direction it defines the plane and it also tells you what the positive direction of rotation is for that plane. Now, in the k hat direction, in this case, I'm going to call it in the arbitrary n hat direction.

ROUGE-1: 19.13, ROUGE-2: 18.37, ROUGE-L: 18.73
BERTScore: 73.16

==============================================
==================== [79/100] ====================
Summary:
In week four of CS224N, we take a break from learning more and more on neural network topics, and talk about final projects, but also some practical tips for building neural network systems. Today's lecture is the primary content for what you'll be using for building your assignment 4 systems. For assignment 4, we give you a mighty two extra days for it. And it's due on Thursday. So please be aware that assignment 4 is bigger and harder than the previous assignments. And then as I mentioned Thursday I'll turn to final projects. Machine translation was hyped as the solution to the Cold War obsession of keeping tabs on what the Russians were doing. Claims were made that the computer would replace most human translators. But despite the hype it ran into deep trouble. The idea was largely canned. Work then did revive in AI at doing rule based methods of machine translation in the 90s. But when things really became alive was once you got into the mid 90s, and when they were in the period of statistical NLP. We start with a source sentence. So this is a German sentence. And as is standard in German. You're getting this second position verb. So that's probably not in the right position for where the English translation is going to be. So we might need to rearrange the words. And in the process, I'll go through in more detail later when we do the neural equivalent. We sort of do this search where we explore likely translations and prune. And eventually, we've translated the whole of the input sentence and I've worked out a fairly likely translation. He does not go home. One French word gets translated as several English words. You can get the reverse, where you can have several French words that get translated as one English word. So here we sort of have four English words being translated as two French words. But they don't really break down and translate each other well. These things don't only happen across languages. They also happen within the language when you have different ways of saying the same thing. So another way you might have expressed the poor don't have any money is to say the poor are moneyless. That's much more similar to how the French is being rendered here. we have is based on the translation model. We have words or phrases that are reasonably likely translations of each German word, or sometimes a German phrase. And so then inside that, making use of this data, we're going to generate the translation piece by piece kind of like we did with our neural language models. And if we're guided by our fairly small, something like 5 to 10, we are going to keep track of the k most probable partial translation. So this is a heuristic method. It's not guaranteed to find the highest probability decoding. But at least, it gives you more of a shot than simply doing greedy decoding. The model is based on the idea that the final hidden state of the encode RNN is going to for instance, represent the source sentence. And we're going to feed it in directly as the initial hidden state for the decoder, or RNN. These models have also been applied not just to natural languages, but to other kinds of languages, including music, and also programming language code. So this is what the picture of a LSTM encoder-decoder neural machine translation system really looks like. Everywhere else as well. So you can do summarization. You can think of text summarization as translating a long text into a short text. But you can use them for other things that are in no way a translation whatsoever. So they're commonly used for neural dialogue systems. So the encoder will encode the previous two utterances, say. And then you will use the decoder to generate a next utterance. Some other uses are even freakier but have proven to be quite successful. So if you have any way of representing the parse of a sentence as a string. Stacked RNNs are more powerful. They require much less human effort to build. There's no feature engineering. You're using the same method for all language pairs. Next week, John is going to talk about transformer based networks. They're typically much deeper. But we'll leave discussing them until we get on further. So that was how we train the model. So let's just go a all parameters of the model end to end in a single large neural network has just proved to be a really powerful idea. stuck with it. And you have no way to undo decisions. So if these examples have been using this sentence about, he hit me with a pie going from translating from French to English. So we generate along and generate our whole sentence in this manner. And that's proven to be a very effective way of getting more information from the source sentence more flexibly to allow us to generate a good translation. I'll stop here for now and at the start of next time I'll finish this off by going through the actual equations for how attention works. In greedy decoding, we usually decode until the model produces an end token. In beam search decoding, different hypotheses may produce end tokens on different time steps. And so we don't want to stop as soon as one path through the search tree has generated end. So what we do is sort of put it aside as a complete hypothesis and continue exploring other hypotheses via our beam search. And then we'll look through the hypotheses that we've completed and say which is the best one of those. And that's the one we'll use. you have many languages don't distinguish gender. So the sentences are neutral between things masculine or feminine. But what happens when that gets translated into English by Google Translate is that the English language model just kicks in and applies stereotypical biases. So these gender neutral sentences get translated into, she works as a nurse. He works as an programmer. So if you want to help solve this problem, all of you can help by using singular they in all contexts when you're putting material online. And that could then change the distribution of what's generated. For languages for which there isn't much parallel data available, commonly the biggest place where you can get parallel data is from Bible translations. So this is a piece of parallel data that we can learn from. Cherokee is not a language that Google offers on Google Translate. So we can see how far we can get. But we have to be modest in our expectations because it's hard to build a very good MT system with only a fairly limited amount of data. There is a flipside, which is for you students doing the assignment. The advantage of not too much data is that your models will train relatively quickly.

ROUGE-1: 20.23, ROUGE-2: 19.56, ROUGE-L: 17.04
BERTScore: 65.33

==============================================
==================== [80/100] ====================
Summary:
Learn about radiation damage and nuclear materials in 22.01. Learn about the different material properties and what they actually mean. Use the weekly Newsquiz to test your knowledge of stories you saw on MIT OpenCourseWare. You can also send in your own photos to share with the rest of the world. The Daily Discussion is a written version of each of the MIT Open CourseWare lectures. Use this week's Daily Discussion to help students with reading comprehension and vocabulary and to help them understand today's featured news stories. can put in until it starts to either plastically deform or it hits its UTS, ultimate tensile strength, where it will just fail. Stiffness is more of a response function, so it's how much does it deform in relation to how much stress you put into it. The effects of this are things like stiffening. An increase in the Young's Modulus. Radiation creates pretty much any and all defects, it's a great way to stiffen and strengthen the material. Michael Short: DPA measures the number of times that each atom has moved out of its original site, but it has nothing to do with how many times it stays. DPA is all over in less than a picosecond. But it can take years for all these different defects to diffuse, to cluster up, and to form these super structures. Short: Temperature determines diffusivities. It also can change phases or crystal arrangements, like for the case of anything iron-based, but that is indeed what happens in the end. energy E and imparting kinetic energy T to another struck atom? That comes right from-- remember our treatment-- I think I've drawn this probably 50 times now. Our hollow cylinder treatment of a charged particle with charge little ze interacting with a particle a big ZE at some impact parameter B. We wanted to know well, for all possible approach paths, the area of this hollow circle, or the probability that this particular approach path is taken, is just the area here 2pi b db. Nuclear materials data looks something like this. Would you be bold enough to draw a trend line through a single data point? No. What about three where it doesn't actually match up with one of them? Or is there any reason why you think they made this parabolic instead of a linear line? I can draw a line that would fit between the error bars of these two right. They're not readable because they're not important. What I do want you to know is what's the quality of this data set you see? Russia has a fleet of sodium cooled fast reactors that can get you 25 DPA per year. If your reactor is going to go to 500 DPA, you have to know whether or not your materials will survive. In order to prove that it's actually safe to continue operation, you must have some amount of material to test and say, OK, this vessel is still ductile, it's still going to work. We only plan to put these vessels in service for 40 years, and folks put 40 years worth of these coupons. pocket of vacuum, then that pressure differential goes down and that void becomes a bubble, and that bubble is more stable. At higher energy starting around 2 MeV, there is a small, but non-negligible chance that a neutron will go in, and a helium atom comes out. As the voids fill with gas, they become more and more stable and a lot of materials generate their own gas. You can also get dislocation buildup. Normally, you would have to deform a material to create and move dislocations, but when you apply radiation, you can just create dislocation. small clusters can collapse into dislocations or the stress induced from irradiating things can cause more stress. You create what's called this network forest of dislocation that makes things a lot harder to deform. So when you actually pull on something, to show you diagrammatically, it deforms something like that. You get a mixture of bending and rotation to make it look like the bar is bending uniformly straight, but on the microscale, it's not. To show you some extreme examples of slip, that's when you have to go nano. the metal that all those guys thought was going to be ductile like metal was more brittle than glass. And any sort of bump would cause just complete shattering of this metal and catastrophic release of radioactive material. So this took them-- let's see-- I think Kazakhstan is smaller than the US. So who here has done a cross-country trip? How long it take you? AUDIENCE: Six days from Seattle to here. MICHAEL SHORT: OK, so this trip took them 13 days because they went slow. These out and you hit them with a very well calibrated hammer. And you can measure by actually turning this dial and letting the hammer turn it. So it breaks right through the material, in this case, it's in a quenched or brittle condition. And by doing this test at a number of different temperatures, you can recreate this ductile-brittle transition temperature curve. So they'll take a few Charpy coupons, they will test them at, let's say, every 25 Celsius, get a bunch of points, and decide where is the material brittle. that gets the most damage, you'd be taking out some of the stainless steel, which is a problem. You could take a piece out from here, maybe the outside, but then you've got a stress concentrator. Any sort of chunk that is missing is where a crack is going to preferentially form. The only way to do that is to go nano. We can go much, much smaller and just take out tiny pieces of the vessel and get the same information from a Charpy test but on the nanoscale. Michael Short: We predict that something stores about 2% of its energy in radiation defense. Short: What if there was a way to know how many of each of those defects there actually were in a material? Short: He used a nano a DSC, or nano differential scanning calorimeter, that can heat about 10,000 a millisecond. He says the data was taken like two months ago, it's pretty fresh, and it's not published yet, so hopefully by the time this video comes out, it will be. Snipes are real. You pretty much have to be British to know it, because they hunt them there for sport, and apparently, they're delicious. That's actually where we get the term snipe because the actual size of the sniper compared to the sniper is about that. If you can shoot that bird, if you can get that shot, you're a snipe hunter. You're not looking for a bird that doesn't exist. It's what we call the ultimate snipe hunt. It actually fits on a chip. There's one that we put our material on, and one as a reference that we both put in the accelerator being irradiated at the same time. This is what they actually look like. The scale bar here is 100 microns, and that transparent spot is a little bit of aluminum that we vapor deposited onto the calorimeter. Right there. That whole thing just went from zero to 450c. That pulse right there. It slowed down by a factor of 1,000. And so the way this process works, is we take our DSC chip, we put a mask over it.

ROUGE-1: 22.06, ROUGE-2: 20.37, ROUGE-L: 17.24
BERTScore: 63.68

==============================================
==================== [81/100] ====================
Summary:
In Western astrology, it's a constellation determined by when your birthday falls in the calendar. But according to the Chinese zodiac, your shēngxiào is the animal assigned to your birth year. Of the many myths explaining these animal signs and their arrangement, the most enduring one is that of the Great Race. The first twelve animals to make it across the river would earn a spot on the zodiac calendar in the order they arrived. Each year is associated with one of the animals in this order, with the cycle starting over every 60 years.

ROUGE-1: 25.00, ROUGE-2: 23.79, ROUGE-L: 24.49
BERTScore: 67.07

==============================================
==================== [82/100] ====================
Summary:
Professor: In almost all cases when you address atoms, you do two photon courses because a photon is scattered. In reality, it is a two photon process and not two single photon processes, he says. Professor: If you have any doubts about some subtleties about how is light absorbed and emitted, the correct answer is always obtained from the two-photon picture. He concludes with a discussion on Raman processes, which involve two different vibrational states of a molecule and two different momenta. many terms do you get if you write down the Hamiltonian? No approximation without. How many do we get? Four. But then I think in the second step you get eight more. So just tell you what is special about two photons. I focus now on a situation, and that's the most common situation in the laboratory. If this intermediate state is resonant with omega 1, then we only want to consider now the process where the first step to the intermediate step is driven by the field e1. So therefore, we have only one term, which dominates out of those four. lowest order perturbation, second order perturbedation theory, in two steps. But if you're asking, what is the transition probability? The transition probability, we have to get the probability to be in the excited state. And when we divide the probability, the amplitude squared by t, we get a rate. And this is Fermi's Golden Rule. It is exactly the same you have seen probably more than 100 times. So this is the correct description of resonant fluorescence and Rayleigh scattering. e to the minus i omega t, I mentioned once to you is the sign plus or minus means whether we absorb a photon or whether we emit a photon. If you're unlucky and don't choose your lasers wisely, the two laser photons could get you high up into an electronically excited state. But in general, I would say if you have more than one process, there is no interesting interference term. You just get two different rates. And you just have both simultaneously. They're not leading to the same final state. Say if omega 1 is very, very strong, you can exactly diagonalize the Hilbert space of states k and a. But again, what happens is you mix those two states. And it is the admixture now in a non-perturbative way of state k. And from this admixture, we can absorb omega 2. And so it looks like, actually, now a two-level system, where we go from the dashed line-- called which is scattered coherent with the incoming photon? Raman: We can determine the phase of the laser in a homodyne or heterodyne experiment. And if the laser is in a coherent state, we can measure phi to any level of accuracy. Perfect phi has now been imprinted into a two-level system for the first time, and it appears to be working, says Raman. Raman: The discovery by Raman, which was rewarded with the Nobel Prize, for suddenly observing when you excited molecules with a very strong light bulb, you suddenly saw very different frequencies. Two photons are absorbed from the left and from the right, and therefore you get a very, very sharp line. Since hydrogen is of methological importance, measurements of-- fundamental measurements of the Lamb shift, comparisons with QED calculations, measurement of the Rydberg constant-- these are all done by hydrogen spectroscopy. So therefore, it is very important to have precision method which suppresses the Doppler effect. If you now estimate what is the relative line width, the delta, the line broadening, in relation to the transition frequency? So just give me a second. line, which is sufficiently sharp, sufficiently narrow, and also insensitive to magnetic fields and electric fields. People who want to measure fundamental constants-- the Rydberg constant-- want to compare lame shift with first principle QED calculations. This is the case for hydrogen. And, actually, with some advance in the numeric calculation of wave functions and all that, it may also be possible to do it with helium. But so far it hasn't kind of-- helium has not replaced hydrogen. It's still is an important technique and important tool for measurements. Coherence exists if there is a well-defined phase between two or more amplitudes, but we can only observe it if those amplitudes interfere. Coherence can be two amplitudes describing two different atoms, or it can betwo amplitudes of two states within the same atom. The first form of coherence I want to discuss is the coherence involved in exciting atoms and the atom emitting light. It's related to the spontaneous emission and scattering problem. To be continued on Monday. of spontaneous emission. The 100% unique and correct answer is the system involves with the following operator. This is the operator which completely describes the interaction of an atom with the electromagnetic field. The fact that we have a unitary evolution with this operator is 100% or 110% true. But this operator will actually lead to final states of the photon field, which may not have a specific phase. And this is actually what I want to work out with you in-- maybe even today, I think ten minutes may be enough-- what is really the information-- the phase information-- which we have in a photon.

ROUGE-1: 16.82, ROUGE-2: 15.73, ROUGE-L: 15.52
BERTScore: 66.81

==============================================
==================== [83/100] ====================
Summary:
Lecture eight is about deep learning software and how the hardware works. We'll talk a little bit about CPUs and GPUs and then we'll talk about several of the major deep learning frameworks that are out there in use these days. We're in the process of assigning TA's to projects based on what the project area is and the expertise of the TA's. So we'll have some more information about that in the next couple days I fine tune them for your own problem. But before I move on, is there any sort of questions about CPU and GPU? Remember to stop your Google Cloud instances when you're not working to try to preserve your credits. For assignment two you really only need to use GPU instances for the last notebook. For all of the several notebooks it's just in Python and Numpy so you don't need any GPUs for those questions. And the final reminder is that the midterm is coming up. We're also in the process of grading assignment one, so stay tuned and we'll get those grades back to you as soon as we can. The midterm will be in class on Tuesday, five nine. It'll be sort of pen and paper working through different kinds of, slightly more theoretical questions to check your understanding of the material that we've covered so far. And I think we'll probably post at least a short sort of sample of the types of questions to expect. So just, Yeah, yeah, so that's what we've done in the past is just closed note, closed book, relatively just like want to check that you understand the intuition behind most of the stuff we've presented. about fancier optimization algorithms for deep learning models including SGD Momentum, Nesterov, RMSProp and Adam. And we saw that these relatively small tweaks on top of vanilla SGD, are relatively easy to implement but can make your networks converge a bit faster. We also talked about transfer learning where you can maybe download big networks that were pre-trained on some dataset and then GPU. But that being said, I think there are still pretty substantial speed ups to be had here. this point about what exactly these things are and why one might be better than another for different tasks. So, who's built a computer before? Just kind of show of hands. So this is a shot of my computer at home that I built. And you can see that there's a lot of stuff going on inside the computer, maybe, hopefully you know what most of these parts are. And the CPU is the Central Processing Unit. That's this little chip hidden under this cooling fan right here near the top of the case.  Torch is in Lua, not Python, unlike these other things. Torch doesn't have autograd. Torch is also older, so it's more stable, less susceptible to bugs. PyTorch is newer, there's less existing code, it's still subject to change. So it's a little bit more of an adventure. But at least for me, I kind of prefer, I don't really see much reason for myself to use Torch overPyTorch anymore at this time. Caffe was from Berkeley, Torch was developed originally NYU and also in collaboration with Facebook. Theana was mostly build at the University of Montreal. These kind of next generation deep learning frameworks all originated in industry. So it's kind of an interesting shift that we've seen in the landscape over the last couple of years is that these ideas have really moved a lot from academia into industry. And now industry is kind of giving us these big powerful nice can compose together these modules to build big networks. In Numpy, you kind of need to write out of these frameworks. In TensorFlow, you can really just, like with one line you can switch all this computation between CPU and GPU. You'll first have a bunch of code that builds the graph and then you'll go and run the graph many many times. So this is the really, this is kind of the big common pattern in Tensor Flow. And now here is where we're actually running the graph. So we're just creating concrete arrays for X, Y, and w1 and w2 using Numpy and then storing these in some dictionary. TensorFlow gives you a bunch of convenience functions that compute common neural network things for you. In this example we were computing the loss explicitly using our own tensor operations, TensorFlow can always do that. But in this case we can also use tf.mean_squared_error and L2 so we don't just use the loss for us, we can just use L2 for us. And here we're now only feeding in the data and labels X and Y and the weights are living inside the graph. And then you might think that this would train the network, but there's actually a bug here. TensorFlow is smart and it only computes the parts of the graph that are necessary for computing the output that you asked it to compute. When we tell TensorFlow we want to run a tensor, then we get the concrete value. But because of this dependency we've told it that updates depends on these assign operations. These assign operations live inside the computational graph and all live inside GPU memory. So then we're doing these update operations entirely on the GPU and we're no longer copying the updated values back out of theGraph. So we've got this full example of training a network in TensorFlow and we're kind of adding bells and you. So one example that ships with Tensor Flow, is this tf.layers inside. So now in this code example you can see that our code is only explicitly declaring the X and the Y which are the placeholders for the data and the labels. And now we say that H=tf.l layers.dense, we give it the input X and we tell it units=H. This is again kind of a magical line because inside this line, it's kind of setting up w1 and b1, the bias. It's setting up variables for those with the right shapes that are kind of inside the graph but a little bit hidden from us. Top of TensorFlow handles sort of building up these computational graph for you up in the back end. Keras also supports Theano as a back end, so that's also kind of nice. In this example you can see we build the model as a sequence of layers. We build some optimizer object and we call model.compile and this does a lot of magic. And now we can call. model.fit and that does the whole training procedure for us magically. But most of the time you will probably not need to define your own autograd operations. want it to optimize over the parameters of the model. Giving it some learning rate under the hyper parameters. And now after we compute our gradients we can just call optimizer.step and it updates all the parameters. So another common thing you'll do in PyTorch a lot is define your own nn modules. So typically you'll write your own class which defines you entire model as a single new nn module class. And a module is just kind of a neural network layer that can contain either other other modules or trainable weights or other other kinds of state. So now here in the initializer of the class we're assigning this linear1 and linear2. a chance to play around with this myself so I can't really speak to how useful it is. Tensorboard actually lets you visualize the structure of the computational graph. And Visdom does not have that functionality yet. PyTorch is kind of an evolution of, kind of a newer updated version of an older framework called Torch. You kind of need to relearn a whole separate set of control flow operators. And if you want to do any kinds of control Flow inside your computational graph using TensorFlow. different where we're actually building up this new computational graph, this new fresh thing on every forward pass. That's called a dynamic computational graph. With a static graph you can imagine that you write this code that builds up the graph and then once you've built the graph, you have this data structure in memory that represents the entire structure of your network. Whereas with a dynamic graph, because we're interleaving these processes of graph building and graph execution, you kind of need the original code at all times. TensorFlow this becomes a little bit uglier. And again, because we need to construct the graph all at once up front, this control flow looping construct again needs to be an explicit node in the TensorFlow graph. So I hope you remember your functional programming because you'll have to use those kinds of operators to implement looping constructs. But what this basically means is that you have this sense that Tensor Flow is almost building its own entire programming language, using the language of computational graphs. point. So this type of thing seems kind of complicated and hairy to implement using TensorFlow, but in PyTorch you can just kind of use like normal Python control flow and it'll work out just fine. Another bit of more researchy application is this really cool idea that I like called neuromodule networks for visual question answering. So here the idea is that we want to ask some questions about images where we maybe input this image of cats and dogs, there's some question, what color is the cat, and then internally the system can read the question. inner product, we compute some loss and the whole structure of the graph is set up in this text file. One kind of downside here is that these files can get really ugly for very large networks. So for something like the 152 layer ResNet model, which by the way was trained in Caffe originally, then this prototxt file ends up almost 7000 lines long. So people are not writing these by hand. People will sometimes will like write python scripts to generate these prototext files.

ROUGE-1: 20.34, ROUGE-2: 19.60, ROUGE-L: 18.08
BERTScore: 72.49

==============================================
==================== [84/100] ====================
Summary:
Part 3 in our series on distributed word representations. We're going to be talking about vector comparison methods. To try to make this discussion pretty intuitive, I'm going to ground things in this running example. On the left, I have a very small vector space model. We have three words, A, B, and C. And you can see graphically that B and C are pretty close together. And A is kind of lonely down here in the corner, the infrequent one. we changed the space as I showed you before. So they're all up here kind of on the units here. And notice that the actual values that we get out are the same whether or not we did that L2 norming step. And that is because cosine is building the effects of L2norming directly into this normalization here in the denominator. There are a few other methods that we could think about or classes of methods. I think we don't need to get distracted by the details.

ROUGE-1: 15.13, ROUGE-2: 14.87, ROUGE-L: 15.13
BERTScore: 67.60

==============================================
==================== [85/100] ====================
Summary:
liyan blake a little bit about him he's kind of out there some of his contemporaries thought he was insane you know nowadays maybe he would be heavily medicated. People think that he's out there and there was that who was William Wordsworth who will read and discuss in the next day or two he said that there is something in the madness of this man which interests me more than the sanity of Lord Byron and Walter Scott so something about this man is more interesting than these other people. anything else that we've seen before that it's almost like a new art form does that kind of make sense? He was an individual who you know was into art and was into eventually you know religion and such became a big part of his life but his as an artist you know he was a very prolific poet but he was also big-time into painting we'll see some of his paintings later on but also a new type of art form kind of relief fetching where you kind of sketch out a negative of what you should have and so he can kind of do that same thing but fetching into a metal. and your parents don't know just throw out William Blake you might be right okay very very famous individual so we'll be covering in the next couple days the lamb by William Blake. okay this is probably the sweetest of the poems that we have to some degree okay looking at it at face value one they have a little picture and these are the the etchings that he does okay you know with the picture that they give us to to help place things you know we have a shepherd talking to his flock okay he's out there alone he's probably has conversations with them. the field but somebody that is passionate about their religion and their faith you know the nature and the common man the shepherd person of the flock or even a child in a sense of children so it's a very short one and it's one that I believe is pretty easy to to comprehend and understand okay. "I believe it is one of the easiest to understand and understand," he says. "It's one of those things that is very, very easy to understand. It's very simple to understand"

ROUGE-1: 47.51, ROUGE-2: 44.63, ROUGE-L: 44.57
BERTScore: 73.60

==============================================
==================== [86/100] ====================
Summary:
Cú Chulainn, hero of Ulster, stood at the ford at Cooley, ready to face an entire army singlehandedly. The army in question belonged to Queen Meadhbh of Connaught. Enraged at her husband’s possession of a white bull of awesome strength, she had set out to capture the fabled brown bull of Ulster at any cost. Cú Chulpainn invoked the sacred rite of single combat in order to fight the intruders one by one. broken heart, leaving behind a land that would remain ravaged by Meadhbh’s war for years to come. “I will never forget you,” he wrote to his son. ‘I will always love you.’ “You are my son,’ he replied, “and I will always be your son.” “And I will never leave you, my son. I will forever love you’, he said, ‘even though I’m no longer your son’.

ROUGE-1: 26.13, ROUGE-2: 22.31, ROUGE-L: 23.54
BERTScore: 64.46

==============================================
==================== [87/100] ====================
Summary:
The Underworld is actually a lovely place to "live" It boasts historic charm and eccentric neighbors with eternal ties to the area. Tartarus is reserved for a select few who some might call the greatest sinners of all time. Elysium is the Underworld’s exclusive VIP section— and your permanent home. The Underworld also features four other waterways: Acheron, the river of woe; Cocytus, river. of wailing; Lethe, river of oblivion; and Phlegethon, a. great source of natural light.

ROUGE-1: 21.21, ROUGE-2: 19.65, ROUGE-L: 15.30
BERTScore: 65.12

==============================================
==================== [88/100] ====================
Summary:
Fluorescence is the emission of light in the absence of heat. Luminescence is the general term. Fluorescence is a little bit more specific. There are different types of luminescence. And you'll get to see some of those varieties of luminecence. The first thing to learn about fluorescence is how to spell fluorescence. It actually is fluor, F-L-U-O-R-E-S. What are you guys whispering about? Did I get something else wrong? Luminescence is the interaction of a chemical with another chemical to give luminescence. Another pretty useful type of luminescent is bioluminescent. I've done a lot of scuba diving in my life. And there's nothing more exciting than a scuba dive where you're trying to find out what's going on under the water. It's a great way to get a sense of the depth of the water, and how the water is moving. I think that's the most famous luminol sort of example. a particular wavelength so it's called its Lambda of excitation. Once it's excited, it just drops its energy back out and goes back to the ground state. This wavelength is higher energy. Obviously you don't create energy when you shine light on something. You'd be breaking a few fundamental rules if you did. The other way is simply to soak a dye into the gel. And the dye, because of the positive charge here, the counter ion gets displaced to the DNA and associates with it quite tightly. Professor Martin: We can't use DNA ethidium bromide, which is toxic. Instead, we can use a series of dyes known as DAPI and HOECHST, H-O-E-C-H-S-T. These dyes bind to DNA pretty differently, Professor Martin says. Professor Martin: It's a combinatorial system to take little pieces of DNA to give you a molecular structure to recognize virtually any target. The BDJ will hear more about what you want to think about in the next segment. was in the major groove, it would be swimming around in that groove. It's almost too big. So what's really cool about these dyes is they slide in between into the minor groove. And they also make some contacts with the phosphodiester backbone. They're literally in the groove. But there's some opportunity for electrostatic interactions. And in fact, they bind in particular regions of DNA where there's AT, not GC. Those are the places where there're just the pair of hydrogen bonds instead of the trio. Professor Martin: antibodies are agents of the human adaptive immune system. He says they have been exploited intensively to study biology. Professor Martin: What you will learn from Professor Martin in two or three lectures time is much more about the nuts and bolts of the immune cells. He'll talk about the technological side of antibodies and how they are useful reagents to study Biology. He will also talk to you about how they recognize their targets and how it mounts a response to disease and other features.

ROUGE-1: 13.92, ROUGE-2: 12.18, ROUGE-L: 11.30
BERTScore: 60.65

==============================================
==================== [89/100] ====================
Summary:
 RAFAEL JARAMILLO: Today, we're going to discuss the many D's of thermodynamics. We'll talk about lowercase d, lowercase Greek d, and uppercase Greek D. Jaramillo: Of the three D's, the U.S. D is the one that contains the most physical, unstated assumptions. He says it is often the hardest for students to understand when you first encounter it. JARamillo says to illustrate the concept of transformation quantities, it helps to draw state function surfaces. In thermodynamics, we have many different transformation quantities that we keep track of. In material science, our most common independent variables are pressure and temperature. But for any transformation quantity, you can at least imagine, if not draw out on a piece of paper, state function surfaces corresponding to the quantity that you're trying to measure, independent variables corresponding to those variables that you are regulating, and a vertical distance. So this is illustration for an isobaric, isothermal transformation, and we're illustrating this for entropy.

ROUGE-1: 28.70, ROUGE-2: 25.36, ROUGE-L: 25.49
BERTScore: 67.39

==============================================
==================== [90/100] ====================
Summary:
In this video I'm going to be going over lung oscilation specifically the sites of where you osculate. We're going to go over some audio clips of normal breath sounds and where you should hear them at CU that's a big thing. We'll also go over abnormal breath sounds. In the next video I'll be performing an assessment on a patient and show you how to listen with your stethoscope to these sides. I'll also give you some landmarks to make your job easier so you'll know what intercostal space correlates with which lobe of the lung. noise this fine crackles is high pitch it sounds completely different than coarse crackles and it has like a crackling a fire sound to it. The last sound is called a plur friction rub this is heard both on inspiration and expiration and it is a low pitch harsh grading sound. What's causing this is that your plora on your lungs those two layers are rubbing against them each other and they normally have this little thin layer of Cirus fluid around the lung but it doesn't right now due to all that inflammation.

ROUGE-1: 11.45, ROUGE-2: 10.60, ROUGE-L: 10.31
BERTScore: 67.35

==============================================
==================== [91/100] ====================
Summary:
Professor Donald Kagan: Spartan constitution is very much an oligarchy. The average Spartan did not ever speak in the assembly, it appears. So it's not a democratic assembly, even though every single citizen is there, if he wants to be. So, the Spartans gain a reputation of being hostile to tyrants because they often fight against tyrants. They will not like to see either extreme. They won't like democracies and they won't. like autocracy. The Spartans will not. like oligarchically governed states. All adult male Spartans were participants in the assembly. They came to the meeting dressed in their military uniform, apparently including their shields. When a question was put to the Spartans, the way they responded was by shouting and banging on their shields, whereupon the presiding official would try to determine which side had the loudest noise. The reasons for that are really what I want to make you see. At the core of it all, according to Thucydides, it was the fear of the Helots. Some scholars go far too far in suggesting that there never was a debate in the assembly. Most of the real life of the state in the earliest days would have been out in the countryside. You must imagine that it is something like, nothing like precisely, but something like the feudal manners that we find in western Europe after the fall of the Roman Empire. The gerousia was, by far, the most significant council in the state, most able to have the necessary prestige and functions that I have talked about. Aristotle tells us that they in fact were just any Joe Spartan, that they were ordinary people, not distinguished in any way. Dorian's versus Achaeans still seems to have some meaning to the Greeks. When they defeat the Tegeans, instead of simply annexing their territory, subordinating the people, they do something different. They offer the TeGeans an alliance. The Spartans, of course, have a need of a collection of states that stand between them and their potential enemies of whom the Argives are the most important. ancient Greeks referred to as the Spartans and their allies, which modern scholars have come to call the Peloponnesian League. The Spartans who had been successful apparently in turning around, to some considerable degree their defeat back in the seventh century in fight with Argos. They suffered a defeat in the region of Arcadia to the north of Laconia--that by the way is mountainous country and poor typically, relatively speaking and it provided some of the toughest warriors in the Greek world. So, it's no miracle that the Spartans had a hard time up there. It's a very important state for the Spartans, not just because it's the neighbor right to the north of them, but because remember what I told you, if you want to get to Mycenae from Sparta you can't go across those mountains. So, its strategic importance is very great. The Spartans got into this war with Tegea and they claimed to have discovered the bones of the great Homeric hero, Orestes and taken it away. Also, there was a legend that maybe they propagated that showed up in some poetry we have. try to enjoy these things in spite of their being barred. Why? Because in a way, necessity becomes a virtue. That's what the Spartans did. Their way of life was imposed upon them by the decision to maintain their command of the Helots. After that it all makes perfect sense. Look what they had to give up, to do it. They said, of course, we gave that up, because that's what makes us the great people we are. It's the way we cope. Attica was not one of the most desirable, certainly agriculturally rich areas in Greece. It has one splendid harbor; up in the northwestern part of Attica is Piraeus, the port of Athens. The silver mines in the south of the peninsula gave the state a source of income that was very, very unusual among the Greek city states. Another natural resource of great value and great blessing to those of us who can still see the remains of the Athenian experience is the marble that comes from Mount Penteli. An aristocratic republic is what we have, not a monarchy, but a republic. Aristocracies love equality; equality among aristocrats, and then tremendous inequality between them and everybody else. Yalies are very nervous about anybody sticking his head up above the crowd, because the question is always why not me? You have high expectations of yourself and so sometimes unless you're invaded by later religious ideas that the Greeks didn't have you're not humble, you're vying for honor. We hear about the new class distinctions, which are now based not on birth but on wealth. early seventh century, the date he gives us, of course we shouldn't put too much credence in it, it's too precise, but it's 683 B.C. On that occasion, we are introduced to a new thing, magistrates are chosen from the aristocracy to do various jobs in the city. In Athens, the magistrates were called archons, it means, in the most technical sense, rulers. One of these was called the War archon, polemarch, presumably he led the army. Next came the archon who was actually the most important archon and gave his name to the year. opponents that he was defeated. The leader of the resistance was the family known as the Alcmaeonidae. They went up there, locked up Cylon and his supporters in the Acropolis, in a temple. They cut the cord and killed the Cylonians. That put an end to the Cylonian conspiracy but it brought something to AlcMAeonidae as well, a curse. They were declared accursed and driven from the city. Later on we will hear they're back again and they're very important. But the curse continues to be attached to the family. that there are the kinds of discontents that we have been talking about which find the leader in the form of a man who is an outstanding figure for some reason, who is willing to try to establish a tyranny. That it fails, I think, is an indication that the same forces haven't reached the power in Athens that they had reached in Megara, Corinth, Sicyon, and places like that. It's a warning about troubles ahead and I'll turn to those troubles in the next hour.

ROUGE-1: 20.42, ROUGE-2: 19.40, ROUGE-L: 16.28
BERTScore: 61.27

==============================================
==================== [92/100] ====================
Summary:
Ka-Yen: "I walked into MIT not knowing a single thing about nuclear energy" "What causes nuclear resurgence? This is the perfect time to talk about why nuclear power's cool" "I'm going to be honest with you. I don't know all that much about potentially spewing out that much information" "How do you calculate this? Bozeman Tzeman. Cool? Whew." "What is the difference between fissile, fertile, and fissionable material?" In 1951, the first nuclear reactor to produce electricity was the experimental breeder reactor, the EDR1. The first nuclear powered submarine, the USS Nautilus, was launched in 1954. The real heyday of nuclear was between 1960 to 1975, when people like Westinghouse were creating nuclear reactors. From 1975 to 2002, you can see a massive decline. And then today, we're kind of-- I say we're back, but basically we're entering what people like to call a nuclear renaissance. Nuclear creates 75 times less carbon emission than coal does, and 35 times less than natural gas does. After a nuclear accident you can see a steep decline in the amount of nuclear reactors that are being commissioned. Nuclear power is accidents. It can serve as a good baseload source of energy. It's just about $0.01 per kilowatt hour, as compared to natural gas, which the majority of the costs of electricity actually come from the fuel. The majority of cost actually doesn't come from nuclear fuel at all. The fuel core is basically just a bunch of rods of uranium, sometimes it's clad in something like zirconium. Neutrons can simulate other fissions, and the control rods are there to make sure that there's not too many fissions happening in the fuel core at a certain time. The heat that gets created during these nuclear fission, that goes and heats up the water. The water flows through the core and heats it up. It creates steam so the steam goes and spins a turbine. The turbine creates electricity. With BWRs there is a higher chance of leaking radioactive material into the environment. Heavy water has a much lower absorption cross-section than light water does. So because it's absorbing less neutrons, you're actually able to use a lower enriched uranium, which is really great because that lowers fuel costs. But the downside is that, even your fuel costs, deuterium is really expensive. It's about 1,000 or so dollars per kilogram. So even though even though you're counteracting the lower fuel with higher water, you actually have to change out your fuel more often. here is basically just showing that there are a lot of redundancy systems inside these reactors. We don't just have one single primary loop and if it fails, it fails. We actually have four at the same time, and this is just called the n minus two redundancy, something like that. So the next kind is something much cooler. It's got a heavy water reactor. Actually it's just a little bit cooler. But the main heavy water reactors that everyone can kind of think of on their minds is CANDU, which is the one that's located in Canada. Three Mile Island, Chernobyl and Three Mile Island are all examples of nuclear reactor accidents. The first one happened in 1979 on March 28. The second one happened on April 25, 1986 at Chernobyl. The third one happened at the Chernobyl nuclear plant in 1986 on March 31. The last one was at the Fukushima Dai-ichi nuclear reactor in Japan in 1986, on March 30. It's the same kind of accident that happened at Three mile Island and Chernobyl, but it's not the same. Yucca Mountain is the primary push by the U.S. to find a deep geological repository somewhere in the United States so we can deal with our spent fuel. If you were to just keep all the spent fuel that we create in fuel casks, it'd take about 300 acres of land, which is absolutely insane. We also have to make sure that there's not a lot of water that leaks through, because the water can carry the radioisotopes and carry them into the environment. Yucca Mountain is located in Nevada. People in Nevada weren't happy about this. There was a lot of opposition. Because of the social opposition there was government opposition and many loopholes we had to jump through. They also realized that it wasn't as geologically sound as they had hoped. There's a lot more groundwater running through and seeping through Yucca Mountain than they thought there would be. So there's a huge debacle. Basically the costs are rising, nothing much was happening. And then 2011, under the Obama Administration, he just called it quits.

ROUGE-1: 15.87, ROUGE-2: 14.21, ROUGE-L: 12.46
BERTScore: 54.88

==============================================
==================== [93/100] ====================
Summary:
Professor Steven Smith: I want to talk today about Aristotle's discovery of America. Smith: Aristotle's proposal for a mixture of oligarchy and democracy seems, in many ways, to anticipate, 2,000 years before the fact, James Madison's call for a government where powers must be separated. He says Aristotle would have been critical of the American tendency to organize into what we call political clubs. Smith says he believes when political functions become concentrated into the hands of, again, the too few hands, we risk arbitrary government. any sensible reader of Aristotle would reach is that Aristotle, in fact, discovered the American Constitution 1,500 or 2,000 years before it was written. Aristotle understands the mixed constitution as a balance of classes--the one, the few, and the many. He doesn't so much insist, as you will see in your reading, on the actual separation of functions of government, putting them into separate hands. But that leads to a further difference. We tend to think of the separation of powers doctrine as necessary for the security and liberty of the individual. to war, but in fact to peace. The citizen of the best regime, he says, must be able to sustain war if duty requires, but only for the sake of peace and leisure. Aristotle tells us he is slow to act, unless something of great importance is at stake. He repays favor with interest so as not to be under any obligations to others. He speaks his mind without fear or favor, somewhat like the New York Times, because to dissemble would be beneath him. the great-souled person or the great-Souled man. The megalopsychos, the gentleman, whatever else he is, is not a philosopher in the strict sense. The statesman, again, to the highest degree is the founder of regimes, laws, and institutions. They provide the constitutional framework within which we, later figures, operate. The qualities we will see beginning on Friday and next week that Machiavelli and later Hobbes or Locke, believe are necessary for the great founders. sense in which trainers know their animals or parents their children. Aristotle describes this political knowledge as phronimos. How is this knowledge acquired? Are we just born with it? Do some people just have it or is it a product of experience? Aristotle doesn't say, but I think the answer is clearly some of both. It is a quality, as I agree with Berlin, possessed by some of the great psychological novelists. I mention the names of some of them in this article. animal means first to possess speech or reason that allows us to participate in a community or a way of life governed by shared standards of justice and injustice. To be a political animal, for him, is to engage or not to demand more precision than the subject matter allows. But that formulation seems, in many ways, to be question begging. How much precision does the subject allow? How do we know? There will always, he suggests, appear to be something ad hoc about the methods used in the study of politics. These four questions are intended to guide inquiry, to shape and direct inquiry. They are not intended to yield sure or certain results, but to guide and inform statesmen and citizens in the U.S. The political scientist must have a grasp of the best regime, given the most favorable circumstances. He must also know something about the techniques of reform and persuasion, what we might call the area of political rhetoric by which existing regimes can be brought closer to the best. And he must have some knowledge of how to render any regime, no matter how imperfect, more stable and coherent. Most contemporary political scientists tend to be liberals. Their values are liberal values. This raises a question. Whether the relation between contemporary political science and liberalism is merely accidental or whether there is some intrinsic, some necessary connection between them. One might do well to ponder which political science is really more scientific--Aristotle's, which is explicitly and necessarily evaluative or that offers advice and exhortation to the statesmen and citizens about how to care for their regime, says Julian Zelizer. the back door. On Friday, let me just remind you, Il Principe. We'll study Machiavelli. On this very partisan note I conclude. On Thursday, we'll study the life of the Italian statesman and play a game of chess with him. We're going to see how well he can play the game of poker. I'm looking forward to it. I hope you'll join us for the game on Friday night at 8 p.m. ET on CNN.

ROUGE-1: 24.42, ROUGE-2: 21.98, ROUGE-L: 18.89
BERTScore: 59.17

==============================================
==================== [94/100] ====================
Summary:
SNLI is the Stanford Natural Language Inference Corpus-- MultiNLI, and Adversarial NLI. The premises are all image captions from the image Flickr30K data set. All the hypotheses were written by crowdworkers. The overall Fleiss kappa measured interannotator agreement was 0.7, which is a high rate of agreement. It's clear at this point, for example, that ensembles of deep learning methods are the best for this problem, says Christopher Potts. The train set is a mix of cases where the model's.pair is independently validated. So in this way, we're kind of guaranteed to get a lot of examples that are very hard for whatever model we have in the loop in this process. And so what we're hoping is that as we progress through these rounds, these examples are going to get harder and harder in virtue of the fact that the model is trained on more data and is getting better as a result of seeing all these adversarial examples.

ROUGE-1: 11.91, ROUGE-2: 11.20, ROUGE-L: 11.48
BERTScore: 61.97

==============================================
==================== [95/100] ====================
Summary:
In a perfectly competitive market, the firm is a price-taker. No matter how many units they produce, they're just going to be able to get that same market price. A firm in an imperfectly competitive market will have their own firm-specific demand curve. The price that they get in the market is higher than the marginal cost and the marginal revenue at that point. Folks are willing to pay more than that marginal cost, but you still have no motivation to produce more.

ROUGE-1: 21.16, ROUGE-2: 19.73, ROUGE-L: 21.16
BERTScore: 65.43

==============================================
==================== [96/100] ====================
Summary:
"Between the Folds" is on PBS at 8 p.m. ET Tuesday. Professor: Today, we're going to talk about infinitesimal rigidity. He says it's about the lack of motion in a graph. Professor says it can be used to tell if a 3D graph is generically rigid in 3D. "I just wanted to say goodbye to Eric Joisel, who died on Sunday, sadly," he says. "That sucks," he adds. of an informal notion-- a valid first derivative of a motion. I want to take such a motion, essentially, take its derivative with respect to time, And evaluate that derivative at time 0. This is useful because if we can ever find something that is infinitesimal and show that it's rigid, then we've determined it's infimally rigid. Question: Is a dot product [INAUDIBLE] at left or at right, that is, the terms that are identical to that at right? In two dimensions, there are four terms in two dimensions in this equation. The fancy way to write this is as a matrix equation, if you want. So we have a whole bunch of these constraints, one for every edge. And what we care about, what we're trying to solve for, is d. We know this entire matrix, because we know c. We close. This is called the nullity of the matrix. There's a fun theorem called the rank nullity theorem from linear algebra. So this is like a conserved quantity about the space of motions. it's another way of thinking about essentially everything we've seen. And if you know linear algebra, it's useful. Otherwise, I'll tell you how we can use it. OK, for this definition we need the notion of a minor. So we have some matrix. Actually, our matrices are pretty rectangular. And I just choose, let's say k columns from the matrix-- let's I'm saying is if it's non-zero for some choice of C, then it should be non- zero for this choose of C. Roland Martin: I'm going to use basically everything I proved today to prove a big theorem-- the Carpenter's Rule Theorem. Martin: If you can find a stress that is nonzero on every particular strut or cable, then all of those things are effectively bars. He says if you compute an equilibrium stress, which is easy to do by linear programming, and draw-- you can directly construct from that the 3D lifting, which would be kind of cool.Martin: It would be fun to actually see it implemented, so another fun project. Tensegrity is a generalization of a linkage and where we allow three kinds of edges. We can have cables, which is the length can decrease, but it can't increase. And there are struts, which are the reverse-- you can increase but not decrease. All these struts are preventing is compression. They don't prevent expansion. So that's the thing we want to model. This is duality. This goes back to Roth and Whiteley, 1981-- good year. I have linear equations. I also have linear inequalities. So in fact, I can write it as some other matrix R prime. Actually, it's the same matrix. R prime times d is greater than or equal to 0. This is the general form of a linear program. You can write an equality constraint in this world just by taking it and its negation. That forces them to be equal to zero. All you need to know is it there are fast algorithms to solve this, also. or you can say those edges have some negative stresses. Or another way to think of it is there's a point off to infinity, which is how this is drawn, and you can have a positive stress. And at infinity, you don't have to satisfy equilibrium. There's a little something special at the boundary, but everywhere else-- interior and all these vertices in here-- you have equilibrium, and all the stresses are positive. And it turns out in this case, you're automatically infinitesimally rigid. In general, you're going to have a lot of non-triangles. Those should all remain planar. One thing you can do-- there's some freedom here. If you have any lifting or, for example, you could not lift them at all. Set all the z-coordinates to zero. That's fine. You could also just lift everything onto some other plane, and generally have a rigid motion of freedom. You to show it can't happen. If everything's a valley, something's going to go wrong at the top picture. the chain. I could take this one, find a motion that straightens the chain. For convex cycles, it's a little less obvious, but it's also true. There's one catch which I didn't say here. I need to add outermost. When you have nesting like this, you're in trouble. This guy is not going to get straightened out. It could be super long. It may not have room to straighten out inside that convex chamber. So these guys will come along for the ride, but they won't actually getting straightened. The outermost guys will get straightening and convexified. might actually only get something around here. But again, you can't get from here to around there without some mountain. It's hard to even imagine, because it can't happen. So that's the one way it can happen. You can't have strict valleys, but you could have them all 0. So in fact, the one case where you can have stress is when you have all of this outside stuff flat. And then inside you don't know. I can prove, using this generalized lemma, that that's not possible. dimensions, and maximum degree 2. We can think about what happens with degree 3. Then you can get locked things. What happens in three dimensions? Then you Can't get locked Things. What happen in four dimensions? then you can't get Locked Things. Oh, and why is it called the Carpenter's Rule Theorem? Because this is a carpenter's rule. And in a car Carpenter's rule, actually, all the edge lengths are the same. But as far as we know that doesn't make this theorem any easier to prove. still buy them at the hardware store. That's it. You can't buy them online. You have to buy them from the store. They are not available online. They can be bought at the store, but you have to pay for them in cash. They're not available on the internet. You must buy them in the store to get them. You cannot buy them on the Internet. They must be bought from the hardware stores. That is it. They have to be bought in the stores.

ROUGE-1: 18.52, ROUGE-2: 17.04, ROUGE-L: 15.49
BERTScore: 66.58

==============================================
==================== [97/100] ====================
Summary:
Chess has been known as a tool of military strategy, a metaphor for human affairs, and a benchmark of genius. The game was originally known as chaturanga– a Sanskrit word for "four divisions" With its spread to Sassanid Persia, it acquired its current name and terminology– "chess," derived from "shah," meaning king. Chess-playing computers had been developed for decades, but Deep Blue’s triumph over Garry Kasparov in 1997 was the first time a machine had defeated a sitting champion.

ROUGE-1: 21.34, ROUGE-2: 20.34, ROUGE-L: 21.34
BERTScore: 63.36

==============================================
==================== [98/100] ====================
Summary:
presenting okay share your screen that's what I'm doing oh you are okay hopefully it is yeah stop sharing yeah you should be sharing my screen under your camera until I can decide if I click on slideshow this is still show my camera uh it does I guess I can minimize it do screen sharing are you recording tooYeah great baseball back yeah I mean it's my first time giving my lecture so I'm as good as I can be do you want that cheers I mean I have to write something to hear me okay okay you know that it works. this week will probably go live tomorrow uh not quite sure yet but you'll try to get it up as soon as possible so I guess like without further Ado let's Jump Right In. I'm gonna go somewhat into detail into what representation learning is and I think this should sort of cap out the last few weeks of deep learning um and probably give you a more comprehensive understanding of what deep learning actually is doing. We'll talk a little bit about what learning transfer is and what the benefits might be when we train a model from scratch which we don't usually do. The machine learning pipeline you start with an input X you extract all the relevant features from it and then you push those into a machine learning algorithm should get an output Y and you sort of optimize based on that. There are special feature extractors for images so this is sort of what classical machine classical CV look like. We will have an entire lecture dedicated to self-supervised learning for Envision Envision might actually be trying to predict the hidden part or property of the input from any observed or unhidden part of the image. but luckily huge models have already been trained before so the sort of question is can we leverage them in some way and the answer is yes absolutely so you might have heard of what we've called pre-trained models. Many of these pre- trained models are frequently used all the time and so we can take a look at how and why we might want to use them so if we train a model from scratch our model parameters or our weights are randomly initialized in the beginning and then we update them gradually through an optimization algorithm such as stochastic radio descent. um and the similarity of the new data set to the original data set so for example in the case one where you have um a small small data set or a lot a large data set. Since it's larger we have more confidence that we won't overfit if we fine-tune. On the other hand if we have a smaller data set even though it's similar to theOriginal model it's not a good idea sometimes to fine tune because you can definitely overfit. And then in the third case in which you have like a smallsmall data set and then it's pretty different from the first task um. to embeddings to something called a latent space so we often prefer to work with lower dimensional data. The idea is that if we can directly work with the significant somehow find a way to represent this line using just one variable instead of three and so here's let's say the XYZ coordinates that's going to be better. This also provides learning algorithms are trying to learn something much more Beyond supervised training turns out that this example is not constrained to just CV you can also do SSL with NLP. as a 70 784 dimensional Vector plus 25 to 784. so what you can do is you can train a model that would classify um what digit the image contains. Your goal is to optimize the network such that this error decreases and your model is trying to output something that is very close to the actual labels y . so in a sense your training process is receiving supervision from the labels your super your labels are guiding what the model must learn and and this whole process is called supervised learning like the name suggests. any labels into that algorithm you just feed in the data Matrix. There was there were no labeling involved you just took each audio signal um projected it down to two dimensions and clustered them with other points so yeah it turns out that dimensional eruption with PCA clustering etc etc are common examples of unsupervised learning and this is sort of. uh hopefully it will give you a clearer picture of what's going on so in the first picture your different points and they have plot labels associated with them so in a classification task you're going to predict what the labels are. anymore but it's also trying to understand what's going on in the image it's it's going to learn that an image can be made up of different parts and those parts are going to be related to each other. There are some more technical details on the slides um I won't go into those but something that the authors actually did I actually go mention this is when they sample the patches and they divide it into a grid instead of taking the grid directly they actually generate each patch a bit.  SSL can also be applied to audio it's a very broad sort of paradigm and I think the currency of the art and audio classification is Wave 2 Vector Q which I think came out a few years ago okay so what if we go back to this idea of where to work we are predicting a single word from some surrounding context. What if you predict a word from the entire sentence that it is a part of and as I think I think you might imagine that this white this might work better because the sentence will give you more context. the slide is it turns out that the current state of the art for CV is actually very similar to Burke so that's a teaser for the lecture that we discuss Advanced Techniques and as software CV. There is a homework for this entire cluster which is the high Crush notebook that should be due next Tuesday even though this lecture doesn't have homework I mentioned before that there will be a lecture on on Advanced SSL for CV and that will have a homework so to work on that homework this slide deck should be up on the website again if you feel free to do that that is it for today a second pause.

ROUGE-1: 22.89, ROUGE-2: 22.26, ROUGE-L: 21.85
BERTScore: 71.30

==============================================
==================== [99/100] ====================
Summary:
Third lecture on Foundation mulative AI. Next time we'll talk about stable diffusion image generation. We'll have two guest speakers and then we'll end with the lecture on AI ethics and regulation as well as a panel. We apply this self-supervised learning where we learn without label data so we can get as much data as we want because there's no human being in the loop. There's no limit how much we can scale this up and what we get from this by learning from observation and learning from the data directly is a very contextual understanding of meaning. Force things to comply to Simple Rules right it kind of abandons our ability to understand and compress what we're seeing and deals with that chaos directly that's why AI is so powerful and so humanlike. I took this quote from a general from the 18 and 1700s and he says this quote that P Theory which sets itself in opposition to the mind and what he meant was that he's a general so he fights in battles and War and at the time people loved to come up and theorize around War. so we have a sequence of words and and then we're just going to try to predict uh the next word based on previous words so let's say we have uh we start with i here as input. Then we want to someh predict the Target right so we know we know or the computer knows somehow by just downloading the text that what this whole sequence is but when it trains this AI model it hides part of it right so it just inputs I to the AI model and then the model is supposed to do something with it. We're going to create scores or predictions for all words in the L like in the human you know vocabulary in the English vocabulary that sounds extremely expensive and it is quite expensive and so I have different tricks to make this work.then it kind of gets it right and then you give some positive feedback back and we're going to kind of do this. Then we're going to pay pay actual human beings to score them and say are they good or not. We want them to be a little bit politically correct at least at least right and and and we don't want to rely on it as much as possible. like a a specific token that says we're happy until we complete a complete sentence for example um and this is kind of expensive to do because you have to generate one thing at a time but of course training is is much faster. How to incorporate planning is something that people talk about a lot and then of course multimodalities so it's not hard to see that this idea of predicting next word based on previous words corresponds really well to videos just to kind of predict the next frame based onPrevious frames. CTP was trained at a scale with an amount of data and parameters that we never seen before so this is a a a year old now but this is I think this was 3.5 or something the first version it was using 175 billion parameters and just training the the final model cost around $5 million just in in Compu electricity bills. Transformer has much less structure and has to relearn a lot of this structure but since we have so much data and we don't need to have labeled data we have we have. is that for every step here that's label with the same uh digit you know they can all be done in parallel so everything at step two here can be do in parallel they don't need to wait for anything. This is called a recurrent new network when we process things this in a sequential way H we try to Pary as much as possible but your current process depends on the previous the previous step and things flow forward this in kind of this sequential way right so to get from uh you know for the information from I to go to the information prob being processed step number nine basically right when you want to predict the period has to travel eight or nine steps here to to to uh be used. and it's going to be much faster to run so it's much less uh uh well that's a modification but uh it's a little bit less sensitive in a sense uh we care about both being fast uh and yeah I mean but somehow H this is going to be much much faster to train than a recurr network so you're going to get much much better performance and then the difference in deployment is less uh significant okay but during training we can do this because we not upend the words we just see them in the sequence we can doing this. just you know words there's no sequence anymore because everything is connected in Transformer but recur not there is still the sequence by how by virtue of how things are processed so how we solve this that is that for for the Transformer we're just going to add to each word a positional encoding so we just add the position again. The Transformer has to figure out if the sequence matter it should use that information even but it now has that information at its disposal because we're going to encode a sequential structure. It's like seeing all the words in a book at the same time like it's fast but it's very confusing. if you read a book or you watch a movie if you want to understand the end part it might be good to kind of go back and look at the the start starting part of the book or something you know or or it's good if you remember that information but probably you know if you don't remember you have to go back to look it up. In a Rec Network Rec Network because we're processing things uh sequentially here so to for something to be used like to for for information about the first uh word in the sequence to be use the last step here. good or bad dialogue so we've solved that okay and the last two problems we are going to solve by using reinforcement learning so what is reinforcement learning well we talked about this a little bit before but uh something is very important and characteristics of reinforcement learning is this delayed feedback. If it doesn't have data run a context or about you and your interest as a person it it won't be able to tailor to you right they cannot create Magic out of thin air it can only do the best of The Prompt and knowledge has so far. is about like how do you figure out what actually helps you reach your goal and optimizing your score function even if it's delayed um okay so another thing in doing this that's very very important it's exploration versus exploitation so let's say now basically that our model has seen these two different cases and have received two feedbacks right. In one of these you will got a pretty good score and in one uh you know in the lower here you got apretty bad score. The robot that just H try to replicate the the human beings putting scores on the answers that we generated prompts. gratification actually leads to very non- GRE and and independent robustness so these are the consequence of applying reinforcement learning where you only get feedback at the very end so there's less you know supervision right you're more on your own. H deal with an uncertainty of not having constant feedback you have to figure out things by yourself which leads to you being more robust and also again in reinforcement learn here the only thing we care about is the signal at the end so we don't care about making the best next step. We care about optimizing the whole output so we're now addressing these things. The Transformer is based on how a normal person learns right when you're learning a new content you to Rel it with everything else you know yeah so I'm just I mean this yeah okay uh so the question is basically is the Transformer inspired by research about how we our kids learn right how the brain works yeah I mean you're going to find a lot of work around you know making those connections and uh then there's a huge debate in in like the Deep Learning Community is that is that actually true is it kind of wishful thinking and in hindsight we make this connection right. the World by looking at videos right you can even sort to understand how human beings work even better because you can see people being upset or sad or happy whatever right in in a video and start picking these cues up. You can connect the vision part to the text part and get a multimodality model that's able to do both in a really really sophisticated way uh also something that I think these these people are working on all right thank you thank you for your time and good luck with your book.

ROUGE-1: 23.68, ROUGE-2: 23.03, ROUGE-L: 21.58
BERTScore: 64.22

==============================================
==================== [100/100] ====================
Summary:
The EM algorithm is an unsupervised version of k-means GMM. In GMM, you guess randomly an assignment of every point to the cluster, the probability. You guess where they're probabilistically linked, that is, what's the probability of these points belong to cluster one, this point belongs to cluster two. And then once you have that, you then solve some estimation problem that looks like a traditional supervised learning. The decomposition is quite important. And we're going to try and kind of abstract that away. These weird pictures? The technical detail is I want to make sure that you understand this key result. And the reason is-- I'll refer to this thing as we go through. We're going to use it. It's not like I'm just teaching you something for your health. Like, this is actually going to be used in the next step. It will actually, in some sense, be the entire algorithm. So it'll become a clear point where we apply Jensen's inequality, where we make it tight. A function is going to be convex if its graph is, OK? As I said. OK, so let's draw an example of this. It's a parabola, kind of a bowl-shaped function, right? Now, no matter how I pick the points-- and clearly, I should really only have to worry about picking on the edge. So this is now z, f of z. This is f ofZ. Does that out a Taylor series for this f double prime-- see the a, a minus z square? If f is twice differentiable and, for all x, f double prime of x is greater than 0, then f is convex, OK? So this says these functions really are bowl-shaped, right? Second derivative being positive means that they have this kind of positive curvature that looks like the U's. Their first dimension-- first derivative goes up and down, but they're kind of always trending. That first derivative is always getting more positive. It's negative on the left-hand side, positive on the right-handside. That's what it means by bowl- shaped. conceptualize it is we solve for some hidden parameter. We solve, and that gives us an entire family of possible solutions. We take a log of the probability that we assign to the data given our parameters. The best way to understand it is just to run it through a couple of the different examples. EM and the next one-factor analysis-- and by the end, you'd be like, oh, OK, that makes sense. It's a lot of notation because we're abstracting out a huge number of things that we're doing. structure. P(x; theta)-- this is a generic term, right? This is just one of the i terms-- says the function factors this way. Looks like a sum over z, where z is our hidden or latent variable. Q(z) has to be related in some way to P x of z for this to work. What is c? Ah, what is z or c? Oh, c is some constant independent of the-- just for some constant c. We don't care what its value is. We just care that it doesn't depend on z. The algorithm is based on the fact that Lt of theta is always below the loss, so it's kind of a surrogate that I'm not overestimating my progress, and it's tight. So if I did happen to have the actual optimal value, it would meet at that point. So I wouldn't think and get fooled that there was a higher loss function somewhere else. But right now, I have a way of going term by term from the likelihood function and getting lower bounds at a particular spot. But I have to pick a certain Q to make this operational. That's the piece. Curve, L of t. And then the M-step, and together, EM, set phi t plus 1 equal to argmax phi Lt(phi). Cool. Just could you reiterate? Like, why are we not using gradients on the original turbulence? Right, so we could imagine doing some kind of gradient descent here, but it's not clear how to deal with this marginalization that happens in the middle. So we'll come back to that claim in a second. be thinking because I told you that Jensen's will have something to do with this. What we're going to do to put it in the form where Jensen's could be used looks wholly unmotivated, OK, totally unmotivation. But it's to shoehorn into what we're doing, and there's some motivation, but it's kind of opaque, let's say. So I'm going to pick Q as a probability distribution. I'll write that in a different color. OK, why? Because now I can make my argument one line. and x. So we're going to have this notation, Q(i) of z because it depends on each different value. So each data point is going to get its own different Q, which is the log of how likely this thing is, OK? And we picked those for each i. So the ELBO of x, Q, z equals x,Q, z. OK? This thing has a very famous name, so I'll write that while I kind of stall for more questions. What we've defined here is called the Evidence-based Lower BOund. sequence that is monotonically increasing or nondecreasing, OK? So it's possible that it would grind to a halt. But eventually, it has to be strict. And so to derive a counterexample, you would just find a likelihood function that had those two bumps. And you would run it in that particular lower bound setting. And what it will do is it will gradually hillclimb. And this is actually not great. Like, it can't go back downhill, right? It's got to just continue to go up. phi 2 was hugely bigger than phi 1, right-- a billion points came from the second source, and only one point from the first source. We'd probably say that it's more likely that it would go to this, right? It would certainly boost its probability. So to automate this, this is Bayes' rule. It just weighs those two probabilities and tells us what should happen. That's it. We ran through exactly those calculations last time. All right, let's take a look at the M-step. equal to the sum over j-- because now I'm summing over the cluster centers, right? The z(i) notation was still very abstract-- wj(i), which was summing. over this part here, log-- and help us all, 1 over 2 pi-- this is a covariance, 1/2. This is the exp of Oh, I decided to write this in four general things. Why do I care about that? Oh,I see why. OK. Transpose sigma inverse x( i) mu j times phi j. Oh, that hurts. When you have a constrained probability distribution, you have to use a Lagrange multiplier. In this case, it makes total sense, though, because these numbers have to sum to 1. So if you don't have a normalization constant here, you're adding up a bunch of numbers which individually sum up to n, right? The sum over all of them is n. And this is just the principle that tells you, you must normalize them by this n factor, OK? So all I care that you take away, if you've seen this a thousand times, is that you understand. do this when you have something that sums to 1. It's not more complicated than what I wrote here, but make sure independently you go through it and ask questions. In the next class, as I said, what we're going to see is this notion of factor analysis. And that is going to tell us how to apply EM to a different kind of setting, which, at first glance, will look kind of impossible to do without a latent variable model. And I think that's all I want to say.

ROUGE-1: 16.63, ROUGE-2: 16.07, ROUGE-L: 15.01
BERTScore: 63.69

==============================================
