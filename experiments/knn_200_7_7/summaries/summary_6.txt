TZ: I want to just review um convolutions and and the architecture of a CNN to make this more clear and put it into perspective how it relates to just standard um dense neural networks. TZ: There's only really like one architecture that you really need to take away from here which is going to be resnet we'll get to that. The rest of them like if it goes over your head like don't worry it's it's fine this is just more for people who who want to know. The convolutional layer will have a bunch of filters right and all the values inside our filter are learned and we also have a bias term that gets added to the output of moving each window on each location of our input we refer to it as a volume simply because it sort of looks like a cube. If you don't want to do a striving convolution a very simple way to do it is to just look at individual little squares and just take the max in this whole area in each one of these areas. again you can kind of think of this as like a layer of a neural network so after we're done doing this we're still going to add our activation function like array Loop or or something else. We want to go deeper right um only having five convolutional layers and three dense layers really limits the amount of information we could synthesize by our feature Maps. So we want to be able to have our model learn higher order feature maps and by that I mean low level features like edges things like that. Alexnet in 2012 was a a big groundbreaking feat in that it proposed stacking layers of convolutions Max poolings following it up by fully connected layers and coming up with not a very deep network. This kind of uh turned a lot of heads when it achieved a around 17 error rate on imagenet and it won in 2012. Understanding the motivation behind these really sets the stage for uh the the future of advancement in this field so yeah convolutional Nets were proposed in 1990 by Yen lacun and he he kind of pioneered that pattern for today. This is an example with vg16 so this is 16 layers um with three dense layers at the end as well. As you go through your convolutions your output Dimension is shrinking. Based on your kernel size your actual image is shrinking but your number of channels increases as you gain information. The motivation behind it is you're applying a linear function that doesn't lose information as you go across better radio right um yeah exactly that's why the size won't change the important thing the channels change though. soft Max will scale more logarithmically um and it'll give you a final like probability map um are there any questions on Alex now actually before uh we move on yeah what's up yeah so uh if you don't specify a certain type of padding valid padding is going to be applied to make sure that as you're sliding your kernel across an image uh you're left with the same dimension is there anything you want to add Jake or no that's I I should have mentioned adding two yeah I mean you did a good job. a lot of one by one convolutions are used to only change the number of channels and not modify the input size. These are deep and wide to capture features at different scales. depth wise convolutions as well as point-wise convolutions these are combined to form a more efficient computationally method. Mobile nuts are very cool in that you're you're using depth wise Convolutions and Point-wise Convolutions to reduce the computations that you are you're doing. The efficient that's that will be talked about as well. of result from just blindly stacking layers um so you might ask uh we saw Alex Ned and we saw bdg which was like basically the same thing but we added a couple more layers why can't we just infinitely stack these layers um and there are a lot of problems that come with that that Inception and resonance um try to fix and that is adding residuals yeah uh what do you mean by branched uh yeah the classifier is at the end uh by this uh again there's there's a drawing for it but uh essentially you have a multi-headed yeah. Vanishing gradients is a common problem as you add a bunch of layers stacked together and that the learning signal or the gradient computation becomes extremely weak the model struggles to learn. adding skip connections makes the identity easier to learn because you're quite literally adding a previous identity to the resultant of a transformation. If your layer is a convolution the dimension can change which is why often this is result like kind of viewed as f of x plus W of X where W is a transformation that you do on X two to make it the same Dimension. bit smaller than one immediately. At a certain point at a certain number of multiplications your partial derivative uh your your chain rule that you've gotten as a result of many many multiplications just gets sent straight to zero. If your values if all of your individual little partial derivatives of all the individual little steps are a little bit bigger than one it's going to explode um and that's just not helpful [Music] um it's problematic uh we would like yeah we would want to update our weights in way that is like sort of uh sort of regular. of identity as you go through your uh your network if that makes sense but that was a very good catch on the dimensions of X yeah this is often viewed as plus W of x there any other questions about resnet all right dope uh so the next thing to talk about is global average pooling um which is designed to replace fully connected layers in cnns. This is also used in resnet in replacement of the fullyconnected layers um you're generating a feature map for each category of the classification task and then that is fed into the softmax layer. This is extremely important when we want to Stack a lot of filters or a bunch of layers. So you're applying this one-dimensional filter to each channel of your image that is applied to to each one. These are then concatenated together and you apply a smaller pointwise convolution one by one by the number of channels you have to end up with a feature. So mobilenet has a lot fewer parameters which results in a lot faster convergence time. It matches Inception of D3 accuracy just by using depth and point wise convolutions. can quickly go over like squeezing anxiety networks basically uh you squeeze you apply this through a couple of dense layers and then you rescale so we talked about global average pooling. I think the slides are pretty good and compressed in a very visual way uh the remainder of the piano more impressive art it's actually the other way. I I hope the main takeaways are that you'll like understand those rules and that you you see that we've like adding all of these different sort of like tools to your tool belt now. and mobile nuts for Edge Computing and things like that. You want to drastically reduce the number of computations that you want to do yep that is basically everything for today thank you guys for coming oh and there will also be a quiz there will Also be a quizz at least is there a homework position closer or not [Music] um and do next Friday [ music] thank you for sure. And next Friday we'll be talking about how to use Edge Computing in the real world. We'll also talk about how you can use Edge computing in the classroom.