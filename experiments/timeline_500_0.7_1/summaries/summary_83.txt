Coded imaging is a co-design between how you capture the image and how you process the image. The concept of a position or superposition applies to all three types, shadows- or refraction- or reflection-based techniques. We'll see how-- we already have some projects that are inspired by biological vision. And we'll see it in a taxi zipping very fast, which is a clever way to take a photo. And I believe Santiago-- where's Santiago? Oh, yeah, his triangle-- the piston. The problem is that lower frequencies are actually being set to 0, which means that in this photo, these frequencies are missing altogether. And there's nothing you can do to recover those frequencies because in the Fourier domain, all you have to do is multiply each of the frequencies in the image by the amplitude of the Fouriers transform of this. So the culprit here is really this low pass filter where some of the even lower frequencies were also being nullified. And if I tried to recover from this photo there is no chance because I have already attenuated and have lost all those frequencies. box function, which is equivalent to-- when you release the shutter, opening the-- release your shutter button. Instead of keeping the shutter open for the entire duration, you open and close it in a carefully chosen binary sequence. Here, it's closed for quite some time, open for a short time. And so on. At the end, you still get just one photo. But now something magical has happened because first of all, if you look at this number one, you'll see that it's not the same as before. that's your 1010 inquiry. Instead of keeping the shutter open for the entire duration and getting a well-exposed photo, the shutter is open for only half of the time. The support for the representation of the Fourier domain of that function that you describe there is infinite, right? So you actually truncate this in order to-- RAMESH RASKAR: It's not infinite because you still have some width. But with a vector length of 52, this filter is in time. filter from space? RAMESH RASKAR: It corresponds automatically to filter in space. So your actual blur in the image may not be exactly 52 pixels. It might be 10 pixels or it could be 100 pixels. And you're saying that it also depends on how far the object is in space because faster-moving objects. So whatever is moving has to move at a constant speed. If we did 100 milliseconds, it picks up speed, then your assumption that the 52-length vector will map to some stretched or shrunk version of 52. point spread function for your time of flight. So that's the same concept here. You just want to call leading the world, take a picture, and see how it works. And this whole field of order dimension is basically engineering of the point spread function. So we're going to engineer activity of the camera. So in this particular case, a point that was moving created a blur like this. And by engineering the time point spread. function, it stops looking a bit like that. It's going to look like fashion [? wise. ?] The goal of coded imaging is to come up with clever mechanisms so that we can capture light. The circuit is very, very simple. You just take the hot shoe of the flash, and it triggers. When you lose the shutter, it triggers the circuit. And then you just cycle through the code that you care about. What can we do for defocus blur that is for motion blur? What can you do fordefocus blur? We, again, want to engineer the point spread function. An alligator came out of the water, and he lost his balance, and the boat flipped upside down. He managed to flip back in. But it completely damaged his camera that was with him, and it just wouldn't work. So he just took out his lens, which is a standard Canon lens. And he said, let's open it all the way. It had all the mud in it and so on. And then he just showed me this thing as is. It's amazing the way it's structured, right? and create one single center of projection for normal cameras. For professional lenses, that's not true. For normal cameras, you have the central projection. But again, conceptually assume that all the rays are going through that point because you can replace this whole thing by one single lens in a [INAUDIBLE]. So finding that plane is actually a tricky problem. But in retrospect, it's very easy. If the lens makers are putting everything there, we should put a recorded aperture also in the same plane. But placing it over there, it turns out you get the same blur. around at the same time of how to make this happen. RAMESH RASKAR: So in 1D, this is what we saw, right? Its Fourier transform is flat. So there are 52 entries here, and almost all of them are the same. Now we're saying, think about the problem in 2D. And what's the Fourier transforms of this? So first, for this one, the Fouriers transform is-- as we see, it's black. And then if you take that in 2d-- so how is the code? I'll give a hint. the values will be constant. So if we're placing a broadband code, certainly we have an opportunity to recover all the information. In communication theory, everything is [INAUDIBLE].. We think about carrier frequencies of radio stations in frequencies. And convolution, deconvolution-- much easier to think in frequency domain. Although all the analysis in the frequency domain, at the end, the solution is very easy-- just flutter the shutter or just put a coded aperture. Extremely simple solution to achieve that. that, or just a software. There are methods you can employ. You need to find this 7-by-7 pattern or even the previous case, the 52 pattern. Take a Fourier transform to see if it's flat. If it's not flat, you go to the next one. So 2 the 52 is pretty challenging. But even if you use a cluster, it's still a pretty big number. So you can start with some code and do a gradient descent and so on. good solutions for 2D. But for 1D, there are some really good solutions to come up with that. For 2D, for certain dimensions, they call it one more 4 or three more 4 because prime numbers can be 1 or 3. And there are certain sequences that are beautiful mathematical properties, of which sequences could have broadband properties and which may not. So it turns out you cannot really use the broadband code here either to give you the best result. But the traditional code's called MURA code, M-U-R-A. In astronomy, you have circular convolution because they use either two mirror tiles and one sensor or one mirror tile and two sensors. If you tile aperture, you'll get really horrible frequency response, unfortunately, because if you put two tiles, that means certain frequencies are lost. In this photo, those frequencies are not lost because all the frequencies are preserved. But that's because our eyes are not very good at thinking about what the original image could be, given either this one or the previous one. Ramesh Raskar: Coded imaging is elegant and beautiful and sometimes complicated. He says there are many ways of engineering the point spread function. RaskAR: For any continuous code, there is a corresponding binary code that will do an equally good job. "It's just one of those things. It's like we are sick of it, so we don't want to do it, but I think it's worth trying," he says of orthogonal motion blur. The effect is very low, though, remember. The effect is extremely low. So maybe you have a pixel and get blurred by 10 pixels or [INAUDIBLE]. It's not a global effect. So this picture, maybe-- this particular diagram is misleading because it seems like this point is going to go all the way. But this is very narrow. And the colors are, for all practical purposes, that'd be the same. So we're painting the rays. We're just adding one glass. blur is only about 10 pixels, no matter where you [INAUDIBLE]. So maybe that was the matter. If you have a point of access, it's still going to create an image that's blurred 10 pixels. This is, again, very counterintuitive, where you go to make the image intentionally blurred. It's just that it's blurred everywhere. And then we also saw this one very early on, where the point spread function-- typically when something goes in and out of focus, it looks like a point. Ramesh Raskar: Compressed sensing is taking a photo and compressing it. He says the idea is to take a single photo and then recover it in a compressed way. Rasksar: If you're on 2 megapixels, then you need to take 2 million [? pics] All right? So the claim this group made at Rice University was that if I wanted a million-pixel image, I don't have to really take a million readings, he says. Rasa: I can take this picture effectively with just 10,000 pixels but recreate a million pixel image. Photography is a record of visual experience, which is great for humans, but it's not so great for computers because computers don't understand any of that. Computational photography is a very, very active field. The secret of success for film, of film photography, is that if somebody had given you this problem before the invention of film, that there is a scene-- and I want to give you a sensation of the same scene-- time shifted or space shifted. You can start with a reproduction of a photo, or you can tap into retina, V1, V2. RASKAR: But the benefit of tomography, which we studied in the last couple of lectures, is it's a very high-dimensional signal. And so usually, in a high- dimensional signal, there's lower sparsity. If you think about taking a CAT scan of your body, there are only like four or five types of materials. So compressive sensing allows you to take less measurements. But the problem is you need to actually have more information about the scene before you take the measurement. you don't know anything about the scene, you take very few measurements. Once you take its transform, some transform, it's very sparse. It can be represented in a complex place. So tomography is the same. It's 4D capture for 3D representation. OK, so I'm sorry we're not taking a break. Should we take a 30-second break before we move on to two very small topics, which is how to write a paper and wishlist for photography.