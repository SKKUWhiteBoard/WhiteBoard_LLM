{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================= [built-in modules] =======================\n",
    "import os\n",
    "import time\n",
    "\n",
    "# ====================== [third-party modules] =====================\n",
    "import yaml\n",
    "from box import Box\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from bert_score import score as bert_score\n",
    "from rouge_score import rouge_scorer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ======================= [custom modules] =========================\n",
    "from utils.eval_similarity import *\n",
    "from utils.utils import *\n",
    "from utils.segment_embedding import *\n",
    "from utils.concat_functions import *\n",
    "from utils.summarizer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    config = Box(config)\n",
    "print('Experiment name:', config.experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading data... \", end=\"\", flush=True)\n",
    "if config.data.source == 'opensource':\n",
    "    datasets = load_dataset(config.data.opensource)\n",
    "    indices = np.load(f'data/gov_indices{config.data.index_set}.npy')\n",
    "    datasets = datasets['train'].select(indices)['report']\n",
    "\n",
    "elif config.data.source == 'youtube':\n",
    "    datasets = load_dataset(config.data.youtube)\n",
    "    indices = np.load(f'data/ytb_indices{config.data.index_set}.npy')\n",
    "    datasets = datasets['train'].select(indices)['content']\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Experiment dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir_path = os.path.join('experiments', f'{config.experiment_name}')\n",
    "if not os.path.exists(save_dir_path):\n",
    "    os.makedirs(save_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_score = 0\n",
    "best_summary = \"\"\n",
    "\n",
    "evaluation_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for di, text in enumerate(datasets):\n",
    "    print(f\" ----------------- [{di+1}/{len(datasets)}] ----------------- \")\n",
    "    init_s = time.time()\n",
    "\n",
    "    # ========================== [Segmentation] ========================\n",
    "    print(\"Segmentating... \", end=\"\", flush=True)\n",
    "    s = time.time()\n",
    "    segments = segmentate_sentence(text, **config.segment.args)\n",
    "    e = time.time()\n",
    "    print(\"Done\", f\"{e-s:.2f} sec\")\n",
    "\n",
    "    # ========================== [Clustering] ==========================\n",
    "    print(\"Clustering...   \", end=\"\", flush=True)\n",
    "    s = time.time()\n",
    "    concat_indices = globals()[config.concat.method](segments, **config.concat.args)\n",
    "    e = time.time()\n",
    "    print(\"Done\", f\"{e-s:.2f} sec\")\n",
    "\n",
    "    max_group_size = max([len(group) for group in concat_indices])\n",
    "    avg_group_size = np.mean([len(group) for group in concat_indices])\n",
    "    print(f\"Num. of Cluster: {len(concat_indices)}, Max group size: {max_group_size}, Avg. group size: {avg_group_size:.2f}\")\n",
    "\n",
    "    # ========================== [Ready to summarize] ==================\n",
    "    batch_clusters = [\n",
    "        \" \".join([segments[gi] for gi in group]) for group in concat_indices\n",
    "    ]\n",
    "\n",
    "    # ========================== [Summarize] ===========================\n",
    "    print(\"Summarizing...  \", end=\"\", flush=True)\n",
    "    s = time.time()\n",
    "    if config.mini_batch.size > 0:\n",
    "        mini_batch_size = (len(batch_clusters)\n",
    "                           if len(batch_clusters) < config.mini_batch.size else\n",
    "                           config.mini_batch.size)\n",
    "\n",
    "        batch_summaries = []\n",
    "        for i in range(0, len(batch_clusters), mini_batch_size):\n",
    "            mini_batch_summaries = summarizer(batch_clusters[i:i+mini_batch_size], **config.summary.args)\n",
    "            batch_summaries.append(mini_batch_summaries)\n",
    "        batch_summaries = \" \".join(batch_summaries)\n",
    "    else:\n",
    "        batch_summaries = summarizer(batch_clusters, **config.summary.args)\n",
    "    e = time.time()\n",
    "    print(\"Done\", f\"{e-s:.2f} sec\")\n",
    "\n",
    "    # ========================== [Evaluate] ============================\n",
    "    print(\"Evaluating...   \", end=\"\", flush=True)\n",
    "    s = time.time()\n",
    "    \n",
    "    rouge1, rouge2, rougeL = calculate_rouge_scores(text, batch_summaries)\n",
    "    b_score = calculate_bert_score(text, batch_summaries)\n",
    "\n",
    "    # scale score * 100\n",
    "    rouge1, rouge2, rougeL = rouge1*100, rouge2*100, rougeL*100\n",
    "    b_score = b_score * 100\n",
    "\n",
    "    e = time.time()\n",
    "    print(\"Done\", f\"{e-s:.2f} sec\")\n",
    "    \n",
    "    print(f\"=> ROUGE-1: {rouge1:.2f}, ROUGE-2: {rouge2:.2f}, ROUGE-L: {rougeL:.2f}\")\n",
    "    print(f\"=> BERTScore: {b_score:.2f}\")\n",
    "\n",
    "    # ========================== [Post-process] ========================\n",
    "    if b_score > max_score: # score는 대소비교 가능한 1가지 방식을 이용\n",
    "        max_score = b_score\n",
    "        best_summary = batch_summaries\n",
    "        best_index = di\n",
    "        # 원본 텍스트의 index는 indices[di]로 찾을 수 있음\n",
    "    \n",
    "    evaluation_results.append({\n",
    "        'rouge1': rouge1,\n",
    "        'rouge2': rouge2,\n",
    "        'rougeL': rougeL,\n",
    "        'bert_score': b_score\n",
    "    })\n",
    "    print(f\"Total: {time.time()-init_s:.2f} sec\")\n",
    "\n",
    "    # append summary and scores to text file (cummulative)\n",
    "    # if there is no file, create one\n",
    "    if config.save_summaries:\n",
    "        with open(f'experiments/{config.experiment_name}/summaries.txt', 'a') as f:\n",
    "            f.write(f\"==================== [{di+1}/{len(datasets)}] ====================\\n\")\n",
    "            # f.write(f\"Original text:\\n{text}\\n\\n\")\n",
    "            f.write(f\"Summary:\\n{batch_summaries}\\n\\n\")\n",
    "            f.write(f\"ROUGE-1: {rouge1:.2f}, ROUGE-2: {rouge2:.2f}, ROUGE-L: {rougeL:.2f}\\n\")\n",
    "            f.write(f\"BERTScore: {b_score:.2f}\\n\\n\")\n",
    "            f.write(\"==============================================\\n\")\n",
    "\n",
    "print(\"===============================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save experiment result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving evaluation results... \")\n",
    "\n",
    "save_dir_path = os.path.join('experiments', f'{config.experiment_name}')\n",
    "if not os.path.exists(save_dir_path):\n",
    "    os.makedirs(save_dir_path)\n",
    "\n",
    "# Copy config file\n",
    "os.system(f'cp config.yaml {save_dir_path}')\n",
    "\n",
    "# Make README.md\n",
    "with open(os.path.join(save_dir_path, 'README.md'), 'w') as f:\n",
    "    f.write(f'# {config.experiment_name}\\n')\n",
    "\n",
    "# Save best summary & index\n",
    "with open(os.path.join(save_dir_path, 'best_summary.txt'), 'w') as f:\n",
    "    f.write(f\"Best index: {best_index}\\n\\n\")\n",
    "    f.write(best_summary)\n",
    "\n",
    "# plot evaluation results\n",
    "metrics = list(evaluation_results[0].keys())\n",
    "data_by_metric = {metric: [sample[metric] for sample in evaluation_results] for metric in metrics}\n",
    "\n",
    "statistics = {}\n",
    "for metric, values in data_by_metric.items():\n",
    "    statistics[metric] = {\n",
    "        'mean': np.mean(values),\n",
    "        'var': np.var(values),\n",
    "        'min': np.min(values),\n",
    "        'max': np.max(values)\n",
    "    }\n",
    "\n",
    "# print and save statistics in results.txt\n",
    "for metric, stats in statistics.items():\n",
    "    print(f\"{metric}: mean={stats['mean']:.3f}, var={stats['var']:.3f}, min={stats['min']:.3f}, max={stats['max']:.3f}\")\n",
    "    with open(os.path.join(save_dir_path, 'results.txt'), 'a') as f:\n",
    "        f.write(f\"{metric}: mean={stats['mean']:.3f}, var={stats['var']:.3f}, min={stats['min']:.3f}, max={stats['max']:.3f}\\n\")\n",
    "\n",
    "for metric, values in data_by_metric.items():\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.hist(values, bins=10, edgecolor='black', alpha=0.7)\n",
    "    plt.title(f'Distribution of {metric}')\n",
    "    plt.xlabel(metric)\n",
    "    plt.ylabel('score')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.savefig(os.path.join(save_dir_path, f'{metric}_histogram.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
