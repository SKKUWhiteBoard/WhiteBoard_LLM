then we are going to continue with the second part of the lecture today which focuses on the problem what actually happens if the gaussian assumption that i have about my constraints doesn't hold. One of the questions actually how to handle that so as we said what we are doing here we are minimizing the sum of the squared errors terms and as we have seen so far this is the same or strongly related depending on how you formulate that. If we would have the possibility to integrate a multi-modal distribution here that would actually be a nice beneficiary. If you have structures in the environment and there's a lot of clutter in the scene the clutter even if it has a repetitive pattern may lead to a multimodal belief about what the relative transformation between two poses let's say. It's not always easy to get this information out of your gps because typically it runs the kalman filter internally or most of the gps devices do that so they get a gaussian belief is screwed up but if you get the raw measurements you may be able to do better by allowing for multi-modal distributions. over here so this is a single constraint you can already see that you don't have a straight wall over here anymore so it's kind of bended a little bit like this due to the single constraint which obviously has a really really large error so the least square error minimization at the error is squared error term tries actually to minimize that if we add i think whatever and two three four five i think there were 10 constraints 10 wrong constraints the map actually gets so distorted that is unusable for navigation. and in this case screwed up the measurement how can we incorporate that into the graph based slam approach um so the problem that we actually have is if we look to our um an individual constraint so the lack of an observation given the given the current configuration of the nodes was a gaussian distribution. If we have this this number of constraints is the product of gaussian distributions if you compute the log likelihood this turns into a sum of the exponents exponents therefore we have these sum of squared error terms. The key trick is to simply ignore all of them except the most prominent one. If the if the means of these gaussians are far apart from each other the approximation error isn't that big if they are near each other disaster or disaster but may have big errors. The system can swap between different modes and therefore the optimization in one iteration takes into account only one mode of the gaussian as you can switch the modes it is you still have the ability to deal with multimodal constraints if i do that. is a bimodal distribution for the inline and the other one for the outliers the red is in leia blue's outlier you can also handle those cases say i'm either there or there or somewhere else. In most cases actually the vehicle executes what you tell the vehicle to do but in some cases simply doesn't move so this max mixture idea is actually a pretty easy idea pretty simple idea just reply funny no one has done that in robotics until recently a few years ago the first one was edwin olsen and pratik. that's actually a nice thing so um another thing is it can handle both things at the same time data station errors as well as multimodal constraints. So the combination of outlier rejection and dealing with wrong data associations is actually kind of nice we also can do this obviously in 3d. So this is again this data set with the sphere that we have seen before robot moving in a virtual sphere with constraints so this is gauss newton and this is the max mixture gaussNewton and um so you can see here there's a non-perfect alignment in here. Just replace this information matrix here with a variant which has a scaling factor so a constraint dependent scaling factor added to that. This leads to the case that constraints which are far away from what we expect have a smaller influence on the optimization. There's actually closed form you can derive that under certain properties where you end up with this operation. The key idea the intuition behind that is if i have a constraint which have a large error so where the um the current configuration is far from what the constraint tells me just reduce the uh or increase the uncertainty that is associated to that so decrease the information matrix. in this area kind of the the core center of attraction both both perform equally well because there's no scaling involved but the further you move out the more the red curve gets scaled. So the the error is weighted down the further i'm away however we still have a linearization point. So if i compute the jacobian over here i still has a jacoobian which drags the system into the right direction so even if i initialize that quite far away i still get kind of pushed into theright direction if you have a small video. Max mixture as well as for dcs is that kind of the tails of this gaussian distributions contain too few probability mass they're too close to zero. Outlier which is really far away from the current estimate the whole mole is tracked in this direction. If you have constraint which introduce large errors these are these outliers. This can actually screw up the optimization when computing the minimal error configuration so one way you can do is or fits into the framework of its so-called robust m estimators which intuitively say we don't assume a gaussian distribution. now you get different properties in the optimization so if you use um if r is just the quadratic function then we exactly have the original problem that's what we minimized x of minus uh squared error so if we have this one we have we examine exactly in the gaussian world. Now there are different techniques how we can actually address that one thing is we could take simply the absolute value so we don't square it just take theabsolute value of the error that's not the parabola that we have but the absolute function. here that by changing this function you can't get much better behaviors kind of deciding which function to use for the underlying optimization problem is not on it's not always an easy and easy choice so this requires some expert knowledge some good intuition on coming up with the way with one of those functions. Next week which is the last week of the term i will briefly talk about front ends and give kind of a short summary on what typical front ends exist obviously we're not going to all the details as we did that here.