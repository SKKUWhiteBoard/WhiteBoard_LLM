then we are going to continue with the second part of the lecture today which focuses on the problem what actually happens if the gaussian assumption that i have about my constraints doesn't hold. As you will see in some small examples having this outliers in your optimization problem is something which hurts dramatically which actually screw up your solution. Already a few outliers can lead to a environment model which is completely unusable for doing any navigation task so where the geometry of what you computed doesn't fit to the real world geometry anymore and one of the questions actually how to handle that. just replace this information matrix here with a variant which has a scaling factor so a constraint dependent scaling factor added to that. This leads to the case that constraints which are far away from what we expect have a smaller influence on the optimization so we can actually visualize this. The further you move out the more the red curve gets scaled so it gets kind of fatter and fatter  and better. The problem that we have in this gaussian distribution and this holds for  the point is actually away there's one technique which you can find which is also quite easy to implement. Max mixture as well as for dcs is that kind of the tails of this gaussian distributions contain too few probability mass they're too close to zero. If you have constraint which introduce large errors these are these outliers this can actually screw up the optimization. If we have one outlier which is really far away from the current estimate the whole mole is tracked in this direction that's the problem that we have with a gaussian distribution. The max mixture approach is actually kind of similar to this corrupted gaussian so if you plot both of them together.