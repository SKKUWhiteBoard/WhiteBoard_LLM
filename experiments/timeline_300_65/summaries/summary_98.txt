presenting okay share your screen that's what I'm doing oh you are okay hopefully it is yeah stop sharing yeah you should be sharing my screen under your camera until I can decide if I click on slideshow this is still show my camera uh it does I guess I can minimize it do screen sharing are you recording tooYeah great baseball back yeah I mean it's my first time giving my lecture so I'm as good as I can be do you want that cheers I mean I have to write something to hear me okay okay you know that it works. this week will probably go live tomorrow uh not quite sure yet but you'll try to get it up as soon as possible so I guess like without further Ado let's Jump Right In. I'm gonna go somewhat into detail into what representation learning is and I think this should sort of cap out the last few weeks of deep learning um and probably give you a more comprehensive understanding of what deep learning actually is doing. So I guess before we jump into deep learning let's talk a bit about shallow learning. might not be something that you can so let's say that you're working on a problem of predicting the price of a house from a house so your X can be a house you can really put that into your model. Different kinds of problems will have different kinds of feature extractors so if your data is arranged in say a table you could maybe say that each row is a single house and each column is one of the features. This process of choosing the right features can get really complicated really fast and this is also a compromise solution in the sense that you are learning the weights of your model but you're still hand programming the feature extractor yourself. what if you combine them and that's exactly what a neural network is a model that combines with feature extraction and output prediction and it learns everything from the data so hopefully it just sort of gives you some context as to why deep learning has been sort of taking off and classical machine learning is not as used and areas like Vision anymore because deep learning allows you to sort of automate this entire process from end to end and in a sense what you're really doing is you're learning a representation of your input right so your features are a way to represent what that input looks like. When we train a model from scratch it takes a lot of time compute and training data so to just to give you an idea of how much data is often necessary to train a pretty good model even just a few thousand examples is often not nearly enough. But luckily huge models have already been trained before so the sort of question is can we leverage them in some way and the answer is yes absolutely so you might have heard of what we've called pre-trained models and many of these models are frequently used all the time. to embeddings to something called a latent space so we often prefer to work with lower dimensional data. A common task is transforming them in from high dimensional data into lower dimensional instructors so we can capture this through something called latent space of features. One way is to learn and embedding as part of our neural network for our Target task so this sort of allows us to get in a bedding that's nicely customized for our particular task but it may take longer than training the embedding separately. as a 70 784 dimensional Vector plus 25 to 784. so what you can do is you can train a model that would classify um what digit the image contains. Once this model has been trained this the weights must have learned something meaningful right. So this means that we could just take some of these layers in the middle take the output of that as a representation of this 20 dimensional 28 by 28 dimensional image okay and so here's sort of an example of some of the results from training networks from scratch versus applying some sort of transfer learning. major advantages of pre-trained networks so a lot of the time pre- trained networks are trained on very very large data sets. We can also pre-compute and store our embeddings instead of using the original High dimensional data. We will transition to the next part of the lecture which is going to be on self-supervised free training. We'll go into some details and examples of this in action but especially um I know this is not an NLP course we talk more about CV but transforming learning is especially huge in NLP. 16b I think you might have seen regression in those classes um there are other examples of object detection segmentation we will discuss those in the coming weeks. Even without any labels we can still learn something meaningful about the structure of the raw data how many have you guys taken 16b before so yeah you may have you might recall something called PCA from the class of principle component analysis it's actually one of the most common unsupervised learning algorithms out there because if you remember correctly you just input some data Matrix into that algorithm. Unsupervised representation learning is a technique that can be used to learn representations from large data sets. Large data sets can be helpful for learning more generalizable representations. The data sets are usually not labeled you can you could like scrap text or image or images from the web but you can't really label them automatically. We want to see if we could use unsupervised learning techniques to the sum level data sets and learn representations using that and this is also appealing because labeling in general is a very time consuming and tedious process say that you want to label a billion images. part or property of the input from any observed or unhidden part of theinput. This information can be hidden across time or space. We will go over some examples soon and actually we will have an entire lecture dedicated to self-supervised learning for Envision in a few weeks so we'll double deeper in that lecture Cube. In computer vision this can be something like you learn the representations from some pretext tasks and you'll use those representations for image classification or object detection or semantic segmentation or whatever for NLP this could be text classification machine translation document summarization question. Great training is a very broad topic and self-supervised learning algorithms can be applied to different domains so I guess we can delve into examples now. The hope is that if the model can learn to predict the original order like what patch goes in the let's say the top left corner versus the top right corner it is trying to learn something meaningful about the image so it's not just focusing on say all level features anymore but it's also trying to understand what's going on in the image it's going to learn that an image can be made up of different parts. this can be done is you could predict a word from its surrounding context so say if you have the sentence the dog with the man you could try to predict the word bed from dog and dog because a dog should kind of imply that the word bit is associated with it so this approach is called a continuous bag of words model. There are many other ways to train virtual back models one example is um skip Ram so instead of predicting aword from a context you instead predict the context from a word so you like kind of like flip the model upside down. the slide is it turns out that the current state of the art for CV is actually very similar to Burke so that's a teaser for the lecture that we discuss Advanced Techniques and as software CV. There is a homework for this entire cluster which is the high Crush notebook that should be due next Tuesday even though this lecture doesn't have homework I mentioned before that there will be a lecture on on Advanced SSL for CV and that will have a homework so to work on that homework this slide deck should be up on the website again if you feel free to do that that is it for today a second pause.