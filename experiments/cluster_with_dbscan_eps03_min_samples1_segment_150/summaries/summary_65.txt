NASEM is the National Academy of Science, Engineering, and Medicine. It's chaired by David Baltimore, who used to be an MIT professor until he went and became president of Caltech. Judge David Tatel is a member of the US Court of Appeals for the District of Columbia circuit. He also happens to have a Nobel Prize in his pocket and he's a pretty famous guy. The NASEM body is an august body of old people with lots of gray hair who have done something important enough to get elected. important circuit court. He happens to sit in the seat that Ruth Bader Ginsburg occupied before she was elevated to the Supreme Court. So these are heavy hitters. And they convened a meeting to talk about the set of topics that I've listed here. The issue of using litigation to target scientists who have opinions that you don't like. And the more general issue of how do you communicate advances in life sciences to a skeptical public. So this is dealing with the sort of anti-science. tenor of the times. A group of us that talked about AI and decision making, I was a little bit surprised by the focus because Hank really is a law school professor at Stanford. Cherise Burdee is at something called the Pretrial Justice Institute, and her issue is a legal one which is that there are now a lot of companies that have software that predict, if you get bail while you're awaiting trial, are you likely to skip bail or not? This is influential in the decision that judges make about how much bail to impose and whether to let you out on bail at all. building convolutional neural network models to detect pulmonary emboli and various other things in imaging data. Suresh Venkatasubramanian is a professor. He was originally a theorist at the University of Utah but has also gotten into thinking a lot about privacy and fairness. And so that that was our panel, and we each gave a brief talk and then had a very interesting discussion. One of the things that I was very surprised by is somebody raised the question of shouldn't Tatel as a judge on the Circuit Court of Appeals hire people like you guys to be clerks in his court? interesting to me. He said, no, he wouldn't want people like that, which kind of shocked me. And so we quizzed him a little bit on why, and he said, well, because he views the role of the judge not to be an expert but to be a judge. To be a balancer of arguments on both sides of an issue. And he was afraid that if he had a clerk who had a strong technical background, that person would have strong technical opinions which would bias his decision one way or another. Tatel: Your duty as a lawyer is to argue as hard as you can for your side of the argument. Tatel: In law school, they teach them, like in debate, that you should be able to take either side of any case and make a cogent argument for it. The truth will come out from spirited argument on two sides of a question, Tatel says, but your duty is to try to argue both sides of the same question. The Supreme Court will hear arguments on the issue of gerrymandering on Tuesday. using computers and actually machine learning techniques to try to figure out how to get Republicans or Democrats elected. So in the law, people are in favor of these ideas to the extent that they inject clarity and precision into bail, parole, and sentencing decisions. However, conversely, the use of technology to determine whose liberty is deprived and on what terms may minimize harms that are the products of human judgment. So by formalizing it, you might win, but you might also lose, too. The decision of whether you get bail or not is going to be made by a computer algorithm, not by a human being. There is some discretion on the part of this county official who will make a recommendation, and the judge ultimately decides. Until there are some egregious outcomes from doing this, it will probably be quite commonly used. The critique of these bail algorithms is based on a number of different factors. One is that the algorithms reflect a severe racial bias. So for example, if you get arrested in California, the decision is made by an algorithm. example, if you are two identical people but one of you happens to be white and one of them is black, the chances of you getting bail are much lower if you're black. The algorithm is learning from historical data, and if historically, judges have been less likely to grant bail to an African-American than to a Caucasian-American, then the algorithm will learn that that's the right thing to do. And then the second problem, which I consider to be really horrendous, is that in this particular field, the algorithms are developed privately by private companies which will not tell you what their algorithm is. answer, but they won't tell you how they compute it. And so it's really a black box. You have no idea what's going on in that box other than by looking at its decisions. So the data collection system is flawed in the same way as the judicial system itself, he says. "They won't even tell you what data they used to train the algorithm," he says, "and so you have no know what's happening in that black box" "It's a flawed system," he adds, "but it's not as bad as we think it is" The Wisconsin Supreme Court ruled that an algorithm's output was not enough to violate a man's rights. The decision is likely to be appealed and maybe overturned, says CNN's John Sutter. Sutter: Algorithms could help keep people out of jail and get them psychiatric help. "I'm sure it'll be appealed," Sutter says of the court's decision to rule against the man. "It's kind of an outrageous decision," he says, "and I think many people consider it to be kind of outrageous" There is a long discussion-- you can find this all over the web-- of, for example, can an algorithm hire better than a human being. If you're a big company and you have a lot of people that you're trying to hire for various jobs, it's very tempting to say, hey, I've made lots and lots of hiring decisions and we have some outcome data.so on. So that's the positive side of being able to use these kinds of algorithms. It's not only in criminality. look like they're a better bet. When I was an undergraduate at Caltech, the Caltech faculty decided that they wanted to include student members of all the faculty committees. And so I was lucky enough that I served for three years as a member of the Undergraduate Admissions Committee. And in those days, Caltech only took about 220, 230 students a year. It's a very small school. And we would actually fly around the country and interview about the top half of the applicants. So we would talk not only to the students but also to their teachers and their counselors. admissions decisions have been made, one of the professors, kind of as a thought experiment, said here's what we ought to do. We ought to take the 230 people that we've just offered admission to and we should reject them all and take the next 230 people, and then see whether the faculty notices. Now, of course, I and others argued that this would be unfair and unethical and would be a waste of all the time that we had put into selecting these people. But then this guy went out and he looked at the data we had on people's ranking class, SAT scores, grade point average, the checkmarks on their recommendation letters about whether they were truly exceptional or merely outstanding. And he built a linear regression model that predicted the person's sophomore level grade. point average, which seemed like a reasonable thing to try to predict. And he got a reasonably good fit, but what was disturbing about it is that in the Caltech population of students, it turned out that the beta for your SAT English performance was negative. So if you did particularly well in English on the SAT, you were likely to do worse as a sophomore at Caltech. And so we thought about that a lot, and of course, we decided that that would be really unfair to penalize somebody for being good at something. What is fair? What characteristics would you like to have an algorithm have that judges you for some particular purpose? PETER SZOLOVITS: It's impossible to pin down sort of, at least might in my opinion, one specific definition, but for the pre-trial success rate for example, I think having the error rates be similar across populations is a good start. And you'll see later Irene-- where's Irene? Right there. Irene is a master of that notion of fairness. to capture that in a short phrase. Societal goals. But that's tricky, right? I mean, suppose that I would like it to be the case that the fraction of people of different ethnicity who are criminals should be the same. How do I achieve that? I could pretend that it's the same, but it isn't the same today objectively, and the data wouldn't support that. So that's an issue. Yeah? AUDIENCE: People who are similar should be treated similarly, so engaged sort of independent of the [INAUDIBLE] attributes or independent of your covariate. are to people similar? And what characteristics-- you obviously don't want to use the sensitive characteristics, the forbidden characteristics in order to decide similarity. But defining that function is a challenge. All right, well, let me show you a more technical approach to thinking about this. I'll show you an example that I got involved in. Raj Manrai was a MIT Harvard HST student, and he started looking at the question of the genetics that was used to determine whether somebody is at risk for cardiomyopathy. well, and eventually, you die of this disease at a relatively young age. So what happened is that there was a study that was done mostly with European populations where they discovered that a lot of people who had this disease had a certain genetic variant. And so it became accepted wisdom that if you had that genetic variant, people would counsel you to not plan on living a long life. And this has all kinds of consequences. Imagine if you're thinking about having a kid when you're in your early 40s, and your life expectancy is 55. Would you want to die when you have a teenager that you leave to your spouse? in the US, there were tests of this sort done, but the problem was that a lot of African and African-American populations turned out to have this genetic variant frequently without developing this terrible disease. So you go, well, we must have learned that lesson. So this paper was published in 2016, and this was one of the first in this area. Here's a paper that was published three weeks ago in Nature Scientific Reports that says, genetic risk factors identified in populations of European. descent do not improve the prediction of osteoporotic fracture and bone mineral density in Chinese populations. So it's the same story. Different disease, the consequence is probably less dire because being told that you're going to break your bones when you're old is not as bad as being told your heart's going to stop working. But there we have it. OK, so technically, where does bias come from? Well, I mentioned the standard sources, but here is an interesting analysis. This comes from Constantine Aliferis from a number of years ago. is choose some family of models to try to fit, and then I'm going to use some fitting technique, like stochastic gradient descent or something, that will find maybe a global optimum, but maybe not. And then there is noise. And so his observation is that if you count O as the optimal possible model over all possible model families, then the bias is essentially O minus L. The variance is like L minus A, it's the error that's due to the particular way in which you learned things. the data, randomizing, essentially, the relationships in the data. And then you get a curve of performance of those models, and if yours lies outside the 95% confidence interval, then you have a P equal 0.05 result that this model is not random. So that's the typical way of going about this. Now, you might say, but isn't discrimination the very reason we do machine learning? Not discrimination in the legal sense, but Discrimination in the sense of separating different populations. can't define a universal notion of what it means to discriminate because it's very much tied to these questions of what is practically and morally irrelevant in the decisions that you're making. And so it's going to be different in criminal law than it is in medicine. And it's feature-specific as well, so you have to take the individual features into account. The government has tried to regulate these domains, and so credit is regulated by the Equal Credit Opportunity Act, education by the Civil Rights Act and various amendments. Until 1967, it was illegal for an African-American and a white to marry each other in Virginia. If you went to get a marriage license, you were denied, and if you got married out of state and came back, you could be arrested. Trevor Noah, if you know him from The Daily Show, wrote a book called Born a Crime. His father is white Swiss guy and his mother is a South African black. It was literally illegal for him to exist under the apartheid laws that they had. He had to pretend to be-- his mother was his caretaker rather than his mother in order to be able to go out in public, because otherwise, they would get arrested. So here are some of the legally recognized protected classes, race, color, sex, religion, national origin, citizenship, age, pregnancy, familial status, disability, veteran status, and more recently, sexual orientation in certain jurisdictions, but not everywhere around the country. OK, so given those examples, there are two legal doctrines about discrimination, and one of them talks about disparate treatment, which is sort of related to this one. is something not right, that there is some sort of discrimination. Now, the problem is, how do you defend yourself against, for example, a disparate impact argument? Well, you say, in order to be disparate impact that's illegal, it has to be unjustified or avoidable. If I brought suit against you and said, hey, you're discriminating against me on the basis of this medical disability, a perfectly good defense is, yeah, it's true, but it's relevant to the job. So that's one way of dealing with it. Now, how do you demonstrate disparate impact? Well, the court has decided that you need to be able to show about a 20% difference in order to call something disparate impact. So the question, of course, is can we change our hiring policies or whatever policies we're using to achieve the same goals, but with less of a disparity in the impact? So that's the challenge. But what's interesting is that disparate treatment and disparate impact are really in conflict with each other. And you'll find that this is true in almost everything in this domain. and you can't square that circle easily. Well, there's a lot of discrimination that keeps persisting. There's plenty of evidence in the literature. And one of the problems is that, for example, take an issue like the disparity between different races or different ethnicities. It turns out that we don't have a nicely balanced set. We tend to know a lot more about the majority class than we know about these minority classes, and just that additional data and that additional knowledge might mean that we're able to reduce the error rate. This talk was given at KDD about a year and a half ago, I think. Moritz is a professor at Berkeley who actually teaches an entire semester-long course on fairness in machine learning. And so he formalizes the problem this way. He says, look, a decision problem, a model, in our terms, is that we have some X, which is the set of features we know about an individual, and we haveSome Y, which are the outcome that we're interested in predicting. now you can begin to tease apart some different notions of fairness by looking at the relationships between these elements. So there are three criteria that appear in the literature. One of them is the notion of independence of the scoring function from sensitive attributes. Another notion is separation of score and the sensitive attribute given the outcome. So this is the one that says the different groups are going to be treated similarly. In other words, if I tell you the group, the outcome, the people who did well at the job and the people Who Did poorly at the Job. scoring function is independent of the protected attribute. So that allows a little more wiggle room because it says that the protected. attribute can still predict something about the outcome, it's just that you. can't use it in the scoring function given the category of which outcome category that individual. belongs to. It says that given the scoring. function, the outcome isIndependent of theprotected attribute. And usually, there are knobs in these learning algorithms, and depending on how you turn the knob, you can affect whether you're going to get a better classifier that's more discriminatory. you that the scoring function has to be universal over the entire data set and has to not distinguish between people in class A versus class B. That's a pretty strong requirement. And then you can operationalize the notion of unfairness either by looking for an absolute difference between those probabilities. If it's greater than some epsilon, then you have evidence that this is not a fair scoring function, or a ratio test that says, we look at the ratio, and if it differs from 1 significantly, then it's an unfair scoring function. Hiring is based on a good score in group A, but random in B. So you might wind up with a situation where you wind up hiring the same number of people, the same ratio of people in both groups. Well, the outcomes are likely to be better for a group A than for group B, which means that you're developing more data for the future that says, we really ought to be hiring people inGroup A.or whatever topic you're interested in. And so what if hiring is based in group B? So for example, what if we know a lot more information about group B than we do about group A? because they have better outcomes. Or alternatively-- well, of course, it could be caused by malice also. There's also a technical problem, which is it's possible that the category, the group is a perfect predictor of the outcome. They can't be independent of each other. Now, how do you achieve independence? Well, there are a number of different techniques. One of them is-- there's this article by Zemel about learning fair representations, and what it says is you create a new world representation, Z, which is some combination of X and A, and you do this by maximizing the mutual information between X and Z. So this is an idea that I've seen used in machine learning for robustness rather than for fairness, where people say, the problem is that given a particular data set, you can overfit to that data set. One of the ideas is to do a Gann-like method where you say, I want to train my classifier, let's say, not only to work well on getting the right answer, but also to work as poorly on identifying which data set my example came from. does in other populations, and the FDA has actually approved the marketing of that drug to those subpopulations. And if you think about the personalized medicine idea, which we've talked about earlier, the populations that we're interested in becomes smaller and smaller until it may just be you. And so there might be a drug that works for you and not for anybody else in the class. But it's exactly the right drug for you, and we may get to the point where that will happen and where we can build such drugs. and I draw ROC curves for both of these populations, they're not going to be the same, because the drug will work differently for those two populations. But on the other hand, I can draw them on the same axes, and I can say, look any place within this colored region can be a fair region in that I'm going to get the same outcome for both populations. So the advantage of separation over independence is that it allows correlation between R and Y, even a perfect predictor. And it gives you incentives to learn. to reduce the errors in all groups. And then the final criterion is sufficiency, which flips R and Y. So it says that the regressor or the predictive variable can depend on the protected class, but the protectedclass is separated from the outcome. So for example, the probability in a binary case of a true outcome of Y given that R is some particular value, R and A is a particular class, is the probability of the outcome of A and R in the binary case. same as the probability of that same outcome given the same R value, but the different class. So that's related to the sort of similar people, similar treatment notion, qualitative notion, again. So it requires a parody of both the positive and the negative predictive values across different groups. So for example, if the scoring function is a probability, or the set of all instances assigned the score R has an R fraction of positive instances among them, then the scoringfunction is said to be well-calibrated. degree of calibration will give you a good approximation to this notion of sufficiency. These guys in the tutorial also point out that some data sets actually lead to good calibration without even trying very hard. So for example, this is the UCI census data set, and it's a binary prediction of whether somebody makes more than $50,000 a year if you have any income at all and if you're over 16 years old. It's almost exactly along the 45 degree line without having done anything particularly dramatic in order to achieve that. for whites versus blacks, the whites, not surprisingly, are reasonably well-calibrated. So you could imagine building some kind of a transformation function to improve that calibration, and that would get you separation. Now, there's a terrible piece of news, which is that you can prove, as they do in this tutorial, that it's not possible to jointly achieve any pair of these conditions. And so you have three reasonable technical notions of what fairness means, and they're incompatible with each other except in some trivial cases. choosing different notions of fairness. So it's a kind of nice graphical hack. Again, it'll be on the slides, and I urge you to check that out, but I'm not going to have time to go into it. There is one other problem that they point out which is interesting. So this was a scenario where you're trying to hire computer programmers, and you don't want to take gender into account because we know that women are underrepresented among computer people. So they say, well, there are two scenarios. Gender is an unusable attribute, they don't like this model. So they say, well, we could use an optimal separated score, because now, being a programmer separates your gender from the scoring function. And so we can create a different score which is not the same as the optimal score, but is permitted because it's no longer dependent on your sex, on your gender. Here's another scenario that, again, starts with gender and says, look, we know that there are more men than women who obtain college degrees in computer science. The probability that you visited the Grace Hopper Conference is dependent on your gender. Computer scientists are much more likely to be programmers than non-computer science majors. The optimal score is going to depend basically on whether you have a computer science degree or not. If you're a historian, you're not likely to have been interested in going to that conference. It's a really cool conference. Grace Murray Hopper invented the notion bug or the term bug and was a really famous computer scientist starting back in the 1940s. the separated score will depend only on your gender, which is kind of funny, because that's the protected attribute. And what these guys point out is that despite the fact that you have these two scenarios, it could well turn out that the numerical data, the statistics from which you estimate these models are absolutely identical. So from a purely observational viewpoint, you can't tell which of these styles of model is correct. So that's a problem because we know that these different notions of fairness are in conflict with each other. work. I got an invitation last year from the American Medical Association's Journal of Ethics, which I didn't know existed, to write a think piece for them about fairness in machine learning. I decided that rather than just bloviate, I wanted to present some real work. And so Marcia, who was one of my students, and I convinced her to get into this, and we started looking at the question of how these machine learning models can identify and perhaps reduce disparities in general medical and mental health. McLean's hospital here in Boston, which both have big psychiatric clinics. The type of insurance you have correlates pretty well with whether you're rich or poor. So we did that, and then we looked at the notes. We wanted to see not the coded data, but whether the things that nurses and doctors said about you as you were in the hospital were predictive of readmission, of 30-day readmission. So these are some of the topics that we wanted to look at. We used LDA, standard topic modeling framework. White patients have more topics that are enriched for anxiety and chronic pain. Black, Hispanic, and Asian patients had higher topic enrichment for psychosis. It's interesting. Male patients had more substance abuse problems. And so we said, what happens when you look at the different topics, how often the different topic arise in different subpopulations? And so what we found is that, for example, white patients haveMore topics enriched for Anxiety and Chronic pain. Female patients had more general depression and treatment-resistant depression. Men still have substance abuse problems in the ICU population. Women have more pulmonary disease. And we were speculating on how this relates to sort of known data about underdiagnosis of COPD in women. By race, Asian patients have a lot of discussion of cancer, black patients have  kidney problems. Hispanics of liver problems, and whites have atrial fibrillation. So again, stereotypes of what's most common in these different groups. Those with public insurance often have multiple chronic conditions. Public insurance patients have atrial fibrillation, pacemakers, dialysis. Private insurance patients, on the other hand, have higher topic enrichment values for fractures. So maybe they're richer, they play more sports and break their arms or something. Just reporting the data. Just the facts. So these results are actually consistent with lots of analysis that have been done of this kind of data. Now, what I really wanted to look at was this question of, can we get similar error rates. And the answer is, not so much. they are for women, statistically significantly lower. So this indicates that there is, in fact, a racial bias in the data that we have and in the models that we're building. These are particularly simple models. In psychiatry, when you look at the comparison for different ethnic populations, you see a fair amount of overlap. One reason we speculate is that We have a lot less data about psychiatric patients than we do about ICU patients. So the models are not going to give us as accurate predictions. for example, a statistically significant difference between blacks and whites and other races, although there's a lot of overlap here. Between males and females, we get fewer errors in making predictions for males, but there is not a 95% confidence separation between them. And for private versus public insurance, we do see that separation where for some reason, in fact, we're able to make better predictions for the people on Medicare than we are-- or Medicaid. So just to wrap that up, this isn't a solution to the problem, but it's an examination of the problem. work here and embarrassing myself. So this is modeling mistrust in end-of-life care, and it's based on Willie's master's thesis and on some papers that came as a result of that. If you look at African-American patients, and these are patients in the MIMIC data set, what you find is that for mechanical ventilation, blacks are on mechanical ventilation a lot longer than whites on average. Now, of course, we don't know exactly why, but that's the case. or social differences, but to a difference in the degree of trust between the patient and their doctors? It's an interesting idea. And of course, I wouldn't be telling you about this if the answer were no. And so the approach that he took was to look for cases where there's clearly mistrust. So there are red flags if you read the notes. For example, patient refused to sign ICU consent and expressed wishes to be do not resuscitate, do not intubate, seemingly very frustrated. and mistrusting of the health care system, also with a history of poor medication compliance and follow-up. So that's a pretty clear indication. And you can build a relatively simple extraction or interpretation model that identifies those clear cases. So the problem, of course, is that not every patient has such an obvious label. In fact, most of them don't. And so Willie's idea was, can we learn a model from these obvious examples and then apply them to the less obvious examples in order to get a kind of a bronze standard. those cases of obvious mistrust are features like the person was in restraints. If a person is in pain, that correlated with these mistrust measures as well. And conversely, if you saw that somebody had their hair washed or that there was a discussion of their status and comfort, then they were probably less likely to be mistrustful of the system. And so the approach that Willie took was to say, well, let's code these 620 binary indicators of trust and build a logistic regression model to the labeled examples. a clear indication, and this gives us another population of people who are likely to be mistrustful and therefore, enough people that we can do further analysis on it. So if you look at the mistrust metrics, you have things like if the patient is agitated on some agitation scale, they're more likely to being mistrustful. If, conversely, they are alert and less likely to have any pain, that means they're in some better mental shape. If the patient was restrained, then trustful patients have no pain, or they have a spokesperson who is their health care proxy. "I understand some of the strong family feelings that happened as a result of some of these historical events," he says. "So I would expect that people in my status might also have similar issues of mistrust" "But the answer seems to be, not so much. So if you look at these severity scores like OASIS and SAPS and look at their correlation with noncompliance in autopsy, those are pretty low correlation values, so they're not explanatory of this phenomenon," he adds. "There is a significant difference in sentiment expressed in the notes" between black and white patients. The autopsy derived mistrust metrics don't show a strong relationship, a strong difference between them, but the noncompliance derived mistrust metric do. There is a lot more work that needs to be done in this area, and it's a very rich area both for technical work and for trying to understand what the desiderata are and how to match them to the technical capabilities. One of the pairs of people, Mike Kearns and Aaron Roth at Penn are coming out with a book called The Ethical Algorithm, which is coming out this fall. fairness popping up at different universities. University of Pennsylvania has the science of Data ethics, and I've mentioned already this fairness in machine learning class at Berkeley. This is, in fact, one of the topics we've talked about. I'm on a committee that is planning the activities of the new Schwarzman College of Computing. The college obviously hasn't started yet, so we don't have anything other than this lecture and a few other things like that in the works, but the plan is there to expand more in this area.