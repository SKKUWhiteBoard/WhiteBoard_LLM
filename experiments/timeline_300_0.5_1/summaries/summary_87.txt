Andrej Karpathy: In this module, I'm going to briefly introduce the idea of differentiable programming. Differentiable programming is closely related to deep learning. It allows you to build up an increasingly more sophisticated model without losing track of what's going on. So let's suppose you want to do image classification. We need some way of representing images. To fix this problem, we introduce convolutional neural networks which is a refinement of fully connected neural networks. So here is an example of ConvNet in action. In NLP, words are discrete objects and neural networks speak vectors. So whenever you're doing NLP with neural nets, you first have to embed words, or more generally, tokens. There's a lot of processing that needs to happen and it's hard to kind of specify in advance. So what we're going to do is going to define an abstract function. An abstract function is something that has an interface but not an implementation. I'm going to talk about two implementations of the sequence models. One is recurrent neural networks and one is transformers. encourage you to consult the original source if you want kind of the actual, the full gory details. Another thing I haven't talked about is learning any of these models. It's going to be using some variant of stochastic gradient descent, but there's often various tricks that are needed to get it to work. But maybe the final thing I'll leave you with is the idea that all of differentiable programming is built out of modules. Even if you kind of don't understand or I didn't explain the details, I think it's really important to pay attention to the type signature of these functions.