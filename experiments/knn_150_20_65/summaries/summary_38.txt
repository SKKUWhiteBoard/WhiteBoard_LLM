So I think I just spend like five minutes, just briefly review on the backpropagation last time. So I didn't have time to explain this figure, which I think probably would be useful as a high level summary of what's happening. This is how you define network and the loss function. So you start with some example x, and then you have some-- I guess, this is a matrix vector multiplication module, and you take x inner product multiplied with w and b. And then get some activation, the pre-activation. And you get some post activation. one of those three lemmas. If you know how to compute the derivative with respect to the output of some module, suppose this is a module, tau is the output. And now you can see what this does kind of like lemma are for. Those lemma, basically, are saying that if you know dg over the d tau, how do you computedg over da? And there's another lemma which says that ifYou know how. to compute dgover da, howDo you compute dG over dz? All of those lemma is about this kind of relationship. Some of the practical viewpoint of ML, like how do you relate to your model, what you have to do in this whole process. So far, we only talk about training. We have some examples, which we have seen when they are training data sets. And often, this is called test distribution. And then you evaluate what's the expected loss on this new test example. And oftentimes, it's not always true. The training loss is less than the test loss. So you have a discrepancy between training and test. time. We're going to discuss what will happen if you change your model complexity, and whether in what cases, you may underfit. In what cases you may overfit, and what is the best response. We are going to decompose the test into two terms, which is called bias. And technically, it's bias squared because the bias is defined as the square root of this term. So plus variance, the test error is the sum of these two. And so the question you want to answer is that if you. change the model. complexity, what's the best test error, right? In some sense, you care about two quantities. You care about the training loss and the gap. You want both of these two to be small. And typically, when l theta is big, there are two failure mode in some sense. So one of the failure mode is called failure patterns. And so overfitting, I'm going to discuss a lot about overfitting. But the first other bit is that the typical situation of like this, maybe this, and something like this. And what's the best fit? The best fit probably would change a little bit. Bias is a decreasing function as the model complexity. The bias is the best error or loss, you can get with even infinite data. If bias is large, even with infinite data, you cannot do anything. So bias is a property of the large family of linear models, right? So bias would be the best [INAUDIBLE] linear model that is that is true that is a linear model. And that error, that closeness, that is the bias, basically is the closest to the ground. In the lecture notes, actually, there are some visualizations of the real models you're going to fit. So for linear models, I guess, you can see a bunch of properties. There's a large training error, training loss or training-- let's call it loss just for consistency. And now, I'm going to show cases where the variance is the culprit to blame for. So basically, you are looking at-- you are kind of like overfitting to the spurious patterns, but instead of the big pattern. the training data set. This is your prediction for this x. And you look at the distance between the prediction and the true label. The training error is pretty big. So this is underfitting, by our definition of underfitting because the tuning is already big. Why the training is big? What's the culprit? The culprit, I would argue, is that it's just because no any linear model can fit your data. It's not just because you don't have enough data, and your to fit a fifth-degree polynomial. A fifth-degree polynomial can go up and down so many times, several times. So the higher the degree is the more times you can go. So if you have high degree polynomials, you can be more flexible. If you have not too many data, but you have very, very simple model, then it's probably still OK. So suppose you have a lot more data. And you observe a lot of more data, roughly. There's a little bit fluctuation, of course. spurious patterns are the fluctuations in some sense. And so in other words, I think you are explaining the noise instead of the ground truth. How do I formulate this? Like one way to kind of formulate this a little bit more mathematically is that you can consider to redraw the samples. So you redraw some new samples with different spurious patterns. They are spurious because they are noise. If your model is specific to the variance is caused by lack of data. And it can be mitigated if you have more data. same distribution. From the same distribution. Yeah. So like if you collect more data from-- yeah. So in some sense, you kind of like the mindset-- I'm not saying this is universally applicable to every situation, but the mindset we are in is that, for example, you have a lot of like medical images. And now, I'm asking I found out my variance is very big. So how do I mitigate that? So probably one thing is that I can just sample more data is not expressive enough. is that this test error should have this U curve. But then, people realized that this is a striking thing. So people realize that if you increase your model number of parameters even more, at some point, you will see that it will be like this. This is the second descent of the test error. That's why it's called double descent because there is a decent here, there's a descent here. So this peak is often happening when-- it's roughly equal to d. And we realized when d is kind of above n, above the number of data points, you had a peak. people really care about it. And what I mean by that is that even within linear models, you can try to change the model complexity. So what that means is that you try to decide how many features you use. So you can start with only using one feature or two features like for example, in the house price, where you can use the square foot as the single feature, or you can collect a bunch of other features. So keep adding more and more features. That means you have more. and this model. So this model seems to have less parameter than this. That's by definition. The norm is actually very big. So in some sense, if you use the norm as the complexity, actually, these peaks have large complexity. So what is the right measure for complexity? So this is a very difficult question. Like for different situations, you have different answers. But there is no universal answer. But norm could be one complex measure. And for linear model, it just happens that for mathematical reasons, I think l2 norm behaves really nice. going to discuss this more next time. So the high level thing is just that something else is driving the norm to be small. Thanks. Going to talk more about this in the next few days. Back to Mail Online home. back to the page you came from. Back To the pageYou came from: Back to thepage you camefrom. Back into the page You came from was from: The Daily Mail. Back onto the pageyou came from, the DailyMail.com page you were from.