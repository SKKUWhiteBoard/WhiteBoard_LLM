This week's lecture will focus on machine translation. In the second half of the week, we take a break from learning more and more on neural network topics. We'll talk about final projects, but also some practical tips for building neural network systems. This is an important content full lecture. We hope to see you in the next week or so for our final week of lectures. Back to Mail Online home. back to the page you came from.. Click here for the next lecture. When they were in the period of statistical NLP that we've seen in other places in the course. And then the idea began, can we start with just data about translation i.e. sentences and their translations, and learn a probabilistic model that can predict the translations of fresh sentences? So suppose we're translating French into English. We can say, what's the probability of different English translations? And then we'll choose the most likely translation. And we're not really expecting you to understand the details here. But I did then want to say a bit more about how decoding was done in a statistical machine translation system. So there's a fair availability from different sources. And we can use that to build models. So how do we do it though? All we have is these sentences. And it's not quite obvious how to build a probabilistic model out of those. So in this case, what we're going to do is introduce an extra variable, which is an alignment variable. And so if we could induce this alignment between the two sentences, then we can have probabilities of how likely a word or a short phrase is translated in a particular way. In the period from about 1997 to around 2013, statistical machine translation was a huge research field. In 2014, the first modern attempt to build a neural network from machine translations and encoded-decoder model. Within two years' time, Google had switched to using neural machine translation for most languages. There are still lots of difficulties which people continue to work on very actively. And you can see more about it in the Skynet Today article that's linked at the bottom. We'd like to translate languages for which we don't have much data. model end to end on parallel sentences. And it's the entire system rather than being lots of separate components as in an old fashioned machine translation system. So these neural network architectures are called sequence to sequence models or commonly abbreviated seq2seq. And they involve two neural networks. There's one neural network that is going to encode the source center. And we're going to feed it in directly as the initial hidden state for the decoder, or RNN. But we do the same kind of LSTM computations and generate a first word of the sentence. In neural machine translation we are directly calculating this conditional model probability of target language sentence given source language sentence. So we have a source sentence. And that's going to strongly determine what is a good translation. And so to achieve that, what we're going to do is have some way of transferring information about the source sentence from the encoder to trigger what the decoder should do. And the two standard ways of doing that are you either feed in a hidden state as the initial hidden state to the decoding, or sometimes you will feed something in. target languages the sentences, and get them to give judgment on how good a translation it is. But that's expensive to do, and might not even be possible if you don't have the right human beings around. So a lot of work was put into finding automatic methods of scoring translations that were good enough. The most famous method of doing that is what's called BLEU. And the way you do it is you have a human translation or several human translations of the source sentence, and you're comparing a machine generated translation to those pre-given human written translations. For assignment 4 this year, we've decided to do Cherokee English machine translation. Cherokee is an endangered Native American language that has about 2000 fluent speakers. It's an extremely low resource language. So it's just there isn't much written Cherokee data available period. And particularly, there's not a lot of parallel sentences between Cherokee and English. So we can see how far we can get. But we have to be modest in our expectations because it's hard to build a very good MT system with only a fairly limited amount of data. that can sometimes improve performance. And we actually have that trick in the assignment 4 system. And you can try it out. So we generate along and generate our whole sentence in this manner. And that's proven to be a very effective way of getting more information from the source sentence more flexibly to allow us to generate a good translation. I'll stop here for now and at the start of next time. I will finish this off by going through the actual equations for how attention works. In the early 1950s, there started to be work on machine translation. It was hyped as the solution to the Cold War obsession of keeping tabs on what the Russians were doing. Claims were made that the computer would replace most human translators. But despite the hype it ran into deep trouble. And so in retrospect, it's not very surprising that the early work did not work out very well. But it was also the beginning of people starting to understand the science of human languages, the field of linguistics. So that's what I'll talk about in the final part of the class.