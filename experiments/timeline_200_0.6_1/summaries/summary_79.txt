Reinforcement learning involves the idea of a model, a value, and a policy. A policy is a mapping from the state you're in to what is the action, um, to take. A model is a representation of the world and how that changes in response to agent's accident. A value is the expected discounted sum of rewards from being in a state and/or an action, and then following a particular policy. Markov Decision Processes is where we think about an agent interacting with the world. The Markov Process is to say that the state that the agent is using to make their decisions is a Markov state. The Markov process involves taking actions that affect the state of the world in some way, and then the agent receives back a state and a reward. Today, we're going to think of an agent, just focusing on the current state, um, so the most recent observation, like, you know, whether or not the robots laser range finders saying, that there are walls, to the left or right of it. A Markov Chain is a sequence of random states, where the transition dynamics satisfies this Markov property. So if you have a finite set of states, you can just write this down as a matrix. What's the probability distribution over next states that you could reach? So if we go back to the Mars Rover example, we thought of a Mars Rover landing on Mars and there might be different sorts of landing sites. And then, it can go to the left or right, um, er, under different actions or we could just think of those actions as being a_1 or a_2. what are the probabilities computed of, like the rewards, I guess, the probability, based on the reward of going from state 1 to 2 [NOISE] or? Great question. So we're assuming right now, this is, um, the, this Markov process is a state of the world that you were, there is some the, the environment you're in is just described as a Markov Process, and this describes the dynamics of that process. We're not talking about how you would estimate those. So, what would this look like if you wanted to think of what might happen to the agent over time in this case or what the process might look like? So, let's say that your initial starting state is S four, and then you could say, well, I can write that as a one-hot vector. I multiply it by my probability. And that gives me some probability distribution over the next states that I might be in and the world will sample one of those. So, it's like the world you know what the dynamics, the dynamics is of the world and then nature is gonna pick one ofThose outcomes. It's like sampling from sort of a probability distribution. rewards for the Markov Decision Process can either be a function of the state, the state in action, or state action next state. Right now we're still in Markov Reward Processes so there's no action. So, in this case, the ways you could define rewards would either be over the immediate state or state and next state, for example. Once we start to think about there being rewards, we can then think about returns and expected returns. We talked about those briefly last time, but it often we think about the case where, um, an agent might be acting forever. long time. The definition of a return is just the discounted sum of rewards you get from the current time step to a horizon and that horizon could be infinite. If the process is deterministic, these two things will be identical. But in general if theprocess is stochastic, they will be different. So, what I mean by deterministic is that if you always go to the same next state, no matter which if you start at a state if there's only a single next state you can go to, uh, then the expectation is equivalent to a single return. General case, we are gonna be interested in these stochastic decision processes which means averages will be different than particularly runs. So, for an example of that well, let me first just talk about discount factor and then I'll give an example. Discount factors are a little bit tricky. They're both sort of somewhat motivated and somewhat used for mathematical convenience. We'll see later one of the benefits of mathematic, uh, benefits of discount factors mathematically is that we can be sure that the value function sort of expected discounted sum of returns is bounded. There's often a bounded number of time you know bounded length of course in many many cases that the horizon is naturally bounded. So, in this case you know what might happen in this scenario we start off in s_4. Um, and then on time-step s_7 we get a reward of 10. But that has to be weighed down by the discount factor which here is 1/2. And so the sample return for this particular episode is just 1.25. And of course we could define this for any particular, um, episode. The Markov structure allows us to decompose the value function of a Markov Reward Process. The value function is simply the immediate reward the agent gets from the current state it's in plus the discounted sum of future rewards weighed by the discount factor times the- and where we express that discounted sum is we can just express it with V, V(s'). So, we sort of say well whatever state you're in right now, you're going to get your immediate word and then you'reGoing to transition. And you could just do this many many many times. And then average. And that would asymptotically converge to what the valuefunction is. to some state s' Um, and then you're going to get the value of whatever state you ended up in discounted by our discount factor. So, if we're in a finite state MRP we can express this using matrix notation. Um, we can say that the value function which is a vector is equal to the reward plus gamma times the transition model times V. And the nice thing is that once we've done that we can just analytically solve for thevalue function. The question was was if it's possible to have self-loops? Um, could it be that this is sort of circulator defined [NOISE] in this case? So, if one of the transitions can be back to itself, um wouldn't it be become a circular to try to express V in terms of V(s)? And if you have N states, it's fine if some of the states that you might transition back to the same state there's no problem. You do need that this matrix is well-defined. so let's say you have N states there's generally on the order of somewhere between N squared and N cubed depending on which matrix inversion you're using. Is it ever actually possible for, uh, that matrix not to have an inverse or does like the property that like column sum to one or something make it not possible? Question was is it ever possible for this not to has an inverse? Um, it's a it'sA good question. I'm trying to think whether or not that can be violated in some cases. Markov Decision Processes are the same as the Markov Reward Process except for now we have actions. So, the agent is in a state they take an action, they get immediate reward, and then they transition to the next state. And as was asked before by Camilla I think, the reward can either be a function of the immediate state, the state and action to the state action and next state for most of the rest of today we'll be using that it's the function of both theState and action. in a state in K action, why is it deterministic what the next state is? Question is same like well why is this- why are there stochastic processes I think. Um, there are a lot of cases where we don't have perfect models of the environment. May be if we had better models then things would be deterministic. And so, we're going to approximate our uncertainty over those models with Stochasticity. So, maybe you have a robot that's a little bit faulty and so sometimes it gets stuck on carpet and then sometimes it goes forward. A Markov Decision Process policy specifies what action to take in each state. And the policies themselves can be deterministic or stochastic, meaning that you could either have a distribution over in the next action you might take given the state you're in or you could have a deterministic mapping. So, if you have an MDP plus a policy then that immediately specifies a Markov Reward Process. And similarly you can define your transition model by averaging across your transition models according to the weight at which you would take those different actions. expected discounted sum of rewards. Now you might ask, okay well they- are they ever guaranteed to stop changing? And we'll get to that part later. We're going to get to the fact that this whole process is guaranteed to be a contraction so it's not going to go on forever. So the distance between the value functions is going to be shrinking. And that's one of the benefits of the discount factor. So if people don't have any more immediate questions, I suggest we all take a minute and then just compare with your neighbor of what number you get when you do this computation. to be pi up there. Yes it was, thanks for catching. All right, so now we can start to talk about Markov Decision Process control. Control here is going to be the fact that ultimately we don't care about just evaluating policies, typically we want our agent actually be learning policies. And so in this case we're not going to talks about learning policies, we're just going to talking about computing optimal policies. So the important thing is that there exists a unique optimal value function. before we do this let's think about how many policies there might be. So there are seven discrete states. In this case it's the locations that the robot. There are two actions. I won't call them left and right, I'm just going to call them a_1 and a_2. Then the question is how many deterministic policies are there and is the optimal policy for MDP always unique? So kind of right we just take like one minute or say one or two minutes feel free to talk to a neighbor. The optimal policy for an MDP and a finite horizon problem where the agent acts forever is stationary. It's stationary which means it doesn't depend on the time-step. If you would always do action a_1 from state s_7 now, um then if you encounter it again in 50 time-steps you still have an infinite amount of time to go from there. So, if you have a lot of compute, you might just want to and this might be better if you really care about wall clock and you have many many many processors. Policy improvement is the next step in the Markov Reward Process. The next critical question is how you bring you function and you function. Is this going to be like finding a local maximum goal then its kind of gets stuck there and [inaudible] for actions. So, we're guaranteed to converge to the global optima and we'll see why for a second. Okay. So this is how it works. You do this policy evaluation and then compute the Q function and then you compute the new policy that takes an arg max of Q. that quantity for each state. But the strange thing is that we're not gonna follow the old policy from then onwards. We are going to follow this new policy for all time. So, it should be at least a little unclear that this is a good thing to do [LAUGHTER]. Should be like, okay, so you're saying that if I were to take this one different action and then follow my old policy, then I know that my value would be better than before. But what you really want is that this new Policy is just better overall. strict inequality if the old policy was suboptimal. So, why does this work? So, it works for the following reasons. Let's go ahead and just like walk through the proof briefly. Okay. This is- what we've said here is that, um, V^pi_i(s), that's our old value of our policy. Has to be less than or equal to max a of Q#pi. Is equal to R(s, pi_i+1(s) If the policy at pi, so the question here was to say, if pi of i+1 is equal to pi i for all states, could it ever change again? Somebody wanna share a guess of whether or not that is true? And the second question is, um, is there a maximum number of policy iterations? Yeah. There's no- you can't have more iterations than there are policies. There- We know that there is at most a to the s policies. You cannot repeat a policy ever. And so, there's no policy improvements to be, yeah, to change. work? The algorithm can be summarized as follows. You start off, you can initialize your value function to zero for all states. And then you loop until you converge, um, or if you're doing a finite horizon, which we might not have time to get to today. And basically, for each state, you do this Bellman backup operator. So you'd say, my value at k plus one time steps for that state is if I get to pick the best immediate action plus the discounted sum of future rewards. on sort of the contraction operator. So, for any operator, um, let's let O be an operator and x denote a norm of x. So x could be a vector like a value function and then we could look at like an L2 norm or an L1 norm or L infinity norm. If an operator is a contraction it means that if you apply it to two different things, you can think of these as value functions. So just to, um- actually, I'll save examples for later. But this is the formal definition of what it means to be a contraction. doesn't get bigger and can shrink after you apply this operator. So, the key question of whether or not value iteration will converge is because the Bellman backup is a contraction operator as long as gamma is less than one. So how do we prove this? Um, we prove it- for interest of time I'll show you the proof. Again, I'm happy to go through it, um, I- or we can going through it in office hours et cetera. Okay. between the two value functions can't be larger after you apply the Bellman operator than it was before. There has to be a unique solution. It's also good to think about whether the initialization and values impacts anything if you only care about the result after it's converged. All right. Class is basically over. There's a little bit more in the slides to talk about, um, the finite horizon case, and feel free to reach out to us on Piazza with any questions.