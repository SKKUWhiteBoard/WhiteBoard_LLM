The EM algorithm is a framework that we use throughout most of the class. It's for dealing with various different notions of latent variables. Maximal likelihood is just a framework. It happens to be the one we're going to use. In GMM, we could guess randomly an assignment of every point to the cluster, the probability. In EM, we just keep running those two loops again and again, and hopefully, it will improve. You'll see this weird picture of a curve that we go up, and that's the loss function. The decomposition is quite important. And we're going to try and kind of abstract that away. That's what I mean by a kind of traditional supervised thing. What they're doing, basically, is they have that decoupling property. If we knew this thing that we couldn't have observed, then, all of a sudden, it becomes a really standard statistical estimation problem. And somehow, we are assuming structure, and that's what we're putting into the latent variable. And then we'll exercise it, basically exercise the notation. The EM algorithm is based on convexity and Jensen's inequality. We're going to draw some curves of a likelihood function that will hopefully be easier to optimize than the original function. And we'll try an iterative algorithm that will look exactly like we talked about before. And then we will is going to be equal to L of theta t-- sometimes call this the tight property, OK? Our hope is Lt is easier to optimized than l. Then we maximize that, and this is formalizing the back and forth. We take that new maximum that looks that new thing, which is our new algorithm. A set is convex if for any a and b element of omega, the line between them is in omega. A function is going to be conveX if its graph is, OK? As I said, let's draw an example of this. So here's 0. Here's minus 1, and I draw this character. Let me erase 0 and 1 because we don't really need them. Their values are kind of unimportant to us. It was just so you knew what I was drawing. We'll draw a. All right, awesome. This is a picture, which maybe won't make perfect sense to start with, but we'll get there. If f is twice differentiable and, for all x, f double prime of x is greater than 0, then f is convex, OK? So this says these functions really are bowl-shaped, right? Second derivative being positive means that they have this kind of positive curvature that looks like the U's. Their first dimension-- first derivative goes up and down, but they're kind of always trending. take what is the obvious thing to do. I'm going to multiply this by lambda. I have to make a statement about this, right? That's what's in my definition above, OK? Well, that's just the same as adding f of z for Lambda, plus 1 minus Lambda. So that shows that this thing, this inequality, holds, f ofZ. Oh, so you've seen the double, is that [INAUDIBLE]? Yeah, this is Taylor's theorem. is, otherwise, this thing, which we actually do care about, is strongly convex. This definition feels like it comes from space aliens otherwise. So for example, f of x equals x squared, which I told you was in my head. Well, this gives me a simple test, right? Its second derivative is 2. That's greater than 0. It is the prototypical strongly conveX function, OK? You also saw those This is to make this parameter sometimes with the curvature one. Doesn't really matter, but OK. I pick a curve, one way to define a curve. And that curve is going to be as a result of sweeping some parameters in a high-dimensional weird space. No matter how I pick the parameters of that curve, anywhere that lives on this thing, that's a probability distribution, a bunch of numbers that sum to 1 in the discrete case. That's going to allow me to build a lower bound for my function, and I'm going to hillclimb using it. We'll see that in just a minute. Are there any questions about this piece here? do something fancier if you want something that's a full probability distribution. This holds even if E is a continuous distribution. The reason you'll always get the inequality the right way is you'll draw the picture of the function and see the chord is always above it. We actually don't want to use a convex analysis. We'll stop at kind of high school calculus. Sound good? All right. Now, everything is defined in the literature traditionally for convex. If you take convexAnalysis, it's the way we define things. structure. In a real sense, when we make a modeling decision, and we say, there exists some structure out there. One version of the prior is, I tell you exactly where every photon comes from. That's clearly a very strong prior. If you knew that, Godspeed. Go do it. You just solved GDA. So let's take a look. What does EM actually do here? So what is EM? EM is very general. You can instantiate it. So what does it mean here? Be theta t plus 1, OK? And we're going to, again, create some new curve, Lt plus 1 of theta, based on that point. And the key aspects of the point that I'll write in a second is this point is a lower bound. This curve is always below the loss, so it's kind of a surrogate that I'm not overestimating my progress, and it's tight. So I wouldn't think and get fooled that there was a higher loss function somewhere else. about the algorithm, but hopefully, it's clear what's going on. Easy-to-train surrogate, and we kind of slowly hillclimb with that. And this is what we were doing in K means. This is the rough algo. I'm just restating what's on the thing, I'm not giving you enough math. But we'll see. All right. Cool. Please. Just could you reiterate? Like, why are we not using gradients on the original turbulence? Right, so we could imagine doing some kind of gradient descent here. just means I have an internal solver that's fast and I kind of trust, and I have something on the outside that's a latent variable that I'm like splitting up the modeling. It's one of a number of decomposition strategies. Doesn't mean it's the only way to solve it, though. All right, so the question is, how do we construct L of t? And I claim we know everything else. So we'll come back to that claim in a second. Jensen's is a probability distribution over the states such that the sum over Q(z) equals 1, and Q (z) is greater than or equal to 0. So we're going to shoehorn Jensen's into what we're doing, and there's some motivation, but it's kind of opaque, let's say. So how does it work? So we have this character-- copy-- in here. This can also be written as an expected value, where z is distributed like Q of this weird-looking quantity. It's just the definition of expectation. Each data point is going to get its own different Q, which is the log of how likely this thing is, OK? And we picked those for each i. So because we did this term by term, we can pick that Q-- Q1, Q2, Q3, all different. And we pick them all so they satisfy this equation. So the ELBO of x, Q, z equals maximizing here over all the parameters, phi and mu and sigma, sigmas, sorry, all the covariants. sequence that is monotonically increasing or nondecreasing, OK? So it's possible that it would grind to a halt. But eventually, it has to be strict. And so to derive a counterexample, you would just find a likelihood function that had those two bumps. And what it will do is it will gradually hillclimb. And this is actually not great. Like, it can't go back downhill, right? It's got to just continue to go up. This just says that it's not going to oscillate wildly. entire curve because we wanted to optimize it. So it wasn't enough to find a point in a lower bound. We needed to find the whole thing that was underneath it so we could run our argmax step. And that was the setting where we would learn all of the parameters and estimate that in a way that was hopefully nice and easy to do. It's a lot of notation because we're abstracting out a huge number of things that we're doing. We'll run through an example of that. model, where we were saying, the way we're going to think about the world was to maximize the likelihood. We didn't get to a global optimum. So I don't mean that we definitely guaranteed that we got the maximum likelihood estimation, just that you can phrase what's going on as MLE. And so when you get into other estimation problems and the subproblems, you just apply the MLE stuff you learned from the first half of the class. And we'll see that in an example. This is a Gaussian distribution with center j. The notation doesn't change except for this is what the Gaussian looks like instead of a square. And then there's the phi j, which is just multiplied times this horrible expression. And this exp parentheses is so I don't have to write it in superscript, right? Just expo the function, just a bad habit that I have. Oh, and this whole thing is unfortunately-- snap 2? Yeah-- over w(i)j. phi 2 was hugely bigger than phi 1, right-- a billion points came from the second source. So to automate this, this is Bayes' rule. It just weighs those two probabilities and tells us what should happen. If you haven't seen this before, this will trip you up in some way. You need a Lagrangian, OK? If you want, I'll post notes about how to compute Lagrangians as well. But the piece is here that it gets you back to an expression which makes sense in this setting. The z(i) notation was still very abstract-- wj(i), which was summing over this part here, log-- and help us all, 1 over 2 pi-- this is a covariance, 1/2. Oh, I decided to write this in four general things. OK, I see why. Transpose sigma inverse x( i) mu j times phi j. On a whiteboard, that's really catastrophic-- phi J, OK? Let me make sure the brackets are clear. so if you just take this and compute the derivative, it doesn't account for the constraint. So you have a bunch of numbers that must sum to 1. If you've never seen this before, I just want to flag for you, when you minimize a function that's constrained to make sure you use Lagrange multipliers. I will put up a little tutorial about them. You do not need to spend a lot of time on them. It's just if you see some challenge where you're actually supposed to do it, just have a little light bulb to go off.