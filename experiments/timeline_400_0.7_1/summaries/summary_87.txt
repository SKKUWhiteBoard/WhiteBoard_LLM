Differentiable programming is closely related to deep learning. It allows you to build up an increasingly more sophisticated model without losing track of what's going on. In a three layer neural network, we start with our feature vector. Now I have a vector and now I can do the same thing again. I apply a matrix, add a bias term, apply an activation function. Apply a matrix which happens to be a vector. And I get a score which then I can happily drive regression or take the sign to drive classification. we're going to see a lot of these box diagrams which are going to represent functions that we can reuse and have a nice interpretation. So the FeedForward function takes in an input vector x and produces an output vector which could be of a different dimensionality. And the way to interpret what people are doing is performing one step of processing. In particular what that processing is, is taking this input vector, multiplying it by a matrix, adding a bias term and applying an activation function. So this is a very compact way of writing something that would otherwise be quite complicated. Play with ConvNets, you can actually click here for Andrej Karpathy's excellent demo. You can actually create and train ConvNet in your browser. So Conv takes an image and the image is going to be represented as a volume which is a collection of matrices, one for each channel, red, green, blue. Each matrix has the same dimensionality as the image, height by width. And what the Conv is. going to do is it's going to compute another volume of a slightly different size, usually the height and width of this volume is going. to be equal or maybe slightly smaller. going to slide a little max operation over every 2x2 or 3x3 region. So the max over these four numbers is going to be used to build this [INAUDIBLE] and so on. If you want to go into the details, you can check out this demo or you can learn more in 231. But again, I want to highlight that there's these two modules. One for detecting patterns and one for aggregating, to kind of reduce the dimensionality. And with these two functions along with FeedForward, now we can define AlexNet. Some words are ambiguous, like product can be-- multiplication or output. So there's a lot of processing that needs to happen and it's hard to kind of specify in advance. So we're going to define an abstract function. An abstract function is something that has an interface but not an implementation. A SequenceModel is going to be something that takes a sequence of input vectors and produces a corresponding sequence of output vectors. I'm going to talk about two implementations of the sequence models. One is recurrent neural networks and one is transformers. A simple RNN works by taking a hidden state, multiply by a matrix, take the input and multiply by the matrix. And then I add these two and I apply an activation function. So at the end of the day, I have the sequence model because that maps input sequence into an output sequence. And I notice that each vector here now depends on not just the input vector but [INAUDIBLE] So if you look at h3, h3 depends on x3, x2, and x1 following this computation map. RNNs are generally fairly well, but they suffer from one problem is that they're fairly local. And so one problem that-- so is a problem that we're going to try to address with transformers. So introducing transformers is fairly involved. So I'm going to step through, introduce a few things before actually defining it. So the core part of a transformer is the attention mechanism. And the Attention mechanism takes in a collection of input vectors and a query vector and it outputs a single vector. Here is one of the input vectors. x1, x2, x3, x4. I'm going to reduce its dimensionality to also 3 dimensions. And now I can take the dot product between these x's and y's. So that's going to give me a four-dimensional vector of dot products intuitively measuring the similarity between the x and the y. Now I can use those probabilities, those weights, when I multiply by x to take away the combination of the columns of x here. themselves. So in contrast with the RNN, you have representations that have to kind of proceed step by step. So each of these vectors is comparing a particular input vector with the rest of the input vectors and doing some processing. So that's an attention mechanism. You can think about this as a sequence model that just takes input sequence and contextualizes the input vector into output vectors. Layer normalization and residual connections. These are really kind of technical devices to make the final neural network easier to train. The basic building block for generation is, I'm going to call it GenerateToken. And you take a vector x and you generate token y. And this is kind of the reverse of EmbedToken which takes a token and produces a vector. And then that gives you just one vector and you can use that to generate a token. Finally we can take language models and we can build on top of them to create them. So the basic idea is that you have input, which is a sequence of words, and you are trying to generate one that's closest to the word that you want to generate. encourage you to consult the original source if you want kind of the actual, the full gory details. Another thing I haven't talked about is learning any of these models. It's going to be using some variant of stochastic gradient descent, but there's often various tricks that are needed to get it to work. But maybe the final thing I'll leave you with is the idea that all of differentiable programming is built out of modules. Even if you kind of don't understand or I didn't explain the details, I think it's really important to pay attention to the type signature of these functions.