B and C as against the infrequent one A. As a stepping stone toward cosine distance, which will behave quite differently, let's talk about length normalization. Given the vector u of dimension n, the L2 length of u is the sum of the squared values in that matrix. And then we take the square root. That's our normalization quantity there. And A and B are now close together. Whereas B and C are comparatively far apart. And that has come entirely from the normalization step. their generalizations to the real valued vectors that we're talking about. And the other class of methods that you might see come up are probabilistic methods which tend to be grounded in this notion of KL divergence. Now I've alluded to the fact that the cosine distance measure that I gave you before is not quite what's called the proper distance metric. To qualify as a proper distance metrics, a vector different choices that we could make, of all the options for vector comparison, suppose we decided to favor the ones that counted as true distance metrics. the results for "bad" using cosine distance in cell 12 and Jaccarddistance in cell 13. And I would just like to say that these neighbors don't look especially intuitive to me. It does not look like this analysis is revealing really interesting semantic information. But don't worry, we're going to correct this. We're going. to start to massage and stretch and bend our vector space models. And we will see much better results for these neighbor functions and everything else as we go through that material. Part 3 in our series on distributed word representations. We're going to be talking about vector comparison methods. To try to make this discussion pretty intuitive, I'm going to ground things in this running example. On the left, I have a very small vector space model. We have three words, A, B, and C. And you can see graphically that B and C are pretty close together. And A is kind of lonely down here in the corner, the infrequent one.