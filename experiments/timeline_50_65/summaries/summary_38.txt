So I think I just spend like five minutes, just briefly review on the backpropagation last time. So I didn't have time to explain this figure, which I think probably would be useful as a high level summary of what's happening. I'm not sure why I was running behind last time, but I'm glad I was able to do it this time. I think it's a good way to get a feel for what's going on in the system. I don't know why I wasn't able to get to it earlier. going to omit all the details. So I guess I'm drawing in this way. Like this is the forward path. This is how you define network and the loss function. So you start with some example x, and then you have some-- I guess, this is a matrix vector multiplication. And then you get some example y, which is a multiplication of y by y. And so on and so on, until you get to the end of the network. And that's how you get the network as a whole. or matrix multiplication if you have multiple examples. But this is a matrix vector multiplication module, and you take x inner product multiplied with w and b. And then get some activation, the pre-activation. And you get some post activation. You take some matrix vectormultiplication. and you get-- I don't know if you can do it yet, but it's a good starting point. It's a very simple way to do it. It doesn't take a lot of time. It just takes a few seconds. In some sense, you can summarize the backpropagation in a way like follows. This Is. How you define a loss, how you define the model. And then you have something that defines the loss. This is the so-called forward path. So basically, if I draw it, so this is, of course, what you really do is you really have to use matrix vector multiplication. And you get to activation, and then you do this. And this is the output of the model, which is the tau. implement this in computer. But if you draw it, in some sense, you are doing it in a backward way. So what you do is you say you first compute, if you look at the flow, the data flow or the process of the backprop process. So you compute the process and then you draw the result in a way that makes it look more like a backprop. But in fact, it's a forward-looking way of looking at the data and the flow. loss with respect to the output first. And this is often very easy. This is like just take the-- because the loss is something like y minus tau squared, times a half. And then you compute the derivative of the loss first. This one is just a very simple formula. And it's a very, very simple way to calculate the loss. It's very easy to do. It doesn't take a lot of time. It just takes a little bit of time, and it's very simple. with respect to-- here, I only have three layers. And then you compute the derivative of loss with respect to z2, z1, and a1. This is the order of the computation you want. It's kind of a kind of an order of magnitude. It doesn't matter how many layers you have, it's just the order you want it to be in. So it's like something like this. You have to do it in layers 1, 2, and 3. like you are accessing this network in a backward fashion in some sense. But how do you do this, each of this arrow? So this is by the lemma that we discussed. So I think we have three lemmas or three abstractions. So and each of these arrow is using each of the three lemma to access the network in some way, in a way that is unique to each of them. So that's what we are trying to do here. We're trying to get to the heart of the problem, which is how do we access this network? one of those three lemmas. If you know the derivative with respect to the output of some module, suppose this is a module. And all of those lemma is about this kind of relationship. And now you can see what this does kind of like lemma are for. Those lemma, basically, are saying that if you know dg over the d tau, how do you computedg over da? And there's another lemma,. which says that ifYou know how to compute dgover da, howDo you compute dG over dz? this module. So if you know how to compute dj over the output of the module, then you want to know how. to compute the derivative with respect to input. So all of the three lemmas are doing this list. I'm not going to the details because it's a very complex problem. I'll show you how to do it in the next episode of this series, which airs at 10pm ET on Sunday, October 14. For more information, visit the dj-demo.com website. we don't have enough time to review again, but that's the basic idea. And also, there's another thing, which is like this is only about the derivative with respect to activation. You can also compute a derivative withrespect to the weights, right? So if you know this quantity, then you can know how much of a derivative it has to do with the activation. So that's another way to look at it. It's not just about the activation, it's also about the weights. I think if you know this quantity, then how to compute the derivative with back to the last layer weight. And from this quantity,. you know how to complete. The derivative with respect to w2 is the sum of w1 and w2. This is how you get the weight of the layer you are working on, and how to calculate the derivative of that weight. The total weight is the total weight of all the layers, and the derivative is the ratio of that to the layer weight, and so on. a derivative with respect to w1. And also the same thing for b's. And this kind of-- the last row, this quantity, right, so they don't depend on, for example, after you get this, you can convert this, right? And after you've got this quantity,. you can come up with w1, w2, w3, w4, w5, w6, w7, w8, w9, w10, w11, w12, w13, w14, w15, w16, w17, w18, w19, w20. these two quantities. But this row, the derivative with respect to activations, you can only do it sequentially. You can not say, you compute this before you do this. So this arrow is kind of the orders of the dependencies between these quantities. And each of these arrow is basically a list of the quantities that are related to each other, and each of them is basically the derivative of each other. And this row is the derivative between these two quantities, and the derivative is the order of the dependency between them. done by one of the lemma that we discussed last time. Any questions? This is just an extension of the last five minutes of the previous lecture. I didn't have enough time to elaborate on this. OK. So good. So far, so good. Now, let's move on to the next part of the lecture. Do you have any questions? If so, please send them to jennifer.smith@mailonline.co.uk. We'll try to answer as many as we can. now in this lecture, and the lecture afterwards, we are talking about, I guess, a few concepts. One concept is called generalization, which is the main point of this lecture. And also, next lecture, we're going to talk about the concept of regularization. And next lecture we also talk about generalization and regularization, among other things, in the course of the lecture. The lecture will be held at the University of California, San Diego, on the evening of September 26. some of the practical viewpoint of ML, like how do you relate to your model, what you have to do in this whole process, right? So like you start with the data process, and then you has to tune the model. And then maybe you need to go back to. some of the real-world examples of what you're trying to do, and how you're going to do it in the future. And so that's what we're doing in this project. We're working on it. change your data, so and so forth. So basically, in these two lectures, I think we're going to discuss this concept. I think that generalization is probably the main thing that we are talking about here. So generalization, as you can see, it's really just about changing your data. So that's what we are going to be talking about in the next two lectures. We will discuss generalization in the second lecture and then in the third lecture, too. how well your model is performing, and syntax examples. So we're going to discuss how do you make sure your model can also generalize to unseen test examples. We have some examples, which we have seen when they are training data sets. So far, we only talk about training. But we will also discuss how to generalize your model to other types of data. We will also talk about how to make sure that your model works with other data sets, and how to get the most out of your data. And we fit some model on them. So now, what we care about is whether this model will work for future unseen examples. And we are going to discuss a bunch of concepts, the balance variance trade-off, which is a kind of a principle when you think about how test error works. We will also talk about how we can use this model to predict future test errors. We'll also discuss how to use it to predict test errors in the future. We're going to talk about some of these concepts in the next few minutes. changes as you change model complexity. And we are going to talk about some of the new phenomena people have found in deep learning, which is a little bit different from the classical understanding. OK. So I guess, that's just a very high level overview. I use a guess, I use the term 'changes asyou change model complex' to describe the phenomenon of deep learning. And I think that's a very good way to start off. OK, so that's what we've been doing. lot of buzzwords. I'm not expecting everyone to follow everything. OK. So I guess, let me start with some kind of basic notations and notions. One thing is this so-called training loss, which you probably already know what it means. For example, if you care about the square loss, then the training loss would just be this. This is the loss function you Care about when you have square loss. And other loss could be cross-entropy loss. It could be like MLE, the maximum likelihood estimator. You derive the maximum likelihood estimator for data sets. And that you use that as your training loss. You use the active log-likelihood as the training lost. So this is basically, so far, what we have focused on in the last few weeks. So how do you get a training loss for a data set? We are trying to find the answer to that. We will share the answers with you in the coming weeks. We hope you will help us out with your feedback. There are many ways to optimize it. For example, in one of the lectures, we use the analytical formulas. So we have the GDA. We analytically compute what is the minimum loss, the minimizer of the negative.loss, and how do you really implement this, and optimize this, right? So there are many Ways to Optimize It, and there are a lot of ways to get the best out of it, including using software and other tools to do it. log-likelihood. And all of the other lectures, we are using numerical algorithm to minimize this loss. So like for example, like in deep learning, we're using stochastic gradient descent. And then we have talked about Newton's method, so and so forth. But so far, everything we've talked about, we've been using numerical algorithms to minimize the loss. We're going to be using these algorithms for the rest of the lecture. We'll be using the numerical algorithm for the next few lectures. is this loss function when we try to find the minimizer of this lossfunction. Not exactly this one, but like other-- but it's always a loss function defined on the training examples. OK. So now, suppose you have obtained-- so suppose we have some parameter theta. So suppose we are trying to find a minimizer for theta of theta, and we have obtained that. We are now looking for the minimizers of the loss function. We have obtained it. have obtained some theta, how do you evaluate whether your theta is good or not? So ideally, you want the model to not only perform well on the training data because for the trainingData, you already know the prediction. Why you care about letting a model to predict something is because you want to be able to predict it well in the first place. For more information on how to use the theta in your business, visit www.theta-research.com. For confidential support, call the Samaritans on 08457 90 90 90, visit a local Samaritans branch or click here for details. you already know? So what you really care about is you care about you want to evaluate on unseen examples. So that's why the test loss is defined on unseen example. And I'm going to use this notion. So suppose, say, you draw-- the process is that you draw some. You want to see if you can do it. You can't do it if you don't see it. So you have to look at it and see if it works. And if it doesn't, it's a failure. new example, x comma y, from some distribution D. And often, this is called test distribution. And then you evaluate what's the expected loss on this new test example. So you look at l theta, which is theexpected loss of-- the expectation is over the randomness of this new new example. And that's what we're trying to figure out in this case. It's a test distribution, but it can be used for any kind of data analysis, too. example drawn from this test distribution. So what's important is that this x and y is not seen in a training. It's a new fresh example. And of course, here, I'm defining it as expectation. So actually, in place,. I'm taking average over the entire distribution. so if you really want to see it, you really need to go to the training and see the example drawn from the test distribution and see how it compares to the rest of the data. So that's what I'm trying to do. want to do it empirically, what it really means is that you draw a bunch of examples. Maybe let's call it x1 test, test 1. You draw maybe n of this. These are not the examples you have used for training. They are new examples you draw during the test. This is not the same example as the one you drew during the training. It's a new example of what you have already drawn. It is not a training example, it is a test example. time. You draw them from d, iid from this distribution d. And then you evaluate the error on this set. And you evaluate average error or average loss onthis set, on the test set. Because you know that if you evaluate on this test test, it's pretty much just a test test. It's not like you're trying to figure out what the average error is. You're just trying to find the average loss or average error of a set of data that you're looking at. approximating this expectation. You are just using an empirical way to estimate the expected value. It's an estimate, and the expectation of any random variable. One way to do it is you just draw multiple copies from the same distribution, and you take the empirical average. That's why the test is called a test of the average. It is a way to test whether a random variable is expected to behave in a certain way. The average is the average of the multiple copies of the random variable that have been drawn. set is a reasonable estimate for the test error. And just to be clear, these test examples, you haven't seen them in a training set. They are something you draw. You can draw them in advance, but you cannot let them to be seen in the training process. And there is no such thing as a perfect test example. There is only one way to test a test example, and that is to draw it in advance and then draw it again in a different way. There are no such things as perfect test examples. is a notion called generalization gap. This is basically talking about the difference between the test loss and the training loss. And oftentimes, it's not always true. When you test, you don't always get the results you want. But oftentimes the training losses are less than the test losses. And that can be a big difference in the test-to-training ratio. It can be huge. It's a big, big difference. And it can be very, very difficult to overcome. find that your model is not as good as you thought before on the training set. Sometimes it's probably a little worse. Sometimes, it's a lot worse. But generally, you shouldn't expect that your test performance is dramatically better than the training performance. And of course, there's always the possibility that the model you've been using is not what you think it is. It could be a lot better, or it could be very, very similar to what you thought it was. course, in extreme cases, you can design the set, such that this happens. But I think in realistic practical situations, I don't think you should expect that at all. So it's often the case that this gap is either very close to 0, or maybe a slightly negative, slightly positive, I think it's more likely to be close to 1. I think that's the most realistic way to look at it, and that's what we're trying to achieve in the game. or is much bigger than 0. So you want this gap to be as small as possible. So basically, in some sense, you care about two quantities. You care about the training loss and you careAbout the gap. You want both of these two to be small. If both are not small, then you're not going to get the best out of the game. So that's what we're trying to do here. We're not trying to win the game, we're just trying to get to the bottom of it. of these to be small-- or if both of them are small, then the sum of them will be small. And that's why your test loss is small. That's the hope. Your hope is this both of these two are small. OK. So this one is something you can control, and that's what we're trying to do here. We're hoping to get the test loss to be as small as possible. We'll let you know how that works out in a few days. in some sense. This is what you try to optimize for, right? But this one is harder to control because you don't-- because you cannot say, I'm going to find a theta, such as l theta is small. Because if you do that, empirically, youtry to optimize theta such as such a small theta. But you can't do that with theta that is so small. So you have to try to control theta in a different way. that the test loss is small, then you have to see the test data set. So that's why you cannot really easily control this because you are not allowed to test the set. Like you cannot choose your theta based on the loss. You can only choose theta first, and you can only do that if the loss is very small. So you can't really control this. You are not able to control this as much as you would like because you cannot see the data set or test it. then you evaluate loss, but not vice versa. So that's why the generalization gap is something that is very hard to control. At least, you cannot directly control it. And the point of this lecture is to discuss in what cases you can somewhat know this is not too big. It's to talk about how to know when the gap is too big and when it's not so big. And that's what I'm trying to do in this lecture. I'm going to try to show you how to sort it out. Like when you can hope that this is not too big? OK. And then before going to do more details, let me also define two notation-- two kind of like commonly used terminology. So of course, we are dealing with the case when l theta-- so we are mostly concerned with that case. So we are going to use that notation for the rest of this article. We will also use two notation for other cases, such as when we are talking about the shape of the body. When l theta is big, the question is, what do we do to change it? Like if you observe that your test loss is very big, then what you can do to make it smaller? That's the question you want to study. And typically, there are two failure mode in some sense. These are not supposed to be comprehensive. But I think, typically, you do have two failure modes in some way when l thetas are too big. So if l thetsa is small, you don't have to worry about anything. are in either one of these two failure mode. So one of the failure mode is called overfitting. And so overfitting, I'm going to discuss a lot about over fitting. But the first other bit is that the typical situation of overfitting is that you're in either a failure pattern or a failure overfitting situation. So that's what we're going to be talking about here. We're talking about overfitting and overfitting in a very different way than we did in the past. overfitting is that the training loss, j, is small, but the test loss is big. So you have this big generalization gap. At least, that's not a definition for overfitting, but that's a very typical characteristic of overfitting. For example, for example, in the U.S. we have a large number of people who have never taken a test before. So we have this huge gap between the training and test loss. That's a typical example of over fitting. I guess, I'll probably draw this very often-- I'll draw this figure very often in this lecture. So suppose you have some x and some y. You have some data set. I guess, the one example I'm going to do is that I's going to have someData set that I've got. I'll be drawing that data set very often, I think, in the lecture. I think that's a good way of showing how data sets can be used to make sense of the world. lives very close to this quadratic function. So it has a one-dimensional problem. So given x, you want to predict y. And you observe some-- so you have a data set. For example, you have four points. So each point is each point in the set. So x and y. are approximately quadratically. So the data are approximatelyquadratic. So X and Y. are one-dimensionally related to each other. The data set is four points, each of which is one-to-one. like this, maybe this, and something like this. You see these four blue points, and you want to fit a line to it or fit some curve to it. And the question is what curve you are going to fit? So suppose, you fit something crazy. And something like that, maybe something likeThis. Maybe something like This. Or this. Or maybe this. or this, or this. And so on and so on, until you get to the right shape. And then you can see the result. like this. Let me think about how do I use the color in a consistent way. So I guess if you fit-- I'm going to use black for the model you fit. So suppose your model your model. Sorry. Letme try to see what color I'm using for this. One moment. Let's see if we can do this. We can try this. If you fit, we'll use black. If your model is too big, we can use white. If it's too small, we're going to go with black. We'll try this again. you fit is something like this. I intend to make this model to pass these points, exactly. So this model face the data, the forward training data perfectly. So the j theta is really small. It's kind of close to 0. But the model is -- I'm drawing something crazy. But this model is-- I intend to make this models to pass this points. So it's not perfect, but it's -- it's a good start, I'm sure. you can imagine this model shouldn't generalize to anything examples. So suppose you-- if you generate some examples, and you kind of believe that, and the examples also are kind of similar, like have a quadratic relationship. Then you generate something like this, maybe somewhere here, maybe some are here. Then the model can be applied to other situations, and it can be used to make sense of the world. It can help us understand the world a little bit better, I think. you can see that the fitting to the right point becomes very worse. So much worse-- becomes much worse. The test loss is very big. So this is a typical situation of overfitting. In some sense, you are saying that you fit the data very well, but you are overfitting in the sense that you kind of like forgot about the test performance. I will discuss why this will happen. I guess, you can probably guess, but this is so far, I'm just defining roughly what overfitting means. that you are not-- you fit the training data, but you don't generalize. And another notion is called underfitting. An underfitting, basically, just means that you face something like this. Maybe let's say you fit this. And then suppose this is another model you fit. So underfitting this means that both the underfitting and the generalization are not the same. And that's a very bad thing to do to a machine learning system, because it means you're not generalizing. j theta is also big. So even your model doesn't even do well on the training set. And that is basically means underfitting. So as the word suggests, that you are not fitting the data. And whether you are in the overfitting regime or the underfitting regime, or in a combination of the two, you are likely to be underfitting or overfitting. And so that's what we are trying to fix. We're trying to make sure that we're not overfitting or underfitting our data. nicer regime, depends a lot on different things. And one kind of decision we are trying to discuss today is that what is the right model complexity. So like what are we going to use? Linear model, maybe use quadratic, or maybe fifth degree polynomial, or neural network, so on. We are going to discuss that today, and we will try to come up with a solution for that, as well as some other things that we can try to do. and so forth. So we're going to discuss what will happen if you change your model complexity, and whether in what cases, you may underfit or overfit. And kind of like as a spoiler, in some cases you may overfit, and what is the best response. Any questions so far? We'll be back in a few minutes to talk about the next phase of the show. Back to Mail Online home. back to the page you came from. Back To The page you were from. sense, like we're going to discuss two-- we are going to decompose the test error, l theta. Actually, I'm not going to show it, mathematically, because I don't think I have enough. The test error is the test loss, and l Theta is the loss of the test. We're going to decompose this into two terms, and we'll talk about them in the next few minutes. Back to Mail Online home. back to the page you came from. time to do that. But intuitively, you are going to decompose the test into two terms, which is called-- one is called bias. And technically, it's bias squared because the bias is defined as the square root of this term. So plus variance, you're going to have this you'regoing to have to do this test for a long time to see if you can get the answer you're looking for. It's a very difficult test to do, but it's worth the effort. going to define these two terms and say that the bias is going to be an increasing function. If you take the sum of them, it will be the test error. So we are going to see something like a test error that increases with time. The bias is also going to increase with time, so we will see an increasing test error as time goes on. The test error is a function of time, and the bias will increase as the time goes by. It is an increase in the time it takes for the error to reach a point. this. The bias is going to be a decreasing function as the model complexity. I haven't told you what the bias is, what the variance is. I'm just giving you kind of like a spoiler on what kind of things we're going to discuss. So the bias will be something like something like this. The variance is something like a function of the complexity of the model. It's going to depend on how complex the model is, but it will be decreasing as it gets more complex. this. And the variance is something like this. So the mechanism is that if you change the model complexity to make it more complex, then your variance will be bigger and so on. So these are-- basically, you are kind of like trying to figure out the underlying kind oflike mechanisms. This is the kind of thing you do to try and figure out what the underlying mechanisms are, and then try to change them to make them less complex or more complex or something like that. bias will be smaller. And your sum of these two functions, which is a test error, will be something like this. And then the best one will besomething in the middle. So this is kind of the quick overview of what we're going to discuss. All right. So so so so. OK. So So So This Is What We're Going To Talk About. This is What We'll Talk About When We Talk About Bias. We'll talk about Bias in the Second Half of This Article. now, I'm going to define bias and variance in a little bit more formal ways. Still not very formal, like I'mgoing to start with it's a gradual process. OK. And I'll show some examples. So any questions any questions? I'll be happy to answer them. I'll start with a little more formal definition of the bias. OK? And I will show some example of bias in action. I'm just going to give you a few examples of bias. Now, go ahead and ask your questions. so far? Why is the bias [INAUDIBLE]. Why is bias this crazy? Oh, squared, I mean. Oh, this is just-- maybe I should draw. This is just because it's kind of a unit thing. You define a bias to be the-- it's just a-- how do I say this? It's a definition. Some people call the bias square bias, actually, in some literature. Sometimes, people take a square root. It's just how do you choose the right unit. going to do is I'm going to have a running example, which is basically like this. And I'mgoing to kind of try what happens with linear, fifth-degree polynomial. And kind of use this kind of as a thought experiment to demonstrate these quantities. So let's start with linear. And we're going to start with a linear, 5th degree polynomials. And then we'll see what happens when we use a different type of 5D polynomorphism. sometimes, this is a thought experiment, but actually, we have some real data experiments in the lecture notes. Here, I'm just drawing this. But I think it's pretty much the same. So suppose you-- OK. Maybe I'll set up just really quick. So my running example is basically like what my running examples are basically like. It's like what we would do if we were doing the same thing in the real world. We would do it in a very different way. I drew above. So I'm going to have some training examples. And these training examples are something like yi is equals close to a quadratic function. Quadratic, which is just this quadRatic, imagine, of xi, plus a little bit noise. This is a small noise. So that's why these blue are why these examples are blue. And that's what I'm doing. I'm drawing a picture of a noise. And I'm trying to make a noise out of it. points are not exactly lies on the quadratic. It's just there's a little bit fluctuation. Sometimes, they are called this h star xi. Just for the sake of terminology, I think, sometimes, they call this the ground truth. And sometimes, the true is that they are not lies at all, but they are just not exactly true. They are not the truth, they just aren't exactly lies. They're not exactly the truth at all. They just don't look like it. function you are trying to find out. But of course, you don't know it. You want to try to recover it. And I'm going to do a thought experiment first. I am going to start with linear model, and then I'mGoing to try. to recover the function you want to recover, but of course you don’t know it, and I’m going to. do a few experiments. I'm Going to try a few. experiments first, to recover. the function. fifth-degree polynomial, and then I'm going to try quadratic. So linear model. I guess, you can probably see, guys, what will happen. So I would draw this again. So you have these four data points, something like this. Then what happens with linear model? I guess you can see what it looks like. It looks like a linear model, but it's a different kind of model. It's not a perfect model. But it's better than nothing. is that you have these four points, what's the best linear fit? Probably would be something like maybe this for this particular data set. And you can see what are happening here. So maybe-- let me see-- how do I-- maybe let me erase this for a moment. I'm going to go back and look at the data again and try to figure out what's going on. And then I'll come back to you and say, "What do you think about this?" to redraw this again. So for linear models, I guess, you can see a bunch of properties. There's a large training error, training loss or training-- let's call it loss just for consistency. And there's a big training loss because I guess-- what's your prediction on? I don't know. I'm just going to say that there is a large loss in the training process. And that's a bad thing because it means that the model is not very good at predicting the outcome. the training data set. This is your prediction for this x. So this is x1, and the prediction is here. And the prediction for x2 is here and the predictions for x3 and x4 are here. You look at the distance between the prediction and the Prediction. And you look at how close the two predictions are to each other. The prediction of x3 is here, the prediction of X4 is here; and the predictor of x1 is x2. And so on and so on. true label. The training error is pretty big. So this is underfitting, by our definition of underfitting because the tuning is already big. And now let's think about so what you should blame. Why the training is big? What's the culprit? The training is a big problem, so let's look at what's wrong with the training. It's the training that's the problem, not the distance or the training error. It is the training which is the problem. It can be fixed by changing the label. culprit, I would argue, is that no any linear model can fit your data. And it's not because you don't even have enough data, it's just because even you have more data, a linear model wouldn't work as well as you'd like. It's not just-- no anylinear model can work. It is just because you have a lot of data and no model that fits it all. And that's what we're trying to figure out. We don't know how to do it yet. well. So this is just because the linear model is not expressive enough. And this is called bias. So when in this kind of settings things happens, like you have the bias. Well. So the bias is basically like it's saying that the reason why-- I don't know. I'm not sure what the answer is, but I think it's something like that. Well, that's what the bias says. So I think that's the answer. So that's where we're at. don't know exactly why people call it bias in the very first time. But I think you can-- see kind of the relationship. So you are imposing a linear structure, but the true data is not linear. So it doesn't matter how many data you see, as long as you just insist that this thing is linear, you're going to fail. Because this is the wrong belief about the relationship between y and x. So that's why this is called bias. And you cannot mitigate-- cannot be mitigated by more data, as I said. And actually, it can also not beMitigated by less noise, even though there is-- and by less Noise data. Because even you have more data and with less noise,. you can imagine what happens. So suppose, you see a little more noise, you can see it a little better. And that's what we're trying to do. We're not trying to make more data. We are trying to improve the quality of the data. more data. Suppose you see some more data as training data. And maybe let's say, you just-- suppose in extreme case,you just see everything exactly on this quadratic line without any noise, still, if you think about what's the best fit. For example,let's say just see all of the all of this data in one place. That would be an extreme case. But still, that would be the best way to look at the data in this way. The best fit probably would change a little bit. It probably wouldn't be exactly this. Maybe it would be something like this. I don't know. You have to trade. And what's the best fit? The best fit is a little different than what you're looking for. That's true. It's not exactly this, but it's a little better than this. And it's probably a little more like this, too. It would be a little less like this or this. It could be a bit more like that. off here. Because whatever you fit, if you fit this, then you don't fit some of these examples. If you do-- there's no any option. Like whatever, it just because the model cannot represent quadratic function. That's it. So that's the typical situation, where you have a large bias. And that's what we're trying to get rid of here. And we're going to get there. We're not going to give up on that goal. mathematically, so the way you define bias, so here, I'm just only talking about some characteristics of having a large bias. So bias is-- I guess, actually, there's some approximation here, depending on what you're talking about. So mathematically, one way to define a bias is that you can say this is the-- so bias is -- it's a way of saying that you have a bias, that you're biased in a certain way. That's the way it works. The best error or loss, you can get with even infinite data. So I guess, suppose you have infinite data, you have a data set with infinite data,. following the same kind of property, so like all generated from this quadratic noise. But roughly speaking, it's the best error you could get with that kind of data set. It's not exactly what your model is, but it's what your data set is. And that's what we're trying to do here. then what's the best you can do? And that's called bias. And you can kind of see that it's probably important for bias to be small because if bias is large, even with infinite data, you cannot do anything. That's the problem with linear models. Any you question? Can you question it? If you can't, don't answer it. If you have a question, please email it to jennifer.smith@mailonline.co.uk and we'll try to get back to you. bias because something like the distance from the [INAUDIBLE] model or something like that? I think that's pretty much-- so for this case, they pretty much are the same. So basically, so in this case,. it's exactly true that the bias is the best linear model. So the closest-- like, the closest model is the one that is most like the bias, which is the bias. So that's the one we used in our study. That's the bias that we used. the closest linear-- the model that is closest. And that error, that closeness, is the bias. Because when you generate infinite data, basically you just generate the ground truth, the whole line, if you have no noise. And by that, by that we mean the linear model that's closest to the groundtruth. That's what we're trying to get at here. We're looking for the closest linear model. We want to get to the truth. We don't want to go too far away from it. you mean, like I don't know. Within the same model, right? You are not changing the model path. You're only using linear. Like you cannot-- OK. So bias would be the best [INAUDIBLE] linear model [INAudIBLE]. Exactly. So in some sense, technically is you say you say, "Bias is the best way to look at the world." You're not saying that. You are saying that bias is the most effective way to see the world. bias is property of the family of models. So the linear model family has a large bias, right? Yeah. We are always talking about model family. So we are talking about eitherlinear model family or family of fifth-degree polynomial or the family. The family of linear models is the one we are most likely to be talking about. It's the one that has the largest bias in terms of the size of the data set. It is the family that is most biased in the way the data is structured. of quadratics. OK. So this is the bias. And now let me talk about the variance. And here, there is-- I'll come back to the variance for this model. But here, the variance is, in some sense, you can say, it's not very important. Only the bias is the important thing in this case. It's the bias of the model that is the most important in the study of quadratic equations. That's what we're trying to get at. culprit. And now, I'm going to show cases where the variance is the culprit to blame for. So you have-- [INAUDIBLE] four points. So the model is something like h theta x is. So I guess, I's going to redraw this. So now,  I'm going to fit a fifth-degree polynomial. So  the model is  something like h theta x is. And  in this case, the variance is the culprit. some theta 5x to the 5 plus up to theta 0. But recall that we can do this with linear regression because you just-- this is still linear in the theta. We have a homework question on this. We also talk about how to do it with kernel methods if if you have a problem with the kernel method. We'll talk about that in the next episode of this week's show, "The Next Episode," at 8 p.m. ET on CNN. you care about efficiency, so on and so forth, right? So we are able to fit this. And in the lecture notes, actually, there are some visualizations of the real models you're going to fit. So here, I'm just going to draw it. So if you fit the fifth-degree polynomial, you can fit it. You can fit the model to the fifth degree polynomials, and so on. So we can fit this model to fit the five degrees of freedom. so probably, you're going to get-- a fifth-degree polynomial can go up and down so many times, several times. So the higher the degree is the more times. I think, technically, you can have, Ithink, four local maximum or minimum, four or five, something like that. So that's what I'm going to try to do. I don't know if I'll be able to do it, but I'll try to get close to it. you can go up and down. Because if you have a quadratic, the only thing you can do is this, or maybe this. And for cubic, you can doing this. For fourth-degree polynomial, you could probably do something like this. So just just go up or down, it doesn't matter. It's the same for any other type of polynomials. So the exact details here don't matter, it's just a function of the type. the point is that if you have high degree polynomials, you can be more flexible. And then if you fit the data-- ifYou fit the polynomial to the data, then possible, you're going to get something kind of pretty flexible, something like this. And actually, if you really look, you'll see that it's possible to do this with a lot of other data as well. And that's the key to the whole thing, I think, and it's a very interesting thing to look at. up some-- like this is not required for this course, but if you look up the book for the calculus of like polynomials, you know that if you have four points, there's always a fifth-degree polynomial with a path for all of them. So in some sense, if you don't know that, you're not doing it right. You're doing it wrong. You've got to learn how to do it right, and you're going to have to learn some new things. have enough points, and your degree is high enough, then you can always make the training error 0, literally 0. So in this case, the trainingerror is literally 0, so I guess, this is expected. And the thing is that this is overfitting. So what's the problem here? Why are we doing this to ourselves? And why do we want to do this to others? And how do we get out of this? And what are we going to do about it? it's overfitting? So why the test is not good? So in some sense, the intuition is that this kind of model fits. So it's fit to the spurious patterns in the small and noise data. So this is because you don't have enough data, and your model is not fit to that data. The test is to see if your model fits to the data, not the other way around, and if it doesn't, if it's not overfitting, then you need to change it. model tries to explain all of this small perturbations, small noise. And because it overexpressed the small noise, at loss, it kind of like didn't pay enough attention to the more important stuff. And the reason why you can overfit to thesmall noise, the following data, is because you overfit the data, and it doesn't focus on the important stuff, which is the big stuff. That's why you get the big data, but the small data doesn't get seen. are so flexible. As long as you just have four points, whatever crazy patterns you see, you can always find a degree of 5 polynomial to explain it. So whatever patterns you sees in four data points, like you can explain it, like a polynomials. So that's what we're trying to do here. We're looking for patterns in four points that we can explain with a 5-point polynomal. That's the way we're going to try to do it. It's like you are looking at-- you are kind of like overfitting to the spurious patterns, but instead of the big pattern. The big pattern is this. The spurious patterns are the fluctuations in some sense. And so in other words, I think you are explaining the noiseInstead of the ground truth. And again, how do you make this intuition a more formal? OK. So like how come your model can explain like everything and anything like a random.it. So that doesn't sounds right. of things I can say about this intuition is that this is saying that you are sensitive. Your model is sensitive or maybe kind of like specific to the noise. How do I formulate this? Like one way to kind of formulate this a little bit more mathematically is that one way is to say that you're sensitive to noise in a way that other people are not. I think that's a good way to think about it. I don't know if that's true for everyone, but I think it's probably true for some. you can consider to redraw the samples. And you ask whether after you read all the samples, are you going to see the same model again? So you redraw some new samples with different spurious patterns. They are spurious because they are noise. If your model is specific to the spurious patterns, you are going to learn the new spurious patterns and have a different model. And if you are not specific or sensitive to spurious pattern, even you have a new data set, you probably shouldn't. change much, but you should still be somewhat the same. You should still opt for the same model. And it turns out that if you have the 5-degree polynomial , you redraw the data sets, then you will find a new model. So what happens is that suppose you reach you reach the 5th degree of the polynomials. You will then find a model that is the same as the one you started with. That model is known as the 4th degree model. all the data sets-- in the lecture notes, there are some real experiments, again, but here, I'm just going to draw them. So suppose, for example, now you still have the same ground truth, but you observe some-- maybe, let's say, here, he's going to have something like a point like that. That's what I'm going to do. I'm not going to say what it is, just what it looks like, but I'll draw it. this, maybe they're like point like this. Maybe we want to keep this. And I'm going to try to make the pattern rather different. Then, maybe you're going to get something different. Maybe, I don't know. You try to find out what the degree of the polynomial is. Maybe you want. to keep it like this, maybe. you want to change it to something else. Then you get a different pattern. You get a new pattern. And then you get another pattern. to get something like this. Actually, these two are still similar, but I can't draw anything. Empirically, you will see that they will be different, just because any small perturbations of this would change a lot. But maybe, you got this. You can also do some local thing, Actually, you can alsoDo some local things, if you want. OK. Now, let's see what you can do. Let's see if we can do this. where suppose you move this points a little bit lower, then you probably will change this function a lot. So just because you are very sensitive to the data points. I guess, we got the same number of samples. [INAUDIBLE] not using the number ofamples. So far, I'm saying. So much of the time, we get the same amount of data points, but we get more data points if we move them a little higher or lower. We get more information from moving them a bit lower. that you draw the same number of samples with similar ground truth-- the same ground truth and the solution. But just their randomness are different. You are using different noise. And that's a good question. That's exactly what I'm going to talk about next. OK. One moment before that. I'm sorry. I meant to say that I was going to speak about the noise. I was talking about the randomness of the samples. But that's not what I was trying to say. So basically, OK, just to summarize here, if you redraw all the examples and you find that a large variation between-- so suppose, you have a-- so you so you have -- so you call this-- So basically, you define a variance to be, in some sense, the variations across models. And so that's what we're trying to do here. We're looking at the variation across models in a way that makes sense to us, in terms of the data we have. learned on different data sets. So for example, you draw five data sets, so each data set has four examples, maybe. And you get five models learned on five different data set. So if you see a lot of differences between these models, so then, then, you then, see how these models work together. So then, we can use these models to make better decisions about what we want to do with our data, and how we can improve our knowledge of the world. that means you have large variance. And if you don't see a lot of differences, then you don’t have a big variance. That's the somewhat formal definition of this. We will have a little more formal version of this, but this is the idea. So maybe, for example, if you’re in a city that’s very different from the rest of the city, you have a large variance, and if you're in a town that”s very similar to that, you”ve got a large variances. get a new data set, you get something like maybe here, here,Here, and here, and maybe you're going to learn something very different, maybe something like this. So here, at least, you can see this one is very different from this one because on the left hand side here, you are looking at a different data set. So on the right hand side, you're looking at the same data set but it's a very different way of looking at it. are going up, here, you are going down. So that suggests that you have large variance. And now talking about data, so suppose, so this one of the characteristic of variance is that variance is something that can be reduced if you have more data. And in some sense, that's what we're doing here. We're trying to reduce the amount of data that we have to make sense of the data we have. We are trying to get a better understanding of what's going on. variance is caused by lack of data. And it can be mitigated if you have more data. So let me continue here. I should just keep all of these markers in my hand. Otherwise, I have to walk back and forth. OK. So the variance, and sometimes, you can say this is caused, at at least, partially, at least one cause is that this is causing by lack. of data, and of course, it's probably, you cannot saying this is only caused by loss of data because if you. have a different model of a variance-- and sometimes there are two. reasons. One thing is like you have lack of data, and the other is you have too expressive models. And these two things are kind of like relative to each other. So if you have a very expressive model, but your data is really, really big, then probably, it's OK. If you have an expressive model and you don't have enough data, then it's probably not OK. It's kind of a trade-off between having too much data and having too little data. On the other hand, if you have not too many data, but you have very, very simple model, then it's probably still OK. The mitigation is that either you get more data or you have simpler model. So technically, you don't have more data. If you have moreData, you should already use them already. But for the understanding, let's see, for example, what happens if you has more data with this thing. Let's see if this are the issue, the reason, then how do you mitigate the variance. you have more data, and you still fit a fifth-degree polynomial. This is the ground truth. And you observe a lot of more data. So you have a million data, roughly. There's a little bit fluctuation, of course. So now you want to know what happens when you get a lot more data and a lot less data. You want to find out what happens if you get more data but less data and more data at the same time. That's what we're trying to do. to fit a fifth-degree polynomial. What happens will be that this is probably not entirely obvious-- OK. One obvious thing is that you probably wouldn't do anything like crazy as this right. Because if you do this crazy thing, maybe this crazy things goes through some points, but you cannot. You can't fit it into a fifth degree polynomials. You cannot. fit it to a fifth degrees polynomalies. It is not possible to do it right. go through all the points. Like for example, you can see here, there's a big match between this part and this point. And here, you have some mismatch, right? So this one wouldn't give you even a small training error. So this is not a best model fit on the model. It's not the best fit for you, but it's not a bad model fit, either, because it doesn't have a big mismatch. It just doesn't fit the way you want it. training data. So what you really will fit, like if you minimize the error on the training data with this so many training examples, then what you will get is probably something like this. Maybe there are still some small fluctuations. It's not like necessarily matching exactly matching exactly, but it's probably going to be a little bit more like this than this. It will probably be a lot more like the previous picture, which is a bit more of an example of what you would get if you had more examples. the ground truth, but you have a small fluctuation, but it will be something like this. Because if you don't do this, then you wouldn't fit the training data as good as well. This is kind of more like a quadratic. But a fifth-degree polynomial-- the family of the degree-- is the one you want to use for training data. It's kind of like a fifth degree polynomials, but with a higher degree. 5 polynomial contains the family of quadratic function because you can just set your theta 5, theta 4 to be 0, then you get aquadratic. So empirically, what you're going to find is that probably, if you really look at the details, the best fit model is to use the 5 polynomials as a fit model for the 4th and 5th coefficients in the theta. The 4th, 5th, 6th and 7th coefficients are all the same, so you can get the same result. degree 5 polynomial. But the set of five, therefore, the first few coefficients are very, very small. So effectively, you are just very close to a quadratic. [INAUDIBLE] What is suppose to be errors because with complicated models, it's harder to train. Because it's so many more possible local coefficients. It's so much more difficult to train [INAudIBLE] because of the number of possible local coincides in the model. minima [INAUDIBLE] So the question is that another possibility is that a failure mode is that you just couldn't find this degree 5 polynomial because some optimization issue. Even though there exist one, that is very good that fits the data, but you couldn'tfind it. That's probably not true. That would be the most likely scenario. It would be very unlikely that you could find it, but it could happen. It could be that you can't find it at all. for degree fifth polynomial for this one toy example, just because this is very simple. But it could be possible for some other cases, where the model does exist, but you can find it. So this is something that we don't discuss, at least, in the scope of this lecture. For more information, visit the Mathematical Foundations website or the Mathematics for Children website. For further information, or for more information on the mathematics for children website, visit the Math for Children website. So in this lecture, we are assuming that you can just-- optimization always works. You always find the best model. So if it exists, then you can find it. So that's why I'm like in this case, even have a lot of data, and even you have a very complex data. Even you have to be very complex to find the right model. But that's what we're trying to do here. We're not trying to find a perfect model, we're looking for the best one. model as a degree 5 polynomial or even degree there's always exist one model that works, which is like something like this, like the ground truth. And we'll find it. For this case, definitely, we will find it because it's a linear regression problem. You will find the best model. It's a Linear Regression Problem (LRP) and it's the best way to solve it. The best model for this case is the one we have right now. OK. And also, another, maybe just to answer the question. In some sense, the problem you are referring to is easier to detect in some sense to some extent. It's not always true because at least, you can detect that from the training. So here, we are more talking about the problem that you are talking about. We are talking more about the problems that you're talking about, and we're going to get to the bottom of it. We're not going to give you the answers you want. about generalization. So here, when I say more data, I really mean that you have-- you just collect-- you have more data from the same distribution. So OK. So any other questions? What happens to the [INAUDIBLE] You got more data. You're getting moreData. Yeah. About generalization, about generalization and about data collection. About data collection and data collection, about data gathering and data analysis. About what happens to more data and how it's collected. same distribution. From the same distribution. So like if you collect more data from-- yeah. So in some sense, you kind of like the mindset-- I'm not saying this is universally applicable to every situation, but the mindset we are in is that, for example, you have -- you have to collect data from more people. Yeah. So that's kind of what we're trying to do in this study. Yeah, that's what we are trying to get at. We don't know if that's going to work in every case. how do I say that? You have a lot of like medical images. So like they are-- for example, there is a million patients with the cancer diagnosis kind of thing. But not all of the data are labeled. So only probably, at the beginning, four images that are labeled at the start of the story. That would be the first four images. And then the rest would be labeled as they come in. So that would be like the first two or three images. Then the rest of them would come in as they came in. labeled as cancer or not, so on and so forth. But these four images are samples from this big population. And now, I'm asking I found out my variance is very big. So how do I mitigate that? So probably one thing is that I can just sample more data, says Dr. H.A. Hellyer. "I found out that my variance was very big," he says. "So probably onething is that we need to do more data sampling," he adds. from the same-- I have like 1 million and label examples. I had four labeled ones, and now I say, I'm going to collect more labels. So I sample like another like 100 examples from the same distribution, and then I label them. And then I run the algorithm, andthen I run it on the examples that I've labeled. It's a very complex algorithm, but it's very, very simple. It just takes a lot of data to get to this point. the variance will be smaller. How do we know the [INAUDIBLE] the ground truth? It is a linear structure. So the question is that how do you-- like if you don't know the groundtruth, so howDo you know how to do it? How do you know the truth? How can you know what the truth is? So how can you do it if you're not aware of it? So that's how you get to the truth. [INAudIBLE] Actually, [INA UDIBLE] to ground truths. When you don't know ground truth, so all of these are so far are for analysis purpose. You cannot really exact like-- let me think. That you are having a large bias? When we don’t know the groundtruth, you cannot really exactly know. When weDon't know the Ground Truth, You Cannot really Exactly Know. That You are Having a Large Bias? When We don't Know the GroundTruth, You Can't Exactly Know The Ground Truth. When We Don't know The Groundtruth, You Don't Know The ground truth. When You don't Knowing The Ground truth, You Won't Be able To Tell The Truth. the ground truth, I think you cannot exactly compute the bias. Because the definition of the bias, actually, requires you to sample a lot of data. So there is no way you can evaluate the bias exactly. So typically, what you do is what we do is we try to find out what the bias is, and then try to measure it, and try to get it to be less or more biased, depending on what the data is, to try and find the bias in the data. you say, you fit the data on the training set. And you see you're underfitting. And that's when you say-- underfitting means you have a large training error. For overfitting the graph that's right behind you, the overfitting bias is the same as the bias in the data you're trying to fit. That's when it's time to change the way you look at the data to make sure you're not overfitting it or underfitting it. It's the same with the data that's behind you. bias square [INAUDIBLE] what's the third one? The third one is the sum of them. This is the test error. And the bias is the total of them? Bias is this one. I'll discuss that in a moment. Because I didn't-- when I drew this, when I draw this, I didn’t-- when he drew it, he didn't draw it. He drew it with his eyes open. He didn't have his eyes closed. I didn't even tell you what this is. I'll go back to come back to this. Are we [INAUDIBLE] for highly imbalanced data set? So maybe let's discuss this offline. I'm not sure whether this-- I think, it probably requires more-- the imbalanceddata set is pretty often. Like we like to say, "We don't have to be perfect, but we can try to be the best we can be," and that's what we try to do. have research on that. But maybe it's not exactly related to the context here. Maybe we can discuss offline. OK. I think I do have something to say about the variance, and then I'll come back to the trade-off. All right. So now, let's see. Any other questions? OK. So Now, let't see. What do you think? What would you like to see happen next? Tell us in the comments below or on Twitter @CNNOpinion. let's briefly summarize. If you have a bias, this is really just about the lack of models expressivity. It's something intrinsic, nothing to do with data, right? This is just a lack of-- if you have large bias, that means you haveA lack of expressivity in your model. The model is the thing that determines whether or not you are a good data analyst. The data is the data, and the model is what determines whether you are an expert or not. is not expressive enough. Doesn't depend too much on the data. I guess, for linear models, you can just say, doesn't depend on the [INAUDIBLE] of data for non-linear models. There is some technicality, which you don't have to make-- like the only reason why I had much is just because I had so much data to work with, I didn't want to waste it. I just wanted to get it out of my system, I guess. because there's some technicality that prevented me from saying this is exactly irrelevant to number of data. But you should basically just believe that it's intuitive. It's not a notion about how many data you have, it's really about how expressive your model is. And variance, if you have a model that is expressive, can be a big factor in how successful you can be. It can be the difference between success and failure for a company or a company. It could be a game-changer for your company. large variance, then it could be two things. One is lack of data. And another thing is that you have too complex of a model. I guess, I'm just repeating and summarizing. And then, I guess we can see this trade-off. So I'll go to here. And also, also, I'd like to talk to you a little bit more about how the data is collected and how it's used in the models. I'll be back in a few minutes. there is way if I were to prove that test is equal to a bias plus variance. I don't think I have-- I will see what I have time to discuss that. But you can also prove the test error isequal to bias squared and variance. But maybe, let's go with the bias squared plus variance approach. That would be a better way to look at the test, I think, than the bias plus Variance approach. It's a little bit more complicated, but I think it's a good idea. just draw this from scratch. So this side is the model complexity. This is the test-- this is how do you draw the bias on this curve as smaller complexity change. So we say that the bias is large, it's huge. So let's first think about how to draw the biases. We want to see how the bias changes as the complexity of the model changes. We need to see the bias change as the size of the complexity changes. So, we need to know how big the bias becomes. because the model is not expressive enough. That means that if your model is more expressive, then your bias should decrease. So that's why the bias is a decreasing function as the model complexity. So this is the bias. And now let's think about how do you draw the model in the first place? That's the first step. The second step is to make the model more expressive. That's where the bias comes in. And the third step is the drawing of the model. variance on this thing. So we said the variance is caused because you have too complex of model. That means if your model is more and more complex, then you should have bigger and bigger variance. And a test error is the sum of all the test errors you make. That's why the Variance is like this. And it's caused by having a too complex model, which means you have a big test error. And that's why it's so difficult to get a good test result. of them. So the test error is like a U curve thing. And so the question you want to answer is that if you change the model complexity, what is the best test error, right? So thetest error is the sum of these two. Of course, it's not the same thing for every model, but it's the best way to test it. So that's what we're trying to do here. We want to find out what the best model complexity is for a given model. it means that it's somewhere in the middle. So actually, I'm going to tell you something different from this in a moment. But suppose you believe in this, then what the conclusion, the implication of this is that you should somehow kind of find a sweet spot when you choose. It's a bit of a paradox, but I think it's a good one. I think that if you believe it, then you should try to find that sweet spot. I don't know if you can do it, but if you try, you might. the model complexity. And the bias is high, it means you are underfitting. It means that your training error is big. So basically, when you see the training error, you kind of see your biases. You kind of believe that your bias is too high, so you underfit. You underfit your model complex. Then suppose your model Complex is very small. Then what happens is the biases is high. Then you underfitting your modelcomplex. Then the biases are high. that's why you should increase the model complexity. And at some point, you find that you are in other regime, where the variance is too high, then you should stop. So basically, you increase theModel complexity to some extent until your bias and variance has a right trade-off. One of the reasons for this is to make it easier for people to make decisions about how much data to use in a model. One example of how this works is that you can use more data to make a decision about how to use the model. of bias and variance first change, did you use different type of model [INAUDIBLE] So I think this figure, so this is the-- OK. You ask a good question. So here is the model complexity of the model you use to learn your parametrics model. So when you're asking, 'Did you use a different model?' you ask, 'What model do you use?' You say, 'I don't know. I don't have a good answer for that' about what happens if the ground truth is different. I think this is not very sensitive to what the groundtruth is, right? There's always a trade-off. But where the trade-offs come from, where the sweet spot is, would depend on the groundTruth. So for example, actually, that's a different story than what you think it is. That's a very, very different story. It's not the same thing as saying, 'I don't think it's a good idea to change the way you look at things' very good question. For example, suppose you, for this data set, So probably, the best thing is to use quadratic. Quadratic has small enough bias because it is, in principle, expressive enough to express our data. So that's why quadratically has small bias. And also, quadRatic is probably, among all, the most expressive. of all languages. It's the language that we use to express ourselves. So it's probably the best one to use. the models with small bias, among all the models that can express your function, quadratic is the least complex. That's probably the best solution. And if you really run the algorithm, thequadratic, you would probably recover something very close. But if you're going to go that far, you need to have a very good idea of what you're trying to do. And that's why you use quadratoin, which is a very complex model, but it's also very, very simple. The sweet spot is like the best trade-off is achieved at cubic, maybe. They don't necessarily have to match each other because it also depends on the data. For example, suppose you are-- maybe let's give you an example. Suppose to Suppose to, it is cubic, then maybe the sweet spot  is at cubic. To get to cubic, you need to have a lot of data. You need to be able to handle many data points at once. You have to have enough data points to get to a cubic sweet spot. say, your ground truth is a degree 10 polynomial. But it's somewhat look like a linear function. So suppose your groundtruth is like almost linear, but with a little of a kind of like small fluctuation. But you don't have a lot of data. You just have like five. You have to think of a way to make it look like it's not linear. So you can make it seem like it is, but it's actually not. It's like a little bit like a small polynomials. data points. So you just have five training data points. And now, if you want the bias to be literally 0, then, of course, you should use degree 10 polynomial because that's only case you are expressive enough. But then your variance is too big. So the trade-off here probably is probably not worth it. But it's a good starting point for the rest of the study. It can be used to learn more about how to use the data set. is closer to be a linear. Because if you use a linear, your bias is not zero, but still small enough, right? And in that case, the variance is small. So the bias, the trade-off, depends on, for example, how many data you have as well. We just [INAUDIBLE] We just have to be more careful with our data, I think, than we used to be. I don't think we're there yet, but we're getting there. for the loss function. So how can [INAUDIBLE]. That's a good question. And the answer to that is that no, you cannot compute the bias and variance. And all of this, all of what we discussed today is more about some internal understanding. So this bias and various is not a problem for us, it's just a matter of internal understanding, and we're trying to get a better understanding of how to work with it, and how to use it. something you can-- at least, in some case you, can estimate them a little bit. But typically, you probably shouldn't really actively estimate the bias and variance in your research. These are mostly just for-- its internal understanding for our research, for ourselves, but not necessarily something you, empirically, evaluate. So, you don't really want to try to do that much of the time. But you can estimate some of the biases and variance, and that's OK, as long as you're not trying to do too much. I guess, so one question, I guess, many of you probably are wondering, if all of these quantities cannot be even evaluated, how do you choose the right trade-off? What's the optimal model complexity? So what you do is actually, that's going to be, I think, what we discuss mostly in the next few minutes. I think that's what we're going to discuss mostly, in the second half of the show. I'll be back in a few hours. next week, next lecture. So this Wednesday, next week. Yeah. So the variance and bias are just for understanding. Empirically, what you really do is that you try a lot of different models. And you select based on a validation set. But this picture would help you a little bit. It's a good way to see how different models work together. But it's not a perfect picture. It doesn't show the whole story. It just gives you an idea of what's happening. in some sense. Because for example, suppose you have tried this and this. And suppose, you believe that this is a U curve, the test error is aU curve. Then should you try even bigger models, bigger family? And so on and so on, until you have twice four model complexity. And then you have a test error of a U. curve. And so you have to change the way you test your models. And that's where the error is. You have to make a change in the way your test error works. of models? Probably, you should. Because you kind of believe that it will be even worse. So you should just try even more in the middle. So that's what this understanding will help you. OK. So there is some more formal definition of the bias and variance. And that's in the book, "Bias and Variance: The New Science of Bias and Bias in the Workplace" The book is published by Simon & Schuster, and can be purchased on Amazon.com. the lecture notes in section 8.1. I think I don't have time to discuss the formal definition. Even I give the definition, I probably wouldn't be able to give you the proof. The proof is actually relatively simple. So if you are interested, you can read that section yourself. I believe I have covered the most of the main points in this section. I hope you have enjoyed the lecture. I will be back next week with the rest of the lecture series. Back to the page you came from. don't think it's required for the exam or anything, but it's a relatively simple word if you're interested. And also, just this kind of bias and variance trade-off, it's not that always easy to achieve, mathematically. So for square loss, there is a classic, well-established kind of decomposition. But if you don't have square loss,. youdon't have MSE, that means squared error, if you have cross entropy loss. Actually, it was an open question. How do you formally decompose this? So that's why in the lecture notes, we only talk about square loss. But the intuition is still kind of fun. So if you don't care about what exactly definition of bias is, it's fun to talk about. But if you do care about the definition, you can talk about it in the lectures and in the literature. It's still fun to have a little bit of fun with it, I think. But it's kind of hard to explain to someone who doesn't know what bias is. So I will spend the next 20 minutes to talk about a new-- something that is actually challenging this picture. So something that-- so this is maybe just follow more context. So this kind of like a U curve test error and bias-variance trade-off. This has been like discovered or discovered or this has been called a "U-curve test error" or a U-curvy test error or a bias test error. So I will talk about this for 20 minutes. kind of like analyzed for I don't know how many years, maybe like 40 years or something like that. "This is like a very classic. However, people realize that there are some very bad things that have happened in the past," he says. "I'm not a historian, so IDon't know exactly which is the first time this is discovered. But this is like one of the classic examples of how the human mind works," he adds. "It's a very, very strange thing to be able to do." issues with this understanding, especially we realize that in deep learning, like you-- actually, people start to realize this in deep, learning but actually, it turns out that even this understanding has an issue for linear models. So this understanding is not complete. It misses some other things. So that's what we're trying to get at with this study. It's a step in the right direction, but it's not the end of the world, it's just the beginning of the journey. what I'm going to talk about. And this is an area of research productive in the last, probably, three or four years. So let me try to find out where should I erase. So this phenomenon that people observe, empirically, at the beginning, and then analyzed theoretically, this phenomenon is this phenomenon. This phenomenon is the phenomenon of the human brain. And it's a phenomenon that we can observe and then analyze theoretically. And we can find out what it is. called double descent. If you are a historian, then I think actually this phenomenon actually dates back to something like 1990. Some papers, actually, at that time, also point out this issue. But I think it just becomes popularized and more relevant these days. And what does this mean? It means that you can be of two different races at the same time. It means you can have two different sets of DNA. It's like having two sets of parents at once. And it's a very interesting phenomenon. that, so basically, I've told you that this is test error. This is model complexity. I guess, technically, here, I'm writing the number of parameters because I want to be precise. Like I'm measuring the model complexity by how many parameters you have. And the classical belief, as we discussed, is that if you have a lot of parameters, it's a good model. If you don't have enough parameters, then it's not as good as you think. is that this test error should have this U curve. But then, people realized that this is a striking thing. So people realize that if you increase your model number of parameters even more, at some point, you will see that it will be like this. So that's what we're trying to do with our test error. We want to make it more like a U curve, so that it looks more like the U.S. curve. That's the way we're going to try to get it. basically, this is the new regime that people got. This is the second descent of the test error. That's why it's called double descent because there is a decent here, there's a descent here. Everything in the blue part, is what people didn't realize as much as in the last. It's a new regime, but it's not as bad as the last one, I think, because it doesn't have as much of the same errors in it," he said. four years, last four or five years. And these are the so-called overparameterized regime. In some sense, this is the regime that if you ask someone 20 years ago, then then the answer would have been different. So which means that in this regime, typically, the number of parameters is larger than the number. of data points. It's a regime that in some sense is the same as the one that we had 20 years earlier, and that is the one we're using now. they will say, this regime is just that no go zone because you should see very, very bad test error. But it turns out that if you have more-- you make it even more extreme, you make the number of parameters bigger in the numbers of data points, you may see even more errors. That's what we're trying to avoid. We don't want to be in a test error zone. We want to make sure that we're getting the right data. We're not just trying to get the right test error rate. actually, not in all cases, but in some cases, you may see the-- actually, I would say, in some case, like in many cases. I'm not sure how to quantify this, but at least in a lot of cases,you will see a second descent. So that's the striking thing. That's what I'm trying to get at, that's what's so striking about it, that you can see the second descent in some of these cases. It's not always the same, but it's there. Is this because we are [INAUDIBLE] the one with much more data? Not directly, I say. Because this is-- at least, on the surface, if you look at this, so this regime is the regime, where the parameters is bigger than the number of data points. So if you want, you can go to the other side of the room and say, 'This is the way to do it,' and they'll say 'Yes, that's the way' to find the right course, I'm not saying-- like you probably will say, at least, to be in this regime, probably, you need to compute. You need a lot of compute because probably, like 10 years ago or 20 years ago, you cannot even afford to run experiments in this world, he says. "You need a very, very large amount of compute," he says, "to be in a regime like this. ... You need to be able to compute a lot" regime because you don't want to use that many parameters. But of course, nowadays, we also have more data points. And because we have now moreData points, because we are using networks, we run larger and larger experiments. indeed, it's correlated with more. data points, and that's what we're trying to do with this study. It's correlated. with more data. points, which is why we're doing it in this way. We want to get the most out of the data we have. data points. Like we do see more data points in these days. This is the so-called double descent phenomenon. And this kind of mysterious. It's about less mysterious these days like after people have studied this in the last five years very carefully. I would talk about some of the data points that we see these days that are very interesting and very interesting to look at in the study of the human brain and the brain's ability to respond to the environment in a very different way to the way other people do. the explanations, intuitions. But before that, let me also give another related phenomenon, which is also called double descent, but it's called data wise double descent. So here, I'm doing a similar-- I'm just showing a similar graph. But on the x-axis, I am going to change the number of data. I'm going to show a graph with more data on one side and less on the other side. The result is a graph that looks like this: points. So here, the y-axis is still a test error. And the x- axis is the number of data points. OK. Maybe you have a guess first. What this curve should look like? As you have more data points, how does the test error change? Right. The guess would be that it would change to the following: The test error would change from the y to the x axis, and the data point to the y axis. It would look something like this: test error would be decreasing. Because I guess, here, at least if you believe in this bias and variance of intuition, then the bias doesn't seems to depend much on the data. The variance will be smaller and smaller as you have more and more data. So then what you do is try to find a way to make the data more representative of what you think it should be, rather than just trying to get the most out of it. That's what we're trying to do here. if you believe in that, then you should say that OK, the test should look at this, and it should continue to decrease as you have more and more data. And it turns out that, actually, in many cases, what happens is that the test error will look like this, says the professor. The test error is a result of the fact that the data is too large to be seen in a single test, the professor says. The professor says that in most cases, the error is caused by the data being too small to see. or increase, at some point, and it will decrease again. And this peak here is kind of similar to the peak here. So this peak is often happening when-- it's roughly equal to d. I guess, by the way, here, like there-- this is active research area, so I'm not sure what's going on here, but it's not a good place to get a good view of what's happening in the area. It's a very, very active area of research right now. being very precise in every places. So what I set here, I think, is basically mostly kind of 100% correct for linear models. But for nonlinear models, whether this is exactly is equal to being very precise. So this is number of examples. This is number. of examples, and this is. number of parameters. I think this is basically basically 100% right for non linear models, but not so much for linear. models. I don't know if this is 100% true for all models, or if it is. d or not is in 2d or the relationship is less clear. But let's suppose, when you think about relatively simple models, then when n, the number of data points, is closer to theNumber of parameters, then in this case, you're going to see a peak. And then after after, after the peak, you'll see a decrease in data points and a rise in the value of the parameters. And so on and so on, until you get to the end of the model. that, you have more data. It actually helps. I saw some questions. So the original double descent, does that like continue to decrease or does it eventually increase again? So in the first figure. This is a good question. So I think I've seen, empirically, that it doesn't continue to go up and down. But it does go down and up and up. It does go back down and down and back up again. That's what we've seen so far. on both cases. Sometimes, it does increase again a little bit, but often, not much. And sometimes, it just keeps decreasing. So I think that's why people probably don't study that part that much. Can you start running again. So this is, again, more than just running. It's more than running, it's running with a friend or a family member. It can be running with friends or family members, and it can also be running without a group of people. this function. This was like this has been for a while. This phenomenon. This one, I think, is also-- actually, the paper that first systematically discussed this is like 2020. About that peak, when was that discovered? The peak? Yeah. This is discovered in the same way as the peak of the black hole phenomenon. The peak of that phenomenon is also found in the Black Hole Phenomenon, which is a phenomenon that has been going on for a long time. paper, the peak. It's not monotone. The fact that there exists a peak was also discovered right, essentially. Yeah. I think, at least, they might like [INAUDIBLE] learning happens so often that someone does something, and then the community forgot about it. That's possible. But at least I would say, but at least it would be nice to know that there is such a thing as a peak, and that it's possible to find it. at least, it's only until 2020 that most people start to realize this. And because of that paper. And what's in the [INAUDIBLE]? I think the paper just called model wise double descent or something like that. Because this is data wise because you want to make sure you're getting the most out of the data you've got. I'm sorry. Datawise double descent. That's the name of the paper. It's a paper called Data wise Double descent. And that's what it's called. are changing the number of data points. This sounds like a mysterious enough. So like a very, very interesting. And what's the explanation? In the last few years, people try to explain what happens, and try to reconcile with our old understanding about this. And also, this is a very exciting time for scientists and for the world at large. It's going to be very interesting to see what the future holds for us and our understanding of how the universe works and how we interact with it. an important question because this regime, this blue regime, is actually-- actually, it's not clear whether when you run like a classical linear, models I don't think necessarily, you are in this regime. But at least, it is pretty clear that it's more true that for deep learning, you're basically basically in the blue regime. It's not quite clear whether that's true for all models, but at least it's pretty clear, I think, that that's the case. always in this regime. I guess this is still-- nothing is never universally true. But I think for most of the vision experiments, you are in a regime, where you have more parameters than a data point. So this is something that is really like empirically irrelevant. So that's why I think it's important to have a regime in which you have a lot of parameters to work with. I think that's a good way to get at what's going on in the world. people really care about it. And maybe another thing I need to clarify is that I think I probably mentioned that the study about linear models, the phenomenon of linear models is more kind of clear, like there are a lot of studies. And we have pretty good conclusion. And I think we have a pretty good idea of what's going on in the world of finance right now. And that's a good thing, because it means that we can do something about it, you know, and we can make a difference. what I mean by that is that even within linear models, you can try to change the model complexity. So what that means is that you just insist that you always use linear model. But what you change is thatyou try to decide how many features you use. So that's what I'm trying to do here. I'm not trying to say that you should always use a linear model, I'm just saying that you can change the complexity of the model to make it more or less realistic. you can start with only using one feature or two features like for example, in the house price, where you can use the square foot as the single feature, or you can collect a bunch of other features. So keep adding more and more features. That means you have more features to play with, and more money to spend on new features. It's a win-win situation for all of us, and it's a great way to make money in the real estate industry as a whole. and more parameters. So even within linear models, you can still change the complexity, just to clarify that. And most of this theoretical study, I think, are for linear models. And they are pretty precise these days. And I'm going to try to kind of roughly summarize the intuition from that study, which is in the form of a book called "The Art of Computer Science: The Science and Practice of Computer Graphics" The book, published by Oxford University Press, is available in English and Spanish. the study of this double descent. So the intuition, I think, I'm going to list a few of them. So some intuition and explanations. And these explanations are mostly for linear models. So I think the first thing to realize is that this peak, so you can argue what is the peak of the double descent, is that it's a peak. So you can argument what is a peak, and then you can try to find a way to get to the bottom of it. the most exciting or surprising thing about this graph. But let's first talk about a peak, this peak in the middle. So I think the first thing is that in some sense, people realize that the existing algorithms, especially if you just talk about, for example, simple gradient descent or simple gradient ascent, are not very good at this sort of thing. That's the most exciting thing about the graph. The most exciting and surprising thing is the peak, which is the middle of the graph, and that's the peak of the algorithm. stochastic gradient descent for linear models. So the existing algorithms underperform dramatically when it's close to d. So both these two peaks are basically like this. So here, you are changing n, the number of data points. And you found that when n is close tod, you had to change n again to get the same result. So that's what we are trying to do here. We're trying to find a way to get a similar result with a smaller number of points. pick. And here, you are changing the number of parameters. And we realized when d is kind of above n, above the number. of data points, you had a peak. So both of these two peaks are showing up here. It's kind of like a peak in the middle of the data set, where you're trying to get it to look like a certain way, but it doesn't look like it. It just looks like a little bit more of a peak than it actually is. It's just that you are changing the axis in some sense. So this is also when n is close to d, when the number of data points is near to number of features. And the explanation is such is the algorithms, the existing algorithms, or the algorithms you are visualizing. It's not a new idea, it's just a new way of looking at the data. It doesn't change the way you look at it, it just changes the way it's visualized. Algorithm that you use to produce this graph, it really underperforms very dramatically. It's not really saying that when n is close to d, the real test error is.here. So when you visualize this, where you do rank-- you do use some algorithms to learn the parameters. So that particular algorithm that you used to produce that graph, that's not what we're looking at here. We're talking about the actual test error that you get when you're trying to rank something. should be this. It's just saying that this algorithm is bad. If you change your algorithm, you probably wouldn't see this peak. So that's why the peak shows up. And what's wrong with the-- the existing algorithm, I really, just mean that for example, some just basic gradient descent. That's what's going on. OK. Now, let's go to the next part of the show. Back to the page you came from. Click here to watch the next episode of CNN Tech. So what goes wrong with the so-called existing algorithm? So this basically gradient descent algorithms. What goes wrong is that a norm of the theta, the linear models you learned, is very big. It's very big, and that's what's wrong with this algorithm. So for linear models, maybe this is-- I say, this is forlinear models. So this is a problem with the existing algorithm, not with the new algorithm, but with the old algorithm, and this is the problem. when n is roughly equals d. And we kind of believe that this is, at least, a partial reason for why this leads to a peak. So this gives the peak. We have some real experimental, real, real data here. So even though-- so OK. So I guess, let me draw something here. Let me draw a picture of what a peak would look like. It looks like this. It's like a peak in the sky. But it's not. If you visualize the norm in the y-axis, you're going to get the same result. But if you draw that norm-- so suppose you change the number of parameters, which means you add more and more features in your data set, so that you have more andMore parameters.data, in the lecture notes. That's going to change the way that the norm is drawn. And if you visualize it in that way, it will look like the same thing as if you were drawing it in the x-axis. to see something like this. And this peak here is roughly corresponds to n is close to d, which is kind of similar to these peaks. So basically, even though, suppose you compare this experiment and this experiment, so here, you have more parameters than this here. But when you compare it to this one, it's kind of the same, because it's the same shape of the peak. And it's similar to the peak here that corresponds to this peak in this experiment. have more parameters, maybe sometimes, you have lower smaller norm. So the norm when n is close to d, for some reason, it is very, very big. Actually, we know the reasons. The reason is that some random matrix is not well behaved when n. is near to d. But the reason is not that n is too big. It's just that the norm is so big that it is not very well behaved by the random matrix. But it's not bad, it's just not very good. I guess we are not going to go into that. But at least, the immediate reason is that when n is close to d, somehow, this algorithm is producing a very large norm on classifier theta, which is you can argue that if the norm is too big, then your classifier is not very good. It's not a perfect algorithm, but it's a good one for the time we're in, so we're going to use it for now. We'll see how it works in the future. model is too complex. In some sense, this is saying that your model is actually very complex. So very complex on [INAUDIBLE] to the norm. So this model, it seem it doesn't have a lot of parameters compared to, for example, this model. That's by definition. The norm is actuallyvery big. So in some sense,. if you use the norm as the complexity, actually, these peaks have large complexity. [INAudIBLE] [INA UDIBLE] Exactly. you got that. I'm implying that the norm seems to be a better metric for the complexity. So what is the right measure for complexity? So this is a very difficult question. Like for different situations, you have different answers. But there is no universal answer. But norm could be. It could be the norm. It's not a universal answer but it could be a good one. It might be the right metric for complexity. But it's not the right one for all situations. one complex measure. In some sense, the norm is also a way to describe how many, like suppose you have a small norm ball. So you have fewer choices to fit your data in some sense. You have fewer degree of freedom if you have-- like you have less choices than a large norm ball, for example, or a large number of norm balls, for a small number of data points, or for a large group of data point data points. It's a way of describing how many data points you have, in a way. options, in some sense, to fit your data. So that's restrict the complexity. And which norm, that's actually, for different situations, you can argue which norm is the right complexity. Actually, there's probably no universal answer. But I guess what I'm trying to say here is that the number of options is limited in some way, and that can be a good thing in some cases. But it can also be a bad thing in other cases, as we've seen in this case. parameters is also not necessarily the right complexity measure. Even if all the parameters are very, very close to 0, that's probably also very simple model. But if you have just a few parameters, but the norm is really, really big, maybe you can use the-- maybe you should also call it very complex. So my short answer is that there is no universal answer to this. The point is that probably the number of parameters is not the only complex measurement. And for linear model, it just is. happens that for mathematical reasons, I think l2 norm behaves really nice. It seems to relate to a lot of fundamental properties like maybe you can argue l2norm is useful because you are measuring a square error in many cases. And it's nice with the linear algebra, so on. I think it's a really nice way to look at the world. It's a very nice way of looking at the universe. I don't think there's any reason not to use it. and so forth. OK. So I guess, let me-- I'm running a little bit late, but I think I'm almost done here. So here, it's just saying that at least for this case, it sounds like norm seems to be a slightly better complex measurement. And actually, if you-- and if you -- and so forth, you can do the same thing with other measurements. And so on and so on. It's just a little different way to look at it. you can test this hypothesis in some sense. So you can say that OK, I'm saying here the existing algorithm underperforms. But if you have a new algorithm, that's regularized, suppose you recognize the norm. I guess, I haven't told you exactly what regularization means. But here, just what I say is that regularization can be used to improve the performance of an algorithm in a certain way. That's what I'm trying to say here, in some way. mean is that you try to find a model such that that enormous small. So you add an additional term that tries to make the norm small. Then you're going to see something different, says the coach. The training loss is part of the process, but so is the training loss for the model, he says. "You don't only train on the training lost, but also you try and make thenorm smaller," he adds. "That's what we're trying to do here" like this. So regularization would mitigate this to some extent. I would discuss more about regularization in the next lecture. But here, it really just means that you don't only care about training loss, but also you try to find a model with small norms. And you have some kind of a model that can be used to train people in a way that doesn't cause a lot of training loss. That's what regularization is for. It's not just about finding a model, it's about finding small norms, too. of balance between them. So you can sacrifice a little bit of training error, but you insist that your norm is small, then you can see this rise. So that, in some sense, explains, partially, why you had the peak. Because the peak is caused because the algorithm was suboptimal. And you can fix that peak by adding norm. But there's one more question, which is there is no peak, but why there's no ascent? So suppose you just see this. Actually here, you will also see this, something like this. this figure is actually pretty reasonable. Because if your data point is increasing, you probably should just have one decrease. You just keep decreasing the test error. So this one, let's say, we are OK with it. We are happy if you see just a decrease, like you just keep Decreasing. That's what we want to see, so we are happy with that figure. We don't want it to go up or down. We just want to get it to stop decreasing. single decrease. But here, suppose you see a single descent, I feel like it's kind of arguable whether you're should be happy with this answer. Why, when the number of parameter is so huge, you can still generalize? So why, when you use, for example, a million parameters, and.single decrease, and not a million? I don't know if you're happy with the answer, but I'm not sure if I'm happy with it. you just have five examples, why you can still generalize? Why you don't have ascent, eventually? In many cases, you don’t have ascent. And in many cases the best one is just you have more and more parameters. And actually, for example, another question is when number of parameter is. It’s not always the same thing. It can be a lot of different things. It could be a very different thing. And it can be very, very complex. bigger than the number of data points, sometimes, you are thinking this is the-- you have too many degree of freedom to fit all the specifics of the data set. But actually, empirically, you do work pretty well. So that's the last, and sometimes, another missing point, he says. "You shouldn't generalize," he says, "but actually, scientifically, you don't work very well at it." "You don't have to generalize. You just have to look at the data and try to figure out what it says," he adds. missing part. And this part, we also have some explanation for that. And the explanation is that-- so n is much, much bigger than d-- sorry. d is these much-- the number of parameters is muchbigger than the d. Sorry, muchbiger than the n, thenumber of data. And so the n is a much bigger number than d, and so the d is a big number, too. And that's what we're trying to figure out. points. Even though it sounds like you are supposed to overfit, but actually, the norm is small. The reason is that somehow, there is some implicit model that is too simple. So, when you have so many parameters, you still learn very simple model? Why? Because it's so simple. It's so intuitive. It just makes sense. It doesn't make sense to think about it in terms of a complex model. It makes sense to just think of it as a simple model. regularization effect, which makes the norm small. So when I applied this method, all of these experiments, they need to have any regularization. So that's why the norm here is very big. But the normhere is small? The regularization effect is the reason why it's so big. It's because I didn't have any explicit encouragement to make the norm big. I was just trying to make it small. That's what I did. I just made it small, and that's what made it big. reason is that your optimization algorithm has some implicit encouragement to make the norm small, which is not used. And that's something I'm going to discuss, I think, more next time. So for this lecture, I Think I'm just-- so we're just going to focus on the loss function for the time being, and then we'll move on to the next part of the lecture. Back to Mail Online home. back to the page you came from. Click here to follow us on Twitter. going to discuss this more next time. So the high level thing is just that something else is driving the norm to be small. Thanks. Going to talk more about this in the next few days. Back to Mail Online home. back to the page you came from. Back To the pageYou came from: Back to thepage you camefrom. Back into the page You came from was from: The Daily Mail. Back onto the pageyou came from, the DailyMail.com page you were from.