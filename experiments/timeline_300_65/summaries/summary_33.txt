John Guttag: Welcome to the 60002, or if you were in 600, the second half of 600. There are problem sets. They'll all be programming problems much in the style of 60001. We do want you to hone your programming skills. There'll be a few additional bits of Python. Today, for example, we'll talk about Lambda abstraction. And the course is really less about programming and more about dipping your toe into the exotic world of data science. The main topic is what I think of as computational models. Science is moving out of the wet lab and into the computer. We'll talk about three kinds of models-- optimization models, statistical models, and simulation models. An optimization model is a very simple thing. We start with an objective function that's either to be maximized or minimized. We then often have to layer on top of that objective function a set of constraints, sometimes empty, that we have to obey. We can build models that sort of explain how the climate has changed over the millennia, and then a slightly different model that might predict what it will be like in the future. A greedy algorithm is a way of looking at problems in a way that maximizes the value of an object. In this lecture, we'll look at a specific optimization problem called the knapsack problem. The problem involves a burglar who breaks into a house and wants to steal a bunch of stuff. The burglar has to solve the optimization problem of stealing the stuff with the most value while obeying the constraint that it all has to fit in theknapsack. We use these things all the time. We can't avoid using optimization algorithm as you get through life. The most obvious solution is brute force. I enumerate all possible combinations of items; that is to say, I generate all subsets of the items that are available. I can now go through and sum up the weights and remove all those sets that weigh more than I'm allowed. And then from the remaining combinations, choose any one whose value is the largest. So it's pretty obvious that this is going to give you a correct answer. You're considering all possibilities and choosing a winner. There is no algorithm that provides an exact solution to this problem whose worst case running time is not exponential in the number of items. The sad answer to that is no for the knapsack problem. But that should not make you sad because while there's no perfect solution, we're going to look at a couple of really very good solutions that will make this poor woman a happier person. So let's start with the greedy algorithm. It could hardly be simpler. We say while the knapack is not full, put the best available item into the knappers. red there's a parameter called keyfunction. That's going to be-- map the elements of items to numbers. So it will be used to sort the items. So I want to sort them from best to worst. And then for i in range len of items sub copy-- I'm being good. I've copied it. I don't want to have a side effect in the parameter. In general, it's not good hygiene to do that. And so for-- I'll go through it in order from best-to-worst. actually works? I hope not because I think it does work. Let's ask the next question. How efficient do we think it is? What is the efficiency of this algorithm? Let's see where the time goes. Who wants to make a guess? By the way, this is the question. So please go answer the questions. We'll see how people do. But we can think about it as well together. The first thing is at the sort. And we heard from Professor Grimson how long the sort takes. See who remembers. Lambda is used to create an anonymous function, anonymous in the sense that it has no name. Lambda does is it builds a function that evaluates that expression on those parameters and returns the result of evaluating the expression. Here we're going to be using greedy by density to allocate-- actually, sorry, this is greedy by cost. We don't want to pass in the cost, right, because we really want the opposite of the cost. And we want the cheaper items to get chosen first. TestGreedys.that chooses a burger, the pizza, and the wine for a total of 284 happiness points. On the other hand, if we use greedy by cost, I get 318 happiness points and a different menu. The problem is that a greedy algorithm makes a sequence of local optimizations, chooses the locally optimal answer at every point, and that doesn't necessarily mean the best answer for the situation at that point in the game. And here's another solution with 318, apple, wine-- yeah, all right. So I actually got the same solution, but it found them in a different order. add up to a globally optimal answer. This is often illustrated by showing an example of, say, hill climbing. So you might choose as a greedy algorithm if you can go up, go up. If you can't going up, you stop. And that's the problem with greedy algorithms, that you can get stuck at a local optimal point and not get to the best one. Now, we could ask the question, can I just say don't worry about a density will always get me the best answer? Well, I've tried a different experiment. and I'm going to allow myself 1,000 calories. Well, here what we see is the winner will be greedy by value, happens to find a better answer, 424 instead of 413. So there is no way to know in advance. Sometimes this definition of best might work. Sometimes no definition ofbest will work. You can't get to an optimal solution with a greedy algorithm. On Wednesday, we'll talk about how do you actually guarantee finding an optimal Solution in a better way than brute force.