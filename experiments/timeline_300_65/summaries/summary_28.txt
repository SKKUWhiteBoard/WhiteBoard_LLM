Last lecture of winter term course on front ends. How to determine if a constraint is likely to be a correct one. What kind of conditions should such a constraint fulfill or the the metrics of the local environment fulfill in order to reduce the probability that this is actually an outlier mesh. What my goal is for today I will introduce three kind of small front end systems on a very abstract level just giving you the idea on how they work with different sensors and then in the second part of this talk today I would like to stress on what kind of condition should such an constraint fulfill. when the robot observes the same part of the environment and so far we always assume these constraints are given so these errors here we assume to be given and today we would like to look at the case how do actually generate those arrows. The two main components of the slam system are the back end and the front end. The back end optimize the graph and returns a new node positions back to the frontend. The front end uses this information together with the new centre information to generate new constraints. to begin by matching observations so we have different observations depending on what platform that can be whatever stereo camera. Different types of sensory modalities and for every sensor of course there's a different way of obtaining those constraints and those constraints may take into account what the sensor actually sees how kind of unique is the data that the sensor generates for specific area or it's a look all corridors exactly the same if you only rely on laser range data and the corridors all look the same from the Rays range data point of view then we may take that into account. one feature or one landmark and the robot map those landmarks by checking for say fitting circles into the laser range data and whenever it found a good circle say that's likely to be a chunk of a tree and use it as a landmark the third class of approaches uses feature descriptors most popular ones are for example sift and surf. These are two features which you can extract from image data and which kind of describe the local surrounding of the place where this descriptor is computed. You can match surprisingly large databases of images using those descriptors. moment and that's my sensor range I can compute where are those other pulses so in this case B 1 and B 2 just two examples could be more obviously and then I can also estimate what is the uncertainty of those poses B 1 or B 2 relative to a do that by eliminating the note a from my linear system and then inverting the resulting Hessian and looking to the main diagonal blocks this gives me the uncertainty here indicated by these dashed lines. Based on this information I know I can never have an estimate of given my current pose where's b1 where's B2 together with The Associated uncertainties certainty estimates. was here indicated with a which can where I could I can obtain by inverting the hessian. In practice this is a pretty expensive operation so you actually want to try to avoid inverting this larger matrix. So you can do an approximation what actually most system in practice do to do that more efficiently. This does is ignores the loop closures so the uncertainty estimates are too big but I can compute this extremely efficient and I may inspect a few places too much but I should get all the places which I need to inspect. A very simplistic front end which will try to identify constraints which uses iterative closest point so scan met a former scan matching and tries to find a match. If this is above a certain threshold I accept the match and if this is the same here I go that's a constraint. If your customer and I've proposed you this solution what would be maybe your argument against that would you buy that this approach which I propose you good what do you see potential problems of this approach from given what you have seen so far absolutely correct. ICP is a combination of the iterative closest point algorithm and the initialization. ICP is sensitive to the initial guess and as a result of that we may end up in a local minima so in something which looks like a match but in reality is not a match. Other things we may identify is how do i sample possible locations where the platform can be if this is kind of the uncertainty is a very large area I may need to sample a lot of different poses in order to do that efficiently. showing you three different examples of systems that we have built here in Freiburg. Some of the mapping techniques we developed here have been used to at least tested on that car so this is a pioneer a two robot which has a two d-day the rangefinder sitting here and sits on a pencil unit so it moves always like this song so it's called a nodding laser and this way generates 3d data you get 3d information about the scene and then it tries to build sorry a 3d map of the environment using this technique. have less local minima so we match those Maps and get a six degree of freedom constraint in this case XY that your patron roll so six dimensional constraint and then we this is actually one of those constraint here then we can accumulate all constraints and then do a graph optimization and obtain a local map so this also a parts of this is building 79 building 51 52 the Mensa building and of course some parts here the green area which robot hasn't seen how does it align those two scans of these are two examples of two scans. darkest stripes these are simply small alignment errors of these individual maps they can they can see kind of small steps over here here that was also probably an alignment error which simply leads to her step which was let's say bigger than and all five centimeters in the ground and therefore it's classified as not reversible anymore. Everything is red over here but what do you see here for example other the bikes which are parked over here and the individual trees here something it's like to have gone wrong or maybe there was some copy. is a parking lot or a 3d model of a parking parking lot where yellow again means drivable areas and red means non drivable area and then you can actually use this this map over here in order to localize the vehicle. So these were kind of in this case 1600 local 3d maps and this was this in this example done with the grid for 20 by 20 centimeter grid cells and this by lining those grid cells you canactually get maps off and say this quality that's something you can expect to get with this technique. an initial inertial measurement unit and one of the advantage of this system is it gives you also the gravity vector this quite accurately at a high frequency. If you know the gravity back door you can eliminate already two of the six dimensions from your state space. The roll angle and the pitch angle can be determined but just by knowing the gravity so you have only one angular component and XY that that you need to estimate and therefore adding this IMU to the tourist error system is is highly advantageous. Surf features provide a local description of the scene of a small noise a scene of the small part of the image and so if you see every of those for each of those points here one of those descriptive Alice is computing their computer. Based on the position of my stereo camera I try to build a local model of the surrounding so what we want to estimate is the X Y that and three angles your roll pitch and yaw. By knowing the gravity vector I kind of get rid of the roll in the pitch and this reduces my problem from six dimensions to four dimensions for every node. The technique is used in three different ways in this approach. The first one is for for visual odometry so there is no wheel encoder on the camera. The second is for matching your current observations against a small part of the environment. The last part which is loop clothing so given I kind of I don't know where I am some a large uncertainty and I can use this approach to see how well do the features that I see at the moment match the features I've seen in the past. Tasker builds a map online and use the map in order to make navigation decisions of where it should actually go. The ICP is sensitive to the initial guess so one thing you can do is try to find arrange things into Maps instead of single scans this helps. If you have descriptors like feature descriptors it can actually help you to find good estimates where you can be so you don't have to try all camera polls and see if the camera poses match you can use your feature descriptor to already pre select images you may consider for for potential loop closures. talk which I Neff was more over more whatever like wait overview about how different approaches work was not going to too many details. The second part of the talk today I would like to talk about ambiguities in the environment and what are good ways for dealing with them and how can we actually build accurate maps consistent maps of the environment so they are are so or the main assumption here is not we simply ignore all n big you T's and say the environment has no ambiguisms and I just consider they are none of them. be the same place but there might be something else which looks exactly than in this place a here so it may not be a good idea to add this constraint unless we have seen all this part over here. In order to make this constraint view maybe it's better to first explore all this scene over here before we can make this data Association. There may be different places where the system can be which are just which are which do not intersect with the place I'm currently considering and therefore I should not do a match but you could you could. or not so maybe a bit imprecise from what I talked before so there here is a global ambiguity which is something we don't want to have and they are they example if the uncertainty is small and I have this Metro they say okay that looks good that's kind of what's called global sufficiency so the opposite side so this is a globally sufficient match so there's no other place in the uncertainty area of that node where the system can fit in these are the things I am interested in finding exactly those it's all situation and say simply the match can't be anywhere else. hard for me to you to to identify and this is also called what's called the picket fence problem so good offense you seem you don't know which part of the fence matches to what you see so far it's a very very long repetitive structure. These are things where you also don't want to add a constraint the curses simply do not know is this is this locally ambiguous or not. You could use the max mixture approach and say simply it'm a multi-modal constraint that I may add I'm either here here or here. The approach that every Doulton and his team proposed we're saying okay we have all slam back and it gives me an estimate into the prior the current estimate of the of the graph that I have and I do a it does a post poll scan measuring very similar to what you've seen so far. Based on the scan matching we can actually do a topological grouping so pulses which are nearby which are in the same part of the environment I just grouped together it's kind of can see this is a small local map Zushi. good fit maybe yeah doesn't sound too bad just add them to kind of a temporary constraint list and this is shown here in red so this one can Michigan this pot this post this post is against this opposes both this post and this guy again. Some of them will be likely to be right some of them are verylikely to be wrong. The questions how do I identify which one a right image or not wrong again the first thing we do is we want to test for local unambiguous so we take one of those groups and check okay. How do we actually get those locally consistent matches that's one of the key questions here the the key trick in here is we have a large number of constraints pairwise constraints between nodes. We want to find we want to check in to how many consistent subgroups are there so if I can assign a kind of a group ID to every constraint and the goal is that among one group within one group they all consistent with each other that's would be the perfect thing. The idea is to say okay in order to check which are consistent I need to check if they kind of transform the environment in the same way and it's done here by taking what we call the prior edge of. these are those add edges which result from odometry or incremental scan matching if I start from this node over here I can take my little madama tree constrain to go here I take my constraint HJ to jump into the second trajectory for the point in time when I visited the place a second time move along the odometry again and then go back kind of with the inverted H I and go back to the same place. If I have this kind of loop of constraints if they are all perfect and agree and consistent I should add up at an identity transformation if I concatenate all of them. and how accurate can you actually align your scans so of course Moodle oh that yeah okay so I have whatever a number of those hypotheses what I can do is I can actually build up my matrix a I J where this simply depends how consistent are the hypothesis using the hype of this I and a hypothesis J together with the odometry so this is this is IJ so I just cannot make this walk around in my graph and say how close am I with respect to the identity if I'm closer than you say that's pretty good if I're far away from identity as they are something has gone wrong here. and if you don't understand what the matrix means it will be hard so every entry of this matrix IJ tell us how well do hypothesis hypothesis J agree with each other just looking to this this pair of it's just a pairwise consistency mention the small if they're small Wireless in there I mean they don't agree they are high values and Daisy but they agree that may be good again so the goal is just to find this vector and then later on find a way and how can we determined what V this vector be all right. once once in this vector and I will get a high score if I have ever two groups in there I have I get one. I get scores among the groups but not between each other. I divide by again a large number of apostasy so we get a small value. I have this function just high values for both elements and low values for bad hypotheses so what can I do again treated as an optimization problem. I try to find the vector B which maximizes this fraction that's exactly what is done. problem I should come up with a very similar solution there's no guarantee that this is the case so it is definitely an approximation is a different problem that I solve if I go from discrete value from this credo from zero and once for binary variables to continuous variables but the assumption that I'm not too far away but there'sNo no theoretical guarantee for that ok how do I maximize this function i compute the first derivative I set first derivative to zero they don't want to go into the details how the derivative is obtained but it turns out solving this equation is equivalent to solving this equations. and if I have multiple solutions for that get MA multiple. The larger the eigen values are the better the score so there's a proof that i get a perfect combination. If I visualize this so one situation over here so this is kind of the first eigen vector second eigenvector third force this is Lambda i. If you have more than one solution I can just say okay what the ratio between the largest eigenvalue and the second largest eigenvalue. If this is a value which is let's say 1 or between 1 and 2's again yeah this is very likely to be a picket fence prom. second time this is the case I mean say I can't be sure that this is a concern it could be a constraint but I'm not sure that it really is a constraint because there's simply an area in the uncertainty on the relevant uncertainty area which I haven't seen so far. What we would in theory need to do we need to really check this area I can see if we can fit it in here an approximation for that is just compute the size of the ellipse on this circle and theEllipse over here. do the global ambiguity test or global sufficiency test so it's a globally sufficient order they say a global ambiguity if there's number you D don't add anything and otherwise I have a loop over which I can say with a high likelihood it's globally consistent and there's no local and acuity and that's a way actually most a lot of different slam system works also the SEM system that we use in here you've seen this video already where the system we have the robot mapping our campus over here. strongly depends on the sensors that I'm using and I need to the better I can exploit the individual properties of my sensor the better it is. The approach of this global ambiguity tests and the local ambiguity test these are important things that a good front that should consider in order to avoid adding false positives constraints. This is done with a single graph partitioning approach this was kind of the techniques there the technical - Olson which uses this and yeah again so regarding the the position uncertainty of the platform the higher the uncertainty is.