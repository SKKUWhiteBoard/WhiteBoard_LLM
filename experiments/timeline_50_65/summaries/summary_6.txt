foreign and I should turn off the zoom background blur Ry options like this oh it does show up uh yeah there's like speaker notes on your screen but there's be careful because I accidentally just put something else in the first longer okay I don't have too many but there are a lot of options to choose from. There are a number of options that can be turned on or off. There is also a feature that lets you change the sound of your voice. There's also a tool that allows you to control the sound on your phone. yeah there's an interview oh this is where it has speaker notes and stuff I think it's fine I don't need this man that eraser is disgusting this one yeah oh wow you're really going at it well I just think I was just too ready to go I usually. I usually don't do interviews. I just don't like to do them. I like to be in control of my own life. I'm a very private person. I think I'm very private. I've been in a relationship with my wife for over 10 years. uh yeah as our slides were they and put which is the product describing to replace the names of it oh no it's fine well it started as their Slide the best facts TZ so yeah it goes. They're probably gonna make their own updates though oh I I did. I did do a little bit of work on it as well. It's just a fun way to get people talking about the things we're talking about. It was fun to do. I'm glad we did it. yeah party on pictures are good first time we got foreign foreign foreign things okay let me make sure it's it was obviously I think it is for the most part of it that's something the green and the yellow look identical we can go to something different oh. Yeah party onpictures are good. First time we get foreign foreignforeign foreign things. okay let's go to the party. yeah party onPictures aregood first time. okay Let's go for a party. Yeah. my gosh it's mine started oh do you want the mic um I think you should be fine though what hello hello all right so I think we can get about started here. I want to start today uh before we get into like talking about uh more advanced CNN. CNN.com is a weekly Newsquiz. We test your knowledge of stories you saw on CNN news.com and send you a photo of your favorite news stories. Submit your photos of CNN news stories from around the world at CNN iReport. architectures I want to start by sort of recapping what we talked about last Tuesday um I apologize that was kind of a rough introduction that was uh that was me making a couple of last minute edits that probably hurt more than they helped so I want just to just start off with a recap of what we were talking about. We talked about the history of architecture. We also talked about how we got to where we are today. We looked at how we came to the point we are at today. review um convolutions and and the architecture of a CNN to make this more clear and put it put it into perspective how it relates to um just standard um dense neural networks. I think it's fine um so we talked I think I think most people felt okay about it, he says. "I think it’s fine," he adds. "We talked about it. We’re going to continue to talk about it." "I’m not worried about it," he says, "I mean, I’ve got a lot of other people to talk to." um the actual mechanics of doing a convolution um and I just wanted to sort of clarify that when we do a. convolution operation um we treat it like a layer like with standard dense neural networks uh we treated matrix multiplication as sort of like a. layer where all the all the data is stored. We treat it as a layer in the same way as a dense dense neural network. We treated matrix. multiplication assort of like an additional layer in a dense neural. network. the Learned parameters were all the values in our Matrix and all thevalues in our bias Vector. It's going to be a very similar story with the convolutional layer where your convolutionAL layer will have a bunch of filters right. All the values inside our filter are learned. The values inside the filter are Learned parameters. The Values inside the Filter are learned parameters. This is how we learn the values of a filter. The Learned parameters are the values that make up our filter. We learned these values from our Matrix. and we also have a bias term that gets added to the output of moving each window on each location of our input. We refer to it as a volume simply because it sort of looks like a cube um and I wanted to to reiterate um this idea that that is what we are trying to achieve with the data set we are working with. It's a very complex data set that we're trying to create. It has a lot of moving parts and it's very complex to work with. We want to make sure that we get the most out of it. again if you have a whole bunch of different filters. If you have one filter you're going to get an output map for every location that we started and put our our filter where every locations that it overlapped and if you've got a whole lot of filters if you're doing a lot of filtering. You'll get a map of where each filter overlaps and where it overlaps with the other filters that you're using. You can do this with any filter that you want to. you have a whole bunch of them then you're going to end up stacking up all of the different outputs um from taking each one of your different filters and running it over your input and you get something that has a number of channels in the output um and. You get something like this with your filter set up. It's a lot of fun to try and figure out how to use it. It can be a little bit of a challenge, but it's a fun way to get around the problem. again you can kind of think of this as like a layer of a neural network so after we're done doing this we're still going to add our activation function like array Loop or or something else. We're still we'restill going to to very much treat it yeah yeah. We'll still be adding activation functions after we've done this. We will still be treating it like a neuralnetwork. We still will be adding our activation functions like array loop or something like that after we's done. as a layer and because all these values in here um because they're all because our output our loss is ours differentiable with respect to all these different parameters we can still take partial derivatives of our loss. We can still do gradient descent um so it's all part of the same process. It's just a different way of looking at the data. So it's a little bit differentiable, but it's the same thing. We're just looking at it as a layer with different parameters. it's simply as opposed to doing matrix multiplication on a vector um when you have images and a CNN you can simply just do convolutions instead of your normal matrix multiplication um which is demonstrated up here. um you do convolution followed by your activation functions you have pooling layers you havePooling layers is a new way to do matrix multiplication. It's a way to get around the problem of matrix multiplication with images and CNNs. It uses convolutions to get rid of the need for matrix multiplication in the first place. if you want to decrease the size of this volume because it can get quite unwieldy you can do pooling simply just looking at each if you have a two by two. If you want a larger convolution you can take this convolution and do it with a smaller convolution. You can also do a convolution with a bigger convolution to make it easier to see how much of a volume you have. The convolution can also be used to make a larger or smaller size of an image. two pooling you're just going to look at every channel look at a little two by two square. Take the maximum look at the next two by 2 square over to simply just chop your the size of your output in half um. Two pooling is a way of reducing the number of channels you have to work with. It's a way to reduce the amount of data that's being sent to the server at any given time. It can also be used to help you with other types of programming. and were there were there questions on on this on the sort of mechanics of like what does CNN is and what it what it looks like mechanically um yes friend um yeah yeah every filters has its own bias so basically every everything yeah you can yeah you Can. You can do it. You're a friend. You've got to do it, right? You're on CNN. You don't have to be a friend of CNN to do this. You just have to go on the show and do what CNN does. think of it like that I I don't really think of it as like after we're done doing all the element wise multiplies um from overlapping our filter with our input uh we then just add the bias corresponding to that filter onto it. But yeah that's like in that's what we're doing. We're adding the bias to the filter and then adding the elementwise multiplies to it. That's how we do it. It's just a little bit different than what you would normally do. probably an equally as intuitive way to do it um is just add the corresponding bias to the corresponding Channel. The corresponding Channel and the output yeah um but again the big the big thing I really want to get across is that you treat it just like another layer. It's just another layer of data that you can add to your computer. It can be used to create new content. It could also be used as a way to test new software or software that is being developed for the next generation of computers. um you're just gonna stack a bunch of them um and then eventually when you want to get to the end you take this volume and you just unravel it in a very specific you just flatten it all out um. Then just do like a dense layer a and then just do  a dense layer  and then  just do like  dense layer a. The final product is a very different look than what you would see in a normal album cover. It's more like a collage, more like an art installation. regular matrix multiplication um at the end yeah yeah no really not really um it's just there because like if we if we have like a huge input that's like 256 by 256 I think it's like huge uh the output assuming you don't do a strided convolution which is just regular matrix multiplication. I don't know if you've ever done that. It's just like a really, really big convolution. I mean, it's a huge number. I'm not sure how big it is. taking your filter overlapping in this case a three by three area and instead of moving over one to overlap in the next era you move it over two um if you don't want to do a striving convolution a very simple way to do it is to just look at the filter and move it to a different area. Instead of moving one over one you move over one over two and then over two over one and so on. If you want to move over two to overlap you move the filter over two. at individual little squares and just take the max in this whole area in each one of these areas and just spit that out um that just immediately cuts the whole thing in half on uh your height and width axis so you have a quarter as many values now. "That just immediately cut the whole things in half" he says. "Just take the maximum in this entire area in these areas" he adds. "And just take that max in these whole area" "And then just spit it out" but it's still ideally still sort of captures all the main information that was in that uh that feature volume um yeah it's it's more just used so we can get our feature volume down to a reasonable size so it doesn't take forever to run convolutions on it just because of the size of it. But it's really just used to help us with our convolutions. It's just a tool to help with convolutions so that we don't have to do so many of them at once. makes it a lot quicker yeah no well so we're going to talk about um segmentation which is where you have like an image like a person in it and you need to Output another image except each pixel has basically been like labeled with like there's a people in. makes it a much quickerYeah no well we'll talk about segmentation later on in the show. We'll be talking about how to use it in the next episode of the show, which airs at 9 p.m. ET on Sunday. all of these pixels inside of like here and then everything else is background so in that case your output is the same size as your input which gets a little bit weird. We'll talk about that more later how you because at face value that would be like at facevalue that would like to be like.all of this pixels in one place and then all the rest of the world is background. So that's a bit of a weird way to do it, but we'll talk more about that later. super inefficient all of your convolutions are on these just huge inputs we'll talk about that more later but generally for things like classification you do want to start bringing it down a little bit um there's no reason to to leave our feature volume huge um if we can. We'll talk more about this in the next few minutes. We're going to take a break for a few minutes and then we'll be back with more details on what we've been working on for the past few hours. shrink the size of it without reducing the amount of information in it hopefully does that make a little sense are there more questions because last lecture was confusing and I apologize that was a pretty rough introduction to CNN's um yeah yes friend yeah uh yeah so for your purposes, I'm going to try to make it a little bit easier to read. If you have any questions, please contact CNN iReport at jennifer.smith@mailonline.co.uk and we'll try to get back to you soon.  input image the depth is like RGB uh like one one channel is what we call it for each pixel but say we had like 10 filters uh it no longer corresponds to like color the output of a convolution layer with 10 filters it simply means there were 10. input image is RGB but the depth of the image is not the same as the color of the input image.input image isRGB but the Depth of the Image is different than the Depth Of The Image. input Image is RGB, but the Color of The Image is Different. different activation Maps one from each filter that we obtained so your your your Channel's Dimension uh would be 10 Deep um yeah it just corresponds to like if if this filter is corresponds to horizontal edges and this filter corresponds to Like edges like this um you can just use the different activation Maps to get around the filter you're looking for. So you can use the activation Maps from the different filter to get round the filter that you're trying to get to. So your your channel's Dimension would be  10 Deep. sort of look along the channel and see like okay like was there an edge that went this way. You can just sort of look in all of the the different features throughout that channel um they just correspond to what the channel is about to do. It's just like looking at a map of the world, you can sort of see where the edge is going to go and what's going to happen next. So you can kind of sort of go along the channels and see what the edge looks like and see where it goes. filter what that filter in the previous layer picked up on does that gonna make sense there's not like an intuitive like sort of like color um as as you after the first layer um but but but yeah excellent question um are there more questions okay um if there are. If there are, please send them to jennifer.smith@mailonline.co.uk. For more information on how to use iReporter, visit www.jennifersmith.com. For the latest from iReport.com, click here. any more questions then I will hand it over to Rohan who's going to talk about more advanced CNN architectures and I just want to contextualize this um with there's only really like one architecture that you really need to take away from here which is going to be the CNN architecture, that's what we're going to focus on here. We'll be back next week with a look at some of the latest developments in the CNN space. Back to Mail Online home. back to the page you came from. resnet we'll get to that um the rest of them like if it goes over your head like don't worry it's it's fine this is just more for people who who want to know. um resnets are the only one we should really like feel free to ask like tons. Like tons is the name of a website that lets users search the web for content. It's a service that lets people search for content on the web. The site is called resnet.resnet.com. of questions because we want to spend the most time on that and all that. So yeah go on hello um so this is going to be kind of a survey on the uh the history of uh neural network architectures for computer vision. Starting from uh there's a number of questions that you might want to ask. We want to hear from you, so go on and answer them. We'll post the results back to you in the next few days. Back to Mail Online home. back to the page you came from. a little timeline here starting from uh alexnet and moving forward um laneet is something that was created quite a while ago. Alexnet is like the first kind of visible Improvement in this field um that kind of starts the motivations behind creating a deep Network that can be used in a variety of ways. It's kind of the beginning of what we're going to see in the next few years in the field of Networking. We'll see a lot more of this in the coming years. be learned from without any gradient problems and things like that um so yeah we're going to be talking about alexnet BGG um the motivation Pine Inception Nets as well as briefly about mobilenets. Landing with with resnets and a couple of other um miscellaneous networks um none of which we've ever heard of before. We'll also be looking at some of the other networks that we haven't heard of yet. We hope to hear from you in the next few days. these are state of the art on imagenet now I believe um things have kind of transitioned uh to Transformers and other much more advanced architectures that we're going to be talking about in the next couple weeks um but this is understanding the motivation behind these um really sets. These are state-of-the-art on imagemet now. I believeUm things have transitionedUh to Transformers  and other muchMore advanced architectures that we're going to be talking about in the next couple weeks. Convolutional Nets were proposed in 1990 by Yen lacun um and he he kind of pioneered that pattern for today. He's currently head of AI at AI at Google. The stage for uh the the future of advancement in this field so yeah convolutional nets were proposed by Yen Lacun um in 1990. He he pioneered thatpattern for today he's currentlyHead ofAI at Google, Google's sister company, Facebook. The future of AI is set by convolutionality Nets, he says. Facebook AI research so furthering the metaverse um for those of you that are into that kind of thing um but alexnet in 2012 was a a big groundbreaking feat in that it proposed stacking layers of convolutions Max poolings following it up by fully connected layers and coming up with a new way of thinking about the world. Alexnet was a big breakthrough in the field of computer science and was developed by the University of California at Berkeley. It was the first of its kind. with not a very deep network but one of the deeper networks that were introduced at the time. It turned a lot of heads when it achieved a around 17 error rate on imagenet. It won in 2012. Most of these architectures one in their respective year um vgg in 2014 I think Inception net in 2015. A lot of really cool advancements over the architecture of alexnet. There's a lot going on over the Architecture of Alexnet and the way it's being used. of things to consider and this makes it look extremely complicated um in reality all this is doing is stacking convolutional layers and pooling layers intelligently to synthesize information from low-level features and work its way up as it passes through the network to higher features which really is the real goal of the system. "This makes it make it lookextremely complicated and this making it look like it's a very complex system. In reality it's just a very simple network function that is being used by a lot of people" motivation behind most of these architectures start off with a bunch of data a full dimension image with all the features that you have in whatever your input is um starts to detect low level Trends create feature Maps pass it through pooling layers to condense the information that you are trying to convey. It then goes through a series of layers and layers of data to create an image that looks like a map of the user's input. It is then passed through a layer of data and data layers to create the final image. have and lastly come up with a prediction or a model that can be used to predict. Now you have a feature map that corresponds to different classification metrics so there are some observations there's only five convolutional layers um and the next slide should have an updated drawing. Have you ever used a computer model to predict the future? Share your story with CNN iReport. Share your photos, videos and other content from around the world. Send your photos and videos to iReport@dailymail.co.uk. that's hopefully a lot easier to understand than the previous one um but this has a convolutional layer followed by a Max pooling layer. another convolutionsal layer follows by Magic pooling then three convolved layers stacked. followed by three convolutiona layers stacked then a Maxpooling layer and it ends with a couple of fully fully-fledged layers. That's hopefully an easier way to understand this one than the last one. That one was a lot harder to understand. This one is hopefully a much easier to understandable. connected um layers um so three Hefty dense layers are following this last stack of convolutions and Max pooling and that results in a lot of parameters. Dense layers are are fully connected so you're doing your matrix multiplication across the entire layer so your number of parameters Stacks. Stacks and convolutions are used to create the layers. The layers are then connected to each other using matrix multiplication. This creates a stack of parameters that can be applied to different layers of the algorithm. up quite a bit this also means that the number of computations that you need to do stacks up quite a lot. Only having five convolutional layers and three dense layers really limits the amount of information we could synthesize by our algorithm. We want to go deeper right now. We're going to have to do a lot more computations to get to the full range of information that we want to get from the data. We'll be working on this for the next few weeks. feature Maps um you might have heard the term deep neural networks um this course I believe is called Deep learning for computer vision so that is something that we're going to touch on um but this this can barely be considered a deep neural network just because of how it works. It's a very complex and complex system. It is not just a simple set of neurons. It has to be very complex to work. It can't just be a set of cells. It must be a whole network of cells that are connected. shallow it is we want to abstract these Concepts more. We want to be able to have our model learn higher order feature maps and by that I mean low level features um like edges things like that. As we go higher um starting to see the correlation between these concepts and how well we are able to predict them. It's a very exciting time for us. We hope to see a lot more progress in the coming months and years. Back to Mail Online home. back to the page you came from. like color spaces um how edges lead to other forms and things like that yeah this is basically about what like what I was talking about um there's a lot of space that we want to have. We want to uh go through this architecture um so yeah this. This is basically what we're trying to do with our architecture. It's basically about how we're going to use the space we have and how we want it to be used. We're just trying to use it in a way that makes sense to us. this is hopefully a lot easier to understand than the previous drawing and it's saying the same thing so you have um a this is a 5x5 image with three channels um similar to what Jake is drawing here actually. This is your original image in like RGB or RGB or something like that. You have to be able to understand what the image is saying in order for it to make sense. You can also draw the image in different colors to make it easier to read. For example, in RGB, the image would look like this: something it's passed in through to a convolutional layer it's Max pooled so its Dimension has been reduced. Then you pass through three convolutions which don't change Dimension you Max pull at the end of that. And then you go into the concept that Jake was talking about earlier which is flattening so by unraveling the final kind of layer that you have your and you end up with a one-dimensional Vector instead of a Vector. It's a very interesting way of looking at the world. of whatever you landed up with after your convolutions these are then passed through three dense layers this is what we were talking about earlier and lastly passing to a soft Max function. Up until this point all of the activation functions are religious or rectified linear um that's what we're talking about. That's the way we get the shape of the shape we're trying to create in the brain. It's a very complex process. It takes a long time to get to the point where the shape is the shape that you want. at the end of each convolutional layer and Max pooling layer softmax at the end will scale on a zero to one scale so it'll give you like your final like predictive class that you would want to have. This is how the sizes are changed as a image would be changed. At the end, the final class will be the same size as the image it would have been in the first place. This gives you the same final class as the original image. The final class is the size of the image that the image would have looked like. go through alexnet so this is say you start off with a 227 by 227 by 3 image um you have a stride of four. This is like approximately quartered um your kernel size and we talked about that is like the the size of the kernel that you're working with. Go throughlexnet.com to see the full transcript of this interview. Click here to watch the full interview on CNN.com. The transcript has been edited for clarity and length. To see the complete transcript, click here. sliding across um the image that's 11 by 11. you pool with a stride of two so you're having a dimension um the actual image dimension you go through another convolution um which adds depth to the image of given your your kernel size um and once again you pull once again to get the depth of the image. It's a process of adding depth to an image of an image that is already a depth of 11 or 11.5 by 11, and then adding depth with a convolution. you're having the dimension of the actual so basically each of these steps corresponds to each of the steps uh as you can see here um you end with three fully connected layers and you go into a soft Max function which ultimately instead of really so scaling linearly scaling uh linearly linearly. You end up with three completely connected layers. You start with three layers and then you go to a softmax function. You go to softmax. You then go to the next layer and so on until you have three layers. soft Max will scale more logarithmically um and it'll give you a final like probability map um are there any questions on Alex now actually before uh we move on yeah what's upYeah so uh if you don't specify a certain type of padding valid padding is going to be going to go away. If you specify a specific type ofpadding valid padding will be going away. It's going to take a certain amount of padding to make a certain number of points. It'll take a number of seconds to get to that point. be applied to make sure that as you're sliding your uh kernel across an image uh you're left with the same dimension is there anything you want to add Jake or no that's I I should have mentioned adding two yeah I mean you did a good job we basically applied the kernel to the image to ensure that it was in the right place and that it had the right amount of space. Jake: "I should have mentioning adding two?" "Yeah, I mean I did. I mean, I did," Jake says. just had a bunch of zeros on the outside um if we want to just pull away the uh the size of our yeah and by having a stride of one um you don't like lose information. You don't reduce size you're quite literally going over every single Pixel in. You're quiteliterally going over all of the information in your head. It's very, very difficult to do that. You have to be very careful. You can't just pull it away. You've got to make it smaller. your image um yeah and as you can see uh we use a relu activation all the way until the end uh vgg is you can think of it as a deeper Alex net um there's not too much to go over here um the motivation behind this is you. Your image is a deep Alex net.your image is you're image. You're image is an Alex net, you're a deeperAlex net. You are a deepAlex net, and you're an Alex Net. You have to be Alex Net to be an AlexNet. want a deeper neural network um that's kind of the purpose of this now instead of five convolutional layers you have 23. You still have the three dense layers at the end which computes a little bit better because computers evolve CPU power is able to handle these high order. Instead of five Convolutional Layer you have23 you still have 23 you have three dense layer. You have 23 convolutionals and 23 dense layers. That's a lot of data to work with. So that's what we're doing. Matrix computations um but yeah you have a lot of parameters um as is kind of a side effect of all these computations that you're trying to do uh so this is an interesting observation using a one by one convolution to create a transformation of input channels um so so this was kind of an experiment in how we use convolution in the matrix. So this was an interesting experiment in terms of how we used convolution and how we can use it in the Matrix. So that was an example of what we did. one by one convolutions can actually be used as a form of padding and dimensionality addition and reduction. We're going to be talking about that a little bit more um as you can see in like I think one or two slides. We do want a higher accuracy but we do want to be accurate at the same time. We'll be talking more about this in the coming weeks and months. Back to Mail Online home. back to the page you came from."We want ahigher accuracy at thesame time" we want a deeper Network we want something that is able to effectively both space and time effectively compute these Matrix products yeah so yeah and you're also adding pixels to uh the the top and bottom as you're sliding if that makes sense there is a yeah there's a yeah. There's aYeah there's also a yeahthere's a Yeah there's an image of the world that you can see on the screen as you slide. There is also an image that shows the world as you move. There are also images that show what the world looks like when you're moving. picture on the next slide I believe uh uh okay yeah yeah so a one by one conclusion uh the the motivation behind it is you're applying a linear function that doesn't lose information as you go across better radio right um yeah exactly. That's why the size won't change. I believe that's the reason for the different size of the different types of transmitters. I think that's why there's a difference in the size of different transmitters across different frequencies. I'm not sure if that's true. the important thing the channels change though oh right the number of filters that you have and my channel yeah you won't change the actual size of the image. Dimension the depth of your like yeah depth is kind of how I think about it but in reality is channels. The important thing is the channels changes though. Oh right theNumber of filters you have. And my channelYeah you won’t change the actually size of your image Dimension the Depth of your. like yeah Depth is kind Of how I Think about It but in Reality is channels that are gradiently moving yeah that was a good question though yeah so this is an example with vg16. This is 16 layers um with three dense layers at the end as well um as you can see as you go through your convolutions your output Dimension is shrinking. That is what happens when you have convolutions with multiple layers. That's what you get when you use a convolutions tool like Vg16 with 16 layers and a convolution tool like vg4. So based on your kernel size your actual image is shrinking but your number of channels increases as you gain information. As you can see in BGG 16 specifically it looks like the size is approximately getting halved every time um and yeah this is more so just a just a show of what you can do with BGG. It's not meant to be a guide to how to use BGG, it's just a way to show you how to work with the game. It doesn't mean you should use it all the time, it just shows you how you can work with it. visualization of how you have multiple layers that are stacked they're pulled together which is where you're having the information that you have between each of these steps um as you apply your kernel of size two by two you're increasing the number of channels and information thatyou're getting. "You're getting more information" is the key to building a good kernel. "I'm getting more and more information," he says. "That's what I'm trying to get out of my code. I'm getting a lot of information" um and lastly you're passing it into three dense layers to do those major Matrix multiplications at the end post flattening all right so um Inception Nets uh this drawing is also quite complicated um but what you need to take away from this is that um you sorry is. um you are doing a Matrix multiplication in this drawing. You are doing one of the most complex Matrix multiplations in the history of the world. You're doing a matrix multiplications in the middle of the drawing. that you want to be able to use one by one convolutions to keep your input size the same as you go through multiple steps as well as do your regular Max pooling and convolutional uh step with a certain kernel. This is going to be explained more clearly in the next few posts. We hope you will find this information helpful. We are also working on a version 2.0 of the code that will make this easier to use. For now, we are working on version 1.2 of the software. there's a visualization for this as well um but essentially you're taking information from the previous layer um and also using it in the current layer that you're using. This is kind of kind of lead into the next topic that we're going to talk about so yeah there's. There's. A visualization forthis as well as wellUm but basically you're take information from. the previous layers and also use it in. the current layers that you’re using and this is. kind of like a step-by-step guide to get there. a lot of one by one convolutions um these are used to only change the number of channels and not modify the input size. These are deep and wide to capture features at different scales um and that is also going to be covered in terms of depth wise. A lot of  one by one convolutions are used to change the number of channels and not modify the input size um These are deep and wide to capture features at different scales. uh convolutions as well as point-wise convolutions these are combined to form a more efficient computationally method while still retaining all the information that we get with a traditional convolution neural network. uh yeah this is uh not explained super well um but essentially um we want to encourage people to try out this new way of thinking. uhYeah this is not explained really well but essentially this is a new way to think about convolutional networks. Uh yeah. We want to try and make them more efficient. uh Yeah. discrimination in lower stages um increase the gradient signal that gets propagated back and provide additional regularization um the motivation behind this is that again we want to learn low level features um in earlier stages of the classifier and then as we go through we wants to consider previous features as well as current ones. The goal is to learn features that are low level in the early stages and then high level features in the later stages. We also want to consider the previous features that have been learned in the earlier stages. layers and previous inputs that we've taken into account um to regularize and make sure that spatial discrimination is maintained in our gradients. As we're tuning as we're going through the the back propagation process there are a couple of common issues that can uh kind of be fixed. We're looking at how we can improve the quality of the images. We've also been looking at ways to improve the speed of the video. We'll also be looking at the way we can make the video look better in the future. of result from just blindly stacking layers um so you might ask uh we saw Alex Ned and we saw bdg which was like basically the same thing but we added a couple more layers. Why can't we just infinitely stack these layers um and there are a lot of layers to go through to get to the bottom of the problem. We're going to try and figure it out. We'll let you know what we come up with in the next few days. Back to the page you came from. problems that come with that that Inception and resonance um try to fix and that is adding residuals yeah uh what do you mean by branched uh yeah the classifier is at the end uh by this uh again there's there's a drawing for it but uh essentially you have a Branched Inception. There's a Drawing for It but uh Essentially you have Abranched inception and Resonance. There are a lot of things you can do to make it better. a yeah yeah you have a multi-headed yeah so you're you're taking whatever your uh input is in a certain step and applying it to the output of another step um this maintains kind of a about it's a backwards way of maintaining a residual value uh which is why which iswhy which is the reason for the 'multi-headed'Yeah yeah you've got to take whatever you're doing in one step and apply it to another step in the next step.a yeahYeah you have to do that in order to get the results you want. it's called Uh residuals um we want higher accuracy and a simpler architecture and these are two things that Inception Nets um are able to give us in some form um but residents bring to another level as well um so yeah this is the intuition that I was talking about. It's called uh residuals. It’s called "Uh residuals" and it's called "Inception Nets" It's a tool that helps people understand their surroundings. It helps them understand the world around them. It can help people understand themselves. about before adding more layers shouldn't hurt. The layer can learn the identity transform which is essentially how you're transforming a certain input in every step so the accuracy should not decrease as you add a bunch of layers. However this is not the case adding more and more layers. Adding more layers should not hurt because the layer can learning the identity transforms which is basically how you are transforming a specific input in each step. This is the key to making sure your layers are working correctly at all times. layers hurts because you're not taking into account the previous identities that you've had post-transformation um this is kind of where residuals come into play and you'll see that soon um Vanishing gradients is a common problem as you add a bunch of layers stacked together and that the learning process is affected. VanishingGradients.layers hurt because you's not taking. into account your previous identities and that you have had. post-Transformation identities. The learning process. is affected by Vanishing Gradients. signal or the gradient computation becomes extremely weak the model struggles to learn um and your weights start Vanishing um the other side of this problem is explode ingredients which isn't as applicable to this but another problem a third problem is shatter ingredients and the the point of shatter and the model can't learn to deal with it. The other way to solve the problem is to use explode ingredients instead of shatter ingredients, which is a different type of shatter ingredient. The model can then learn to work with shatter ingredients. ingredients is that as I go deeper into an extremely deep convolutional neural network my gradients actually start resembling white noise so there's no uh pattern my model isn't actually learning anything because of the depth of the network. This chain rule that never updates dates based on a previous date is one of the key ingredients of the model. It's also the reason why the model doesn't have a pattern that looks like it's learning anything. It doesn't look like the model is learning anything at all because it's so deep. step it only goes backwards in time it never learns the identity transform this is where residuals come into play that's why it's called resnet the solution is make it easier to learn at least the identity so keep information from previous stages into future computation that's like the the resnet.step. It only goes back in time, it never learning the identity transforms. This is where resnet comes into play. It's called Resnet and it's used to communicate with other computers. key motivation behind residuals so yeah this is what I was talking about earlier um which is that you have X which is some identity that you want to keep in mind um you have f of x which is the function that you're applying through your weight layer and so on. So that's what we're trying to get at here. We're looking at residuals as a way of getting at the identity that we want to have in our body. So we're looking for that identity in the weight layer. And we're also looking at the f of X as well. relief activation function um as your X goes through a weight layer the function is applied you go through another weight layer. This f of x kind of encompasses that process this is the function that you've applied to X now your output is whatever f of X is the relief activation function is the result of the weight layer process. For more information on how to use the tool, go to: http://www.cnn.com/2013/01/28/technology/how-to-use-the-cnn-relief-activation-function-in-a-painful-way.html. motivation behind residuals is that after your f of x has been applied you add X back into your network um so by multiplying or by adding the resultant X by whatever your original identity was and using that as the input for the next layer you've maintained a semblance of a semblance. You can also add X by multiplying the original identity by whatever the original X was and use that as an input for your next layer. For more information on residuals, see residuals.co.uk. of what you had prior to whatever function you've applied so this helps especially when you're adding you're doing this computation like hundreds of times as your layers and your the depth of your network increases okay yeah so as you can see each one of these jumps is a jump. of what you have prior to the function you're applying. So this helps. especially when we're adding layers and our layers and as our network increases. okayYeah so as we can see, each of these jumping is a jumps.ofwhat you had before the function that you're using. residual that's being computed. A 34 layer residual will have jumps between every two. This is a process known as bottlenecking versus if it was after every layer. Adding residuals will increase the time to convergence because you're increasing the number of backwards considering computations that you have so if you're if your bottleneck isn't as big your time to converge will be smaller so as your uh as your residual skips more and more levels your time of convergence will also be smaller but your results may also not be as good. because you're not negating the problem this is highly dependent on what system you're using to compute these um as well as like where the model is eventually running so yeah um yeah dude that was an example of a very long uh residual net adding skip connections makes the problem worse. It's a very, very long way of saying that you're going to have to make some changes to your model to get it to behave as you want it to. But it's a good way to start. identity easier to learn because you're quite literally adding a previous identity to the resultant of a transformation uh as you can see this is a gradient map. Lost surface of resnet with and without skip connections uh oops yeah as we can see um this is used. This is used to create the gradient map of the resnet surface. The gradient map shows the surface of the Resnet surface without and with skip connections. The map also shows the Surface of the Lost surface with and Without skip connections, as well as the Surface Of The Lost surface. to uh this makes the loss a lot smoother um because your identity is preserved as opposed to trying to retrain after every transformation yeah I am not sure actually yeah because like the loss you'll get after every update step is like a number so I'm not super high. "This makes the Loss a Lot smoother" "It's like a lot of fun" "I like it a lot" "Yeah, it's fun. I like it. It's fun." "I don't know if I'm super high on it. I don't think I am" event like a low dimensional projection s yeah yeah this is like probably like really important thing for today but like this idea of like why it'd be important to sort of be able to learn the identity like it's sort of a weird thing um are there any questions? We'll be back in a few days with a new episode of "This Is Life with Lisa Ling" at 10 p.m. ET on Channel 4. For more information on "This is Life With Lisa Ling," go to www.channel4.com/LisaLing and follow Lisa Ling on Twitter @LisaLang. or comments or concerns about that yes yeah for sure right so like if you have a dent snail Network like like let's just ignore convolutions right now. If you like a dense neural network trivially you have to have a matrix called the identity Matrix which is just ones along the edges of the network. Yes yeah forSure right solike if you. have a dented snail Network. like likelet's just ignored convolutions. right now right now if you like. a dense Neural Network triviallyyou have a Matrix called the Identity Matrix. Yes forSureright so like for sure. diagonal and it spits out the exact same thing that it took in so if I have like a vector you and I multiply it by like the identity Matrix and like X1 X2 whatever uh I'm going to get that exact same Vector here x that I took in. That's what I'm trying to do. I'm just trying to get the same thing out of a vector that I've taken in. I don't know how to do it. I just want to get it out. so like if I have a neural network and I just trivially add so I have like a whole bunch of like layers right like this this new connection to the next layer right like just a dense neural network. If I just make each layer like the identity Matrix if I make all the weights correspond to the identityMatrix like there's no reason I shouldn't be able to make like a million length neural network which is kind of absurd. But like in practice we've observed that like if you add if you put a million layers on a dense Neural network it's going to just learn like garbage like it's not going to work at all. took in it's just kind of curious that it was observed that deeper networks don't work and this makes it super easy so if your weights are literally all zeros and your biases are literallyAll zeros you're going to spit out exactly what you took in so it makes it easy. It's justKind of curious. that it's observed that deep networks don’t work. It’s just kind Of Curious that It was observed. that deeper Networks don't Work. And this makes It super easy. it really really easy for the network to just say like hey okay we've got enough information at this point in the network like we don't need to learn more complicated features. I could just send the weights to zero and just ship exactly what I have about halfway through. It's really easy to just ship what you have at this stage in the process. I've been doing this for about a year and a half and it's been a lot of fun. I'm really proud of the work we've done. the network all the way to the end does that kind of make sense like it's way easier to learn to just uh to change what you have um or it's it's much easier to just spit out exactly your features like halfway through the network if your network decides to do something different. It's way. easier to learning to justUh to changewhat you haveUm or it'm it's very easy to change exactly what you've got um or you can just spit it out exactly how you want it. like okay like we've got enough information to like make a good classification but we're only about like halfway through the network with with this. It's just super easy for it to learn like okay we don't need the rest of these layers they're they're only going to go so far, he says. He says it's easy for the system to learn that it doesn't need to go to the next layer. "It's just like super easy," he says, "to say like okay, we're going to get to the top of the network" resnet just used two uh two is a fine Choice it's just something you can tune and there's nothing wrong with it.just confuse the signal um uh uh how do you know exactly like how many years to have in your block hereYeah so I mean resnet justused two uhtwo is afine Choice it is just somethingyou can tune. There's nothingwrong with it, just confused the signal. Yeah so Imean resnetjust used two Uh Two is a Fine Choice it’s just something You can Tune. like foreign so like if you're doing the chain rule it just results in a lot of multiplications right like the more like um like we talked in the third lecture about applying like the chainrule um to deep neural networks and if you know um every little individual is like foreign like foreign. Like foreign.like foreign. like foreign so if you do the chainRule it just Results in A LOT of Multiplications. Like Foreign.likeForeign.like Foreign. like Foreign. LikeForeign. step in your network if you just multiply the partial derivatives of all of those steps you can find the the derivative of your loss with respect to a given parameter just with the chain rule. But if all of these different things that you're multiplying are even a little. little then you can get a sense of how much of a loss you have by multiplying all of them together. If you multiply all of the steps together, you get the total loss of the network. You can also get a feel for how much loss you've lost by multiplying them all together. bit smaller than one immediately. At a certain point at a certain number of multiplications your partial derivative uh your your chain rule that you've gotten as a result of many many multiplications just gets sent straight to zero um and on the other hand if your uh values are too small, they just don't get added together. If you add two numbers together, the result is the result of a multiple of two. If two multiplications occur at the same time, the resulting product is the product of two different multiplications. if all of your individual little partial derivatives of all the individual little steps are a little bit bigger than one it's going to explode um and that's just not helpful [Music] um it's problematic uh we would like yeah we would Like to update our weights in way that that's a good idea. We would like to see a change in the way that we weigh ourselves. We want to see the weight of our bodies change. We'd like to change the way we weight ourselves in a good way. is like sort of uh sort of regular a little bit more consistent um so that our weights aren't either just exploding because the gradient steps are huge or they're just literally never going to change. So does that make sense yes yeah if you're stacking like a bunch of. weights like a lot of people have done in the past? "Yes" "Yes, that's what we're trying to do" "So that we don't end up exploding" "We're not going to explode. We're just going to stay the same" sub 1 multiplications you're going to be left like a super small number as you go back yes and it doesn't necessarily mean you're close to a minimum either like because your lost surface can be like a little Plateau. It just means that for some of our our parameters it's going to leave a little bit of a gap. It doesn't mean that you're not going to get a lot of data, it just means you'll have to work with a little more of it. really early in our Network where the gradients that we're getting are either huge or super small it just means it's not moving. There's other things that help with Vanishing gradients like batch Norm helps as well as batch Norm. It's not necessarily the same thing as being at a minimum but it's a good start. It just means that it doesn't move as much as it used to, and that's what we're trying to achieve. We're still working on it, but we're making progress. well um and it's probably like a bigger contributor to like stopping the vanishing gradient problem then uh and residuals but like residuals help. I think residuals are more for like shattered gradients so that's when like you introduce a bunch of like meaningless noise into your gradients. so they're shattered. Well um and It's probably a bigger contributions to like stopped the vanishing gradients than uh and  residuals. But I think residuals help too. I think reinforced  gradients help too. slightly different problems where your gradients start resembling noise instead of something that's actually meaningful as you go through back prop because that was no it comes from again like the the depth of the matrix multiplication that you're doing uh so without like by losing form of the identity. It's a little bit of a different way of looking at it, but it's the same thing. I think it's a good way to look at it. I don't know if there's any other way to do it. uh that is the reason that stacking a bunch of layers doesn't result in like better performance or strictly better performance even like equivalent performance. You would think that like going from like three to twenty layers uh this is like a very small example but like that. uh you would thought that going fromlike three to 20 layers would result in better performance, but it doesn't. It just results in equivalent performance even if it's a lot of layers on top of each other. That's the reason why you don't get better performance. shouldn't produce accuracy when in reality like if this is scaled it can yeah awesome group wise uh your Dimensions so yeah this is a really good point um so if your layer is a convolution um the dimension can change which is why often this is result like kind of kind of like kind. of an inaccurate result. If you are trying to make a map of the world, you need to be able to see the whole world. You can do this by adding layers on top of each other. of viewed as f of x plus W of X where W is a transformation that you do on X two to make it the same Dimension exactly yeah. To make sure your Matrix addition stays the same like like basically you have like the X number of layers and you have to make sure you have the same Matrix addition on each layer. The Matrix is a set of layers that you add on top of each other to make them look the same. It's a type of Matrix that can be used to create different Matrix layers. things like during the state actually with that you know like like like they are here you're talking about which weights are you saying that like such that it's more so that you maintain information that you had previously post a feature map being applied so.things like that you've learned unnecessary like this. It's just so that we can maintain information. that we had previously posted a featuremap being applied. so that it doesn't get lost in the shuffle. so we can keep track of things like that. you're not necessarily zeroing out uh learned weights uh this like Jake was saying like other ways of uh normalizing your data as you go through like Bachelor affect the vanishing gradient problem more than residuals do the main point of this is that you want to maintain a semblance of a semblance. You're notnecessarily zeroing in learned weights, but you're trying to normalize your data so that it doesn't look like you're zeroing it out. You want to keep a semblance that it looks like it's normal. of identity as you go through your uh your network if that makes sense but that was a very good catch on the dimensions of X yeah this is often viewed as plus W of x there any other questions about resnet all right dope uh so the next thing is to figure out how to connect to the internet in a way that doesn't look like a web address. That's the first thing. The second thing is that you have to be able to talk to each other on the internet. to talk about is global average pooling um which is designed to replace fully connected layers in cnns. This is also used in resnet in replacement of the fullyconnected layers. You're generating a feature map for each category of the classification task so once you have a map, you can then use it to sort the tasks into categories. It can also be used to create a list of categories for a particular task. It's also used to generate a list for a certain type of task. the vector that you want to classify your classification Vector you're generating a feature map for each of those averaging those and then that is fed into the softmax layer. This is the typical dense layer that you previously had that's facilitating these connections. It does not enforce correspondences. It's just a way to get around the problem of how to connect all of these different layers together. It doesn't have to be perfect, but it can be a good starting point. It can be used to get a better idea of the problem. between feature maps and categories you're kind of throwing feature Maps down the drain as you go through the three dense layers at the end of the network. There's no parameter to optimizing global average as well which saves overfitting time um because you're just generating a feature map averaging. It's a great way to get rid of some of the overfitting that happens when you're trying to add new features to a network. It can also be used to reduce the amount of data that needs to be added to the network at a time. those and feeding that into a softmax function you're not actually tuning a parameter to your data. In dense layers this often results in if I have a very deep neural network that is trained on a certain subset of data. If you have a bunch of dense layers, you're going to have a lot of data that's not being used. You're not going to be able to use that data in a way that's going to make a big difference to your results. It's a bit of a pain, but it's a way to get at the problem. at the end it's very easy to overfit to the data that I have provided uh for training um so this kind of prevents that. This comes more into play also in Mobile nuts and uh the efficient that's that will be talked about as well all right all right. Back to Mail Online home. Back into the page you came from. Back To the pageyou came from, back to the page where you come from. Click here to return to the beginning of the page. um there any questions on the previous kind of topics all right all right that was kind of the meat of this lecture uh but mobile nuts are very cool in that you're you're using depth wise convolutions and point-wise convolutions to reduce the number of computations that you are doing.um there were any questions about the previous topics? There were some questions about mobile nuts. There was a question about the use of depth wise Convolutions and Point-wise Convolutions. There were also questions about using depthwise Convolution with Point- Wise Convolution. you're doing um yeah so as we get deeper the number of channels can get large which leads to a lot of parameters. What if we process each Channel separately and then intelligently combine those at the end how can we still retain data from each Channel while reducing the amount of data we lose? How do we make sure we don't lose any of the data that we're trying to retain? How can we keep the data we're looking for while reducing our loss of data? What do you think? number of uh computations that we need to do the answer is depth wise separable convolutions so this is a a pretty decent visualization of how that works. If you have like a three channel uh like some X by X image you're applying a uh like a feature map. It's a pretty good visualization of what's going to happen if you're trying to do a depth wise convolutions on an image that's three channels wide. That's what we're doing here. We're doing a convolution on a three-channel image that has three channels. to it um that results in one product and then you're left with one product by one channel um instead of that what if we took each Channel individually we applied a smaller or a lower Dimension feature map uh to it created three depthwise layers um and then combine.to it. To create the final product, each channel was individually applied to a different part of the product. The result was a product that was one channel wide and one channel deep. The final product was one that was three channels wide and three channels deep. those with a in this case it would be a three three a one by one by three convolution so once I have these three layers I can get the same output size here by applying a oneby one by 3 convolution feature to it if that makes sense. If you have any questions, please post them in the comments below or email them to jennifer.smith@mailonline.co.uk and I'll try to answer them as best I can. Thanks for your support. um hopefully this will explain it a little bit more so if I have an eight by eight by three uh kind of channeled image after a convolution step we want to increase or decrease channels. We want to decrease it so that we're left with the 8 by 8 by 3 image. In this case, we're going to increase it by one channel so that it's 8 by 7 by 3. In the past, we've used 8 by 6 by 2 by 2 to get the same result. same output size as we would have if we did this three by three byThree map so by taking one by one by three images uh kind of calculations we're left with the same eight by eight by one image that we we wanted to get um yeah this this map. We're going to be using this map for the rest of the year. We'll be using it for the next year or so. We want to be able to use it for a lot of things. We don't want to have to do it every single day. is extremely important when we want to Stack a bunch of filters or a lot of layers so when this computation is being scaled over a a wide variety of filters that we're running in this case it's 256. We want to be able to do these computations more so that we can scale them over a wider range of filters. We're running 256 filters in this example, but we're going to be running a lot more filters in the future so we'll be scaling these more and more. efficiently uh and there's a little example that will hopefully show how the math behind this works. So essentially yeah if you're involved by 12 by 3 image and a like five five three feature you would have to do and this result in an eight by eight by one. That's what this is all about. It's just a way to get the most out of a picture. It doesn't mean you have to be a genius to do it. It just means you can do it more efficiently. at the end of your process you'd have to do 75 computations as this is applied into like a smaller subset over here and this is three channels. um times this would be 64 times right so this wouldbe times you're 64. um and then this would then be. And then you'd then be done with the whole process. And that would be when you were done with all of your computations. That would be at the end, when you're done with your whole process, you'd be done. multiplied uh 256 times or however many channels that you're doing um on the other hand uh if we did the other metric which is instead of a five by five by three we have a uh five byFive by one. Then we still have uh 25 which is the amount of computation we're doing on a single pane times 64 times again there are three channels um sorry 25 times 64. uh which gets you you're saying eight by eight by one as you're stacking this is then multiplied by 256. so now you have 64 times 3 when you're one by one. by one convolution is applied to uh the kind of single single eight by eight by one image that you have. If that makes sense um there's some visualizations on the next slide that make a lot more sense yeah so you're. 64 times three when you's one by. one byOne convolution, applied to a single single. 8 by 8 by 1 image is a convolution of one image to another image. That's what we do with convolution. applying this one-dimensional filter to each channel of your image that is applied to to each one these are then concatenated together and you apply a smaller pointwise convolution one by one by the number of channels you have to end up with a feature uh or a a resultant. The result is an image that looks like a 3D model of the user's face. The image can be viewed by clicking on the image below to see it in its entirety. The resulting image is a 3-D version of the image that the user can see in its full glory. that's the same Dimension as you would be expecting which highly reduces the number of computations that you need to do so yeah normal you would do three by three by eight which is like the size of your output is um versus now you're doing a one by one by three times eight. I guess I guess that's the way to look at it. I don't know what the answer is to that. I'm just going to say that it's a good way of looking at it because it reduces the amount of work that's needed. output channel so if I had a starting off at the the last step right if you have an eight by eight by three image that you're you're running this on now instead of going through and like I guess multiplying this by 256 which is like how it would be. It would be like how you would start off with an 8 by 8 by 3 image and then go through and multiply it by 256. It's like how I would start it off with a 8 by8 by 3 and then a 256 by 256 image. generally work I'm running a thin one by one by three layer here so this is 64 times three it's 192. and this is what is being multiplied by the 256 and added to our previous product which is 74 times 64. so these two are being added together to make a total of 192. This is the result of adding the 256 to the previous product and then adding the new product to the old one. The result is a final product of 192 times 64 times 64, or 74 times 74. end up with your your final computation for how many I guess multiplication parameters you have other questions about this yeah. So this is the same kind of chart that we had before but applied to the mobile net um and thinking about it in terms of like simpler problems. It's a very simple way of looking at the problem. We're going to be using it for the next few months. We'll be doing a lot of work on it. It'll be a big part of our training. simpler layers kind of helps you visualize it better yeah uh I'm trying to okay yeah so mobilenet has a lot fewer parameters which results in a lot faster convergence time um and it matches Inception of D3 accuracy just by using depth and point wise convolutions and combining.simplerlayer helps you visualization it betterYeah uh I've got a lot of questions for you. You can ask them in the comments below or on Twitter @CNNOpinion or @cnnOpinions. those so you can think about it instead of doing one step that results in one map and multiplying that by the number of filters you have you're not doing that. So you're applying this one step to every channel and then your next step is applying a different sized convolution to do your filter multiplication. So instead of one step you have two steps that are being combined um which reduces complexity quite a bit alrighty I guess we. I mean we'll have to wait and see what happens. can quickly go over like squeezing anxiety networks basically uh you squeeze you apply this through a couple of dense layers and then you rescale. So we talked about global average pooling um how you come up with a feature map for each Channel. You're compressing this through an array of layers and it's then applied to a map of the world. We talked about how to make a map and how to apply it to each channel. We also talked about the different ways to make the map. connected layer passing it through relu and with a fully connected layer you can also expand this back to whatever Dimension you originally had um rescaling according to the layer output is also not as computationally intensive. I think the slides are pretty good and compressed in a very visual way. Click through the gallery to see the slides in their original form. Click here for more information on how to use this tool to help you with your own presentations. For confidential support, call the Samaritans on 08457 90 90 90 or visit a local Samaritans branch or click here for details. way uh the remainder of the piano more impressive art it's actually the other way. I I hope the main takeaways are that you'll like understand those rules and that you you see that we've like adding all of these different sort of like tools to your tool belt. That's what we've been trying to do here. We've like added all of this different kind of tools to our tool belt to help you play the piano. We hope you'll take some of this with you and use it. now so like residual connections you can just use them in place of regular conclusion of Step by Step local convolutions um and one by one convolutions. uh instead of using a standard convolution like they're just different little tools that you can you can use instead of a regular convolution. Now we're going to take a look at some of the things that we can do with these. We'll be back in a week or so to talk about the next step in the project. We're going on a road trip to New Jersey. swap out if you're very well CNN building plus um you know yeah I think understanding like the the base of how optimizations are held and the problems that certain optimizations face and others don't really sets the stage for like future networks like the efficient net in 2020. um. Swapping out ifyou're very good CNN buildingplus um you you knowYeah I think Understanding like thethe base ofHow optimizations areheld and the Problems that certainoptimizations face and other don't face. um youknow yeah I Think understanding the Base of howoptimizations are held is key. we can straight up just go by what we've already learned in that we know we can pass through a one by one convolution a depthwise convolution which is where we apply this filter to each Channel individually um recombine them using our squeeze and exide networks and then pass them through. We can straightup just goby what we're already learned. We know we're capable of passing through aOne by one Convolution a Depthwise Convolution. We're able to pass through this filter and then recombine the channels using our squeezing and exides networks. It's just taking a bunch of building blocks and putting them together yeah this is kind of a graph comparing efficient net in latency and accuracy to other networks. It's just a bunch. of building Blocks and putting. them together. It is just taking. one by one convolution to maintain like dimensionality um so really uh that's all these architectures are it's just. taking a lot of building. blocks and Putting them togetherYeah this is. kind of an example of how you can do that. so these are some things that these models wanted to optimize over time accuracy performance and model size. um model size is something that has a trade-off if you get too big you lose out on other metrics like accuracy. performance issomething that directly corresponds to depthwise convolutions. so these were some things they wanted to optimizing over time Accuracy performance andmodel size. accuracy performance is somethingThat directly corresponds. to Depthwise convolution. Performance is something That directly correspondsto depthwise Convolutions. and mobile nuts for Edge Computing and things like that. You want to drastically reduce the number of computations that you want to do yep that is basically everything for today thank you guys for coming oh and there will also be a quiz there will Also be a Quiz in the next episode of The Daily Discussion. Back to the page you came from. Share your thoughts on the show with us on Twitter @dailyconspiracy and @wiredconspiracy. Send your photos and videos to jennifer.smith@mailonline.com. at least is there a homework position closer or not [Music] um and do next Friday [Music]. thank you for sure [Music]: I'll see you next Friday. [Music: Thank you for your time] and I'll be in touch next week. [ music: "I'll be back in a week or two,"] and we'll talk more about this week's episode of This Is Life with Lisa Ling on CNN.com on Friday night at 10 p.m. ET.