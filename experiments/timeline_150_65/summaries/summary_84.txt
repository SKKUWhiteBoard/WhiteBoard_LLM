This is part 3 in our series on distributed word representations. We're going to be talking about vector comparison methods. To try to make this discussion pretty intuitive, I'm going to ground things in this running example. On the left, I have a very small vector space model. We have three words, A, B, and C. And you can imagine that we've measured two dimensions, dx and dy. And then you can see graphically that B and C are pretty close together. And A is kind of lonely down here in the middle. corner, the infrequent one. We can measure the Euclidean distance between vectors u and v if they share the same dimension n by just calculating the sum of the squared element wide differences, absolute differences, and then taking the square root of that. Let's look at that in terms of this space. So here we have our vector space depicted graphically, A, B, and C. And you can see that Euclideans distance is capturing the first perspective that we took on the vector space, which unites the frequent items. B and C as against the infrequent one A. As a stepping stone toward cosine distance, which will behave quite differently, let's talk about length normalization. Given the vector u of dimension n, the L2 length of u is the sum of the squared values in that matrix. And then we take the square root. That's our normalization quantity there. And A and B are now close together. Whereas B and C are comparatively far apart. And that has come entirely from the normalization step. we changed the space as I showed you before. So they're all up here kind of on the units here. And notice that the actual values that we get out are the same whether or not we did that L2 norming step. And that is because cosine is building the effects of L2norming directly into this normalization here in the denominator. There are a few other methods that we could think about or classes of methods. I think we don't need to get distracted by the details. their generalizations to the real valued vectors that we're talking about. And the other class of methods that you might see come up are probabilistic methods which tend to be grounded in this notion of KL divergence. Now I've alluded to the fact that the cosine distance measure that I gave you before is not quite what's called the proper distance metric. Let me expand on that a little bit. To qualify as a properdistance metric, a vector must be valued as a probability value. comparison method has to have three properties. That is, it needs to be symmetric. It needs to give the same value for xy as it does to yx. KL divergence actually fails that first rule. And crucially, it also needs to satisfy what's called the triangle inequality. It just happens that this distance here is actually greater than these two values here, which is a failure of the statement of the triangleequality. But this is also kind of a useful framework. different choices that we could make, of all the options for vector comparison, suppose we decided to favor the ones that counted as true distance metrics. Then that would at least push us to favor Euclidean distance, Jaccard for binary vectors only, and Jensen-Shannon distance if we were talking about probabilistic spaces. And we would further amend the definition of cosine distance to the more careful one that I've given here, which satisfies the triangle inequality as well as the other two criteria. And by this kind of way of dividing the world, we would also reject matching J Accard, Dice, Overlap, KL divergence, and symmetrical KL divergence. And so that might be a useful framework for thinking about choices in this space. Right, these shortcomings might be addressed through weighting schemes though. But here's the bottom line. There is valuable information in raw frequency. If we abstract away from it, some other information might come to the surface. But we also might lose that important frequency information in distorting the space in that way. And it can be difficult to balance these competing pressures. Finally, I'll just close with some code snippets. Our course repository has lots of hand utilities for doing these distance calculations and also length norming your vectors. the results for "bad" using cosine distance in cell 12 and Jaccarddistance in cell 13. And I would just like to say that these neighbors don't look especially intuitive to me. It does not look like this analysis is revealing really interesting semantic information. But don't worry, we're going to correct this. We're going. to start to massage and stretch and bend our vector space models. And we will see much better results for these neighbor functions and everything else as we go through that material.