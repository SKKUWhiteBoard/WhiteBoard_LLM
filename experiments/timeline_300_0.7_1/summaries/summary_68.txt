Today we're gonna talk about learning in the setting of games. Can you still be optimal if you reveal your strategy? It's actually not the size that matters. It's the type of strategy that you play that matters, so just to give you an idea. And then, er, towards the end of the lecture, we wanna talk a little [NOISE] bit about variations of the game- the games we have talked about. So, uh, how about if you have- how about the cases where we have simultaneous games. going to pick bucket A, bucket B, or bucket C. And then the opponent is going to pick a number from these buckets. They can either pick minus 50 or 50, 1 or 3 or minus 5 or 15. So if you want to maximize your, your utility as an agent, then you can potentially think that your opponent [NOISE] is trying to, trying to minimize your utility, and you can have this minimax game, kind of, playing against each other. game of chess. And if you think about the game of chess, the branching factor is huge. The depth is really large. It's not practical to u- to do the recurrence. So we, we started talking about ways to- for speeding things up, and, and one way to speed things up was this idea of using an evaluation function. So instead of the usual recurrence, what we did was we decided to add this D here, um, this D right here which is the depth that un- until which we are exploring. In chess, you have this evaluation function that can depend on the number of pieces you have, the mobility of your pieces. Maybe the safety of your king, central control, all these various things that you might care about. So, so the hand- like you can actually hand-design these things and, and write down these weights about how much you care about these features.and figure out what is a good evaluation function. So to do that, I can write my evaluation function, eval of S, as, as this V as a function of state parameters. how learning is applied to these game settings. And specifically the way we are using learning for these game. settings is to just get a better sense of what this evaluation function should be from some data. And, and that kind of introduces to this, this, um, temporal difference learning which we're gonna discuss in a second. It's very similar to Q-learning. Uh, and then towards the end of the class, we will talk about simultaneous games and non-zero-sum games. at a simplified version of it. So in this simplified version, I have Player O and player X, and I only have four columns. I have column 0, 1, 2, and 3. And the idea is, we want to come up with features that we would care about in this game of backgammon. So maybe like the location of the X's and O's. The number of them. Maybe fraction of X's or O's that are removed, whose turn it is. these indicator functions. You might ask number of O's on the bar that's equal to 1, fraction of Os that are removed. So, so we have a bunch of features. These features, kind of, explain what the sport looks like or how good this board is. And what we wanna do is we wanna figure out what, what are the weights that we should put for each one of these features and how much we should care about, uh, eachOne ofThese features. In general, you would need to do something like Epsilon-Greedy. But in this particular case, you don't really need to. Because we have to get- we have this die that, that you're actually rolling the dice. So we, kind of, already have this, this element of randomness here that does some of the exploration for us. Yes. And you just mean like unexplored probability? Yes. So, so that's the key idea. We call these policies. We go over them to make things better and better. So, so what you have is you have a piece of experience. Let's call it s, a. You get some reward. Maybe it is 0. And you go to some s prime through that and you have some prediction. Your prediction is your current, like, your current V function. So, so my target the thing that I'm trying to like get to is the reward plus Gamma V of s prime, w. And then we had a target that you're trying to get to. And my target, which is kind of like my label, is going to be equal to my reward. is simple, right? 2 reduced, 2 gets canceled. Gradient is just this guy, prediction of w, minus target, times the gradient of this inner expression. So so far so good. Um, so this is the TD learning algorithm. This is all it does. So temporal difference learning, what it does is it picks like these. pieces of experience; s, a, r, s prime, and then based on that pieces of. experience, it just updates w based on this gradient descent update. This is very similar to Q learning. There are very minor differences that you'll talk about actually at the end of this section, comparing it to Qlearning. All right. So, so I wanna go over an example, it's kind of like a tedious example but I think it helps going over that and kind of seeing why it works. Especially in the case that the reward is just equal to 0 like throughout an episode. So it kinda feels funny to use this algorithm and make it work but it work. Is it that possible to have, an end state and not end state have the same feature vector, or no? If you use like, uh, initialize rates do not be zeros which you update throughout instead of just to the end. To differentiate between S4 and s- S9 to, to differentiate between them, you should pick features that differentiates between them. If there were kind of the same and have same sort of characteristics, it's fine to have feature that gives the same value. So for the second row, I'm not gonna write it up cause that takes time. So, uh, so okay, so let's start wi- with a new episode. We started S1 again but now I'm starting with this new W that I have. Uh, they always have to be the same dimension, and what should we be thinking about that would make a good feature for updating the weights specifically, like- so, so it should be a representation of how good your state is, and then it's- yeah, it's usually like hand designed, right. of 10, uh, if you're using the same feature extraction for both, how does that affect the generalized ability of the model, the agent? Yeah, so, so you might choose two, two different features and one of them might be more like so. So there is kind of a trade-off, right? You might get a feature that actually differentiates between different states very well, but then that makes learning longer, that makes it not as generalizable. So, so picking features, it's, itâ€™s an art, right, so. [LAUGHTER] that we are looking at in Q-learning, but there are some minor differences. So, so the first difference here is that Q- learning operates on the Q function. A Q function is a function over state and actions. Here, we are operating on a value function. On V. And V is only a function of state, right? And, and part of that is, is actually because in the setting of- in setting of a game, you already know the rules of the game. The idea of learning in games is old. People have been using it. In the case of Backgammon, this was around '90s when Tesauro came up with, with an algorithm to solve the game. So, so that's what we've been talking about so far. And the idea of using learning to play games is, is not a new idea actually. Of this idea of, I have this evaluation function, I wanna learn it from data, I'm going to generate data from that generated data I're going to update my W's. In 2016, we had AlphaGo, uh, which was using a lot of expert knowledge in addition to, um, ideas from a Monte Carlo tree search. And then, in 2017, we have AlphaGo Zero, which wasn't using even expert knowledge, it was all, like, based on self-play. Uh, and the idea of TD Learning is, is to look at this error between our prediction and our target and try to minimize that error and, and find better W's as we go through. Minimax sca- strategy seemed to be pretty okay when it comes to solving these turn-based games. But not all games are turn- based, right? Like an example of it is rock-paper-scissors. You're all playing at the same time, everyone is playing simultaneously. The question is, how do we go about solving simultaneously, okay? So let's start with, um, a game that is a simplified version of rock- Paper-Scissors. This is called a two-finger Morra game. So, we have player A and player B. We have these possible actions of showing 1 or 2. And then, we're gonna use this, this payoff matrix which, which represents A's utility. If A chooses action A and B chooses action B. So, so I'm trying to like get good things for A. Yes. In this case it's not at the end [inaudible] ? Uh, yeah. And this is like a one-step game too, right? So like you're just playing and then you see what you get. stochastic policies. So, so pure strategies are just actions a's. And then you can have things that are called mixed strategies and they are probabilities of, of choosing action a, okay? All right. So here is an example. So if, if you say, well, I'm gonna show you 1, I's gonna always show you1. Then the- if you can, you can write that strategy as a pure strategy, that says I'm. gonna always with probability of 1 showYou 1 and with probability 0 show you 2. game. If someone gives me the policy, how do I evaluate how good that is? So, that is going to be the setting where, uh, Pi A chooses action A, Pi B chooses action B times value of choice A and B, summing over all possible a and bs. So, so for this particular case of Two-finger Morra game, let's say someone comes in and says I'm gonna tell you what Pi A is. Policy of agent A is just to always show one, and policy of agent B is this, this mixed strategy which is half the time show one. And then the question is, what is the value of, of these two policies? How do we compute that? [NOISE] Someone tells me it's pi A and pi B, I can evaluate it. I can know how goodpi A andpi B is, from the perspective of agent A. But with the challenge here is we are playing simultaneously, so we can't really use the minimax tree. So right now I'm going to focus only on pure strategies. I will just consider a setting- very limited setting and see what happens. How bad would it be if we were to play sequentially? So that's what I wanna do for now. If we have pure strategies, it looks like if you're going second that should be better. So, ah, so going second is no worse. It's the same or better. And that basically can be represented by this minimax relationship, right? So, so agent A is trying to maximize. Player B wants to minimize. So in the second case, um, we are maximizing second over our actions of V of a and b, and Player B is going first. So this is going to be greater than or equal to the case where Player A is going, uh, first. that's the question we're trying to answer. Okay? So, so let's say Player A comes in, and Player A says, "Well, I'm gonna reveal my strategy to you" So the value of the game, uh, would be, maybe I'll write it here. Pi A is already this mixed strategy of one-half, one- half, right? It's going to be equal to Pi- is this- yeah, actually. All right. So well, that's equal to minus 1 over 2 Pi B of 1, plus minus $3. So, if someone tells me, "Well, this is a thing I wanna do," I should try to minimize value of Agent A, right? So, so what I'm really trying to do as Agent B is to minimize this, right, because I don't want Agent A to get anything. So the best thing that I can do as a Agent 2 is to follow a pure strategy that always shows 1 and never shows 2. So that tells me that never show 2 and always show 1. In this more general case, Player A is playing first, uh, and is following a mixed strategy but doesn't know what p they should choose. They're choosing a p and 1 minus p here. And then Player B has to follow a pure strategy. And by pure strategy, what I mean is you always end up like putting as much possible like 1, like all your probabilities on the negative turn and nothing on the positive turn because you are trying to minimize this. So that's kinda like intuitively why you're getting this pure Strategy. to 7 over 12 here, like these two values end up being equal. Equal, right? [inaudible]. [OVERLAPPING] Uh, none of them are actually equal. The reason that they end up be equal is you are trying to minimize the thing that this guy is trying to maximize. So no matter what your opponent does, like you're gonna get the best thing that you can do. Okay. So I'm player A, I gotta be citing a p. That's not gonna be too bad for me. The key idea here is revealing your optimal mixed strategy does not hurt you. The proof of that is interesting. If you're interested in looking at the notes, you can use linear programming here. The reason, kind of the intuition behind it is, is if you're playing mixed strategy, the next person has to play pure strategy and you have n possible options for that pure strategy. So that creates n constraints that you are putting in for your optimization. You end up with a single optimization with n constraints, and, and. and you can using like linear programming duality to actually solve it. the pay off matrix. So, uh, so you have two players A or B. Each one of you have an option. You can either testify or you can refuse to testify. So you can- B can testify and A can refusal to testify, and I am going to create this payoff matrix. This payoff matrix is going to have two entries now in each one of these, these cells. And, and why is that? Because we have a non-zero-sum game. different players. A Nash equilibrium is setup policies Pi star A and Pi star B so that no player has an incentive to change their strategy. In a collaborative Two-finger Morra game, it's not a zero-sum game anymore and, and you have two Nash equilibria. And Prisoner's dilemma is the case where both of them testify. So we just actually solve that using the minimax- von Neumann's minimax theorem, right? So there would be if you're playing a mixed strategy of 7 over 12 and 5 over 12, you might kind of modify your Two- finger Morragame and make it collaborative. look at them, would be useful for projects. And with that, I'll see you guys next time. Back to the page you came from! Back to Mail Online home. Back into the fold! Back To the pageYou came from: Back To The Page You Came From. Back from the Page you Came From: back to the Page You came from.back to thepage you came From: Back to The Page you cameFrom: The PageYou Came from: ThePageYouComingBack to: ThepageYouComing Back to: