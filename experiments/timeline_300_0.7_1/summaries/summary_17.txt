Machine learning is about how to acquire a model from data and experience. In the next few lectures, we're going to work through a sequence of different takes on machine learning that are going to highlight different subsets of the big ideas on this topic. We'll start with model-based classification, and, as an example of that, we'll work through some details of how the Naive Bayes models work. And we'll see more in-depth examples and more structured examples of these kinds of problems later on when we talk about applications. an output. And the whole point is to automate the prediction of the output on the basis of the input. So the input might be an email, and the output is the decision. Is this spam or is this a good email, which people who work on spam call ham. In practice, in the real world, getting the right kind of data is often one of the hardest parts of building and deploying a machine learning system. And we'll see today exactly how data kind of goes through the mill and gets turned into a model. The boundary between what is actually spam, unsolicited commercial email, and emails you just don't want, can be a fuzzy boundary. Machine learning is going to do some amount of work, but something has to power this. There has to be something about those first emails that's going to give you the clues that something's fishy here. And so, what kinds of features can you include? Defining these features is a big part of deploying machine learning systems. For example, maybe any dollar sign followed by some numbers is a bad sign, and that's a feature that abstracts over individual words. Machine learning can be used to make predictions about spam or ham. The training set for machine learning is very noisy, so it can be hard to label. Machine learning is not perfect, and some inputs are just really, really hard, and they're going to look like this and we're just not all even going to agree on what that's supposed to be. The goal is to be able to predict labels of new images that are not the ones we've already seen, OK? So that's actually subtle, but it's super-important. representations that if the thing gets tilted or it's a little bit lighter. We could look at how many connected components of ink there are. What's the aspect ratio? How many loops are there? It's increasingly the case, especially for problems like this, that we feed in low-level features like pixels, and higher level features like edges, tend to get induced increasingly more as our machine learning methods get better at doing that. We'll talk about that in a couple weeks when we talk about narrow nets. In this lecture, we're going to talk about model-based classification. In model- based classification, rather than directly learning from errors that you make in the world from experience, think like reinforcement learning model-free. After today we'll look at the model- free methods. In this case, we'll build a Bayes net called a Naive Bayes model. You build a model where the output label and the input features are random variables. There's going to be some connections between them, and maybe some other variables too. representations be more compact, or because that's data that's easier for you to elicit. What structure should the Bayes net have? Today, we're going to give it the simplest structure that could possibly work, and it turns out it often does. How should we learn the parameters of a model from data once we've decided it's structure? So here's what a Naive Bayes model would look like for digits. It's in fact super-simple as Bayes nets go. The Naive Bayes assumption is an extremely radical assumption as far as probabilistic models go, but for classification, it turns out to be really effective. So if we have a single digit recognition version of this, we might have a feature for each position of the grid. They'll all be binary valued. That means this number one here maps to this feature. But if my features were instead, how much ink because they're on the left side? How many loops are there? Then there would be zeros and ones as well, but they would no longer correspond to the raw image. There's a lot of features. A general Naive Bayes model places a joint distribution over the following variables, y, which is your class, and some number of features, which you get to define. You're going to have to write code which extracts them from your input. So if your spam feature is, have more than 10 people received this email in the past hour? The machine learning will do the work to connect the probability of that taking on a certain value up to the class. And that means when you go to make a prediction, it decomposes into a product of a bunch of different feature conditional probabilities. The Bayes net is a model that assumes all the features are conditionally independent, given the label. For each feature-- which is each kind of evidence-- we're going to need to compute a bunch of conditional probabilities for each class. These things collectively, all these probabilities that we use to plug and chug and get our numbers out, are called the parameters of the model. It's the product. of a prior probability of the class, which says whether or not before you see the evidence, this is a class that's common or not. In a Naive Bayes model, each class 1 to 0 is equally likely. The product of-- across each position in the document, the probability of the word at that position given the class. In real, if I went to like a real collection of data, I could get 1,000 examples of the number 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 69, 68, 70, 71, 72, 73, 74, 78, 79, 80, 81, 82, 83, 84, 85, 86, 88, 89, 90, 91, 92, 93, 94, 94. to learn is for each class, what is the histogram of words? What is the probability distribution over words in that class? These are tied distributions. This is called a bag of words model, and this is different than the standard case because in the standard cases, each feature gets its own distribution. Here, we assume the features are all identically distributed, but there are multiple copies of that feature for the different positions. And this is nice, because when the document is longer or shorter, you don't really need to change your model. In this example, we're the spam classifier. We're going to produce a joint probability by multiplying in evidence terms as the words come in. In spam, the most likely word is the. Somewhere far down that list is the word free. For ham, it's the too. All right, so far, we think it's ham because the log of that product is higher than the probability of the given word. Gary, remember, this is a generative model, and that means that we have the features that we want to use. It's actually very common when you're multiplying probabilities to just add log probabilities instead. In the end, when you want to turn it into probabilities, you do need to sum them. And summing the logs won't do that. You need to do a sort of log sum, which one way to do that is to convert them back to probabilities by taking exponentials. That's actually not the way you would do it. You would sort of shift them by their minimum or their maximum as appropriate so that you don't get underflow. word depends on the class and also the previous word. This Is a better model of language. If you started, if you did prediction in this, and you cranked out a pretend document, it would look significantly more like a real email than if you just did a bag of words. However, will it be more accurate for classification? It really depends. In general, it will be a little more accurate, but at a cost of having significantly more complicated conditional probabilities to estimate. If I take a spam document, and I permute all the words randomly, Ive definitely, like it is no longer syntactically-valid English. class which is not kind of strongly connected to the actual ordering, Naive Bayes is really good. Otherwise, you add other correlations like this to fix it. OK? Other questions? Good questions. All right. A couple more general slides, and then, we'll take a break. We are now into the machine learning section which means we are done with the spooky Halloween time ghost-busting section. But I have candy. So during the break, everybody get up and come grab candy if you would like. The basic principle of machine learning is something called empirical risk minimization. We would like to find the model, classifier, whatever, that does the best-- whatever the best means-- on our true test distribution. So maybe we would like, get the Naive Bayes spam classifier which is most accurate at finding spam in real people's inboxes today. We don't actually know that true distribution. Like, we don'tactually know what it's going to see at test time, in the same way that when you go to your final exam, you don't know what questions you're going to get. Find the best model on your training set is usually phrased, at some level, as an optimization problem. The main worry is that you over-fit. You can have not enough training data, and there's just no way to have sort of seen the whole space. And then, you can have tons of data drawn from the wrong distribution. The problem might not be with the training set, though, the problem is that the learning over-fits to the trainingSet. We'll see some examples of that even today. for CS189, and then you walk into the CS188 final. You're like, something's wrong here. I understand the concepts, but it's just not lining up. OK, that's a drift in distribution. That's where your training examples were plentiful, but they were drawn from a distribution which does not match the one that you're going to see at test time. This idea of the test distribution sort of being not stationary against the training distribution is something that's really important in the real world. to have a test set, which is not the real future test use that it's going to be put once it's deployed, but you need something that is not in your training data to check. In practice, there's usually other little shards of the data that you're going to want to have. So, for example, one common one is held-out data. We'll see today and in future lectures what that's for. And so we take our data and we break into training, where we learn our parameters, and tests,where we check our accuracy. your training data, and after one or more passes through, you're going to know your parameters. Then you'll-- and you usually select those on the basis of some held-out data. And then finally, once you're ready to see how your experiment turned out, you take that model and you test it on the test data. You don't want to test your classifiers on the data that was used to train them, because they will do surprisingly well. But you as a researcher, are going to be tempted to grab that test data and look at it in great detail and be like, ah, I see. we have held-out data, which gives you something you can peek at. I ran 20 experiments. How did they go? Am I doing well? Is this thing good enough to release? You need to have some metric, and there's a lot of possible metrics. An easy one is accuracy. For how many of these emails did I make the correct decision? Fraction of instances predicted correctly, but actually, that's actually not a great metric for spam detection. Any ideas why? What's wrong with accuracy? different metrics people have for different kinds of tasks. We want a classifier which does well on the test data. And then we try to come up with methods where the training accuracy is going to mean something about the test accuracy. Over-fitting means fitting the training data very closely, but not generalizing well. There's also the opposite, which is under-fitting, where you're just like, I don't know what's going on. That's not over-fit. It's not going to work very well. Spam is being generated by people who are trying to defeat spam filters. Spammers are going to double down on what's working. And so in that sense, over time, spam classification doesn't actually look like a standard classification problem because it's adversarial. In general, we're going to do discrete classification. But for this example, let's imagine that we're looking for language, but then suddenly, you start using methods like primarily looking at sender information. And now you have spammers who want to buy contact information so that they can spoof that. the thing we're trying to do is to fit a curve to this data. So you say, what is the fit? Is the fit getting as close as possible to the last dot? So what is my constant approximation to this? Does anybody want to hazard a guess? Let's call it five. OK, did I fit something about this data? Yeah, I felt something about the data. I fit basically it's mean. Did I capture the major trends? No. All right, let's try again. Let's fit a linear function. It's close, right? It's a better fit than the constant function. than the quadratic. It's about fitting your data to the point where the patterns that you are capturing are ones which generalize to test, and that's a tricky balance. Over-fitting shows up not just on these continuous functions. It also shows up, for example, let's imagine in a hypothetical digit classification, we might say, here is an image I've never seen before. Let's use Naive Bayes to classify it. So what would we do? We'd do our running total. We'd say, all right. Well, before I look at any features, the numbers are the numbers two and three. a 3 than a 2, let's imagine. So, so far, a three is winning. But eventually, I'm going to get to some pixel, maybe like this one here. And in my training data, this is almost never on. This is in a corner where there's no number. And maybe it turns out that this number, that this pixel happened to be on once or twice in the data for the digit two, but zero times in theData for digit three. the ratio is one, it means it's equally likely. Whether it's common or uncommon, it doesn't affect the competition. It's things that are more common in one than the other that have a big impact on these odds ratios. So let's look at words. What do you think, in my training data for ham versus spam, things with the highest odds ratio for ham would be? These are things that're significantly more likely for ham than for spam. It turns out, there are a bunch of words in this data which occur in spam once, and it could occur in once and occurs in spam zero. over-fitting usually shows up as sampling variance. To do better, we need to smooth, or regularize, our estimates. We can use elicitation, right? You can ask a human. You can go to a doctor and say, hey, I'm building a classifier. What fraction of people with meningitis will present with a fever? And a doctor can give you a guess. It may be qualitative. You could also do that empirically. You Could go collect a bunch of records. The maximum likelihood estimate, or relative frequency estimate, is a way to estimate the likelihood of an outcome. The more samples you draw, the more accurate your estimate will be. But in practice, you need some smoothing to make your model more accurate. We want our model to assign probability to events it's never seen, so that one errant pixel or word that is rare doesn't completely torpedo an otherwise very nuanced balancing of evidence. It's basically the same thing, and it almost is, except for one term. This is actually, due to Laplace, hundreds of years ago now, who's a philosopher who kind of worried about things like, well, how do I estimate the probability? Like, what is the probability the sun will rise in the morning? Every morning so far it's risen, so probability one. But I know that can't be right. So I need some way of mechanically incorporating the fact that there are events which I haven't seen, but which I know to be possible, or at least that I'd like to model as being possible. good idea to take into account the probabilities in your observation, but you should hold out an extra observation for everything you didn't see to reflect it potentially happening at some point in the future. So basically, add one to all your counts, including the ones that are zero. So the maximum likelihood estimate for red, red, blue, if I say, what's the probability of red, comma, probability of blue? 2/3, 1/3. Laplace plus would say, instead of saying there's two of one and one of the other, adding one to each. there's 100 blues. Now how many reds do I have? Well, I do my computations as if I had 102 reds and 101 blues. And suddenly, even though there are still more reds than blues, in my posterior estimate here, it's pretty close to 50-50. So as I crank up k, I have a stronger prior, and I fit less. If I crank down k,. I fit more, and so I now I have. a dial which can trade off the amount of fitting against generalization. probably in there somewhere. If you see money, that's a good sign that it's spam. There are some things that indicate ham. This looks like general English text. What is going on there? Helvetica vs. Verdana. This reflects the default fonts that were in use at this time across different platforms. And so one of the things you find in machine learning is, you know what you think the features are going to be, or rather, which features areGoing to be useful. But you might be wrong. In general, your model is going to make errors. We're talking a bit more about this starting next lecture. Here's some examples of errors that the quick Naive Bayes system I whipped up makes on this training set. One was spam that should have been ham, one was ham that should've been spam. These are tricky cases, and it's actually very hard to tell from the words which one's which. And then there's this other one that's also an error, which is, to receive your $30 Amazon promotional certificate, click on this. In spam classification, we found out that it wasn't enough to just look at words, you've got to look at other sort of metadata from the ecosystem. For digit recognition, you do sort of more advanced things than just looking at pixels. Try to do things that are invariant to rotation and scale and all of that the vision folks think about. You can add these as sources of information by just adding variables into your Naive Bayes model, but we'll also talk in the next few classes about ways to add these more flexibly, and also ways to induce these.