In week four, we take a break from learning more and more on neural network topics, and talk about final projects, but also some practical tips for building neural network systems. So first of all, I'm going to introduce a new task, machine translation. And it turns out that our task is a major use case of a new architectural technique to teach you about deep learning. And so we'll spend a lot of time on those. And then there's a crucial way that's been developed to improve sequence to sequence models, which is the idea of attention. Machine translation is the task of translating a sentence x from one language to another. In the early 1950s, there started to be work on machine translation. It was hyped as I don't think we'll ever replace the translators of that type of material. But despite the hype it ran into deep trouble. And so in retrospect, it's not very surprising that the early work did not work out very well. And then we use as the translation. OK. Get straight into this with machine Translation. Machine could be able to translate one to two million words an hour. This would be enough to cope with the whole output of the Soviet Union in just a few hours of computer time a week. But neural machine translation systems also have some disadvantages compared to the older statistical machine translation system. They're less interpretable. They also tend to be sort of difficult to control. It's hard to know what they'll generate. There are various safety concerns. So there's still tons of stuff to do. When they were in the period of statistical NLP that we've seen in other places in the course. And then the idea began, can we start with just data about translation i.e. sentences and their translations, and learn a probabilistic model that can predict the translations of fresh sentences? So suppose we're translating French into English. We can say, what's the probability of different English translations? And then we'll choose the most likely translation. And it's not immediately obvious as to why this should be because this is sort of just a trivial rewrite with Bayes' rule. The European Union produces a huge amount of parallel text across European languages. The Canadian Parliament conveniently produces parallel text between French and English, and even a limited amount in Inuktitut, Canadian Eskimo. And then the Hong Kong parliament produces English and Chinese is working out the correspondence between words that is capturing the grammatical differences between languages. So you can have words that don't get translated at all in the other language. So in French, you put a definite article "the" before country names like Japon. So when that gets translated to English, you just get Japan. One French word gets translated as several English words. You can get the reverse, where you can have several French words that get translated as one English word. So here we sort of have four English words being translated as two French words. But they don't really break down and translate each other well. These things don't only happen across languages. They also happen within the language when you have different ways of saying the same thing. So another way you might have expressed the poor don't have any money is to say the poor are moneyless. That's much more similar to how the French is being rendered here. We start with a source sentence. So this is a German sentence. And as is standard in German. You're getting this second position verb. So that's probably not in the right position for where the English translation is going to be. So we might need to rearrange the words. And so we explore forward in the translation process. And we could decide that we could translate next the second word goes, or we can translate the negation here, and translate that as does not. And in the process, I'll go through in more detail later when we do the neural equivalent. We sort of do this search where we explore likely translations and prune. we have is based on the translation model. We have words or phrases that are reasonably likely translations of each German word, or sometimes a German phrase. And so then inside that, making use of this data, we're going to generate the translation piece by piece kind of like we did with our neural language models. And if we're guided by our fairly small, something like 5 to 10. And at each step of the decoder, we are going to keep track of the k most probable partial translation. "he." And so then doing LSTM generation just like last class, we copy that down as the next input. We run the next step of the L STM, generate another word here, copy it down, and chug along. And we've translated the sentence, right? So this is showing the test time behavior when we're generating the next sentence. And so our representation of the source sentence from our encoder is then this stack of three hidden layers, whoops. And then that we use to then feed in the next word. The models have also been applied not just to natural languages, but to other kinds of languages, including music, and also programming language code. So you can train a seq2seq system, where it reads in pseudocode in natural language, and it generates out Python code. And if you have a good enough one, it can do the assignment for you. So this central new idea here with our sequence to sequence models is we have an example of conditional language models.everywhere else as well. each step, as we break down the word by word generation, that we're conditioning not only on previous words of the target language, but also each time on our source language sentence x. Because of this, we actually know a ton more about what our sentence that we generate should be. So if you look at the perplexities of these kind of conditional language models, you will find them like the numbers I showed last time. They usually have almost freakily low perplexities, that you will have models with perplexities that are something like 4 or even less.  multilayer or stacked RNNs are more powerful. It's almost invariably the case that having a two layer LSTM works a lot better than having a one layer L STM. After that, things say, I'm going to use this hidden representation to look back at the source to get information directly from it. And so we'll be training the model here to be saying, well, probably you should translate the first word of the sentence first, so that's where the attention should be placed. are somewhat flimsy terms. The meaning isn't precise. But typically, what that's meaning is that lower level features and knowing sort of more basic things about words and phrases. So that commonly might be things like what part of speech is this word, or are these words the name of a person, or a company? Whereas higher level features refer to things that are at a higher semantic level. So knowing more about the overall structure of a sentence, knowing something about what it means, whether a phrase has positive or negative connotations. become much less clear. It's normally very hard with the model architecture that I just showed back here to get better results with more than four layers of LSTM. Normally to do deeper L STM models and get even better results. You have to be adding extra skip connections of the kind that I talked about at the very end of the last class. Next week, John is going to talk about transformer based networks. They're typically much deeper. But we'll leave discussing them until we get on further. So that we have our LSTM, we start, generate a hidden state. It has a probability distribution over words. And you choose the most probable one the argmax, and you say "he", and you copy it down and you repeat over. So doing this is referred to as greedy decoding. Taking the most likely word on each step. And it's sort of the obvious thing to do, and doesn't seem like it could be a bad thing toDo. But it turns out that it actually can be a fairly problematic thing todo. Stuck with it. And you have no way to undo decisions. So in this case, so I can fit it on a slide. The size of our beam is just 2. Though normally, it would actually be a bit bigger than that. And the blue numbers are the scores of the prefixes. So these are these log probabilities of a prefix. So we might generate he hit, he struck, the most likely following word. And then for each of those, we generate the k most likely next words tart, pie, with. In greedy decoding, we usually decode until the model produces an end token. In beam search decoding, different hypotheses may produce end tokens on different time steps. And so we don't want to stop as soon as one path through the search tree has generated end. So what we do is sort of put it aside as a complete hypothesis and continue exploring other hypotheses via our beam search. And then we'll look through the hypotheses that we've completed and say which is the best one of those. And that's the one we'll use. In a newspaper, the median length of sentences is over 20. So you wouldn't want to be having a decoding model when translating news articles that says, huh, just generate two word sentences. They're just way high probability according to my language model. So the commonest way of dealing with that is that we normalize by length. So if we're working in log probabilities, that means taking dividing through by the length of the sentence. And then you have a per word log probability score. BLEU gives a score between 0 and 100 where your score is 100. If you are exactly producing one of the human written translations, and 0 if there's not even a single unigram that overlaps between the two. For assignment 4 this year, we've decided to do Cherokee English machine translation. Cherokee is an endangered Native American language that has about 2000 fluent speakers. It's an extremely low resource language. So it's just there isn't much written Cherokee data available period. And particularly, there're not a lot of parallel sentences between Cherokee and English. Many languages don't distinguish between things masculine or feminine. When that gets translated into English by Google Translate is that the English language model just kicks in and applies stereotypical biases. So if you want to help solve this problem, all of you can help by using singular they in all contexts when you're putting material online. And that could then change the distribution of what's generated. And people also work on modeling improvements to try and avoid this. Here's one more example that's kind of funny. People noticed a couple of years ago. That if you choose one of the rarer languages that Google will translate, that the gender neutral sentences get translated into, she works as a nurse. such as Somali, and you just write in some rubbish like ag ag ag. Freakily, it had produced out of nowhere prophetic and biblical texts, as the name of the Lord was written in the Hebrew language. As far as I can see, this problem is now fixed in 2021. So there are lots of ways to keep on doing research. NMT certainly is a flagship task for NLP and deep learning. And it was a place where many of the innovations of deep learning NLP were pioneered, and people continue to work hard on it. For languages for which there isn't much parallel data available, commonly the biggest place where you can get parallel data is from Bible translations. So this is a piece of parallel data that we can learn from. Cherokee is not a language that Google offers on Google Translate. So we can see how far we can get. But we have to be modest in our expectations because it's hard to build a very good MT system with only a fairly limited amount of data. There is a flipside, which is for you students doing the assignment. The advantage of not too much data is that your models will train relatively quickly.