Today we're gonna talk about learning in the setting of games. So what does learning mean? How do we learn those evaluation functions that we talked about? And then, er, towards the end of the lecture, we wanna talk a little [NOISE] bit about variations of the game- the games we have talked about. So, uh, how about if you have- how about the cases where we have simultaneous games or non-zero-sum games? So that's the, that's the plan for today. So I'm gonna start with a question that you're actually going to talk about it towards the end of the lecture, but it's a good motivation. So, uh, think [NOISE] about a setting where we have a simultaneous two-player zero-sum game. And an example of that is rock, paper, scissors. So can you still be optimal if you reveal your strategy? So lets say you're playing with someone. If you tell them what your strategy is, can youstill be optimal? That's the question. In the game of chase- che- and chess, you have this evaluation function that can depend on the number of pieces you have, the mobility of your pieces. Maybe the safety of your king, central control, all these various things that you might care about. So the idea of using an evaluation function was to speed things up. And, and potentially, a designer can come in and say, "Well, I care about nine times more about how many pawns I have." So the hand-designer can actually hand- design these things and write down these weights. "I care about the number of kings and queens and these sort of things that I have, but I don't know how much I care about them. And I actually wanna learn that evaluation function. Like what the weights should be." "So to do that, I can write my evaluation function, eval of S, as, as this V as a function of state parameterized by, by weights Ws" "And, and my goal is to figure out what these Ws, what these weights are. And ideally I wanna learning that from some data" In the first part of the lecture, we're going to look at backgammon. And then towards the end of the class, we will talk about simultaneous games and non-zero-sum games. And, and that kind of introduces to this, this, um, temporal difference learning which we're gonna discuss in a second. It's very similar to Q-learning. Okay. So let's think about an example and I'm going to focus on the linear classifier way of looking at this just for simplicity. can actually, like, roll two dice and based on the outcome of your dice, you move your pieces various, various amounts to, to various columns. So your goal is to get all your pieces off the board. But if you have only one piece and your opponent gets on top of you, they can push you to the bar and you have to start again. Um, there are a bunch of rules about it. Read it, read about it on Wikipedia if you're interested. But you are going to look at a simplified version of it. maybe like the location of the X's and O's. The number of them. Maybe fraction of X's or O's that are removed, whose turn it is. These features, kind of, explain what the sport looks like or how good this board is. And what we wanna do is we wanna figure out what, what are the weights that we should put for each one of these features and how much we should care about, uh, each one. So, so that is the goal of learning here. somewhere. So, so one idea that we can use here is we can try to generate data based on our current policy pi agent or pi opponent, which is based onOur current estimate of what V is. Right. So currently, I might have some idea of what this V function is. It might be a very bad idea ofwhat V is, but that's okay. I can just start with that and starting with, with that V function that I currently have, what I can do is I can, I can call arg max of V over successors of s and a to get a policy for my agent. "We generate episodes and then from these episodes, we want to learn. These episodes look like state action reward states and then they keep going until we get a full episode" "The reward is going to be 0 throughout the episode until the very end of- end of the game. Until we end the episode and we might get some reward at that point or we might not" "We go over them to make things better and better. So that's, kind of, the key idea" So s, take an action, you get a reward. You go to some s prime from that and you have some prediction. Your prediction is your current, like, your current V function. And then we had a target that you're trying to get to. And my target, which is kind- kind of acts as a label, is going to be equal to my reward, the reward that I'm getting. So I'm gonna treat my target as just like a value, I'm not writing it as a function of w, okay? is simple, right? 2 reduced, 2 gets canceled. Gradient is just this guy, prediction of w, minus target, times the gradient of this inner expression. So the objective function is prediction minus target squared. And then the update is this, this particular update where we move in the negative direction of the gradient. This is, this is what you guys have seen already, okay. All right. So so far so good. Um, so this is the algorithm we're going to use. is the TD learning algorithm. This is all it does. So temporal difference learning, what it does is it picks like these pieces of experience; s, a, r, s prime, and then based on that, it just updates w based on this gradient descent update, difference between prediction and target times the gradient of V, okay? So what if my V of sw is just equal to w dot phi of s, yeah phiof s. So what happens to my update? Minus Eta. between the two? Yeah, so this is very similar to Q learning. There are very minor differences that you'll talk about actually at the end of this section, comparing it to Qlearning. All right. So, so I wanna go over an example, it's kind of like a tedious example but I think it helps going over that and kind of seeing why it works. Especially in the case that the reward is just equal to 0 like throughout an episode. So I want to just go over like one example of this. If you use like, uh, initialize rates do not be zeros which you update throughout instead of just to the end. Yeah. Okay and section two, so S4 and S9 are the same future of activities but you said S4 is S9 [OVERLAPPING]. Uh, this is a made up example, [LAUGHTER] so don't think about this example too much though. Well, is it that possible to have, an end state and not end state have the same feature vector, or no? As one, uh, entry that's always isn't [inaudible] like instead of 1, 2, we have 1, 0 leading to the, the final weight then the weight corresponding to that. Is going to- [OVERLAPPING] Yeah. It will never converge. And that kind of tells you that that entry in your feature vector, you don't care about that. If it is always 0, it doesn't matter what the weight of that entry is. So in general, you wanna have features that are differentiating and, and you're using it in some way. on the relationship between the features and the weights. Uh, they always have to be the same dimension, and what should we be thinking about that would make a good feature for updating the weights specifically, like- So, uh, okay so first off, yes, they need to be always in the same- in dimension cause you are doing this, um, dot-product between them. And then it's usually like hand designed, right. So, so i- i- it, it's not necessarily- you shouldn't think of it as how is it helping my weights. of 10, uh, if you're using the same feature extraction for both, how does that affect the generalized ability of the model, the agent? Yeah, so, so you might choose two, two different features and one of them might be more like so. So there is kind of a trade-off, right? You might get a feature that actually differentiates between different states very well, but then that makes learning longer, that makes it not as generalizable, and then at the end- on the other hand, you might get one that's pretty generalizable but then it might not do these specific things. to 0.25 and 0.75 then it kind of stays there, and you are happy. All right so, so this was just an example of TD learning but this is the update that you have kind of already seen. And then a lot of you have pointed out that this is, this is similar to Q-learning already, right? This is actually pretty similar to update, um, it's very similar, like we have these gradients, and, and the same weight that we have in Q- learning. The idea of learning in games is old. People have been using it. In the case of Backgammon, um, this was around '90s when Tesauro came up with, with an algorithm to solve the game. And he was able to reach human expert play. And then more recently we have been looking at the game of Go. So in 2016, we had AlphaGo, uh, which was using a lot of expert knowledge in addition to ideas from a Monte Carlo tree search and then, in 2017, we have AlphaGo Zero, which wasn't using even expert knowledge. Minimax sca- strategy seemed to be pretty okay when it comes to solving these turn-based games. But not all games are turn- based, right? Like an example of it is rock-paper-scissors. You're all playing at the same time, everyone is playing simultaneously. The question is, how do we go about solving simultaneously, okay? So let's start with, um, a game that is a simplified version of rock- Paper-Scissors. This is called a two-finger Morra game. Game is a one-step game too, so like you're just playing and then you see what you get. So we have pure strategy which is almost like the same thing as, uh, as deterministic. And again the reason I only like talk about one way is we are still in the setting of zero-sum games. So, so I'm trying to like get good things for A. In this case it's not at the end [inaudible] ? Uh, yeah. policies. So a pure strategy is just a single action that you decide to take. We have also this other thing that's called mixed strategy which is equivalent to, to stochastic policies. And what a mixed strategy is is a probability distribution that tells you what's the probability of you choosing A. So, so pure strategies are just actions a's. And then you can have things that are called mixed strategies and they are probabilities of, of choosing action a, okay? All right. game. So, so for this particular case of Two-finger Morra game, let's say someone comes in and says I'm gonna tell you what Pi A is. Policy of agent A is just to always show one. And policy of agent B is this, this mixed strategy which is half the strategy. Pi A chooses action A, Pi B chooses action B times value of choice A and B, summing over all possible a and bs. Okay, so let's look at an actual example for this. time show one, half the time show, show two. And then the question is, what is the value of, of these two policies? How do we compute that? [NOISE] Well, I'm gonna use my payoff matrix, right? So, so 1 times 1 over 2 times the value that we get at 1, 1, which is equal to 2. Ah, so you might be interested in looking at what happens in repeated games. In this class right now we're just talking about this, one step one play. We're playing like zero-sum game um, but we's playing like we'll say, rock-paper-scissors and you just play once. Someone tells me it's pi A and pi B, I can evaluate it. I can know how good pi A is, from the perspective of agent A. But with the challenge here is we are playing simultaneously, so we can't really use the minimax tree. So what should we do? So I'm going to assume we can play sequentially. So that's what I wanna do for now. So right now I'm gonna focus only on pure strategies. I will just consider a setting- very limited setting and see what happens. In a more general case, I'm gonna make a lot of generalizations in this lecture. So I'll show you one example I generalize it, but if you're interested in details of it, like, we can talk about it offline. So, so, so setting is for any fixed mixed strategy Pi A. What I should do as Agent B is I should minimize that value. I should pick Pi B in a way that minimizes that value, and that can be attained by pure strategy. to 7 over 12 here, like these two values end up being equal. Equal, right? [inaudible]. [OVERLAPPING] Uh, none of them are actually equal. The reason that they end up be equal is you are trying to minimize the thing that this guy is trying to maximize. So no matter what your opponent does, like you're gonna get the best thing that you can do. So, so that's kind of the idea. All right. So let let's say I would pick a p that doesn't make these things equal. The key idea here is revealing your optimal mixed strategy does not hurt you which is kind of a cool idea. The proof of that is interesting. If you're interested in looking at the notes, you can use linear programming here. So, so let's summarize what we have talked about so far. Next 10 minutes, I want to spend a little bit of time talking about non-zero-sum games. In real life, you're kind of somewhere in between that, and, and I Want to motivate that by an example. dilemma? Okay. So, uh, so you have two players A or B. Each one of you have an option. You can either testify or you can refuse to testify. So you can- B can testify and A can refusal to testify, and I am going to create this payoff matrix. This payoff matrix is going to have two entries now in each one of these, these cells. And, and why is that? Because we have a non-zero-sum game. Because this was for player A, player B would just get negative of that. different players. So the von Neumann's minimax theorem doesn't really apply here because we don't have the zero-sum game. But do you actually get something a little bit weaker, and that's the idea of Nash equilibrium. So a Nash equilibrium is setup policies Pi star A and Pi star B so that no player has an incentive to change their strategy. So, so what that, that means is if you look at the, the value function from perspective of player A. There's a theorem which is, er, Nash's existence theorem. If you have any finite player game with a finite number of actions, then there exists at least one Nash equilibrium. In a collaborative Two-finger Morra game, it's not a zero-sum game anymore and, and you have two Nash equilibria. And then Prisoner's dilemma is the case where both of them testify. So we just actually solve that using the minimax- von Neumann's minimax theorem. There's a huge literature around different types of games, uh, in game theory and economics. Uh, but we have multi- we also have multiple game values from- depending on whose perspective you are looking at. If you're interested in that, take classes. And yeah, there are other type of games still like Security Games and or resource allocation games that have some characteristics that are similar to things we've talked about. And with that, I'll see you guys next time.equilibria.com.