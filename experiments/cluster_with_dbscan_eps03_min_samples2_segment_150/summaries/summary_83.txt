fine tune them for your own problem. This is one way that you can attack a lot of problems in deep learning, even if you don't have a huge dataset of your own. So that's kind of the brief introduction to like sort of GPU CPU hardware in practice when it comes to deep learning. And then I wanted to switch gears a little bit and talk about the software side of things. The various deep learning frameworks that people are using in practice. But I guess before I move on, is there any sort of questions about CPU GPU? Yeah, question? debate. So if you guys have AMD cards, you might be in a little bit more trouble if you want to use those for deep learning. And really, NVIDIA's been pushing a lot for deeplearning in the last several years. It's been kind of a large focus of some of their strategy. And they put in a lot effort into engineering sort of good solutions to make their hardware better suited forDeep learning. So in practice, you tend not to end up writing your own CUDA code for deepLearning. most people in deep learning when we talk about GPUs, we're pretty much exclusively talking about NVIDIA GPUs. So to give you an idea of like what is the difference between a CPU and a GPU, I've kind of made a little spread sheet here. And there's a couple general trends to notice here. Both GPUs and CPUs are kind of a general purpose computing machine where they can execute programs and do sort of arbitrary instructions. But they're qualitatively pretty different. For GPUs we see that these sort of common top end consumer GPUs have thousands of cores. TensorFlow is a pretty safe bet for just about any project that you want to start new, right? Because it is sort of one framework to rule them all. However, you probably need to use it with a wrapper and if you want dynamic graphs, you're maybe out of luck. Some code ends up uglier but in my opinion, maybe that's kind of a cosmetic detail. If you're just writing research code, I think PyTorch is a great choice. But it's a bit newer, has less support out there, so it could be a bit of an adventure. Numpy is definitely CPU only. And you're never going to be able to experience or take advantage of these GPU accelerated speedups if you're stuck working in Numpy. So, kind of the goal of most deep learning frameworks these days is to let you write code in the forward pass that looks very similar to Numpy, but lets you run it on the GPU and lets you automatically compute gradients. TensorFlow has this magic line that just computes all the gradients for you. So now you don't have to go in and write your own backward pass yourself. PyTorch provides pretrained models. And this is probably the slickest pretrained model experience I've ever seen. You just say torchvision.models.alexnet pretained=true. That'll go down in the background, download the pretrained weights for you if you don't already have them, and then it's right there, you're good to go. PyTorch also has, there's also a package called Visdom that lets you visualize some of these loss statistics somewhat similar to Tensorboard. different where we're actually building up this new computational graph, this new fresh thing on every forward pass. That's called a dynamic computational graph. For kind of simple cases, with kind of feed forward neural networks, it doesn't really make a huge difference, the code ends up kind of similarly. But I do want to talk a bit about some of the implications of static versus dynamic. And what are the tradeoffs of those two? So one kind of nice idea with static graphs is that because we're kind of building up one computational graph once, and then reusing it many times, the framework might have the opportunity to go in and do optimizations on that graph. Caffe.is this framework from Berkeley. Which Caffe is somewhat different from the other deep learning frameworks where you in many cases you can actually train networks without writing any code yourself. You need to define, now instead of writing code to define the structure of your computational graph, instead you edit some text file called a prototxt. Here the structure is that we read from some input HDF5 file, we perform some super new, it was only released a week ago. [laughter] So I really haven't had the time to form a super educated opinion about Caffe 2 yet. is really great for research. If you're interested in production deployment, you should probably look at Caffe, Caffe 2 or TensorFlow. PyTorch is a great choice. But it's a bit newer, has less community support, less code out there, so it could be a bit of an adventure. So it's kind of unfortunately, there's not just like one global best framework, it kind of depends on what you're actually trying to do, what applications you anticipate. - Hello? Okay, it's after 12, so I want to get started. So today, lecture eight, we're going to talk about deep learning software. This is a super exciting topic because it changes a lot every year. But also means it's a lot of work to give this lecture. But as usual, a couple administrative notes before we dive into the material. So as a reminder the project proposals for your course projects were due on Tuesday. We're also in the process of grading assignment one, so stay tuned.