homework two is out now. We're gonna be having some more background on deep learning this weekend. You're not expected to become, or, or to be a deep learning expert to be in this class, but we, you only need to have some basic skills in order to do homework two, um, be able to use function approximation with a deep neural network. Um, we're also gonna be reaching, uh, releasing by the end of tomorrow. Uh, but the sessions will be a good chance to catch up on that material. [NOISE] Um, just a quick humble, which of you have used TensorFlow or PyTorch before? The assignments, [inaudible] are they limited to TensorFlow? asked the question. I'm pretty sure that everything relies that, er, you're using Tensor Flow. Um, I'll believe you guys also should have access to the Azure credit. If you have any questions about getting setup without feel free to use the Piazza channel. We also released a tutorial for how to just sort of set up your machine last week. So, if you're having any questions with that, that's a great place to get started. Piazza: We're gonna go ahead and get started. Um, uh, what we're gonna be covering today is sort of a very brief overview about Deep Learning, um, as well as Deep Q Learning. Could look at the tutorial, you canLook at the video, or you can reach out to us on Piazza. Any other questions? All right. So, we've been discussing how to learn to make decisions in the world when we don't know the dynamics model of the Reward Model. We need representations of models. T, T or R or a state-action values Q, or V, or our policies, um, that can generalize across states and our actions. You might never see the exact same image of the world again, but we wanna be able to generalize from our past experience. And so, we thought about it, instead of having a table to represent our value functions, we were gonna use this generic function approximator where we have a W, which are some parameters. when we thought about doing this, we're gonna focus on function approximations that are differentiable. Um, and the nice thing about differentiable rep-representations is that we can use our data, and we can estimate our parameters, and then we can take use gradient descent to try to fit our function. And that information now could be [NOISE] in the form of episodes or it could be individual tuples. When I say a tuple, I generally mean a state-action reward next state tuple. is the same as the full gradient update. Our objective function is again the mean squared error. And the key hard thing was that we don't know what this is. So, this is the true value of a policy. But the problem is we donâ€™t know what the trueValue of a Policy is, and so we can't get a value for a policy that we think is the real value of the policy. And so, we have to change the way we think about a policy to get a true value. otherwise we wouldn't have to be doing all of this learning. And so, the two ways we talked about last time was inspired by a work on Monte Carlo, or on TD learning is we could either plug-in the return from the full episode. Or we could put in a bootstrapped return. So, now we're doing bootstrapping. Where we look at the reward, the next state, and the value of our next state. And in this case we're using a linear value function approximators for everything. all for linear value function approximation, but there are some limitations to use the linear valuefunction approximation, even though this has been probably the most well-studied. So, if you have the right set of features, and historically there was a lot of work on figuring out what those rights set offeatures are. They often worked really well. And in fact when we get into, I think I mentioned briefly before. When we start to talk about deep neural networks you can think of, a deep neural network is just a really complicated way to get out features. and how easy is it for us to converge to that. So, one alternative that we didn't talk so much about last time is to use sort of a really, really rich function approximator class. Um, where we don't have to, have to have a direct representation of the features. Er, and some of those are Kernel based approaches. These actually have a lot stronger convergence results compared compared to other approaches. The problem is, um, that the number of data points you need tends to scale with the dimension. The intuition is that if you want to have sort of an accurate representation of your value function, um, and you're representing it by say, uh, local points around it. For example, with the k-nearest neighbor approach. then the number of points you need to have everything be close like in an epsilon ball scales with the- the dimensionality. So, basically you're just gridding the space. Um, and everyone just [inaudible] name first please to stop me. a square, you're gonna need four points so that everything can be somewhat close to one of the points. Generally, the number of points you need this going to scale exponentially with the dimension. A really cool thing about averagers is sort of by their name. They're guaranteed to be a non-expansion, which means that when you combine them with a bellman backup it's guaranteed it'd still be a contraction. So, that means these sort of approximators are guaranteed to converge compared to a lot of other ones. These for things like, um, health care applications and how do you sort of generalized from related patients. So, they can be useful but they generally don't scale so well. What we're gonna talk about today is thinking about deep neural networks which also have very flexible representations but we hope we'll be able to scale a lot better. Um, now, in general we're going to have almost no theoretical guarantees for the rest of the day, and- but in practice they often work really really well. In this example, we're going to use a loss function called j which is like our Q. Then we're gonna push that into another function, and throw in some more weights. I'm gonna do that a whole bunch of times, and then at the very end of that you can output some y. Then, we can output that to some loss function j. These are- happen a lot in unsupervised learning like predicting whether or not something is a cat or not or, you know, an image of a particular object. functions, um, you could represent really complicated functions by adding and subtracting and taking polynomials and all sorts of things you could do by just composing functions together. But the nice reason to write it down like this is that you can do the chain rule to try to do stochastic gradient descent. So, we really want, you know, dj with respect to all these different parameters. We can write down dj of hn and dhn of dwn and we can do- do this kind of everywhere. When I first learned about deep neural networks, you have to do this by hand. One of the major major innovations that's happened over there, you know, roughly what? Like last 5 to 8 years is that there's auto differentiation. So, that now, um, you don't have to derive all of these, uh, gradients by hand instead, you can just write down your network parameter. And then your network of para- which includes a bunch of parameters and then you have software like TensorFlow to do all of the differentiation. sort of hand writing down of what the gradients are. So, what are these h functions? Generally, they combine, um, both linear and nonlinear transformations. If it's nonlinear, we often call this an activation function. So- so, you can choose different combinations, uh, of linear functions or non-linear functions,Um, so- so- you can think of hn is equal to some function hn minus one. Um, basically they just- they have to be differentiable. and as usual we need a loss function at the end. Typically, we use mean squared error. You could also use log likelihood but we need something that- that we can differentiate how close we are achieving that target in order to update our weights. [NOISE] Yeah? Name first. So, this ReLU function is not differentiable, right? It is differentiable. It's ended up being a lot more popular than sigmoid recently, though I feel like it [OVERLAPPING]. Yes. But I don't see how gradient [inaudible] is gonna work on the part where it's flat. Nodes are basically just sort of a sufficiently complicated set of features and functions. This is a universal function approximators which means that you can represent any function with the deep neural network. So, that's really nice. We're not gonna have any capacity problems if we use a sufficiently expressive function approxIMators. Um, they're good to be aware of, um, and we're also happy to give other pointers. But yeah, if it's flat, it's okay, you can still just have a zero derivative. About what we're doing with linear value function approximators, it was clearly the case sometimes that you might have too limited features and you just wouldn't be able to express the true value function for some states. That will not occur for, um, uh, deep neural network if it is sufficiently rich. Another benefit is that potentially you can use exponentially less nodes or parameters compared to using a shallow net which means not as many of those compositions to represent the same function. Then the final thing is that you can learn the parameters using stochastic gradient descent. Convolutional neural networks are used extensively in computer vision. One of our primary sensory modalities is vision, and it's very likely that we're going to want to be able to use similar sorts of input on our- our robots in our artificial agents. So, if you think about this, um, think about there being an image, in this case, of Einstein. Um, and there's a whole bunch of different pixels on Einstein, and then they would be going as input to another. layer and, um, you might want to have a bunch of different nodes that are taking input from all of those and so you can get a huge number of weights. So, you can have lots of sort of functions being computed in parallel. You might have 10 to the 6 weights per- the st- the- often, we think about sort of- I know I haven't given you enough details about this, but often we think of there as being sort of this deep neural network. Convolutional neural networks try to have a particular form of deep neural network that tries to think about the properties of images. So, in particular, images often have structure, um, in the way that our- our brain promises images also has structure and this sort of distinctive structure. Um, if we think about doing this many times and having lots of hidden units, we can get a really an enormous number of parameters. There'd be 10 to the 6 parameters here. That's a lot. And then if we want to do this for doing different types of weights all in parallel, then that's gonna be a very very large number. features in space and frequency. So, when you have a convolutional neural network, we think of there being particular types of operators. Having so operators again here are like our functions, h1 and hn, which I said before could either be linear or nonlinear and then convolved neural network learn a particular structures for those, um, uh, for those functions to try to think about the properties that we might want to be extracting from images. The point of doing this is gonna be trying to extracting features that we think are gonna be useful for either predicting things like whether or not, you know, a face isn't an image. the key idea- one of the key ideas is to say that we're gonna have a filter or a receptive field. At the beginning, that's just gonna be a subset of our image and instead of, um, taking in the whole image, we're just going to take in part. So, it's like we're trying to compute some properties of a particular patch of the image. There's also this thing called zero-padding which is how many zeros to add on each input layer and this determines sort of help determine what your output is. this case, if you have an input of 28 by 28 and you have a little five-by-five patch that you're going to slide over the entire image, then you're gonna end up with a 24 by 24 layer next. So, one thing is instead of having our full x input, we're just gonna take in- we're gonna direct different parts of the x input to different neurons which you can think of just different functions. Um, but the other nice idea here is that we'regoing to have the same weights for everything. So, you can imagine I'm trying to detect whether or not there's something that looks like a horizontal edge in that part of the image and I try to- and that is determined by the weights I'm specifying. So, now the weights are identical, and you're just moving them over the entire image. We think that often, the brain is doing this. It's trying to pick up different sort of features. In fact, a lot of computer vision before deep learning was, um, trying to construct these special sorts of features, things like sift features. captures important properties of the image, but they're also may be invariant to things like translation. This means also that rather than just computing, uh, you- you can do this. You'll use the same weights all the way across the feature, er, all across the image. There's a really nice, um, discussion of this that goes into more depth from 231-n, which some of you guys might've taken. Um, and there's a nice animation where they show, okay, imagine you have your input, you can think of this as an image. then you could apply these different filters on top of it, which you can think of as trying to detect different features. Um, the other really important thing in CNNs, is what are known as pooling layers. They are often used as a way to sort of down-sample the image. So you can do things like max pooling to detect whether or not a particular feature is present, um, or take averages or other ways to kind of just down, ah, and compress the, the information that you got it in. for really high dimensional input and kind of average and slow down until we can get to, um, a low dimensional output. So, the final layer is typically fully connected. We're kind of computing this new feature representation of the image, and at the very end, we can take some fully connected layer, where it's like doing linear regression, and use that to output predictions or scalars. So these type of representations, both Deep Neural Networks and Convolutional Neural Networks, are both used extensively in deep reinforcement learning. How we could use these type of approximations for Atari. So why was the surprising? I just sort of wandering back. In around 1994, personally in 1994, we had TD backgammon which used Deep Neural Networks. Well, they used neural networks. I think there was someone that deep, and I think out like a world-class backgammond player out of that. So, that was pretty early on. And then we had the results that were kind of happening around like 1995 to maybe like 1998, which said that, "Function approximation plus offline off policy control, plus bootstrapping can be bad, can fail to converge" success and then there were these results in sort of the middle of the nineties that indicated that things could be very bad, and the risk was some of the- In addition to the theoretical results, there were sort of these simple test cases, that, you know, these simple cases that went wrong. So, it wasn't just sort of in principle this could happen, uh, but there were cases which failed. And so I think for a long time after that, the, the community was sort of backed away from Deep Neural Networks for a while. DeepMind, DeepMind combined them and had some really amazing successes with Atari. And so I think it sort of really changed the story of how people are perceiving using, um, this sort of complicated function approximation, meters, and RL, and that yes, it can fail to converge. But it is also possible of them that despite that- you know, the fact that we don't always fully understand why they always work, that often in practice, we can still get pretty good policies out. the mid '90s? Or, is it just that kind of through increases in computational power and the ability to gather a lot of data, that when it failed, it kinda doesn't matter, and we can try some different, like we- you know, try it again and kinda put it together and just keep trying until it works? I guess my question is, did we actually overcome any of the problems that arose in the late ' 90s, or is itjust that we're just kinda powered through? The question is how we sort of fundamentally resolve some of the issues of the late my '90's, or, um, we kind of brute forcing it. I think people knew about this when they started going into 2013, 2014. And so they tried to think about, "Well, when might this issue happen, and how could we avoid some of that stuff?" Like, what's causing that? And so at least algorithmically, can we try to make things, that people often talk about stability, so can wetry to make sure that the Deep Neural Network doesn't seem to start having weights that are going off towards infinity and at least empirically have sort of more stable performance. where they changed things to be more on policy or, or which we know can be much more stable. Um, ah, they are doing deep learning in, in this case, Deep-Q Learning. And so it can still be very unstable, but they're gonna do something about how they do with the frequency of updates to the networks. We'll see how it works here. Anyone else? Okay, cool. So, we'll- we'll see an example for breakout shortly, um, of what they did. Dennis and David, both had sort of a joint startup on, um, video games, a long time ago. So, their idea was, we'd really like to be able to use this type of function approximator to do Atari. They picked Atari in part.as well as in our derivative. And whether we're gonna see is, is uh, an alternative to that. Okay. Um, so, they're both interest in this it's clearly, uh, games are often hard for people to learn. I'm so, it's a nice sort of, uh,. demonstration of intellect. In these games, you typically need to have velocity. So, because you need velocity, you need more than just the current image. So what they chose to do is, you'd need to use four previous frames. So this at least allows you to catch for a velocity and position, observe the balls and things like that. It's not always sufficient. Can anybody think of an example where maybe an Atari game, I don't know how many people played Atari. Um, uh, that might not be sufficient for the last four images. visible on screen, it maybe sad, um, and, er, maybe it stored in inventory somewhere. So you have to sort of remember that you have it in order to make the right decision much later or there might be some information you've seen early on. And the reward can be the change in score. Now notice that that can be very helpful or may not be it, depends on the game. So in some games it takes you a really, really long time to get to anywhere where your score can possibly change. are the important things that they, um, did in their paper, this is a nature paper from 2015, is they use the same architecture and hyperparameters across all games. Now just to be clear, they're gonna then learn different Q functions and different policies for each game. But their point was that they didn't have to use totally different architectures, do totally different hyperparameter tuning for every single game separately. It really was the sort of general architecture and setup was sufficient for them to be able to learn to make good decisions for all of the games. approximators act. And the nice thing is that, I think this is actually required by nature. They, they released the source code as well. So you can play around with this. So how did they do it? Well, they're gonna do value function approximators. They're going to minimize the mean squared lost by stochastic gradient descent. Uh, but we know that this can diverge with value function approximation. And what are the two of the problems for this? Well one is that there is this or the correlation between samples which means that if you have s, a, r, s prime, aprime, a prime, r prime, double prime. lot of correlations. Um, and also this issue with non-stationary targets. What does that mean? It means that when you're trying to do your supervised learning and train your value function predictor, um, it's not like you always have the same v pi oracle that's telling you what the true value is. That's changing over time because you are doing Q-learning to try to estimate what that is and your policies changing. So you don't have a stationary target when you are even just trying to fit your function. standard approach, just uses a data point. A data point is really one of these sar, S prime tuples. In the simplest way of TD Learning or Q-learning, you use that once and you throw it away. That's great for data storage, um, it's not so good for performance. So the idea is that we're just gonna store this. Uh, we're gonna keep around some finite buffer of prior experience. We're gonna re-basically redo Q- learning updates. data instead of just using each data point once, you can reuse it and that can be helpful. So, even though we're treating the target as a scalar, the weights will get updated the next round which means our target value changes. This is like saying that periodically I- like let's say I went s1 a1 r1 s2 and then I keep going on and now I'm at like s3 a3 r3 s4. That's really where the data is going. I am in the world, I'm now in state four. It's like I suddenly pretend that I'm back in s1, took a1, got r1, and went to s2 and I'm gonna update my weights again. The reason that that update will be different than before is because I've now updated using my second update and my third update. So, it'll cause a different weight update. In general, one thing we talked about a long time ago is that if you, um, uh, do TD learning to converge it, which means that you go over your data mu- like, like, an infinite amount of time. learn to model, a dynamics model, and then the planning for that which is pretty cool. So, this is getting us closer to that. But we don't wanna do that all the time because there's a computation trade-off and particularly here because we're in games. Um, but we can talk more about that later. Yeah, question and name first please. And so, the experienced replay buffer has like a fixed size. Is those samples, like, replaced by new samples after fixed amount of time? Or is there a specific way to choose what samples to store? you remove items from it. It's a really interesting question. Different people do different things. Normally, it's often the most recent buffer, um, can be for example the last one million samples, which gives you a highlight of how many samples we are gonna be talking about. But you can make different choices and there's interesting questions of what thing that you should kick out. It also depends if your problem is really non-stationary or not, and I want to mean there's, like, the real world is non- stationary, like your customer base is changing. based on the experience replay versus getting, um, putting new samples into there. So, generally right now is really heuristic trade-off. Could certainly imagine trying to optimally figure this out but that also requires computation. This gets us into the really interesting question of metacomputation and metacognition. Um, but if, you know, your agent thinking about how to prioritize its own computation which is a super cool problem. Which is what we solve all the time. use in that value of S prime for several rounds. So, instead of always update- taking whatever the most recent one is, we're just gonna fix it for awhile and that's basically like making this more stable. Because this, in general, is an approximation of the oracle of V star. What this is saying is, don't do that, keep the weights fixed that used to compute VS prime for a little while. Um, one is gonna be this weight and the other is this weight. minus. I'll call it minus because, um, well there might be other conventions but in particular it's the older set of weights, the ones we're not updating right now. Those are the ones that we're using them as target calculation. So, when we compute our target value we, again, can sample and experience tuple from the dataset from our experience replay buffer, compute the target value using our w minus, and then we use stochastic gradient descent to update the network weights. help, um, in terms of stability? In terms of Stability, it helps because you're basically reducing the noise in your target. If you kept your target fixed forever, you would learn the weights that- that minimize the error to a constant function. That would then be stable because you always have the same target value that you're always trying to predict. And eventually you'd learn that, and that would eventually be stable. And that's what we're trying to do with GT. This is just reducing the noise and the target that we're trying to sort of, um, if you think of this as a supervised learning problem, we have an input x and output y. The challenge in RL is that our y is changing. If you make it that you're- so your y is not changing, it's much easier to fit. Uh, assuming we want to do [inaudible] approximator. Is there something that's specific to the deep neural networks? This is not specific. This is really just about stability and that's- that's true for the experience replay too. Experience replay is just kinda propagate information more- more effectively. Uh, so these aren't sort of unique using deep neural network. I think they were just more worried about the stability with these really complicated function approximators. Yeah, in the red. Do you every update the- Minus at all, or is that [inaudible]. Great question. So, the- Di- Dell? Dian. Dian's question is whether or not, um, we ever update w minus, yes we do. These sort of Q learning are not true gradient descent methods. They're- they're are approximations to such, they often do shockingly well given that. There's often particular patterns or- or hyper- it's a hyperparameter choice of how quickly and how frequently you update this. Um, and it will trade-off between propagating and updating the gradient. This is hopefully gonna help but we have no guarantees. Yeah? Uh, George, uh, so in practice, do people have some cyclical pattern and how can they refresh the- the gradient that's used to compute the gradients? information fester, um, and possibly being less stable. If n is infinity, that means you've never updated it. There's a- there's a smooth continuum there. We notice, like, for w, there are better initializations than just like zero, uh, something, if you take into account, I guess like the mean and variance. Uh, would you initialize w minus just two w or is there like an even better initialization for w minus? Yeah, his questions is about, you know, the- the impact of how we, um,. uh, initialize w ca- can matter. It stores the transition in this sort of replay buffer, a replay memory, um, use sample random mini-batches from D. So, normally sample in mini-batch instead of a single one. You do your gradient descent given those. Um, you compute Q learning using these old targets and you optimize the mean squared error between the Q network and Q learning targets, use stochastic gradient descent, and something I did not mention on here is that we're typically doing E-greedy exploration. of what the agent is doing. So, remember the agent's just learning from pixels here how to do this. As it gets more and more data, it learns to make better decisions. But one of the things people like about this a lot is that, uh, you can learn to exploit, um, the reward function. Uh, so in this case, it figures out that if you really just want me to maximize the expected reward, what the best thing for me to do is to just kind of get a hole through there and then as soon as I can start to just bounce around the top. the reward, it'll- it'll learn the right way to maximize the reward given enough data. Um, and so this is really cool that sort of it could discover things that maybe are strategies that people take a little while to learn when they're first learning the game as well. So, when they did this, they then showed, um, some pretty amazing performance on a lot of different games. Many games they could do as well as humans. Now, to be precise here- oh yeah I'm sorry. Uh, I'm just wondering why, um,. why was it [inaudible] around a lot, like, it wasn't sure of its movements. clearly sort of an inexperienced player to do that. That would be a strange thing but from the agent's perspective, that's completely reasonable. Um, and it does not give him positive or negative reward from that. So, it can't distinguish between, you know, stay in stationary versus going left or right. If you put it in a cost for movement that could help. This might become a little bit of [inaudible] but is there a reason to introduce a pulling layer? Puling layer? There might be one in there. uh, they're not talking about how long it took them or their agent to learn and as you guys will find out for homework two, it can be a lot of experience. So, they did very well on some domains. Some domains, they doing very poorly. Um, I think that it's clear that the really important feature is replay. We'll probably talk- uh, we'll talk a lot more about exploration later on in the course. so, what was critical? So, I- I like the, uh, there's a lot  lovely things about this paper and one of the really nice things is that they did a nice ablation study.  replay is hugely important and it just gives us a much better way to use the data. Using that fixed Q here means you seem like a fixed target. You do replay and suddenly you're at 241. Okay, so throwing away each data point what- after you use it once is not a very good thing to do. Um, and then if you combine replay and fixed Q you do get an improvement over that but, uh, it's really that you get this huge increase, um, at least in break out in some of the other games. So, the question is like, "Well, maybe we could use, like, linear-" also I guess I should be clear. So, we've done some work, um, using a Bayesian last layer, using like Bayesian linear regression which is useful for uncertainty. But you could certainly imagine trying linear plus replay and it seems like you might do very well here, it might depend on which features you're using. There's some cool work over the last few years looking also at, uh, whether you can combine these two. a talk about reinforcement learning, and like 40 people would show up, but most of them you knew, and, um, and then, uh, then it started really changing. I think I was maybe in 2016, when, er, ICML, I was in New York and like suddenly there were 400 people in the room for reinforcement learning talks. And then, this year at NLP's which is one of the major machine-learning conferences, it's sold out in like eight minutes. So, there's been a huge amount of excitement based on this work. double DQN is kind of like double Q learning, which we covered very briefly at the end of a couple of classes ago. The thing that we discussed there was this sort of maximization bias, is that, um, the max of estimated state action values can be a biased estimator of the true max. This is to try to separate how we pick our action versus our estimate of the value of that action. It turns out that it gives you a huge benefit in many, many cases for the Atari games. back to the Mars Rover example. Um, er, so, in Mars Rover we had this really small domain, we are talking about tabular setting through just seven states, um, and we're talking about a policy that just always took action a1 which turned out to mostly go left. So, it was this. And the first visit Monte Carlo estimate of v for every state was 1110000, and the TD estimate with alpha equal one is this. That was when we talked about the fact that TD only uses each data point once and it didn't propagate the information back. So, let's say you get to choose two replay backups to do. Maybe it doesn't matter if you can just pick any of these, you're going to get the same value function no matter what you do. So, are there two- two updates that are particularly good, and if so, why and what order would you do them in? [NOISE] Hopefully you had a chance to think about that for a second. First of all, does it matter? So, I'm going to first ask you to pick the same one twice. guys, uh, the question. Vote if you think it matters which ones you pick, in terms of the value function you get out. That's right. So, it absolutely matters which two you pick. You will not get the same value function no matter which two. Now as for another voting, I will ask for which one we should do first? Should we update- should we do four first? Four is the last one on our replay buffer. Should we do three first?should we do two first? you get to backup that information. So, um, if you wanted to get all the way to the Monte Carlo estimate. What you would wanna do here, is you'd wanna do S3, a1, 0, S2 which would allow your V of S3 to be updated to one. So it definitely matters. It matters the order in which you did, do it. And that's the same as the last time I [inaudible] That's right. Yes. have changed. Um, so ordering can make a big difference. Uh, so not only do we wanna think about like, what, um, was being brought up before but I think to say like what should we be putting in our replay buffer. What order should be in a replay buffer but also what order do we sampled them can makea big difference in terms of convergence rates. There's some really cool work from a couple of years ago looking at this formally of like how, at what the ordering, matters. The number, of, um, updates you need to do until your value function converges to the right thing can be exponentially smaller, if you update carefully and you, you could have an oracle tells you exactly what tuple the sample. Which is super cool. But you can't do that. You're not gonna spend all this. It's very computationally, expensive or impossible in some cases to figure out exactly what that uh, that oracle ordering should be. Um, but it does illustrate that we, we might wanna be careful about the order that we do it. In the old ways and uh, example we're just going through, after you like propagated the one back once, you wouldn't be able to do anymore because your value's totally zero. So there's gonna be this tension between, when you fix, um, uh, your w minus, then, if you were looking at our case that we had before, then you would be able. to continue propagating that back, because you wouldnâ€™t update yet, yet, that's exactly right. things versus her propagating information back. So why does it, what does ordering matter, that if you're fixing, and so you are not changing, uh, like, then it wouldn't matter what order we sampled those previous ones, right? Uh, okay. So basically, ordering matter at all, in that case. It still matters because we're still gonna be doing replay, o- over. So that buffer could be like, million and you might re-update your weights like every 50 steps or something like that. among the existing tuples? So out- so Pi is our, uh, sort of basically our DQN error. If we set Alpha equal to zero, you know, it's right. Yeah. So, so this sort of trades off between uniform, no prioritization to completely picking the one that, um, like if alpha's infinity then that's gonna be picked the one with the highest DQn error. So it's a trade-off. Most, the time prioritize replay is better and there's some hyper parameters here to play with. One of the best papers from ICML 2016 was dueling.do, short through this just so you're aware of it. Um, the idea is that, if you want to, make decisions in the world, they're working some states that are better or worse, um, and they're just gonna have higher value or lower value. What you really wanna be able to do is, figure out what the right action is to do, in a particular state. So I'm looking at this advantage function. A2 minus V of s.a2 minus S.a1 is called prioritize replay. It's an architectural choice and learning a recombine these for the Q. Empirically, it's often super helpful. So, again compared to double DQN with prioritize replay, which we just saw, which was already better than w- double D QN, which is also better than DQn. Um, this again gives you another performance gain, substantial one. So basically these are sort of threes, three different ones that came up within the- for two years after DQ n that started making some really big big performance gains. It could be super tempting to try to start, by like implementing Q learning directly on the ATARI. Highly encourage you to first go through, sort of the order of the assignment and like, do the linear case, make sure your Q learning is totally working. Even with the smaller games, like Pong which we're working on, um, it is enormously time consuming. It's way better to make sure that you know your Q Learning method is working, before you wait, 12 hours to see. whether or not, oh it didn't learn anything on Pogge. So, that, that- there's a reason for why we, sort of build up, the way we do in the assignment. Um, another practical, to a few other practical tips, feel free to, to look at those, um, and then we were on Thursday. Thanks. Back to Mail Online home.back to the page you came from. Back from the page where you come from. back to MailOnline home.