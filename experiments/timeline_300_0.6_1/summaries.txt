==================== [1/100] ====================
Summary:
John ESSIGMANN: I measure my blood sugar at different times during the day. Gluconeogenesis technically means new synthesis of glucose from non-carbohydrate precursors. The medicine I take is called Metformin. It has a number of targets, but one of them is one of the enzymes, called PEPCK, Pyruvate Carboxykinase, that's in the gluconeogenic pathway. The liver provides a constant stream of glucose to organs that absolutely require it, like our brain.

ROUGE-1: 28.97, ROUGE-2: 26.69, ROUGE-L: 28.97
BERTScore: 70.94

==============================================
==================== [2/100] ====================
Summary:
In this lecture, we introduce and develop the concept of independence between events. If I tell you that a certain event A has occurred, this will generally change the probability of some other event B. In such a case, we say that events A and B are independent. We will then proceed to define the independence of a collection of more than two events. Finally, we will close with an application in reliability analysis and with a nice puzzle that will serve as a word of caution about putting together probabilistic models.

ROUGE-1: 62.59, ROUGE-2: 60.96, ROUGE-L: 62.59
BERTScore: 82.29

==============================================
==================== [3/100] ====================
Summary:
In today's video i'll show you the importance of de-gassing your bread dough as it's fermenting. No matter how gentle you try to handle your dough you will always de-gas it if only a little bit in today's comparison video we'll make 4 breads they will be made from the same dough but they will all be treated differently. The first one of the four breads will be left alone from the beginning of fermentation until it's baked. The final one will be folded shaped and degassed three times and we won't be fermenting them for the same amount of time.

ROUGE-1: 11.35, ROUGE-2: 11.05, ROUGE-L: 11.35
BERTScore: 70.42

==============================================
==================== [4/100] ====================
Summary:
Of the nearly 11,000 amendments proposed in the centuries since, only 27 have succeeded as of 2016. The founders of the United States were trying to create a unified country from thirteen different colonies. For an amendment to even be proposed, it must receive a two-thirds vote of approval in both houses of Congress. To actually change the Constitution, the amendment must be ratified by three-quarters of all states. The result of such high thresholds is that, today, the American Constitution is quite static.

ROUGE-1: 24.96, ROUGE-2: 23.60, ROUGE-L: 24.96
BERTScore: 67.92

==============================================
==================== [5/100] ====================
Summary:
50 years ago, John McCarthy and Marvin Minsky coined the term artificial intelligence. The idea of merging brain research and computer science in the quest to understand intelligence. This was the people that we put together from different labs, from neuroscience, from computer science, from cognitive science, and from a number of institutions in the US. Let me tell you a bit more about the background here. Part of the reason for this was progress and convergence we saw between different disciplines. And one of them was progress in AI. And this started, really, with Deep Blue, I guess it was called. I think it's a golden age for intelligent applications. We are still very far from understanding how people can answer questions about images. It's not enough to pass the Turing Test. We want to have a system that does it in the same way as our brain does it. And we want to compare your model, our system, with measurements on the brain of people, or monkeys, also during the same task. So we are not yet at the point in which we can answer all those kind of questions at all these different levels. But some, we are.

ROUGE-1: 28.94, ROUGE-2: 27.41, ROUGE-L: 27.80
BERTScore: 65.95

==============================================
==================== [6/100] ====================
Summary:
Chef Todd Moore shares with you the seven basic skills that I think everyone should have to cook food consistently in the kitchen and be proud of the results. The chef test highlights the skills that everyone should possess if they want to learn to cook anything at any time and be confident that it'll always come out great. If you already have all seven of these skills and cooking techniques great you can work for me on the other hand if you only have one or two ofthese skills that's still fantastic. skill down when I can give someone three different oils types of oils put them in a pan and they can tell me where the variable smoke points are so you pass this test when the chef notices the oil starting to change from being perfectly smooth to beginning a convection process then adds the protein product to the pan just before there's visible smoke. Brown foods are attractive F Foods this is what attracts the eye and to develop golden color in the sauté pan you have to get the sugars in your product to caramelize at 320° fah. one that's lost moisture one that's burned one that shows the lack of involvement in the preliminary steps of the sauté process Chef test number four here's the answer thicken a liquid to make a sauce. Rue flour and cornstarch are fantastic thickening agents but you need to have an understanding of how much to use and this can only happen with controlling the process of gelatinization of starches Chef that can show me that they pass they take a cup of milk they turn it into a thickened sauce that's shiny that's velvety. control heat in a moist convective process is an important Chef's skill number six was to roast something delicate like fish. There's a very fine line between the coagulation of proteins the stiffening and shrinking of what you're cooking at 165° F and the 212° fah when moisture starts evaporating. The key to cooking with dry heat is being able to live in this temperature zone between 165 and 212 where the food Cooks for drying out so you pass the chef test if you give me a nice piece of flounder or tilapia even Cod or Whitefish catfish anything that's fully cooked and it retains its moisture without drying out. and it's dry it's much smaller in its uh cooked State than it was in its raw State because of the drying effect of the oven this Chef doesn't know how to retain moisture in a dry cooking process Chef test number seven how to tell when a grilled steak is done. If you can Master this you are a master griller most people just burn stuff you fail this test if you give me three St Stakes all cooked to the same temperature or if you can't tell which steak is rare medium well done if you're taking them off the grill guest atam the chef that can that can't control direct to Source conductive heat would create waste in my restaurant waste over sales for Stakes being sent back to the kitchen. I don't want this this is a chef's level skill so this is what's going on in my kitchen. If you want to cook great food more consistently you'll want to pay attention to the cooking techniques and have repeatable methods. These are the same methods I'm going to reveal in my webinar it's discover the three secrets to making your cooking a winner every single time.

ROUGE-1: 41.69, ROUGE-2: 40.64, ROUGE-L: 40.35
BERTScore: 61.33

==============================================
==================== [7/100] ====================
Summary:
as a nurse you want to be familiar with heart blocks and in this review i'm going to be talking about third degree heart blocks also known as a complete heart block. The reason is occurring is because electrical signal from the atria isn't making it to the ventricles. The person could be born with it so it could be congenital or the person has severe heart disease or they have a myocardial infarction or they're taking some type of medication that they become toxic on like digoxin. block and if you'd like to watch more videos about heart blocks in this series you can access the link in the youtube description below. Click here to read the first video in the series about a heart block. Read the second video to learn more about the heart block and how to block it. Click the third video to see the next video about the block and learn about the different types of block that can be used to block a person's heart. The third video is the final video of the series and is about the blocking of a heart valve.

ROUGE-1: 39.81, ROUGE-2: 29.93, ROUGE-L: 27.96
BERTScore: 64.02

==============================================
==================== [8/100] ====================
Summary:
The final contest, which is due tonight, is to design an agent that plays together with another agent to try to collect food pellets while not getting eaten by ghosts. Submissions for that, your last chance to submit are tonight at midnight. And on Thursday in lecture, we'll discuss the results. The idea behind these two lectures is to look at advanced applications. We will not quiz you on these application lectures, on the final exam, or anything like that. It's more meant to give you more perspective rather than extra study materials. Search is very prevalent in language processing, and we'll cover some of that on Thursday. Adversarial games. Learning to acquire behaviors. We'll look at some real robot behaviors in flight and in legged locomotion. And dealing with uncertainty. So AlphaGo. Today's state-of-the-art in Go is that there are computer players better than the best human players. But actually, if you went back to March 2016, that was not the case yet. So how do you make an AI for Go? DeepMind's AlphaGo system can predict who will win a game of Go. It uses a neural network to evaluate the value of a position. This can be used to decide who is likely to win from a given situation. AlphaGo can also use Monte Carlo rollouts to predict the outcome of a game. It can then use this data to make decisions about which moves to consider. The system is currently being trialled in a beta version of the game against human players. It is expected to be released to the public later this year. AlphaGo Zero has no prior knowledge. It starts at a pretty, actually, negative elo rating. Loses all the time initially. It's playing itself, but then gets tested against a set of players. After 21 days, it goes past where AlphaGo Lee Sedol was. And then it was still creeping up after 40 days. Once you reach that level, essentially, there's no further to go, because you solved the game. With reasonable compute power, it traverses the whole tree, even with alpha beta pruning. CNN's John Defterios: How do you get a helicopter to do something like this? He says it's hard to stabilize a helicopter when you only know where you are up to a couple meters. Defterio: We set up a hidden Markov model, where we considered the state unknown. He says we can build a model for learning a bunch of parameters that predict the next state, and solve it at this point in the learning.to until this experiment was run. To learn more, visit CNN iReport. saw it making these wild motions, overcompensating. It pushed the controls so hard that the engine died. The engine just couldn't push it, died. At that moment, the blades stopped spinning, or they slowed down. Then what happened is our human pilot took back control to try to save the helicopter. And believe it or not, they actually saved this. It landed a little harder than you want to land, but it landed on its feet and it could be recovered from that, which was pretty crazy. Hertz: When we collect paths from a human pilot, they tend to be noisy. Hertz: We could learn the trajectory from these as noisy observations. Hidden Markov models. We can penalize our helicopter for deviating from the data. Herts: We can run reinforcement learning in simulation to find a good helicopter controller and run it on the real helicopter. That point, was much better than our autonomous controller. That's why you might say, only 20 seconds of controls? We're looking at the helicopter, not the controls. The helicopter uses roughly a fixed amount of fuel, anyway, per time. So it's more that it has less weight to carry as it has used more fuel. This helicopter had inverted slide, where it has more power, 3 Gs. It can generate three times the power of gravity. OK, let's take a short break here. And after the break,. let's do legged locomotion and manipulation. All right, let't restart. Any questions about the first half? Yes? is a separate linear feedback controller for each time slice. So essentially, value iteration, but in a continuous space. If there is no wind, you can actually just run the linear feedback control. It will be fine. But if there's some wind gusts that could throw you off, you want to use the value functions and the two second look ahead. If you have a locally linear dynamics and quadratic reward function, it turns out that's the one continuous state action space scenario where you can run exact value iteration. against those value functions to do the controls. Can it be done with one unified network? Likely, it can. It might take some work, exactly, figuring out how to do it. Here's an example of how hard this can be. This is a video from 2015, there was the Doppler Robotics Challenge, which was held in Pomona, just east of Los Angeles. It's called the "Doppler Challenge" and it's a competition for roboticists. It was won by a team from the University of California, San Diego. of Los Angeles. People had two years to work on this. And what did the robot have to do? It had to, essentially, drive a car or walk, but driving the car was recommended. Then get out of the car, walk a little bit, open a door, grab a drill, drill a hole, walk some more. So doesn't sound that complicated. But actually, it turned it's very complex to get a robot to do that. And it's indicative of how hard it is to do walking with robots. build a model of the world. They would have sensors. Based on that model, you could simulate the world, predict what you should do, not unlike what we did with the helicopter. We learned models of how helicopter dynamics works. Think about what will happen as a function of which actions we take, and maybe have a value function, and then take the actions that lead to the good outcome. The thing is modeling these situations proved even harder than modeling helicopters, because your sensing needs to understand whether or not you're already making contact, and making contact or not. DARPA has been working on autonomous driving since 2005. The goal is to have a car that can drive itself in a desert race. The team has developed a way to teach a robot to find a path across rocky terrain. The robot can learn the weights and the features of the terrain to find the best path. It can then plan on that new terrain with that reward function and see how well it works, says Dr. Michael Bociurkiw, head of DARPA's autonomous driving program. Stanford robot becomes first vehicle in history to drive 132 miles by itself. Stanley drives for six hours and 53 minutes at an average speed of 90 miles an hour. To finish, they must wind through a treacherous mountain pass. Stanley leads the pack, and just five robots remain on the course. The first time it's ever been done, autonomous vehicles. In 2005, four cars finished the 150 miles. That's a Berkeley entry. Only motorcycle in the race. Yeah, you were [? not to take a seat in the autonomous cars of 2005. With a camera, you can often look further ahead. LIDAR sends out a laser beam, measures how long it takes to get back. With HMM, you get 0.02% false positives of where there might be obstacles. An HMM can de-noise those readings into a more reliable estimate of the geometry of the world around you. It would see that the readings are different and decide it needs to steer around that.an obstacle. If you're an urban environment, there'll be a lot more obstacles. A camera will be better at that than a LIDar. Self-supervision is a trick that's very widely used to reduce labeling efforts. So the camera now knows all the red here is road. In urban environments, there's even more need to recognize, not just road versus not road. A lot of progress has been made this is video from 2013. This was before deep neural networks were heavily used for this kind of thing. It's only getting better to recognize what's in scenes, thanks toDeep neural networks. Instead of classifying into which categories in the image, you would classify each pixel, as to what is in each pixel. tail of special events that can happen when you're driving. You can measure progress by just demo videos, which is one way, and it gives you some kind of feel for what's going on. Another way to measure progress is to see how are these cars doing relative to human drivers. If you test in California, you have to report this data to the DMV to see if your car is doing well or not. It's a number of events per 1,000 miles driven. Red there is human fatalities. Then yellow is human injuries. In green is the Google slash [? wave ?] mode disengagement. doing this. Early on, they were the only ones, as far as I know, but there's more companies now. And you can look at them on these plots and see which companies, how far along, in terms of how many disengagements they need per 1,000 miles driven. Still, so far, quite far removed from human accident rate levels. Another thing people have been pushing, as a consequence of all this, is lower power neural networks. So for example, Kurt Kuetzer at Berkeley has projects on this called SqueezeNet. If they're gigantic, use a lot of power. That's a problem. Let's see what we can do to build smaller networks to make decisions.

ROUGE-1: 24.95, ROUGE-2: 22.33, ROUGE-L: 21.62
BERTScore: 66.33

==============================================
==================== [9/100] ====================
Summary:
Price of good 1 has gone up substitution, income these are the effects and overall. When P 1 goes up subs because of substitution effect x 1 will. Come down. And for income also because of income effect it will come down. While inferior goods substitution effect quantity demanded would come down while income effect is. Up. So, theoretically speaking what is happening? The price of good1 is increasing and if these two are true then the quantity demanded ofgood 1 is increasing such good are called Giffen good. Can you give me an example its very very difficult to find Giffan good in real life why?

ROUGE-1: 21.14, ROUGE-2: 20.37, ROUGE-L: 21.14
BERTScore: 74.68

==============================================
==================== [10/100] ====================
Summary:
which we are to for next week and here and I'll talk about that actually week from today because it's callay on Monday so it will get behind but if we get any further behind I'll have to make it up sometime the but if you don't have that you be copies available upstairs is it c um come to my office okay to dany's office he doesn't have copies of it there that's six six upstairs in the first floor immediately after CL and then on the next Friday I will talk about the parts in the in the inquiry that were assigned and also a that essay of of the oral concept. The English Constitution was at that time widely regarded this a sort of accepted view on all sides it's a hex Constitution that this problem as I say arose during the exclusion crisis of 1679 and 81. Now by definition a mixed Constitution is a constitution in which two or more constitutionally defined agents share in and each have a part in What L calls the legislative power that is the supreme power in the constitution and in the English case of course it's the crown and Parliament so we're only actually dealing with two agents in this case and neither is supreme so we want to say that they are coordinate powers. changed his mind he thought perhaps it was not all together safe never knew in England the English were regarded in those days as very unstable people uh and not now we think of them as how and settled down or any rate 200 years after that they we regard as a model and longer of stability and and political common sense. Mark was afraid well who knows uh may be different I don't want people know what I said in the past it might be unsafe. His view was that Charles II then had violated his trust and it altered the form of the Constitution and therefore the government was dissolved. and who does not have the right to vote and what the conditions on it are maybe we want to free it from that of course what I'm thinking of eventually um is that that that would be one motivation for introducing an idea like the origal position that say it's a way of conceiving how con power might be exercised. No point in criticizing someone for something they didn't intend to do okay well I think it's stop so remember there's no class here Monday so the next class here will be next Friday.

ROUGE-1: 13.30, ROUGE-2: 13.01, ROUGE-L: 13.24
BERTScore: 67.79

==============================================
==================== [11/100] ====================
Summary:
David Kaiser: particle cosmology is a new subfield within physics. He says it studies the smallest units of matter, the fundamental forces and elementary constituents of matter. Kaiser: The field is doing pretty well these days by other measures. Its annual budget just within the U.S. is on the order of $1 billion a year, roughly, he says. It is really a booming, booming subject of study, Kaiser says. The field literally didn't even exist 45 years ago, he adds. and '60s. There was quite a different set of conversations happening around the same time. This had nothing to do, at least on the surface, with Mach's principle or even with Einstein's general theory of relativity. But the challenge became very clear very quickly. These nuclear forces are somehow mediated was what arose from particles exchanging certain kinds of force-carrying particles. The point is there were new kinds of matter. And then when particles tossed these back and forth, that would give rise to things like radioactive decay. Self-evidently of short range, unlike gravitation or electromagnetism, the nuclear forces really exert themselves across nuclear dimensions, very tiny fraction of the size even of a single atom. The idea was could have finite range nuclear forces if these force-carrying particles had a very large mass. That will make it very unlikely for that force to be felt across a large distance because of the whole set of ideas about virtual particles and the uncertainty principle. So that broke half of the motivation for it. Carl Brans and Robert Dicke proposed the Brans-Dicke theory of gravity in 1961. It was an attempt to modify Einstein's theory of general relativity in response to this challenge of Mach's principle. In Einstein's version, you can represent Einstein's general theory of relativity by an action, in a sense, by writing down a Lagrangian. It's the geometer's tool of quantifying the warping of space and the curvature scalar of time. The idea was that instead of having a single constant unit strength of gravity labeled by Newton's constant capital G-- that's the G that's in like Newton's force law. Their version would depart from the ordinary behavior from general relativity. As omega becomes larger and larger, the field is much less likely to vary either over space or time. And in the limit that omega becomes arbitrarily large, then phi, in a sense, can't afford to vary at all. The kinetic energy cost is too high. So you get back to the Einstein-like limit. So they had this very clever fudge factor, a coupling constant, so that, in principle, the local strength of gravity could be changing all the time. As omega becomes large, the behavior reduces to the kind of constant strength of gravity of the original Einstein theory. The idea is that this new form of matter, this new field phi, is extended throughout all of the universe through every nook and cranny of space. All of matter interacts with phi. So this would be a way to incorporate Mach's principle going beyond even just Einstein's version of the theory. It's almost like a new ether, you might say, It's everywhere. Jeffrey Goldstone was among the first to show that a certain kind of characteristic shape for a potential energy function could be used to break the symmetry of the nuclear forces. Goldstone introduced a new hypothetical form of matter, the Higgs field, which is responsible for giving everything else the masses that we measure, including those force-carrying particles. If this new field gets stuck in its local minima, it now acts like it has a molasses-like effect on the universe. An induced mass coming from this spontaneous symmetry breaking. So now, again, you can have your symmetries and your short range. So fascinating, a lovely idea being introduced right around the same time as Brans-Dicke also as one way to try to get to this question of why do objects have mass. It got a lot of attention, as I'll say more about in a moment, because it offered the first really concrete quantitative alternative to Einstein's general theory of relativity in nearly 50 years. No one suggested, at least in print, that these two scalar fields with the same Greek letter might be similar or even worth considering side by side for nearly 20 years, until the later 1970s. The sets of ideas were published to very wide acclaim in their own separate fields as early as 1961. It took more than 15 years until people began to say, hey, these two Scalar fields are meant to pervade all of nature. All of other forms of matter interact with them. They give rise to what we measure as local inertial effects. Each of these papers-- the Brans-Dicke paper and the Higgs papers-- became technically renowned within fewer than 20 years. So you can see more than 500 each. In fact, it's 1,083 distinct papers doing the citing if you add up all the ones between these two plots. And yet, only six of those-- so less than 1%-- cited both the BrANS-DICke paper. and theHiggs papers in the same article during this whole 20-year period. on these two fields being literally the same field-- only one new field of nature, not two. So it's not that they're somehow intrinsically totally separate or different from each other. So instead, their status, really, is historical. How people assess them or what they thought they were good for was changing over time. And so what had changed? That's what we'll pick up in the next part. Let me pause here and ask for any questions. So Alex asks, was it an accident that both parties chose phi? Not really. more than just the letter that they chose. There was a lot of what we might have considered similarities. And yet, the two sets of ideas really were treated so separately. So we might wonder, well, was it changes in data? Did experiments force a new evaluation? No, not really. Let's look at the gravity side first. So I mentioned that part of what got the gravitation community so excited about Brans-Dicke gravity was it now gave them something very tangible, very specific to try to test for. By the end of the '70s, things did not look very good for Brans-Dicke gravity experimentally. That is, all the tests were easily consistent with the predictions from Einstein's theory within experimental errors. To match all of these increasingly precise new experimental measurements, Brans' Dicke theory was not highly favored. So that's on the gravity side. Meanwhile, there was literally zero experimental evidence, a big fat goose egg nothing in favor of the Higgs-Boson until July of 2012. two sets of ideas consider them side by side. The main story that's mostly given-- I alluded to this in the very beginning of today's class-- is actually hearkens to changes in ideas and, in particular, on the particle physics or particle theory side. These ideas are well worth appreciating. And they came in rapid fire in 1973 and 1974. The first of them is called asymptotic freedom. And actually, it's the reason why our friend and colleague here at MIT, Frank Wilczek, received the Nobel Prize. vast majority of those came really in the later '70s, in the wake of these pedagogical reforms. So remember that big report comes out in 1972. You start seeing curricular changes as early as '73, '74. By '75, '76, '77, you start seeing, in some sense, the market respond with many textbooks being really rushed into print. Some of these textbooks were basically mimeographed lecture notes. And now there's very, very fancy books published in a more typical way. GUTs, or GUTHs-- I'll throw him a bone-- are the symmetry groups associated with the different kinds of forces. Each of those three symmetry groups are actually sub-- you think about it. You can represent any of them as matrices. And there is the smallest group that includes those three as subgroups is an SU5. So that led to new phenomena that were predicted that turned out not to be measured, but that was the idea at least. And that's super cool and fun and lots more to be said about that but that is, indeed, where that nomenclature came from. In 1979, two separate theorists introduced a whole new model where they didn't only cite Brans-Dicke and Higgs, they literally united them. They wanted to ask why gravity appears to be so weak compared to all the other forces. The idea was that this local strength of gravity, Newton's constant, would get anchored to a very small value when phi gets stuck at a relatively large value. So why is gravity so weak? They suggested maybe it's because it's arising from some broken symmetry. He wandered into this field really accidentally. He finished his PhD at Harvard in 1970. His work was squarely in particle physics, the fancy new symmetries and new nuclear forces. That's what he studied for his thesis. He then had a sabbatical early on in his faculty career, and he happened to go to Paris. He swapped apartments with a Parisian physicist who was about to come to the United States. And as Tony recalls, he found these stacks of preprints all around the apartment that looked interesting. Lee Smolin was the other person who independently introduced that broken-symmetric theory in 1979-- same year as Tony Zee. He was actually, from the start, combining the two fields, both in the courses he took and eventually with his advising team for his thesis and for his dissertation itself. So unlike this accident of trading apartments in Paris and reading a few preprints, more and more members of Lee Smolin's generation were going through a training like his, partly by design, in the wake of the National Academies report. Alan Guth: Few physicists today think Brans-Dicke theory of gravity best describes our Universe. He says interest in the field grew even as it was getting experimentally less and less favored. Guth: New generation of theorists trained to work at this interface. They had all kinds of potential roles to play in new work that we'll look at together next time on inflation, he says. "We'll have more opportunities to make fun of Alan Guth, which is what's in it for me" counts as natural can shift in a pretty short time scale. And as we've been seeing throughout the whole term and including today, those shifts can be driven as much by things well outside of the physicist's control, geopolitics, and national scale budgets. So that's where I'll actually pause. And we have time for a few more questions or comments. Any other thoughts on that? With my last moment here, I'll say, before the construction for the new Center for Theoretical Physics, I had an office just down the hall from Alan's. And by a quirk of the old building 6, we had the same key. soon, everyone. Soon, everyone will be able to play together again. Soon. Everyone will be playing together.soon. Everyone. Will be. Playing together again soon.soon,everyone. Everyone, playing together againsoon.soon,. everyone. soon, everyone, will be. playing together once again. soon.everyone. will be Playing Together again.soon! everyone. will. be.playing together soon. soon! Everyone. will play together soon! everyone will.be. playing again soon! soon.

ROUGE-1: 24.85, ROUGE-2: 23.13, ROUGE-L: 23.37
BERTScore: 57.39

==============================================
==================== [12/100] ====================
Summary:
Frida Ghitis: Ferdinand Magellan may have been the first person to actually circumnavigate the globe. She says Spain and Portugal had their eyes on the same prize: trade routes to the Spice Islands. When a Portuguese defector claimed that a westward route existed, King Charles made him captain of a Spanish armada, she says. Ghitis says Magellan's legacy lingers: galaxies and space programs named after him, and he was celebrated in Spain. of the "Victoria" sailed into harbor in southern Spain in September 1522.

ROUGE-1: 17.48, ROUGE-2: 13.88, ROUGE-L: 12.94
BERTScore: 60.28

==============================================
==================== [13/100] ====================
Summary:
Ani was a real person, a scribe from the Egyptian city of Thebes who lived in the 13th century BCE. His Book of the Dead was a 78-foot papyrus scroll designed to help him attain immortality. Ani's epic journey begins with his death. His body is mummified by a team of priests who remove every organ except the heart, the seat of emotion, memory, and intelligence. It's then stuffed with a salt called natron and wrapped in resin-soaked linen. The wrappings are woven with charms for protection and topped with a heart scarab amulet. can imagine him happily tending his crops for all eternity. Can't imagine him being happier than when he was tending to his crops. Can imagine him growing his crops all day and all night. Couldn't imagine a better way to spend the rest of his life than in the fields. Can picture him growing crops all night and all day long. Could imagine him tending his crop all day, all night, all day. Could he be happier than he was right now? Could he ever be happier?

ROUGE-1: 34.15, ROUGE-2: 25.37, ROUGE-L: 29.48
BERTScore: 63.27

==============================================
==================== [14/100] ====================
Summary:
JUDY HOYT: We're going to begin this lecture on handout number 14. We'll be moving now to chapter 7. This will be our first lecture on chapter 7 on the topic of dopant diffusion and profile measurement. The placement of those regions determines many of the so-called short channel characteristics of MOSFETs that we'll talk about. And finally, the doping of other materials, not just the silicon itself, but of the polysilicon gate affects things like things like point defects in silicon. gate depletion and limits how well the gate voltage controls the channel potential. So we really need to understand the placement of these atoms. Let's go on to slide number 5. Here, again, I'm picturing that same MOSFET structure with the resistance of the different regions. And in general, as a rule of thumb, of course, we'd like the resistance. of the regions that are extrinsic to the device, such as. the contact resistance, the source and drain resistance, and the. resistance of these extension regions, hopefully should be no more than 10% of the channel resistance. detailed device physics, but it's just to give you a flavor for why studying dopant diffusion is such an important topic. So let's go on to slide number 8 and talk about a topic called the short-channel effect. This basically takes place when the distance between the source and drain-- that is the channel length L-- becomes comparable to the MOS depletion width in the vertical direction. And then that the source-drain potentials themselves from theSource and drain regions end up having a strong effect on the control of the current in the device. In silicon IC processing, there are two different steps that we refer to in diffusion historically. The first step was so-called predeposition. And what this refers to is that you had an initial step in which the dopants were introduced into the silicon wafer. In the more modern technology, it's usually done by ion implantation, which is a process that we're going to discuss later and is covered in detail in chapter 8. The second process in creating a region of the wafer with a certain doping is typically referred to as the drive-in. This is a subsequent anneal after the pre-dep that then diffuses and redistributes the dopant. get of these dopants if you did a predeposition at that temperature. At 1,000 degrees, you can get something like 3 to 4 times 10 to the 20 electrons per cubic centimeter by introducing arsenic into the lattice. If you introduce more arsenic than that, it may still be below the solid solubility, but you won't get any more electrons. It's not electrically active. It may not precipitate until you get up into the 10 to 21 range. Dopant diffusion is described by Fick's first law, which describes how the flux or the flow of dopant depends upon the doping gradient. When the concentration gradient goes to 0, essentially, the dopant or the atoms are uniformly distributed, say, in the solid, and the flow would stop. Later on, we'll talk about the more atomistic diffusion mechanisms and effects of dopants in the silicon lattice. We're going to consider macroscopic first-- macroscopy models for diffusion. which it's possible to write down or to derive relatively simple analytic solutions to the diffusion equation. In all the other cases and most of the cases we'll end up using in this course, we'll have to do numerical solutions. But for now, let's look at a couple of special cases where we can solve this equation by hand. The first case is pictured on slide 24, which is called the steady state. And what that refers to is that, in fact, we have a profile that is not varying in time. In semiconductor processing, linear scales for dopants are not all that useful. So we often care about how the dopant falls off over many, many orders of magnitude of concentration. In the second case, which is a fixed dose Q, just like we talk about, constant in time. But now we're diffusing near a surface. We can solve it by using, essentially, the prior solution, pretending that the medium is semi-infinite. And the third case is essentially an infinite source of dopant which is made up of small slices. of cases where there are analytic solutions. We talked about the diffusion of a Gaussian profile with a fixed dose. We apply this diffusion to a constant surface concentration. And finally -- we talk about the diffusion of a complementary error function, which we apply for a constant level of surface concentration. The diffusion of this error function can be applied to a fixed level of surface concentration, or a constant level of substance on the surface, for example.

ROUGE-1: 16.06, ROUGE-2: 15.20, ROUGE-L: 15.20
BERTScore: 72.02

==============================================
==================== [15/100] ====================
Summary:
Atas model shows what happens if aggregate demand increases and firms respond to this by saying we want to make more output. As people demand these higher wages shortening our supply curve decreases basically all the way back to essentially where it was before. The price of tuition to Missouri State goes down but my wage is fixed under contract right and so they're gonna have to pay me the Saints of tuition declines and my input prices stay the same that's bad for them and bad for all of the firm's too. Keynesian model has problems too it doesn't work perfectly either so that's why we have these changes in aggregate demand. We'll get to that when we look at fiscal and monetary policy okay so I will see you guys on Wednesday we only have the exam. We only had the exam we only has the exam so we will be back on Wednesday to talk about fiscal and Monetary Policy. We will be talking about the impact of the stimulus package on the U.S. economy and the global economy.

ROUGE-1: 8.64, ROUGE-2: 7.07, ROUGE-L: 7.30
BERTScore: 57.57

==============================================
==================== [16/100] ====================
Summary:
As a nurse you play an important role in teaching the parents about car seat safety and this education actually starts at birth before the child even goes home from the hospital in their first car ride. In this lecture we're going to concentrate on the main concepts that you need to know as a nurse and for exams first let's talk about the four types of car safety restraints that you can use in a motor vehicle. The back seat of the car is actually the safest place for a child 12 and under.

ROUGE-1: 7.06, ROUGE-2: 7.00, ROUGE-L: 7.06
BERTScore: 64.70

==============================================
==================== [17/100] ====================
Summary:
Machine learning is about how to acquire a model and acquire the parameters of a model, from data and experience. In the next few lectures, we're going to work through a sequence of different takes on machine learning that are going to highlight different subsets of the big ideas on this topic. We'll see today exactly how data kind of goes through the mill and gets turned into a model. And we'll see more in-depth examples and more structured examples of these kinds of problems later on when we talk about applications. Machine learning can be used to make predictions about spam or ham. The training set for machine learning is very noisy, so it can be hard to label. Machine learning is not perfect, and some inputs are just really, really hard, and they're going to look like this and we're just not all even going to agree on what that's supposed to be. The goal is to be able to predict labels of new images that are not the ones we've already seen, OK? So that's actually subtle, but it's super-important. representations that if the thing gets tilted or it's a little bit lighter. We could look at how many connected components of ink there are. What's the aspect ratio? How many loops are there? It's increasingly the case, especially for problems like this, that we feed in low-level features like pixels, and higher level features like edges, tend to get induced increasingly more as our machine learning methods get better at doing that. We'll talk about that in a couple weeks when we talk about narrow nets. We're going to talk about model-based and model-free learning. Model-based learning involves building a model from our data, and then doing inference in that model to make predictions in the world. After today we'll look at the model- free methods. We'll also talk about how to learn the parameters of a models from data once we've decided it's structure. And then, the thing we'll mostly talk about today is how to teach a model to recognize a person. word depends on the class and also the previous word. This Is a better model of language. If you started, if you did prediction in this, and you cranked out a pretend document, it would look significantly more like a real email than if you just did a bag of words. However, will it be more accurate for classification? It really depends. In general, it will be a little more accurate, but at a cost of having significantly more complicated conditional probabilities to estimate. If I take a spam document, and I permute all the words randomly, Ive definitely, like it is no longer syntactically-valid English. class which is not kind of strongly connected to the actual ordering, Naive Bayes is really good. Otherwise, you add other correlations like this to fix it. OK? Other questions? Good questions. All right. A couple more general slides, and then, we'll take a break. We are now into the machine learning section which means we are done with the spooky Halloween time ghost-busting section. But I have candy. So during the break, everybody get up and come grab candy if you would like. Machine learning theory is based on trying to say something precise about the connection between what's going on in your data and this future used to which you're going to put the classifier to. The main worry is that in picking the parameters of your model, you do a really good job of capturing that training data, but it doesn't generalize. This is like you download all the exams from past years, and you optimize. You learn all those answers, and then you go to the final exam and it's totally, totally different questions that look nothing like those. Spam detection is, in some ways, a very poor example of a canonical classification problem. Over-fitting means fitting the training data very closely, but not generalizing well. Under-fitting, where you're just like, I don't know what's going on, I'm just guessing randomly everywhere, is not going to work very well. Spam is being generated by people who are trying to defeat spam filters. And now you have spammers who are primarily looking at information or contact information. the thing we're trying to do is to fit a curve to this data. You can always fit your data more. It's about fitting your data to the point where the patterns that you are capturing are ones which generalize to test. This is definitely over-fitting. Over-fitting shows up not just on these continuous functions. It also shows up, for example, in a hypothetical digit classification, we might say, here is an image I've never seen before. Let's use Naive Bayes to classify it. So what would we do? We'd do our running total. over-fitting usually shows up as sampling variance. To do better, we need to smooth, or regularize, our estimates. We can use elicitation, right? You can ask a human. You can go to a doctor and say, hey, I'm building a classifier. What fraction of people with meningitis will present with a fever? And a doctor can give you a guess. It may be qualitative. You could also do that empirically. You Could go collect a bunch of records. The maximum likelihood estimate, or relative frequency estimate, is a way to estimate the likelihood of an outcome. The more samples you draw, the more accurate your estimate will be. But in practice, you need some smoothing to prevent things like zeros in these estimates. This is actually due to a philosopher who kind of worried about things like things like how do I estimate the probability the sun will rise in the morning? Every morning it's risen, so far so far. Every morning the sun's risen. So I need some way of incorporating that into my estimate. there's 100 blues. Now how many reds do I have? Well, I do my computations as if I had 102 reds and 101 blues. And suddenly, even though there are still more reds than blues, in my posterior estimate here, it's pretty close to 50-50. So as I crank up k, I have a stronger prior, and I fit less. If I crank down k,. I fit more, and so I now I have. a dial which can trade off the amount of fitting against generalization. In machine learning, you know what you think the features are going to be. But you might be wrong. So we learn our parameters from the training data. We tune them on some different data, like some held-out data. And then eventually, you're going to take the best value, do some final tests, test run. It's important for when we start to get to neural nets, where the story here is going to change. We're talking a bit more about this starting next lecture. In spam classification, we found out that it wasn't enough to just look at words, you've got to look at other sort of metadata from the ecosystem. For digit recognition, you do sort of more advanced things than just looking at pixels. Try to do things that are invariant to rotation and scale and all of that the vision folks think about. You can add these as sources of information by just adding variables into your Naive Bayes model, but we'll also talk in the next few classes about ways to add these more flexibly, and also ways to induce these.

ROUGE-1: 16.07, ROUGE-2: 15.07, ROUGE-L: 15.02
BERTScore: 64.10

==============================================
==================== [18/100] ====================
Summary:
in this video we're gonna talk about how a country can gain from exporting goods or services through international trade. We're gonna look at how consumer surplus producer surplus and total surplus are going to change when we introduced the idea of trade in allowing Chile's copper manufacturer producers to trade on the global market. The world price of copper is five thousand four hundred and forty dollars a ton. Because the world price is higher than the price in Chile Chile will export copper. There is a shift of some of the consumer surplus is going to go to the producer surplus.

ROUGE-1: 14.64, ROUGE-2: 13.69, ROUGE-L: 13.40
BERTScore: 67.04

==============================================
==================== [19/100] ====================
Summary:
Thiazide tells us that this medication works in the early part of the distal convoluted tubule that's found within this nephron. This transporter is called the sodium chloride co-transporter and it is considered a cyanide sensitive transporter so hence why this drug works so well. While loop diuretics are a lot more effective than a thiazide diuretic but the thiazid does provide a nice diuresis effect. They're less effective in patients who have a compromised GFR a go merrill ER filtration rate. bathroom all the time so make sure you're not doing that and orthostatic hypotension this is where the when the patients maybe they've been sitting or lying down they get up they can fall they become dizzy. You want to teach them to change position slowly because we're altering the fluid status in their body. As a nurse you want to make sure that this medication is being effective is it doing its job how do their lungs sound is their blood pressure coming down maybe they're getting this drug for hypertension how is their weights are they gaining weight or losing weight so we play a huge role with that as well okay so that wraps up this review over thighs I diuretics.

ROUGE-1: 11.30, ROUGE-2: 10.99, ROUGE-L: 11.30
BERTScore: 63.73

==============================================
==================== [20/100] ====================
Summary:
CrashCourse U.S. history will be won by - Harry Styles in 2020. Future John Green tells you that in a stunning turn of events, Harry Styles will win the 2020 presidential election. We're going to change the constitution to make it possible. Because… that’s how much we love Harry Styles. The Wall Street Wamboozle, the Financial Fartstorm. The Major Recession of 2008 - 2012. The 2nd worst economic crises in the past 150 years. A mixture of public and private activities that tilted towards short-term economic thinking. many millions of Americans, including certain Crash Course US History hosts, bought real estate assuming that its value would increase rapidly and forever. It turns out this was essentially a pyramid scheme and, my friends, I was not at the top of the pyramid. So back then you could buy a house with a so-called NINJA loan which sadly this did not involve mutant ninja turtles or pizza. Ninja stands for No Income, No Job, and No Assets. All this created a classic housing bubble, which was doomed to burst. When the mortgages turned bad, these securities became toxic assets. When banks stop lending, business can’t function. The stock market collapsed, with the Dow Jones Industrial Average dropping from above 14,000 to around 8,000. By mid-2009 more women than men held paying jobs for the first time in American history. And World Trade cratered and that led to unemployment and misery worldwide. The event that triggered the chaos was the failure of the investment bank Lehman Brothers in September, just 2 months before the presidential election. In 2008 Obama’s election seemed a political watershed and not just because he was the first African American president. He appealed to young people and minorities, and he harnessed the power of social media to communicate with supporters, and get out the vote, and also raise TONS of money. So Obama promised to change the culture of Washington. He would end partisan squabbling…. sorry sorry. The rules here are simple - he said for the final time. No more shocks. The getting shocked part of my life has come to an end. Hopefully in Crash Course Literature when I get things right I’ll get a puppy and when I got things wrong I'll get a rainbow! He also wanted to end the wars in Iraq and Afghanistan and reverse global warming. A lot of this was more rhetoric than action, as in his verbal support for the revolution that overthrew Hosni Mubarak in Egypt. He did keep some of his campaign promises, for instance he signed into law the Lily Ledbetter Fair Pay Act, which made it easier for women to sue when they had been systemically underpaid. He also appointed two of them to the Supreme Court, Elena Kagan and Sonia Sotomayor. Obama has been criticized internationally for backing off his promise to close the Guantanamo Bay detention camp. But the Obama administration has deployed far more unmanned drones to kill suspected militants around the world. Despite provoking outrage on the left and the right, Americans generally appear to support the use of drones and extra-legal assassination of accused terrorists. Obama was fortunate to have a Democratic Congress for his first term in office, so he could push through a lot of legislation. This included a sweeping stimulus package with nearly $800 Billion in new spending, most of it on infrastructure. The Affordable Care Act is arguably the most significant piece of social legislation since Medicare. Obamacare aims to reduce the number of Americans without health insurance by making it easier and less expensive for the uninsured to buy it privately. In 2012 the core of the law was upheld by the Supreme Court when they ruled that thiants was a constitutional use of the government’s taxing power. Not one Congressional Republican voted for Obamacare, and many used it to campaign against Democrats in the 2010 mid-term elections. The 111th congress was one of the least productive in American history. Unwillingness to compromise precipitated a series of mini-fiscal crises over things like the budget and raising the debt ceiling. Meanwhile, the economy has slowly added jobs and looks halfway decent at the moment mostly because Europe looks so bad. The particular brands of ideological certainty that we see today may seem new but if you look at American history you realize that this has been going on for a long time. The Tea Party is right that the founding fathers would be astonished by the extend of the American government and the extent to which it’s involved in the lives of Americans. We have to ask ourselves again, “What does freedom really mean?’ Can you be free when the government can go to a secret court to read your text messages? We know that you can’t be free if you’re dead, so is the government’S job to protect you not only by having a standing army but also making you wear your seat belt? Crash Course World History has been on the air for two years. The show celebrates two successful years of teaching history. This has been one of the great professional joys of my life and I’m so grateful to everyone that has helped make the show and everyone who has watched it. Thank you again for watching, and as we say in my hometown, “Don’t forget to be awesome.” You can find a full list of your reading for Crash Course Literature in the doobly-doo.rolling.

ROUGE-1: 44.82, ROUGE-2: 42.52, ROUGE-L: 41.83
BERTScore: 68.04

==============================================
==================== [21/100] ====================
Summary:
Simple graphs are directed graphs where the arrows have a beginning and an end. A directed graph might have a self loop, an edge that starts and begins at the-- starts and ends at the same vertices. A simple graph has a nonempty set, v of vertices, and it has a set E of edges, but the edges now are somewhat different since they don't have beginnings and ends. An edge just has two endpoints that are in V, and we don't distinguish the endpoints. The handshaking lemma is an application of graph theory to sex. It asks, are men more promiscuous than women? And there have been repeated studies that are cited in the notes that show again and again that it's consistently the case that the men are assessed to have 30% more, 75% more and sometimes 2 and 1/2 times. We're going to come up with a very elementary graph theoretic argument that says that this is complete nonsense. The proof is trivial, but let's make something of this.

ROUGE-1: 16.38, ROUGE-2: 15.11, ROUGE-L: 15.65
BERTScore: 67.57

==============================================
==================== [22/100] ====================
Summary:
Robotics is a really cool and important direction for the future. I really believe that in the future we will have AI assistance whether they are embodied or not to act as our guardian angels. These agents will help us with cognitive and physical work. With AI we will see such a wide breadth of applications for instance these technologies have the potential to reduce and eliminate car accidents. We have three types of learning and you have seen different aspects of these methodologies throughout the course we have supervised learning, unsupervised learning and reinforcement learning. easily the images that get fed from the camera streams of cars um to the decision-making engine of the car. With all small perturbations you can turn the stop sign into a yield sign and you can imagine what kind of chaos this would create on a on a physical Road so machine learning is very powerful for building perception systems for robots but as we employ machine learning in the context of robots it's important to keep in mind the scope when they work when they don't work. In 1995 a Carnegie Mellon project called nav lab built a car that was driven by a machine learning engine called Alvin and Alvin drove this car all the way from Washington DC to Los Angeles. The car was in autonomous mode for a large part of the highway driving but there was always a student there right ready to um to take control. In 1986 German engineer Ernst Dickman started thinking about how he could turn his van into an autonomous vehicle and so he put computers and cameras on the van and began running tests on an empty section of the German Autobahn. It's really super interesting to think about how visual processing improved from one frame per 10 minutes to 100 frames per second and this has been a game changer for autonomous cars. The other thing that happened in autonomous driving was that the lidar sensors decreased the uncertainty and increased safety and today we have many companies and groups that are deploying self-driving cars. It's really exciting um okay now when we think about autonomous driving there are several key parameters that emerge as we Think about what the capabilities are. Alexander: We have very effective and Deployable solutions for robot cars that move safely in Easy environments where there aren't many static nor moving obstacles. Many companies and research teams are deploying and developing self-driving cars. Many of these preconditions revolve around certainty in perception planning learning reasoning and execution before we can get to Robo taxi but we can have many other robot solutions that are much that that can happen today. Alexander: We can use deep learning and reinforcement learning to take us from images ofroads onto steering and throttle and what you can do with this is really great. standard deep neural network and we have asked this network to solve this problem and the attention map of the network is really all over the place you can see that the network the the Deep neural network solution is very confused but check out something else the data that we collected was summertime data and now it's fall so the background is no longer green we have we don't have as many leaves on trees and so the context for this task has completely changed by comparison the the liquid network is able to focus on the task and is not confused. This kind of this kind of ability to transfer from one set of training data to completely different environments is truly transformational for the capabilities of machine learning. that yields models that generalize to unseen scenarios essentially addressing a challenge with today's neural networks that do not generalize well to unseen test scenarios because the models are so fast and and compact you can train them online you canTrain them on edge devices and you can really see that they are beginning to understand the tasks that they're given so you can see that we're really beginning to get at the semantics of what these systems have to do so what does this has to do with the future well I think it's so exciting to use machine learning to study nature and to begin to understanding the nature of intelligence and we in in our lab here at C cell we have one project that is looking at whether we can understand the lives of whales. There is so much opportunity for developing improved machine learning using existing models and inventing new models. If we can do this we can create an exciting work world where machines will really Empower us will really augment us and enhance us in our cognitive abilities and in our physical abilities. Just imagine waking up enabled by your personal assistant that figures out the optimal time and helps you organize all the items that you need for the day and then brings them to you. Just-in-time Holograms could be used to make the virtual world much more much more realistic much more connected. the kind of future that machine learning artificial intelligence and robots are enabling. I'm personally very excited about this future with robots helping us with cognitive and physical work. This future is really dependent on very important new advancements that will come from all of you and so I'm so excited to see what you'll be doing in the next years in the years ahead so thank you very much and uh come come work with us. "Thank you verymuch and uhcome come work for us," he said.

ROUGE-1: 19.40, ROUGE-2: 18.61, ROUGE-L: 18.99
BERTScore: 60.30

==============================================
==================== [23/100] ====================
Summary:
Tatiana and her team eagerly learned a few of the new recipes last night. The menu has been completely revamped. The message is clear nobody scared to walk through that door and get their hands dirty in that kitchen no we're not tomorrow is a big day let me tell you I need everyone on their game good night guys get some sleep thanks thank you very much chefs oh my God this is amazing last night last night Tatiana andHer team eagerly learn a few new recipes. The new menu has a dynamic simple clean menu right now.

ROUGE-1: 26.11, ROUGE-2: 23.21, ROUGE-L: 24.04
BERTScore: 62.53

==============================================
==================== [24/100] ====================
Summary:
HONG LIU: This is a key relation between the bulk and the boundary theories. The more you go to the interior of the space time, then corresponding to the lower energy process when viewed form the field theory. This actual dimension can be considered as a geometrization of the energy scale. It's called the normalization group flow, how physics evolves when you change the energy or length scale. So you can also view that the evolution in the gravity side, in the field theories. say from the boundary to the interior, and that this flow in the z direction can be considered, again, to geometrize the normalization group flow of the field theory. We have N equals 4 super Yang-Mills theory. And then here, you have type IIB string in Ads5 times ds5. So there's a 4 supersymmetry, which just comes from N equal to 4. And because we have conformal. symmetry, then the conformal symmetry does not actually commute with this 4 supers asymmetry, then. generate another 4. For each global symmetry in the field theory side, there's a corresponding local symmetry on the gravity side. The isometry is important for the following reason. Even though this is a subgroup, when we talk about quantum gravity, the AdS5 times S5 specifies the asymptotic geometry of the space time. So-called large transformations is that they don't go to the Identity at infinity. These large gauge transformations can be considered as the global part of the diffeomorphism. The gauge symmetry is just redundant freedom. You never see it on the other side. expanding 1 over N squared. So as we said before, we often do dimensional reduction on S5. Let me get a five dimensional Newton constant. And the difference is the volume of S5, which is equal to pi cubed R to the power fifth. And then from here, you can just work out. G5 has dimension 3. Then G5 divided by R cubed, again only related to N given by pi divided by 2N squared. And here, when I write these relations, I have all set h bar equal to 1. The classical gravity limit is the same as QFT, Quantum Field Theory in curved space time. So gravity does not fluctuate. But your matter field can fluctuate, h bar equal to 1. In type IIB super gravity, there are many, many such kind of matter fields, and they all should be treated quantum mechanically. Just don't treat the gravity as quantum, but those matter fields should be treating as quantum. And now, we see that the decoupling of the string effect requires on the field. theory side the strong coupling. This is also something roughly we said before. The strong coupling, then the diagram with many, many vertices will dominate. And then the most dominated diagrams are those diagrams with not a lot of vertices. And they essentially are going to continue to limits. So that means that we can actually use classical gravity to, in principle, solve problems which are strongly coupled. So also, of course, there are corrections beyond this. So this is a classical gravity limit on this side. HONG LIU: In principle, the corrections beyond this limit can again be studied on the gravity side. In the classical string limit, still you can see the N go to infinity, which corresponds to GN. But here, the alpha prime can be arbitrary. And here the [INAUDIBLE] coupling can be arbitrarily. So let me just say alpha prime finite no longer zero. And then this is just corresponding to Lambda finite, which is no longer infinite. So this is a standard t Hooft limit. a general correspondence between some conformal field theory and some AdS gravity theory. HONG LIU: Yang-Mills theory lives on Minkowski space. And then you say you can imagine that this is the boundary, this relation is related to the bulk and the boundary. And this is a postulate based on that fact. AUDIENCE: So if it's a sort of postulate that's just thinking about it, it's not required that the theory live on the boundary of AdS. a few minutes. So now, given this mapping, any operator is due to a bulk field. Then you can ask some immediate questions. For example, the quantum numbers of these operators will map to the quantum number of the bulk fields. And that's something I said you can check their symmetries. So for local operator on the field theory side, we can immediately ask questions related to operators on this side. And ask the story about the field on the gravity side, and ask what's happened. We can start developing the relations. guesswork is based on some very small clues. Good physicists can see non-ordinary things from ordinary things. So here, I will try to deduce the answer to this question by starting from this relation. The GS string coupling can be considered as the expectation value of the dilaton field. So expectation value essentially can be associated with the boundary value of. the field. And the second is that phi 0 O, in the boundary theory, adding a delta G, says the Lagrangian of N equal to 4 super Yang-Mills. phi 0 O in the boundary theory must be related-- now I'm generalizing this story. The bulk field phi due to O has a boundary value phi 0. In principle, you can choose any O. You can break the symmetry if you want. Impose this kind of boundary conditions. Then you may break AdS symmetry too. So in the end, what we will describe is a self consistent story. I will not contradict myself in my later discussion. So if we assume this, I can also use this to argue. N equals 4 super Yang-Mills theory and this type IIB gravity. Any conserved curve in the boundary theory must be equal to some gauge field in the gravity side, and the stress tensor should always be due to the metric. I can use this identification to make star and star star natural for any duality, not only N equals 4super Yang-mills theory. It's that any conserved Curve in the Boundary Theory must be Equal to Some Gauge Field in the Gravity Side. and the field theory. And that also tells you that if you have a theory which due into a higher dimensional theory, and then that theory has a stress tensor, then this bulk theory must have gravity. So you can say, if any field theory is due to a theory of one higher dimension, that theory must involve gravity-- nothing about quantum gravity. Let's stop here. We're not going to get into quantum gravity right now, we're just going to talk about the theory of gravity.

ROUGE-1: 23.29, ROUGE-2: 22.17, ROUGE-L: 21.72
BERTScore: 66.72

==============================================
==================== [25/100] ====================
Summary:
Bolek Wyslouch: How do you convert a given physical system with all the forces, et cetera, into some sort of fixed form, fixed type of notation? Bolek: We will work on two, again, simple physical systems, one that consists of two pendula driven by forces of gravity, each of them. And we will discuss various interesting-- even though the system is very simple, just two masses, a spring, a little bit of gravity on top of that. For most of those problems, what you do is you simply focus on the mass in question. You take all the forces, you calculate them, and then this coupling will somehow appear in the equations. The trick in this whole mathematics, and calculations, and the way we do things is how do you solve those coupled equations? OK? So what I would like to do is-- and there is multiple ways of doing that. Let's write down everything in the matrix form, because it turns out that linear matrices are very useful for that. But hold on. OK, all kinds of complex numbers can write, but any particular-- AUDIENCE: [INAUDIBLE] BOLESLAW WYSLOUCH: That's the physics answer, all right? Complex notation is a mathematical answer, how to solve a mathematical equation. The physics answer is to find fixed frequency modes us such that the system, the complete system, oscillates at one frequency. Everybody moves together. This is so-called normal mode. It turns out that every of the system will have a certain number of frequencies. Real-world examples of how to find the oscillating frequency of a system. The system has been built. The only variable which we have to change is the spring constant and the mass this affects is given. We need to find a parameter which sets this to 0, and then I can put the two identical terms so I can solve this equation. So this is the equation which we need to solve to obtain the solutions to at least one normal mode. We expect that there will be two normal modes, because we have two masses. is basically equivalent to the following equation g over l plus k over m minus omega squared. So there is one solution, one oscillation, that does not depend on the spring constant. And there's a second solution which corresponds to minus, where omega squared is equal to g overl plus 2k over m. And this is what we have. We now know that there are two oscillations and two normal frequencies. And the next step to finish our understanding of the system in a mathematical way, to describe it fully, I have to know the shape of oscillations. In principle, we know, now, at the end of the day, I still want to know how much 1 moves, how much 2 moves. So we have to put it all together. We have identified the frequency and the kind of, in the matrix notation, shape of the node. But of course, the final solution is a linear superposition of all possible normal modes. And then you calculate a shape of a normal mode. Is that clear? Any questions at this time? Right? The shape, 1 and 1, and 1 minus 1 is fixed, because these are the shape of normal modes, which corresponds to those frequencies. And the superposition of x1 plus x2 gives you the most general combination of possible motion. So if I write this down now in terms of position of number 1 and number 2, so I have a position of X1 as a function of time. In general, it will look like this. It will be some sort of constant alpha, cosine omega 1 t plus phi. The magenta is normal mode number 2. And blue and the red are the actual pendula. And the motion of blue and red is simply a linear sum of the two. And this is exactly what-- this is the computer simulation that shows you that one of them is going up, the other one down, et cetera. This is for the certain combination of initial conditions. I could go change initial conditions in my program and have a different behavior. But whatever happens, I would be able to-- it will always be a combination of thetwo motions. could put it with me some spaceship, and go to a place where the gravity is different, right? Why not? So what would happen? So if gravity changes, then basically what will happen is both this term and that term will change. So let's try to see what happens on the Moon. It's a little bit not completely clear what's going on, but you see, actually the motion is kind of a little strange. Look at the red one. The red one is stopping. Then it's going halfway out. It looks kind of messy, doesn't it? equations. Let's look at these equations here. This is mass 1 and mass 2. So I can rewrite those solutions a little bit different. And what you get is x1-- x1 of t is equal to minus x0 sine of omega 1 plus omega 2 divided by 2 times sine omega 1 minus omega 2 divide by t. So again, we did zero physics here. We just rewrote the simple trigonometric equations. But there is-- we have those two frequencies which are playing a role. 2 is like omega, right? 100 plus 105 divided by 2 is about 100. Whereas this one here carries information about the difference of frequencies-- 100, 102, the difference is 2, which is very small. So we have-- so this term here-- it basically oscillates at the frequency of omega. And the other term is much, much smaller. How does this look? Well, it turns out that if you make a sketch of this, if you do signs, it looks like this. Two speakers go on very, very similar frequencies, all right? So they both work at similar frequencies. And so when I switched on, you should hear-- hear the sound. So if you have two, and I can adjust the frequency, and the frequency is close, then this frequency of changing is very slow. So you can actually hear it. So this is the effect of beats. I can maybe show you another simulation of this works. Let's see what it is. OK, so this is just a single frequency. BOLESLAW WYSLOUCH: I don't have to go to Jupiter to modify it, because this one is just a little mass here, right? [TONE] Ah, cool. AUDIENCE: Should both of those sine and cosines have Ts in their arguments? BOLesLAW: Of course always. They are both time dependent, yeah. This is the fast thing, and this is this time-dependent modulation. All right, so where are my notes? So this is the-- this is how the-- so we were able to set up the system. simply x1 plus x2, and I define u2, which is x1 minus x2. So instead of talking about x1 and x2 independently, I have a sum of them and difference. So if I add and subtract the two original equations of motion, which I don't know if I have them somewhere, and you can look back, then you end up having those crossed terms drop out. And you have one, which has only this coefficient, the other one which has that coefficient. The determinants needed no matrices, no nothing. We just added and subtracted the two equations, and things magically separated. So you can always have a linear combination of parameters for arbitrary size coupled oscillators system where you combine different coordinates, and you basically force the system to behave in a way in which it induces the single oscillation, single frequency. So this is, again, a very powerful trick, but usually for most cases, you can do that only after you have solved it, after you've found out normal modes, et cetera.

ROUGE-1: 23.46, ROUGE-2: 22.73, ROUGE-L: 22.35
BERTScore: 64.46

==============================================
==================== [26/100] ====================
Summary:
GILBERT STRANG: Take a bar, a material bar. The ends of the bar are kept at temperature zero, they're frozen. Heat is flowing around in the bar, and where is it going? It's flowing out the ends. So I insulate the bar and I put it in the freezer. And I have an ordinary bar and an ordinary temperature. And the heat is escaping out the sides, so the end x equals 0 and the end f equals 1. And that's the solution. differential equation. We have a whole function to match, so we need all of those. And Fourier series tells us how to do that matching, how to find these Bk's. So that's a separate and important question, Fourierseries. Thank you for your time. Back to Mail Online home. Back To the page you came from. Back into the article you came From. The story behind the story: Click here to read the full transcript of this article. Back onto the page of the story you come from.

ROUGE-1: 20.68, ROUGE-2: 16.05, ROUGE-L: 15.75
BERTScore: 54.96

==============================================
==================== [27/100] ====================
Summary:
James W. Swan: I hope everybody saw the correction to a typo in homework 1 that was posted on Stellar last night and sent out to you. We don't want to give you a homework assignment that's punishing. We're still going to talk about transformations of matrices. We looked at one type of transformation we could utilize for solving systems of equations. Today, we'll look at another one, the eigenvalue decomposition, and on Monday, we will look at the singular value decomposition. be stuck. It won't proceed after that. So it's the difference between getting a solution and writing a publication about the research problem you're interested in and not. So how do you do reordering? Well, we use a process called permutation. There's a certain class of matrix called a permutation matrix that can swap rows or columns. So if I want to swap columns, I multiply my matrix from the right, IP transpose. If I swap the rows and then I swap them back, I get back what I had before. James W. Swan: We discussed sparse matrices and a little bit about reordering and now permutation. He says permutation is a form of preconditioning a system of equations. He shows a simulation that tells us how probable it is to find the Plinko chip in a particular column. Swan: We could construct an alternative model that didn't have that part of the picture that we wanted to have. He's happy to answer questions on.at and he'll be back in a few minutes. For a real N-by-N matrix, there will be eigenvectors and eigenvalues, which are the amount of stretch. Eigenvalues are special vectors that are stretched on multiplication by the matrix. They're non-linear because they depend on both the value and the vector, the product of the two, for N plus 1 unknowns. So we can never say what an eigenvector is uniquely. We can only prescribe its direction. And that describes the eigen vector-eigenvalue pair. roots of the secular characteristic polynomial. They are the eigenvalues of the triangular matrix. Eigenvalues have certain properties that can be inferred from the properties of polynomials. These can sometimes come in handy-- not often, but sometimes. The eigen values of that matrix are going to tell us something about how different rate processes evolve in time. We'll see in a minute what those eigenvectors are in those transformation that reflect that transformation in physical processes. We want to know the eigenvector of the rate matrix having eigenvalue 0. This should correspond to the steady state solution of our ordinary differential equation. Can you do that? Can you find this eigen vector? Try it out with your neighbor. See if you can do it. And then we'll compare results. Are you guys able to do this? Sort of, maybe? Here's the answer, or an answer, for the eigenector. It's not unique, right? It's got some constant out in front of it. James Swan: Can you find the eigenvalues and some linearly independent eigenvectors of this matrix? And if you find them, what are the algebraic and geometric multiplicity? He explains why this is important in a second, but understanding that this can happen is going to be useful for you. He says the matrix then is said to have a complete set of eigendecomposition pairs, and that's useful for solving differential equations, ordinary differential equations or for transforming systems. triangular form for this matrix. We'll talk next time about the singular value decomposition, which is another sort of transformation one can do when we don't have these complete sets of eigenvectors. You'll get a chance to practice these things on your next two homework assignments, actually. So it'll come up in a couple of different circumstances. I would really encourage you to try to solve some of these example problems that were in here. Solving by hand can be useful.

ROUGE-1: 15.96, ROUGE-2: 14.53, ROUGE-L: 14.93
BERTScore: 61.76

==============================================
==================== [28/100] ====================
Summary:
A random variable is a number that's produced by a random process. Number of faulty pixels in a monitor is also produced from an unpredictable randomness. The number of alpha particles that are detected by a Geiger counter in a second is believed to be a random number. And finally, we'll look at the standard abstract example of flipping coins. And if I flip coins then the number of heads in a given number of flips-- let's say I flip five heads in one second. That's a random variable. flip a coin n times. The number of heads is a number that comes out of this random process of flipping the three coins. C is greater than or equal to 1 when there is at least 1 head. M greater than 0 means all the coins match. This is an obscure way of describing the event all heads, and it has a course probability 1/8. So formally, a random variable is not a variable. Or it's a function that maps the sample space to the real numbers. Random variables are mutually independent if the events that they define are all mutually independent. If I have an event A, I can package A into a random variable. The indicator variable for any event A is 1 if A occurs and 0 if A does not occur. If two events are independent, then their indicator variables are independent. That is a lovely little proof for you to verify. It's a concept of independence that holds for random variables. And it means that really I can think of events as special cases ofrandom variables.

ROUGE-1: 19.00, ROUGE-2: 17.77, ROUGE-L: 18.18
BERTScore: 67.69

==============================================
==================== [29/100] ====================
Summary:
s gawan and the Green Knight um I believe this is the last piece where we don't know who the author is where it's unknown um I find the background interesting and that through textual Clues they're able to figure out who wrote it. It's interesting when they talk about some writer character character character is whatever that word is SAR Gowan as a ruthless bloodthirsty Warrior um we really don't see him in that regard. Throughout the story of seral when we again come back to what a knight truly is. King Arthur even welcomes the Raper. He says I am not here for trouble no armor down there at the bottom I didn't bring anything there's no other weapons. Arthur says on page 177 that I am here not for war and I shall offer to him this fine axe freely that they strike a blow in return for another so I will let you use my axe you can take a shot at me but I want to get ashot at you. When I survive the blow that you give me with this ax you will have to come and find me afterwards on your honor can you see how this is a game of honor? sir GNE being called a coward we've got to understand that mindset who would that be calling a coward Arthur and everybody else the Knight came here specifically to challenge the best that Earth has ever bred is what he was saying and so this is the moment and now GNE has a year to think about his impending doom is he going to go do we think he's going to take the axe shot is this all a test is he Going to trick something I mean these are all thoughts that should be going through your mind. you receive the the hunter the owner of the house the husband says whatever you receive you must give to me that seems kind of weird what possibly could he receive while he's there well we find out that the wife really kind of starts to come on to him over time okay real flirty she's she's a temptress okay very seductive uh she gives him a kiss what does he do when the husband comes home that day cuz the husband's going to share his his food share his everything with him as long as GNE you share with me and so he gets kissed. comes home and GNE does not give up the girdle because why would I give this this is going to save my life I'm not going to so he breaks his word okay he becomes a coward. He is selfish and coward about his life about hey you gave me a this is a wild card he doesn't know I'm going to have an ace up my sleeve come time to get that shot in the head um and so he does leave and he keeps that GLE for himself um and doesn't give that away page 185. thing and so he hears it you know happening and um so he screams out and the and the um and the Green Knight comes he says by there said one on the bank above his head and you shall swiftly receive what I once swore to give you. The Knight addresses him they have a little back and forth and he gets ready to hit him and it's one of those kind of like you've had friends before probably have come up to you and go like that. He chastises him and says do it again I will not flinch go on game. Celebrate was it New Year's is that what it is we'll come back and celebrate this cold New Year I'm not mad at you so now we need to go back and look remember the archetypes we talked about. What was the purpose of the Green Knight simply to challenge challenge King Arthurs and his crew and see if they truly are as honorable as they say as legendary as Legend has as the stories have it. Can we really call him a villain probably not but yet isn't he that antagonist early on? good person away with the forbidden fruit you know something similar we could maybe connect uh down the road but it's very similar to to those so um so go and the green. I hopefully you enjoyed it um really story-wise we're done with knights uh. It's a very similar story to the one we did with the knights. It was a very different story but it was very similar. I hope you enjoyed the book. I'm glad you read it. I'll see you next week.

ROUGE-1: 29.72, ROUGE-2: 28.17, ROUGE-L: 28.23
BERTScore: 62.37

==============================================
==================== [30/100] ====================
Summary:
hey what's going on YouTube boy Robert and my mission is to teach you everything in the kitchen now earlier this week at work I learned and I was wondering what are the fruits and vegetables I can turn in the salt as well. Today we have blueberries strawberries Kiwis red beets yellow beets pineapples right in fruit and cucumber this is the equipment you'll need for today you don't need one blender sheet trays mason jars parchment paper aluminum foil a spice grinder a bowl with a strainer and a plastic spatula.

ROUGE-1: 27.87, ROUGE-2: 27.37, ROUGE-L: 27.87
BERTScore: 66.38

==============================================
==================== [31/100] ====================
Summary:
Sarah with register nurse rn.com goes over how to check Vital Signs. Vital Signs are pain oxygen saturation temperature heart rate respirations and blood pressure. You want to provide privacy to the patient and tell them what you're going to be doing. If they do have pain ask them the quality what does it feel like and where it is at. There's several ways you can take a temperature every facility has a different system set up so use what you want. For confidential support call the Samaritans on 08457 90 90 90, visit a local Samaritans branch or see www.samaritans.org. they have but you can take it orally you can taken it axillary you cantake it tanic in the ear or you can takes it temporally or rect um rect is the preferred route usually on your pediatric patients but in adult patients normally we do it orally. axillary and temporally the readings are going to run about one degree lower than oral. For tanic and rectal temperatures it's going to usually run about 1 degree higher than your oral reading. A normal temperature is about 97° fit to 99° F okay and take the thermometer out and read it and his temperature is 98.2 and then clean it properly. In this video we're going to go over the one step blood pressure of how to obtain it manually. We are going to palpate the brachial artery this is in the bend of the arm and make sure you ask the patient which arm you can take their blood pressure in because you don't want to take it in arms with if they've had blood clots or they have a shunts things like that. We're blowing it up to about 200 mm of mercury and we're listening for the first sound and that first sound will be our blood pressure. Mark now we're listening for whenever it stops and whenever it stopped that's our diastolic okay it stopped right at 65 so his blood pressure is 114 over 65 so that is how you check bottle signs now whenever you're done remember to let the patient know what their bottle signs were and um do hand hygiene and clean your equipment before you go to the next patient so be sure to check out all my other videos on nursing skills and thank you so much for watching and for all your support.

ROUGE-1: 38.52, ROUGE-2: 35.76, ROUGE-L: 37.17
BERTScore: 70.13

==============================================
==================== [32/100] ====================
Summary:
Grenade algorithm uses a minimal set of three-point correspondences to solve the camera pose estimation problem in the grenade formulation. The disadvantage of this particular formulation is that it ends up with a four degree polynomial which means that it could give up to a total of four possible solutions. The question becomes whether can we find the solution directly from any four point correspondences such that the solution is unique? The answer is yes and it was formulated by quan at all in a paper that was published in the 1990s. any of the unknowns in the the depth where we simply write it as s i square over here so this can be derived in in this way so here i wrote out all the six combinations of the polynomial equations f i j over here we can see that each one of this equation it's a just a function of two unknowns. The unknowns which are x 4 x 3 x 2 x and 1 can be written as a vector over here 0 0 and as a result we'll get a three by five matrix and a five by one vector. s1 we know that t5 here is actually a vector that is made up of the same entry but of different order so x4 x3 x2 x and 1 of different entry. We know that by observation any two elements the product of tij is going to be equal to tk and tl for this constraint to be valid where i plus j must equal to k plus l. We can substitute the respective components of t5 into this particular constraint to get this equation over here. The solution would be the one that corresponds the last column of v over here. This is actually an over determinant system because b here is seven by three so this means that we have seven equations over here but only three unknowns that we want to solve. Taking svd of b we get the left singular factors multiplied by the singular values as well as the right singular vector matrix over here so once we have solved for the null space vector y using the svd what we can do is to proceed on to solve for lambdas and rho. basis vectors e4 and v5 respectively earlier on when we solve for t5 so we can see that the first row of t over here is actually equals to one 1 and this equates to Lambda multiplied by the first component of v4 plus rho multiplied by v5. Together with this equation over here you can call this equation 1 and then with equation 2 over here that provides a constraint on Lambda and rho we have two equations and two unknowns and we can solve for lambda and Rho uniquely here. where we can solve for since we know that x equals to s1 squared we can solved for the final depth by taking the square root of x. Once s1 is solved we can back substitute s1 into the polynomial equation of f i j s i and s j equals to zero. Once f sj is solved, we can then back substitute it into another equation with sj and solve for the other unknown depth. Finally we will get all the depth after we have gotten all the unknown depths we can do the same thing to to apply absolute orientation to recover the camera pose as in the grenade algorithm. equation we can stack them all up so one two all the way to six equations and we end up with a coefficient matrix with an a matrix of of dimension 6 by 5. in order for non-trivial solution to exist then this guy here better be of a maximum of rank four so what we can do here is that we can take the svd of a and this will give us u sigma v transpose where we simply the vector that corresponds to the least singular value in sigma. to the solution of s1 over here now it turns out that the same algorithm that we use to solve the linear four point and the linear five point can be applied to any number of point correspondences that is four or uh more points. Here we just need to solve for the svd of a m minus 1 multiplied by m minus 2 divided by 2 by 5 uh matrix of a to get the solution for the vector t 5 but the problem here is that the overall complexity is a cubic in the order of the number of points that is used to form a. proposed by lapati in the year 2006 is that instead of using every single point correspondences that is given to us the the core idea is that here we'll make use of all these points to define four control points. Even for a very large n the number of control points still stay constant as uh fall. So now this control point becomes our unknown so that we also need to solve for in addition to the camera rotation and translation where these 3d points they are given and they are known. The number of basis solutions that we can get from solving the equation mx equals to zero over here depends on the size of m and which in turn depends on number of point correspondences that we have. In the case where we have only one basis solution where capital n here equals to 1, we can write the solution as x equals to beta of v where v here is the null space vector of the equation. In this particular case, we'll make use of the known distance between the control points to solve for the unknown beta over here. camera reference frame we can substitute it back into this equation to recover the 3d points that is defined in the camera frame. Once we know this we will be able to solve for the rotation and translation between this two sets of points. We can solve r and t between these two frames using the absolute orientation algorithm. And then finally we look at the efficient pmp algorithm which is a on complexity algorithm that solves the camera post estimation problem using the efficient orientation algorithm and the efficient perspective endpoint problem. for the camera pose using a set of control points using four control points in particular and we saw that this uh it's uh the complexity is a linear in terms of the number of points which is much easier to compute then we also saw the degeneracy cases for the camera post estimation problem in particular. If all the points plus the camera center forms a plane then this is also a degenerate case and that's the end of today's lecture thank you mx equals to zero. Mx equals zero.

ROUGE-1: 23.91, ROUGE-2: 23.16, ROUGE-L: 23.28
BERTScore: 67.96

==============================================
==================== [33/100] ====================
Summary:
Markus Klute: If there is a time dilation effect due to gravitational fields, then there's also a redshift which is of gravitational fields. He asks you to estimate the magnitude of this effect. You want to use the example of a 22 and 1/2-meter-tall tower. He says the speed of light is pretty fast, 3 times 10 to the 9 meter per second. And this distance is only 22 and1/2 meters, so we find that this is a tiny, tiny,tiny effect. to know more about this, you can, for example, look up a small description in Wikipedia here. But there's quite some literature on those experimental tests [INAUDIBLE] There's a lot of information on the subject on Wikipedia. It's a very interesting area of research. There's quite a bit of literature on it. It can be a very exciting area of study. It could lead to some very interesting results. But it's a little bit of a mystery as to how it works.

ROUGE-1: 52.65, ROUGE-2: 40.63, ROUGE-L: 43.22
BERTScore: 72.58

==============================================
==================== [34/100] ====================
Summary:
During the semester we have a few recitation instructors they help with the students during the recitation section. During those sections your the the instructor will solve a similar problem like what is actually covered during the same during the lecture and that give the students another chance to look at more example and to get for media will get used to the calculation which we carry how for the first time during the the lecture. We did not record the Recitation sections during the fall semester in 2016 on the other hand we included problem-solving videos from Professor with Busha.

ROUGE-1: 67.77, ROUGE-2: 66.22, ROUGE-L: 67.77
BERTScore: 85.45

==============================================
==================== [35/100] ====================
Summary:
In this video, we're going to compute some useful quantities for the exponential random variable. The CDF of x is the probability that X is less than or equal to little x. We use the standard formula, which is minus infinity to infinity t times fx of t dt. For this, if you evaluate the balance, 0 makes this 0, and 0 to get 1 over. And so the expectation is 1 over lambda, and so the variance is part c, so OK, so far so good. So it's going to be-- Notice the similarity between this and this. The only difference is this has a 2 lambda in there. That means that w is an exponential random variable with rate 2 Lambda. You can also take the derivative of this and find that you get this. OK, so we're done with the problems. We computed some interesting quantities for the exponentialRandom variable in this. So then the PDF isgoing to be an exponential, whatever it is for an exponential.

ROUGE-1: 15.91, ROUGE-2: 14.60, ROUGE-L: 14.40
BERTScore: 69.97

==============================================
==================== [36/100] ====================
Summary:
Professor: Can you explain the physical significance of the crystal momentum? Professor: The information about the momentum can be encoded in these spatial variation of the phase of the wave function. Professor: A central property from the Schrodinger equation is at the time variation d dt of p is equal to the expectation value of minus d the potential of x d x. The momentum expectation value is time independent, so it can be written in the form of e cubed, equal to e to the minus i upon h bar p l. equal to alpha, then acting on u sub q, by translate by l, on U sub q of x, is equal to-- well, if we act on Phi sub q we pick up a phase e d i alpha. What happens to e to the minus i q x? x goes to x plus l. So if q l is equalto alpha, those two phases cancel, and we just get u back. u subq of x. If we want u to be always real, we can't impose this periodicity on different wave functions. the imaginary part, h bar over 2 m i. Well, hbar over m times the imaginary part of SI complex conjugate derivative, with respect to x, which is the current, in the x direction of SI. And we need this to be imaginary, or we will get no current. You show this in a problem set, if you have a pure, real wave function, for example. A single real exponential, that's decaying, as on the wrong side of a barrier. can always write it as some q naught plus n pi over l. And so now really what's going to happen, what I'm doing here is I'm labeling q, not by a single number. And then we pick a different function, felicitously, called Phi sub q, which is contained in Phi q. So this is the potential. So in this statement that what we have this translation by x is just some function. It has to do with the potential of the wave function. So if we translate by nothing, then I construct a new function. Professor: When you have a perfect lattice, there is actually no current flowing in response to an applied electromagnetic field. Professor: You can build a system that has exactly a periodic potential. He says you can do it with lattices not of atoms, but lattices of dielectric. Professor: Optical fibers can be used to control the phase of the light that's shining through a glass fibers, and you can shine your light in a way that's reasonably well localized, and that's a lot of fun. beautiful block oscillations. And this has been proved in a very small number of real honest quantum mechanical systems. The most elegant experiment that I know of was done by Wolfgang Ketterle, who's here at MIT. And he got three data points because it was preposterously difficult and declared victory. But it really needs to be done well. So one of the interesting questions in this part of the field right now is we know that it's true. But we want to see it. We want to feel it. talk about the 1 d potential. In order for the system to be neutral, I must have one electron for every well. If we let the system relax with lowest energy configuration, every state in this lowest band will be filled. The next band is empty, but in order to take an electron from here and put it into this state, we have to put them in a superposition of other states. In 1 d, do these bands ever overlap? No. By the node theorem. macroscopic amount of energy. Well, it's not macroscopic. it's large. It's not infinitesimally small. That means that there's a minimum amount of. energy that that incident light must have in order to excite the. electron in the first place. Crystals are transparent unless you look at sufficiently high frequencies. If you looked at low frequencies, your crystal should be transparent. Well that's really interesting. In particular, we immediately learn something cool about two different materials. Consider diamond and copper. These are both crystals. In 3D, this isn't such a big deal, because those splittings are tiny, and so the states can sort of overlap. But in 1D they can't. So I mean, that's also not exactly true, but it depends on exactly the details of the system, is what I wanted to get to. So let me talk about the same phenomena in an easier context, where we don't have to worry about spin, which we haven't discussed in detail, in the class. we add in a lattice we get bands again. The structure's a little more intricate because it depends on the momentum. But these bands now can overlap. OK. Everybody see that? Because there's nothing preventing states from different-- in different multiplates from having the same energy in three dimensions. There's no nodes here that tells you have to keep the ordering constant as you turn on the potential. Now we turned on the multiple particle potential, and they can interact, they can overlapped. As a consequence, when we fill up, let's say we two electrons per potential. the induced electric field? Nothing. Any electric field that you send in will be opposed by an induced current. So this behaves like a classical conductor. You turn on an electric field, and the charges will flow to oppose that externally imposed electric field. You get charges then building up on the walls of your capacitor plates. So, this is where we have a conductor. Because there's an unfilled band. And back here , we had an insulator because we had filled bands separated by gap. The gap between the filled band and the next available band is actually called a band insulator. yeah. There aren't [INAUDIBLE]. But what would we need? What we need is one of two things. We need either the band gap coincidentally is ridiculously small. What's a good example of that? A free particle. In the case of a free particle, these band gaps go to 0. Right? And so that's a conductor. Just an electron. It conducts, right? OK. But that's sort of stupid. But a better answer would be, well, can you have a system where there are bands but you didn't have one electron per potential well? And yeah. You could orchestrate that in lots of ways. Photonic crystals have bands of allowed energy and gaps of disallowed energies where no waves propagate through. Photonic crystals form exact crystals that reflect at very specific wavelengths. So why would a butterfly put a photonic crystal on its surface? Well it's extremely light. It's fairly rigid. And it's like the best thing you could ever do if you wanted to be a shiny, fluttery, flying thing. OK. So that's it for band gaps. And I want to move on to the remainder, the last topic of our course. Which is going to be entanglement and quantum computation. The probability of finding the particle at point A is given by chi a squared. And similarly, the probability that we find the second particle at b is this thing norm squared. But we also studied the symmetric configuration, which was equal to 1 over root phi, root 2. Chi of a Phi of b. Symmetric, plus chi of b phi of a. And this tells us something totally awesome. So there's a factor of one half. We either find it at chi of a or chi. So it's fine. If we integrate over all phi b, this is the norm squared integrates to 1. "This should be yet another moment of serious discomfort," he says. "How can something here dramatically change the state, the configuration, the initial configuration, of a particle arbitrarily far away?" "If you think about relativity, this should be all the more deeply disconcerting," he adds. "We've run into a bunch of these over the semester. But this one should be troubling to you," he concludes. "It's a problem that should be deeply concerning to all of us" that's kind of amazing is that he created a thought experiment which we're going to study in detail next week called the EPR experiment. And there's a beautiful historical story about the setting and the meaning and the particular person. But unfortunately, I'm not a historian so I can't tell you that story. It sure would be nice if we had someone who wrote a biography of Einstein to tell you a little bit about that. Oh look, it's Tom Levenson. Tom is-- of describing this problem of entanglement. And it's based on his description of an actual person. Bell used to say, if you saw Bartelstein and you could only see one leg and that sock was pink, you knew to a certainty that the other sock was not pink. Same thing. If you have a coin and you cut it in half down the-- so you've got two coin shape disks. And they're separated. They get handed to two different gamblers. And one gambler tries to cheat the gambling establishment by tossing in his half coin. And you see the head that you know somebody-- somebody at some other casino is cheating. Einstein, Podolsky, and Rosen argued that the EPR paradox suggested that quantum mechanics was incomplete. They said that if you can perform a measurement, you know that quantity absolutely. But you can't do the-- so on the one hand, quantum mechanics says you can’t know physical reality to this level of precision. And on the other hand, the fact that you can do that measurement violates the relativistic picture of reality. But the question is whether or not the framework of quantum mechanics is somehow unsatisfactory in any formal sense. stupid. He was Albert Einstein. But he was aesthetically incapable of pursuing this new physics in ways that were possible under the research possibilities of the time. And that is what I would leave you with. Physics is an aesthetic as well as an intellectual pursuit. So thank you all. [APPLAUSE] Back to Mail Online home. Back to the page you came from. Follow us on Twitter @dailymailonline and @MailOnlineernews. Back To the pageyou came from, back to the Daily Mail home.

ROUGE-1: 21.89, ROUGE-2: 20.76, ROUGE-L: 20.41
BERTScore: 61.73

==============================================
==================== [37/100] ====================
Summary:
First up the proper way to chop fresh herbs to get maximum flavor. The secret is to chop them not bruised them now basil this is a soft herb so treat it with some respect. If you have fruit that's not perfectly ripe the tip is to put a banana in a pot of water. The best way to cook parsnips is to cook them in the same pot as the parsnip. The most important thing to do when you're chopping herbs is not to cut your finger. Cutting a mango the easy way homey you stalk end up cook either side of the stone cut all the way into the flesh making squares without cutting through the skin then turn it inside out and carefully cut your pieces off. A great tip to prevent burning sensitive skin when working with chilies to get rid of that spice and that heat on your fingers a little bit of lemon squeeze a littlebit of lemon juice and that instantly gets rid of the heat fresh lemon juice for perfect ball potatoes always start them off with cold water and never boarding water.

ROUGE-1: 37.31, ROUGE-2: 32.16, ROUGE-L: 32.10
BERTScore: 71.29

==============================================
==================== [38/100] ====================
Summary:
hey everyone it's sth register nurse rn.com and in this video we're going to be going over our weekly inlex practice question. Don't forget to check out the other practice questions in this series. Let's see what our question says a patient who has a health history of uncontrolled hypertension coronary artery disease and diabetes militis is prescribed to take propanolol. You have provided the patient with education about this new medication which statement by the patient indicates your teaching was effective.  beta blockers block your sympathetic nervous nous system. They increase a process called glycogenolysis which is going to increase that blood sugar. If a patient misses a dose of their beta blockers they don't need to double the dose they need to take it as soon as they remember unless unless the next dose is double they're at high risk of a cardiac event. If they're taking a beta blocker that is blocking that response they're not going to have that classic Tac of cardia so they may later on their sugar will drop so much. look and see why D is wrong this patient says that they're going to stop their beta blocker immediately if they experience cold hands and feet well this is a normal side effect with these non-selective beta blockers. You would never just immediately stop taking a beta blocker they need to be tapered off of this because if they just all of a sudden quit taking that medication they can go have cardiac death or something worse can happen so that answer is wrong for that reason okay so that wraps up this inlex practice question.

ROUGE-1: 32.97, ROUGE-2: 31.49, ROUGE-L: 29.83
BERTScore: 68.95

==============================================
==================== [39/100] ====================
Summary:
This is part 2 of a guide to clinical reasoning or how to create an accurate differential diagnosis from a patient's presentation. The patient is a 75 year old woman presenting with epigastric pain for four hours. She was in her usual state of health until four hours prior to presentation at which time she developed the onset of abdominal pain. The pain is relatively well localized to the midline in the region between her humble ankus and xiphoid process. There did not seem to be any particular trigger and the duration from initial to final pain was relatively short. onset to its maximal intensity of 8 out of 10 was about 45 minutes the pain is constant does not radiate is not exacerbated by anything including nor alleviated by anything she had moderate nausea it has refused to attempt to eat or drink anything since the pains onset because she is concerned that it will concert a vomit which she has not yet done. She denies changes in her bowel habits shortness of breath chest pain changes her skin or eye color her past medical history is notable for hypertension and diabetes she has had no surgeries. exam reveals normal oral pharynx but poor dentition her neck is supple without breweries and without lymph adenopathy her chest is clear to percussion and auscultation bilaterally her cardiac exam has a regular tachycardia hypertension her jbp is undetectable at either 45 degrees or when she's completely sipping on abdominal exam she has non distended and there are no scars. bowel sounds are unusually quiet but present rectal exam with normal tone no tenderness and minimal guaiac negative brown stool musculoskeletal exam revealed full range of motion in all joints without bony abnormalities. The problem representation is a one to two-sentence summary using precise medical terminology of the most highly relevant aspects of the patient's history exam and diagnostic tests. The primary symptom using semantic qualifiers might sound something like acute constant epigastric pain. The soft abdomen and lack of rebound would be grouped into the collective descriptor of absent peritoneal signs. The elevated lipase would also be included in the summary of the highly relevant diagnostic data. The organization of a particular framework may just appeal more to some people than others in general when the primary problem is some form of abdominal pain most people most people will have pain. The epigastrium would obviously be the most critical anatomic region to include organs that lie directly underneath it. The major structures are of course the liver and gallbladder along with other components of the extra hepatic biliary system. The spleen can cause pain with either a splenic infarct or splenic abscess. The chest referred pain to the upper abdomen from intrathoracic diseases is a common phenomenon organs in the chest to consider include the esophagus and the heart. and what that impact was that is obviously very tedious and time-consuming an expert clinician would probably start with the key features focusing on the ones that were either most prominent most unusual or those which clearly clustered together. Then thinking about which specific diagnoses were impacted by those features for the purposes of demonstration I'm going to take an approach here somewhere in between by going through each diagnosis one at a time but only mentioning those key features would seem particularly relevant so first up is gastritis the patient has some risk factors for gastritis with our alcohol use in smoking so these increase the probability of this diagnosis also gastritis can be associated with nausea. any key features which have not yet been incorporated into the framework that is are there any key features that don't seem to impact the likelihood of any of the diagnosis in the framework in this case there are essentially three. The first is the normal troponin and CK so why was this piece of information not incorporated into our framework discussion it's because they were actually miss identified as key features. The second unused key feature is the guaiac negative stool this key feature was critical in establishing the problem representation in step three. element does not fit into the framework yet still seems to be a key feature the framework must be incomplete in this case I would add another category of diagnosis to our four existing categories. That fifth category is acute abdominal pain secondary to systemic toxic metabolic problems. The four major members of this group are heavy-metal poisoning a rare genetic disorder called acute intermittent porphyria another virgin etic disorder called familial Mediterranean fever and finally angioedema for any of these to be the final diagnosis this patient would need to have an atypical presentation of a rare disease and thus it would be a highly unlikely diagnosis. of diabetic ketoacidosis which can definitely present with acute abdominal pain and nausea in a diabetic however the normal metabolic panel which implies a normal glucose completely rules out this possibility. In my opinion the most likely diagnosis for this patient is acute pancreatitis. pancreatitis is associated with her heavy alcohol use it explains her nausea her distress the vital signs her tenderness on exam leukocytosis and is the best explanation for the elevated lipase the only key feature arguing against it is the fact that the pain is not made better or worse with changes in position. atypical presentation those diagnoses which are rapidly fatal if missed for which this could plausibly be a presentation. Finally any diagnosis that is specifically suggested by an unusual element of the presentation even if the diagnosis itself is rare common disorders which could be a typical or atypical Presentation include gastroenteritis food poisoning and either gastritis or peptic ulcer disease which I have grouped together because I think it's very difficult to tell the difference between them without endoscopy don't miss diagnosis for this presentation would include a bowel infarction and acute coronary syndrome. that will increase the likelihood of. establishing the correct diagnosis sooner in outpatient hospital course you. That will. increase the chance of establishing the right diagnosis sooner. that will increase. the likelihood that you will be seen by a doctor sooner in hospital. that you. will be more likely to be admitted to hospital for treatment. that. will increase your likelihood of being seen by doctors sooner in the hospital. and that. you will have a better chance of being treated. of being diagnosed with a condition that is more serious.

ROUGE-1: 37.95, ROUGE-2: 35.20, ROUGE-L: 35.65
BERTScore: 66.29

==============================================
==================== [40/100] ====================
Summary:
Week 6 of CS224N is now past the halfway point. Today is the day that you have to have done the mid-quarter survey by. If you haven't, this is your last chance to get the half-point for that. Final project proposals are due. We really encourage you to try and hand them in on-time. And then today, delighted to have our first invited speaker. Danqi Chen, one of the foremost researchers in question answering, and she's particularly well known in recent work. here to give this lecture on question answering. The goal of question answering is to build systems that can automatically answer questions posed by humans in our natural language. Question answering, or, let's say QA in short, is one of the earliest NLP tasks, and the early systems can even date back to the 1960s. The question and answer has enabled a lot of really useful real world applications. For example, today today we can answer questions like, "What do worms eat?" and they'll finally retrieve the answer, that's the grass. if you just put your question in a search engine like Google. And those kind of systems are also able to handle more complex questions like how-to questions. So the question is, how can I protect myself from COVID-19? So there isn't really a simple and short answer to this question. So you can see that the system actually returns a very long paragraph, including the best way to prevent illness is to avoid being exposed to this virus. So actually this paragraph is actually a summary from CDC article. So this is also one type of question answering problems. Question answering is probably one of those fields that we have seen the most remarkable progress in the last couple of years driven by deep learning. In this lecture, I will be mostly focusing on the text based, or textual question answering problems. And another class, bigger class of the question Answer problems is called visual question answering. So this problem basically requires both understanding of the questions and also images, and is actually a very active field between the computer vision and NLP. So if you have interest in these type of problems, I encourage you to check out those problems, but I'm not going to go into them. dig into these problems today. So next, I'm going to start with a part 2, reading comprehension. So reading comprehension is a basic problem that we want to comprehend a passage of text and answer questions about the content. So basically to answer this question, so you need to find this sentence, like, in 1861, Tesla attended this school where he studied German, arithmetic, and religion, and only German is a language. So this is really just similar to how we humans actually test the reading comprehension test to evaluate how well we actually understand one language. Stanford Question Answering Dataset is a supervised reading comprehension dataset. It consists of 100K annotated passage question and answer triples. Each answer is a short segment of text, or we called it span in the passage. And this kind of a large scale supervised dataset are also very key ingredient for training the effective neural models for reading comprehension. But also just to caveat, that this is also a limitation. Because not all questions have the answers in this way. But today, so this dataset, I forgot to say that. This dataset was collected in 2016 by several researchers at Stanford. Stanford, so it's called Stanford Question Answering Dataset. Today, after four or five years now, so SQuAD still remains the most popular reading comprehension data set. So the state-of-the-art AER systems still have time to just train or sequence tagger on top of the word. Danqi, one question you might answer is, so if you can do other tasks like named entity recognition or relation extraction by sticking something ontop of BERT and fine tuning for it or do it as a question answering, does one or the other method work better? answer to that. So next, I'm going to talk about how to build neural models for reading comprehension, and in particular, how we can build a model to solve the Stanford Question Answering Dataset. So the input of this problem is let's take a context or paragraph. And also we take our question, Q. And the question consists of n tokens q1 to qN. And because the answer has these constraints, the answer must be a section of text in the passage. We are going to predict a start and then end.  query-to-context attention is trying to measure the importance of these context words with respect to some question words. So by taking the max for each row in this matrix, so it's basically trying to see, OK, which question word is actually most relevant to this context word? And then you can just apply your softmax. And then this will give you a probability that OK, what is the probability of this condition i, would be based on the start position of the final answer string. the dot product between w end and this vector, and this can produce all the probability over all the conditions which predict how likely this position will be the end the position of the answer. So by passing the mi to another bedirectional LSTM, their reasoning is that they're trying to capture some kind of dependence between the choice of the start and end. OK, so this model is actually achieved-- like on SQuAD data set, it achieved a 77.3 F1 score. If you remove the context-to-query attention, the performance will drop to 67.7 F1 score. And then if you remove this part, it will drop a 4-point F 1 score. So basically this theory tells us that these kind of attention scores can actually capture the similarity between the question words and the context words. And now here is our attention visualization to show that how this smorgasbord of attention actually can capture the similarities between the questions and the contexts. BERT is a deep bidirectional transformer encoder pre-trained on large amounts of text. It is trained on the two training objectives, including masked language modeling and the next sentence prediction. The BERTbase has 110 million parameters and the BERT-large model has 330 million parameters. If you just take this BERT model, and by just optimizing all the parameters together, it can give you a very high performance. And even if you use a stronger pre-training models, they can even lead to better performance on SQuAD. proposed in SpanBERT. So first idea is that instead of using only the masking of individual words, we propose that we want to mask a contiguous spans of words in the passage. So we are trying to mask out all these possible answer spans from the passage as our training objective. And the second idea proposed in SpanberT is-- because at the end of it, it goes around to predict an answer span. So it's actually essentially try to predict two endpoints as well as the answer. Danqi can stay for a bit to answer questions, but not forever. Because she doesn't have a Stanford login, we're going to do questions inside Zoom. So if you'd like to ask a question, if you use the raise hand button, we can promote you so that you appear in the regular Zoom window and can just ask questions and see each other. If you hang around and don't leave the Zoom for more than a few minutes, maybe we'll just promote everybody who's still there into people in theregular Zoom for some bits of discussion. If we can leverage a very large and very powerful pre-trained language model, there is a possibility that we can actually do the question answering well with only a small number of examples. And also there are some other promising directions, including unsupervised question answering so by using some kind of approach like some form of un supervised machine translation. So this kind of model is huge, like what number? How many parameters I forgot in the GPT Stream model, yeah. So it's avery large variety of of model but-- OK. Do you want to ask a question? Yes. Next is PM on the East Coast of the U.S. Next is a question about the future of NLP. Do you think that in order to solve NLP in the sense that you can perform on par with humans on all NLP tasks it's sufficient to only interact with text data? Or do you think we'll eventually need the sort of experiences and common sense that we get only from seeing and viewing the world and having a set of interactions that we as humans have? Next is an open-ended question. yeah, I guess I'm just a little worried about who comes up with the test cases? Who determines what the right answer is? I mean, we will have more discussion of toxicity and bias coming up very soon, including actually Thursday's lecture as well as a later lecture, not specifically about QA though. OK. Next person is-- Thank you for the lecture. Yeah, my question is also related to the open domain question answering. So there is some very specific designs like domain server alignments and efficient level disentanglement techniques that has shown some interesting performance on other tasks. The key difference between the generative model and the extractive model is that for generative models, you can actually leverage more input passage together and do the generation. So for this model, there isn't any retrieval. So you can always find the answer from the question, right? So this model really has you relying on all the parameters you memorized, all the information. So it's actually very hard to-- yeah, it's a definite balance between memory and the generalization from it, yeah. There is a lot of interest in extending these question answering techniques or just encoding techniques, embedding techniques to recommender systems. The first question is whether these techniques can be generalized to other languages. If we have, actually, I think that the techniques could be generally applied to other language. There has been a lot. of work trying to do cross-lingual question answering and stuff like that. But there has been some constraints that a. lot of models or systems that I described here actually require very strong pre-trained language model. and also requires lots of training examples for the Pure-DSS.

ROUGE-1: 19.27, ROUGE-2: 18.61, ROUGE-L: 18.04
BERTScore: 65.62

==============================================
==================== [41/100] ====================
Summary:
 homework two is out now. What the default projects will be, uh, for this class. Um, and you guys will get to pick whether or not you wanna do your own construction project or, um, the default project. The assignments, [inaudible] are they limited to TensorFlow? asked the question if, if the assignment is limited toTensorFlow. If you have any questions about getting setup without feel free to use the Piazza channel. We also released a tutorial for how to just sort of set up your machine last week. That's a great place to get started. "We wanna be able to deal with really complex, um, information about customers, or patients, or students, where we might have enormous state and our actions spaces. And so, when we started talking about those, I was arguing that we either need representations of models. T, T or R or a state-action values Q, or V, or our policies," he says. "With the idea being that we may in fact never encountered the exact same state again. You might never see the exactsame image of the world again, um" as, um, a mean squared error. So, we can define our loss j, and we can use gradient descent on that to try to find the parameters w that optimize. And just as a reminder stochastic gradient descent was useful because when we could just slowly update our parameters as we get more information. And that information now could be [NOISE] in the form of episodes or it could be individual tuples. When I say a tuple, I generally mean a state-action reward next state tuple. the last layer being a linear combination of those features. For most of the time when we're talking about deep RL with, um, a deep neural networks represent the Q function. So, linear value function is often really works very well if you're the right set of features, but is this challenge of what is the rightSet of features. Um, and there are all sorts of implications about whether or not we're even gonna be able to write down the true p- um, value function. to linear value function approximators. The intuition is that if you want to have sort of an accurate representation of your value function, um, and you're representing it by say, uh, local points around it. For example, with the k-nearest neighbor approach. then the number of points you need to have everything be close like in an epsilon ball scales with the- the dimensionality. Um, but they haven't far been used for in a very widespread way. Deep neural networks are very flexible representations but they don't scale very well. In practice they often work really really well. They become an incredibly useful tool in reinforcement learning and everywhere else really in terms of machine learning. So, what we're gonna talk about today is thinking about deep neural networks which also have very flexible. representations but we hope we'll be able to scale a lot better. We'll talk in a second about what these type of functions are and then we'll talk about how to use them. Instead of having our full x input, we're just gonna take in- we're gonna direct different parts of the x input to different neurons which you can think of just different functions. We think that often, the brain is doing this. It's trying to pick up different sort of features. So, we want to sort of extract features that are relevant for deciding whether or not a face, for example, is a face or not. This means also that rather than computing this sort of variance in translation, you can do this all the way across the image. Deep Neural Networks and Convolutional Neural Networks are both used extensively in deep reinforcement learning. In 1994, we had TD backgammon which used Deep Neural Networks. And then we had the results that were kind of happening around like 1995 to maybe like 1998, which said that, "Function approximation plus offline off policy control, plus bootstrapping can be bad, can fail to converge" So I think it sort of really changed the story of how people are perceiving using this sort of complicated function approximation. In a paper from 2015, a group of researchers used the same architecture and hyperparameters across all 50 games. They didn't have to use totally different architectures, do totally different hyperparameter tuning for every single game separately. It really was the sort of general, um, architecture and setup was sufficient for them to be able to learn to make good decisions for all of the games. And the nice thing is that, I think this is actually required by nature. They, they released the source code as well. DQN, deep Q-learning addresses these is by experienced replay and fixed Q-targets. Experienced replay, prime number is we're just gonna stroll data. So, even though we're treating the target as a scalar, the weights will get updated the next round which means our target value changes. And so, um, you can sort of propagate this information and essentially the main idea, is just that we're gonna use data more than once, and that's often very helpful. Learn to model is a dynamics model, a word model, and then the planning for that which is pretty cool. But we don't wanna do that all the time because there's a computation trade-off and particularly here because we're in games. So, the experienced replay buffer has like a fixed size. Um, is it the most recent and- and how do you remove items from it? It's a really interesting question. Different people do different things. But you can make different choices and there's interesting questions of what thing that you should kick out. use in that value of S prime for several rounds. So, instead of always update- taking whatever the most recent one is, we're just gonna fix it for awhile and that's basically like making this more stable. Because this, in general, is an approximation of the oracle of V star. What this is saying is, don't do that, keep the weights fixed that used to compute VS prime for a little while and that just makes the target, the sort of the thing that you're trying to minimize your loss with respect to, morestable. Our w minus, and then we use stochastic gradient descent to update the network weights. So, intuitively, why does this help, um, in terms of stability? In terms of. stability, it helps because you're basically reducing the noise in your target. Do you every update the- Minus at all, or is that [inaudible] Great question. Di- Dell? Dian. How is this really, not grade- gradient descent, like, all those assumptions? Researchers have trained an agent to play Atari games as well as humans. The agent can learn to exploit the reward function to maximize its chances of winning. The team is now looking at whether there is a pulling layer in the network to help the agent learn how to play the games. The next step for the team is to train the agent to be able to play a variety of different Atari games at the same level of performance as a human. The first step is to teach the agent how to move the paddle around. Replay is hugely important and it just gives us a much better way to use the data. Double DQN is kind of like double Q learning, which we covered very briefly at the end of a couple of classes ago. Greedy Policy is where we average networks, average reward networks, and then use one of the Qs as the target for the other network. Then we update Q2 with 50 percent probability, we pick the next action from the next network, this is a pretty small change. So, let's say you get to choose two replay backups to do. Maybe it doesn't matter if you can just pick any of these, you're going to get the same value function no matter what you do. So, it absolutely matters which two you pick in terms of the resulting value function. It matters the order in which you did, do it. If you had done S3, a1, 0, S2, your S3 wouldn't. be in this case. Lemme just to clarify, if we set alpha to zero, what's the rule for selecting tuples among the existing tuples? It's a great question. Lemme just clarify. If you are fixing, um, uh, your w minus, then, if you were looking at our case that we had before, then you wouldn't be able to continue propagating that back, because you hadn't update yet, yet, that's exactly right. So there's gonna be this tension between, when you fix things versus her propagating information back. It's very computationally, expensive or impossible in some cases to figure out. could be super tempting to try to start, by like implementing Q learning directly on the ATARI. Highly encourage you to first go through, sort of the order of the assignment and like, do the linear case. Even with the smaller games, like Pong which we're working on, um, it is enormously time consuming. It's way better to make sure that you know your Q Learning method is working, before you wait, 12 hours to see whether or not, oh it didn't learn anything on Pogge.

ROUGE-1: 18.17, ROUGE-2: 16.79, ROUGE-L: 16.85
BERTScore: 64.01

==============================================
==================== [42/100] ====================
Summary:
 salt makes up a tiny part of any bread though which has a huge effect on it and most bread is made with salt nowadays. salt has a tightening effect on the gluten it strengthens the dough and makes it more cohesive as yeast consumes the sugars in the dough. salt helps with controlling fermentation it draws moisture through the cell walls of yeast in a process called osmosis. salt can help with preserving the color and flavor of flour unbleached flour has carotenoid pigments which give the crumb of our bread the creamy color and a wheaty aroma. starter or yeast you would mix your flour water yeast or starter and then leave to ferment for several hours ahead of time before making the final dough pre-ferments add a great deal of flavor improve the texture and the keeping quality of your bread. Normally brief mints don't contain any salt that's why in hot kitchens or hot climates they can ferment too rapidly. There are ways of controlling this you can lower the temperature of the briefment you can place it in a cooler area or even lower the hydration of it by adding salt.

ROUGE-1: 17.01, ROUGE-2: 16.60, ROUGE-L: 17.01
BERTScore: 58.75

==============================================
==================== [43/100] ====================
Summary:
In this section, we're going to talk about the relativistic Doppler effect. And we make good use of our space-time diagrams, which we discussed earlier. So the question is not how this observed-- how this is seen by the source but how it is being seen by an observer. So we have to apply Lorentz transformation. So in the last section of Special Relativity, we'll look at the effect on the speed of light, and how it relates to space and time. We find delta t prime is equal to gamma delta t minus v over c squared delta x. Tau prime is then gamma times c tau over c minus v times 1 minus v square over c square. And we find then-- this is a little bit of an algebra exercise here-- that the period now is given by 1 plus beta over 1 minus beta square root of that times tau. And the frequency is the inverse. So we just calculated relativistically how the period and the frequency of a wave is Lorentz transformed.

ROUGE-1: 52.24, ROUGE-2: 47.44, ROUGE-L: 47.91
BERTScore: 75.45

==============================================
==================== [44/100] ====================
Summary:
Hollywood Legend Will Smith owns a team in the new E1 racing series that aims to prove the potential of electric power in the Marine industry. The nine teams have the same boat but they're working out how to push the tech and try to get ahead of the competition. The boats can reach 50 knots that's around 93 km per hour so how do they reach those speeds? The key bit here is getting up on the thin bits of the foil and staying above the water to have the speed that's right. attracted some big name investors despite only being in his first year. There are challenges ahead can it keep those celebrity backers and can it build a big audience for this high-tech reing. I think the ground workor for this Championship is amazing you know the names behind it now are incredible. I've heard some good rumors of teams coming in for next year as well so I Think the format is really exciting I think a lot of people are really interested in excited about the new technology and also the sustainability message.

ROUGE-1: 20.42, ROUGE-2: 19.60, ROUGE-L: 18.31
BERTScore: 60.69

==============================================
==================== [45/100] ====================
Summary:
Marginal rate of substitution of good 1 with respect to good 2 is infinity. When we are increasing the amount of 1 good to bring what will happen using this, if we use the monotonicity. What we are talking about is an indifference curve of a person who exhibit, whose preference exhibits convexity. To get 1 good you will have to give up the other good, both what we are assuming that both these items are good, means they give certain satisfaction or certain you know utility to the person. see, that MRS is given as a positive number, that only means that the author has introduced a negative sign here to convert the MRS into apositive number. So, it does not matter. Is it clear? Do you know the answer to this question? If so, please email us at jennifer.smith@mailonline.co.uk. If you don't, we would like to hear from you. Please send us a photo of your MRS and we will send it to you.

ROUGE-1: 21.66, ROUGE-2: 17.29, ROUGE-L: 16.42
BERTScore: 59.12

==============================================
==================== [46/100] ====================
Summary:
in this video we're going to discuss what externalities are in economics. An externality is when you do something that affects the well-being or the good of another person or a company but you're neither harmed or rewarded for what you did to that person so the externalities can be positive they can be negative. A negative exTERNality is where you've harmed someone you've done something to somehow impose a cost on someone or some some company or something and you haven't reimbursed that person. would help them sell their home because the neighborhood would look great but you don't have an incentive to do that why because you're only considering your own private benefit. If you do something nice and for your home then that would help their their home's value but they wouldn't turn around and compensate you necessarily. So that's a positive extra ality and for that reason these type of goods that have a positive externality would be undersupplied in situations where you have a negative externally like pollution or something like that.

ROUGE-1: 29.06, ROUGE-2: 28.48, ROUGE-L: 28.90
BERTScore: 66.56

==============================================
==================== [47/100] ====================
Summary:
Political philosophy is the oldest of the social sciences. It can boast a wealth of heavy hitters from Plato and Aristotle to Machiavelli, Hobbes, Hegel, Tocqueville, Nietzsche. The study of the great books or great thinkers of the past can easily degenerate into a kind of antiquarianism, into a sort of pedantry, says Professor Steven Smith. But these works provide us with the most basic questions that continue to guide our field, he says. "The ideas of John Maynard Keynes, both economists and political philosophers, when they are right, are more powerful than is commonly understood," says Smith. Fierce loyalty, partisanship: it is inseparable from the character of regime politics. Henry Adams once cynically reflected that politics is simply the "organization of hatreds" This raises the question whether it is possible to transform politics, to replace enmity and factional conflict with friendship, he says. Is such a thing possible? It can't be ruled out, but such a world administered by international courts of law, by judges and judicial tribunals, would no longer be a political world. The study of regime politics is in part a study of the distinctive national character types that constitutes a citizen body. This raises a further set of questions that we will consider over the term. How are regimes founded, the founding of regimes? What brings them into being and sustains them over time? For thinkers like Tocqueville, for example, regimes are embedded in the deep structures of human history that have determined over long centuries the shape of our political institutions and the way we think about them. see you back, and have a very good but thoughtful September 11^(th). See you back on 9/11/11. I'll see you back in a week or so. Thanks for your support and good wishes. I love you all, and see you on 11/11 /11. Back to Mail Online home. back to the page you came from. See you in a month or so, and I'll be back to see you in the U.S.

ROUGE-1: 13.98, ROUGE-2: 11.38, ROUGE-L: 12.59
BERTScore: 59.48

==============================================
==================== [48/100] ====================
Summary:
Ahern: I have never had an exam where I had fewer questions. There were maybe 10 questions I got on the exam and that was for a class of this side. So I hope that's a good sign. I don't like to have exams out too long before you get a chance to look at them. Ahern: Why do I videotape my exams? I find it decreases the looking around factor by a factor of about 100. [student laughs] It does, Ahern says. Professor Ahern videotapes students' answers to questions. He says he finds most students are honest. But he has had a handful of situations where dishonesty has been an issue. Ahern: "There's just too many eyes here and not enough of our eyes" to see what's going on. "I will post the key outside my door after we give the exams back," he says. "That's something that you should know. I know I will get some answers on that that don't say none" Ahern: How can you have two apparent Kcats? and the answer is it depends on how you calculate the concentration of the enzyme. If you take Vmax and divide it by the total concentration of enzyme, you will see a reduced Kcat compared to the uninhibited enzyme. The fewer the questions I have, the more points each question is worth. And so people are always worried about that. So I have to try to strike a balance. And this balance, I think works fairly well. at least who indicated time problems. Next term, the format of the exam changes in 451. Most people like 451 better because the maximum number of points on a question I think is 3. Professor: We have a lot of shorter answer questions. But we're working problems, we have to work through problems and that's why. If you're going to spend a fair amount of time on something, it should be worth more points. But literally, I rarely have much of an issue with the other two exams. Ahern: I hope everybody got how I start my lecture. The first thing I say is "Okay folks, let's get started." I don't live consciously, I just blurt it out. Alright, well let's turn our attention. Thank you for your feedback. That's not a lot of feedback, but I do appreciate feedback and I'm always happy to listen to what you have to say about exam formats. And I do take suggestions. The suggestion about having other possible choices is one, as I said, I've done and I won't rule out. S1 proteases are a class of enzymes that are called serine proteases. They all have serine, histidine, and aspartic acid as a catalytic triad at the active site. It's very easy to alter the genetic code for any of these proteases and change which amino acid is presence at any given place. When they do that, and they compare the activity, so this is the log of Kcat. The wild type enzyme has an activity up here. One 10 millionth as active when that serine is changed to an alanine. Business of creating nucleophiles is not unique to proteases. One of these is an enzyme we've been talking about some already, that's the carbonic anhydrase. Students: Would you like a joke? Ahern: Yeah. Okay, so let's stretch. So this my, this is my magic genie joke, alright? This is not a computer joke. Depends on where you lean, I suppose. "This is really awesome, "I'm a really powerful guy. "I want every woman to love me." and poof, he turns into a box of chocolates. Carbonic anhydrase can catalyze the conversion of a million molecules of substrate into product per second. Most enzymes have a fairly narrow pH range where they work that's ideal. At pH 9, the enzyme is far more active than it is at pH 7, indicating that a very, very important step is the removal of that proton. The rate of formation of the nucleophile is the critical step in the catalytic action of this enzyme like that of the proteases that you saw before.

ROUGE-1: 16.85, ROUGE-2: 15.67, ROUGE-L: 15.26
BERTScore: 66.83

==============================================
==================== [49/100] ====================
Summary:
AA and I are going to show you how to make vanilla extract so easy so yummy so good for all the things that you would use it for cakes whatever really really good isn't it. All you want for this is about 1 oz or roughly 30 G of vanilla beans so they'll be you know long Dobby whacker things and just cut them up into small pieces. Store this in a dry cool place and just as I said just shake once a day for at least a month preferably 2 to 3 months.

ROUGE-1: 32.65, ROUGE-2: 32.08, ROUGE-L: 30.61
BERTScore: 66.41

==============================================
==================== [50/100] ====================
Summary:
RAFAEL JARAMILLO: Let's talk about intermediate phases and line compounds. He recalls intermediate phase in a three-phase system. He says the solution model becomes very narrowly shaped, like a pin. All possible common tangents are going to converge at the same point, he says. "I no longer need a solution model. I only need one point," Jaramillo says. 'I can't help myself. I got to draw the mouse-face plot' Magnesium nickel system has a number of different phases. At high temperature, this Laves phase develops some width. But when you drop down to low temperature, both of these intermediate phases appear as line compounds. These are very distinct structures, and they're only occurring at very distinct compositions. That's a hallmark of an intermediate phase. That is a line compound. Let's talk about compound formation energy. It is the free energy change for formation of 1 mole of compound from elements in their reference states.  silicon tends to be a pretty useful additive for copper-based alloys. There are silicon bronzes and silicon brasses-- that is, bronze and brass alloyed with silicon. There's an infinity of examples of line compounds, so I just want to show you some of different types.have some examples here. So here is a copper silicon system. So you can see that you can get a fair amount of silicon into copper. That's 10%. That's this kind of purple region. Those are ternary systems. solid solution phases. There's this little guy here and this little guys here and then, of course, the big liquid phase. But how many line compounds are there? Trick question. Somebody, please, how many lines are there in this system? Is it three? RAFAEL JARAMILLO: It's not three. that's why it's a trick question. somebody else? AUDIENCE: Four. RAFAel JARamillo: Four Yeah. It's four. Doping semiconductors is why we're able to talk to each other over Zoom. Doping some metals into silicon is as important as it gets. Silicon isn't fast enough for electronics or you need to use light in addition to electronics. Silicon carbide is one of the leading candidates for high-power electronics that are being developed today. It has a bunch of line compounds that broaden it to intermediate phases as you raise the temperature, which is 2,670 C. The last one, titanium-sulfur system, doesn't melt till 1,670C. RAFAEL JARAMILLO: Let's look at the silicon-carbon system at low temperature. Carbon here is a line compound. Silicon is an element that actually does have a pretty wide, solid solution, great range. At low temperature, similarly to gallium arsenide, it's going to be a really boring free-energy composition diagram. It's just a triangle. So it's simplified a lot. Any other questions on the meaning or importance of this stuff? And then if there are no more questions, I'll finish up on the board. tie line there, alpha liquid equilibrium. We have internal composition variables. Chemical potentials are the same in both phases so that the coefficients are 0. The coefficient is 0. That's ensured by the common tangent construction. So this is just recalling, right? Now let's imagine two line compounds. I drew an unnecessarily-complicated example. Let me just follow through on that. B3A2 and B4A3. The Gibbs free energy of that thing is 3 times the Gibbs freeenergy of B and its reference state. It's 4 times that of pure G. RAFAEL JARAMILLO: I want to introduce this and get this in your minds. Let's imagine reacting metal M with 1 mole of oxygen to form an oxide. So zM plus O2 gas reacting to form MzO2. What's z? How do I determine z? Anybody? Does anyone know some oxides? Name for me a common oxide that you know. What is rock? What's the main component of rock? JARamillo: Silicon oxide, OK. z is an integer or a rational fraction, and it's fixed-- SiO2, magnesium oxide, Al2O3, so forth. So when we return on Wednesday, we're going to talk about the thermodynamics of this reaction. We'll use this property of being line compounds, and we're also going to use a bunch of other things as well. Back to the page you came from. Follow us on Twitter @CNNOpinion and @cnnopin.

ROUGE-1: 24.82, ROUGE-2: 22.61, ROUGE-L: 23.15
BERTScore: 61.45

==============================================
==================== [51/100] ====================
Summary:
A random variable can take different numerical values depending on the outcome of the experiment. Some of the possible numerical values of a random variable will be more likely than others. We will describe these relative likelihoods in terms of the so-called probability mass function, or PMF. The PMF is also sometimes called the probability law or the probability distribution of a discrete random variable. We use a subscript, X, to indicate which random variable we're talking about. And it has a probability. And in our case this probability is equal to 1/2.

ROUGE-1: 12.48, ROUGE-2: 12.10, ROUGE-L: 12.22
BERTScore: 72.99

==============================================
==================== [52/100] ====================
Summary:
Protein three-dimensional structure and its implications for the binding of small molecules such as drugs. The ABCs -- the alpha helix, the beta sheet, and the coil -- can be characterized by the hydrogen bonds that hold it together. How can we use these to recognize other macromolecules, other proteins and nucleic acids? We will in short order get to the scary pumpkin-like molecule. It's a catch-all phrase that includes everything except alpha and beta, but a particularly well-formed type of coil has its own nomenclature. The binding constant is measured in the molarity, roughly where you get half maximal or equilibrium binding. This is not going to be by staring at long sequence alignments where we're going to get the weight matrices. We can get them by actual experimental measures of the binding in vitro. Then I'll show you a little math how we got those apparent binding constants, which means you can get the binding constant at a lower concentration. Instead of being a single stranded nucleic acid, it's ready to bind to double stranded. It's not the only way, but this is a way. sequence that you're interested in is present. And what you do is quantitate the fluorescence of the zinc finger protein indirectly by the binding of the covalently-attached phage to the antibodies, which are fluorescently labeled by [INAUDIBLE] fluorescence. But how you relate that to the binding constant we had in the previous slide is the subject of this slide number eight. Now we call this the apparent equilibrium association constant because these experiments, just like many binding in living cells is not at equilibrium. that the helix that causes the dimerization of proteins-- you can think of this as your really most elementary protein-protein interaction code. A very fundamental one that comes again and again, so-called coil-coil, two alpha helices interacting. Can we extend this to RNA? This is a much more complicated situation with RNA because you don't have these long perfect double helices anymore. You have these very short RNA helices that I I think I can predict new regulatory protein DNA interactions with double-stranded DNA. showed in the last couple of classes. This is transfer RNA, one of our favorite molecules here, with the anticodon at the bottom of each of the pink structures. And the amino acid acceptor three prime end of that 70-some nucleotide-- 70 to 80 nucleotide nucleic acid. So the pinks are all the tRNAs, and there are at least 20 different types of amino acid and has 20 types of-- at least20 types of transfer RNAs. The topic today is proteins, and this really is the main contact between the exquisite regulatory mechanisms. Here, we need sensors to sense the environment. We need actuators to then deliver back into the environment what the cell wants to do or to interact with other cells. You have to have feedback, synchrony, so on that you can basically program the almost digital nucleic acid world inside the cell but via clearly analog inputs and outputs. We also-- I've listed some of the scariest proteins that I could think of. And we're going to talk about three of them. of the constant for that process is around 50 nucleotides per second. These are important numbers, because a typical gene size piece, say, after RNA splicing in higher organisms or naturally, it might be a kilobase. So that's about a half a minute to transcribe and translate. That could be used as a timer in a circuit of these longer time frames, like cell cycle, circadian rhythm, very long time frames in ecological systems with bamboo and various pests, and then development and aging. your accuracy, as you can get from NMR and X-ray crystallography, you now are in a position to study catalytic mechanism and design and improve ligands, such as drugs. This is really where we want to be. There may be a day where we can do this all from ab initio prediction or modeling at very great distances. But for now, modeling atvery short, say, 80% to 90% amino acid similarity, is important. We will contrast this, or show the interplay of the computational biology, that can be aided by actual measurements of drug binding. The AIDS virus changes. the wild type, the position in amino acids from the end terminus is the number, and then the third-- or the last far right letter is the new amino acids. And for example, D67N means a [? sparcade ?] at position 67 and wild type changes to an asparagine. And that causes a drug resistance in the HIV, with unfortunate consequences for the patient. We can take-- now, making mutations in polymerases is not entirely of negative consequences. There are ways that you can program, and conditional, meaning you can regulate under what conditions the protein is expressed or not or active or not with an entire domain, or with single nucleotide polymorphisms. Another way is by modulating the activity of the proteins from the outside with drugs or drug-like molecules and chemical kinetics. And under the subheadings for that, you can make these by combinatorial synthesis, which can be based on design principles, not just completely random. It can take into account what you know about the nature of the interaction of similar proteins. that we didn't discuss before. But it's related to what we've been talking about. In the case of the zinc finger, we made an altered specificity. We made new zinc fingers with bind to completely new trinucleotides. With the DNA polymerase, by changing one amino acid, we could make it now accept almost four logs better an inhibitor. And here, many different-- many of these are enzymes, where you can not just knock out the enzyme, but actually make it recognize a new substrate. think of these things in terms of proteins. You can have-- a disulfide is a very important thing to lose. They tend to be highly conserved. If you introduce a proline into what would normally be an alpha helix, this is something where knowledge of the three-dimensional structure would say, oh, that proline, this a priori, without any knowledge of conservation, could be a huge change. And then these multi-sequence profiles are a good way of looking at the conservation. That's a way of prioritizing single-nucleotide polymorphisms. Solid phase comes up again and again in arrays. It's very obvious why you have a solid phase. You want to be able to address it by its positions in x and y on the array. But the other reason-- technical reasons are, it's a fantastic way of getting purification of your products simply by washing. And it allows you to, in the case of beads, there now-- you can think of it as an ultimate and flexible array. You can move the beads around and put them in new arrays, and identify them later. This is a completely synthetic way of getting short peptides. You can think of these as drug-like molecules. These are naturally related-- they can be analogs of nucleic acids and proteins, not just straight ones. And we'll talk about opportunities for making these analogs. We'll talk more about protein synthesis as part of quantitation next time, and aspart of networks in the last three lectures. And the process is cyclic in the sense that each cycle, you return, and the polymer gets a little bit longer. capping step that can soak up any excess that was left over. After you're done with all of these cycles, then there'll be a step where you remove the protecting groups altogether and remove the polymer from the solid phase if you so choose. Or you leave it there, if you have an array. These are other examples of protecting groups. Some bases don't need protection, like thiamines. If you have a exocyclic amine, then you typically need a benzyl or an isobutyryl group. which you can make small molecule diversity which are less cyclic than processes we just talked about. These are more a set of ordered reactions that has a conceptual repeat, but in a sense, you can think of it as a linear program. And that's to make these polyketides, which are shown on the right-hand side of the slide. A large class of pharmaceuticals, including most of the antibiotics, are made by a fairly small set of organisms, such as streptomyces in certain plants. In next class, we'll talk about ways of getting direct information on protein through cross-linking and mass spectrometry. Another way is indirectly setting up these reporter assays, where you take advantage of the binding properties of known proteins to analyze two unknown proteins. This is a source of information, which is intrinsically computational in the sense that there's a large amount of it. And typically we want to select targets for solving the structures of proteins in order to look at their ligands in more detail.

ROUGE-1: 29.32, ROUGE-2: 28.33, ROUGE-L: 27.80
BERTScore: 61.19

==============================================
==================== [53/100] ====================
Summary:
More than half of large US firms plan to use AI within the next year to automate tasks that were previously done by staff in a bid to cut costs boost profits and make their work more productive. The New York Times reports that generative AI could automate activities equivalent to 300 million full-time jobs around the world. open ai's chief executive that's Sam mman says governments will need to assume the bulk of responsibility in supporting workers AI labor market disruptions and the question will employees just end up training AI systems only to them be replaced by them. Chintan: There's a danger that we focus on all of this and on what it can deliver but frankly it needs people that understand it. There is no AI without data without the network to actually connect the you know act as the connective tissue to to the algorithms that are answering our questions and giving us our responses. If you can't secure that end to end then that's where that trust issue really comes into as you mentioned around cyber security. Chintan says we have to train the Next Generation in both these Advanced AI skills but also some of the fundamental essential digital skills that we need you. We're going to focus on the impact of AI on the world workplace and how some are now fighting back around the world and the UK. There's also now a concern that intelligent chatbots could replace roles that have traditionally relied on a more human touch things like customer service or call center center. We're also going to speak to one reporter who's been researching the threat posed to workers and how they are now fight back around world. We'll also be looking at how we can make AI valuable for everybody and not just the usual people who profit from technology. And we're going back to AI decoded to see if we can teach AI how to do our own jobs. Emma asks Emma if we will actually see an increase in Union membership for private sector workers in the United States. She says a lot of workers want to feel that their voices are part of the process in deciding how AI is going to be used. Emma says the model for unions needs to be updated for the 21st century too. Emma: I think we're looking at lower level jobs as a result of AI it's like we're flipping it on its head and maybe we could actually replace CEOs or Executives. were most at risk because of automation. People are realizing you have to throw out the door any ideas you had about who is really at risk and say every job is going to be changed. We just hope that the workers whose jobs are changing have a voice in saying how yeah it's turkeyy is not going to vote for Christmas are they if those are the ones that could find their jobs being replaced but there's so much in there and Stephanie just a final thought from you in all of this about briefly if you will. this and now Emma thank you so much we'll be reading closely uh as your investigations continue uh that is it we're out of time we'll do it at the same time next week bye-bye. This and Now Emma is on CNN at 10pm ET on Monday, November 14. For more, go to CNN.com/This and Now Emmett, or click here for more information on the CNN iReport app. The app is available in the UK and the U.S. and costs £3.99.

ROUGE-1: 26.05, ROUGE-2: 22.58, ROUGE-L: 22.02
BERTScore: 62.97

==============================================
==================== [54/100] ====================
Summary:
Sadhana sadhana is a machine learning scientist at Themis Ai and the lead TA of the course intro to deep learning at MIT. She'll be teaching us more about specifically the bias and the uncertainty of AI algorithms which are really two key or critical components towards achieving this Mission or this vision of safe and trustworthy deployment of AI all around us. Sadhana will talk about how we can build very modular and flexible methods for AI and building what we call asafe and trustworthy Ai. behalf of Themis so over the past decade we've seen some tremendous growth in artificial intelligence across safety critical domains in the Spheres of autonomy and Robotics. A lot of these Technologies were innovated five ten years ago but you and I don't see them in our daily lives so what is what's the Gap here between Innovation and deployment. We'll talk about how Themis is innovating in these areas in order to bring new algorithms in this space to Industries around the world after we talk about the ramifications for this for real world AI. Commercial facial detection systems are everywhere you actually played around with some of them in lab two when you trained your vae on a facial detection data set. The biases in these models were only uncovered once an independent study actually constructed a data set that is specifically designed to uncover these sorts of biases by balancing across race and gender. There are other ways that data sets can be biased that we haven't yet talked about so so far we've assumed a pretty key assumption in our data set which is that the number of faces in ourData is the exact same as theNumber of non-faces in our Data set. A de-biasing algorithm that automatically uses the latent features learned by a variational autoencoder to under sample and oversample from regions in our data set. This approach basically approximates the latent space via a joint histogram over the individual latent variables. Once we apply these this debiasing we have remarkable results this original graph shows the accuracy gap between the darker Mills and the lighter Mills and this is the devising algorithm where once we apply the debiased algorithm we're pretty pretty pretty. lecture so so far we've been focusing mainly on facial recognition systems and a couple of other systems as canonical examples of bias however bias is actually far more widespread in machine learning. Consider the example of autonomous driving many data sets are comprised mainly of cars driving down straight and sunny roads in really good weather conditions with very high visibility. In some specific cases you're going to face adverse weather bad um bad visibility near Collision scenarios and these are actually the samples that are the most important for the model to learn. tend to amplify racial biases a paper from a couple years ago showed that black patients need to be significantly sicker than their white counterparts to get the same level of care. That's because of inherent bias in the data set of this model. In all of these examples we can use the above algorithmic bias mitigation method to try and solve these problems and more so we just went through how to mitigate some forms of bias in artificial intelligence and where these Solutions may be applied. We talked about a foundational algorithm that Themis uses that UL will also be developing today. the core idea behind uncertainty estimation so in the real world uncertainty estimation is useful for scenarios like this this is an example of a Tesla car driving behind a horse-drawn buggy which are very common in some parts of the United States. The exact same problem that resulted in that video has also resulted in numerous autonomous car crashes so let's go through why something like this might have happened there are multiple different types of uncertainty in neural networks which may cause incidents like the ones that we just saw we'll go through a simple example that illustrates the two main type of uncertainty that we'll focus on. model won't be able to compute outputs for the air points in this region accurately because very similar inputs have extremely different outputs which is the definition of data uncertainty we also have regions in this data set where we have no data so if we queried the model for a prediction in this part of the data set we should not really expect to see an accurate result because the model's never seen anything like this before and this is what is called Model uncertainty when the model hasn't seen enough data points or cannot estimate that area of the input distribution accurately enough to Output a correct prediction. names to the types of uncertainty that we just talked about the blue area or the area of high data uncertainty is known as aliatoric uncertainty it is irreducible as we just mentioned and it can be directly learned from data which we'll talk about in a little bit. The green areas of this just the green boxes that we talked about which were Model uncertainty are known as epistemic uncertainty and this cannot be learned directly from the data. This can be reduced by adding more data into our systems into these regions. epistemic uncertainty let's go back to our real world example let's say the again the input is the same as before it's a RGB image of some scene in a city and the output is a pixel level mask of what every pixel in this image belongs to. We can use bias and uncertainty to mitigate risk in every part of the AI life cycle. analyzing the data before a model is even trained on any data we can analyze the bias that is present in this data set and tell the creators whether or not they should add more samples. Themis is a company that develops and deploys trustworthy AI across Industries and around the world. We're hiring for the upcoming summer and for full-time roles so if you're interested please send an email to careers themesai.io or apply by submitting your resume to the Deep learning resume drop and we'll see those resumes and get back to you thank you foreign thank you for joining us today. We'll use capsa in today's lab to thoroughly analyze a common facial detection data set that we've perturbed.

ROUGE-1: 21.47, ROUGE-2: 20.42, ROUGE-L: 20.32
BERTScore: 56.89

==============================================
==================== [55/100] ====================
Summary:
HONG LIU: So if you want to compute, say, some scattering amplitude from alpha to beta-- so alpha's some initial state and beta's some final state. Say alpha consists of momentum p1 and PN-- or pm, and beta, say momentum p m plus 1 and pn. And then you can get this scattering amplitude just by taking your momentum-space correlation function, OK, for the n points. So this is the relation, and here I have stripped out the momentum conservation on both sides. I will explain a few things. OK, so remember this Gn, so let's go back to the definition of this Gn. So this is obtained by doing a Fourier transform. Say-- I think it would be minus sign. By doing the Fourier transforms, the coordinate-space correlation function can be written as the following. So-- yes? AUDIENCE: Right, so time-ordering of x1 to xn right here? HONG LIU: Yeah. It doesn't matter. It just has to act to the right. HONG LIU: To derive that theorem, the time-order matters. So we mentioned that if you have a diagram like this, say minus P1, minus P2, to P3, P4, and then the square root and amplitude are just given by minus i lambda, because you need to throw away all the external propagators. OK, you just need to-- you have to truncate all the External propagators, so that means you also throw away diagrams like this. You can consider arbitrary, complicated diagram, OK, as far as that only happens-- yeah. The Dirac equation is one of the most beautiful equations in mathematical physics. It's actually describing electrons, so it's not only beautiful, but it's actually useful. The Dirac theory is a prime example of how people make great discoveries often for the wrong motivations. But the key is that if you are good enough, you will find something new and that something new will be useful. We'll talk about fermions in the next section of the lecture. We're going to talk about the Dirac equations and its covariance. go to a different frame. OK, yeah, that's what we mean by-- just when you go to adifferent Lorentz frame, the equation, the form of the equation looks the same. Just different observers in different laboratory, they see the same equation, OK? OK, so but for this to be Lorent z covariant, remember, LorentZ transformation transform t to x, so immediately, you conclude that H must be first-order in spatial derivatives. OK. So the only-- then the most general way you can write it is alpha minus i.  Dirac came up with the idea of Lorentz covariant equations. He reasoned, OK, if this is constant which does not work, and then let's make alpha and beta to be matrices. Then in order for that equation to make sense, then psi has to be a n- component vector. OK, it's really, say, a stroke of genius. It's really a strokes of genius because there was nothing like this before. Just even from a mathematical point of view, it is purely, purely imaginative, OK? They're just some constant Hermitian matrices. And then, so then he reasoned that for this equation, if we want this equation to be Lorentz covariant, then at least it should have the relativistic plane wave as its solution. If it does not even have that type of solution, then you, yeah, of course cannot be covariant. So this will be satisfied-- can be satisfied if square of star, OK, satisfies reduced to the Klein-Gordon equation. equation, then you just get partial partial t square psi equal to H square psi. And then we try to make this of this Klein-Gordon form. OK, so the right-hand side, we just have the form minus i with alpha dot with this, and then plus beta m square psi. So cross term now has the form beta alpha i plus alpha i beta. But now remember, beta and alpha, they are not constant. So you have to be careful about the orders. And so now you just try to find matrices, satisfy those conditions. HONG LIU: If we take a n by n matrix where n is not so small, then we can use that --? HONG LIu: No, you get-- just we are not using those matrices in the efficient way. Yeah, this become-- you can reduce always-- yeah, just from physical purpose, you can always reduce it to 4, yeah. OK? Good? OK, so for later convenience, let's introduce a slightly different notation, so now we have this equation. HONG LIU: I know some people develop psychological fears for fermions because you have to deal with those gamma matrices, OK? For a long while, actually, I have this psychological fear myself. But these are beautiful objects if you get used to them. OK, so now I will denote-- introduce a new notation so that it looks nicer. I'll denote the gamma 0 equal to i beta and then the gamma i equal to  i times beta alpha i. And then let's all pull it to the same side, and then this becomes the following equation. they're all physically equivalent. They're just corresponding to a change of basis. So but different forms of the gamma, they may be useful for different purposes. For example, this I, solution I we wrote down before, it's convenient for if you want to take the nonrelativistic limit. OK, they make your algebra a bit more convenient. So now if you look at the Klein-Gordon equation, let's see how this is covariant. And this is a Lorentz scalar, so this is also equal to that.

ROUGE-1: 19.46, ROUGE-2: 18.53, ROUGE-L: 17.84
BERTScore: 68.42

==============================================
==================== [56/100] ====================
Summary:
The U.S. Supreme Court is the highest federal court in the United States. The job is for life, barring resignation, retirement, or removal from the court by impeachment. So far, six justices have been foreign-born, at least one never graduated from high school, and another was only 32 years old when he joined the bench. Most presidents nominate individuals who broadly share their ideological view, so a president with a liberal ideology will tend to appoint liberals to the court. Of the 112 justices who have held the position, not one has yet been removed from office. to be debated and dissected by the ultimate judges, time and history. To be debated, dissected and debated, we need to look to the past, present and future. We need to see how far we have come and what we can learn from the past. We also need to learn from our mistakes and learn from each other. We want to be the best we can be, and we want to do it in a way that honors the past and the present. We hope you will join us in this quest.

ROUGE-1: 37.21, ROUGE-2: 26.26, ROUGE-L: 23.99
BERTScore: 58.92

==============================================
==================== [57/100] ====================
Summary:
This is the second time we are having this class. We had it last year in a smaller version. That was for six units of a credit, and we had it once a week. And mostly practitioners from the industry, from Morgan Stanley, talking about examples how math is applied in modern finance. And so we got some good response last year. So, with the support of the math department, we decided to expand this class to be 12 units of credit and have twice a week, as you know. "Hopefully, this will give you enough information to decide this is a field you would like to pursue in your future career," he says. "Last year when we finished the class, we had a few students coming to work in the industry" "Some work at Morgan Stanley, some work at elsewhere. So that's really the goal. And at the same time, obviously, you will further solidify your math knowledge and learn new content," he adds. "Some terminologies will be used, which you may not have heard before" and the Sloan school here. So anyway, thanks for that. We will be doing a bit more polling along the way, mainly to get feedback of how you feel about the class. Last year we had it online, so if you feel the class is going too fast, or the math part is. going too slow or the finance part is a bit confusing, the easiest way is really just to send us. emails, which you will find from the class website. So Anyway, I will start today's lecture with a story, and a quiz at the end. Don't worry, it's not a real quiz. Just going to ask you some questions you can raise your hand and give your answer. you the story of Pi or "Life of Pi." That's not a financial story. The rest of the story, alpha, beta, delta, gamma, theta, which you will learn from Peter and Choongbum and Vasily's classes. So I'm going to talk about vega. It's a measurement about a book or portfolio or position's sensitivity to volatility. It is really a measurement or indication of how volatile, or what's the standard deviation of a price can change over time. I did that in the last 20 years. So the point I'm trying to tell you is, before you dive into any details of mathematics or any concept in finance in this class, just bear in mind, this is a field developed in. the last mostly 30 years, or even shorter. So with that, I will give you some background on how the financial markets actually started, and that's really the history part of this industry. So, financial products is really just one form of trading. There are many other ways of trading aside from exchanges. endowment support, some will come from some other form of research budget. Just borrow from the public-- local governments, states, counties, even. So, that's debt product. Commodities, actually, you know. Metal, energy, agriculture products are traded, mostly in the futures format. When you actually buying and sell, you build a warehouse to take them. You ship a tank to store above the ocean. And the real estate, you're buying and selling houses. type of player is really bank. After 1933 Glass-Steagall legislation, there were two main types of banks. One is called commercial bank, the other is investment bank. Some people blame that, and probably for a very good reason, for the cause of 2008 financial crisis. But why do we need financial markets? This comes back to what I described before. There's a need for it. It's really the need to bridge between the lenders and the borrowers. That's really coming down to the essential relationship. Brokers take principal risks, market makers make the trade happen, earn commission. Hedge funds find opportunities to profit from inefficient market positioning or pricing. Private equity funds look to invest in companies and either take them private or invest in a private equity form to improve the company's profitability. Government always have a very large impact on the market, because they are the policymakers, and the different policies they push out will generate different outlooks for the future markets, therefore, profitability. The first type is really just hedging. That means you're not proactively adding risk to what you have. In the new regulation, obviously, proprietary trading is banned, right? And so the third type is really the proprietary trader, the risk taker. These are the hedge funds or some portfolio managers. They need to focus on generating return and control the risk. So, that's where the beta and alpha, the concept comes in. It's a format. You want to beat S&P 500, so you want to basically have certain tracking of this index, but you Want to return more on top of that. I mentioned the currency example. Let me give you another example. There are a lot of people issue bonds, or issue debt. So this example I'm going to give you is, let's think about Australian corporate. Because interest rate in Australia is higher than in Japan, so typically, people like to borrow money in Japan. And they invest it in Australia. You earn higher interest rate. So let me ask you a question. Who can tell me, why don't people just do that all day long, just borrow from Japan and invest in Australia? Even if you are not a finance guy, you work in a corporate, you just do you import, export, or building a factory, you have to know, actually, what the exposure is. Risk management, nowadays, becomes pretty widespread responsibility. So, that's on the hedging side. If you're entering in a merger deal, and one company is buying another, you need to hedge your potential currency exposure and your interest rate exposure. And whatever is on the assets or the liability, or the balance sheet, you must hedge your exposure. that liquidity, and then takes the risk. They manage the book by balancing those Greeks, which I mentioned earlier. Delta, which describes the relationship of this whole book to the underlying stock, or underlying whatever currency. That's called delta. Gamma is really the change of the portfolio. Take the derivative to the delta, or the underlying spot. So, that's second-order derivative. Delta is the first order. So gamma, now you have curvature or convexity coming in. And theta is really-- nothing changes in the market. Nothing changes in your position. Vasily Kuznetsov: Arbitrage is really to find the relationships between prices, and try to profit from those relationship mispricing. He says the systematic trader builds computer models. Kuznets: Math is very useful in risk management, which I will give you some quiz -- questions -- to test your knowledge of the market. He adds that math plays a very important role to quantify how much exposure you have to the risks you have in your bank account or in the stock market. Prof. Jake Xia: "I just want to give you the sense how math and intuition and judgment can come into the same place" "When you go to the market, you buy a stock. When the stock goes up, makes bit of money, the natural tendency is to let's take profit," he says. "If you have the discipline to get out, that's great. Trading is really all about how do you risk manage," he adds. "I'm not trying to tell you which one is right or wrong, but think about it" your bank account balance is-- let's say you are a freshman student. Your bank account is $800. Your choice will be very different from someone has $100,000 in his bank account. And also, your risk tolerance, how much you can tolerate. I'm not going to give you say, this is right or wrong. But with that, let me move on and give you some homework. So, before I give you the homework, I want to make a few more comments. Do people always learn from their experiences? In science, we collect evidence, we build models. Learn the math, learn the finance first, but keep those questions along the way when you are learning during this class. Go to the course website, read what we have put up for the financial glossary. If you still have things you don't understand, compile your own list of financial concepts, which you can search on the web or even ask us. So, that's really-- and read other materials on the course work. So we got maybe-- how about this? We still got about 15 minutes or 12 minutes left, so I'll pass it to Vasily. every time. So, if you want to differentiate this functions and get a derivative, then this derivative will be quite noisy. And so, instead of getting the true derivative, you might obtain something quite different from true derivative just because there is a confidence interval around any point. So obviously, there is somewhere balance, and the question was, is there an optimal shift size to get the derivative? And that's what-- uh oh, the slide got corrupted. There was an answer. And we are implementing it in our systems and plan to use it in practice. what we had, we had the noisy observation of broker data and it was coming out at different non-uniform times. So, we decided to use Kalman filter and to study how it can predict. And that's one of the nice graphs [INAUDIBLE] produced, which again, we will use this strategy and the Kalman filters which he constructed in our e-trading platform in Moscow. Just to remind, the website is fully functional. We will be posting a lot of materials there. Probably most lectures will be published there.

ROUGE-1: 34.62, ROUGE-2: 32.92, ROUGE-L: 34.08
BERTScore: 68.47

==============================================
==================== [58/100] ====================
Summary:
Climate change is changing us from the inside out in the UK one in four adults and one in 10 children experience mental illness and there's growing evidence that dealing with a changing climate is adding to that burden. In 2022 we had the hottest year on record where daytime temperatures soed over 40° for the first time in history the past hour or so we've had the UK Met Office issuing its first ever red warning for extreme heat after the 2022 heat wave. Charles and a team of researchers set out to study how the extreme heat affected people's well-being over half of the the people they spoke to experienced negative impacts on their mental health. Clayton and Charles's Rec search has shown that there are things you can do to support your wellbeing engaging with nature. Spending more time in Green Space people use that as a way to cope with stress. We are not separate from our environment we are connected not just to the world around us but of course to one another and it and it is only in working with one another that we're going to be able to move forward [Music]"These effects are real they're they're serious and and they can be scary"

ROUGE-1: 33.28, ROUGE-2: 32.27, ROUGE-L: 31.01
BERTScore: 66.19

==============================================
==================== [59/100] ====================
Summary:
Adam Martin: How do you go from something you're interested in learning about an organism to actually identifying genes and mechanisms that are important for that? Martin: The two heroes of today's lecture will be the roundworm, Caenorhabditis elegans, and the fruit fly, Drosophila melanogaster. He also highlights a couple important vertebrate-model organisms-- the zebrafish and the mouse. Martin: Most of them are fairly small, and they're easy to house large numbers of them in a lab. regulator of this orchestra, if you will? And so conceptually, what a genetic screen would involve is taking hundreds, maybe thousands, of orchestras like this one, and just shooting an individual in this orchestra. And rather than taking a gun and shooting members of the orchestra, in genetics, you try to identify mutations. So we're looking for mutations. And these mutations could be spontaneous mutations, meaning you didn't do anything to induce it, but they just appear as a variant in the population. mutations. In the way we can induce mutations is by using some type of mutagen. If you're feeding an organism some chemical mutagen, you're inducing random mutations in different places. And you don't know which are the ones that you want until you look at their phenotypes and try to find the needle in the haystack. In model organisms, we can actually find these types of mutations in the body patterns of flies, humans and other animals, Martin says. "Wingless" mutant defined a gene that has a homologous gene in humans. Haystack: How can you identify genes that have that function in a given process? Haystack: Eric Wieschaus and Christiane Nusslein-Volhard did one of the more famous genetic screens that had been done, and they won the Nobel Prize for their results in 1995. How do you look for incorrect body patterning in flies at some stage of development that are defective in patterning? Yes, what's your name again? And so I'll show you how to do it. Drosophila larvae looks like. And you can see it has a segmental pattern here. But you see there are these segments that alternate between smooth cuticle and hairy cuticle. And because there's a lot of hairlike projections here, it reminded the researchers of a hedgehog, and so this mutant became known as "hedgehog" The hedgehog gene was the founding member of an entire signaling pathway that plays important roles in human development and also, human cancer. There are now a number of drugs that are being developed to target the hedgehog pathway. Robert Horvitz won the Nobel Prize for his work on how cells decide what fates they give rise to. His lab at MIT studied the worm Caenorhabditis elegans, which has 947 somatic cells. 131 cells, during the development of this animal, underwent programmed cell death. This is not random cell death; it's the same cells every time. A mutation that affected cell death that blocked this process is called ced-1. And what you see in this worm is a bubble-like structures that are dead cells but haven't been engulfed.  defects in the genes that were identified in Drosophila actually are relevant to human sleep disorders. So this screen identified a gene called "period" This is a gene. And there's a hammer log of the period gene in humans. And the gene in human is associated with familial advanced sleep-phase syndrome. So these mutants that the Benzer Lab identified had altered period of the sleep-wake cycle, and therefore, the gene was named "period." All right, I'm all set.

ROUGE-1: 18.23, ROUGE-2: 16.67, ROUGE-L: 16.61
BERTScore: 61.07

==============================================
==================== [60/100] ====================
Summary:
In this chapter we will discuss two applications, one price control and second taxation, so right. Sir, does this slope of this graph denote anything price demand upon, some price upon some quantity? So, wait little later we will talk about that that topic, right now we are just talking about movement and shifts, the direction of movement. We are not talking about the slope. So, what is price control what do we mean by price control? Price control is how we can regulate the prices of the goods in the market. Price ceiling is below the market equilibrium level and this is the price ceiling. Excess demand and when we have excess demand what happens there. There is an upward pressure, but by law sellers cannot increase the rent or buyers cannot pay more. So, then what do we observe in the market. Role of government. Black marketing. Black market you know under the table payment, what else. Nothing will change. If market is operating at the equilibrium price and let us say the price of 1000 rupees per month. Let us look at what happens when we have a price floor. Here you get excess supply then you have non-market rationing mechanism. Like the payment is not done at the time of buy, it is done after year or after few months. That can be one, again it is very difficult to discern it because of some other reason. And one more thing we have to pay a minimum wage to the laborers. So, to escape from that thing we include child labor. That is stretching too far. we have price control. It is to incentivize the production for a few like for wheat or if we talk if there is a new production of potato and if we put price floor on potato. So, it will incentivize production and they can serve the market better. That is also one more reason now that we do not have a proper distribution system by which we can distribute like some people with like below poverty line people, they are dying because of hunger, and we are throwing a lot of grains as surplus.

ROUGE-1: 20.69, ROUGE-2: 19.91, ROUGE-L: 20.41
BERTScore: 68.85

==============================================
==================== [61/100] ====================
Summary:
The size of the ribosome is similar to the size of a rhinovirus. There are many features in this part of the sequence that are very important for translation. Exonucleases might end up chewing up your transcript, but that probably suggests that the messenger RNA has been retired and it's going to retire to a better place. And this, once again, plays other functional roles with respect to being recognized as a transcript and helping to get out of the nucleus of the cell. fix errors is editing when you've loaded the wrong amino acid onto a tRNA. But what happens when the DNA message, the DNA starting point, is wrong, which means the messenger is wrong? So I'm just going to give you a couple of terms here. The first type of error is a nonsense mutation, which might be leaving out a base pair, inserting one, substituting one. And this would, ultimately, cause an error in the DNA. Then the last ones are the ones where we start to encounter errors in DNA. "I think this field is fascinating. Once you get used to the mechanics of it, it's really cool to think of how you go from DNA to RNA to folded proteins," he says. "Don't forget my office hours on Monday if you need them," he adds. "I'll be in the lab all day, every day, if you want to talk to me about it." "I've got a lot of questions for you," she says, "but I'll be happy to answer them."

ROUGE-1: 7.22, ROUGE-2: 6.25, ROUGE-L: 5.72
BERTScore: 56.04

==============================================
==================== [62/100] ====================
Summary:
Professor Donald Kagan: We are living in the early years of a polis sometime in the eighth century B.C. The date that's typical for the general phenomenon of colonization coming out of the mother cities of Greece is 750 roughly. The earliest date according to Greek tradition, if my memory is correct, was something like 773 where the Greeks date the foundation of what they thought was the earliest colony they ever established -- a place that they called Pithaecusa, which is the island of Ischia. the natural thing to do given the character of life, which was based upon farming, if you leave you lose your farm, and based upon the difficulty of transportation. The Greeks were, even though they went to sea plenty, they were terrified of the sea for very good reason. So, what I'm saying is, when you leave the place you were born, you leave your ancestors as well. All of that means we need to explain why these folks do what they end up doing, and we have some hints, but of course we do not know with any certainty or any confidence. The vast overwhelming majority of people needed to farm land, in order to stay alive. One answer is that the growth of population that we have mentioned in connection with the rise of the polis is still working. But I don't think that's the only answer. I think the desire for commerce would have been also--I agree with the traditional view which is that this would has been at a lesser consideration, but still very important. They would have had to be damn fools to have settled there without that being in their minds. Delphi was thought by the Greeks to be the navel of the universe, the center in every way. There the god Apollo had established an oracle. Gases would escape through this gap in the earth and there were priests who worshipped Apollo there and who took care of this phenomenon. They would place a young woman there who would sit as these gases came up and she would after a while begin to speak in tongues. She would rattle off a lot of language which nobody could understand what she was saying. will decide whether it's a good idea for him or not. Recruiting is tremendously important because you need to have a certain number of settlers to make the settlement viable. So however many that is, that is what you try to recruit and you recruit typically at a time when it's easy to get people together so you can tell them the story. The best time would be at some great festival. There are festivals held in each city just for its own citizens and my guess is that when you could do that, when you felt that you could recruit a full colony from your fellow citizens in Corinth, that's what you did. Syracuse is an independent polis, autonomous, self-governing, whatever regime it wants. It is not a subject of anybody, not Corinth or anybody else. Corinth sent out a lot of colonies, which is why we know something about their arrangements. At the other end of the spectrum it's again Corinth and they have a colony called Corcyra. The first battle between them is a navel battle and thereafter we hear of quarrelling and fighting. It's been around for a while, while a while for a colony, very early times. More time, that the overwhelming normal situation is the first one I described, friendly relations. These guys that have gone out, let us say to Syracuse, they are your people, they have relatives back home. It is natural--oh by the way, they're accustomed to worship the gods in the same way that the Corinthians do. We do know, again, Thucydides is our source, that it was customary for colonies to send representatives back to the mother city for the religious observations that were common to them all. Kagan: The question is who gave permission for a colony to go in the mother city. The best guess and that's the only thing we have. These would have been aristocratic republics at this stage of the game. Kagan: Nobody was compelled to do anything. Everybody--all of this is voluntary on both sides of every agreement. The British practicing mercantilism passed navigation acts, saying what ships could carry what and so on--nothing like that in the Greek world. By the tenth century B.C., we see Greek cities lining the coast of Asia Minor on the west, and even around on the bottom and to some degree on the north, and on the islands in the Aegean. So, there has been a Greek--what's the word I want? There is an expansion of the Greek world already by the 10th century. The way the Greeks did their immigration into Asia Minor actually had a pattern so that you can go from north to south and you will find some consistency. There are no Greek cities as you keep coming down and pass Palestine. You reach Egypt, and of course, Egypt is one of the great empires of antiquity. When you get beyond the Carthaginians advance into Spain, there are now Greek cities on the coast of Spain. There continue to be Greek cities, not everywhere, but in a spotty way into France. So is Nice a Greek town. Nice was Nikea, victory town and there are several others. So, in the period we're talking about there are no Romans that you have to worry about. interesting information from their neighbors in the east and the south and there's no question about it. Anybody who looks at the earliest Greek art for quite some time, I'm talking about sculpture and temple building, will see the influence of Egypt enormously powerfully. The Greeks are sopping up tremendously useful information and talent, and skills, and all sorts of things that help explain what's going to be coming. It is inconceivable the Greeks could have developed a civilization that they did without contact with these eastern civilizations and learned a great deal from them. More and more people would be making their living in a way that was not agricultural. They would be in commerce and industry in this small sense and doing all the various things that are not farming. So you now have classes or groups of people who have interests rather different from those of the most primitive polis you could imagine. Some scholars early on in the century, moved by Marxist theories, suggested that you had a capitalist class growing up, there's just no evidence of that. My guess is that the earliest traders of any scope were probably noblemen who also had land and estates back home. way has to be found to fit them in because, as I say, they don't fit. That creates trouble. As we shall see very shortly, that trouble often takes the form of first of factional struggles within the aristocracy, which then after awhile come to involve people outside the aristocracy. So that's the, what you might say, is the negative side of the story. But colonization, especially, some scholars have pointed out, I think persuasively, also for some considerable time provided an answer to that problem. that colonization provided something analogous to that for the Greek people. So now, here we are somewhere in the seventh century, most of these places I've been talking about have been settled, the currents that I have been describing are flowing and the kinds of problems they have give rise to what will be felt in most of the towns. That is the proper introduction to the next topic, which I'll discuss next year. No not--it seems like a year, but it's next Tuesday actually.

ROUGE-1: 23.62, ROUGE-2: 22.59, ROUGE-L: 23.14
BERTScore: 61.58

==============================================
==================== [63/100] ====================
Summary:
Instructor: We are asked, which of the following correctly identifies the areas of consumer surplus, producer surplus, tax revenue, and deadweight loss in this market after the tax? So pause this video, have a go at it. Even if you struggle with it it will make your brain more attuned to when we work through it together. All right, now let's work through this together. And I just want to sort of understand what's going on here before I even try to answer their questions. original total surplus was this entire triangle. Now the total surplus is this trapezoid that's the sum of all of these areas. And so what we lost is this area right over here. So that is the deadweight loss. So T plus W is equal to the dead weight loss. And we're done. Back to Mail Online home.back to the page you came from. Back To the pageyou came from, back to thepage you came From. Back into the page.

ROUGE-1: 26.43, ROUGE-2: 24.39, ROUGE-L: 24.35
BERTScore: 71.33

==============================================
==================== [64/100] ====================
Summary:
Sir Gawain, nephew of King Arthur, was invited to a party at Camelot. A towering knight riding an emerald steed burst into the room and proposed a game. The Green Knight declared he would allow the bravest warrior present to attack him with his own axe. If they could strike him down, they would win his powerful weapon. However, the knight would be allowed to return that blow in one year and one day. Gawain tried to forget this bizarre vision, but despite the strangeness of the knight’s game, he was determined to act honorably.

ROUGE-1: 24.43, ROUGE-2: 21.68, ROUGE-L: 22.14
BERTScore: 69.18

==============================================
==================== [65/100] ====================
Summary:
well welcome back everybody to uh the last lecture 162. this is kind of a a special lecture um i did get some requests for more information about distributed storage and quantum computing and so i think we're going to do that. Today i want to tell you about the cord algorithm which has been turned into storage systems of many sorts including those used by amazon et cetera okay facebook. Before we get there i wanted to remind you of this notion of recursive versus iterative lookups. idea okay because it's just not going to scale the billions of entries very well all right and back in the early 2000s myself and a bunch of other researchers started looking at how do you deal with peer-to-peer technologies as a way to solve this problem. One solution here is consistent hashing which i'm going to tell you about. The chord algorithm lets you get by with only knowing essentially a logarithmic number of nodes in the total system and you can still do this well. means that no particular part in the in the world here might be a hot spot it means unfortunately though that we don't have the most uh local of look up because if we start at node four it'd be nice if we could just go down to 15 and back okay now this is a really good question here about redundancy how do we get redundancy out of this for the moment uh suspend that question for just a second certainly we could put raid servers or what you know raid storage on each of these nodes and that would be great if the disks fail but uh we would like something even more powerful because i don't know if there's a big earthquake and california falls off into the ocean it'dbe nice to know that key 14 survived somehow. with dns you need to know how to talk to local at least one dns server somewhere before you can start resolving names. We're going to have this dynamic stabilization procedure so every node can run stabilize okay in which it asks its successor its current successor node who the predecessor was and then if it finds a problem it can run notify to help reconnect the ring. If you lose two nodes in a row then what i've just described to you is no longer going to work so there is a way to completely break the ring such that the stabilized procedure won't work. The power of the powerful thing about this is once i've got all these nodes now i can do a really fast routing process to figure out how to find which node is going to store the key i'm interested in. If you have log m where m is the number of nodes of the system you can end up with a situation where you can find data even if half of your nodes fail. So that's kind of what's proved in that chord paper and that's not that many because it's a logarithmic number. algorithm and so what's good about this is like i said you store the data in the cord ring and it it's very hard to destroy okay why are they called leaf sets that's a good question the reason they're called Leaf sets is because in some sense you can view the uh if you take any given um starting node like 58 and you view the set of fingers that’s a tree and so eventually you get to the leaf set and so it's like a tree with leaves so that's where the leaf is coming from and here's an example of that. with our replication but it also serves as uh as part of the last couple of hops we can use the leaf set to basically find who's supposed to have our data all right um now so let's look at replication from a physical standpoint right so if you look again at this ring i showed you a little while ago that ring is mapped physically to things that are spread widely. Now we can see another big advantage of the randomness introduced in chord and that advantage is that these copies are actually stored in geographically separate places. it adapts automatically which is pretty good okay so what i wanted to do next uh i'm going to talk a little bit about security and then um talk through a couple of things and then i want to uh try to get to quantum computing as well so we can i know there was some of you asked some questions about that so i'mgoing to leave this topic unless there's more questions okay. Security is kind of dealing with actions of a knowledgeable attacker who's really trying to cause harm and we want to make sure that uh they can't really screw us up. about by using new techniques and the distinction between protection and security i think is an important one because protection is the set of mechanisms that we talk about in this class. Security is basically using those mechanisms to prevent misuse of resources so for instance virtual memory is a mechanism that can be used for protection security policy would be making sure that when we use virtual memory we don't let malicious processes or different processes owned by different people use the same memory and have a potential for screwing each other up. making sure that you can't repudiate things that you've previously said and so i'm hoping that if you haven't taken 161 it's on your list because there's a very interesting set of things that people can talk about. The idea of a secure hash function is one where you take data and you run it through a hash function and you get a bunch of bits out of it. If you change the data even slightly you end up with a good hash function with something that essentially roughly half of the bits change. how we want to be dealing with data all right sorry if that's a lot of information but i wanted to see if there's any questions there before i switch over to some quantum computing all righty give me a second i'll be right back. So first question is how do we know the data is secured so um just like with a blockchain let me just back up to the picture here which i think is a is a good one to be talking about um what we know is the following. The vision here really is of pretty much everybody using data capsules everywhere okay and if you can get that to happen then you could potentially have a very interesting scenario here. We're working with roboticists and machine learning folks to put their data and their models for grasping and so on inside of data capsules and as a result they can reside securely in the edge in say your robots or whatever in a way that can't be breached. If any of you want to come work on this project come talk to me uh separately we have plenty of places we can talk to you. Those are particles like protons or electrons have this intrinsic spin and so now i got one and zero or up and down okay and a representation called the heisenberg representation looks at this messy physical situation like this which is either a zero or a one in these brackets. What you see here is actually a superposition of zeroness and oneness together okay now you know i realize this looks a little weird we don't normally get a wave function notation in 162. But the thing that's very interesting about this is that this is a description of a combination of zERONess and Oneness where the probabilities can be adjusted anywhere. you do a bunch of computing on it such that the probabilities are kept and you measure okay and the way it looks is that you take uh let's say you put an input with all possible combinations of the input input of the inputs being equal values all possible probabilities it looks like you're doing computation on all possible values at once but then when you measure you pick up exactly one and that's the answer you get okay. So basically what we're talking about here looks like a random computation like you get in cs70 or 170 where you randomly pick an input you compute on it you look at the result so that's not very interesting right. The difficulty of factoring rsa is figuring out how to take a large number which is publicly known and factor it into two large factors that are primes. If you can do that you break the cryptography so classically this is an exponential time algorithm and so as long as these are big enough nobody's going to break it quantum computer can do it polynomial time and let me show you how here's how it is in a nutshell you pick a random x between 0 and n that's easy. This out what r makes this equiv equation satisfied and we could do that quickly then um we win and that's something that uh you can't do easily classically but with a quantum computer what we can do. i can set up a situation where my input to my algorithm is all the possible k's uh if i take a bunch of values and i compute uh the the value x to that value and i add them all together as a superposition and i do a fourier transform what i'll find is that x to the r congruent to one as i have r go through all its possible values. question is is this something to worry about the answer is well so far no but it's looking like um it's getting closer and closer okay. We actually investigated ways of optimizing that and we could actually look at performance of different options for the shortest factoring algorithm as quantum circuits. We built a cad tool to do that so um i i don't know i think it's a pretty interesting area right now and there's a lot of interest in it all right so um sorry i kept you guys way over but this is the last lecture i figured if anybody was interested. i think it's it's pretty exciting project we got working on it if anybody's interested in that and then we told you a little bit about quantum computing and uh feel free to come ask me or also look at 151 or 191 excuse me um which is an interesting class on quantum computing all right well thank you everybody sorry for going way over today thank you for those of you that stuck around and uh i hope you have a good uh finalizing of project three andThose of you listening in cyberspace later as well you are all great.

ROUGE-1: 20.31, ROUGE-2: 19.88, ROUGE-L: 20.16
BERTScore: 63.76

==============================================
==================== [66/100] ====================
Summary:
Early astrologists did not recognize the lymphatics so they thought there are only three system there is a branch of portal or this branch of puerto bean. Originally doctors thought that every corner these three things are present and they call it portal triad. But later on of course the new it is not portal Triad it is portal triads. The liver is really working is that right that blood is again there are two input system hepatic arterial input and what was this portal venous input.

ROUGE-1: 14.07, ROUGE-2: 13.44, ROUGE-L: 14.07
BERTScore: 61.32

==============================================
==================== [67/100] ====================
Summary:
so in the next portion of today's lecture we're going to talk about how we can modify the policy gradient uh calculation to reduce its variance. In this way we can obtain a version of the policy gradients that can be used as a practical reinforcement learning algorithm. The first trick that we'll start with is going to exploit a property that is always true in our universe which is causality causality says that the policy at time t prime can't affect the reward at another time step t if t is less than t prime. the proof is somewhat involved so i won't go through it here but once we show that this is true then we can simply change the summation of rewards. Instead of summing from t prime equals one to capital t simply sum from t Prime equals t to capitalt basically discard all the rewards in the past because we know the current policy can't affect them. For a finite sample size removing all those rewards from the past will actually change your estimator but it will still be unbiased so this is the only change that we made. in the previous lecture we will get much more into this in the next lecture when we talk about extra critical algorithms but for now we'll just use a similar symbol with a hat on top to note that it's a single sample estimate all right now the causality trick that i described before you can always use it you'll use it in homework two it reduces your variance there's another slightly more involved trick that we can use that also turns out to be very important to make policy gradients practical and it's something called a baseline. grad log p by r of tau we multiply by r  where b is the average reward this would cause policy gradients to align with our intuition this would make policyGradients increase the probability of trajectories that are better than average and decrease the probabilities of those that are worse than average. We can show that subtracting a constant b from your rewards in policy gradient will not actually change the gradient in expectation although it will change its variance meaning that for any b doing this trick will keep your grading estimator unbiased. The average reward turns out to not actually be the best baseline but it's actually pretty good. different policy parameters you'll have one value of the baseline for parameter one a different value for parameter two. In practice we often don't use the optimal variance we just uh sorry we typically just use the expected reward but if you want the optimal baseline this is how you would get it all right so to review what we've covered so far we talked about the high variance of policy gradients algorithms. We talked about how we can lower that variance by exploiting the fact that present actions don't affect past rewards and we can use baselines which are also unbiased.

ROUGE-1: 32.53, ROUGE-2: 31.30, ROUGE-L: 32.34
BERTScore: 68.75

==============================================
==================== [68/100] ====================
Summary:
Today we're gonna talk about learning in the setting of games. Can you still be optimal if you reveal your strategy? It's actually not the size that matters. It's the type of strategy that you play that matters, so just to give you an idea. And then, er, towards the end of the lecture, we wanna talk a little [NOISE] bit about variations of the game- the games we have talked about. So, uh, how about if you have- how about the cases where we have simultaneous games. going to pick bucket A, bucket B, or bucket C. And then the opponent is going to pick a number from these buckets. They can either pick minus 50 or 50, 1 or 3 or minus 5 or 15. So if you want to maximize your, your utility as an agent, then you can potentially think that your opponent [NOISE] is trying to, trying to minimize your utility, and you can have this minimax game, kind of, playing against each other. game of chess. And if you think about the game of chess, the branching factor is huge. The depth is really large. It's not practical to u- to do the recurrence. So we, we started talking about ways to- for speeding things up, and, and one way to speed things up was this idea of using an evaluation function. So instead of the usual recurrence, what we did was we decided to add this D here, um, this D right here which is the depth that un- until which we are exploring. In chess, you have this evaluation function that can depend on the number of pieces you have, the mobility of your pieces. Maybe the safety of your king, central control, all these various things that you might care about. So, so the hand- like you can actually hand-design these things and, and write down these weights about how much you care about these features.and figure out what is a good evaluation function. So to do that, I can write my evaluation function, eval of S, as, as this V as a function of state parameters. how learning is applied to these game settings. And specifically the way we are using learning for these game. settings is to just get a better sense of what this evaluation function should be from some data. And, and that kind of introduces to this, this, um, temporal difference learning which we're gonna discuss in a second. It's very similar to Q-learning. Uh, and then towards the end of the class, we will talk about simultaneous games and non-zero-sum games. at a simplified version of it. So, so what are some features that you think might be useful? Remember the learning lecture. How did we come up with, like, feature templates? Yes. Currently, still bound with the [inaudible]. So maybe like the location of the X's and O's. The number of them. Yeah. All right. So okay, so that was my model. So now, the question is where do I get data? Like where and because if I'm doing learning, I got to get data from somewhere. So one idea that we can use here is we can try to generate data based on our current policy pi agent or pi opponent. So, so that's kind of how we do it. We call these policies. We get a bunch of episodes. We go over them to make things better and better. So then we generate episodes and then from these episodes, we want to learn. These episodes look like state action reward states and then they keep going until we get a full episode. And then we had a target that you're trying to get to. And my target, which is kind- kind of acts as a label, is going to be equal to my reward, the reward that I'm getting. Is it possible to have, an end state and not end state have the same feature vector, or no? If you use like, uh, initialize rates do not be zeros which you update throughout instead of just to the end. If there were kind of the same and have same sort of characteristics, it's fine to have feature that gives the same value. If it is always 0, it doesn't matter like what the weight of that entry is. So in general, you wanna have features that are differentiating and you're using it in some way. The idea of learning in games is old. People have been using it. In the case of Backgammon, um, this was around '90s when Tesauro came up with, with an algorithm to solve the game. And he was able to reach human expert play. And then more recently we have been looking at the game of Go. So in 2016, we had AlphaGo, uh, which was using a lot of expert knowledge in addition to ideas from a Monte Carlo tree search and then, in 2017, we have AlphaGo Zero, which wasn't using even expert knowledge. Minimax sca- strategy seemed to be pretty okay when it comes to solving these turn-based games. But not all games are turn- based, right? Like an example of it is rock-paper-scissors. You're all playing at the same time, everyone is playing simultaneously. The question is, how do we go about solving simultaneously, okay? So let's start with, um, a game that is a simplified version of rock- Paper-Scissors. This is called a two-finger Morra game. So, we have player A and player B. We have these possible actions of showing 1 or 2. And then, we're gonna use this, this payoff matrix which, which represents A's utility. If A chooses action A and B chooses action B. And this is called the pay-off matrix. So, uh, so now we need to talk about what does a solution mean in this setting? So, so what is a policy in the setting? And, and then the way we refer to them in this case are as strategies. Someone tells me it's pi A and pi B, I can evaluate it. I can know how good pi A is, from the perspective of agent A. But with the challenge here is we are playing simultaneously, so we can't really use the minimax tree. So what should we do? So I'm going to assume we can play sequentially. So that's what I wanna do for now. So right now I'm gonna focus only on pure strategies. I will just consider a setting- very limited setting and see what happens. that's the question we're trying to answer. Okay? So, so let's say Player A comes in, and Player A says, "Well, I'm gonna reveal my strategy to you" So the value of the game, uh, would be, maybe I'll write it here. Pi A is already this mixed strategy of one-half, one- half, right? It's going to be equal to Pi B times 1. So what I'm really trying to do as Agent B is to minimize this, because I don't want Agent A to get anything. The idea of the Prisoner's dilemma is that you have a prosecutor who has to decide whether or not to convict someone. So, next 10 minutes, I want to spend a little bit of time talking about non-zero-sum games. So so far, we have talked about zero- sum games, where it's either I get the negative reward or I get some negative reward. There are also these other things called collaborative games where we are just both maximizing something out of both of us. It's a single optimization that's kinda like a single maxim. look at them, would be useful for projects. And with that, I'll see you guys next time. Back to the page you came from! Back to Mail Online home. Back into the fold! Back To the pageYou came from: Back To The Page You Came From. Back from the Page you Came From: back to the Page You came from.back to thepage you came From: Back to The Page you cameFrom: The PageYou Came from: ThePageYouComingBack to: ThepageYouComing Back to:

ROUGE-1: 16.46, ROUGE-2: 15.54, ROUGE-L: 15.46
BERTScore: 64.23

==============================================
==================== [69/100] ====================
Summary:
The famous example was posed by Comte de Buffon back in the 18th century. It marks the beginning of a subject that is known as the subject of geometric probability. The problem is pretty simple. We take a needle that has a certain length-- l-- and we throw it at random on the plane. The needle might fall this way, so that it doesn't cross any line, or it might fall that way, and it ends up crossing one of the lines. If the needle is long enough, it might actually end up crossing two of the Lines. that these days, physicists and many engineers use methods of this kind quite often and in many important applications. The method is used in a variety of scientific and engineering applications. For more information, visit: http://www.physicists.org.uk/physicist-methods-of-this-kind-used-in-many-important-applied-technologies-and-practical-practices.html. For information on how to use this method in your own research, visit www.physiologists.org/technology/technological-technology.

ROUGE-1: 12.64, ROUGE-2: 9.90, ROUGE-L: 10.83
BERTScore: 62.62

==============================================
==================== [70/100] ====================
Summary:
Professor: We're describing interaction between the electromagnetic field and atoms. Professor: If you have a perturbative treatment, even if you carry to infinite order, you have, formally, divergences if you have resonant interaction. I want to show you what are the tools to treat those infinity source diverGences in a consistent and a systematic way, professor says. MIT OpenCourseWare is a free, online education resource. To make a donation or view additional materials from hundreds of MIT courses, visit MIT Open courseWare at ocw.mit.edu. In perturbation theory, we can sum up an infinite number of diagrams. We want to understand the time evolution of this system. Our tool is a time evolution operator. We have a problem with one state. Our problem is the excited state b. And we have a resonant excitation state a. So this is a discrete eigenstate of the unperturbed Hamilton operator. And therefore, we actually divergent terms which are 1 over Z minus Eb. And this is just to make the connection, this is sort of a description that in the complex plane and can have imaginary values. The T-matrix is actually the matrix, the relevant matrix, of the time evolution operator. R has a real and imaginary part. One is a self energy, and the other one is a decay rate. We can go from here to there with any combination of states you want. But one thing is not allowed-- to involve the state b. We have now found an exact expression for Gb of Z. And this is the main result, which is the non-trivial expression. light scattering. I just go now and apply to an excited atomic state. So the state we are interested in is the atomic state b and no photons. And the property of the atomic. state is obtained when we know the function Gb of Z. And this is the matrix element between state B0B0 and the time. evolution operator. So we are calculating, of course, the Fourier transform of the time evolution of the state b by the. Fourier. transform through the resolvent G. Professor: In general, a real part and imaginary part. It's a function of the initial energy E. And it has an imaginary part, and it's the same if you have the function 1 over x. The imaginary part gets us Fermi's golden rule. And the real part has actually-- remember when we discussed the AC Stark shift. The AC Stark Shift has a 1 over [INAUDIBLE] dependence. So this is actually nothing else than theAC Stark shift not due to a laser beam, but due to one photon per mode. we calculate here-- has no resonant structure at the energy Eb. So therefore, we can neglect the energy dependence of that and simply replace the argument E by the energy we are interested in, namely energies close to Eb. This replace, neglect E and set, or replace the dependence by E, by taking the value at Eb. And we obtained, as promised, the imaginary part, which we can approximate by Fermi's golden rule. And you should now-- well, this is what we may have expected. But there are two things you should learn. The first thing is that the exponential decay would be different if we had not made the Markov approximation. The density of states of your photon field is pretty much constant around here. And then, this approximation is excellent. So I hope what you have learned from the treatment is number one, where the exponential decay comes from. If we hadn't done the infinite summation of diagrams, if we had done a perturbative expansion, we would have never obtained exponential decay. We would have obtained some polynominal decay. So it's pretty much an exponential function is non-perturbative. In general, if you write down the total Hamiltonian and do the time evolution, something will come out which, in general, is very complicated, very entangled. So you have to know, to keep track of all the photons, which have been scattered in the lifetime of an atom. But often, what we do is we simply put the photons in a trash can. We trash them. We're not interested in what the photons are doing. We really want to focus on what happens to the atom. How does an initial state of the atom propagate into a final state? This is done by optical Bloch equations. it causes stimulated emission. And it causes absorption described by the Einstein b coefficient. And you have a similar equation for the excited state. So this is clearly the semi-classical limit of what we want to accomplish. We want to know more. We really want to find the full quantum time evolution. We have to be careful. The time evolution as a Hamiltonian, if you now bring in the environment, cannot be simply included by adding an imaginary term. This here violates the unitary time Evolution. This is actually something which is the frontier of our field, both in theory in ion traps and with neutral atoms, that we have some evolution of an atomic system by coupling it to the environment. So can you engineer the environment in such a way that it does something really fancy to your system? Well, you can dream of it. But you dreams are restricted by the mathematical structure of all possible master equations in the world. Because the environment cannot do everything for you. The environment can only do for you what can come out of all Possible Hamiltonians. Damping is a consequence of the fluctuation dissipation principle. We do not get any form of damping without at least the fundamental quantum noise. The tool which we use for that is the density matrix. The density matrix can be written as a probabilistic sum over states. This will actually play a major role. We will make certain models for damping. And it's really beautiful. On Monday, I will give you the beam splitter model for the optical Bloch equation. Because it's a microscopic model. processes. OK, any last questions? Well then, let's enjoy the open house with incoming graduate students, and I'll see you on Monday. I'll be back on Monday to talk to you about the classes we'll be teaching next year. Back to Mail Online home. back to the page you came from. Click here to read the full transcript of this interview. Back To the pageyou came from, click here to see the full Transcript of this Interview. Back in the page, please share your questions and comments.

ROUGE-1: 22.57, ROUGE-2: 21.03, ROUGE-L: 20.80
BERTScore: 67.02

==============================================
==================== [71/100] ====================
Summary:
David KAISER: Today, we're going to pick up where we were on the most recent classes. We'll look at the coalescence of what came to be called the Big Bang model. And then we'll see how cosmic inflation emerged from that particular moment to try to address some of the shortcomings while retaining some the successes. And the asterisks are to remind you there's a set of strictly optional lecture notes on the Canvas site which go into a little bit more detail of some of these parts from the lecture. or our own Milky Way galaxy. Or if we zoom in even closer to home with the solar system or even really in human terms, there are concentrations of enormous matter and energy and activity separated by huge voids. The question is, what could account for that structure across these scales from meters or kilometers up to tens of billions of light years? It turns out that ordinary gravity-- even Newtonian gravity, let alone Einstein's fancier version that we looked at in class, general theory of relativity-- that these gravitational frameworks are sufficient. Einstein's equations describe the shape of the universe depending on the amount of matter and energy. If the universe had a uniform density of stuff per volume, it would have a positively curved surface. But if the universe was overdense, space itself would warp back onto itself. Einstein thought this was horrible, but other colleagues showed it was consistent with his own equations to have universes that would change over time, that could either expand or contract. The evidence for this began to come in the late 1920s with enormous telescopes. to collect information about not just the distribution of distant galaxies, but could also measure how rapidly they were moving with respect to us. And Hubble found this remarkable trend that the further away from us a given galaxy was, the faster it tended to be moving. And the basic trend holds. There's some interesting deviations. But the idea is nonetheless evidence consistent with our universe expanding, not just having a shape to it, but actually stretching and getting larger over time. So you can actually then work backwards and say for how long has our observable universe been stretching? When did this stretching or expanding phase begin? George Lemaitre was an ordained Catholic priest. He was also an MIT trained PhD astrophysicist. He studied briefly in Cambridge, England with one of the first converge to general relativity, Arthur Eddington. Then he came to MIT to finish his PhD and then was finding many of these solutions to Einstein's field equations even before Einstein did. And it's consistent with the beginning of that expansion being not quite 14 billion years ago, billions of years ago. It's also consistent with things moving further apart from each other on average today. Georges Lemaitre wrote about what comes to be known as the Big Bang model. He was calling it a primeval atom, that there was this initial fireball from a very, very hot, dense state. After the Second World War, new groups began coming back to these somewhat old questions. They realized that if the universe was very hot and dense at early times, then the conditions in which these elementary particles would find themselves should be quite different than what we find commonly around ourselves today. At early times in cosmic history, the universe should've been opaque. You literally wouldn't have been able to see anything because the mean free path of any given photon would be very, very short. As the volume of space stretches, as you have an expanding universe, the average temperature of all the stuff inside it should fall. This is all work that they predict around 1948, '49, '50, Alpherov, Gamov, Herman and Alpher. It should be filling the sky in every direction, a uniform glow. In the early universe, charged particles had a mean free path to cross the dance floor. Gamov, Alpher, and Herman were putting real numbers to try to make sense of these different phases of the very early universe. Very high temperature, early dense state should be qualitatively different in its behavior than a later lower energy state. They predicted as early as 1948 that there should be this remnant glow from the Big Bang, all those photons that only then at 380,000 years after the Big bang were able to start streaming freely. 20 years later, two radial physicists working at Bell Labs, Robert Wilson and Arno Penzias, were using a new horn antenna sensitive to radial microwave and radial band frequencies. This should've been among the most precise instruments available on the planet for that band of the spectrum. They couldn't get rid of a residual hum. At one point, they climbed inside that huge horn antenna on their hands and knees to scrub out what they graciously called special dielectric materials from pigeons who had made a nest. In the early universe, there were photons that were too high energy to form stable, electrically neutral atoms. The photons are coming from every single region of space. They're like sitting in a bathtub full of these photons. And they're just losing their energy as the overall size of space continues to grow. And all that's changing is their average temperature per photon is the main idea. The average energy per photon had fallen steadily since the time when they were first released in that early dance party. Every part of space that we could see today once had been at the Big Bang, so to speak. The Big Bang happened there. It happened at x equals 0, and x equals 1, equals 2. Any place we could put spatial coordinates to on this model, those all experienced the Big bang at the same time. So this is a safe, remarkably successful set of ideas that eventually becomes called the Bigbang model. And people began to get worried about some of these features of the model for some of its successes. what we call conformal time, often labeled by the Greek letter tau, is basically a variable tick rate that is really convenient because then we can start making a dynamical changing spacetime look just like the spacetime of special relativity. In these special, very simple fine coordinates, light just travels on 45 degree diagonals. And then light comes to us at 145 degrees. That sounds very abstract. We do that all the time. This is just an example in space time of a conformal mapping of a sort that we all use every day like a Mercator projection. stretching our time coordinate so time gets more and more stretched out towards earlier times. We can take that into account. We know how to use a Mercator projection. And it makes other relationships remarkably easy. Likewise our conformal maps here make the paths of light, for example, very easy to follow. When people begin using these convenient coordinates, they also go back to some questions about or features of the Big Bang model, and they start having new questions. It's the same Robert Dicke. So he introduced this conundrum in 1969, so soon after the discovery of the cosmic microwave background radiation. stuff per volume, the actual density of matter and energy per volume. If omega is larger than 1, you have more stuff for volume. You expect it's open or hyperbolic geometry. So far so good. Then Dicke plugged this quantity into Einstein's own equations. A universe should generically become more and more different from flat over time. And that seems like this very strange or unexplained fine tuning. If the universe has been stretching for 14 billion years, what set it to be so arbitrarily close to flat is an unstable equilibrium point? or even exchange a single tweet, to have absolutely no information of this part of the sky. How could they have become indistinguishable in the signals we receive today? That became known as the horizon problem. It became clear that signal really is uniform to one part in 100,000. It's remarkably uniform to a tiny fraction of a percent. That signal is uniform across every direction we look in the sky today, even though it's coming from all these regions that when that light was emitted couldn't have possibly had any physical interaction with each other. like circa 1980. He was not originally asking questions about the cosmos, but he was haphazardly encountering some of those questions, again, very much like Tony Zee around the same time. What Alan was interested in was in things like spontaneous symmetry breaking and the Higgs mechanism. He realized that this kind of feature could actually lead to a cosmologically distinct kind of evolution. If you have a time even briefly during which the energy density, the amount of stuff per volume gets stuck, then the energy could remain constant. That stretch of growth in the size of space will stretch exponentially. grows very fast in time-- while the energy density remains nearly constant. Now you'd see this expression, the deviation of the universe from spatially flat. That deviation should rapidly fall to 0. The universe today should look indistinguishable from a flat universe because the difference from flatness was driven to 0 dynamically. By having even a very brief phase of exponentially rapid accelerating expansion, you drive the universe towards a flat shape rather than having it flow away from aflat shape. So the latest measurement from the Planck collaboration using a satellite is that this parameter in our actual universe today is 1. If inflation happened, there should've been a very brief period before what had previously been called the Big Bang. So we're adding more real estate along our time axis. We're unfurling a little bit extra time that hadn't been taken into account in the standard Big Bang model. And so the universe was so tiny, it could very easily have been in a kind of equilibrium or at least a causally self-connected state. So during this tiny blink of an eye, the universe grew exponentially. higher energy photons in the CMB and slightly lower energy photons. And the idea now is that the regions of the sky from which these photons were emitted are telling us about the very, very tiny unevenness in the distribution of matter and energy. They have been mapped by three generations of satellites above the ground with increasingly precise ground based measurements as well. Each of these came out 10 years apart with an increase of about a factor of 30 in the angular resolution of thesky. And so we can now make plots like this. Inflation says similar kinds of things should've been happening in the earliest moments everywhere in space through this very violent, rapid stretching of space. At the moment when the electrically neutral atoms start to form, if there were this sea or bath of gravitational waves, then the high school auditorium-- imagine where this dance is happening. These are mathematically more complicated structure. You should have this periodic squeezing, stretching, and then inverted by 90 degrees. So a version of these were found by the LIGO collaboration and announced early in 2016. March of 2014, a team using the BICEP satellite at the South Pole announced they had actually measured exactly that corkscrew pattern. Unfortunately, pretty soon after that, it turned out the BiceP team had measured data consistent with local noise. So basically, the signal they had hoped to measure was actually swamped by foregrounds they had not yet been able to control. And this was found by a number of very sophisticated analyzes soon afterwards. So it remains an open question to this day whether these primordial curling, twisting patterns really can be detected. Maybe there's such small magnitude, it'll evade our detection. makes specific predictions for what we should see on the sky today, including very minute statistical predictions for things like the cosmic microwave background radiation. The simplest models fit to unbelievable accuracy despite what my mean dormmates used to say in the mid '90s. Now we have extraordinary agreement with many, many of these predictions, albeit not the final one. So why is the universe lumpy? Why is this cascade of scales? Because space time is wiggly, and matter is jiggly.

ROUGE-1: 25.21, ROUGE-2: 24.10, ROUGE-L: 24.05
BERTScore: 60.89

==============================================
==================== [72/100] ====================
Summary:
The Peloponnesian War was fought in 431 BC. After the war, Spartan power had grown to an unprecedented degree. For the first time there were lots of Spartans, who had lots of money. The Spartans had choices that they could take. They could either stay in the Pelop onnesus, or they could contest it in their power to control the entire Greek world in the east. Or they could have some control of the Aegean and the Hellespont. should be no democracy in Athens. It was an easy point of view to arrive at in 404. People who were not friendly to the democracy could simply point to the fact that the democracy had just lost this great war. And it was exactly the kind of idiotic idea that a democracy would come up with. So, that was the basic widespread view of what was natural in the Greek world. Now, however, when they set up the Thirty to rule Athens, they agreed to the idea. Spanish Armada was heading for England trying to gain control of the island for the Pope and Catholicism and one thing and another. Great wind came up and it blew the ships out of their path and wrecked many of them. From that day forward there sprang up the legend in England of the Protestant Wind, which had come along to save the new English faith against the forces of the Pope. Well, if they can invent a Protestant Wind I think it's okay for me to speak about the democratic snow that fell on Phyle that went. forces in the state had come, and we might mention also that the ancient sources estimate that something like 1,500 Athenians may have been killed by the Thirty tyrants. When the Thirty brought an army out to try to defeat him there he defeated them. They were forced to flee to Eleusis on the northwestern frontier of Attica, and the democrats were in position to take control of the city again. The Thirty were deposed by the 3,000, because it was obvious they were losers.

ROUGE-1: 7.02, ROUGE-2: 6.48, ROUGE-L: 6.72
BERTScore: 56.89

==============================================
==================== [73/100] ====================
Summary:
The coherent state has a simple definition, simple but subtle. It's an eigenstate of the annihilation operator, and it has a complex eigenvalue alpha. The person who popularized those states was Glauber, and he got amply rewarded for that. We are now using the coherent states to look at any other quantum state of the electromagnetic field, any statistical operator which describes photons by forming the diagram matrix element of the statistical operator with alpha. And now we realize that coherent states are not as wonderful as I believe. In quantum mechanics, number and phase are complimentary. If the number of photons is fixed, you know nothing about the phase. The energy is sharp of a number state, since the energy is e squared. But what you get is also something blurred on the order of unity. And I want to say something about that in a second. Finally, we discuss the time dependence. And this is almost like the disclaimer now. Just to give you a bigger picture, we want to achieve with quasi-probabilities what we can achieve with phase densities. The second order coherence function for classical light, which has a classic description, is always larger than 1. But quantum mechanically, we will see that the g2 function is not necessarily larger than1. And that's actually an interesting-- you can see-- litmus test for the quantumness. And then you have a final state of your harmonic oscillator, and your zero-point fluctuations has zero squared fluctuations. So what is more closely related to an experiment is how you measure the correlation function. I'm not sure if it's right, but it's consistent. By the way, if there is a question, I sometimes make a question mark in my notes. And when I post the notes, the question marks are eliminated. So this is now drawing our attention to the single photon. And this is our next subsection. This shouldn't come unexpected. It comes from the quantization of light. And that only happens when you go down to similar photons. This is when certain fluctuations are most pronounced, because the energy is dependant on a singular photon. Vladan Vuletan: Coherent states, as I've just shown you, are very classical. They've always a g2 function of 1. And attenuation is not changing it. So a coherent state with an expectation value of 1 photon is not a single photon. But of course, there are ways how you can get single photons. And this is, well, you start with single atoms. Namely, if you have a single atom in the excited state, it can emit only one photon. So that's a way how we can create non-classical states of light. Hanbury Brown Twiss experiment involves two photodetectors and a beam splitter. The first photon is observed by the first detector, and the second photon is detected by the second detector. In the quantum version, especially when we have a single photon, only one detector can go to this extreme case. If you put a light bulb into a cavity or couple the light from a light bulbs into a fiber, the light becomes spatially a single mode. That's the only way how you can distinguish a lightbulb from a laser beam.

ROUGE-1: 10.35, ROUGE-2: 9.65, ROUGE-L: 9.84
BERTScore: 63.13

==============================================
==================== [74/100] ====================
Summary:
This lesson will first dive into some signal Theory and then move on into things that we're more familiar with things like deconvolutions and using Transformers for next note prediction. The first thing we want to talk about is how can we sample and quantize a continuous time signal. We'll also talk about the different kinds of models that we've been talking about and how they can be used for generative audio. The last part of the lesson will be on how we can use Transformers to generate sounds using these models. periods and two by quantizing our level so instead of dealing with A continuous scale we can quantize at certain levels for example a frequency of like 2 4 6 8 Hertz what this allows us to do is come up with discrete points. The analog to digital converter uses something called the sample and hold circuit. The motivation behind this is that we we maintain a signal that's the pass band where we allow every signal to pass when we hit certain cutoff frequency we Wane our signal by a certain factor. voice and Pitch it up very fast right what quantization level do we do we want there. Can we do a lossy pitch up with a uh with by filling in the the blanks in some intelligent way through prediction or kind of note fitting which is an interesting consideration I think given the fact that audio is a continuous time signal um the digitization process and the choices you make matter a lot and because of that this field is so interesting and there's a lot of really didactic work around how we can take these continuous signals discretize them. is less than double of the highest frequency present aliasing will happen. This asserts that you need at least two samples per period. aliasing is the byproduct of poor sampling. A lower wave resolution will result in a modified output signal as compared to the original input that we're trying to process. There's a lot of literature about um aliasing effects including spatial illnessing. The aliasing phenomenon is incredibly interesting this happens both visually and auditorily. It's a very prevalent problem in any problem. familiar with perhaps in terms of how we can use those to reconstruct signals and ultimately how we Can use Those to generate audio right predict the best uh kind of next node um so looking at the next the next step here we want to use deep learning for reconstruction right where we are are reconstructing a low quality audio to high resolution audio right um and this is this is the kind of uh model framework um that we can used for this um you might notice it really closely resembles a unit which is something that we talked about during image segmentation. Transformers can help increase our sample of training data and generalize key scales and beats throughout a data set. A single song can be transformed into 12 songs of different Keys. The more data you have the better your model will be and the more generalizability you have in your Transformer the better it'll perform. Transformers will far outperform classical methods of of both computer vision and natural language processing. It's easier for machines to predict keys without flats and Sharps which has you know similar to what humans do. hidden state memory Transformer memory specifically to this model enables very fast inference for music prediction right we've done a lot of things to optimize for for our prediction we're including a beat embedding so that's not something it has to learn. We're able to get a sense of of relative position with Transformer XL whereas vanilla Transformers will use Absolution absolute position only. It's important for music models to know the position of each token relative to one another because positionality matters right the order that you're playing the notes really is is what matters the most. are the original Pachelbel's Canon um as you can see this does deviate a bit but honestly it sounds pretty good. The Transformer model is able to do this next note next sequence prediction pretty pretty well. So yeah there's a a lot to do in this field um a lot of really cool things happening um and yeah I hope you guys learned something about uh about generative audio today and are inspired to kind of give some of these things a try yourself. thank you guys for tuning in have a good one.

ROUGE-1: 18.03, ROUGE-2: 17.01, ROUGE-L: 16.82
BERTScore: 68.07

==============================================
==================== [75/100] ====================
Summary:
Professor: The amygdala is closely connected to the basal forebrain. Professor: The abnormal brain connections that we know occur, at least many types of schizophrenia. He says the earlier the lesion, the greater the plasticity, the more chances of sprouting the connections that are the basis for this idea. In green, there are the very widespread catechol-oligosynaptic projections, which show the extent of the brain's plasticity. In blue, the acetylcholine containing neurons that you see in the medial septum, which we mentioned last time. Early in evolution, there was no dorsal striatum. It was a link between the olfactory, [INAUDIBLE],, and motor control. And we also know the outputs of that region go to hypothalamus and subthalamus. They influence the endocrine system and motivational states by these projections. They also have some connections in the midbrain, where they can influence the stacking patterns, especially locomotion. If the prefrontal cortex is functioning abnormally because of sprouting of these axons, then binding to the receptors will move it more towards the normal. unique in that way. And it's critical for the balance of this system. So anything that goes wrong with the subthalamic nucleus can cause major problems with movement. OK, so let's stop there today. We're going to take a break from talking about the human body. We'll be back next week with a look at some of the other animals in the animal kingdom. Back to Mail Online home.Back to the page you came from. Back To the pageYou come from.

ROUGE-1: 8.83, ROUGE-2: 7.18, ROUGE-L: 6.45
BERTScore: 59.33

==============================================
==================== [76/100] ====================
Summary:
Learn how the solar cell device converts sunlight, the input energy, to some usable output energy, which is in the form of electricity, typically, from a solar panel. Learn how to minimize the amount of light reflected or not absorbed into maximizing amount of life that's actually absorbed. Learn about the duality of light, or how to think about light as a particle, or alternatively, as a quantified particle. Use the weekly Newsquiz to test your knowledge of stories you saw on MIT OpenCourseWare. very simple yet very powerful formulation that describes not only the interaction of light with the solar cell material but also light through the atmosphere, light the water, many other forms of optical absorption. And for that, I'd like to call Joe up for a quick demo that will allow us to actually plot out Beer-Lambert's Law. What we're going to be doing is taking many sheets of material. This is just some polyethylene material, a little bit discolored. And we'll be inserting these panes of plastic in the middle. in some function to that medium and a certain amount of light is transmitted, we know, of course, from our little experiment that it follows some exponential function. So if we assume that the change of intensity within that medium in each little delta thickness is going to be affected by some sort of scattering intensity within the medium. We collapse the n and the sigma here into an alpha. That alpha is an absorption coefficient. The general equation is the same one that drives the reduction of light intensity as it travels through the atmosphere. We're going to look at two different materials, silicon and gallium arsenide. And we're Going to calculate the thickness necessary to absorb 90% of the incoming light at 550 nanometers. Did anybody manage to walk all the way through that calculation? TONIO BUONASSISI: I'm glad people are asking those questions. And then over the next few classes, we'reGoing to get to exactly what physical processes are going on. But I'm starting to get into the semester, so the energy level starts going down. There is a limit to how much we can trap light simply by modifying or corrugating the surfaces to enhance the optical path length. A gentleman by the name of Eli Yablonovitch, who's now a professor in Berkeley calculated these parameters I think back in 1982 and came up with an upper limit to the optical paths length. And that's a pretty good litmus test for the ability of a material to trap light. If you have silicon, for instance, with a refractive index of, let's say, in the infrared some around 3.6, your Yabonovitch limit is around 50, relative to the thickness of your material. fancier ways of light management as well that don't involve light trapping necessarily but light manipulation or even semiconductor manipulation. You can, for instance, change the wave length of the incoming light. If we can eliminate the longer wavelength stuff out here, which is heat, performance of most solar cell suffers when they get hot. And so if we manage to do spectral up converting or reflect that long wavelength light away from our device, we can improve performance there as well. So again, I wanted to really emphasize that light management is necessary devices. a coating on the back to reflect the light back so that it gets a second bounce through the material. I've engineered the front surface, texturized it so that we have not only the benefit of two bounces, double the chance of light going in, but also the Snell's Law working in our favor. And so all told, the one reason why this boost is so big right here is because I'm increasing the optical path length. And as a result, I'm getting a much larger current output. I'm absorbing much more light inside of my material. surface of a material, let's say right here, then you can cause each node, each point within your material, to lag by an increasing amount, so that your wave front now bends. And that will cause the light, essentially, if you trace through the points of maximum intensity, say the pink, you'll see that the light is bent. And so it's really exciting. There's stuff coming up every day on light trapping and light management. Mostly it's for photonic devices, but they can be transferred over into solar cells. today in class, don't let that constrain your thinking. That's my final message. Thanks. Back to Mail Online home. back to the page you came from. Click here to read the rest of the article. Back To the pageYou came from: Back to thepage you camefrom. Click HERE to see the full transcript of this week's episode of "This Is Life With Lisa Ling" with Lisa Ling, Lisa Ling and Lisa Ling on "Lisa Ling: The Next Generation"

ROUGE-1: 12.05, ROUGE-2: 10.95, ROUGE-L: 10.99
BERTScore: 60.81

==============================================
==================== [77/100] ====================
Summary:
Professor Steven Smith: I want to look at two sets of issues. One is Locke's theory of the constitutional state, particularly focusing on the role of the executive, vis-a-vis the legislative branch of government. The other is thinking about Locke and the American regime and the current state of political philosophy, modern contemporary American political philosophy. Smith: Locke doesn't endorse necessarily one particular form of government from any other. He is an advocate of what we have come to call limited government, of constitutional government.

ROUGE-1: 3.06, ROUGE-2: 2.85, ROUGE-L: 2.88
BERTScore: 68.24

==============================================
==================== [78/100] ====================
Summary:
JACK HARE: Let's do a little recap on electron cyclotron emission, and then we will go on to a few other things. We didn't actually derive the emissivity of it, but we gave ourselves a hand-wavy reason why there may be multiple peaks here. And these peaks are going to be occurring at frequencies-- I'm going to switch into angular frequency units, omega m. And they're evenly spaced. And this is just for a single particle. that, the frequency depends only on the magnetic field. And so, if you see some emission at some certain frequency, then you know that it's been emitted by a region of plasma. And that was particularly useful when we considered a tokamak because, if we have some magnetic field that goes 1 upon R, then different regions of our toroidal device are going to be emitting electron cyclotron emission with different frequencies. And this is a technique which will give us, by looking at the spectrum for lowish frequencies, we can work out what the temperature is as a function of space. This is correlation ECE, often called CECE. We want to measure temperature fluctuations within the plasma that are maybe on the order of 1% of the baseline temperature, delta T upon T about 0.01. And that 1% is actually extremely hard to measure. And this is because the noise is just too high on these systems. There are lots of different contributions to the noise. But, in general, they all add up to make it very hard toMeasure these very small fluctuations. want to understand in plasmas so that we can build an economically viable fusion reactor. So it'd be very nice to be able to know. Now, the fact that the noise is too high does seem like a big limitation. But there are some clever tricks that we play where we use correlations. And I'll talk now about what exactly these correlations are and how they provide us with information that allows us to get a signal out despite the overwhelming [INAUDIBLE]. So our setup here is borrowed from ASDEX Upgrade. We're not trying to measure the temperature profile throughout the entire plasma. We want to measure it inside some very small region. So the size of our turbulence of R turb, is on the order of 100 microns. That's the width of a human hair. We're zoomed in on only quite a small frequency range. Each channel samples a non-overlapping region in frequency. And, therefore, in real space, because, again, we've got this very strong link between our magnetic field and our spatial position. Many oscillations. And so, it will average out to 1 or something like that. So it's some DC offset that we can subtract off after. So all this is doing here is mixing the signal down so it gets to a regime that we could effectively digitize. Yeah, another question. STUDENT: [INAUDIBLE] JACK HARE: So if we were doing geometric optics, which we're not, then you would have a lens like this. That lens could collimate your beam. And if it collimated that beam, that would mean there'd be a focus point at some distance f away. very small region on the order of 100 microns. Anyone know how micro splitters work? Looking at you? STUDENT: RF. JACK HARE: The answer I got was RF, which I don't think is much more satisfying than my answer. But, you're right, you could come up with some clever analog way of doing the splitting. But this is getting way beyond what I was hoping to talk about on this. So let's get on to correlation ECE see what that does. The technique is used to measure temperature in plasma. It involves two adjacent channels, S1 and S2, measuring from adjacent parcels of plasma in real space or inside the tokamak. The two signals are carrying the same components that we're trying to measure. We are going to find that the noise is uncorrelated at random, but these two signals will correlate together, and we'll be able to measure it. And, again, these are really, really closely spaced, 100 microns or so apart. do correlations, and I'm not going to go into them, but I will give you a citation at the moment. We get out a term here that looks like the temperature term squared, the thing we want. And because this noise is just random, when we do some sort of averaging, this could be in time or best to think of it is a short time integral, then these terms are all going to drop out. And we'll just be left with a correlation signal that is proportional to the temperature squared here. fluctuations in a tokamak. And this is something that has revolutionized our understanding of turbulence. Now we can finally characterize it. That was a lot. Any questions? We're going to move on from ECE [INAUDIBLE]. Yes. Any other questions? Anything online? Now we are going to go to the next segment, which is about turbulence in the Earth's atmosphere. The next segment is about the atmosphere in the atmosphere of the Earth, and how turbulence affects the atmosphere. Bremsstrahlung is a term used to refer to a type of radiation in the universe. It involves electrons being deflected and breaking and emitting these photons. There are lots of different ways to deal with bremsstahlung to do the actual calculation. The main thing we can say in a classical treatment is that the bremstahlt is going to be isotropic in every direction. That's actually quite different from electron cyclotron emission, even though we didn't really look at the anisotropy of that in any detail. here with the ions and electrons as point particles. There's a semi-classical approach where you start bringing in some quantum physics and treat, I think, the electron as a wave. And then there's a full quantum approach. What's remarkable about all of these approaches is they all give the same answer with just a very slightly different coefficient. So we get a small change in coefficient. The scalings are the same. So in some sense, although it's important to get the exact coefficient, it doesn't matter exactly which one of these techniques you use. bremsstrahlung emissivity, this is equation 5.3.40 in Hutchinson. This is ne, ni, z squared, T to the minus 1/2 e to minus h nu upon T. And then, there's a factor called G bar, the Gaunt factor, which we'll talk about in a moment. And these constants are things like e, the electron charge, and the electron mass, and epsilon 0, and h bar, and c. There is no mode in a plasma which propagates below the plasma frequency. So it goes back to being optically thin here for the high energy photons. But the low energy photons will get absorbed as well. There's actually another effect, which is in Hutchinson's book, which I haven't covered here. But we're skipping over that this year. But it's in there if you're interested. Yeah. Yeah, I think someone told me that maybe for a high field tokamaks synchrotron could start being significant. But I actually have no idea how big a deal it is. mostly on the magnetic field and the radius. And those are two things in a tokamak you already know. But maybe there's a really clever diagnostic you can do, like fast particles or something like that. So worth thinking about. JACK HARE: Right, but I'm not interested in [INAUDIBLE]. This is a diagnostics course. Any questions online while we pause? So this is bremsstrahlung radiation, and often people call this free-free. Professor Jack HARE: We're going to switch to a slightly quantum model of the atom. We have a range of different discrete energy levels that the electrons can occupy. These energy levels are labeled by the principal quantum number n. And, up here, infinity, this is ionization. If your electron gets this much energy, it becomes free again, HARE says. But, of course, there's another case where this photon takes away so much energy that this starts, I guess it becomes imaginary. Bolometry is, in some sense, a very simple diagnostic. It cares not at all about the detailed spectrum of what the emission is. It just wants to know how much power is being radiated by the plasma. We have a little sensor sitting at the wall of our vacuum vessel. And that sensor is just a resistor. It has a resistance M, and that resistance M is a function of temperature here. And, from that temperature, we can make an estimate of the radiation power incident upon it. have a thick block some distance in front of it so it can't see the plasma. It's the heat transport kappa grad T that gives us the time constant for thermal conduction through the substrate from the absorber to the resistor. And, effectively, this tau here sets the timescale at which we can measure. So the larger tau is the slower our measurement of the radiated power is going to be. And if tau gets very large, because we've got a very thick substrate here, or it doesn't have very good heat transport, then we're going to have a very poor time resolution. difference between those. neutron damage leads us to use much thicker substrates, which gives us a longer time response and so, therefore, a worse bolometer. So, ironically, the bolometers that will be used on [INAUDIBLE] are significantly worse than the bolometer used on existing devices. And I'm sure that [? Spark ?] will have exactly the same problem. Almost everyone in the world uses this design of bolometer that they pioneered on ASDEX Upgrade in the '80s. No one has come up with a better system yet.

ROUGE-1: 25.83, ROUGE-2: 24.65, ROUGE-L: 24.80
BERTScore: 70.13

==============================================
==================== [79/100] ====================
Summary:
So, last time we were starting to talk about the sort of the general overview of what reinforcement learning involves. We introduced the notion [NOISE] of a model, a value, and a policy. Can anybody remember off the top of their head what a model was in the context of reinforcement learning? So what we're gonna do today is, sort of, um, build up for Markov Processes, up to Markov Decision Processes. And this build, I think, is sort of a nice one because it allows one to think about what happens in the cases where you might not have control over the world. The definition of a return is just the discounted sum of rewards you get from the current time step to a horizon and that horizon could be infinite. If the process is deterministic, these two things will be identical. But in general if theProcess is stochastic, they will be different. So, if we go back to our Mars Rover here and we now have this definition of reward, um, what would be a sample return? So, let's imagine that we start off in state s_4 and then we transitioned to s_5, s_6,. s_7 and we only have four-step returns. Markov Decision Processes are the same as the Markov Reward Process except for now we have actions. So, the agent is in a state they take an action, they get immediate reward, and then they transition to the next state. The reward can either be a function of the immediate state, the state and action to the state action and next state for most of the rest of today we'll be using that it's the function of both theState and action. If it's trying to do action A1 and for action A2 it'll move to the right unless it hits a_7 and then it'll stay there. Otherwise it will generally stay in that state. you do this computation. Just to quickly check that the Bellman equation make sense. So the immediate reward of this is zero plus gamma times [NOISE] 0.5. And that's just an example of, um, how you would compute one Bellman backup. So that's back to my original question which is you seem to be using V_k without the superscript pi to evaluate it. Oh, sorry this should, yes. This should have been pi. That's just a typo. There exists a unique optimal value function. The optimal policy for an MDP and an infinite horizon finite state MDP is deterministic. There are two choices for every state and there are seven states. The [NOISE] number of policies is |A| to the |S|. It's a mapping from states to actions so it's gonna be 2 to the 7th. We'll talk about- probably a little bit clearer to when we talk about contraction properties later. Um, any one want to take a guess of whether or not the optimal policy is always unique? I think there might be cases where it's not. strict inequality if the old policy was suboptimal. So, why does this work? So, it works for the following reasons. And the next questions that might come up is so we know we're gonna get this monotonic improvement, um, so the questions would be if the policy doesn't change, can it ever change again? And is there a maximum number of iterations of policy iteration? Here iterations is i. It's a kind of how many policies could we step through? So, in terms of policy iteration, this is very similar to what we saw before you can think of it in these Bellman operators and doing this argmax. Wanna see if we can get to a little bit on sort of the contraction operator. So this is what, um, value iteration does. It's a very similar policy iteration and evaluation. So, if an operator is a contraction it means that if you apply it to two different things, these are value functions, then the distance between them shrinks after.

ROUGE-1: 9.52, ROUGE-2: 9.16, ROUGE-L: 9.05
BERTScore: 66.01

==============================================
==================== [80/100] ====================
Summary:
 angular momentum is a set of operators that provided observables, things we can measure. We found that they satisfy a series of commutators in which lx with ly gave ih bar lz. We showed that any component of angular momentum, be it lx, ly, or lz, commutes with l squared. And therefore, we set up for the search of those wave functions that are simultaneous eigenstates of one of the three components of angularmomentum. wanted single valued wave functions-- wave functions would be the same at phi, and at Phi plus 2 pi, which is the same point. You must choose m to be an integer. For the l squared operator we also explained that the eigenvalue of this operator should be positive. That is achieved when l, whatever it is, is greater than 0. And the discussion that led to the quantization of l was a little longer, took a bit more work. Happily we have this operator, and operator we can diagonalize, or we can find eigenstates for it. Professor: If you have a system you want to figure out what are the properties of the states. In general, you will be led in any physical problem to look for the maximal set of commuting operators, he says. Professor: We'll begin the hydrogen atom and this task why? Having a proton and an electron we can reduce this system to as if we had one particle in a central potential, so that will be also very important physically. He says the answer was simple. You have a particle with some momentum in one direction or in the reverse direction. In general when we choose a general l, if you choose an arbitrary l, then m goes from minus l, minus l plus 1 all the way up to l. If you choose state with l equals 1, or eigenfunctions with l equal 1, there is the possibility of having m equals minus 1, 0, or 1. The spherical harmonicas are going to be those wave functions. And they have a normalization, n l m, an exponention, and all that.

ROUGE-1: 29.98, ROUGE-2: 28.72, ROUGE-L: 27.23
BERTScore: 75.17

==============================================
==================== [81/100] ====================
Summary:
In order to do that, I basically have to do the integral. So here it is. We have psi of x and t. It's integral dk phi of k e to the ikx minus omega of kt. If you want to see the distortion, you have to keep that [INAUDIBLE]. We'll do that in a week from now. And then, you say, look. There's lots of things making it look like a difficult integral, but it's not as difficult as it looks.

ROUGE-1: 15.65, ROUGE-2: 15.13, ROUGE-L: 15.65
BERTScore: 62.97

==============================================
==================== [82/100] ====================
Summary:
There are three common uses of a rotation matrix. The first is to represent an orientation. The second is to change the frame of reference of a vector. And the third is to rotate a vector or frame. To demonstrate these, I will use these three coordinate frames, representing the same space with different orientations. To help you visualize these frames in 3 dimensions, I’ll use my handy tinkertoy frame. This is the z-axis, this is the x-axis and the y-axis. we will learn how to represent the angular velocity of a frame. We will also learn about how to use the frame to represent a frame's angular velocity. We'll also look at how the frame is used to represent an object's speed. We hope you will join us for the next few weeks of classes on how to work with a frame in this class. The next class will be on the physics of the frame in which we are working. The final class will take place in the next week or so.

ROUGE-1: 35.22, ROUGE-2: 26.36, ROUGE-L: 29.92
BERTScore: 66.09

==============================================
==================== [83/100] ====================
Summary:
Today, we'll talk about coded imaging. It's a form of a co-design between how you take a photo and how you recover the sharp detail afterwards in software. We'll see how-- we already have some projects that are inspired by biological vision. And I believe Santiago-- where's Santiago? Oh, yeah, his triangle-- the piston, kind of-- so some really great ideas. So I'm glad a lot of these concepts are coming together in the final projects. It is going to be very popular. capture the image and how you process the image. In a typical film camera, or even it is digital camera, you take the picture, and that's basically the end of the story. So when you try to recover this information, you start getting this banding artifacts. And we'll see it in the next slide, why that happens, and how to get rid of the banding and noise in the image you're trying to capture. Back to Mail Online home. back to the page you came from. have basically a 1D convolution that's converting this image into this image. But the Fourier transform has some zeros, so you cannot divide those frequencies by 0 and recover an image. So the culprit here is really this box function, which is equivalent to-- when you release the shutter, opening the-- release your shutter button-- opening the shutter and keeping it open for exposure duration and closing it. But that's not the most effective. So what if you change that? What if you open and close it in a carefully chosen binary sequence? frequencies-- they're all preserved. Of course, they're attenuated. It's not as high as-- it's not 1.0.0, it's reduced. Maybe it's 0.1 or so. So there is still some hope to recover this photo back from this because, in the denominator, we will not have seen. The problem is each of the frame will be extremely dark. So you are basically adding up a lot of noise. Every frame is dominated by noise. that's your 1010 inquiry. Instead of keeping the shutter open for the entire duration and getting a well-exposed photo, the shutter is open for only half of the time. The support for the representation of the Fourier domain of that function that you describe there is infinite, right? So you actually truncate this in order to-- RAMESH RASKAR: It's not infinite because you still have some width. AUDIENCE: Right, but you have infinite high frequencies there by the sharp conditions. CNN's Ravi Agrawal is a senior producer at CNN.com. He's known for his work on deep learning and artificial intelligence. He recently developed a tool that can detect motion in moving objects. The technique can be used to create a virtual blur in a moving image. It uses a filter that is dependent on distance and speed to create the illusion of motion. The technology is still in its early stages, but it could be used in the future to help with other types of imaging. your shutter? RAMESH RASKAR: When you're-- OK, so when you're setting up your shutter, if the car is moving really slow, and you don't expect it to blur by 52 pixels, then using a 52 sequence is overkill. Maybe you should use a new sequence that's only about 10 long or 11 long, right? So it's just like-- AUDIENCE: OK, but that's just so you can get more light. RAMESH: No, that's so that it's most optimal for that setting. to engineer activity of the camera. So in this particular case, a point that was moving created a blur like this. And by engineering the time point spread function, it stops looking a bit like that. And then it just turns out that this one is easier to deal with than this one. So that that's why a computational camera is doing the computation not just in Silicon but also in optics. And the circuit is very, very simple. You just take the hot shoe of the flash, and it triggers. When you lose the shutter, it triggers the circuit. would you apply spatial coding? AUDIENCE: Coded aperture? RAMESH RASKAR: C coded aperture. So this is coded exposure, coded aperture-- very easy. And all you're going to do is put some kind of a code in the aperture of the lens. And this is how, actually, it started in the days of-- in scientific imaging, especially in astronomy, coded apertures are very well known. And so I thought, it must be useful for something in photography. Lenses are very carefully designed by camera makers to be the same plane where you put your aperture. When you change your f-stop and decrease it and increase it, it's all happening in the center of projection. When it's in focus, it doesn't really matter what the code is, so the photo will be half a square, so you're talking about the light, so half a light, right? And that's why you have some dust on your lens and so on, unless you have the lens on your front. digitally. In 1D, this is what we saw, right? Its Fourier transform is flat. So there are 52 entries here, and almost all of them are the same. Now we're saying, think about the problem in 2D. And what's the Fourier transforms of this? So first, for this one, the Fouriers transform is-- as we see, it's black. And then if you take that in 2 D-- so how is the code? I'll give you a hint. the values will be constant. So if we're placing a broadband code, certainly we have an opportunity to recover all the information. It's much easier to think about convolution and deconvolution in frequency domain than in primal domain. In communication theory, everything is [INAUDIBLE].. We think about carrier frequencies of radio stations in frequencies. And we think about guard bands and audio bands. And that's the same thing that's going on here. And convolution, deconvolved-- much easier than frequency domain. At the end, the solution is very easy-- just flutter the shutter. don't know whether this matters. But you're right. If you're looking at something that's-- we have bright lights in the scene. At a distance, take our false photo. They will all look like this. Or you could put hearts in it, or, like-- AUDIENCE: Right, yeah, I was thinking maybe, that's totally possibly. [LAUGHTER] RAMESH RASKAR: So an interesting art problem is how do you create-- how do we create a mask that visually looks aesthetic but is mathematically also invertible? the depth. When it comes into sharp focus, my edges, that must be the right depth. Unfortunately, it doesn't work out in this case. The main reason is that, because it's coded aperture, no matter where you refocus, it still looks like it has very high frequencies. So you need to find this 7-by-7 pattern or even the previous case, the 52 pattern. Take a Fourier transform to see if it's flat. If it's not flat, you go to the next one. code and do a gradient descent and so on. There's no good solutions for 2D. But for 1D, there are some really good solutions to come up with that. For 2D, for certain dimensions, they call it one more 4 or three more 4. Basically, when you divide by 4, the remainder can be 1 or 3. And there are certain sequences that are beautiful mathematical properties, of which sequences could have broadband properties and which may not. So it turns out you cannot-- there's a little bit of cheating going on here. filter to the beginning of the signal. This particular filter is actually not circular, but it's linear. So when you apply the filter here, when you start applying the filter at the end of the image, you don't go back to the front. It turns out, for circular convolution, the match is very clean and beautiful and smoother course work. Or for linear convolution,. there is no good mechanism. So we came up with our own code called RAT code, R-A-T, which is after three quarters. In astronomy, you have circular convolution because they use either two mirror tiles and one sensor or one mirror tile and two sensors. If you're tiling the mask at aperture, but you are using single-tiled aperture, you'll get horrible frequency response. In this photo, those frequencies are not lost because all the frequencies are preserved. But that's because our eyes are not very good at thinking about what the original image could be, given either this one or the previous one. It's just pure x equals b, x equals a backslash b. Ramesh Raskar: There's just one way of engineering the point spread function, one in motion and one in focus. He says for any continuous code, there is a corresponding binary code that will do an equally good job. RaskAR: Eventually, it's going to have a 2D projection. "It's amazing because motion is time, and the focus is space. They're completely orthogonal. So you can play with it," he says of motion and focus. A lot of it is actually being used in cell phone cameras. They put this face mask between the object-- near the lens so that the image does not come into sharp focus ever. The benefit of that, it turns out, is that it preserves the spatial frequencies, and it has the benefit that, no matter which steps you are at, you have the same defocus blur. So whether if I hold myself on camera, whether I'm here or here or at infinity, I get the same amount of blur. Ramesh Raskar: Compressed sensing is taking a photo and compressing it. He says the idea is to take a single photo and then recover it in a compressed way. Rasksar: If you're on 2 megapixels, then you need to take 2 million [? pics] All right? So the claim this group made at Rice University was that if I wanted a million-pixel image, I don't have to really take a million readings, he says. Rasa: I can take this picture effectively with just 10,000 pixels but recreate a million pixel image. which is how to write a paper and wishlist for photography. Which isHow to Write a Paper and Wishlist for Photography: How to Write A Paper and Write A Wishlist For The Camera. For more information on writing a paper or wishlist, go to: http://www.cnn.com/2013/01/30/photography/how-to-write-a-paper-and-wishlist-for-photography-how- to- Write-A-Paper-And-Wishlist.html.

ROUGE-1: 23.18, ROUGE-2: 21.05, ROUGE-L: 21.49
BERTScore: 67.96

==============================================
==================== [84/100] ====================
Summary:
Prof: You know why I am dressed up? When I do this course and when I do the first half of the French course I do a lecture on the bourgeoisie, the middle classes. Middle class was a form of self-identity that was constructed in the way being a worker was constructed, or being a noble. So, as a result, look at this. I wear this about once a year. Unfortunately, I wear it to funerals. The last time I wore it was something Bill Clinton had, some mutual friends. The middle classes started dressing like this in the nineteenth century, dark with a little bit of color. When you see Daumier or you see Delacroix's famous Liberty Leading the People, and you see the bourgeois, there with his top hat, he's dressed in a bourgeois uniform like this. That emerges out of the bourgeois century. The French Revolution, and here's an important point, opened the way by removing legal blocks in very many places to the open talents of each soldier. At the very top there are the great bourgeoisie, the big bourgeoisie. These are people who are big financiers. Then at the bottom you have the petty bourgeoisie, and everybody making things together. The middle class likes to see themselves as useful. You find lawyers reaching in there and, slowly, doctors. Doctors increase a self-identity and become more important in the nineteenth century. Notaries know all of the secrets of people with money. Can you imagine going to a conference like the World Congress of the Petty Bourgeoisie? All they do is start up your body and look at your body. as a jungle. Then you have to imagine this as a ladder, like this. Social mobility is the goal. You want to have enough money to leave to your 2.2 children. Then to really make this go you'd have to have vines up here like the jungle. And then you would have to grease this pole through bad economic times. Let's say in Europe 1816-17--don't write this down, if you do, you're compulsive--I'm compulsive. If you can't get credit because people withdraw the credit, same thing. Then here you go. Look out. "This is how the people on the bottom part of this ladder viewed the demands of the working class. They want to vote, too. What if they vote and somebody wants to raise your taxes or something like that? Boy, that's scary. But what's down here? This is ordinary people. This is your jungle and you're trying to make it up there to the big time. The chances are that in these bad years you're going to fall down. But yet lots of people get up and the ranks of the middle class increases everywhere" man doing? He's counting his money. That's a very nineteenth-century profession, as it is for every subject. They still had arranged marriages. Love could count for something, but marriages were still essentially, less so for the middle classes than for ordinary people, but economic relationships. They were economic relationships, wrangling over the dowry and that kind of thing. The bourgeoisie didn't kiss and hug a lot. But he's got his hand draped rather daintily on the old guy's arm. He's not about to embrace him and give him a big kiss on each cheek. LZ Granderson: Buildings reflected how much money you had, and where you lived reflected how well you had. LZ: The more you go up there, you're still within the middle class. People were aware of what these symbols meant, he says. He says the emergence of the public concert as opposed to the chateau or church concert is another theme. Lz: The piano replaces the harpsichord in J.E. Plantinga, who's a retired professor of music, has got great stuff on this. I was trying to explain how people on the top rung were trying to beat down people at the bottom. I ended up smashing this umbrella, sort of the imaginary of somebody smashing their guitar onstage. But the point is that the umbrellas come with the middle class. They're not these big colored things you have now. It was the idea of protecting that one suit. I'm from Oregon. We didn't carry umbrella, because it rained all the time anyway and I'd just lose it. until you have universal male suffrage, by how much taxes you paid and how much property you own. Property reflects one's belief in one's social worth. No longer was it the worth of blood. So, they formed these national guards, particularly after revolutions and after 1848, or after 1830. For all the variety within the middle classes, so beautifully depicted by Daumier and other people, they still, when push came to shove, shared an awful lot in the bourgeois century, that of the nineteenth century.

ROUGE-1: 18.75, ROUGE-2: 17.70, ROUGE-L: 17.27
BERTScore: 68.85

==============================================
==================== [85/100] ====================
Summary:
Researchers have confirmed a second smaller space Rock smashed into the sea off the coast of West Africa creating a large crater during the same era. It would have been a catastrophic event and scientists say it would have caused a tsunami at least 800 M High to tear across the Atlantic Ocean. It's exciting that it happens to be potentially close to the same time as the chicku event 66 million years ago known to be the the main cause of the extinction event that killed the dinosaurs and 75% of life on Earth absolutely fascinating. meters of water it hit would have generated a pretty significant tsunami. It may not have been a global event however it wouldn't have been large enough to have say caused a global catastrophe in the way that chicks Loop did. It's worth saying we don't know the exact timing because it hasn't yet been drilled H really interesting. It could have occurred at a similar time and and it's worth say weDon't know exactly when it happened. It was in the early hours of the morning on the morning of September 11, 2001.

ROUGE-1: 60.69, ROUGE-2: 55.65, ROUGE-L: 52.53
BERTScore: 72.05

==============================================
==================== [86/100] ====================
Summary:
In automotive design Dynamics plays a very important part. In Dynamics there two sides to it one is here's the system what is its trajectory going to be in other words how will its various degrees of freedom behave over time. The Yang is you here's a system how do you modify it so that it behaves in a way that that's analysis so that's the first kind of the yin. The yin and the Yang are the same thing, they're just different ways of looking at it. that you want all right and that is called it starts with a C control so control is given a dynamic system how do you put an actuators Motors Rockets Etc so that the trajectory is what you wanted to be. So we're doing Dynamics end of the course we're going to solve the dynamical equations dou4 is controls where you actually try and put in extra things like cruise control so to make the system behave in a way you want it to behave so if you have a rocket open loop right it's called open loop and open loop system is a system that is not where you don't close the loop. AJ: I'm a little woozy today uh I had some serious drugs this morning prescription drugs and the result is that I might start babbling but you won't notice because I Babble anyway right so all right Sam didn't say that I said that I bet but anyway s was very respectful okay so uh in the last class we did uh uh we did a problem essentially the whole Pro class was we looked at you know the skier situation and there's a handout AJ's published right on the web where we do the energy formulation and we solve the problem. class but he also solves the problem the direct way and by coincidence the problem not comes out to be the same is it a coincidence no I'm joking it's not a coincidence the answer is the same both ways. Today we're going to do angular momentums we're still in points Point masses but this is the stepping stone the link to rigid bodies then we'll do a problem we're very problem oriented in this class and then we'reGoing to do yet another problem but we'regoing to do multi- particle. There's no class next Monday and um we posted the solution to that problem from class uh just one last thing I won't have officers today only because you don't want to hear me Babel I'm really sick um but I'm also going to change my officers um several people suggest so the timing isn't right um so we'll talk about the end of class but I I might go to like a Monday off M um like later on a Monday or maybe Wednesday later or something like that okay does it make sense would you prefer it not be midday Wednesday? I'm referring to something with no Dimensions but with a finite Mass you know that someone asked me the other day and I just want to be sure to say this. Let's say that you have a particle Point Mass heading that way some direction and let's define its velocity we'll call it a v p okay so I have um two questions both of which you probably know the answers to the first is what is the angular momentum of that particle just from your memory go ahead say it aha. this this this thing acceleration right so this is going to come out to be M or in fact I can re you know I can just kind of write the terms simply like this R QP cross m a acceleration of Point P right that's that term H that's right we'll write it yeah we'll get there that's exactly right okay exactly you're getting but you're you're exactly right so give me I'll write in The Next Step all right let's write this what do we do with this guy it's an ugly term right what do you do withthis guy here's what we're going to do. be clear this whole thing is this term and this is thisterm now let's let's write this The Next Step what is this what this m a acceleration of P or acceleration of p with respect to a hm H it's the force on particle P right cross product rqp yes now so I'm just going to put a dotted line so you know that that's what this is is this I don't think you'll disagree let me just write some draw some lines I'm trying to save space as I said. There is this term and you need to know about it okay it just so happens the term vanishes in many situations but it's key that you know see for Force f is equal to D by DT of P momentum completely coer Crystal Clear when you come to angular momentum it's say artifice yeah there's this funky term which vanishes. The reason this will become important is that when you look at rigid bodies it it actually you will end up doing something so this is a silly term but these are the two conditions yeah Q is fixed. it's not Kosher can't do it if I do I need to include that term right and you you've got a figure that a simple thing like my a robot is a very typical Dynamics application okay common mistake okay now that we've done this let's try and figure out why angular momentum is useful let's solve a problem any questions about this any questions bottom line if you're taking angle momenta about moving points be careful bottom line many conditions it'll be okay we'll elucidate we'll Express we'll we'll nail this conditions. she helps Congressional uh you know Congress analyze things from a physics point of view so you know when Katrina occurred someone asked if it would be possible to change the temperature in the when when a hurricane approaches to dissipate the temperature. She did some analysis and showed that you need something like a nuclear weapon but like you know the most the largest nuclear weapon ever conceived to even you know impact it by like 2% because the energy in a in a hurricane all right I told you I'd Babble all right let's do a problem here's a problem so imagine a um a table right. call this Theta the initial length is L one and the initial velocity is we'll call it a scalar because this is how I'm defining the problem of V1 when we actually solve it we might have to define a vector okay. As the thing's going around this person is going to pull the string down and as it kind of goes around it's going to spiral in and end up it's a new length L2 and the question is what is V2 going to be okay. an acceleration in this direction what else is happening maybe there's a potentially a tangential acceleration kind of an Oiler acceleration as well potentially right kind of thing okay maybe it's a drag slowing down speeding up we don't know right so in that direction there might be some acceleration okay so there's going to be a you know a net Acceleration kind of this way do you think we do right so what is the force on this in 2D where is the Force coming from on from this is so where does force get applied on this the string so the string is applying a tension force onthis right are there any other forces gravity is not an issue here are thereAny other forces no so the only Direction it can really accelerate in in fact is what towards this in this Direction right so there is a force onThis so is momentum conserved no is momentum Conserved in any direction linear momentum how about tangentially yeah yeah yeah tangentially it is conserved right it's just not conserved towards the center but the problem with the tangential momentum is when it's here it's the directions change. P right torque of Point Q about Point p is equal to going to be r p R QP cross F and FP is pointing inwards so what is going to happen to that zero right so there is no torque on this particle at any point in time it's just a very long way to say listen things going in circles and the only force is radial if we take a cross product it's going to vanish which is why this is such a convenient formalism. Works in things where Works in situations where the force or something always points to the center. would have to do one of two things I would have had to either calculate this term or calculate or make you know make my frame attach it to the truck. The whole point is to show you they could be surprises but be careful any questions about this all right snap quiz in the next 3 minutes I want you to calculate for me the final velocity literally 3 minutes because I have toDo the dumbbell problem now you know the irony is this problem you could have done before you took this class the only difference is you know all the ways you can do it wrong. you the initial velocity I'll give you the initial angle of veloc angle of speed Theta dot Theta one dot so I'm solving a slightly different problem here with the ma is but it's the same same problem in terms of math. If I have the length of the cord the angular speed is going to double and the reason it's more interesting it could because when you watch a skater you know do the uh what's what's it called the twirl you know when figure skaters kind of rotate.  angular momentum is a vector out of plane in 2D you see that okay is energy conserved for this particle why not well think about it you wor yeah work it out has the energy gone up or gone down. The energy kinetic energy is gone up by the square a velocity right and um so energy is not conserved so where did the energy come come from yeah that's because you were doing work by pulling it right potential energy is constant so the can energies because you can also do this with energy by the way right. Two particle masses basically a dumbbell and they're attached rigidly by a massless bar massless and I scoot them across and it's rotating it's hurling through through you know across this rink. I'm going to try and identify understand the behavior in fact I'll make it even more complicated by attaching two rockets get it Rockets right to this thing. The Rockets are designed such that they always Point North all right so they always point in in the horizontal Direction in the on the Whiteboard right. p and particle Q is that reasonable it's perfectly reasonable but do I have a kinematic constraint and what would the kinematics constraint be H yeah it's a they connected by a rigid body so they can do whatever they want as long as the length between the two of them remains constant right by the way if instead of a rigid bar if I had a string connecting them what would  the kinemic constraint be less than equal to to R right but in this case it's  a rigid bar so it's equal to R to R. stick if I have something at the end of a stick can I apply like a you know does it only have to be a force inwards right. Next week we'll pick up on this and essentially what we'll do is I'll do the three body diagram. Define angular acceleration and moment of inertia and a more General sense okay so let's stop here because we are over time. CL you I mean you're getting that's right so there are a couple of ways to interpret this okay it's a massless rod and I'm invoking the strong form of Newton's third law.

ROUGE-1: 34.44, ROUGE-2: 33.53, ROUGE-L: 33.44
BERTScore: 64.83

==============================================
==================== [87/100] ====================
Summary:
Differentiable programming is closely related to deep learning. It allows you to build up an increasingly more sophisticated model without losing track of what's going on. The FeedForward function takes in an input vector x and produces an output vector which could be of a different dimensionality. So now we can write our three layer neural network using FeedForward and the way I'm going to do this is score is equal to you take x or distribute phi of x. And you can write this as FeedForward cubed as to be more compact. This is a very compact way of writing something that would otherwise be quite complicated. you want to do image classification. We need some way of representing images. The FeedForward function that we just introduced, takes a vector as input and we can represent an image as a long vector by, for example, adding all the rows. But then we would have this huge matrix that we would need to be able to transform this vector resulting in a lot of parameters which may make life difficult. To fix this problem, we introduce convolutional neural networks which is a refinement of a fully connected neural network. first is Conv. Conv takes an image and the image is going to be represented as a volume which is a collection of matrices, one for each channel, red, green, blue. Each matrix has the same dimensionality as the image, height by width. The way that Conv is. going to compute this volume is via a sequence of filters, and intuitively. what it's going to do is try to detect local patterns with [AUDIO OUT] So here is one filter and how it works is I'm going to slide this filter across the image. going to slide a little max operation over every 2x2 or 3x3 region. So the max over these four numbers is going to be used to build this [INAUDIBLE] and so on. That's all I'm going to say about MaxPool. If you want to go into the details, you can check out this demo or you can learn more in 231. But again, I want to highlight that there's these two modules. One for detecting patterns and one for aggregating. And with these two functions along with FeedForward, now we can define AlexNet. In NLP, words are discrete objects and neural networks speak vectors. So whenever you're doing NLP with neural nets, you first have to embed words, or more generally, tokens. We're going to define an EmbedToken function that takes a word or a token x and maps it into a vector. And all this function is going to do is it's going to look up vector in a dictionary that has a static set of vectors associated with particular tokens. So this representation of the sentence is not going to be a particularly sophisticated one. is something that has an interface but not an implementation. So a SequenceModel is going to be something that takes a sequence of input vectors and produces a corresponding sequence of output vectors. I'm going to talk about two implementations of the sequence models. One is recurrent neural networks and one is transformers. So an RNN, or a recurrent neural network, can be thought of as reading a sentence left to right. So the intuition again is reading left-to-right, updating the hidden state as you go along. Collapse takes a sequence of vectors and returns a single vector. There's three common things you can do. If you're doing text classification, you probably want to pick the average to not privilege any individual word. But as we'll see later if you're trying to do language modeling, you want to take the last. These types of functions where the input and output have the same type signature are really handy because then you can compose them with each other and get multiple steps of computation. The core part of a transformer is the attention mechanism. The attention mechanism takes in a collection of input vectors and a query vector and it outputs a single vector. So in contrast with the RNN, you have representations that have to kind of proceed step by step. And the number of steps is the length of a sequence which causes these long chains which prevents kind of fast propagation, whereas attention solves this problem. So there's two other pieces I need to talk about before I can talk about the transformer. was this complicated thing that I mentioned at the beginning. So BERT is this large unsupervised pretrained model which came out in 2018 which has really kind of transformed NLP. And what BERT does on a sequence of tokens is it's going to embed the tokens, and then it's just going to apply the TransformerBlock 24 times. So we can take language models and we can build on top of them to create what is known as a sequence-to-sequence model. So this is by and large how a lot of the state of the art methods for, for example, machine translation works. encourage you to consult the original source if you want kind of the actual, the full gory details. Another thing I haven't talked about is learning any of these models. It's going to be using some variant of stochastic gradient descent, but there's often various tricks that are needed to get it to work. But maybe the final thing I'll leave you with is the idea that all of differentiable programming is built out of modules. Even if you kind of don't understand or I didn't explain the details, I think it's really important to pay attention to the type signature of these functions.

ROUGE-1: 32.30, ROUGE-2: 31.32, ROUGE-L: 32.16
BERTScore: 65.16

==============================================
==================== [88/100] ====================
Summary:
Professor Amy Hungerford: Today it is my great privilege and pleasure to introduce Andrew Goldstone, a TF in this course. Andrew is a fourth-year student in the Ph.D. program in English, and he is writing a dissertation on the autonomy of the work of art in modernism. On Monday we had three main themes that were used to introduce this novel to you. And that's the question that I'm mostly going to focus on today. I'm going to bracket the ethical question, leave that for Monday's lecture. For Nabokov, the highest value is originality. He says this in his last Russian novel, The Gift. "Any genuinely new trend in art is a knight's move, a change of shadows, a shift that displaces the mirror," he says. "In chess the knight doesn't move in a straight line. Unlike any other piece, it skips over pieces in the way" The strategy of the knight'smove is to frustrate your expectations, to leap over the apparently important events into something else. The danger for Nabokov is that he will fall too in love with something too like himself. He has to hold off this possibility of being too attracted to these male predecessors who are too similar to him. To think about the word "queer," the treatment of Gaston Godin, that funny French character in Beardsley, to think about Humbert's constant protestations that he's attractive to all women, about his supposed virility. And it should just make you wonder whether pedophilia is in itself a kind of knight's move from homosexuality. Nabokov's relationship to this modernist past is not just the burlesque that he visits on Eliot. An element of admiration is also present, and that's really part of his relationship to Joyce. I'm just going to name for you four features of Joyce's style that are important to Nabokov: stylistic virtuosity, the ability to imitate any style; at the same time, a scrupulous attention to the banality of everyday life and all its detail. Third characteristic, the constant use of a superimposed structure. MaryBuck, Danielâ€¦ [and so on; I'll come back to this list, actually; just skip to the bottom on page 52] Talbot, EdgarTalbot, EdwinWayne, Lull,--[a lull in the book, right?] Williams, Ralph Windmuller, Louise. So strange and sweet was it to discover this Haze Dolores: she, in its special bower of names with its bodyguard of roses, a fairy princess between her two maids of honor. Nabokov's Lolita is a kind of artificial, processed, bland, easily consumable version of fate. In other words, chance is already fated. The thing that stands for randomness in this book, the thing that looks like ordinary detail, has already been arranged to give you artistic pleasure. This kind of transformation of arbitrary, real fated events into conspicuously artificial tricks is a response to exile, in particular to Nabokov’s condition of exile. foreign country, lives in a kind of denaturalized world, a world where, instead of everything making instant sense everything has to be decoded. In that afterword to this book, Nabokov says he had to invent America. That's because he didn't know it already; it wasn't given to him. Now, in a way this is a terrible state, a state of discontinuity with the world you exist in. But it has a payoff, kind of, a payoff which is the possibility precisely of inventing. This connection, to remember that the knight's move as a way of avoiding obstacles, in particular, keeps skipping over forms of violence. Nabokov will say that his private tragedy is that, let's see: I had to abandon my natural idiom, my untrammeled, rich, and infinitely docile Russian tongue for a second-rate brand of English devoid of any of those apparatuses. And you might think about that homophobic attitude to a Proustian past. the fear that it's too like what he wants to do. But the main point here to think about is that feeling of damage. On page 152--oh, and by the way, this book was written on road trips. Nabokov's wife, Vera, drove him on thousands of miles of trips around the country while he was writing this novel and hunting butterflies, so think about that--but here is 152, evocation of the landscape. By a paradox of pictorial thought, the average lowland North American countryside had at first seemed to me something I accepted. American landscape is already a work of art, already part of a European memory. "But gradually the models of those elementary rusticities became stranger and stranger to the eye," Humbert says. "Inutile loveliness" is kind of the key word of Nabokov's technique, and he says the novel has as its only purpose to provide aesthetic bliss. But, I don't want you to think that this just means everything's okay. Of course, everything is not okay. in a much more ambivalent position, someone who's trying to become an American writer, as Nabokov says he's doing. Humbert is trying to invent America, trying to bridge the gap between Russian and English, but always finding that English is only a kind of second best. So, that suggests that it's not all to the good; it hasn't been saved by taking up these knight's move techniques, the defamiliarizing techniques; there's still a record of damage.

ROUGE-1: 24.45, ROUGE-2: 23.44, ROUGE-L: 24.23
BERTScore: 63.96

==============================================
==================== [89/100] ====================
Summary:
NORVIN RICHARDS: Today is phonetics, which means that today we begin making funny sounds at each other. So everybody limber up your vocal tracts. Let's see. I'm trying to remember if there's anything that I ought to announce. You remember, maybe, that problem set 1, which confusingly is your second problem set, is due on Thursday. Normally, it would be due on Tuesday. But because I am technologically challenged, it's due onThursday. I just figured out how to get the projector to project over there. Linguists have a system for writing sounds down so that we'll all know what kind of sound we're talking about. A lot of the symbols of the International Phonetic Alphabet resemble letters of the English alphabet. The symbol for the sound at the beginning of "paint" is the letter p. And so that's a symbol of theInternational Phonetics Alphabet. As we go along, we will be seeing weirder and weirlier symbols from the InternationalPhoneticphabet. like "thistle" and "this," where you're sticking your tongue between your front teeth and making air flow out. English doesn't spell these differently, at least not reliably, right? So we spell both of these with a "th" That's one of the reasons the International Phonetic Alphabet is there. We'll talk about the difference between them shortly, but at this point, maybe it's just clear they're not the same sound. There's a type of sound that has been called alveopalatal, also called postalveolar. Inventor: "r" is one of the kinds of sounds that people classically have trouble with. "z" is voiced and that "s" is voiceless. "g"s and "zz" are consonant sounds that agree with the consonant that's at the end of "cat" and "dog" The difference between "cats" and 'dogs' is voiced or voiceless, the phonologist says. "R" might be an example, but we'll get to "r," eventually. Polish has a rule that changes "g" to "k" at the ends of words. Voiced "b" becomes the voiceless version, which is "p" Joseph: There's a reason that I started with sounds like "s" and "z," and "f" and “v." Joseph: In homeworks, if you ever need to write about plosives, feel free to call them either "tt" or "plosives," because they're named after the fact. The way nasal stops work is you're stopping the flow of air somewhere in your vocal tract, but you're allowing the air to flow through your nasal cavity. You can have an alveolar nasal, right? "n," or a velar nasal, "ng" What would a glottal nasal sound like? Trick question. You would need surgery. So you would need, again, as I say, probably there are unethical surgeons who would modify you so that you could makeglottal nasals. English has interdental fricatives and alveolar stops, "t" and "d" There are languages out there that have what are called dental stops. Part of your job, if you're learning Tagalog, for example, is to learn to make dental 't's instead of alveolars. There's all this work on what people do to compensate for various kinds of obstructions in the vocal tract. And on the website, there will be a link to charts that will have more official charts by the IPA. "w," let's say, in the middle of a word like "away," that's not a stop. "y" sound at the beginning of "year" or "yard," and "l" and the "r" are all approximants, yeah. They are sometimes divided into glides and liquids. And I'm hoping that nobody will ask me how you know whether something is a glide or a liquid. But that's the prima facie reason for distinguishing them because there are rules for the distribution of sounds for which it's useful to have that distinction. distinction as long as it turns out to be useful for explaining stuff. Yeah? Good, all right. So those are glides and liquids, OK. And then I recall we were asking about this before. Sounds like chuh and juh-- I think I said you could think of chuh as a stop, a "t" followed by a fricative, shuh, or juh. There's a little bit of a debate about whether what I just said is the right way to think about this or not. English has a very large number of vowels and a not-very-good system for writing them. English spelling is so difficult that we can actually have competitions where you watch people spell. In many languages, the spelling bee would just never end because every word is pronounced exactly the way it's spelled. So here are two vowels. The vowel in "bead" and the vowel in “bad” Go ee-ah, eo-ah,. ee, ooh, ee. OK. Student: "Sue says he's a bad egg." Student: "Top chopstick shops stock top chopsticks" Student: [INAUDIBLE] Student: "[INAUDITED] Top chopsticks. Top chopstick. shops stockTop chopsticks." Got that from a book of tongue twisters. Student: ["INAUDible] Top Chopstick. Shops stock top Chopsticks." Student. "Top Chopstick shops. stock top. chopsticks". Student: "Top chop stick shops stock. top chop sticks." Student.: " top chop stick shop stock top chops" Student."Top Chop Stick Shop Stock Top Chopsticks"

ROUGE-1: 13.37, ROUGE-2: 12.22, ROUGE-L: 12.65
BERTScore: 60.78

==============================================
==================== [90/100] ====================
Summary:
Expectation is a basic question that will come up again and again when we look at random variables and probability theory. We're imagining n independent flips of a coin with bias p. The probability of heads is p. It would be biased in favor of heads if p is greater than 1/2. And we want to know how many heads are expected. So what's the expected number of heads? Well, we already know-- we've examined the binomial distribution B n,p.

ROUGE-1: 19.54, ROUGE-2: 18.43, ROUGE-L: 15.40
BERTScore: 71.67

==============================================
==================== [91/100] ====================
Summary:
Aristotle was born 384,15 years after the trial of Socrates. He was sent by his father to go to college. Unlike most of you, Aristotle did not spend four years at the Platonic Academy. He remained attached to it for the next 20, until the death of Plato. Unlike his intellectual godfather, Socrates, who wrote nothing but conversed endlessly, Aristotle wrote disciplined and thematic treatises on virtually every topic. He collected constitutions, 158 of them in all, from throughout the ancient world. Aristotle says man is, by nature, the political animal. He says participation in the life of the city is necessary for the achievement of human excellence. A person who is without a city, he says, who is apolis--without a city--must either be a beast or a god. The city is natural in that it allows human beings to achieve and perfect what he calls their telos, that is to say their end, their purpose. But there is a second sense for him, in some ways, in which he says the polis is by nature.

ROUGE-1: 6.73, ROUGE-2: 6.32, ROUGE-L: 6.25
BERTScore: 58.15

==============================================
==================== [92/100] ====================
Summary:
Mathematically, a consumer is trying to maximize his utility. And this utility maximization has to be done with respect to some constraint and the constraint the budget constraint we take P 1 x 1; P 2 x 2 should be less than or equal to I. In real life, it is possible that a person derives some satisfaction from having some money left in his pocket, but the way this problem has been framed here the person’s satisfaction depends only on his level of consumption of good 1, and good 2. I is P 1 x 1 plus P 2 x 2. What is e? E is nothing. We are checking we have set up the problem such that this I is equal to this u naught and what we have learned is that x 1 m P 1, P 2 I is. equal to x 1 h P 1 P 2 uNaught. This is an identity. not just equal to. this is always true. I can write it further if you pay attention to this that x1 m P1 comma P 2 and I is e of P 1 comma P2.

ROUGE-1: 17.57, ROUGE-2: 16.84, ROUGE-L: 17.57
BERTScore: 68.87

==============================================
==================== [93/100] ====================
Summary:
MIT OpenCourseWare offers high-quality educational resources for free. To make a donation or to view additional materials from hundreds of MIT courses, visit MIT OpenCourse Ware at ocw.mit.edu.The following content is provided under a Creative Commons license. Your support will help MIT Open courseWare continue to offer high- quality educational resources. The i3 is an efficiency and urban-optimized transport vehicle made by BMW. It's an all-electric, the fact that it's fundamentally an electric vehicle is key to the i3. like the smart car. And so architecturally, distinguishing feature is definitely the electric drive. So I think it's good, but I think you're just missing a little bit on the technology there. What about i3 and EPFL? Anybody do the i3? Nobody. Anybody else here? Yeah, Veronica. AUDIENCE: Thank you. OLIVIER DE WECK: OK. There's a lot there. I would throw a question. A concept you defined could almost apply to a Tramway as well. If this was like a streetcar, don't you think it would apply to that as well? bank, a cafeteria, student services. But you could encompass it in a meeting area. You meet and you do some stuff like eating, going to the bank. In that sense, it's pretty similar. But I think the Rolex Center is such an iconic building that it also serve a kind of a prestige function, to put the institution on the map in terms of it's a statement. It's not just a utilitarian building. Whereas, I would argue our MIT student center, it has very similar functions to theRolex Center. It doesn't have that wow factor. is the beneficiary? That's a stakeholder. The stakeholder has needs. And then this is a funny thing here. The needs have these little bumps. This is meant to indicate a cloud. So this means that the needs are somewhat vaporous. They're not very well defined. And so you interpret and incorporate some of the needs into goals, which become requirements. The goals then are an instrument of the primary delivery value delivering process, which is your value proposition. To then actually deliver that value you need to design the product. EPFL's Olivier De WECK explains how to reduce the spoilage rate of food. He says there are a number of ways to do it, including irradiating, drying, chilling and preserving. He also explains how bears use fat to transform it into fat and store it inside their bodies. The full interview is available on CNN.com and on the CNN iReport app for iOS and Android. For the full interview, visit CNN.co.uk or the CNN YouTube channel. For more, go to www.cnn.com/news. In order to chill, we need a chiller, and there are different types of chillers, like a cooler or refrigerator. The combination of this specific way you're going to operate the system is what we call concept. Once we have that, we can start managing complexity, decomposing function and form. Design, then, selects the actual values for those design variables, and then optimize the cooler and then the refrigerator for those values, if we look at the example of a cooler. The goodness of an architecture is really a pretty complex concept where we have multiple objectives to satisfy, including performance, resource utilization, cost, operability, safety, capacity, and so forth. So architecture requires consideration of both function and form related through concept. It's about starting with the operand. What is the thing that the beneficiary, the stakeholder cares about, and how do we transform that? Concept then elaborate these into architectures that have form function and structural complexity. So the NASA approach is basically described in the system engineering handbook in the SE engine as step 3 called logical decomposition. System design and management program is a full-year program. It's focused on decomposition, which is an important part of architecting. So let me talk about methods and tools for concept generation. So what are different ways of stimulating or organizing creativity? And what I'm showing you here is-- that's essentially a mind map of how to think about the creativity space. And we'll talk about very briefly mind mapping and then morphological matrices. And then we have this whole area here, which I'm going to mention, but we're not going to do as part of the class. How to Think Like Leonardo, Seven Steps to Genius. The seven da Vincian principles of creativity in Italian. Avoid the killer phrases. Creativity killer sentences. This will never work. We don't even need to talk about this. Everybody does it this way. I've already studied this problem for years. Don't worry, I know I'm right. How long have you been with this company? Anyway, so that's the idea. All right Leonardo. Who's been to Italy or tour in France, or who's seen one of the exhibits know where his notebooks are on display? Leonardo's seven principles of creativity are: curiosita, lifelong quest for learning. Dimostratzione, testing your knowledge through experience, trying things out. Sensazione, continual refinement of the senses. Mastering ambiguity, paradox, uncertainty. Arte/Scienza is the whole brain thinking, left-right brain. Corporalita, balance of body and mind, so a healthy mind and a healthy body. Connessione is interesting. That gets close to system architecture, which is the appreciation of patterns, relationships, connections, and systems. conceptual design, don't do all the details. We just go down two levels of abstraction. And then the really cool part, the exciting part in concept generation is the one that it's really a creative activity. And when you look at the set of creativity techniques, you can think of group dynamics like the brainstorming. That's used very heavily, but you have to do it the right way. If you organize a brainstorming session, and there's some wiggle room, but if you violate some fundamental principles of brainstorming you're not going to get the full benefit.

ROUGE-1: 25.31, ROUGE-2: 22.94, ROUGE-L: 23.19
BERTScore: 69.92

==============================================
==================== [94/100] ====================
Summary:
then we are going to continue with the second part of the lecture today which focuses on the problem what actually happens if the gaussian assumption that i have about my constraints doesn't hold. One of the questions actually how to handle that so as we said what we are doing here we are minimizing the sum of the squared errors terms and as we have seen so far this is the same or strongly related depending on how you formulate that. If we would have the possibility to integrate a multi-modal distribution here that would actually be a nice beneficiary. If you have structures in the environment and there's a lot of clutter in the scene the clutter even if it has a repetitive pattern may lead to a multimodal belief about what the relative transformation between two poses let's say. It's not always easy to get this information out of your gps because typically it runs the kalman filter internally or most of the gps devices do that so they get a gaussian belief is screwed up but if you get the raw measurements you may be able to do better by allowing for multi-modal distributions. over here so this is a single constraint you can already see that you don't have a straight wall over here anymore so it's kind of bended a little bit like this due to the single constraint which obviously has a really really large error so the least square error minimization at the error is squared error term tries actually to minimize that if we add i think whatever and two three four five i think there were 10 constraints 10 wrong constraints the map actually gets so distorted that is unusable for navigation. and in this case screwed up the measurement how can we incorporate that into the graph based slam approach um so the problem that we actually have is if we look to our um an individual constraint so the lack of an observation given the given the current configuration of the nodes was a gaussian distribution. If we have this this number of constraints is the product of gaussian distributions if you compute the log likelihood this turns into a sum of the exponents exponents therefore we have these sum of squared error terms. The key trick is to simply ignore all of them except the most prominent one. If the if the means of these gaussians are far apart from each other the approximation error isn't that big if they are near each other disaster or disaster but may have big errors. The system can swap between different modes and therefore the optimization in one iteration takes into account only one mode of the gaussian as you can switch the modes it is you still have the ability to deal with multimodal constraints if i do that. is a bimodal distribution for the inline and the other one for the outliers the red is in leia blue's outlier you can also handle those cases say i'm either there or there or somewhere else. In most cases actually the vehicle executes what you tell the vehicle to do but in some cases simply doesn't move so this max mixture idea is actually a pretty easy idea pretty simple idea just reply funny no one has done that in robotics until recently a few years ago the first one was edwin olsen and pratik. that's actually a nice thing so um another thing is it can handle both things at the same time data station errors as well as multimodal constraints. So the combination of outlier rejection and dealing with wrong data associations is actually kind of nice we also can do this obviously in 3d. So this is again this data set with the sphere that we have seen before robot moving in a virtual sphere with constraints so this is gauss newton and this is the max mixture gaussNewton and um so you can see here there's a non-perfect alignment in here. Just replace this information matrix here with a variant which has a scaling factor so a constraint dependent scaling factor added to that. This leads to the case that constraints which are far away from what we expect have a smaller influence on the optimization. There's actually closed form you can derive that under certain properties where you end up with this operation. The key idea the intuition behind that is if i have a constraint which have a large error so where the um the current configuration is far from what the constraint tells me just reduce the uh or increase the uncertainty that is associated to that so decrease the information matrix. in this area kind of the the core center of attraction both both perform equally well because there's no scaling involved but the further you move out the more the red curve gets scaled. So the the error is weighted down the further i'm away however we still have a linearization point. So if i compute the jacobian over here i still has a jacoobian which drags the system into the right direction so even if i initialize that quite far away i still get kind of pushed into theright direction if you have a small video. Max mixture as well as for dcs is that kind of the tails of this gaussian distributions contain too few probability mass they're too close to zero. Outlier which is really far away from the current estimate the whole mole is tracked in this direction. If you have constraint which introduce large errors these are these outliers. This can actually screw up the optimization when computing the minimal error configuration so one way you can do is or fits into the framework of its so-called robust m estimators which intuitively say we don't assume a gaussian distribution. now you get different properties in the optimization so if you use um if r is just the quadratic function then we exactly have the original problem that's what we minimized x of minus uh squared error so if we have this one we have we examine exactly in the gaussian world. Now there are different techniques how we can actually address that one thing is we could take simply the absolute value so we don't square it just take theabsolute value of the error that's not the parabola that we have but the absolute function. here that by changing this function you can't get much better behaviors kind of deciding which function to use for the underlying optimization problem is not on it's not always an easy and easy choice so this requires some expert knowledge some good intuition on coming up with the way with one of those functions. Next week which is the last week of the term i will briefly talk about front ends and give kind of a short summary on what typical front ends exist obviously we're not going to all the details as we did that here.

ROUGE-1: 31.21, ROUGE-2: 30.41, ROUGE-L: 30.04
BERTScore: 66.48

==============================================
==================== [95/100] ====================
Summary:
John Stuart Mill is the principle expositor of neoclassical utilitarianism. The rights-utility synthesis signals that we're looking for an attempt to put together both a commitment to utilitarian efficiency and respect for individual rights. The transition for classical to neoclassicals utilitarianism really went on in all fields of thinking about the human sciences at more or less the same time. What you're also going to get as a by-product of today's lecture is everything you ever needed to know about neocassical economics in 45 minutes. system of thinking about economics and the theory of value that was going to be tremendously influential. At the same time, more or less, there were very important developments in moral philosophy that I just want to alert you to, that we're going to return to later when we come to consider Alasdair MacIntyre's book, After Virtue. And this movement that I'm mentioning here is the doctrine that would come to be called emotivism. It was associated with a man by the name of Stevenson who wrote several books advocating the emotivist doctrine. At the start of this narrative, you'll start to see why the transition from classical to neoclassical utilitarianism in economics was essential for the transition in political theory. We will return to the anti-Enlightenment and Alasdair MacIntyre's book, After Virtue. But today we're going to focus for the rest of our time on the economics of the transition to utilitarianism. And I'm going to ask you to suspend your disbelief and just follow me through the ABC's of neoclassicals price theory. by-product is, you're going to get the whole of ECON 101 reduced to a single lecture. Because indeed it is true that enormously complex and subtle, and as sophisticated as the neoclassical theory of microeconomics is. It's all built out of three ideas that I'm going to spell out for you in what some of you might initially regard as laborious detail. But I think you'll see what I'm getting at once we get towards the end of today's discussion. So imagine a world in which there are just two commodities; in this case wine and bread. than anything I've talked about in these lectures. They didn't want to do that because they were actually concerned with quite another problem. The problem they wanted to solve was to understand the behavior of markets. They wanted to be able to more precisely to predict what prices were going to be in markets. So moving from cardinal to ordinal utility is going to turn out to have huge ideological consequences, which I'm going to unpack for you towards the end of today's lecture. But as an analytic matter, looking at this from the inside, it had the great virtue of providing the building blocks for a theory of price behavior. Prof: These indifference curves cannot cross. Can anybody tell us why? Why can't they cross? Wait for the mic. Student: Because at the intersection they should have the same utility. Prof: So you've got a kind of contradiction on your hands, is that right? Student: Yeah. Professor: Okay, and just to spell out the contradiction more emphatically--I think you basically made the point. So we have a distribution here, okay? Now what Pareto said, he said, "Let's draw a line north-south through the status quo" which the radical fangs of classical utilitarianism have been ripped out and it is now a doctrine that is very friendly to whatever status quo happens to be generated in a market system. So it ceases to be this radically redistributive doctrine, and in the process imports into utilitarianism a very robust, some would say, hyper-robust doctrine of individual rights. We'll see how that played out in political theory when we come to look at John Stuart Mills' harm principle next Monday.

ROUGE-1: 18.84, ROUGE-2: 18.08, ROUGE-L: 18.27
BERTScore: 73.19

==============================================
==================== [96/100] ====================
Summary:
HONG LIU: So, first, we talk about chiral fermions. So, previously, we showed that the Dirac equation requires, actually, psi to have four components. And so there are two ways to reduce it, and one is called the Majorana fermion. So we first talk about the chiralfermion and one way to do it. And then, we will look at the specific representation of gamma matrices, OK? Consider-- so now I will use a representation which is different from what you-- so i divided by 4. HONG LIU: Let's look at this choice of gamma matrices, OK? So I will call this choice to be star. And so you can also work it out. So you find that they have the following form. So do you see something? Yes? AUDIENCE: They're block diagonal. HONG LIu: Yes, they are. So if sigma is block diagonal, then that means this S is also block. So when S is block, that means, if I write psi x into two component vector, they transform within themselves. HONG LIU: You don't need four components to be able to transform under Lorentz transformation. At least two components can already transform. So this tells you, in a sense, that the LorentZ covariance only requires two component spinors. So we have to do a little bit of work to do much work if you actually find the right trick to do this for any choice of gamma matrices. So you can check yourself that the gamma 5 is actually Hermitian. You need i for this to be true, OK? You can also take the product of all the gamma matrix together and then with a factor of i, OK?" HONG LIU: In the Dirac spinor, which we have talked about so far, is four components. In the chiral spinor we talked about, you have two complex components. The next one I'm going to talk about is the Majorana, in which case, I would argue, we have 4 times 1 real component. And, now, let's talk about the last case-- this case, which is-- you have four real components, OK? So what do you do? Again, we follow the similar strategy to see whether it's possible to have fourreal components. to impose in this basis. And this is now independent of massless or massive particles? HONG LIU: Yeah, yeah, yeah. Yeah, this is-- yeah. Good? So this concludes our discussion of the Majorana spinor. Do you have any questions on this? Yes? AUDIENCE: So is the orthogonal component of psi-- the Majoran fermion [INAUDIBLE]?? HONGLIU: Sorry. The orthogona component of this Majorana species-- like, possible-- in the chiral one, like, psi L. we have-- so this real scalar theory-- we can see that before. So this theory has a discrete symmetry because this is invariant under phi. It goes to minus phi, OK? So this is-- if you do it twice, you go back to itself. OK, so this is called the Z2 symmetry. And there are also spacetime discrete symmetries. So you can have so-called time reversal, which corresponding to your t, x goes to plus t. And so, altogether, they are called CPT symmetry. Yeah.

ROUGE-1: 11.68, ROUGE-2: 11.04, ROUGE-L: 11.37
BERTScore: 72.84

==============================================
==================== [97/100] ====================
Summary:
The best-case scenario for expansionary fiscal policy is when there are lots of underemployed resources in the economy. By increasing spending, the federal government can try to counteract falling aggregate demand. In one scenario, government spending doesn't have to be as large as the fall in "C," or consumption, to counteract the recession, and that's because of the multiplier effect. But, as always, shifting lines on a graph is much easier than shifting around real resources in a multi-trillion dollar economy.

ROUGE-1: 30.09, ROUGE-2: 28.77, ROUGE-L: 30.09
BERTScore: 67.40

==============================================
==================== [98/100] ====================
Summary:
Sarah thread sterner shows you how to wear and take off a mask. She explains how to determine which part of the mask is the front versus the back. She also shows how to mold the nose piece of the face mask with the finger tips of both hands. Finally, she shows you the best way to remove the mask by placing it over your ears and pulling it under your chin when removing the mask it's important to remember that the front is considered contaminated so remove it with the fingertip and then perform hand hygiene.

ROUGE-1: 29.04, ROUGE-2: 20.29, ROUGE-L: 23.82
BERTScore: 64.55

==============================================
==================== [99/100] ====================
Summary:
In this lecture, we're going to talk about how neurons function and how researchers are able to control that function in order to modify behavior. Then we'll talk about synapses and how synapses function to communicate between neurons. And this is going to involve also sort of understanding how certain antidepressants, like Prozac, work. And then we'll end by talking about how researchers did this experiment to wake up the mouse. And it all starts with something that I told you about at the beginning of the semester, which is the plasma membrane. In a resting state, the cell's resting potential is negative 70 millivolts. The cell can lose this polarity and not have a charge differential, or it can flip and be positive on the inside. Stephen suggested opening ion channels. If you open these, it's going to depolarize the cell. Because remember, high on the outside, out here. And so if you open them, positive ions are going to flow in to make this less positive. So what is an action potential? Everyone see how it's a transient potential. A nerve cell fires an action potential and how it propagates along the entire cell length. In the case of the sciatic nerve, this has to happen across an entire meter, OK? That's a very long distance to propagate this change in electrical signal, at least for a cell. And so we're going to talk about the mechanism. And I'm going to start at the beginning, when this action potential initiates. This is a voltage gated sodium channel. And you can see, it's closed because of this red rod that's a positively charged alpha helix. There are different types of signals that nerve cells can send. Signals can be excitatory, meaning it will tend to depolarize the neuron. There are other signals that bind to different type of receptors that are inhibitory. Neurons are prepared to send signals to each other. They have everything ready to go when they get the word from the next cell that they're ready to send a signal to. The way that multiple neurons communicate with each other are through a type of signal known as a neurotransmitter. Prozac and Zoloft affect this reuptake process. And what that does is it keeps the neurotransmitter in the synaptic cleft for longer. And so the idea behind these drugs is that if you are suffering depression from a lack of serotonin, then you can rescue that by preventing the rapid reuptaking of the neurotransmitters. The way they function is to leave the neurotransmitted in the. synaptic clefts for longer so that you enhance signaling, even if you have low levels of the. neurotransmitter to begin with. optogenetics is an approach to control the activity of a cell with light. In this case, we're going to have light inducing depolarization. And the way this is done is there's a protein discovered from photosynthetic algae that's responsive to light, and it is a sodium channel. And if that's expressed specifically in the neurons that you're trying to test, you can then shine a light into the brain of the organism and activate, specifically, this type of neuron. you to test the function of the neuron in the behavior of an organism. So, in this case, this mouse, the light is shined into its brain, and they're testing a specific type of neuron that is involved in arousal of the mouse. And it's going to wake up right now. There it goes. It woke up. You see now its muscle activity is going, OK? So you can test thefunction of specific nerve cells using this approach, and it's because you have a light-sensitive sodium channel.

ROUGE-1: 18.07, ROUGE-2: 17.27, ROUGE-L: 17.55
BERTScore: 65.50

==============================================
==================== [100/100] ====================
Summary:
In this problem, we're going to be dealing with a variation of the usual coin-flipping problem. But in this case, the bias itself of the coin is going toBe random. And we're told that the expectation of this bias is some mu and that the variance of the bias isSome sigma squared. And what we'll be asked is find a bunch of different expectations, covariances, and variances. We'll see that this problem gives us some good exercise in a few concepts, a lot of iterated expectations. not equal to j, we also calculated in part B. That's just sigma squared. All right, and now if we compare these two, we'll see that they are proportionally exactly the same. So what do we learn from this problem? Well, we saw that first of all, it's very useful to use law of iterated expectations. But the trick is to figure out what you should condition on. And that's kind of an art that you learn through more practice.

ROUGE-1: 10.67, ROUGE-2: 10.25, ROUGE-L: 10.61
BERTScore: 70.94

==============================================
