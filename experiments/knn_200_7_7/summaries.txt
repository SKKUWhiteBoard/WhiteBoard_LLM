==================== [1/100] ====================
Summary:
CASEY RODRIGUEZ: We proved the simple and squeeze theorems last time. We're now moving on to the topic of limsup and liminf of a sequence. What's the underlying question we're going to try to answer? Roddiguez: The first analysis is to look at examples that you just don't quite understand, but you don't have to understand all of them to understand them, just start writing down some actual things that you know. but same proof works with absolute values. So let's use this to do a few more special limits if you like. If p is positive, then limit as n goes to infinity of p clear, because then I just get 1 for the whole sequence. Now let me do p bigger than 1. And since this is equal to its absolute value, that means that n to 1/n minus 1 is bigger than or equal to 0. And so my goal is to show that n is limit as x as x goes to n. to the 1 over n equals 1. And the third is-- it's just that a certain limit exists-- limit as n goes to infinity of n to the 1over n. OK? Let me make a small comment here. So far in our discussion of the real numbers, we've only defined what it means to take a real number to an integer power, but-- and n-th roots. So using that, we can then define how to take an integer number to a rational power. So 0 must be less than that real-- positive real number. Bolzano-Weierstrass theorem says that every bounded sequence does have a convergent subsequence. We're going to prove it by introducing limsup and liminf, because these are also two important objects that arise in analysis. We define limsup x sub n. And sometimes I'll write n goes to infinity underneath. Sometimes I'll just write limsup. And that's also a general fact that will prove, that for any bounded sequence, there exists some sequences converging to the lim Sup and the liminf. An n underneath it. This is supposed to be a number, and this is equal to the limit as n goes to infinity of a new sequence obtained from the old sequence x sub n. What are the entries of this new sequence? This is sup of x of k, k bigger than or equal to n. And the liminf is similar, except it's now with infs. OK? So for each natural number n, I take the supremum of the set of elements x sub k. So this is a bounded set because the sequence is bounded. as n goes to infinity of a sub n exists, and the limit as n went to infinity. of b sub n also exists. So in particular, since we know that, if we have a monotone sequence which is bounded, it has to converge, that means these two limits exist. And then the second part of this term is the simple statement that the liminf of x sub n is less than or equal to the limsup [INAUDIBLE] x Sub n. subsets of real numbers, such that [INAUDIBLE] bounded and A is a subset of B. The inf of B is less than or equal to the inf of A. And this is always less than, equal to, or greater than the sup of A, OK? So what this says is that, if I take a subset. of B, then that increases the inf and decreases the sup. And that inequality reverses for infs. All right? And this just follows immediately from the definition of sup and inf, so I'll just prove the sup statement. the n. Then, if I am looking at this set x sub n, n bigger than or equal to k, and writing-- instead of x sub N, let me just write what it is, minus 1 to the n. So what is this set? This is just a set consisting of two elements, 1 and minus 1. And therefore, the sup of this set is just 1. This is equal to the limit as n goes to infinity of 1 equals 1. So I want to show that there's something converging to the limits of the a sub n's. 1. The liminf is minus 1 for this set. That's just supposed to be a squiggly line, not necessarily looking like sigma. OK, so there's one sequence. How about our next favorite sequence, x sub n equals 1/n? So we're looking at now the set 1 over k, where n is-- where k is bigger than or equal to n. So as I move to the next entry, things are getting smaller and smaller. And in fact, this sequence just here now, written as a new-- so this is not a sequence-- this is a set. If the limsup equals the liminf, then the original sequence converges. The original sequence, which we know-- or shown last time-- doesn't converge. So that's another way of thinking about limsups and liminfs is that they also somehow measure just how divergent your sequence is, or at least the difference between them. If that difference is 0, then your original sequence is convergent, OK? All right. All right, so I think we'll stop there. The limit as k goes to infinity of xm sub k produces the liminf. OK. And so before I prove this, this immediately gives the Bolzano-Weierstrass theorem, which is that every bounded sequence has a convergence subsequence. In fact, we have something stronger, in that we have at least two subsequences which converge to these two numbers, which may or may not be the same. OK? So the reason this is so powerful and so strong is that it doesn't require you to show something as strong as showing there is a sequence converging to that. is a general nonsense that you stick into your machine or function that spits out output. And these outputs are approaching the maximum or approaching the minimum. And what you'd like to say is that there does, in fact, exist an element that you can stick in your machine and produce the maximum amount of output. But you don't have to work that hard is what this theorem says. It says that what you really need to do, and which is much more straightforward, or simpler, or impossible really, is to show that that sequence of inputs is a bounded sequence. sandwiched between things converging to the limsup, and use the squeeze theorem. All right? OK, so since this is the supremum of this set that-- and because this thing is not an upper bound for this set, there exists something from the set. So some element k bigger than or equal to n sub 1 plus 1, which I'm going to call n sub 2. And then I just keep doing this. Since a sub 2 plus 1 equals the supremUM of the x of k, such that k isbigger than orequal to nSub2 plus 1.

ROUGE-1: 25.46, ROUGE-2: 24.25, ROUGE-L: 21.55
BERTScore: 67.66

==============================================
==================== [2/100] ====================
Summary:
The configuration of a particle is given by, or described by, a wave function psi of x. In 3D, the wave function would be a function of all three positions x, y and z. If there are two possible configurations the system can be in, it's possible to find the system in a superposition of those two psi is equal to some arbitrary linear combination alpha psi 1 plus beta psi 2 of x, OK? So some things to note-- so questions about those before we move on? No questions? Nothing? You're going to make he threaten you with something. All reasonable functions are equally reasonable as wave functions. There's no primacy in wave functions or in states. Some wave functions are more equal than others. A state with a definite momentum has the property that, when you hit it with the operation associated with momentum, you get back the same function times a constant, and that constant is exactly that momentum we ascribe to that plane wave function. The probability that I'm going to fall over in 10 seconds is not equal to 1% or 3%. It's one of those. Hopefully is much lower than that. Any function can be expressed as a superposition of wave functions with a definite momentum. Fourier didn't think about it that way, but from quantum mechanics, this is the way we want toThink about it. We want some good definition of p given that we're working a wave function. Hint the first is that a wave with a wave number is associated, according to de Broglie and Davisson, to a particle-- a particle having a particle with momentum p. But if I measure it to be anything else, that is one and the same. a general statement that any state can be expressed as a superposition of states with a well defined observable quantity for any observable quantity you want. In 2D, this is a perfectly good vector, right? Now here's a question I want to ask you. Is that a superpositional? Yeah. I mean every vector can be written as the sum of other vectors. And it can be done in an infinite number of ways. So there's no such thing as a state which is not asuperposition. Fourier: In order to reproduce that as a superposition of states with definite momentum, I need arbitrarily high wavelength. To construct or detect an arbitrarily small feature, you need arbitrarily large momentum modes. Fourier: What should be true of the Fourier transformable wave function does that mean they're not supposed to be transformable? That's usually a condition of a Fourier wave function, and we don't know exactly what that is, but it's a good question to ask. say this. When you have a sine wave, what can you say about it's-- we know that a sines wave is continuous, and it's continuous everywhere. Its derivative is continuous and differentiable everywhere, because it's a cosine, right? So if yo you take a superposition of sines and cosines, do you ever get a discontinuity? No. So how would you ever reproduce a thing with a discontinuous using sines or cosines? Well, you'd need some infinite sum. quantum physicist: Average age is the sum over all possible ages of the number of people with that age times the age divided by the total number. Average need not be an observable value, professor says. Average value of the square of ages is, well, I'm going to do exactly the same thing. It's just a squared, right? 14 squared, 15 squared, 16 square, 16 squares, 16 squared. The expected value of a is equal to the sum of a times the probability of measuring that value. The average value of a number is 0. The standard deviation is 0, as long as there's no width, which is why it's a good measure of width or uncertainty. The square root of the standard deviation squared is the uncertainty in a given probability distribution. Different probability distributions are going to give me different delta a's, so we can either write it in this fashion or this fashion, and the notation for this is delta a squared. It's a little odd, because really you'd want to call it thestandard deviation squared. But whatever. notation that says which distribution you were talking about. When you have multiple distributions, or multiple possible probability distributions, sometimes it's useful to just put given the probability distribution p of a. This is not very often used, but sometimes it is very helpful when you're doing calculations just to keep track. Another notation that will come back-- you'll see why this is a useful notation later in the semester-- is this notation, psi. And we define the uncertainty in x is equal to the expectation value of x squared minus the expected value of X quantity squared. of x, how do we get the probability that you measure p? Do I want to do this now? Yeah, OK I do. And we need a guess. Question mark. We made a guess at the end of last lecture that, in quantum mechanics, this should be dp minus infinity to infinity of the Fourier transform. Psi tilde of p up to an h bar factor. OK, so we're guessing that the Fouriers transform norm squared is equal to the probability of measuring the associated momentum. And so on your problem set you're going to prove it. ways. You can either say the expectation value of x, or the expectation of x in the state psi. And this would be pronounced one of two ways. Why is there a double notation of psi? Yeah, we'll see later. Roughly speaking, it's but I don't want to take the time to do it, so ask in office hours. OK, good. The second part of your question was why does the Heisenberg relation work out nicely in terms of these guys? We'll see that. because in computing this expectation value, there's a psi squared. And so this is to remind you of that. Other questions? Terminology is one of the most annoying features of quantum mechanics. Yeah? AUDIENCE: So it seems like this [INAUDIBLE] variance is a really convenient way of doing it. How is it the Heisenberg uncertainty works exactly as it does for this definition of variance? PROFESSOR: That's a very good question. In order to answer that question, we need to actually work out the He Eisenberg uncertainty relation. And one answer is, indeed, the uncertainty relation works out quite nicely. a wave, a plane wave e to the iks, how do I get h bar k out of it? Note the following, the derivative with respect to x. The units of h bar are momentum times length. Momentum is about velocities, which is like derivatives. So if this is supposed to be true in some sense, what is momentum have to do with a derivative? We've been led to the idea that using wave functions that there's some relationship between the momentum, the observable quantity that you measure with sticks, and meters, and stuff. Noether's theorem underlies an enormous amount of classical mechanics, but also of quantum mechanics. To every symmetry is associated a conserved quantity. Conservation of momentum is associated with time translational symmetry. Rotational symmetries. x, as a vector, goes to some rotation times x. What's conserved by virtue of force? What's p dot? Yep. Noether’s theorem is solid. It's not shocking that in quantum mechanics, where we're not interested in the action of things on positions, but on functions of position, it's very interesting. of rotational symmetry? AUDIENCE: Angular momentum. PROFESSOR: Rock on. OK So quickly, I'm not going to prove to you Noether's theorem. It's one of the most beautiful and important theorems in physics. But let me just convince you quickly that it's true in classical mechanics. And this was observed long before Noether pointed out why it was true in general. What does it mean to have transitional symmetry? It means that, if I do an experiment here and I do it here, I get exactly the same results. translate by L. And what translate by L does is it takes f of x and it maps it to f of X minus L. So this is a thing that affects the translation. And why do I say that's a translation by L rather than minus L? Well, the point-- if you have some function like this, and it has a peak at 0, then after the translation, the peak is when x is equal to L. OK? So just to get the signs straight, define this operation. It's a Taylor expansion for a particular function. a priori did the world have to look like. Physics tells you this is a good model. And to the degree that it doesn't fit the data, it's wrong. This isn't something we derive. This is something we declare. We call it our model, and then we use it to calculate stuff, and we see if it fits the real world. Out, please, please leave. Thank you. [LAUGHTER] I love MIT. I really do. We live in a world governed by probabilities. The poem was written in the 19th century. It was written by a poet-turned-professor. The poem is called "The Poem of Love and Poetry" It is written in rhyme with the words "Love, Love, Love" and "Poem, Poem, Love." The poem was published in the early 20th century by the poet-in-chief, Christopher Smith, and was published by the publisher, Macmillan, on Valentine's Day, 1913.

ROUGE-1: 24.63, ROUGE-2: 22.97, ROUGE-L: 20.11
BERTScore: 63.10

==============================================
==================== [3/100] ====================
Summary:
the Lord Byron George Gordon Lord Byron very interesting very outgoing very flamboyant personality he stood out. The literary celebrity down at the bottom had spoken that he was a you know kind of born into not necessarily nobility like a monarch or anything but he was born into his title he didn't do anything to achieve it okay Hinda and he was an individual that you know probably was a bit of a celebrity to some degree he had the money he was wanting it he it said that he got in trouble because he had a lot of love affairs going on. lay the armaments which Thunder strike the walls of rock build cities bidding nations quake and monarchs tremble in their capitals the okhla by Athens. I have loved the ocean and my joy of youthful sports was on thy breasts to be born like thy bubbles onward from a boy I want and with thy breakers they to me were a delight. If the freshening sea made them a terror twas a pleasing fear for I was as it were a child of thee and trusted to thy billows far and near and laid my hand upon thy mane. that are like cities floating cities with tens of thousands of people I imagine I don't know definitely thousands of sailors okay but you know they've land and they move around and they have food and things like that on there. A storm can just mess that thing all up if there's a hurricane coming that ship heads the other way tries to get around it okay because it would turn it into like a toy in a bathtub when you guys used to have you know a little toy they used to.

ROUGE-1: 22.12, ROUGE-2: 21.69, ROUGE-L: 22.12
BERTScore: 62.85

==============================================
==================== [4/100] ====================
Summary:
When you do not have unlimited amount of everything, you will have to make a choice, what to produce? In what quantity you should produce a particular good. Allocation is nothing but assignment, allotment, share, but in economics we are more technical about this particular term allocation. So, allocation here means solving these three fundamental or basic questions of economics, and the first question is what to produced? The second is how to produce and the third is for whom to produce. And depending on who makes these decisions we have different form of economy, who make these decisions.

ROUGE-1: 19.11, ROUGE-2: 18.38, ROUGE-L: 18.92
BERTScore: 68.36

==============================================
==================== [5/100] ====================
Summary:
Albert Meyer: Random variables are an absolutely fundamental concept in probability theory. He says in the bigger number game, two teams pick two different integers between 0 and 7 inclusive. The other team sees these two pieces of paper whose other side has different numbers written on them. Meyer: Team 1 picks these two numbers, but they have to pick a low number that's less than a high number. The probability that you win these games is 4/7, because there's a 1/2 that you'll win the time. the game is that Team 2 wins if they wind up with the larger number. If you were lucky enough to guess the right threshold between low and high, you're going to win. So the probability that you win, given the middle case occurs, is 1. Now, in case H, that's the case where Z happens to be chosen greater than or equal to the high number that Team 1 shows. Well, in that case, Z just isn't telling you anything. So what's going to happen is that both numbers are going to look high to you because they're both less than. 1 apart, they can't have the same number on both pieces of paper. So one of them has to be less than the other. OK, now we can consider three cases of what happens with your strategy. The most interesting case is the middle case. That is, when your Z, which was chosen at random, happens to fall in the interval between low and high. And that means that you'll win, if and only if, you happen to turn the low card over first.

ROUGE-1: 22.10, ROUGE-2: 20.63, ROUGE-L: 16.24
BERTScore: 63.70

==============================================
==================== [6/100] ====================
Summary:
TZ: I want to just review um convolutions and and the architecture of a CNN to make this more clear and put it into perspective how it relates to just standard um dense neural networks. TZ: There's only really like one architecture that you really need to take away from here which is going to be resnet we'll get to that. The rest of them like if it goes over your head like don't worry it's it's fine this is just more for people who who want to know. The convolutional layer will have a bunch of filters right and all the values inside our filter are learned and we also have a bias term that gets added to the output of moving each window on each location of our input we refer to it as a volume simply because it sort of looks like a cube. If you don't want to do a striving convolution a very simple way to do it is to just look at individual little squares and just take the max in this whole area in each one of these areas. again you can kind of think of this as like a layer of a neural network so after we're done doing this we're still going to add our activation function like array Loop or or something else. We want to go deeper right um only having five convolutional layers and three dense layers really limits the amount of information we could synthesize by our feature Maps. So we want to be able to have our model learn higher order feature maps and by that I mean low level features like edges things like that. Alexnet in 2012 was a a big groundbreaking feat in that it proposed stacking layers of convolutions Max poolings following it up by fully connected layers and coming up with not a very deep network. This kind of uh turned a lot of heads when it achieved a around 17 error rate on imagenet and it won in 2012. Understanding the motivation behind these really sets the stage for uh the the future of advancement in this field so yeah convolutional Nets were proposed in 1990 by Yen lacun and he he kind of pioneered that pattern for today. This is an example with vg16 so this is 16 layers um with three dense layers at the end as well. As you go through your convolutions your output Dimension is shrinking. Based on your kernel size your actual image is shrinking but your number of channels increases as you gain information. The motivation behind it is you're applying a linear function that doesn't lose information as you go across better radio right um yeah exactly that's why the size won't change the important thing the channels change though. soft Max will scale more logarithmically um and it'll give you a final like probability map um are there any questions on Alex now actually before uh we move on yeah what's up yeah so uh if you don't specify a certain type of padding valid padding is going to be applied to make sure that as you're sliding your kernel across an image uh you're left with the same dimension is there anything you want to add Jake or no that's I I should have mentioned adding two yeah I mean you did a good job. a lot of one by one convolutions are used to only change the number of channels and not modify the input size. These are deep and wide to capture features at different scales. depth wise convolutions as well as point-wise convolutions these are combined to form a more efficient computationally method. Mobile nuts are very cool in that you're you're using depth wise Convolutions and Point-wise Convolutions to reduce the computations that you are you're doing. The efficient that's that will be talked about as well. of result from just blindly stacking layers um so you might ask uh we saw Alex Ned and we saw bdg which was like basically the same thing but we added a couple more layers why can't we just infinitely stack these layers um and there are a lot of problems that come with that that Inception and resonance um try to fix and that is adding residuals yeah uh what do you mean by branched uh yeah the classifier is at the end uh by this uh again there's there's a drawing for it but uh essentially you have a multi-headed yeah. Vanishing gradients is a common problem as you add a bunch of layers stacked together and that the learning signal or the gradient computation becomes extremely weak the model struggles to learn. adding skip connections makes the identity easier to learn because you're quite literally adding a previous identity to the resultant of a transformation. If your layer is a convolution the dimension can change which is why often this is result like kind of viewed as f of x plus W of X where W is a transformation that you do on X two to make it the same Dimension. bit smaller than one immediately. At a certain point at a certain number of multiplications your partial derivative uh your your chain rule that you've gotten as a result of many many multiplications just gets sent straight to zero. If your values if all of your individual little partial derivatives of all the individual little steps are a little bit bigger than one it's going to explode um and that's just not helpful [Music] um it's problematic uh we would like yeah we would want to update our weights in way that is like sort of uh sort of regular. of identity as you go through your uh your network if that makes sense but that was a very good catch on the dimensions of X yeah this is often viewed as plus W of x there any other questions about resnet all right dope uh so the next thing to talk about is global average pooling um which is designed to replace fully connected layers in cnns. This is also used in resnet in replacement of the fullyconnected layers um you're generating a feature map for each category of the classification task and then that is fed into the softmax layer. This is extremely important when we want to Stack a lot of filters or a bunch of layers. So you're applying this one-dimensional filter to each channel of your image that is applied to to each one. These are then concatenated together and you apply a smaller pointwise convolution one by one by the number of channels you have to end up with a feature. So mobilenet has a lot fewer parameters which results in a lot faster convergence time. It matches Inception of D3 accuracy just by using depth and point wise convolutions. can quickly go over like squeezing anxiety networks basically uh you squeeze you apply this through a couple of dense layers and then you rescale so we talked about global average pooling. I think the slides are pretty good and compressed in a very visual way uh the remainder of the piano more impressive art it's actually the other way. I I hope the main takeaways are that you'll like understand those rules and that you you see that we've like adding all of these different sort of like tools to your tool belt now. and mobile nuts for Edge Computing and things like that. You want to drastically reduce the number of computations that you want to do yep that is basically everything for today thank you guys for coming oh and there will also be a quiz there will Also be a quizz at least is there a homework position closer or not [Music] um and do next Friday [ music] thank you for sure. And next Friday we'll be talking about how to use Edge Computing in the real world. We'll also talk about how you can use Edge computing in the classroom.

ROUGE-1: 28.41, ROUGE-2: 27.20, ROUGE-L: 22.77
BERTScore: 60.48

==============================================
==================== [7/100] ====================
Summary:
The principles of baking show us how to make bread with and without a bereavement. We'll make three breads one with the brief mint and two without one of the breads without the bereavement will be bulk fermented in the fridge for 24 hours. The third one will be made with a brief mint which will be left for 12 hours to ferment then it'll be mixed into the main dough. The breads will then take the same amount of time to make but why would you add the extra step of making a briefment when cold fermenting?

ROUGE-1: 13.99, ROUGE-2: 12.17, ROUGE-L: 12.58
BERTScore: 67.22

==============================================
==================== [8/100] ====================
Summary:
Bayard Rustin was the chief organizer of the 1963 March on Washington for Jobs and Freedom. Rustin grew up in a Quaker household, and began peacefully protesting racial segregation in high school. He was jailed in 1944 as a conscientious objector to World War II. In the 1980s, he publicly came out as gay, and was instrumental in drawing attention to the AIDS crisis until his death in 1987. In 2013, fifty years after the March On Washington, President Barack Obama posthumously awarded him the Presidential Medal of Freedom. are or who we love.” “I’m so proud of you,” she says. “You’re so beautiful.’ ““I love you, too,’ I say. I love you so much.“” I’ll always love you. ”I will never forget you. ””“We’ve been through a lot. We’d rather be here than there. ’’”

ROUGE-1: 27.06, ROUGE-2: 22.61, ROUGE-L: 22.81
BERTScore: 56.37

==============================================
==================== [9/100] ====================
Summary:
Christine Hayes: Deuteronomy describes God's choice of Israel as the chosen one. She says Israel was chosen by Yahweh in an act of spontaneous love--;it does not imply her perfection. Hayes: Northern Kingdom Israel is going to come in for a very, bad press at the hands of the Deuteronomistic writers. Northern kings are going to be denigrated, she says, because they maintain cults that rival the central sanctuary of Jerusalem, and this is what does them in. Moses warns the Israelites: Don't be tempted to say to yourselves, "My own power and the might of my own hand have won this wealth for me" He emphasizes, it is only because the wickedness of the Canaanites is so great that the Lord has to drive them from his land, and now he is giving you a chance. But it is conditional for you, just as it was for them. Don't fail him or he will, he says. There is one proper response to God's mighty acts on behalf of Israel, and that is resolute observance of the book of the Torah of Moses. imagery, used for God and Israel, but we also have this parent and child imagery that appears. In Deuteronomy 32:10, the image is that of an eagle that bears its young on its wings. It almost seems to play on the idea that when teaching its young to fly, the eagle will push them out of the nest, swoop under them, bear them up for awhile over and over until they get the idea. The use of purity and purity language to inscribe boundaries between Israel and other nations is very characteristic of the post-exilic period. Deuteronomy is not simply the concluding book of the Pentateuch, or the story that began in Genesis. It's also the first part of a much larger, longer literary work, as I mentioned last time. The last dated event that is mentioned in 2 Kings is something that occurred in 562. So the work was probably concluded shortly after that date: so in exile or towards the end of the exilic period. The whole unit, as a whole, was redacted after 622: that's clear. There is a great deal of ideological baggage that is involved in the dating of the sources. The anti-priest, anti-cult sentiment is apparent in the history of biblical scholarship. P espouses a communal ethic, and post-exilic priests are going to turn increasingly to an individual ethic. Many sections of P do not seem to assume a central sanctuary. P contains no universal ban on intermarriage. It does not employ its purity laws or language to mark an inseparable boundary between classes within Israel. The Bible is divided into two parts: the "Former Prophets" and the "Latter Prophets." The Former Prophets include the books of Joshua, Judges, 1 and 2 Samuel. They read as a theologically oriented account of Israel's history from the conquest of Canaan to the destruction of the state by the Babylonians in 587-586 BCE. The Latter Prophets are a collection of books, each of which bears the name of the individual whose prophecies it purports to contain. thought that are evident from Joshua through 2 Kings. There is also a belief in the divine election of David as the king of Israel and his dynasty. In Genesis through Numbers none of the legal materials say: when you have a king this is what he shall do. It is only the book of Deuteronomy that assumes or prepares for a monarchy and contains legislation for a king, and the things that he should do. The other four books of the Pentateuch never mention a king. The formation of the nation state, Israel, was much more complicated than the picture that's presented in Joshua 2 through 12. Scholars have proposed three possible models to explain the formation of Israel. Archaeologists have, indeed, found several sites in the central hill country -- which is pretty exciting-- and they were clearly newly established in the thirteenth, twelfth, eleventh centuries. They extend throughout the land, but mostly the central highlands. And these are thought to be Israelite, especially because they appear in places that the Bible identifies as strongholds. The region was once controlled by Egypt at the purported time of the Exodus. It was also the site of many of the elements that went into the final mix that would emerge as the nation Israel. Archaeology supports this picture of merging of peoples, rather than conquest or even large-scale immigration. Some elements within this group may have brought with them the story of a miraculous escape from Egypt. They may have understood this to be the work of Yahweh, a god known probably from southern regions. Most bloody battles in Israel's history. Then next to that north-south central hill country, you've got also running north to south, what we call the Great Jordan Rift Valley. And the Jordan River runs through this valley. It rises in the Sea of Galilee or the Kinneret in the north, and then it flows about 65 miles, I believe, down to the Dead Sea. At the northern extreme of the Rift Valley, is Mount Hermon, which is the highest point. group that seems to join them. And then 10 and 11 give us two further military campaigns. In Joshua 10:40, we read: "So Joshua defeated the whole land, the hill country and the Negeb" And again, in verse 23, the insistence: "Thus Joshua conquered the whole country, just as the Lord had promised Moses" So Chapters 13 and 21 go on to describe the division of the land among the tribes and then we have some sort of tidying up at the end. In Judges, we read that they had not been captured: they were captured later, after Joshua's death. Archaeological evidence contradicts the picture in Joshua. Excavations at Jericho and Ai indicate that both of these towns were laid waste at least 200 years before the probable time of Joshua. Some of the sites that are said to be destroyed by Joshua and the Israelites weren't even occupied in this period, the late Bronze Age, beginning of the Iron Age. The Iron Age begins around 1200. to have been peasant farmers, like other Canaanites. One interesting difference is the absence of any pig bones, which is kind of interesting. But in any event, this suggests that these settlements were established peacefully, not by a group coming in and conquering. Maybe they emerged from within, rather than being established by peoples immigrating from without. The revolt model proposes that Israel began really as a social revolution within Canaan. Some have suggested that Israelites escaping from Egypt may have joined with these disaffected Canaanites in revolt. perhaps not exclusively, and adopted the national story of the Exodus as its own at some point. The Hebrew tribes, themselves, were likely still in the process of formation. But the tribal structure of Israelite society that would develop would be strengthened by the natural division of the land into these separate geographical areas. And these local tribes probably did assimilate elements of the local population. We've really seen already the ethnic mix of various elements reflected in religious imagery and institutions. We have seen that Yahweh is also represented in terms reminiscent of Baal of the Canaanite pantheon, the God of the settled Canaanite population. practice, which is known as herem or the ban, is not unique to Israel. There is a very important, famous inscription from the ninth century BCE, written by King Mesha of Moab. In the inscription he writes, he boasts: "And the god Chemosh said to me, go, take Nebo from Israel" It is likely that such claims are hyperbolic in Moab, and it is likely they were hyperbolics in Israel. But that does not lessen the shock value for a modern reader, even though war in our time is no less savage. to this when we reach the conclusion of the Deuteronomistic history in 2 Kings. To this we can refer to the end of the book of Deuteronomy as the beginning of the Book of Kings. We can also refer to 2 Kings as the "Book of the Kings" as the start of the "Deuteronomy" book of the Bible. The book is called "2 Kings" and tells the story of the reign of King David and his son, King Solomon.

ROUGE-1: 27.77, ROUGE-2: 26.16, ROUGE-L: 22.19
BERTScore: 61.21

==============================================
==================== [10/100] ====================
Summary:
Bogdan Fedeles: Today we're going to be discussing in detail the mechanism of HMG-CoA synthase. The enzyme is a key enzyme in central metabolism responsible for making the five carbon building blocks from which all sterols, such as cholesterol and steroid hormones are made. To help us understand the mechanism, a crystal structure of the enzyme is provided in this problem. Your support will help MIT OpenCourseWare continue to offer high-quality, educational resources for free. To make a donation or view additional materials from hundreds of MIT courses, visit MIT Open courseWare at ocw.mit.edu. structure, H233. But if you look at this histidine, it's quite far from our cysteine here. And it's only with collecting many different kinds of experimental evidence that we can put together a more definitive mechanism. Once acetyl-CoA has reacted with the enzyme, hence, formed the thioester with the cy Steine 111, we're now ready to proceed with the reaction and form a carbon-carbon bond. Let's try to write the mechanism for that part. of the problem. Second question of this problem is asking about the stabilization of the tetrahedral intermediate, which actually we have just discussed. Question 3 is asking us to review the mechanisms by which enzymes can achieve their amazing rate acceleration, which is on the order of 10 to the 6 to 10 to 15 times over the uncatalyzed reaction. Part 4 of the problem is asks us to look up the structure of coenzyme A, or CoA and then contrast the reactivity of say, acetyl-CoA, the thioester with CoA. with the reactivity of a thioester with a much smaller thiol group. The resonance that we observe in oxygen esters is almost completely absent in thioesters. And this fact makes the Thioesters less stable and therefore, more reactive. Let's take a look at the structure of coenzyme A, which is a huge dalton complex. The advantage of having such a long arm is that it provides a way to insert the substrate, which will attach here, to say, acetyl-CoA. a look. Here is an oxygen ester that shows a proton in alpha position. And as you know, the lone pairs on this oxygen can conjugate with the carbonyl group. So the electrons can move like this, and then we're going to have a negative charge here and a positive charge here. And this is possible because the electrons on both oxygens are found in orbitals of comparable energies. By contrast, in the case of a thioester, we have a sulfur.

ROUGE-1: 22.10, ROUGE-2: 21.13, ROUGE-L: 18.90
BERTScore: 68.83

==============================================
==================== [11/100] ====================
Summary:
Alberto Riva: Most important resources for finding and using biomedical information, especially information connected with the study of the human genome. Most of them will be websites, where you can find information, and to talk about how this information is stored and represented, how it's accessible, and what it can be used for. In the following, we are to see examples of all of these different ways to look at the gene. And finally, we get to the phenotype then, we could put a very long list of things here, but could talk about population genetics. Protein is ultimately what is responsible for essentially all the external manifestations, all the observable properties of our biology. So we have names, of course, for the extremes of the spectrum. We call genotype-- this information is encoded in the DNA. And on the other hand, we call phenotype whatever is at the other extreme of the Spectrum, anything we can observe, we can measure, from the outside. So how is all this information represented? What are the different ways that we can store and describe this information? Where does it come from, where is it stored, how do we find and use it? of DNA dates back to 1952. The Human Genome Project, that was officially declared a success last year, brought us to the point where we now know the exact base pair sequence of our genome. But even if we're all human beings, there are no two human beings that are exactly the same. These differences are due to polymorphisms, like single nucleotide polymorphisms. And it's going to be very interesting to the human genome because it's essentially identical to other organisms, including a chimp. bioinformatics and molecular biology, because now we finally have the tools of looking at our genome with this level of detail. We're going to need different methods to manipulate this data, according to what the purpose of our work is. SNPs are locations where different individuals don't have the exact same nucleotide. At a level of RNA, it might be interesting to look at alternative splicing transcription rearrangements-- these are all things the potential consequences of SNPs. And finally, SNPper is a resource that tries to integrate information from all the places that I sited so far. Finding genes in a DNA sequence is not trivial. It's still something that's very hard to do computationally. The ways that people have been using to do this are usually based on pattern matching. This is one of the things we are working on in our lab. And it's going to take a lot of work but the rewards are potentially very interesting because this is something that will then allow you to build the networks that describe how genes regulate each other. And that's something that, of course, has a lot to potential interest. Association studies are studies that try to correlate the presence of a certain genotype with an observed phenotype. SNPs are the most common form of variation in our genome. They're important because for example, they can be used as genomic markers. For example, if you have a SNP in the coding. subsequence of a protein, you're going to get a protein that has an abnormal sequence. And that can be a change that doesn't cause any consequence, or it might be a very dramatic change. GenBank. And GenBank is at the basis of the NCBI cluster. So the National Center for Biotechnology Information, is a branch of the NIH, that has the task of assembling the largest possible number of databases of biomedical information. So in GenBank, you just have sequences by themselves that can be very short, very long, but they're just independent sequences that were put of all these sequences. And you can probably tell that the number-- we're sequencing longer and longer sequences, because the blue graph grows more rapidly than the red one. LocusLink is a repository of information about genes, and it collects everything that is known about the genes. Golden Path was the first site to provide something like this. It gives you the absolute position of all the known elements of our genome. It provides arbitrary DNA sequences, so you can ask for any region of any human chromosome, you'll get back the exact DNA sequence for that region. It's very easy to find all the information you need about a certain region of the chromosome. Most SNPs are deleterious. But in some cases, the SNP can also provide an advantage, if it generates something that was not present before, and that works better than the original. So if you have a SNP that introduces a change is beneficial, then you will see that the frequency of the SNPs increases in the population. If a SNP is neutral, then there is no selective pressure, and it will either go away by chance, or will stay at a certain basic level of frequency. a second population that is affected by a disease, and you find that in that second population, the alternative allele occurs with a frequency of 40% instead of 20, then that might be an indication that the SNP has something to do with the disease. But in order to be able to do this, you have to know what is the baseline frequency. If a SNP arises in a population, then it tends to be limited to that population. You're not going to find it in a different population, unless there is some genetic interchange between the two. SNPper provides a way of exporting this data in different formats to make it easier to process later. It tells you that this gene is in the coding sequence of the gene, and it actually causes an amino acid change at position 319. It affects protein domains-- this is the list of protein domains that are affected by the SNP. This is a list of some [? matters ?] of the investigators who have observed this SNP-- and it's a long list, so it means that this is definitely a true SNP. that these domains can be overlapping, just because the Swiss people, they annotate the protein sequence saying OK, from here to here, we know that this happens. So it doesn't necessarily mean that the SNP affects all of them in some meaningful way. This one is probably the only one that could be affected by the presence of a SNP. So don't get confused by this place. Just a list of Swiss [INAUDIBLE] domains that include that location. Now, the next step I'm going to talk about genes again. LocusLink provides a nomenclature of genes. No LocusLink assigns a name to each gene, and if you stick to that name, then you're sure that everybody knows what you're talking about. In many, many cases, genes have lots of different names, even if it's the same gene, people have been calling them with different names. UniGene is another resource at NCBI that takes a slightly different approach. It's an attempt at collecting all the GenBank sequences that refer to a region of the genome where a gene is known to be. least in a set of organisms. And if they find a good match between the two sequences, then this pair is added to the HomoloGene database. Right now it encompasses 25 organisms. In these 25 organisms, they have 470,000 ortholog pairs-- so pairs of genes from different organisms that are highly similar to each other. All these are put into the database. And then if you find there are three organisms that share a similarity relationship, this is marked, because it means that you're finding a match that has an even higher quality. is a consequence of the fact that there is a very complex machinery behind it that determines which genes are active or not, and how much, in different conditions. We're slowly working to try to understand how it works. The first thing you need to do is need to be able to reliably identify which transcription factors bind to a given gene. And that, in turn, will determine the spatial, temporal, dependent expression of the target gene. So what we-- OK, sorry. So the first step, again, we're moving the first steps. pattern matching, there is lots of things that you can try. Most of the information in TransFIC is experimentally validated. So you can actually trust the fact that particular piece of sequence they give you is the binding site for the transcription factor in question. And this is one of the things that we're currently working on at CHB-- there are various ways of doing this. And again, it's a rather difficult problem, from a computational point of view, because these patterns that you have to look for are not very very large. GEO is a database of gene expression and hybridization array data. The Stanford microarray database is a repository of all the-- of a large number of micro experiments performed at Stanford. NCI60, again, from Stanford, is a famous data set that includes gene expression profiles for 60 human cancer cell lines. Cardio genomics provide microarray data on mouse models of cardiac development and signal transduction. And finally, human gene expression index-- these are just some of the most important most useful public resources of microarrayData. is OMIM-- OMIM is a catalog of human genes and genetic disorders. It's basically a collection of text articles that talk either about a gene or about a disorder, and they're linked with each other. And finally, PubMed is, a database of citations from the biomedical literature. It contains 12 million entries starting from the mid-'60s, and it's one of the most used resources in the world. It is not very up to date, but you can imagine that it's been growing at least at this speed or faster since '98. I think it's just repeating what we're saying so far that we are drowning in data and converting this data into knowledge is not easy. We need automated tools to access this data, to make sense of it, to convert into formats that we can use. And of course, this is a challenging task, because as we saw, biomedical data covers the whole spectrum of knowledge representation and management techniques that we know about. It's a very, very exciting time for the field of biomedical data.

ROUGE-1: 27.86, ROUGE-2: 26.89, ROUGE-L: 23.94
BERTScore: 69.95

==============================================
==================== [12/100] ====================
Summary:
This week in week three, we're actually going to have some human language, and so this lecture has no partial derivative signs in it. So, I just sort of want to explain something about human language sentence structure, and how people think about that structure. And then talk about how you can make can kind of come for any reason you want, but it might be especially good to come to me if you want to talk about, um, final projects. Okay. Let's get started again. neural, um, dependency parsers. Um, going on just, you know, a couple of announcements. So, assignment two was due one minute ago, so I hope everyone's succeeded in getting assignment two out of the way. Coming out just today is assignment three. In assignment three, what you're doing is building a neural dependency parser, and so we hope that you can put together what you learned about neural networks last week and the content of today, and jump straight right in to building a Neural dependency parser. um, writing a few lines of code at a time. Hopefully that works out for people. Um, if you have any issues with, with that, um, well, obviously, you can send Piazza messages, come to office hours. I mean, the one other thing you could think of doing is that there's sort of a one hour introduction to PyTorch on thePyTorch site, where you down- where you're directed for installing PyTorCh. Um,. now the final mentions, yes. Linguist: I can see patterns, like the cat, a dog, the dog, a cat, et cetera. So, I can also see other examples in my language of the large cat, or a barking dog, or the cuddly cat. And so I want to put those into my grammar. But at that point, I noticed something special, and these things look a lot like the things I started off with. So I could write something like, "The cat by the large crate on the, um, large table, by the door." Right. what you find is that prepositional phrases following the verb in English. But if you go to a different language like Chinese, what you find are the prepositions come before the verb. And so, we could say okay, there are different rules for Chinese, um, and I could start writing a context-free grammar for them. Um,so that's the idea of context- free grammars, and actually, you know, this is the dominant approached linguistic structure that you'll see in linguistics. dependence on other words. So, we have a sentence, ''Look in the large crate in the kitchen by the door''. And if we want to we can give these word, words word classes. But to represent the structure, what we're going to say is, "Well, look here is the the root of this whole sentence" And so, overall, you know, then- let me just so say here, you might want to why do we need sentence structure? You know, the way, um, language seems to work when you're talking. To be able to have machines that interpret language correctly, we sort of need to understand the structure of these sentences. Unless we know what words are arguments and modifiers of other words, we can't actually work out what sentences mean. And so, we need to know what is connected to what in order to do that. And one of the ways of saying, um, that's important is saying, ''What can go wrong?'' Okay. So, this is prepositional phrase attachment. It's sort of seems maybe not that hard there, but you know, it, it gets worse, I mean, it's a real example of a sentence from The Wall Street Journal. cops that are killing. So, this is what we'll say is the subject of kill, is the cops, and I'll just call them the San Jose cops here. And well, there's what they kill which say that, the man is an object of killing. And then while one person is the, the cop using knife to kill the person. And so that's then that this is, um, modifier and here if we complex we call it an instrumental modifier to say that the cops are killing people with a knife. ambiguities in the parsing of English, right? So, here's our prepositional phrase from space. And so, again, we can start to indicate the structure of that using our dependency. So, we have again two possibilities that either we have issues and the dep- and the dependencies of issues is that there are no issues. Okay, that's, um, one. Um, That one is not very funny again. So- so, here is a funnier example that illustrates the same ambiguity effectively. acquisition by Royal Trustco Limited of Toronto for $0.27, $27 a share at its monthly meeting. Boring sentence, but, um, what is the structure of this sentence? Well, you know, we've got a verb here, and we'veGot exactly the same subject, and for this noun,Um, object coming after it. But then what happens after that? well, here, we have a prepositional phrase. And so, well, what we wanna do is say for each of these prepositions what they modify, and starting off there only two choices. pattern of how things are modifying. Once you start having a lot of things that have choices like this, you stop having- if I wanna put an analysis ac- on to this sentence I've to work out the, the right structure. And so, if you get into this sort of combinatorics stuff the number of analyses you get when you get multiple prepositional phrases is the sequence called the Catalan numbers. Ah, but that's still an exponential series. And it's sort of one that turns up in a lot. of places when they're tree-like contexts. So, if any of you are doing or have done CS228, where you see, um, triangular- triangulation of, ah, probabilistic graphical models and you ask how many triangulations there are, that's sort of like making a tree over your variables. And that's, again, gives you the number of them as the Catalan series. But- so the point is, we ha- end up with a lot of ambiguities. So, that's actually a determiner, ah, no issues. Um, and then it's sort of like no heart or cognitive issues. So, heart is another dependent. It'ssort of a non-compound heart issues. And, um, at that point, we ha- have the "Cognitive" as an adjective modifier of the "Issues" and the "No heart", the determiner is just a modifier of "Heart", and then these being conjoined together. modifier of "Experience" and the "Job" is also a modifier of " experience" And then we have the same kind of subject, object, um, reading on that one. Um, but unfortunately, this sentence has a different reading, where you change the modification relationships. [NOISE] We've got this big phrase that I want to try and put a structure of to be used for Olympic beach volleyball, um. And then, you know, this is sort of like a prepositional phrase attachment ambiguity but this time instead of it's a prePOSitional phrase that's being attached, we've now got a verb phrase. to get out of language most of the time. The results demonstrated that KaiC interacts rhythmically with SasA Ka- KaiA and KaiB. We can sort of see this repeated pattern where you have, um, the noun subject here interacts with a noun modifier, and then it's going to be these things that are beneath that of the SasA and its conjoin things KaiA or KaiB are the things that interacts with. [NOISE] I actually mis-edited this. This should also be nmod:with. sentence at the top, submitted and then you say the dependence of submitted, uh, bills were in Brownback. We have a system of dependency labels. So, if you go to the Universal Dependencies website, it's not only about English. You can find Universal Dependency analyses of you know, French, or German, or Finish, or Carsac, or Indonesian, lots of languages. If you have a- a big calling to say I'm gonna build a Swahili Universaldependencies um, treebank, um, you can get in touch. In the later parts of the first millennium, there was a ton of work by Arabic grammarians and essentially what they used is also kind of basically a Dependency Grammar. There was this guy Wells in 1947 who first proposed this idea of having these constituents and phrase structure grammars, and where it then became really famous is through the work of Chomsky. So, in modern work, uh, there's this guy Lucie Tesniere. He sort of formalized the kind of version of dependency grammar that I've been showing you. that wasn't such a good idea, and it turned out to be much better to have these kind of treebank supporting structures over sentences. Once you have a treebank, it's reusable for all sorts of purposes that lots of people build parsers format. But beyond that, this sort of just became necessary once we wanted to do machine learning. So that if is it could parse at 469 sentences a second. I mean, strictly we won over here and we are a fraction behind on UAS. final issue, um, which is we don't want things that, um,. is whether we want to allow dependencies to cross or not, and this is an example of this. So, most of the time,Um, dependencies don't cross each other. Uh, but sometimes they do. And that's sort of rare, that doesn't happen a ton in English, but it happens sometimes in some structures like that. And so, what we say is that the positive sentence is projective if there no crossing dependencies and it's non-projective ifthere are crossing dependencies. And when it's not is when you kind of have these constituents that are delayed to the end of the sentence. to look at in terms of understanding what, um, a shift-reduce parser does. And here's a formal description of a transition-based shift- reduce parser and which also doesn't help you at all. Um, so, instead we kinda look at this example, uh, [LAUGHTER] because that will hopefully help you. So, what I wanna to do is parse the sentence "I ate fish". And yet formally what I have is I have a why I start, there are three actions I can take. And if I would've explored exponential size tree of different parsers, I wouldn't be able to parse efficiently. There's a set of actions and so you gonna build a classifier with machine learning which will predict the right action. Joakim Nivre showed the sort of slightly surprising fact that actually you could predict the correct action to take with high accuracy. Now if you wanna do some searching around, you can do a bit better, but it's not necessary. But so if you're doing this just sort of run classify, predict action, run classify and predict action. We then get this wonderful result. as we chug through a sentence, where we're only doing a linear amount of work for each word and that was sort of an enormous breakthrough. So the conventional way to do this is to say well, we want to have features. And well, the kind of features you wanted was so the usually some kind of conjunction or multiple things so that if the top word of the stack is good, um, and something else is true, right, that the second top word is verb, more to do with each other than others. To build a dependency parser, you're meant to build it with a certain accuracy. And so we can just count up the number of dependencies and how many we get correct. So here in my example, my dependency paths, I've got most of the arcs right but it got this one wrong. So I say my unlabeled attachment score is 80 percent or we can also look at the labels and then my parser wasn't very good at getting the labels rights, so I'm only getting 40 percent. that's sort of similar to when we were making those window classifiers and then we can concatenate a bunch of stuff together. So that gives us in our input layer. Um, so from there, we put things through a hidden layer just like last week. We do Wx plus b and then put it through a ReLU or a non-linearity. And then on top of that, we're simply gonna stick a softmax output layer. So multiplying by another matrix, adding another, um, bias term, and then that goes into the softmax which is gonna give a probability over our actions. bigger, deeper and spend more time choosing the hyper-parameters. Um, Beam search can really help. So in Beam search, you know, rather than just saying, "Let's work out what's the best next action, do that one and repeat over", you allow yourself to do a little bit of search. You sort of say, "Well, let's consider two actions and explore what happens." Um, quick question. Do humans always agree on how to build this trees and if they don't, what will be the agreement of humans relative to [inaudible] [OVERLAPPING] [NOISE]

ROUGE-1: 30.43, ROUGE-2: 29.41, ROUGE-L: 27.81
BERTScore: 70.91

==============================================
==================== [13/100] ====================
Summary:
okay folks let's get started how's everybody doing pretty good Santa Claus has come to town and you know Santa does with naughty kids Hees them finals he gives them finals. He gives them very evil finals is what he does okay so look out for Santa Claus he's really a really a bad guy uh let's see let's think about a couple of things in terms of announcements. We have a couple surprises today one of which is standing in front of you with all this on and there's more surprises as well. have an extra credit question I see well nodding yes doesn't do it it's got to be no that's okay I'm not. I think you know what I like to to have when it comes to extra credit right so we'll we'll see how that plays out there might be some music today I don't know so that's that's possible okay um let's see so that'm basically it uh the the format as I said is the same as before the point changes are different. There are 25 questions in section one that tells you anything. Glycogen phosphor ASE a as the more active form is that usually in the cell when it's present it's in the r State because only when glucose is present will it get converted to the t-state if glucose is not present or glucose is is present in very very low quantities then there will be no gluc glycogen phosph or as a in the T State. glycogenosphor B on the other hand is much more likely to be found in the R State. Def phosphorilation on theOther hand favors the breakdown of glycogen. things all come together now again I'm not going to ask you to rank them or do complex scenarios with these that's not the point of telling you all that information but it is important that you understand all those different types of Regulation. The phosphorilation of proteins is favoring the breakdown of glycogen. The r&t involve allosteric affectors okay all right okay so that's uh where we start now um there's a couple things about regulation that we haven't talked about and you say my God it's already you know murderous right well. scheme that exists in muscle all right well what does this tell us it tells us first of all that pp1 is bound to another protein and the unfortunate name is this protein is called G subm in this case and we'll call it g subm that's not a g protein it's just holding on to phosphoprotein phosphatase phosph protein phosphat enzyme okay now what happens well this phosphotase has to be regulated okay when thisosphatase is in linked to GM as you see it here it is the most active okay it is most active so we're going to follow from left to right what happens when epinephrine gets. Liver is the only tissue in the body that produces glucose glucose. When I stop exercising my tissues don't need it. When my blood glucose levels go up what starts happen happening to my liver's ability to push out glucose. glucose is a poison we're avoiding the poison with this very very Rapid Control that we've got here it's a very very important Point okay everybody understand what I've just told you so being able to carry around its own off switch allows this guy to merily go along its the most efficient way that it can turn itself off really quick. There are a variety of what are called glycogen storage diseases and these diseases arise as a result of efficiency of certain enzymes either in that pathway or related to that pathway. There are several of these voner disease uh this one lacks a glucose 6 phosphatase and affects liver and kidney and what happens to the glycogen. I will autograph these for you you can sell them on eBay I you know you don't know how much you might make from your knowledge here okay so knowledge is power knowledge may be money so you might meet some exciting people from having these things too. thing if we look at what happens with the thing here's here's a a plot of the concentration of ADP. The person with McArdle disease sees ADP levels go high and then it falls meaning that the cells are catching up and making ATP now for that last chance at a CD my question to you is what's making this possible. It's something we learn during the term something we learned a very important process that allows these people who have this disease to lead a fairly normal life. that's really odd you know there's all these things out there that relate to music that involve the letter B okay so the Beatles I mean they were wow what awesome things and you guys aren't old enough but what succeeded The Beatles was this group called The Beggs anybody hear the bgs oh you heard the BS okay Night Fever Night Fever right okay so anyway and what succeeded the bGS was a really important group known as right the back street boy what a group huh and of course what succeeded them we know of course was um yeah low moment in music I think but if you thought you've seen low moments in music You Ain't Seen Nothing Yet let me introduce the next bees that are going to turn the music world upside down. hard okay it goes how you doing uh they're back in my office come back and see me my office there now uh I'm going there in a minute yeah I got to take everything down but [Music] yeah e e e. hard okay it went how youdoing uh they were back in his office. Come back andsee me myoffice there now.hardOkay it goesHow you doing? How you doing.hard? How are you doing?""I'm fine. I'm fine," he says. "I'm going over to my office. I've got to get everything down"

ROUGE-1: 29.28, ROUGE-2: 27.43, ROUGE-L: 26.15
BERTScore: 67.83

==============================================
==================== [14/100] ====================
Summary:
JUDY HOYT: Hopefully everybody's recovered from their Thanksgiving feast. What I'm showing up here is the schedule to orient us. This is lecture 22. We'll talk about silicides, device contacts, and I've added in a new material this year on novel gate materials. And then we have one more lecture, which is on strained silicon and silicon germanium growth and processing. Next week we have two class periods scheduled. There'll be four speakers in each. And those will be the oral reports and the student reports. how we make the structures and make good, electrical contacts and the device. This lecture is kind of the first link to the backend technology course. The contact itself is still generally considered part of the frontend because you're contacting the silicon. And then beyond, that everything is in the 6.773, which is the backend technology. So this is the rho Civeiveive lecture. We'll go through it at the beginning and how they compare now and how we do that next time. In the early days, if this was the drain or the source of a transistor, a MOSFET, people simply used aluminum. It's a very stable metal, and it makes good electrical contact to heavily doped silicon. The only other metal of consequence that has a lower resistivity is copper. Copper has a much high melting point. Copper, if it gets into silicon, can be a deep level. Copper oxidizes very readily, even at room temperature. But copper is the material of choice for today's interconnect. Aluminum has a finite solubility for silicon. The silicon actually gets sucked into the aluminum and causes this spiking effect. Today if you do this with any kind of reasonably shallow junction, when you go to heat the thing up to do the final forming gas anneal, you're going to spike the junction. The way people go today is to form what they call a barrier layer. It forms a barrier between this aluminum and the silicon. It still makes a good contact, but it cannot allow the aluminum or the silicon to talk to each other. An ohmic contact, or a tunneling contact, occurs when you can actually tunnel right through this barrier. And it tends to happen, particularly with aluminum and most metals, if you make the surface doping very high. The resistance of a given contact in ohms is just the voltage divided by the current flowing through that contact. And you can calculate it by rho sub C, which typically has units of ohm centimeters squared or ohm micron squared-- so it's got a units of resistance times an area. As I make the area the current goes through smaller, the resistance of that contact goes up. And this is a problem if I'm scaling devices. If I'm packing more and more devices on the chip, it means I want to make the total area occupied by every device smaller. But if I leave the contact resistivity the same, I don't do anything different about how we make the contact, then the area of the contact is going down. That means the resistance is going up as devices are scaled. A Schottky contact is governed by thermionic emission. So it's a thermionic process emitting carriers over this barrier. So if we change that barrier height, we can change rho sub C. So in fact, for a tunneling contact, if you go back to your quantum mechanics, you know that the tunneling current is exponentially dependent on those two quantities. And that happens when you make the doping high. So how do I make the depletion layer in the semiconductor, is very, very small. Rho C is not really scaling because of the doping electrical solubility limit. So the contact passivity doesn't scale as we shrink technology. This first was discussed by Ming and Lynch back in the late 80s. So our spreading is one that generally has to be computed either by a two-dimensional simulator or you have to get it out of test. It's a very two- dimensional problem. You can't calculate it by hand. So this that's called the RSP here in this little diagram. The resistance of the current just to flow through this contact is 100 ohms on the drain side. It's going to be 100 ohm on the source side, as do something to all these other parasitic resistances. So there are three resistances we really want to be able to think about. These back-of-the-envelope calculations are terrific because you can do them in five minutes in class or in 10 minutes in your office or whatever. You'll get three numbers, and you'll get you three numbers. Figures 11-35 show how the N plus diffusion structure is made. The dashed lines represent the metal level. This is going to take at least three masks to powder. And these little square regions with the X's going through them are the contacts. So that's the region where one layer contacts the layer above it or below it. And you get 3.2 times 10 to -7 ohm centimeter squared. It's still about a factor of 2 too high, but it just gives you an idea. same probe points? And the reason you do this is because you don't want to have extra contact resistance, say, of your probes going down. And there's always a voltage drop there. So you just divide the voltage drop divided by the current. And that gives you a resistance number. In fact, that's 32 ohms. And by definition, that’s equal to rho C divided by. the area of the contact, so you can solve for rho. C here. it is oxide. These are 5 by 5 micron holes. These were annealed by rapid thermal processing. And then the aluminum layer was removed just to see what happened. This was aluminum directly in contact with silicon at 10 minutes at 425. You can see it's created all these voids. There's a solubility of silicon in aluminum. Silicon from the substrate has actually gone up into the aluminum. And it leaves behind voids, which end up being filled by the overlying aluminum. When you etch the aluminum off, you see all these holes. The materials, titanium and tungsten, have reasonable contact resistivity. They are still not low enough to meet a lot of the ITRS requirements in the future, but today they're good enough. Slide 26 shows you some of the different uses of silicide in silicon technology. And at the end, I've added a few slides that show you that people are actually using silicides as metal gates. They put it back in the back in RTA and they pop it up to 800 or 900 or 900 to form anneal. a conformal layer of SiO2, say low temperature oxide, and then you etch it back anisotropically. You end up with these little stringers on the edges, which we call sidewalls. So they're going to form sort of a blocking region, which will be critical in the salicide process. We then deposit metal, M, over everything. So you can do some kind of PBD deposition. And here's your titanium layer that goes everywhere, obviously across the whole wafer. that does not etch the silicide itself. We've created a material, a titanium di silicide or a metal disilicide, that because of the virtue of its chemical structure, it now stands up to the etch, which will remove the unreacted metal. Big problem with nickel is you have to be careful of your thermal budget. Nickel is a very fast diffuser, remember, in silicon. Nickel contamination of equipment and of the wafers is an issue. But nickel silicide is becoming more and more prevalent in research and development. it's not that thermally stable. Industry moved a number of years ago primarily from Ti-silicide, although some people may still use it, to cobalt bisilicide. Cobalt has a little less lateral encroachment over the oxide spacer. So cobalt was used and is still used in some processes. The latest silicide that people are exploring in research and development-- and at some point will probably be in production-- is nickel silicide. Nickel silicide has the lowest silicon consumption. Nickel silicide can be used to make a better gate material than Ti-silicide or platinum. It's not as simple as using the old fashioned salicide process where the thickness that you silicide on the gate was the same as the thickness you went in the source and drain. But it's not quite as complicated as completely replacing the gate in the etch-out process. FUSI gates are also interesting, just because it's a little easier to integrate the processing. Poly is an extremely easy material to integrate. People know how to etch it. It's not reactive with SiO2. That's not the case for metals. A metal has 10 to the 23rd.centimeter. So they want to remove the semiconductor from the gate. They would like to replace the poly with a gate, with a metal gate. The problem with poly, as we've mentioned, is it doesn't have enough carries. It tends deplete. So people want to use metals. did, they get a reasonably smooth interface. This is a device that has some advantages, although it's a little tricky to make. It's a double gate device. You end up getting two channels in this thin silicon film. So they may have FinFET and they demonstrated they could make a nickel silicide gate going all the way around that fin. In fact, this is the gate dielectric shown here, this amorphous-looking region with Nickel silicide on the outside.

ROUGE-1: 21.15, ROUGE-2: 20.20, ROUGE-L: 16.94
BERTScore: 62.51

==============================================
==================== [15/100] ====================
Summary:
The dagger algorithm aims to provide a more principled solution to the imitational and distributional Shi problem. The idea in dagger is to actually run the policy in the real world see which states it visits and ask humans to label those States. The goal is to collect data in such a way that P Pi Theta can actually learn from the data it's trained on. The basic version of dagger works like this and that's the version that you will all be implementing in your homework. It's a very simple algorithm to implement if you can get those labels. It can actually get up to fly pretty reliably through a forest dodging trees. copter rotors to make it do some really complex aerobatic trick if you want humans to control all the joints in a complex humanoid robot that might be even harder maybe you need to rig up some really complicated harness for them to wear. If you want to control a giant robotic spider well good luck finding a human who can operate that. When learning autonomously in principle machines can get unlimited data from their own experience and they can continuously self-improve and get better and better in principle exceeding the performance of humans. negatives of one another and the reason that we see both sometimes is the same kind of a cultural distinction that I alluded to before remember I mentioned that we have S a uh which comes from the study of dynamic programming that's where the reward comes from in optimal control it's it's a bit more common to deal with costs in America we are all very optimistic and we think about life as bringing rewards maybe there's something to that but for the purpose of this class don't worry about it C is just the negative R.

ROUGE-1: 28.75, ROUGE-2: 27.41, ROUGE-L: 27.29
BERTScore: 60.02

==============================================
==================== [16/100] ====================
Summary:
Jonathon Gruber: What stops people from bingeing on everything? It's their budget constraint. He says budget constraints can help with a lot of kind of decisions in life. Grubers: How do you determine your marginal rate of transformation? He says we're going to answer that question very explicitly in a lecture next week, and then we'll talk about why changes from price changes differ from income changes and what are the underlying mechanisms behind that. He ends the lecture with an example of food stamps. budget has to be divided between pizza, where there's the price per slice of pizza times the number of slices of pizza, or cookies. The budget constraint represents-- the budget constraint, the slope of the budget constraints, is the price ratio. Every extra cookie that you buy, holding your income constant, lowers the amount of pizza you can have by p sub p. The rate at which you can trade off pizza for cookies is minus 1/2, OK? That is every additional cookie would require giving up half goes from $12 up to $18. The more you spend on one, the less you get of another. By having more of one, we're getting less of the other. The marginal benefit to you of another cookie relative to another pizza is higher than what the market will charge you to turn pizza into cookies. Weight Watchers has taken this principle to heart to develop the best method of weight loss in America, which is Weight Watcher's. The rate at which you are willing to give up cookies for pizzas-- I'm sorry, let me say it a better way. Jonathon Gruber: Weight Watchers sets up a budget constraint and asks you to follow it. He says they assign point values to every good you might consume. People like to have choices, and they like to let choice drive things, he says. GrubER: If people knew best to do this, then there'd be no reason to do it. It's paternalistic, but in this case, it might make sense, Grubers says. He ends by saying, "Let's change the label on the y-axis, just a small change" changed, only the other price. So it's a pivot inward. The other thing here, you'll notice we have all these funny dots and stuff, OK? That represents what has happened to what we call your opportunity set. Your opportunity set is the set of choices available to you given your income and market prices. So your chance set initially was the black dots plus the red dots. Now your opportunitySet has shrunk. Your chance set is now just theblack dots. Given your income, you can now get less stuff, same amount of cookies, but less pizza. math in a little bit, and you'll do more math in section on Friday, OK? But, essentially, you can solve-- we'll show you-- you'll drive on Friday how you take this utility function and literally can draw the indifference curves from it. And what we see is that point D is the furthest out indifference curve you can achieve while still meeting your budget. And, therefore, we say that the optimum, graphically, is the tangency between your indifference curve and your budget constraint is the optimal constrained bundle. The marginal utility of pizza is 1 over square root of 10. The marginal rate of substitution is minus 2.5. That is a meaningful concept. Utils are not, but that is that. So we only care about it in ratios. So let's do the marginal Utility of cookies. That's dU dC, which is 0.5 times P oversquare root of P times C, which. is 2. 5 over the squareroot of 10, OK? That's the marginal utility. Jonathon Gruber: You are willing to give up 2.5 slices of pizza to get one cookie. He says the market is saying we'll let you have a cookie for half a slice of pizza. GrubER: Eat less pizza. Eat more cookies. That will unambiguously make you happier, he says. The whole course is fundamentally all about one equation, which is marginal benefits equals marginal costs, Grubers says. "Consumer theory is all about this balancing act," he adds. Jonathan Gruber: Why not just give people cash? Why not give them food stamps? Gruber asks: Why does it make no difference for person y if I give him food stamps or cash? He shows you graphically how we think about the trade-off, and then we'll come to the answer. Gruber says the marginal benefit is the MRS. The marginal cost is theMRT, and we want to set them equal. He says if you get food stamps, you have to spend at least $500 of your money on food. Jonathan Gruber: People always get to the point that makes them happiest. He says if you force them to spend $500 on food, they must be less happy. Gruber says people would rather spend some of that money and find a nicer place to live, but we're not letting them. He asks: Do people understand the graphics here and the conclusions I drew? Do people-- I don't want to-- I just want to know if people understand what I'm talking about. Jonathon Gruber: If the taxpayers' goal is to help poor people, then why shouldn't you make them as happy as possible? He says in developing countries, the answer seems to be just giving people cash makes them better off. Grubers: Evidence is starting to pour in that it might not be worth it because there's starting to be a lot of experiments where we're giving people just cash, especially in Africa in particular, some in the US, he says. women up nine years later. The effect had totally gone away. They're not worse off, but it looks like, at least what in the short run made them better off, well, that effect fades over time. But the bottom line is, at this point, I think the evidence is sort of probably in favor of being less paternalistic and just giving people cash, but that runs into a lot of difficulties in terms of our concerns about how people will spend it. So let me stop there.

ROUGE-1: 20.42, ROUGE-2: 18.05, ROUGE-L: 16.31
BERTScore: 61.91

==============================================
==================== [17/100] ====================
Summary:
Professor Shelly Kagan: Life on the experience machine is perfect as long as you've got the right tape playing. But the vast majority always says, no, there's something missing from that life, she says. Different theories of well-being might answer that in different ways, he says. He says we don't have any kind of accomplishments, we're not in the right kinds of loving relationships, because we want to have explanations as to why we're valuable to be with. In principle we could still evaluate rival lives. We could still say it's--how good your life is, is a matter of adding up the goods and subtracting the bads. But we would now have a somewhat broader, or more encompassing or inclusive, list of goods, and a more broad and encompassing list of bads--not just experience, but also these various other accomplishments, whatever exactly that list comes to. It varies from case get bad enough, that can outweigh the value of being alive so that the grand total is negative. Hedonism is a version of the neutral container theory. Life may have value in and of itself, but it's not mere life. What we want is the life of a human. We want a life in which we're accomplishing things, there's agency. Because you have to be a knower in order to have knowledge. The life of somebody who can have an emotional side is something like theLife of a person that, when we say, that being alive per so is valuable, presumably what they mean is being alive as a person per se. subtract the ignorance and deception. Doing that in terms of the contents gives you a subtotal, but that subtotal is no longer the entire story. If we accept a valuable container theory, we also have to add in some extra positive points to take account of the fact that, well, at least you're alive or have the life or a person--or whatever it is that you think is valuable in and of itself. Even if your content subtotal was negative ten, that doesn't mean you're not better off alive. If you're a modest, if you accept the modest version of the valuable container theory, then if the contents get bad enough, that can outweigh the positive value of life. Against that, fans of the fantastic valuable container Theory can say, it doesn't really matter whether Williams is right. The value of being alive per se outweighs that. So more life would always be better, no matter how horrible the contents might be. So being immortal really would be a good thing for you. Death always is a bad thing. optimists say, "Even if immortality would be bad eventually, the next chunk of life would've been good for all of us" pessimists might say "Boy, death comes not a moment too soon for any of us." moderates say "For some of us, death does not come too soon" Some of us make it to the ripe old age of 80,90 a 100 or more. Others of us die at 20, or 15, or 10, or younger. Even if death were inevitable, it wouldn't have to come in different-sized packages. LZ Granderson: The fundamental badness of death is that it deprives me of life worth having. He says there are other features of death, as we experience it, that are separable from the deprivation account. LZ: We might ask, what about this inevitability of death? Does that make things worse? He says if we realize that there's nothing I can do about the fact that I'm going to die, then perhaps some of the sting, some bite, is eliminated. in life is necessary, then we'd get a kind of emotional distance from it. We could no longer be disappointed, because to be disappointed in something presupposes that it could've been some other way. Spinoza thought if you see that it couldn't go any other way, then you can't be sad about it. Well, maybe that's right, but going back to the firsthand, I don't know how many of you have read Dostoyevsky's short novel The Underground Man. even worse is I'm going to get even less than the average amount of life. That's clearly an extra-bad. But we might then wonder, for every person who gets less than. the average, there's another person who has more than the median amount of. life. We care more about being short-changed than we do about being, as we might put it, overcompensated. We've had inevitability; we had variability. What about unpredictability? Not only is it inevitable that you're going to die; you don't know how much more time you've got. kinds of plans. And in particular, it's hard to know how to pace yourself. We all know the Horatio Alger story right? care about the overall shape of our lives, we might worry about wanting it to have the right shape overall. Where and when do you want to peak, as it were, in terms of your accomplishments? Well, that matters to us, but the trouble is, without predictability you don't know where to put the peak. All of this suggests then that the unpredictability of our death adds an extra negative element. It makes it harder to plan what the best way to live my life would be. Somebody starts out poor and makes his way through hard work and dedication to riches and success. That's the Horatio Alger life--H.A. Great life. Instead of the rags to riches life, imagine the riches to rags life. It's the reverse. The story "bad to good" is the kind of story we want for ourselves, while the story "good to bad" is a story we don't want for our own lives. Why do we care more about future non-existence than pastnon-existence? to be in the future or did it take place earlier today? You don't remember. We're not indifferent. We want the bad behind us, not the bad in front of us. We have to worry then that because of the unpredictability of death that our lives may not have the ideal shape. A lot of us might feel that a life like this, where we peak but then we stick around--you know, isn't--can at least fail to be as desirable in which we end with a bang. time.com will feature iReporter photos in a weekly Travel Snapshots gallery. Please submit your best shots of the U.S. for next week. Visit CNN.com/Travel next Wednesday for a new gallery of snapshots. Visit Time.time.co.uk/Travel each week for a different gallery of photos from across the globe. Click here for a gallery of the best shots from around the world. Visit TIME.com: Travel next week for snapshots of the world's most popular destinations.

ROUGE-1: 25.38, ROUGE-2: 23.22, ROUGE-L: 22.10
BERTScore: 62.09

==============================================
==================== [18/100] ====================
Summary:
One banana contains a minuscule but measurable amount of radioactivity. To boost your confidence on any sort of radiation measurement, boost your signal strength or to boost your counting time. If you eat bananas, you're intaking a fair bit of radioactive potassium, which is a positron emitter, and also it does electron capture and all that fun stuff. To measure some count rate in some experiment, we'll put this in units of counts per minute, which would be the number of counts divided by the count time. detector when it's a certain distance away? So I've actually laser-cut out a little Geiger counter jig from a previous class. And you guys can all do this too. Anyone want to guess what the maximum value of the red curve is on the graph? Anyone want a guess on the distance from the source to the detector in meters? MICHAEL SHORT: A centimeter. Luke, do you know what the relationship is between dose and distance or measured activity and distance? formula would break down. What happens as r goes to 0? What happens to our solid angle or our approximation for our solid angles? AUDIENCE: Goes to Infinity. MICHAEL SHORT: It goes to infinity, right? Can a detector actually take up infinity area on, well, anything? Never mind that unit sphere. Never mind the thickness of the plastic. Nevermind the surface area of a unit sphere with radius of 1. Do you know how many of you have heard of solid angle before? Students take home about 50 pounds of bananas or 50 bananas-- I forget which one. The only thing you don't know is the activity of this bag of bananas. You're going to be able to measure the number of potassium 40 counts that the detector picks up. So this is just picking up all the gammas coming out of the bananas and everything else that happens to get through the banana and all on the weekend. So it'll take a while to let this count until Tuesday. Because, why not? And I don't feel like coming in over the weekend to do it. as a motivating example. Michael Short: "There's no better concentrated source of smoking radioactivity than a smoke shop" "How long would you actually have to bring a detector in and count in order to be sure that there's any sort of measurable difference?" "It's still late-stage. It's like town-to-town. Most of the Boston area is 21. But once you leave Boston-- "I don't think it is where I'm-- from Swampscott, I don't Think it's 21" the less certain you can be that the number that you're measuring is actually accurate. By counting for longer you can decrease your standard deviation. The more error you allow, the shorter time you have to count for. Count for more time in the background, and it takes less time to distinguish whatever is your source. So if you want to get to the uncertainty and confidence you want, you need to double your count time in order to get the confidence and the time from your background to get it. If you just add together the two standard deviations, you actually always get an overestimate of the true error. So in this case, that would be your worst case scenario, which is not your most likely scenario. What you actually want is to do what's called uncertainty in quadrature, where you actually add up the sum of the square roots of those errors. With enough statistics, if you count for long enough or you count enough counts, then these things, on average, are going to add in Quadrature. Michael Short: How do you know that we're 95% confident of our count rate plus or minus 5% error? That's the main question for today. We'll spend the rest of today's class taking apart that statement and getting at what it should be. We've got a nice relation now between the count rate and counts per minute, and the required time to get 5% uncertainty. So this is actually how you decide how long you have to sit in the smoke shop to count in order to satisfy what we asked for. radiation detector. How long do you have to be here, looking all weird? You want to have an answer. And so if you get some initial estimate of C g, you can tell him this is my approximate t g. So why don't we just start, divide by 2, right? Divide by 2. We can square both sides. And there's a C n there. Square both sides, and we end up with 0.000625 C n squared equals C b over t b squared plus C g over t g squared. Cobalt 60 gives off two gamma rays per second. Activity is measured in disintegrations, not in number of gamma rays emitted. So you've got to know what material you're looking at in order to know how many gamma or how many betas or more that you're going to get per disintegration. If you just add the errors up, you're probably overestimating the error and selling yourself short. The idea here is that, again, if you've looked at the chart of the nuclides, you can tell that there's all the sort of parameters you would need to calculate. There's not a whole lot to see in there. If you've never seen it, it's worth a look. You can't really see the crystal. The black part is just a carbon fiber window, because you don't want to cut off the low energy gamma. And the 23.9, you've got what the gammas are-- 685, 479. it's got a whole mess of gammas. So you could, knowing how big that peak is, calculate how efficient the detector is for collecting that peak. out the thermal noise. Because you're looking for really tiny little signals here, so you cool everything down. So that way, it's not too noisy. These guys are OK warming up. It doesn't destroy the detector. The old detectors you had to keep cold all the time. And if they warmed up, then they were just paperweights. So this is just the counting lab. I've got an actual sample counting in here right now. We'll take a look at the spectrum in a minute. Guys chemists at any point in your life? You all took some chemistry at some point? OK. So you've run a standard, which means a material that how much tungsten is in it or how much a whole mess of other things are. So I don't actually use the cross sections, or the flux, or any of that other stuff-- all of those parameters disappear. Notably, the detector efficiency disappears out of the equation, because that's the parameter that you usually have the funniest idea about. When you're running NAA, you really want to avoid having all these fast reactions. There's usually an up a valve and let more neutrons in. And when you get to the place where you want to be, you basically close that valve again. So you basically add reactivity and then stop that reactivity addition by bringing the absorbers back to about where they started from. When the rest of you sit down here, we'll guide you through those-- the log book entries that she's making and so forth. NAA has been doing NAAs since the '70s. The environmental side of this has kind of quieted down a lot. But it's still useful for a bunch of things. NAA is really good for rare earth elements, which are hard to measure by other methods. By picking out various rare earths and the ratios, it can help identify where things are from in the world. The full size of a lead brick is about 2 inches by 4 by 8 inches, weighs about 25 pounds. Fish samples that we actually did the fresh fish samples. And we had this kind of titanium blender-- you remember the Bass-O-Matic? We had this titanium blender that we dropped the fish in, and you completely homogenized the fish. And then you took a little sample of it, and freeze dried it and then analyzed it for mercury. And because we did a fairly short of radiation, after a while the activities died down and we gave the samples back. We found that it didn't correlate with the well water or how small it was. This is some soil from Montana next to a mine, so it's nicely contaminated with some metals. This is my IAEA mercury and hair standard. And this is kind of what everybody uses for standards. And you just kind of have a whole collection of them. And depending on what elements you're looking for, you try to mix and match them so you cover what you want without having to run five or six of them in one place. And so that's how I do the comparative method. well-- they say, is there anything dangerous in the reactor? The dangerous thing is dropping lead bricks on your feet. So I've got steel toast. If I miss the toe, I'd probably break my-- I don't want to think about it. Have you toured the reactor yet? AUDIENCE: [INAUDIBLE] MICHAEL AMES: So there's that giant crane there, and they move five-ton pieces a shielding. And that's the other dangerous thing in there, dropping really big things. The experiment is in two parts. The first is raising reactor power using a low worth absorber called a regulating rod. And the second part will be lowering reactor power with a high worth absorbers. To actually do this experiment, we need two licensed people in here, one at least has a senior reactor operator. The only way you can actually do these manipulations is if you're in my training program or in a program that needs you to actually operate the reactor. The experiment we're doing is basically change reactor power by half a megawatt. reactor is on autocontrol. And when we do these manipulations, the reactor operator is going to take manual control. That'll cause an alarm to come in. And this will only happen for the first time. And that should be the only time you hear this alarm, because we'll leave it on manual control until the final participant has done their manipulations. AUDIENCE: All right. I hope to get to 1 megawatt at 17.11 [INAUDIBLE].. FRANK WARMSLEY: OK. period meter, she's at a negative period, and the reactor power is decreasing. She's driving the absorber out again to slow down how quickly the power level is going down. When she's done, the shim blade will end up about at the same point where it started, the 13.42 inches out of the bottom of the core. It might not make it all the way back up to [INAUDIBLE]. It'll be close. Compensate with the reg rod if you need to.

ROUGE-1: 22.45, ROUGE-2: 21.30, ROUGE-L: 17.69
BERTScore: 60.38

==============================================
==================== [19/100] ====================
Summary:
Professor: We don't need to invoke any notion of rationality in evolutionary game theory. Instead, you simply have mutations that sample different strategies, and then you have differences in fitness that just lead to evolution towards the same solutions of the game. Professor: All the selection that we've been talking about in the last few weeks, that all is consistent with game theory in the sense that the fitness of individuals depends upon the rest of the population. The more fit individuals spread in the population, you evolve to the same or similar solutions. Professor: In a population with a constant population size N equal to, in this case, we'll say 1,000, mutation rate is 10 to the minus 6. Each time that an individual divides, it has a 1 in a million probability of mutating. In practice, it doesn't actually matter, because all these mutations are non-neutral, once you fix this state or this one, you can't go back. Professor: If anything, in some ways, this actually provides a bias going towards the 0, 1 state. Professor: We're going to start in the 0, 0 state with 1,000 isogenic individuals. He asks: What's going to happen eventually? And in particular, what path will be taken on this landscape here? Professor: You can start thinking about it while I write out some possibilities that we can vote for, and I'll give you a minute to think about it. The population will get there, and the 1, 1 genotype will fix in the population, he says. In some ways a very simple problem. But in another way, you have to keep track of lots of different things, and which regime we're in. If you understand what's going on, you can answer it in a minute. But if you don't understand it, it'll take you an hour. OK, so we do have a fair range of answers. I'd say it might be kind of something like 50-50. And that's great. It means that there should be something to talk about. So turn to a neighbor. You should be able to find somebody that disagrees with you. that are present maybe in one copy. In order for this individual to fix, he has to survive stochastic extinction, which happens with the probability of 2%. And the 1, 0 individual has to go extinct, which happening 90% of the time. So this is, indeed, answering the question that if you had one copy of each of these two mutant individuals in the population, that's the answer. But that's a slightly different question than if we ask, we're going to start with an entire population at 0, 0. The population is dividing. Every now and then, a mutation occurs in the population. It could be either the 0, 1 or the 1, 0. But in either case, the fate of that mutation is resolved before the next mutation occurs. So you don't need to worry about them competing. Instead, just at some constant rate they're appearing. And given that they appear, there's some probability that they're going to fix. So that leads to effective rates going to each fix. And it's tiny. In the Prisoner's Dilemma, the defector fitness is always above the cooperator fitness. So for any population composition, defectors have higher fitness than cooperators. So evolution brings you to the pure defecting state, where you have fitness 1. If the population is A, that means that the A here is at 5. But then it goes down to 0. Whereas over here, B here is 3. And then it go to 1. Because these two lines cross, does that mean that you have bi-stability? 10 to the 8. So it's not this. But it's tiny, right? AUDIENCE: Yes, [INAUDIBLE]. PROFESSOR: Yeah, because this didn't make sense. Because this was of the same order as-- well, this would be larger than 1 over N, so it's totally nonsensical. This is a deleterious mutation. It's not even nearly neutral. So this whole thing is 10 to the minus 10, or something like that? In a Nash equilibrium, everybody's interacting with everybody else with equal probability. If A follows strategy one, B follows strategyOne, then individual A gets little a fitness. If both players had followed this strategy C for cooperate, D for defect, then both individuals would be getting fitness 3, or payout 3. But the problem is that that's not evolutionarily stable, or in the context of game theory, that is cheatable in some ways. The question is whether you as an individual would have the incentive to switch to the other strategy? And the answer is no. in this regime where everything's linear. And the probably best well-known of these is this Prisoner's Dilemma, which is the standard model of cooperation in the field of game theory. So the idea of the prisoner's dilemma is that if you set up these jail sentences in the right way, then it could be the case that each individual has the incentive to confess. And we'll call this-- so this is for individual one, say and individual two. So there are different strategies you can follow. what happens if we're a population of cooperators. Now everybody has high fitness-- fitness 3. Question is, what happens if there's a mutation that leads to one individual following the D strategy? Is he selected four or not? AUDIENCE: Yes. PROFESSOR: Yes, so the point here is that you always will have higher fitness, regardless of what your opponent does in the context of a game theory situation, or regardless of the distribution of cooperation and defection in the population. In a population, if you have genetic A's and genetic B's that are each giving birth to their own type, then you evolve to some coexistence of genotypes. So long as you have some members of both A and B in the population, you'll always evolve to the same equilibrium. Whereas in this situation over here, we have coexistence. Does not matter where you start. So that's a molecular mechanism for how you might get heterogeneity. Another question to have a variety of different strategies to cope with that uncertainty. is, what is the evolution explanation for why that behavior might have evolved? Now in general, we cannot prove why something evolved, but we can make educated guesses that make experimentally testable hypotheses. In the coming weeks, we'll talk about this idea of bet hedging-- that given uncertain or fluctuating environments, it may be advantageous for clonal populations to hedge their bets on the outcome of a game. It may be implementing the solution of some game that is a result of such frequency dependence. There are other possible explanations to this. unilaterally increase your fitness by switching. PROFESSOR: Right, it's an equality, which means it is a Nash equilibrium. Because it's saying that you don't have the incentive to change strategy. It's true that you're not dis-incentivized. So it's not a strict Nash equilibrium, but this thing is true. And this other thing that's interesting is that-- so this tells us that it's actually one of these ESS's. And if you have questions about this, I'm happy to answer it.

ROUGE-1: 21.42, ROUGE-2: 20.52, ROUGE-L: 17.15
BERTScore: 63.58

==============================================
==================== [20/100] ====================
Summary:
 sinus tells us that we're dealing with a heart rhythm that originates in the sa node. The sa node is located in the upper part of the right atrium. If this process is occurring like it should it should cause this heart to beat at about 60 to 100 beats per minute. If the sa nodes is not firing rapidly it's called sinus bradycardia. The sympathetic nervous system is really causing this to just fire electrical signals fast because maybe you're exercising or you're really stressed out you're in a fearful event. With sinus hack the atrial rate is going to be greater than 100 beats per minute. The p wave represents atrial depolarization so with that you want to make sure that you count your p waves. The qrs complex represents the ventricular rate so just like with the p wave you're going to count your qrs complexes and that's going to give you the rate. With sinus pack the qrs Complex isn't too wide or too narrow so it should be less than 0.12 seconds so it measures about 0.08 boxes. to confirm that measurement throughout by just going down throughout the strip and measuring that complex then i want to check out that pr interval. We want to make sure that it falls within normal limits about 0.12 to 0.20 seconds. Lastly we want to look at the qt interval which is found at the beginning of the qrs complex to check that that is presenting like how it should upright and here in this rhythm it is presenting the way it should. We have a pr interval of 0. 12 seconds and then i'm just going to go down through the strip to confirm that it is like that.  sinus tachycardia is not always a bad thing like whenever we're exercising we want to heart is just literally failing so you'll get the backup of blood blood will start backing up into the lungs. sinus attack may be the only symptom for some of these patients so if no other cause is causing this it may be their thyroid y is for yelp. giving them some pain medication can sometimes decrease the heart rate and get them back to normal h is for hemorrhage or hypovolemic shock e for emotional stress slash fear a for anemia r for respiratory conditions like pulmonary embolism chronic lung diseases. Let's say you're patient develop sinus tachycardia well you'd want to be thinking okay possible pulmonary embolism and be looking for those other signs related to that. Ekg to analyze the rhythm making sure that nothing underlines going on cardiac stress tests can be done an echocardiogram the patient can wear a halter monitor if they're at home having this and this will monitor their rhythm and then it'll go back to the cardiologist who will read that and see what's possibly going on. that wraps up this review over sonos tachycardia. If you'd like to watch more videos on this topic you can access the link in the youtube description below. You can also click on the link below to go to the next video in the series. Back to the page you came from. Share your thoughts on this article. Send us your photos and videos of your sonos experience on our Facebook page and our Twitter account. Visit our social media accounts to share your photos, videos and more.

ROUGE-1: 24.95, ROUGE-2: 22.86, ROUGE-L: 20.10
BERTScore: 63.82

==============================================
==================== [21/100] ====================
Summary:
In this problem, we're given a collection of 10 variables, x1 through x10, where each i, xi, is a uniform random variable between 0 and 1. And we'd like to develop a bound on the probability that some of the 10 variables being greater than 7 using different methods. In part A we'll be using the Markov's inequality written here. And in part B, we'll use the Chebyshev inequality, which takes into account the variance of random variable x. subtract out its means, which is E of the same summation, and further, we'll divide out, what we call normalize, by the standard deviation of the summation. So if we perform this procedure right here, then as the number of terms in the sums going to infinity, we will actually see that this random variable will converge in distribution. And since we know how the distribution of a standard normal looks like, we can go to table and look up certain properties of the resulting distribution. So that is a plan to do.

ROUGE-1: 19.98, ROUGE-2: 19.33, ROUGE-L: 19.87
BERTScore: 72.31

==============================================
==================== [22/100] ====================
Summary:
So now that we've combined pulley A, string 2, platform, and washer as our system, we can now address our question. If we measure the acceleration of the person, what is the force that the person pulls the rope down with? Well, of course, that will just be the tension in the string. And with this simple system,we can now apply Newton's second law, F equals ma. And so by thinking about how to choose a system, what could be a very complicated problem, with lots of equations, is simply one equation.

ROUGE-1: 49.74, ROUGE-2: 48.95, ROUGE-L: 49.74
BERTScore: 80.28

==============================================
==================== [23/100] ====================
Summary:
In order to be successful whenever you're drawing blood or starting IVs you really have to know a couple things number one you need to know the name of the vein that you're going to use and its location along with what can that vein actually handle. Some veins can only handle about a 20 or a 22 gauge IV cannula versus some of them can handle 18 gages 16 gages. I also like to use accessories cephalic vein along with the median vein of the forearm and of course those hand mains the dorsal venous network.

ROUGE-1: 13.00, ROUGE-2: 12.62, ROUGE-L: 13.00
BERTScore: 67.49

==============================================
==================== [24/100] ====================
Summary:
When we talk about complement and substitute, we have to be clear whether we are talking about the demand side or the supply side. So, what do we mean, when do we say a good is complement in production or complement in supply? Can you give an example, first substitute think about it? Plastic chair to iron chair like. Boeing is a manufacturer of airplanes; it makes civilian airplanes as well as military aircrafts. If there is an increase in price of military aircraft, what would happen? Boeing devote more a space to military. To manufacturing, to manufacture military. aircrafts, it would go down.

ROUGE-1: 28.57, ROUGE-2: 27.27, ROUGE-L: 28.57
BERTScore: 66.31

==============================================
==================== [25/100] ====================
Summary:
The ability to assign thoughts or internal mental states to other people is called theory of mind. This ability has been studied from relatively simple perceptual phenomena to understanding some of the most complex, abstract ideas that we ever encounter. The ability to think about other people as containing internal mental lives, mental representations, is called social cognition. This is the ability to represent what somebody else thinks and separate it from the state that you were doing while you were using your mind. It is a very simple way of encapsulation of the world. FMRI is a way to study the mind in many different ways and contexts. In an experiment, people were asked to make a moral judgment of a character. The more wrong it was, the higher their hand went. The right TPJ is selectively involved in theory of mind, and so selectively depends on all the experiments I didn't show you. It has many important dimensions. This is a story about a parachute, for example, for a faulty parachute. Within that story, there's all kinds of things going on. There's a really important difference between thinking about what you know, what you see, and what you think-- versus states like what you want and how you feel. This problem was set up as kind of a litmus test for our ability to think about other people's minds, starting in the late '70s and coming out of comparative psychology. We and many, many other groups that tried this in many different ways found a whole group of brain regions where metabolism or blood oxygenation is higher if you're thinking about others' thoughts. A three-year-old is asked, "Which sandwich is he going to take?" and he says, "I want my cheese sandwich" The task is called passing the false belief task. The child correctly predicted that Ivan would take Joshua's sandwich. But the knowledge the kid is bringing to bear is a way richer than just his correct prediction and includes him, in fact, trying to stop me in the story to warn me of what's coming. The task became so famous is that not all participants perform the same way. modern techniques in fMRI to try to get further. This is partly because I think it's interesting what we've learned. But it's mainly because you guys might not actually want to know about theory of mind. And so I'm going to focus on three ways to use modern techniques infMRI to study interesting representations in the human mind. I'm hoping that either you'll learn something about Theory of mind or something about how you could use fMRI. to pursue your own interests. words, to know they are words in English, to put them together in sentences, and then to make a response by pushing a button. So we're using everything from your eyes to your fingers and most of the brain in between. And then the question is, the part that required you thinking about thoughts-- is there any sense in which that's special or different from the whole rest of the logical and cognitive capacity of your brain? So to ask that question we designed a control condition in which you similarly read stories that involve something that was true and becomes false. Sally Kohn: People make moral judgments by reading different stories. She says studies have looked at how different stories affect different parts of the brain. Kohn says theories of mind and social cognition don't reveal much about this dimension of mind, she says. The results of these studies can be used to help people make better moral judgments, Kohn writes, but they're not represented in the right TPJ and they vary across people across the country and around the world, she adds. MVPA is a technique for getting more information from fMRI data. It's an analysis of thinking about data, not a way of getting data. MVPA could be used to rediscover all of the things Nancy already discovered using the traditional analyses. If you're not careful, what you will do is just re-go over old territory with new math. I am more interested in these techniques when they let us see things we could never see before. The reason why I think MVPA is giving a new life to fMRI is because many of the most interesting questions about cognition and cognitive science that we wanted to answer are now possible. V1 is called V1 because information goes from your eyes to the LGN of your thalamus. It's the first cortical stop of visual information. One way that we know that it's very involved in vision is that if you're seeing visual stimuli, you get a big response in V1. But that misses pretty much all the interesting contributions that visual cortex makes to vision. We want to know what transformations over the information coming from LGN is V1 implementing-- what computational transformations, what representations. fMRI that V1, for example, has an orientation map, that neurons in V1 have an orientation preference. And the answer in standard analyses is-- no, you can't, because V1 as a whole will activate to big images regardless of the orientation of the content of the image. You need to be able to say there are different subpopulations of neurons inside V1. And it's the relative activity in those two populations that would let you say, is the line like this, or is it like that? MVPA is a brain-computer interface that allows scientists to manipulate the way people think about things. In an experiment, scientists manipulated people's mental states to make them think about different things. They found that when people read different stories about knowing and unknowing harm, their brains reacted differently to the stimuli. The researchers are now trying to find out if this is a real phenomenon or just a result of the way they've done the experiment in the past. They hope to use the technique to help people understand the difference between intentional and accidental harm. Haxby style correlations were the first form of MVPA introduced, and they were introduced by Jim Haxby in 2001. The idea is, take a region you care about and ask this basic question. For some future that I wonder if it's represented, is the correlation across neural responses more similar when the stimuli share that feature? So that it differs across regions. So we could show that this was present in the right TPJ but not present in other regions. And that's a bunch of stuff you would want to know. knowingly murder than to unknowingly murder. But there is variability in how much worse. Some people think that basically what you thought you were doing is all that matters in these stories, whereas other people think both of those things matter. So there's individual variability. And one thing that we can look at is, how does the individual. variability in the behavior relate to the individual variability in. the representation of knowing harm compared to unknowing harm? And then relate that to, while you were reading that story, how different were the patterns in your brain. gives you a pretty robust measurement, because you're using all the voxels in the region to get one number out. In this case, it can be sensitive to pretty minimal stimulus variations. As I showed you, this is a two- to four-word variation on an 80-word story. So it's sensitive to small distinctions in the stimuli. Here we showed that it generalizes. So we used totally independent stories in the train and test set. It gave us a measure that was stable within a participant in the sense that the measure in each individual related to that individual's behavior. something compared to one set of stories about hearing something. The stimulus set had another distinction in it, which is whether the thing is good or bad that's happening to you. And so in the right TPJ, what we found is that stories about seeing are more similar to other stories aboutSeeing. And stories about Hearing are more like other stories when you cross that feature. And we've actually found this a whole bunch of times. The rightTPJ doesn't care about valence. Other regions do.

ROUGE-1: 24.22, ROUGE-2: 21.18, ROUGE-L: 18.79
BERTScore: 58.57

==============================================
==================== [26/100] ====================
Summary:
Joanne Stubbe's lab works on the only cool enzyme in the world-- ribonucleotide reductase. It's the only way in all organisms that you make the building blocks de novo that are required for DNA biosynthesis and repair. If you inhibit this enzyme, you have no building blocks. You can't survive. So from a practical point of view, it's the target of drugs they use therapeutically in the treatment of cancer. And I think in probably not so distant future in the antibacterials because there are sufficient differences between humans and bacteria reductases. and do the same chemistry, but they have different metal cofactors depending on where they evolved. The function in all cases is to generate a radical in the active site and then the chemistry is the same in all these things. And the function of the metalcofactors in all case is to create a radical, which is the key to the chemistry in all of these cases, says Dr. Michael Bociurkiw, a professor of chemistry at the University of California, San Diego.

ROUGE-1: 38.51, ROUGE-2: 33.92, ROUGE-L: 31.74
BERTScore: 70.88

==============================================
==================== [27/100] ====================
Summary:
GILBERT STRANG: Differential equations is the big application of calculus. He says you really do need to know basic derivatives. Strang: The derivative of e to the x, which is e tothe x, is the solution to a basic differential equation. He shows how to use the fundamental theorem of calculus to check the formula. "It's kind of interesting to see what part of calculus, what information and what ideas from calculus, actually get used in differential equations," Strang says. Derivative is a linear operation. The product rule fg prime plus gf prime. The quotient rule. And above all, the chain rule. The derivative of this-- of that chain of functions, that composite function is the derivative of f with respect to g times the derivative. of g. That's really-- that it's chains of functions that really blow open the functions or we can deal with. OK. So I have a function t times another function of t. I'm going to use the product rule and show that the derivative is one term will be y and the otherterm will be q.

ROUGE-1: 17.69, ROUGE-2: 16.22, ROUGE-L: 14.23
BERTScore: 73.99

==============================================
==================== [28/100] ====================
Summary:
okay so welcome to the last lecture of this course here in this winter term and what we discussed so far in the course were mainly the so-called backends or optimization engines or probablistic estimation techniques. Today I would like to give a very very of course brief short overview about front ends and one important aspect inside successful front ends on how to determine if a constraint is likely to be a correct one. I will introduce three kind of small front end systems on a very abstract level just giving you the idea on how they work with different sensors and then in the second part of this talk today I will like to stress on what kind of conditions should such a constraint fulfill or the the metrics of the local environment fulfill in order to let's say I would say ensure to be out layer free. to begin by matching observations so we have different observations depending on what platform that can be whatever stereo camera or different types of sensory modalities. For every sensor of course there's a different way of obtaining those constraints and those constraints may take into account what the sensor actually sees how kind of unique is the data that the sensor generates for specific area or it's a look all corridors exactly the same. Other approaches use features for example we had those the Victoria Park where trees have been extracted in this case from the laser range data. image you have a typically quite good description of the image so you can just match those descriptors and based on those descriptor identify if two images are recorded from the same place. You can do what's called a visual odometry so based you inspect the images and consecutive frames estimate the positions of features and then estimate the movement of the camera based on the feature that you see and the 3d location of the feature is exactly in the same way. The last part which is loop clothing so given I kind of I don't know where I am some a large uncertainty and I you can use this just consider they are none of them if you say they are. moment and that's my sensor range I can compute where are those other pulses so in this case B 1 and B 2 just two examples could be more obviously and then I can also estimate what is the uncertainty of those poses B 1 or B 2 relative to a do that by eliminating the note a from my linear system and then inverting the resulting Hessian and looking to the main diagonal blocks this gives me the uncertainty here indicated by these dashed lines. Based on this information I know I can never have an estimate of given my current pose where's b1 where's B2 together with The Associated uncertainties certainty estimates. was here indicated with a which can where I could I can obtain by inverting the hessian in practice this is a pretty expensive operation so you actually want to try to avoid inverting this larger matrix you can do an approximation what actually most system in practice do to do that more efficiently. This is a thing by they say okay we simply ignore the loop closures for the moment just for estimating the uncertainty here and you just do what is also called Dijkstra expansion so we expend propagate the uncertainties through the graph. ICP is sensitive to the initial guess and as a result of that we may end up in a local minima so in something which looks like a match but in reality is not a match other things we may identify is how do i sample possible locations where the platform can be. The approach here takes additionally into account it kind of separates areas that such is just a simple say classification or segmentation of the environment like and if I wall think that stick out and first met results against each other and only in the end it does the alignment of all points. showing you three different examples of systems that we have built here in Freiburg. Some of the mapping techniques we developed here have been used to at least tested on that car so this is a pioneer a two robot which has a two d-day the rangefinder sitting here and sits on a pencil unit so it moves always like this song so it's called a nodding laser and this way generates 3d data you get 3d information about the scene and then it tries to build sorry a 3d map of the environment using this technique. take the tree the pole and the walls and match them first you if you can separate those part of the skin reliably he typically are less likely to end up in a local minima and then in the end you get this kind of alignment so this the iterative procedure nice based on ICP which aligns those scans and then I can do this for a large number of individual scans. Then I end up with a map like this so you can see you may observe some of those small stripes over here so these kind of darkest stripes these are simply small alignment errors. is a parking lot or a 3d model of a parking lots where yellow again means drivable areas and red means non drivable area and then you can actually use this this map over here in order to localize the vehicle. This was actually work of China that he built or he realized autonomous parking using this map representation which was built here for that vehicle in that parking lot well it's actually the picture of a garage and so you can see even though it's here three floor building and the corresponding 3D model where the car and how to heal. This is exactly the approach that I presented here and actually a couple of the slides that I use in here or of the images material at least comes from a tin Olsen. This is a procedure which is very very similar to Rancic it's actually a variant of good sake I think which was used here. The technique is used in three different ways in this approach the first one is for for visual odometry so there is no wheel encoder on the camera so if you take the camera on a flying vehicle or we take into our hand and moving it over the ground. Based on the position of my stereo camera I try to build a local model of the surrounding so what we want to estimate is the X Y that and three angles your roll pitch and yaw. By knowing the gravity vector I kind of get rid of the roll in the pitch and this reduces my problem from six dimensions to four dimensions for every node or for every camera pose which makes my life easier and therefore it is kind of exploited here so based on the staring from stare information into account. image and you have a database of all the sort of features that you have seen so far in the past what is the first thing you do is you try to make a nearest neighbor query in the descriptor space to try to find the best matching descriptors over your map or in those areas which are in line with the credit unit. If you have descriptors like feature descriptors it can actually help you to find good estimates where you can be so you don't have to try all camera polls and see if the camera poses match. may see a small bias it's not centered around zero but given kind of this self-made stereo setup just excluding two cameras together that was actually a very nice result then you can use the same system for example on a blimp this was an example where we used it in the end God used here was exactly the same approach but only a single camera and the SONA which was measuring the depth information. Tasker builds a map online and use the map in order to make navigation decisions of where it should actually go. be the same place but there might be something else which looks exactly than in this place a here so it may not be a good idea to add this constraint unless we have seen all this part over here. There may be different places where the system can be which are just which are which do not intersect with the place I'm currently considering and therefore I should not do a match but you could you could. The key trick in here is we have a large number of constraints pairwise constraints between nodes and we want to check in to how many consistent subgroups are there. hard for me to you to to identify and this is also called what's called the picket fence problem so good offense you seem you don't know which part of the fence matches to what you see so far it's a very very long repetitive structure and these are things where you also don't want to add a constraint the curses simply do not know is this is this locally ambiguous or not. If you have more than one solution I can just say okay what the ratio between the largest eigenvalue and the second largesteigenvalue. If this is a value which is let's say 1 or between 1 and 2's again yeah this is very likely to be a picket fences prom. sufficiency it'll be so these two tests I have okay we would like to go through these three steps over here the first one is the topological grouping which is easy to be done so I just take my post graph and I take okay which poses are nearby and then I try to match all of them. The questions how do I identify which one a right image or not wrong again the first thing we do is we want to test for local unambiguous so we take one and then we take another. add up at an identity transformation if I concatenate all of them. Sampson theme aligned because the similarity or how far away from your identity matrix simply depends on how accurate is your dormitory information and how accurate can you actually align your scans. Moodle oh that yeah okay so I have whatever a number of those hypotheses what I can do is I can actually build up my matrix a I J where this simply depends how consistent are the hypothesis using the hype of this I and a hypothesis J together with the odometry. which basically means I'm far away from identity we may use just a Gaussian about how far I am away you're away from the from the identity so what you end up you have a matrix with those values in here and some values are have high well we have some elements with high values inHere and some elements in here. If I have a 1 here at the field I it means that the assumption that H I is correct and if there's a zero of it if AJ is incorrect or it's not correct so I get effect. okay my Lambda is now a function a Lambda as a depends on my variable indicator vector B and I try to maximize this expression. The problem I have in here is that my indicator vector V has a constraint that only allowed to take zeros and ones in under this constraint that is an np-hard problem. I simply don't treat my vector V as discreet I said I just allow continuous variables because then I can actually optimize this and then get a solution and the end I simply round to 0 or 1 it may not be perfect. for determine this I could use the SVD that we discussed for example with the diagonalization in whatever a few months ago when we when we discussed this. The larger the eigen values are the better the score so there's a proof that i get a perfect combination I get a couple of eigen vectors with current putting eigenvalues. If I have multiple solutions for that get MA multiple so if I haveMultiple solutions for this eigenvalue problem this is simply they are multiple maxima in my in my problem.

ROUGE-1: 26.77, ROUGE-2: 26.16, ROUGE-L: 23.67
BERTScore: 69.44

==============================================
==================== [29/100] ====================
Summary:
Marketing is about four things creating communicating delivering and exchanging value so marketing is about creating communicating delivering and exchanging value that's in chapter one so who can tell me what marketing is somebody good what's your name genius chingus go ahead tell us what is marketing. Write this down marketing marketing that's capital n a you guys do it okay come on stay with me here no sleeping no facebooking who's facebooking now anybody no text messaging all right no periscoping. The new high definition monitor that's on the market is 4k so you're going to get 4k technology and 3d capability and smart capability so it's going to be a smart tv. Value does not mean low price so that's a good example anybody have another example of where something might be very expensive but not the lowest price yes go ahead tell us your name theresa bad um i mean would you consider that clothing yes go Ahead like so like i find people saying like buying a pair of jeans and zara is expensive because there's like 65 but they last longer than buying these at all. Cars provide the same generic functionality transportation what makes one car different from another is the brand. The brand is what distinguishes one product from another we could look at that through market research to understand the perceptions that consumers have for our brand importantly relative to other brands so not just where we are on the perceptual map the value of perceptual mapping is that we could see where we're positioned in the market based on certain dimensions quality price innovation relative to our competitors so remember it's the perception it's not a question of whether or not our product is expensive or whether it's a high quality. and those that are 40 to 49 does that make sense so you want to have a commercial for example that's going to resonate with your target audience usually you're not going to be able to have one that resonates with everybody in your audience. So when you're showing a commercial do you think that those men that are seeing the commercial between the ages of 18 and 29 want to see somebody in the commercial who's 60 or 65? That's probably not something that's gonna resonate with them right unless maybe if it's coach. do is get a representative random sample from that population so it doesn't need to be 350 million. A thousand can be statistically significant a thousand to fifteen hundred but in most categories in the united in the United States that number is about a thousand people. i think it has to be a balance sample like from every type of person that we take somebody up out of this group and from different states maybe and let me do this research with these people absolutely so we have to have males and females people of different age groups peoples in their 20s 30s 40s 50s 60s.

ROUGE-1: 28.95, ROUGE-2: 27.91, ROUGE-L: 22.86
BERTScore: 64.43

==============================================
==================== [30/100] ====================
Summary:
Jeremy Bentham's principle is that "the greatest happiness of the greatest number" is maximized. Bentham thinks all utility is quantifiable. He allows interpersonal comparisons of utility. If you take one unit of utility from one person and give it to another person their utility will go up and the first person's utility is going to go down. The total here's bigger than eighteen, so it's obviously preferable, on the scheme, to the status quo. Maximal utility is perhaps the greatest majority, perhaps the majority of the majority. In Bentham's system, classical utilitarianism, we see that he get the next marginal increment of utility. So we should expect rich people to be greedy by this theory, not to become more and more indifferent to money. Every serious economist since the eighteenth century has assumed that the principle of diminishing marginal utility is true, including Jeremy Bentham. If you want to see how people are to get it right, you need to get this idea right. It's going to get you closer to the truth than not. Focus on the difference between the status quo and distribution IV here. These people might be on the verge of starvation. Surely giving them a unit of utility is going to be much more enhancing to their happiness. Anyone know what the principle behind that idea is? Anyone want to take it? How many of you have done ECON 101, the first econ course? Yeah, so what is the principle that would tell you if you have no food and I give you a loaf of bread, your utility goes up a lot more than if I have ten loaves of bread. Somebody? Okay. Diminishing marginal utility. integrity is a great example because once you start putting values like that out there it, I think, threatens the idea that it's all reducible to a single index, right? Because you can't--having a little bit of integrity is sort of like being alittle bit pregnant, right?" Professor Ian Shapiro: What about health? It's not quite binary because you can be in medium health, but I think it would be pretty useful to be healthy and then super healthy, ad infinitum. some people are sighted and some people are blind and you could do eye transplants. Arguably the blind person would gain more utility from getting one eye than the sighted person would lose from losing one eye. So that can also give you some ways of proceeding that would make you queasy, right? I don't think those are actually deep problems for Bentham's theory because I think what he would say is, "Well, you'd drink beer and at some point you would sell the beer" with some of these counter examples. Let's suppose a two-person society, again, and let's suppose it consists of Donald Trump and a homeless woman living out of a left luggage locker in Grand Central Station. And the question is, should we take a dollar from Trump and give it to the bag lady? What? Should we? Yes? No? How many think yes? Okay, yeah, almost everybody. Why? Because by assumption with the principle this point of practical equality, or when the gains from downward redistribution are offset by the losses from the shrinking of the pie. to be profoundly radical and frightening to rich men, because it has this built-in impetus for downward redistribution. Now, Bentham was a fairly radical guy. He was a supporter of democracy, which was a radical thing at that time. But he wasn't as egalitarian as all that, and he wanted to temper the downward redistribution that flows from his principle, and so he makes a distinction between what he refers to as "absolute" and "practical" equality. Reagan thought a top marginal tax rate of 40 percent was beyond the point of practical equality. because it's where you start to see our old friend the workmanship ideal creeping by the backdoor into utilitarianism. Bentham says, "Law does not say to man, Work and I will reward you but it says: Labour, and by stopping the hand that would take them from you, I will ensure you the fruits of your labour" So another way of thinking about this is, that Bentham's idea of the state is essentially regulatory. It stays the hand of somebody else who would steal your goods, but the government cannot itself create utility. find, if you go back now and look at what happened during the 1980s, perfectly credible economists will line up on both sides because they cut the taxes. And disentangling how much the tax cuts were responsible for what happened, versus how much many other things that happened were responsible, nobody really knows. And so a lot of the problem in debating incentives, once you get into the real world of macroeconomic policy-making, is that (a) you never have the counterfactual.

ROUGE-1: 24.45, ROUGE-2: 23.13, ROUGE-L: 21.36
BERTScore: 59.95

==============================================
==================== [31/100] ====================
Summary:
Lipids are molecules that are mostly hydrophobic, which can also be referred to as lipophilic. Some of the lipids are what are known as amphipathic. Some amino acids may actually have polar groups in them, such as tyrosine. The next set of amino acids are those that are polar and charged, and I've shown you the most common state of them. This quanine group is likely to be charged. This amidinium group's likely to. be charged, but the word for it is arginine. The phospholipid structure is a very important semi-permeable membranes. They are made up through the non-covalent, supramolecular association of monomer units. Many of these bonds show that there's restricted rotation in that bond that's unique in the polymeric structure. We will see later on how proteins provide the opportunities to cargo things into cells the structures are cool. Now, let's talk about the structure of a peptide, or peptide amide. or out of cells, even very large entities, and there are certain mechanisms whereby that happens through a semi-permeable membrane, OK? I want to show you the other feature of membranes. They are self-healing. What this means is if you poke them, you poke a hole in a cellular membrane, You? Basically push apart those non-covalent forces. Once you take the thing away, be it a needle or a very fine glass capillary, they seal right back up. you comprise 50% of all of the macromolecules. amino acids are called amino acids because they have an amine, the carboxylic acid. The amino acids that are encoded in our proteins are all what are known as alpha amino acids. Proline is a little odd because its side chain is kind of in a cyclic structure, and towards the end of the class, I'll talk to you about collagen, whose structure is totally dependent on the involvement of proline. Life is made of 20 simple building blocks with functional groups. The building blocks are not functional themselves. It is the polymers that are made up of amino acids, and I'll always call them AAs because it's easier for me. All the functions of proteins are dictated by the order of the amino acids. These are the ordered elements of secondary structure. You don't have to be able to figure them out, but you have to. pick them out in order to understand the structure. When the proteins are smaller, they are not capable of adopting too much ordered structure, and we mostly call them peptides. When I condense three amino acids, I spit out two molecules of water, and I put in place two amide or peptide bonds. If I go down this backbone, every third bond is going to be fixed, fairly fixed. There's not freedom of rotation around it. The folded structure is called protein folding. It's not simple because what you're doing is you're solving a massive energy diagram. about the hierarchical structure of proteins as put in place. And that primary sequence is kind of a cool thing because it's very specific. It defines-- it's got encoded into its structure, the three-dimensional fold of the protein, OK? All the information for the folded, compact, globular structure that's functional is encoded in thatPrimary sequence. It's a cryptic code. We may not be able to tell by looking at it what it really looks like, but all the information is there in order to program the folding. to have the capacity to be involved in hydrogen bonding interactions, as I've suggested here, all right? What else is there here? When I write the MIT peptide, I write M first, I second, T third. If I wrote TIM, it would be a completely different chemical structure with different chemical properties. So the directionality is important to understand, and there you have it. So now you can go home and practice your name in amino acids and draw them out. If you drew them out fairly sort of sharply, then you'll never get confused about what end's what and where the substitutes are. reinforce to you, the primary sequence is established by covalent bonds. tertiary structure is enabled by all these other interactions, electrostatic, hydrogen bonding, hydrophobic forces. When the secondary structure is in place, a lot of the side chains are near each other, and they can engage in long-distance contacts. And so for example, I'm going to walk you through this, so you can sort of get a sense of how these three-dimensional puzzles work on a very small scale. The video is a simulation of a protein that holds reversibly under appropriate conditions. The protein, GB1, is trying to find its thermodynamic minimum, and it's actually failing pretty badly. You see that nascent helix, in the background, the red and the blue, is starting to form strands that are a little bit aligned. The video is about 30-- 60 seconds of the simulations, so I made a point to myself to take you to about minute one, where things start to get fairly interesting. A single amino acid change in the primary sequence of collagen can destabilize the structure, so it is no longer viable. The disease type I'm going to talk to you about is a set of diseases known as collagenopathies. A lot of babies with this defect can't even be born through the birth canal because it would crush the bones, and many of them don't survive very long at all. So I think that's a good place to stop and I'll pick up next time with hemoglobin. will propagate through every single strand. If this is one strand made up of three polypeptide chains, it propagates all the way through the structure. There's a set of magenta residues in the middle, which come from a defect in the sequence where a glycine has been changed to an alanine. That defect is caused by the change of a hydrogen to a methyl group on three residues that come together, and that bulges out that fibrillous structure and makes it not as compact and beautiful as it should be.

ROUGE-1: 24.01, ROUGE-2: 22.59, ROUGE-L: 18.96
BERTScore: 60.33

==============================================
==================== [32/100] ====================
Summary:
The universe is rotationally invariant. In any direction if I measure the angular momentum of the electron along that direction, I will discover that it takes one of two values. This is also true of the L equals 1 states. The only sensible angular momentum, here-- is the two state tower, which can't be represented in terms of rotations on a sphere. And this is the energy when when an electron is in a state corresponding to one of those particular values, regardless of whether it's Lx, Ly or Lz. in a tower, with either one state-- corresponding to little l equals 0-- or two states -- with l equals 1/2-- or three states-- with little l equal 1-- or four states--with l equals 3/2 -- and so on, and so forth. We quickly deduced that it is impossible to represent the half integer states with a wave function which represents a probability distribution on a sphere. We observed that that was impossible. If you did so, then when you take that wave function, if you rotate by 2pi, the wave function comes back to minus itself. But the wavefunction has to be equal to itself at that same point. So, this is strange. There's nothing special about the origin. of said, well, look, these are some other beasts. But the question is, these furnish perfectly reasonable towers of states respecting these commutation relations. So, are they just wrong? Are they just meaningless? And what we're going to discover is the following-- and this is really gonna go back to the very first lecture, and so, we'll do this in more detail, but I'm going to quickly tell you-- imagine take a magnet, a little, tiny bar magnet. If you have a gradient of a magnetic field, then one end a dipole-- one end of your magnet-- can feel a stronger effective torque then the other guy. An electron just has it.momentum. If you send an electron through a Stern Gerlach Apparatus, it always hits one of two spots. And the legacy of these little L equals 1/2 states, is that they represent an internal form of angular momentum that only exists quantum mechanically. That was a very long answer to what was initially a simple question. But we'll come back and do this in more detail, this was just a quick intro. Yeah? AUDIENCE: So, for L equals 3/2, does that mean that there's 4 values of spins? we're gonna talk about real, physical systems in three dimensions. And as we'll discover, it's basically the same as in one dimension, we just have to write down more symbols. But the content is all the same. So, anytime we have a system which is rotationally invariant, we can write the energy operator in the energy fashion. And now, you have to answer the question, well, can we back to the trick I was referring to earlier? Because that's going to be a lot easier if we work in eigenfunctions of L. let's ask how they act on function. 1 over r dr squared r, acting on f of r, well, these derivatives can hit either the r of the f. There's going to be a term where both derivatives hit f, in which case the rs cancel, and I get f prime prime. So, why is this a particularly useful form? We'll see that in just a minute. If you see this, declare in your mind a brief moment of triumph, because you know what technique to use. Can we find functions which are eigenfunctions of E and of L, simultaneously? And so, the answer to that question is, well, compute the commutator. L squared trivially commutes with itself and, again, r doesn't matter. And ditto, r and L squared commute. These commute. So, we can find common eigenbasis. We can find a basis of functions which. are eigenefunctions both of E or of L squared. The potential, V effective of r, does the following two things-- whoops, don't want to draw it that way-- suppose we have a potential which is the Coulomb potential. V on phi is equal to-- well, V doesn't have any r derivatives, it's just a function-- so, V ofphi is just 1 over r V on u. Plugging that together, gives us this energy eigenvalue equation for u, the effective wave function, which is 1d problem. is a little u. OK, this is my little u and so, now I'm gonna have to-- oh jeez, this. is horrible, sorry-- this is the potential, capital U with a bar underneath it. Oh, for the love of OK. So, let's do some examples of using these central potentials. The first example is gonna be-- actually, I'm going to skip this spherical example-- because it's just not that interesting, but it's in notes, and you need to look at it. if there were nothing else, equals zero. If there were no other terms here, then this would say, ddr plus c over r is phi. But if phi goes like r to the minus c, that's not the exact solution to the equation, but I can write phi is equal to r to minus c times u. And then this equation becomes ddr, dot, dot u equals zero, OK? Very useful little trick-- not really a trick, It's just observation-- and this is the second order version of the same thing. term? Well, what this is telling you is that if you have some angular momentum-- if little l is not equal to 0--- then as you get closer and closer to the origin, the potential energy is getting very, very large. If you're spinning, and you pull in your arms, you have to do work, right? You have to pull those guys in. You speed up. You're increasing your kinetic energy due to conservation of angular momentum. Well, you should have done this in classical mechanics as well. classically that system decays, right? Irradiates away energy. Does the angular momentum barrier save us from decaying? Is that why hydrogen is stable? No one wants to stake a claim here? Is hydrogen stable because of conversation of angular momentum? AUDIENCE: No. PROFESSOR: No, absolutely not. All of those things are false that I just said. But if all those things were true, in that imaginary universe, this would be the salient problem to solve. So, what should be true of u? Can u diverge? Is that physical? Does u have to vanish? Can it take a constant value? So, I've given you a hint by telling you that I want to exactly the way you need to get the delta function. OK, which is pretty awesome. So, what that tells us is that if we have a wave function that goes like 1 over r, then the energy contribution-- energy acting on this wave function-- gives us a delta function at the origin. the potential is going to be this, so, here's r equals 0. And if it's a spherical infinite well, then I'm gonna say, the potential is infinite outside of some distance, l. OK? And it's 0 inside. So, what does this give us? Well, in order to solve the system, we know that the first thing we do is we separate out with yLms, and then we re-scale by 1 over r to get the function of u. This is an energy times a length. Also known as p squared l, momentum squared over 2m, 2 times the mass times the length. And if you actually take mu e to the 4th over h bar squared, this is off by, unfortunately, a factor of 4. This is equal to 4 times the binding energy, which is also called the Rydberg constant. And this is observed binding energy of hydrogen. It's probably something like the expectation most don't have a simple solution like a Gaussian or a power large. Solving a differential equation is a sort of involved undertaking. Most differential equations of some, maybe if you're lucky, it's a special function that people have studied in detail. We first did, we did asymptotic analysis. Then we did a series approximation. Now, do most differential equations have a simple closed form expression? A solution? No, most differential equation of some are not simple, but they can be solved in some way, if you know how to do it. As rho goes to 0, asymptotic analysis is gonna tell us that u goes like rho. Two derivatives, we pulled down a rho squared, and so two derivatives in this guy,. We pulled down an l, then an l plus 1. So, this should go like. rho to the lplus 1. But that is also badly diversion at the origin, it goes like 1 over 0 to the. l. That's bad. On the other hand, if l is equal to 0,. then this is the only term that survives, so we'd better make sure that that behaves gracefully. calculations, this is a sweet little calculation to take this expression. Plug it in and derive this recursion relation, which is root-- or 2 root-- epsilon times j plus l plus 1 minus 1 over j plus 1 j plus 2 l plus 2 aj. And in order for this terminate, we must have that some aj max plus 1 is equal to 0. So, one of these guys must eventually vanish. And the only thing's that's changing is little j.

ROUGE-1: 23.12, ROUGE-2: 22.15, ROUGE-L: 19.61
BERTScore: 67.63

==============================================
==================== [33/100] ====================
Summary:
John Guttag: This is the 60002 course, or if you were in 600, the second half of 600. The main topic of the course is what I think of as computational models. The course is really less about programming and more about dipping your toe into the exotic world of data science. The final exam based upon all of the above will be a test based on experience writing object-oriented programs in Python, preferably Python 3.5. The lectures will be-- and maybe I'm speaking euphemistically-- a bit faster paced. Science is moving out of the wet lab and into the computer. We'll talk about three kinds of models-- optimization models, statistical models, and simulation models. An optimization model is a very simple thing. We start with an objective function that's either to be maximized or minimized. We then often have to layer on top of that objective function a set of constraints, sometimes empty, that we have to obey. We use these things all the time. Today you really can't avoid using optimization algorithm as you get through life. A greedy algorithm, as we'll see, is not guaranteed to give me the best answer. There is no algorithm that provides an exact solution to this problem whose worst case running time is not exponential in the number of items. But that should not make you sad because while there's no perfect solution, we're going to look at a couple of really very good solutions that will make this poor woman a happier person. The winner will be greedy by value, happens to find a better answer, 424 instead of 413. The problem is to find a vector v that maximizes the sum of V sub i times I sub i. I want to get the most valuable V I can get subject to the constraint that if I look at the item's dot weight and multiply it by V, theSum of the weights is no greater than w. The power set of a set includes the empty subset. It includes the set that includes everything and everything in between. It's pretty obvious that this is going to give you a correct answer. You're considering all possibilities and choosing a winner. Python uses something called timsort, which is a variant of something called quicksort. Timsort has the same worst-case complexity as merge sort. The problem is that a greedy algorithm makes a sequence of local optimizations, chooses the locally optimal answer at every point, and that doesn't necessarily add up to a globally optimal answer. John Grimson: With greedy algorithms, you can get stuck at a local optimal point and not get to the best one in the world. The best answer is to always get the best answer. Lambda is used to create an anonymous function, anonymous in the sense that it has no name. Lambda does is it builds a function that evaluates that expression on those parameters and returns the result of evaluating the expression. My view on lambda expressions is if I can't fit it in a single line, I just go right def and write a function definition because it's easier to debug. But for one-liners, lambda is great. Let's look at using greedy. So here's this function testGreedy, takes foods and the maximum number of units. TestGreedys.that chooses a burger, the pizza, and the wine for a total of 284 happiness points, if you will. If we use greedy by cost, I get 318 happiness points and a different menu, the apple, the wine, the cola, the beer, and a donut. I've lost the pizza and the burger. I guess this is what I signed up for when I put my preferences on. And here's another. And so I picked up some names and the values. This is just the menu we saw.

ROUGE-1: 22.62, ROUGE-2: 21.08, ROUGE-L: 17.67
BERTScore: 58.30

==============================================
==================== [34/100] ====================
Summary:
Expectations play a huge role in economics, says Ricardo Caballero. He says they play a big role in the decision of all economic actors. The IS-LM model really overweights the present, he says. Future wealth is affected not by current income, but future real interest rate, CaballERO says. "There is a lot that says that there is also suspected value that enters into the consumption and investment decisions that we care about," Caballeros says. 'I hope you'll get the gist of what expectations can do in economics,' he adds. future into the IS-LM model. And by then, you would have seen-- you will have seen all that I wanted to communicate, at least, in this set of lectures. So let's think about, first, consumption. And up to now, we assume that consumption depended only on disposable income, on current disposable income. But that's not really the way it works. One of the first in formulating, more or less, formally how consumption decisions are really made is Milton Friedman. fund the consumption, which is above your current income just because you have more financial wealth. The very rich seldom sell assets. They borrow against those assets to fund consumption. That's the way it works. And the very rich often have no income, [CHUCKLES] at least labor income. All the income comes from returns on assets. And again, they mostly borrow against that. But in any event, the point there is that what really brings out your consumption is your wealth, not the current flow of income. on expected profits. And when you think about expected profits, you need to think about interest rate as well. In that environment, investments that give you a return, a quick return, are worth more than things that have a pay-off in the very long run. The higher the expected present discounted value of buying a machine, the higher the investment is. But many firms, especially smaller firms, have the opportunity to buy things that are in distress, especially if they have larger deposits. In the case of Apple, it's not something to do with financial constraints, although it's a good idea to relax them. a price. The first thing you need to know is, well, how long will this machine last? And a reasonable assumption is, for most machines, is some sort of geometric depreciation. So as a machine depreciates, the probability of the machine breaking down over a year is 5%, then 1 minus delta is 0.95, say. What is the probability that the machine is still producing two years from now? Well, 1 minusDelta squared and so on and so forth. Central banks play with your minds. They want to influence aggregate demand. They need to convince you that this stuff will last for some time. One of the problems they're having now, actually, when the Fed is trying to cool the economy, is that they keep hiking rates, but the loan rates have begun to decline already. If the Fed cuts the interest rate but doesn't persuade anyone that this rate will remain low in the future, then it is going to get very small effect on output. to determine current output. If taxes go up today, this IS will shift to the left. Do you think it will shift more or less than it did in lecture 3 or 4? So suppose we increase taxes by 10%. Will that reduce output more orLess than when we had the static IS-LM model? Yeah? AUDIENCE: Depends on the expectation. RICARDO CABALLERO: OK, but I haven't moved. These are parameters from my curve. So I don't get the right to move them. Most of the time when you have episodes of fiscal consolidation in environments that are not of very high distress, financial crisis, and so on, it typically sort of-- how successful that is depends a lot on whether people expect to be a sort of implicit deal between the central bank and the Treasury. If people expect that that fiscal contraction will come with much looser monetary policy conditions, then the fiscal contraction is not as contractionary as could be otherwise. So most of the fiscal contractions are contractionary. But there are some famous episodes of what are called expansionary fiscalcontractions.

ROUGE-1: 18.09, ROUGE-2: 16.71, ROUGE-L: 15.17
BERTScore: 63.68

==============================================
==================== [35/100] ====================
Summary:
We have in essence three different views of fiscal policy and how well it works orHow well it doesn't work. Keynes says we can change aggregate demand in such a way that we can make. Classical view says that people are rational. Supply-side view says we know eventually the short-run aggregate supply curve will increase and we go to the full employment level of output if you guys engage in policy where you're in essence borrowing money to create a deficit and the today means that at some point in time in the future you're going to have to increase taxes. Atas: We have a problem where we've got say real GDP, our price level, aggregate demand, and our short-run aggregate supply. Atas: Eventually what's going to happen here is an irate demand it's what's changing the short- run aggregate supply right eventually this recession. We're at some full employment level of output YF and here we are initially we're in some recession right. We know that eventually SRA s increases to S aureus - output equals YF price level equals po2. change the demand for loanable funds so you have this market right if you're gonna have the government go out and borrow money which is what they're going to do on this expansionary fiscal policy. When they borrow money they got to go into the loanable Funds market so when the government says give us money for the savings bond you can give it to your kid. There's only one way for them to get this additional money when the bonds come due and that's to raise taxes. Thanks to interest it won't be 20 that you have to borrow or in raise taxes for it'll actually be like say 30. and we say okay go out and borrow 500 billion dollars or spend an extra four hundred billion dollars. The G causes aggregate demand to go up but the change in interest rates causes consumption and investment to come back down. Shortland implication is is that you can't just plug in the number from here and get to where you want to be if this guy's going down what does that mean for the long-run do what well of investments decreasing what else is decreasing investment goes down? The quantity and quality of resources and the level of technology are what determines this guy right. could be anything now you have this deficit here of 20 all right and then next year things get fixed so here they are maybe even revenues go back up to a hundred and they also say basically once you start spending that money it's hard for people to stop spending the money they want their money right. If you don't you still have the deficit all you to do is just move the bonds down right and so that maybe now you're back down here to 100 and now you've back down to 100 all right. revenue will we collect how many guys are going to work later today okay what's what's your name again Paige okay what do you do again okay. So here the supply-side view this is called the Laffer curve. It's the rate it's the marginal tax rates that are important right it's not what you spend it's how much you take. If tax rates are 20% if the government takes 20% of my pay they are in essence taking all of my labor every Monday. doesn't make any sense I'm not gonna do that I'm gonna goof off. I don't want it because they're facing a marginal tax rate that's really really high so which view is Keynesian new classical supply? I'll write the paper and win the Nobel Prize and I can go to Sweden and get the nice little bitty gold medallion that I couldn't wear around my neck for the rest of life at such a such recession. alright this is the end of test three material I will.

ROUGE-1: 23.75, ROUGE-2: 22.67, ROUGE-L: 20.06
BERTScore: 65.88

==============================================
==================== [36/100] ====================
Summary:
Professor: We're going to look at this unitary time evolution and calculate this operator u, given the Hamiltonian. Then we will look at the Heisenberg picture of quantum mechanics. Professor: The way we think there is a Schrodinger picture that we developed in which we have operators like x, p, spin, Hamiltonians and wave functions. And then we are going to define a new way of thinking about this, which is called the He Eisenberg picture. And we'll discuss that. We'll find the Heosenberg equations of motion and solve them. So our goal today is to find U given H, because as we mentioned last time, for physics it is typically more easy to invent a quantum system by postulating a Hamiltonian. So the first thing I'm going to do is multiply this equation by u. This operator is unitary, so u dagger u is one. So we have plus U dagger Hs dU dt, so U. Well, that's bad. It's actually quite nice. And then the last term, which I have very little to say, this is a dependent operator. Partial respect to time, it would be 0 if it depends, just say, on X, or Sx, or any of those things. The Schrodinger equation is there. OK so now let's solve this. We'll go through three cases. Case one, h is time independent. So H of t is really H like that. No explicit time dependence there. So what do we have? ih bar d dt. Let's write dU dt is equal H times U. And we tried to write a solution of the form U use equal to e to the minus iHt over h bar times U0. Does that work? A little time dependence is an idea, the sign to make it possible for you to solve the equation, even though it has some time dependence. So you could have Hamiltonians that are time dependent, but still have a simplifying virtue. So if you have a magnetic field that is fixed in one direction but change in time, you can have a situation where your Hamiltonian is time dependent but still at different times it commutes. But later on as we do nuclear magnetic resonance, we will have the more interesting case. If the Hamiltonian were to be time independent, you could take it out. That brings you back to this case, so this looks reasonable. But here is the claim R dot commutes with R. Claim R dot and R commute. Well, R dot depends on H. And R is an integral of H as well, but the H at different times commute anyway. So this must be true. There's no place where you can get a contribution, because R dot is like an H. on the other side, because it commutes with R, but it's better here. And therefore you've got this very nice solution. So the solution is not that bad. Now finally, I want to discuss for a second the general case. So that's case-- there was a 1, a 2, a 3 H of t general. What can you do? Well, there's not too much you can do. But it's interesting anyway that there's a way to write something that makes sense. So here it is. When t is equal to 0, U of t-- of 0 0 is the operator that propagates no state, so it's equal to the identity. If you have the unit operator in the Schrodinger picture, what is the unit operators in the Heisenberg picture? Well, it would be U t 0 dagger 1 U t0. U dagger with U is 1, and therefore it's the same operator. The unit operator is the same. It just doesn't change whatsoever. So there's a nice correspondence between those operators. Heisenberg operators are supposed to be given by this formula, but we've seen that calculating U can be difficult. So what we try to do in order to simplify that is find an equation that is satisfied by the Heisenberg operator, a time derivative equation. One last comment on these operators. How about conserved operators? What are those things? A time independent As is set to be conserved if it commutes with a Schrodinger Hamiltonian. And therefore by point 1, by 1, you have dAh, which is equal to 0. Professor: X and P are time independent Schrodinger operators, so that equation that I boxed holds. X Heisenberg, as you remember, just commutes with P Heisenburg. So instead of the Hamiltonian, you can put this. This is X He Eisenberg P Eisenberg squared over 2m. It better be the same, but it will be clearer if we now write what it should be in general. Have a U dagger and a U from the right. They come here, and they turn this into P heisenberg. get the operator still, but we got an equation. So how about P dP dt. So ih dP Heisenberg dt would be P Heisen Berg with H Heisenburg. And this time only the potential term in here matters. So what do we? We get 1/2 m omega squared. Then we get again a factor of 2, then we get one left over Xh. And then a P with Xh, which is a minus ih bar. So the ih bars cancel, and we get dPh dt is equal to. m omega squared cosine squared omega t X squared. And the cross term. Plus 1/2 m omega squared over m omega times cosine omega. t sine omega t XP plus PX. Schrodinger Hamiltonian. So you confirm that this theoretical expectation is absolutely correct. All right, so that's all for today. I hope to see in office hours in the coming days. Be here Wednesday 12:30, maybe 12:25 would be better, and we'll see you then.

ROUGE-1: 21.37, ROUGE-2: 20.34, ROUGE-L: 19.08
BERTScore: 65.37

==============================================
==================== [37/100] ====================
Summary:
Borat: I want to talk about my favorite part of the Second Discourse, a book that never grows old. Borat: Inequality begins in a faculty or a disposition that is the first and most durable to be miserable. He says Rousseau takes the side of the poor and the dispossessed but it isn't property, or poverty rather, that rouses Rousseau's anger as it is the attitudes and beliefs shaped by inequalities and of wealth and power. In many ways, Rousseau like Plato finds his voice when discussing the various complexities of the human soul, Borat says. Amour-propre is an untranslatable word but related to a range of psychological characteristics such as pride, vanity, conceit. Rousseau distinguishes amour- propre from another disposition that he calls amour de soi-meme, a sort of self-love. He speculates that the passion of vanity was born as soon as people became conscious of the gaze of another, he says. "Each one," he says, "began at the first hand and shame and envy on the other hand" Rousseau would argue that the deeper cause seemed to be that the protesters believed was disrespect being shown to them, to their beliefs. Toleration in many ways is a liberal virtue because it requires us to distinguish between beliefs that we may take with utmost seriousness in private life. But it is one thing, you might say, to tolerate other views and another thing to accord them respect and esteem, says Julian Zelizer, a professor of history at the University of California, Los Angeles. Rousseau's Second Discourse ends on a note of utmost despair. It offers no positive answer to cure the problem of civilization but only hints at best at two possible solutions. Democracy for him is the social condition that most closely approximates the equality of the state of nature. The Social Contract begins with theState of war? This is the question that the Social Contract sets out to answer and to which his formulation, his famous formulation of what he calls the general will, is the solution. This feeling of existence without a thought for the future, without care or fear, the individual somehow psychologically returns to the natural state. Only a very few people, Rousseau writes, are capable of finding their way back to nature. Rousseau believed himself to be one of these people. Maybe you also are one of them. Yes? But it requires you, in some way, to distance yourself severely and psychologically from all of the possibilities of society. It was that inward journey that Rousseau took and that he writes about so powerfully in his Confessions. In the state of nature, we are born free, equal and independent. Only in society do we become weak, dependent, and enslaved. How did this take--how did this change take place, Rousseau asks. What can give these chains or bonds moral legitimacy? He says considerable philosophical abstraction whose leading concepts are abstractions like the social contract, the general will, and so on. The Principles of Political Right suggests that are appropriate to human beings conceived as free agents responsible to themselves alone. assumption that each individual has a deep rooted interest in securing the conditions of their own liberty. Each of us has a desire to preserve his or her own freedom and that social order will be rational or just, that allows us to preserve that freedom. The state of nature quickly becomes a state of war based on conflicting desires and conflicting again means of liberty preservation. So how do we preserve our liberty without lapsing into anarchy, that is how to preserve our freedom without lapses into anarchy?

ROUGE-1: 20.04, ROUGE-2: 18.44, ROUGE-L: 18.50
BERTScore: 63.77

==============================================
==================== [38/100] ====================
Summary:
So I think I just spend like five minutes, just briefly review on the backpropagation last time. So I didn't have time to explain this figure, which I think probably would be useful as a high level summary of what's happening. So you start with some example x, and then you have some-- I guess, this is a matrix vector multiplication module, and you take x inner product multiplied with w and b. And then get some activation, the pre-activation. And you get some post activation. This is how you define network and the loss function. done by one of the lemma that we discussed last time. Next lecture, we're going to talk about the concept of regularization. And next lecture we also talk about some of the practical viewpoint of ML. Any questions? This is just an extension of the last five minutes of the previous lecture. OK. So good. So now in this lecture, and the lecture afterwards, we are talking about, I guess, a few concepts. One concept is called generalization, which is the main point of this lecture. The test loss is defined on unseen examples. So suppose, say, you draw some new example, x comma y, from some distribution D. And then you evaluate what's the expected loss on this new test example. And typically, when l theta is big, there are two failure mode in some sense. So one of the failure mode is called overfitting. So a typical situation of overfitting is that the training loss, j, is small, but the test lost is big. In some sense, you care about two quantities: the training loss and the gap. You want both of these two to be small. An underfitting, basically, just means that you face something like this. So you can see that there's a large training error, training loss or training-- let's call it loss just for consistency. And now let's think about so what you should blame. Why the training is big? What's the culprit? The culprit, I would argue, is that no any linear model can fit your data. So suppose you have some x and some y. You have some data set. The data are approximately quadratic. So given x, you want to predict y. And you observe some-- so you have aData set. For example, you have four points. You want to fit a line to it or fit some curve to it. And the question is what curve you are going to fit? So suppose, you fit something crazy like this. Let me try to see what color I'm using for this. Sorry. The bias is going to be a decreasing function as the model complexity. The variance is, in some sense, you can say, it's not very important. Only the bias is the culprit. And now, I'm going to show cases where the variance is the. culprit to blame for. So any questions so far? Why is the bias [INAUDIBLE]. Why is bias this crazy? Oh, squared, I mean. Oh, this is just because it's kind of a unit thing. The bias is basically like it's saying that the reason why-- I don't know exactly why people call it bias in the very first time. So this is just because the linear model is not expressive enough. And this is called bias. And you can kind of see that it's probably important for bias to be small because if bias is large, even with infinite data, you cannot do anything. And by that, reasons. One is like you have lack of data, and the other is you have too expressive models. If you don't have enough points, and your degree is high enough, then you can always make the training error 0, literally 0. And the thing is that this is overfitting. So basically, you are looking at-- you are kind of like overfitting to the spurious patterns, but instead of the big pattern. The big pattern is this. The spurious patterns are the fluctuations in some sense. And so in other words, I think you are explaining the noise instead of. the ground truth. When you draw the same number of samples with similar ground truth-- the same ground truth and the solution. But just their randomness are different. And that's a good question. That's exactly what I'm going to talk about next. OK. When we don't know the ground truth, you cannot really exact like-- let me think. How do you know that you are having a large bias? You cannot really exactly know. When you don't have infinite data, there is no way to evaluate the bias. So typically, what you do is you say, "Underfitting means you have a large training error" bias square [INAUDIBLE] what's the third one? The third one is the sum of them. This is the test error. And the bias is the total of them? Bias is this one. I'll discuss that in a moment. I think I do have something to say about the variance, and then I'll come back to the trade-off. OK. Are we [INAudIBLE] for highly imbalanced data set? So maybe let's discuss this offline. it means that it's somewhere in the middle. So basically, when you see the training error is big, you kind of see your biases. You kind of believe that your bias is too high, so that's why you should increase the model complexity. And at some point, you find that you are in other regime, where the variance is toohigh, then you should stop. Basically, you increase theModel complexity to some extent until your bias and variance has a right trade-off. of bias and variance first change, did you use different type of model [INAUDIBLE] So I think this figure, so this is the-- OK. You ask a good question. So probably, the best thing is to use quadratic. Quadratic is, in principle, expressive enough to express our data. That's probably the best solution. And if you really run the algorithm, the quadratics, you would probably recover something very close. But if you're going to go with a different model, there's always a trade-off. Double descent is the second descent of the test error. In this regime, typically, the number of parameters is larger than the numbers of data points. This peak is often happening when-- it's roughly equal to d. This is basically mostly kind of correct for linear models. But for nonlinear models, whether this is exactly equal to 2 or not is less clear. In some sense, the norm is also a way to describe how many, like you have a small ball-- you have fewer choices to fit your data. that, you have more data. It actually helps. I saw some questions. So the original double descent, does that like continue to decrease or does it eventually increase again? So in the first. figure. This is, again, more than this function. This was like this has been for a while. For this one? Yeah. This phenomenon. This one, I think, is also-- actually, the paper that first systematically discussed this is like 2020. About that peak, when was that discovered? The peak? This is discovered in the same paper, the peak. people really care about it. Even within linear models, you can still change the complexity, just to clarify that. And most of this theoretical study, I think, are for linear models. And they are pretty precise these days. And I'm going to try to kind of roughly summarize the intuition from the study. And what I mean by that is that you can try to change the model complexity. So you can start with only using one feature or two features like for example, in the house price. That means you have more and more parameters. The peak is caused because the algorithm was suboptimal. The norm of the theta, the linear models you learned, is very big when n is roughly equals d. So regularization would mitigate this to some extent. But there's one more question, which is there is no peak, but why there's no ascent? So suppose you just see this. Actually here, you will also see this, something like this. So this one, let's say, we are OK with it. We are happy if you see just a peak. I guess we are not going to go into that. But at least, the immediate reason is that when n is close to d, somehow, this algorithm is producing a very large norm on classifier theta. You can argue that if the norm is too big, then your model is too complex. So very complex on [INAUDIBLE] to the norm. So this model, it seem it doesn't have a lot of parameters compared to, for example, this model. That's by definition. The norm is actually very big.

ROUGE-1: 20.58, ROUGE-2: 19.82, ROUGE-L: 17.41
BERTScore: 69.73

==============================================
==================== [39/100] ====================
Summary:
Vladimir Ilyich Ulyanov, AKA Lenin, helped overthrow the Russian tsar Nicholas II in 1917. He founded the Soviet Union, one of the worst dictatorships of the 20th century. But was he a hero who toppled an oppressive tyranny or a villain who replaced it with another? It's time to put Lenin on the stand in History vs. Lenin, says Alexander Nekrassov, the author of the novel "The Death and Life of Vladimir Lenin"

ROUGE-1: 13.29, ROUGE-2: 10.62, ROUGE-L: 9.44
BERTScore: 61.39

==============================================
==================== [40/100] ====================
Summary:
The goal of this course is to give you the tools to interpret complicated phenomena. We have electronic structure and the hydrogen atom as a way of understanding what electronic structure is. When we go to molecular orbital theory, we take what we know about atoms, and build a minimally-complex interpretive picture, which is sort of a framework for understanding complicated molecular interactions. The relationship between the effective quantum number and the ionization energy of a state then provides a hydrogen-atom-based structural model for everything you can observe. A rigorously good quantum number corresponds to a eigenvalue of an operator that commutes with the Hamiltonian. The main obstacle to being more than two electrons in an orbital is that we're protecting you from unnecessary knowledge. The goal is to be able to do the algebra in a way that maybe you can't describe to your friends because it's too complicated. But it is something that you can learn, and you can ask a computer to do it, and there are all sorts of intuitive shortcuts where you can look at a problem. able to evaluate these matrix elements is the permutation requirement. And it turns out that there is a really simple way of dealing with the requirements for electron permutation, and that is to write the wave function as a determinant of one-electron orbitals. If you have two identical columns or rows, it's 0. And that is the Pauli exclusion principle-- not what you learned in high school. What you learned is a small consequence of that. So if we can build anti-symmetric wave functions, we have aufbau. you the things you have to think about and understand. And a fit model also tells you what are the import the actors. And maybe they're in costume,Maybe they're not, but we can deal with them. But the truth is really very complicated. And as I said many times, when you go from hydrogen to helium, you can't solve the Schrodinger equation exactly. This was perhaps a little bit of a surprise, but I think it was only a surprise in newspapers. can reduce it simply by, instead of writing psi every time, just writing the state. And so for the ground state of helium 1s squared, and we would do this. And we use this notation, J tilde minus K tilde. So for every two-electron thing, we're going to get this kind of-- now these are simple integrals, and some of them are any Slater determinant. And the sign comes from switching the order of the orbitals. That's how the determinants work. Tilde notation says, well, this is what we start with, and we have to convert it into things that really matter. When we do 1s squared, we have an alpha with an alpha for the J term, and analpha with the beta for the K term. And alpha with beta is 0, because the operator cannot change the alpha into beta. So this tilde notation is a convenient thing, because you can use allowed to say which one is the lowest. And it's the one with the maximum L. So we have orbital angular momentum. And we can add the orbital angular momenta of the electrons following certain rules. And so what ends up happening is you get z effective, which is dependent on distance from the nucleus. And it goes from the integer value that you know, from the atomic number, down to 1, because you've taken one electron away from a neutral atom, and taken it outside. And now we have this wonderful thing called the centrifugal barrier. So if we have a non-zero l, it can't get in so far. And the larger the l is, the less it can see this extra charge. Hund's rules is all about, of all of the states that belong to a particular configuration, which one is the lowest? One-- which one, not the second lowest. And all you care about, all you're when N is equal to 3, the lowest state is usually an S state. So Hund'srules tell you how to identify, without knowing beans, what is the low energy state. Well, maybe sometimes wrong, but that's because of one of my things where you have a perturbation between states belonging to two configurations.

ROUGE-1: 23.33, ROUGE-2: 22.58, ROUGE-L: 18.50
BERTScore: 66.90

==============================================
==================== [41/100] ====================
Summary:
As a nurse we will transfuse a patient who is low on red blood cells with new blood cells via a venous access of some type. Red blood cells are very vital for our survival and how our body works so in other words our body can't function very well without them. Most hospitals require that you're a registered nurse in order to transfuse the blood so again follow your Hospital protocol with that. You can access the free quiz that will test you on this procedure so let's get started. past and if they have how many because if they're at risk for febrile nan hemolytic transfusion reaction where their body has just built up these antibodies from all those previous transfusions and they can start running a fever. A lot of times physicians like to pre medicate them and you'll want to let the physician know if they do have a history of that. Sometimes they're pre-medicated with benadryl or Tylenol acetaminophen before hand orally or when about 30 minutes before you start the transfusion and that will help prevent that. transfuse blood you use special tubing which is called Y tubing with an inline filter which helps filter some of those substances out of the blood before it actually goes to the patient. A lot of protocols say only one set of Y tubing per unit that you transfuse so you'll have to you're going to transfuse the patient with more than one unit multiple sets or some hospitals say it's only good for four hours so keep that in mind when you need to change your filter your tubing next you'll want to grab a bag of normal saline 0.9% normal saline. like that you want to use a special device if need be next before you even start the transfusion you're gonna be doing this verification process. If there's a discrepancy you'll need to notify the blood bank immediately and just from personal experience this has happened with one of my patients I was doing the whole verification process with another nurse and we were looking at the blood bag. There was one letter that they had did a clerical error on so I had to send the blood and we had to go through the whole process again. breath headache backache or nausea and vomiting and if this happens you'll immediately want to stop the transfusion okay now it's actually time to start the transfusions so you're gonna have your blood ready hung and it's going to be controlled by an infusion pump which will deliver it to the patient. You want to start  slowly about two milliliters per minute for those first 15 minutes in addition you want to stay with that patient at their bedside looking at them monitoring them. A 418 is your patient saying I have a backache all of a sudden or I'm having chest pain or my head is hurting that's a red flag c-4 chills t4 tachycardia especially if it's really increased from baseline I for increased respirations same thing with that increase from baseline oh for all glory. You really want to be looking at your patients urinary output during this blood transfusion and after are they putting out low or are they just putting out no urine at all are they an Urich. graft-versus-host disease and again like I said this is rare but it's deadly and it tends to occur days to weeks after the transfusion. This is where the donors T lymphocytes cause an immune response in the recipient but actually in grasping in the marrow of the recipient and attacking the recipients tissue. Other complications that can arise that really aren't immune related it's like septicemia where the blood is contaminated so these T lymphocyte are usually killed by the recipients body but however maybe the patient has a suppressed immune system. diuretics also some labs are going to be ordered they want to look at those claudine levels because remember if this is hemolytic type. You'll be collecting urine urine on them looking for the free hemoglobin that's came from those red blood cells I have lysis and whenever you are disconnecting your tubing over here do not throw it away don't throw any of it away because you'll be sending that one with the leftover blood and any other documentation to the blood bank who's going to test it. the patient is currently doing okay so that wraps up this review over blood transfusion. thank you so much for watching don't forget to take the free quiz and to subscribe to our channel for more videos. Back to Mail Online home. back to the page you came from. Click here to read the full transcript of the blood transfusions video. Back To the pageyou came from, click here to watch the full video. CLICK HERE to see the full Transcript of the Blood transfusion Video.

ROUGE-1: 30.65, ROUGE-2: 29.20, ROUGE-L: 28.79
BERTScore: 64.80

==============================================
==================== [42/100] ====================
Summary:
In particle physics and in nuclear physics, use a system called natural units. This system is based on fundamental concepts of quantum mechanics and special relativity. In this class, we'll use natural units in some examples, and SI units in others. This will always be clear from the problem we're looking at, so we're always looking at the problem at hand. We'll use Heaviside-Lorentz units and combine them with some measurable units we discussed in the previous class.

ROUGE-1: 18.27, ROUGE-2: 15.06, ROUGE-L: 12.25
BERTScore: 55.14

==============================================
==================== [43/100] ====================
Summary:
Jeffrey Grossman: We're going to talk about energy storage. He says the planet is kind of a storage device for that thing over there. Most energy storage needs are going to be around a year or less, he says. Grossman says electricity is more and more important in our lives. We'll go into batteries and then talk about the chemistry of batteries and throw in a couple of, why this matters, he writes..com/jeffrey-grossman/energy-storage-technique. Pumped hydro is extraordinarily limited. You have to do it at very large scales. You can convert energy into some fuel and store it and then combust it. This is what we're going to talk about today, electrochemical. There is an advantage about using electrochemistry, and what is electro-- I should tell you what that is, right, and it's the relationship between electricity and chemical reactions. I love saving time, rxns. Change not going from your computer to your phone but from your car to your house to a grid. Most power plants at best are around 50% efficient in terms of converting the thermal energy into electrical energy. Most of that gasoline's energy is going into waste. So you could do chemical reaction to heat. That's how we power most of our world today. There's a lot of reasons why electrochemical batteries are so appealing. No penalty on the second law is one of them. No pollution at the point of use. Also, the batteries can be charged by renewable technologies like solar wind. They're very highly efficient. a higher entropy? How many of you think it's the one on the right? So the thing is it'sThe thing is that entropy is not about smoothness and it's not about disorder. Entropy is about accessibility to states. It's about how many states you have to be in. A messier room does not have higher entropy, and in this case, it's just simply the rules of the algorithm. I have rules here about the dots being able to touch each other. That means that those dots had fewer possibilities. Little is a transfer of electrons from electrons to electrons, a process known as the electron transfer process. Little is a program that teaches people how to use electricity on a smaller scale. Today's episode of Little is the first in a series on the history of electricity. The second is a look at how electricity works on a grid scale, and how it can be harnessed for the benefit of the public. The third and final episode will be about how electricity can be used on a small scale to power a solar cell. Sun. You just press this and you can read at night, right. That doesn't happen unless you've got electrochemical energy storage, right, and there's some stats up here. $25 billion liters of kerosene are used to meet the basic lighting needs in a lot of these places. That releases tremendous amounts of toxic fumes. But you know, it's sort of the same as cooking. So the challenge is, but you don't cook. If you have a solar cooker, that's great except that most people don't cooking during the day. lots of metals. That's what he did. And he showed that the frog moved with most of them, so it's the metals. So let's take these two classic metals and show what happens. You've got copper and you've got zinc. Now, in this case, I'm just going to have a zinc. This is the zinc piece of metal, and this is a piece of zinc, and I put it in a solution of copper ions. And the copper in the solution plates onto the zinc, plus those 2 electrons goes to copper like that. You can use volt meters to make your own lithium-ion batteries. You can use paper clips and copper wire to connect different metals. You are the salt bridge, so what that means is you are actually plating and taking metals off of these two electrodes right now. Your hands are doing that and you're trading electrons. You're a conduit for that to happen, right. It's pretty cool, and you can play with these. You know, you have volt meters. You have lots of items on the table tomorrow maybe at a dinner. The power isn't coming from you as Galvani thought. No, the power is coming from the difference in potentials of those metals wanting or not wanting electrons. Everybody wants electrons, let's be honest, but who wants its more? It all comes back to the same stuff we talked about. The delta v is all about the chemistry. It's all about all of the things that we've learned, but at the core of all of this is how strongly are electrons bound to an atom, to a metal. The growth in solar and wind has been incredible over the last 10, 20 years. The problem, as I think I've shown you, is the variability. There's no way to use renewables at large scales unless you store it. You can't solve renewables at the scale of the grid without storage. And no option exists today. It might be batteries, but there's nothing that actually does it today at the scales that we need. And I hope you guys have a great Thanksgiving and maybe hook up some different metals.

ROUGE-1: 20.29, ROUGE-2: 18.10, ROUGE-L: 17.54
BERTScore: 63.39

==============================================
==================== [44/100] ====================
Summary:
We are going to start talking about the optimization perspective in deep learning for two lectures. The main focus is to analyze what the functions you are optimizing look like so that you can use some trivial or some standard optimization algorithm for it. Note that it's not like these algorithms, like gradient descent or stochastic gradient descent, can work for all functions that you may optimize. There are some negative results about the NP-hardness of optimizing nonconvex functions. So the way that this is really reconciled is that the lower bound, the impossibility results is about worst case functions. Theory of gradient descent is that it cannot always find local mean or global minimum. Finding global minimum of general functions, general nonconvex functions, is NP-hard. But it doesn't really mean that there is no subset of functions that you can easily solve. And I guess the observation four is that objectives in deep learning are to find global minimum in polynomial time. That's what we're trying to do in this talk. We're going to talk about the goals of deep learning and how to achieve them. is nonconvex. It's almost trivial, but not entirely trivial. Most of the convex functions we know are somewhat kind of simple functions. And as soon as you go beyond two layers, it's not convex. And observation five, I think I mentioned it. Sort of like, gradient descent does work. Gradient descent or stochastic gradient descent finds-- let me be precise about this-- approximate, or even sometimes you can claim it's almost exactly, global minimum of loss functions in deep learning. Finding a local minimum is also NP-hard. So we have to consider these kind of pathological cases, which makes things harder. So the way to go beyond it is to remove some of the pathological cases as well so that you can find a localminimum in polynomial time. All note that number number works right, right? All right, cool, that's not entirely important because we are not going to be very quantitative about this. So everything is polynomially time. So I guess let me start with some definitions to formalize it. The problem here is that, when the gradient of fx is 0 and also the Hessian is not strictly positive semidefinite, it's just a positive semidrive. In that direction, you are pretty flat, right? So that makes it tricky because then the higher order gradients start to matter. And when you look at this-- and once it becomes about the third order derivative or fourth order derivative, things becomes much more complicated. So there's no set of cases in your function. This is strict-saddle. you still have to do the Scribe kind of from scratch in some sense. OK, cool. So the definition of strict-saddle, I'm citing this paper just because it's not like every paper is exactly the same definition. So we say f is alpha, beta, gamma strict-Saddle if, for every x in RD, it satisfies one of the following. The condition itself is not supposed for people to numerically check. So this condition is not something that you are supposed to check numerically. kind of like informal just because I'm not-- it's pretty formal in the sense that all the bounds are correct. It's just that I wouldn't specify some of the details. So suppose f is alpha, beta, and gamma strict-saddle. Then many optimizers, for example, GD, SGD, and many other articles, like cubic regularization, I guess many algorithms can do this. So far as can converge to a local min with epsilon error in Euclidean distance in time. exactly the thing that we did for the strict-saddle. But if you think about it, it's basically the same statement. OK. Anyway-- so cool. So we are basically done with the first part, so about identifying the subset of functions that are easy to optimize. And next, we are going to show some examples where these kind of properties can be proved rigorously for machine learning situations. But these examples are pretty simple. They are not deep learning. So these are still roughly the best that people can do in some sense. in machine learning, like especially if you think about nonlinear cases. And now, still I think it's used in the recommendation system. OK, cool. So any questions so far? I guess let's talk about PCA first. So we are assuming that we are given a matrix M in dimension d by d. And we want to find the best rank one case. So in this case, the bestrank one approximation is basically the eigen vector times eigenvector transpose up to some scaling. So how do we prove this? So as you can imagine, the proof is pretty simple. The plan is very simple. You first find out all stationary point, the first order stationary points. And then you prove that they are all global minimum. So basically, it's just more or less like we solve all of these equations and see what are the possible local minimum you can have, right? So let's firstly use the stationary points, a gradient condition. So gradient of g of x, I'm not going to give a detailed calculation here. This is equal to minus this times x. The methodology also applies when you talk about the Hessian. So the methodology, I'm not going to go to all the details. But roughly speaking, what you do is the following. So g of x plus some linear term in epsilon plus some quadratic term and so on and so forth, past the higher order term. And then if you have this, then this basically corresponds to something like v dot g square gxv. And so this is a very simple way to compute the Hessians. top one eigenvalue-- eigen vector with the right scaling. So the second case is that x has eigen value, let's say, lambda, which is strictly less than Lambda 1. And then because x is an eigenvector and also the eigen Value of x is orthogonal to v1, then x1 is Orthogonal. There is no guarantee that two eigenvectors are always orthogona because they could have the same eigenvalues and they are just in the same subspace. But if they have different eigen values, then they have to be orthogonial. Theory: P omega of A is the matrix obtained by zeroing out every entry outside omega. Top left corner is 1, so there is no way you can recover this matrix unless you observe that top left corner. Theory: If there's no other structure in this matrix M, there isNo way you. can recover the other entries because they can be arbitrary. Theorem: If p is something like poly mu and log d over d epsilon, then p omega of M is basically a random subset. by the users. Every user probably have an opinion about every item, right? Either they like it or not, so and so forth. So every user only buys a very small subset of the item. And that's why you only see some entries in this matrix. Amazon only sees some of the entries. And Amazon wants to understand what each user's preference is, want to know that each user likes which item. So the Amazon has an incentive to just fill in the entire table. it's unlikely it can work. So basically, that is saying that p is bigger than roughly 1 over d. And speaking of the objective functions, this is actually a pretty commonly used method in practice. So you just say I'm going to minimize this function that's called fx, which is defined to be that basically you have a parameterization called xx transpose. And you want to say this matrix actually faced all my observations, right? So you are taking a sum over all possible observed entries because these are the only cases you know what the entries are. no local minimum, all local minimum are global. So the infinity norm is less than mu over square root d. A mu is considered a constant or logarithmic in d. So what it's saying is that, this factor z, the norm is 1. And also, the entries are spread out. You cannot just have all the mass concentrated on one entry. So this is called incoherence assumption. And this assumption is necessary. People know it. So I guess we assume, for example-- first of all, we assume the ground truth has norm 1. assume that the input are linearly separable, then there is a proof for this. And there are a bunch of other cases where you can have some partial results. Next week, in the next lecture, maybe the second half of next lecture,. I'm also going to give another result, which is somewhat more general. It applies to many different architectures, but it has other kind of constraints. First of all, it doesn't really show exactly these kind of landscape properties. It shows that these kinds of properties holds for a region, for a special region in the parameter space.

ROUGE-1: 26.65, ROUGE-2: 25.14, ROUGE-L: 23.63
BERTScore: 68.31

==============================================
==================== [45/100] ====================
Summary:
Iceland is one of the world's most active volcanic hotspots crafer has erupted 30 times in a thousand years and last blue in the 1980s. Scientists are now preparing to drill into it the is to learn more about how volcanoes behave so that we can better predict eruptions and also tap into a super hot source of energy volcanoes can be spectacular but they're also devastating around the world millions of people live close to them here in Iceland. Researchers here hope their work will change that helping to save lives and money while also pioneering a form of volcano power.

ROUGE-1: 23.42, ROUGE-2: 22.80, ROUGE-L: 23.42
BERTScore: 66.24

==============================================
==================== [46/100] ====================
Summary:
Sales and watched by taxes is one of three different kinds of tax we're gonna have aggressive so with our regressive tax as people earn more income base they are attacked a smaller amount of smaller percentage of the rounding. So it's not the dollar amount it's gonna be obvious that everyone is must and a larger dollar amount in taxes their income goes up it's nothing down it's the percent of their income that you're spending that's what determines put the tax is regressive and what we see here is taxes. We have these marginal tax rates and say okay look as we earn a dollar from it some amount of that dollar is going to be paid in tax but that percentage that you're paying tax changes so let's assume that we made for $90,000 you're single and we wanted to know how much you pay. The difference here between 86 and 36 has been to thousands with $50,000 we've got 4,000 yes this guy is easy the decimal point four thousand five so 12,500 plus we're going to pay 18,000. want to penalize people for being married does that make sense right I mean society should not encourage people to do just shack up right exactly make sense of maybe that for least generally we've got what we're done matter so we know we don't want this that's not the answer is is that there is no answer you cannot devise but these are just kind of illustrates some of the problems that you have in developing the taxes. When we're looking at fiscal policy Keynes's. We can have for the government the revenue for them the taxes so we can say a budget deficit this is like food stamps. have a deficit the deficit is going to get larger or if you have a surplus what would happen in the surplus it'll get smaller by contraction fiscal policies exact opposite here. We have this new classical view of fiscal policy and we have those called the supply side now. What we'll see on Wednesday and what these guys actually do is change the way we think about fiscal policy. We can engage the fiscal policy that makes these guys change on purpose we can have it's called discretionary fiscal policy where we go out and we after the change times more government spending leather or making them rise Falls.

ROUGE-1: 24.33, ROUGE-2: 23.49, ROUGE-L: 22.28
BERTScore: 66.41

==============================================
==================== [47/100] ====================
Summary:
PhilipPE RIGOLLET: What I want is to have a small bias, hopefully a 0 bias. If this thing is 0, then we see that the estimator is unbiased. But we'll see that it's actually maybe not enough. Something that's slightly better is the risk, really the quadratics risk, which a small variance. If you reduce one too much, then the variance of the other one is going to increase. That happens a lot, but not so much, actually, in this class. is expectation of-- so if I have an estimator, theta hat, I'm going to look at the expectation of thetahat n minus theta squared. And so for example, if the quadratic risk goes to 0, then that means that thetaHat converges to theta in the L2 sense. So the risk is really telling you how much fluctuations I have around my expectation if unbiased. So when theta of the risk, the theta that you have here if you're unbiased is really the expectation. So that's really just the variance. for sum of independent random variables, now it's time to wake up. So we have the variance of something that looks like 1 over n, the sum from i equal 1 to n of Xi. So it's of the form variance of a constant times a random variable. We would like somehow to say that this is the sum of the variances. And in general, we are not allowed to. But we are because my Xi's are actually independent. And that's by independence, so. is very close to 0.5, I'm very happy. When theta gets farther, it's a little bit annoying. So now the thing with the risk of this guy is that it will depend on n. So as n increases, it is going to look more and more like this. It's the same curve divided by n. And so now I can just start to understand that for different values of thetas, now I'm going to have to be very. close to theta is equal to 1/2 if I want to start saying that Xn bar is worse than the naive estimator 0. 5. to be the same. So here, I'm going to get some bias, but the variance is actually going to be much better, because I get to average all the coordinates for this guy. As n increases, the variance decreases, like 1 over n or theta, 1 minus theta over n. And so this is how it happens in general. In this class, it's mostly one-dimensional parameter estimation. But if you do, for example, non-parametric estimation, that's all you do. what a confidence interval is. And so we fixed a statistical model for n observations, X1 to Xn. The parameter theta here is one-dimensional. Theta is a subset of the real line, and that's why I talk about intervals. A confidence interval of level 1 minus alpha-- so we refer to the quality of a confidence intervals is actually called it's level. The closer to 1 it is, the better the confidence interval, and the closer to 0, the worse it is. So we know from the central limit theorem that Xn bar minus p divided by square root of p1 minus p converges in distribution as n goes to infinity to some standard normal distribution. This is by definition of the quintile of a standard Gaussian and of a limit in distribution. We know that this is just the probability that the absolute value of sum just take the largest possible value for p1minus p, which makes the interval as large as possible. So this by now, hopefully after doing it three times, you should really, really be comfortable with just creating this confidence interval. another couple times in your homework. So just make sure you're comfortable with this. That's one of the basic things you would want to know. Are there any questions? Yes. OK. So it's important, because now we're going to switch to the real let's do some hardcore computation type of things. So that's what the function here-- the function you're interested in is 1 over square root of X1 minus X. So what does this function look like around the point where you think P is the true parameter? Its derivative really is what matters. and so you feel like you're a little more-- you have a more precise answer. Now, if you really need to be super-conservative, then you're actually going to go with the P1 minus P. So depends on-- I mean, there's a lot of data in statistics which is gauging how critical it is for you to output valid error bounds or if they're really just here to be indicative of the precision of the estimator you gave from a more qualitative perspective. When we do maximum likelihood estimation, likelihood is the function, so we need to maximize a function. And if I give you a function, you need to know how to maximize this function. Sometimes, you have closed-form solutions. You can take the derivative and set it equal to 0 and solve it. But sometimes, you actually need to resort to algorithms to do that. And so there's actually a way to compress it by just looking at the basically function distance or vector distance between probability mass functions or probability density functions. true theta star, the one that generated some data, X1 to Xn, in an iid fashion. The goal of knowing thetaStar is so that you can actually know what P theta Star is. So in a way, what does it mean to have two distributions that are close? It means that when you compute probabilities on one distribution, you should have the same probability on the other distributionPretty much. So what we can do is say, well, now I have two candidate distributions. And so here is the strategy to implement our goal. If we have continuous random variables-- so by the way, I didn't mention, but discrete means Bernoulli. The max of those two guys, if this maximum is equal to 0-- I have a maximum of non-negative numbers, their absolute values. So what it means is that the two densities have to be the same pretty much everywhere, which means that the distributions are the same. That's the formal way of saying it. But let's go to this definition-- which is gone. the positive integers, non-negative integers. And so now we have also the continuous ones, such as Gaussian, exponential. And what characterizes those guys is that they have a probability density. So the density, remember the way I use my density is when I want to compute the probability of belonging to some event A. The probability of X falling to some subset of the real line A is simply the integral of the density on this set. That's the famous area under the curve thing. for yourself that graphically, this I can represent as an area not under the curve, but between the curves. Now, this guy is really the integral of the absolute value. So this thing here, this area, this is 2 times the total variation. The scaling 1/2 really doesn't matter. It's just if I want to have an actual correspondence between the maximum and the other guy, I have to do this. So we have this definition. And so we have a couple of properties that come into this. PhilipPE RIGOLLET: The fact that you need two definitions of the [INAUDIBLE],, is it something obvious or is it complete? PHILIPPE Rigollet: I'll do it for you now. So let's just prove that those two things are actually giving me the same definition. So what I'm going to do is I'm actually going to start with the second one. I just don't want to have to write indices all the time. A star is the set over which the function delta is non-negative. If I start adding something to A, the value goes lower. The integral over A of delta is less than the integral over the set of X's such that delta of X isNon-negative of Delta of X, dx. That's an obvious fact, just by picture, say. And that's true for all A. It's actually still be true, even if there are extreme cases, like in this case. was-- if this was a constant, that would still be true. Just need to make sure that there is someplace where it is, but that's about it. So it's a distance. It's symmetric, non-negative, equal to 0, if and only if the two arguments are equal, then it satisfies the triangle. If it's not satisfying this thing, it's called pseudo-distance or quasi-distance. Or just metric or nothing at all, honestly. another notion of distance that sort of has the same properties and the same motivations as the total variation distance. But for this guy, we will be able to build an estimate for it, because it's actually going to be of the form expectation of something. And we're going to able to replace the expectation by an average and then minimize this average. So this surrogate for total variationdistance is actually called the Kullback-Leibler divergence. But it has some roots coming from information theory. don't bother with maxima or anything. I mean, there is something like that, but it's certainly not as natural as the total variation. And so it's just very hard to manipulate, like this integral of absolute values of differences between probability density function. So it's very difficult. But if you can actually-- and even computing it between two Gaussians, just try it for yourself. And please stop doing it after at most six minutes, because you won't be able to do it. here. The problem is that-- actually the star here should be in front of the theta, not of the P, right? That's P theta star, not P star theta. But here, I still cannot compute it, because I have this P thea star that shows up. And that's now where the log plays a role. If you actually pay attention, I said you can use Jensen to prove all this stuff. You could actually replace the log by any concave function. That's called an f divergence.

ROUGE-1: 24.44, ROUGE-2: 23.70, ROUGE-L: 21.47
BERTScore: 69.43

==============================================
==================== [48/100] ====================
Summary:
Markus Klute: This class will be taught in an inverted classroom or flipped classroom setting. The course evaluation or your evaluation in this course will be made up 50% out of homework. The great divide-- the great divide or the grade divide-- at 85% between A and B, 70% between B and C, 60% between C and D, and below 50% earns you an F. The grading scheme will not be worse than what I've given you here, Klute says.

ROUGE-1: 23.03, ROUGE-2: 21.64, ROUGE-L: 19.24
BERTScore: 55.97

==============================================
==================== [49/100] ====================
Summary:
In this problem, we are given a joint PDF for x and y. And then we are asked to compute the variance of x plus y. You can think of x as a new random variable whose variance we want to compute. And moreover, we're told we should compute this variance by using something called the law of total variance. So this really can seem quite intimidating, because we have nested variances and expectations going on, but we'll just take it slowly step by step. And now we can look at this conditional PDF to figure out what this is. if you fix an arbitrary x in the interval, 0 to 1, we're restricting ourselves to this universe. So y can only vary between this point and this point. In the unconditional universe, x and y were uniformly distributed. So it follows that in the conditional universe, y should also be uniformly distributed, because conditioning doesn't change the relative frequency of outcomes. So that reasoning means that we can draw the conditional PDF of y conditioned on x as this. We said it varies between x and x plus 1. And we also said that it's uniform, which means that it must have a height of 1.

ROUGE-1: 23.06, ROUGE-2: 22.43, ROUGE-L: 21.86
BERTScore: 73.41

==============================================
==================== [50/100] ====================
Summary:
In the second half, we'll talk about multisequence alignment. We'll look at how to get an empirical substitution matrix from distantly related protein sequences. And we will say that this is the optimal multiple alignment. The second half of the MIT OpenCourseWare course is available on the Web. For more information about the course, visit ocw.mit.edu or follow us on Twitter @MITOpenCourseWare and @CerebralCereb. The course is free and open-source. The time complexity is have to do 2 to the k comparisons per node. And the larger k is, the more you can explore. It's like doing a huge mutagenesis experiment and exploring viable mutants. The two that we'll illustrate in the next couple of slides is a tree alignment, as illustrated by ClustalW. And we'll show a star alignment. And then when we get, later on into the transcriptome part of the course, we will talk about the Gibbs algorithm. The final branching closest to the trunk of the tree or the roots of tree is called the dendrogram. The next step is aligning each of the sequences, which you already had to have done in order to calculate the similarity matrix. The common ancestor for all the sequences would be the common ancestor of the common ancestors of the first two clusters. And you could imagine keeping doing this hierarchical process. If there were additional sequences which are even more distant related, let's say S5, you would take this alignment of S1, S2, S3, S4 and align it with a single sequence S5. the sequences. And so we'll use S1 as the focus of the star geometry. And we'll get to the Gibbs sampling later. In general when you have a hard problem, where you can't comprehensively go through the entire space, what you do is sample it. You say, let's try a few things, and try to randomly sample it, and maybe even develop locally. If, after randomly sampling in certain places look better, then look near there, and find other solutions, and keep optimizing. Underrepresented in general invertebrate genomes, and over-represented in promoter regions upstream from genes. Promoters and CG islands are sort of degenerate. They're weak sequence signatures. There's a high variety, and they're used in combinations. We need a lot of codons in a row to see a preference over random sequences. Random sequences will also contain some of the same codons. If you need longer ones, then you'll miss tiny proteins. And we'll talk about this in just a moment, specific examples. George Church: Proteins in annotated genomes drop off at 100 amino acids. Church: Why are there so few proteins that are short? And there are slightly more short proteins in Mycoplasma? Church: There are more but we can't find them. When we get to proteomics, we'll talk about ways that you can empirically, by mass spectrometry and so forth, find those small proteins, and genetically, of course, you can find them, he says. be A, C, G, or T. These are four different sequences, real start sites, that we've aligned, either manually or by computer. This is dead easy to do the alignment, but the interpretation here is the position upstream of the start codon doesn't matter. The T and the G position at the 3-prime end of the codons are, in this small sample, invariant. And so they get a count of 4 for the correct base and count of 0 for all the alternatives. Bayes' theorem: Probability of a sequence given a model is equal to the probability of the model times the sequence given the model divided by the sequence. In a database search, we would go through. all sequences in [INAUDIBLE] that look like a serine protease. This would be asking for recognition multiple times, over and over. In the next slide, we'll see what all this Bayesian stuff is useful for. We're going to be doing-- of the various applications, we had recognition discrimination and database search. In a Markov model, you can have nonrandomness at every order of a chain, meaning every length of sequence. You might have a bias where C would be rare because the Cs mutate into Us. In organisms that lack a uracil glycosylase, Cs will change into Us because it's a common chemical reaction. CGs are underrepresented in the genome as a whole, and they're over-represented in promoters. And in the oceans where they're lost, you expect the CG, this particular transition from C to G, to be low. island on the right, in bold and capital letters. You're given this as a learning set. Somebody has, by hand, decided that the boundary occurs at this first CG dinucleotide. And here it's unobserved in this little toy example that I gave you, so it's a 0. So 43% in this actual example. Now we're going to plug these numbers-- basically, I've cut off the transition tables, which are off to the right. Now let's use them to actually do an HMM. That's a pretty extreme case. But this is actually using the numbers from the previous slide, which were taken from real oceans and islands.

ROUGE-1: 21.11, ROUGE-2: 19.54, ROUGE-L: 19.68
BERTScore: 61.49

==============================================
==================== [51/100] ====================
Summary:
in this module of tools and equipment we're going to explore cookware and storage wear there are five basic metals that are used in cookware aluminum copper stainless steel cast iron and carbon steel aluminum pots and pans are the ubiquitous pans that you'll find in most every kitchen. There are various different kinds of specialty pans including crepe pans which are shallow and thin and flat and used to make a crepe using the method of smoothing out the batter across the bottom roasting pans are used for roasting meats and vegetables. A saute pan or satwa is one with the straight sides and has a larger surface area which makes it ideal for tasks like searing meat or reducing a pan sauce. A sawtooth or skillet has a slope side and is used mainly in sauteing the slope sides providing the ample and perfect angle for flipping your food or as it's referred to as saute or to jump a rondeau is a favorite staple in a chef's arsenal and should be in any home cooks as well. These polycarbonate and stainless steel containers called hotel pans are designed to be used to store small amounts of ingredients on a refrigerated make table such as a sandwich station. They come in sizes ranging from full size hotel pans two-thirds pans half pans third pans fourth pans six pans and ninth pans and various other sizes each size is available in six six inch four inch and two inch deep models. sip top bags often referred by their trade name ziploc are an indispensable storage device for kitchens they allow the storage of dry goods and other small quantity items. If you want to get a job in a professional kitchen as a prep cook which is where all cooks and eventually chefs start off at you have to first understand the basic terminology of some equipment in the kitchen. This will also help you if you're a home cook you just want to pick up some stuff at your local restaurant supply store so here we have our basic hotel pan and hotel pans are defined by their depth. There are also half hotel pans which are simply just half the size of a hotel pan still coming in a two inch four inch and six inch depth. however it's not an exhaustive list for more information i suggest ktom.com which can be accessed at the link on the screen. It's a good place to start if you want to find out more about your local football team. It can also be used as a starting point for a discussion about the game. For more information on the game go to www.nhs.org.uk/sport/football/index.html. For a list of all the football teams in the UK, go to http://www.sportstats.co.uk/. For more football news, visit Sportsmail's football page.

ROUGE-1: 24.66, ROUGE-2: 22.82, ROUGE-L: 19.84
BERTScore: 58.09

==============================================
==================== [52/100] ====================
Summary:
Charles Brockden Brown's new book, Edgar Huntly, is a critique of The Enlightenment. Brown argues that The Enlightenment is parasitic on negative types of thinking. He argues that reason itself may not be as powerful as we think. The book may be a book that would seem at first to take many of these things for granted, he says. It's turning the rock to see what's in the shadows that are cast by this light, Brown says. And if you look back at the back of your writing, it's even a possibility that you might do what Brown is trying to do as a philosopher or professor. We called him also a kind of pre-romantic because he makes use of images that are drawn from nature. The soul of a true Christian appears like such a little white flower that we see. It's a literal process of enlightenment right? It opens itself up to the light of the sun, and we've talked I think over the past couple of days about the ways in which the poems and writing that have to do with The Enlightenment often use light itself as a trope. This is a very good passage for thinking about that. David Frum: Is it possible that The Enlightenment underestimates the power of passion? He says the book, the back of the book tells you it's set in Philadelphia in 1787, all right? The same time as the Constitution is being framed a hop, skip, and a jump away, he says. He says this is also if not a critique, then a cautionary tale you might say about take God out of that equation, and you say this is all you know we're gonna rest it all on human beings. The Power of Sympathy is thought to be the first American novel published. The novel is a meditation on what the dangers to the new nation might be. Charles Brockden Brown is one of the first US writers to try to take advantage of first American copyright law which was passed in 1790. The copyright law does something interesting; it makes a profession of authorship possible, because it suggests that writing is property, and if you can write and copyright what you write then you can sell what you writing. In Edgar Huntly there's another kind of scientific fact that is played with right? Sleepwalking. Take a look on the next page, Chapter 12, The Box. I surveyed it with the utmost intention. All its parts appeared equally solid and smooth. It could not be doubted that one of its sides served the purpose of a lid and was possible to be raised. But there was no projecture which might be firmly held by it. Mere strength could. not be applied to raise it. Brown is writing in order to transform the novel into a higher form than it's thought to be. He's writing something that is not within the main line of novelistic writing. Brown is writing for an emerging market. The Power of Sympathy is, but it addressed to young women. So there's something feminized about the form of the novel in this period which is part of why it is, you know regarded as a kind of lower form of writing, not really literary. some sense of the way novels were regarded in this period, this is a quote from the New England Quarterly in 1802. Without the poison instilled by novels into the blood, females in ordinary life would never have been so much the slaves of vice. So this conflation of this kind of carnal house, you know disgusting flesh, morbidity, is part of this stream of the Gothic in which as I suggested the supernatural as real. The sublime is kind of like the romantic emotion par excellence. It's what they are try, they are striving to recreate. was published in 1764 on Christmas Eve and subtitled, A Gothic Story. Gothic, you know is a form of architecture I suppose, first and foremost. It's a kind of you know the pointy churches after the Romanesque arches that are more rounded. Gothic is more pointy. But it becomes a pejorative term by this time that is, that takes on the medieval association. Romantic writers are gonna mobilize the medieval and Gothic and dark over against The Enlightenment to show you what's wrong with The Enlightenment. Classic Scooby-Doo is Ann Radcliffe. The ghosts are never real. It's always Mr. [inaudible] in the blah blah blah. But there's another tradition of Gothic that happens at the same time. Matthew Gregory &quot;Monk&quot.; Lewis who got this nickname because he wrote a very famous book called, The Monk. The Monk you might say is really horrific Gothic. In it the ghosts are real, It is blasphemous. The story goes something like this, again Elvira who is a noble lady. it's Italian right so you can see there's almost always in English Gothic a kind of political anti-Catholic, anti-Italian sort of strain and you know anti-French I guess as well. So Juan Ambrosio is the abbot of the capechins [assumed spelling] He's soon readily corrupted by a satanic woman named Matilda who gains access to his cell by disguising herself as a young nun. They become lovers, naughty, naughty. But he soon gets tired of her and of course she's promoting this, and he dreams of possessing a 15 year old girl. Edgar Huntly wrote a letter to his friend Waldegrave after he was murdered. Huntly was supposed to transcribe the letters for posterity. But he changed his mind when he realized he might have misinterpreted the letters. He now wants to find the killer and find out what he knows. The letter is written in the style that Wordsworth would later write about the way poetry springs, from emotion recollected in tranquility. In the letter Huntly says that he was not insensible at that moment of the impulses of vengeance. But they were transient. Gothic castles and chimaeras are the materials usually employed for this end right? If you're not writing seduction obviously you're writing Gothic. But here in the United States we have an opportunity. The incidents of Indian hostility and the perils of the western take it up again on Wednesday, the experience of the woods. I want you to look for the motif of light in the panther scene, and also track one particular word, and that word is savage. All right we'll leave it there. Thanks a lot. Edgar's reasoning seems reasonable at first but, is curiosity, like virtue its own reward? Virtue never killed the cat, right? Curiosity may not be as valuable or as you might say free from danger. Is knowledge a value for its own sake? And is it true that it's valuable, but it's even more valuable when you have a personal stake in it? There's something odd about Edgar's reasoning here, and the text is, youmight say that this is again one of these texts where the way which Brown has put it down seemingly through the words of Edgar should be allowing us to open up space between what Edgar is ostensibly realizing about himself and what we are realizing about Edgar. to emulate a father's clemency and restore this unhappy man to purity and peace. That you might say is the ultimate statement of enlightenment principles and also of enlightenment hubris. Edgar will fulfill the role of a father confessor. He will dispense the healing balm of rationality and reason, right? How does that work out? Chapter 4, page 34. The bottom of the page, Clithero abrades him. The inference with to have drawn with regard to my designs and my conduct are a tissue of destructive errors. the hand and by which force could be exerted. Some spring therefore secretly existed which might forever elude the senses, but on which the hand by being moved over in all the directions might accidentally light. So look at that. He says physical force isn't gonna work, but he can use his reason to come up with a mechanism. But interestingly that mechanism has to make use of accident. The process was effectual. A touch, casually applied at an angle drove back a bolt and a spring at the same time was sent in action by which the lid was raised above half an inch. a box that can't be closed. Chapter 13, the chapter that follows cements our understanding of the perils between Clithero and Edgar. It give us an account of Edgar's relationship with Waldegrave. The opening of Chapter 13 suggests that Edgar may be feeling guilty too because you might say he's torn by conflicting allegiances. Take a look on page 124 where Edgar tells us that he isn't sleeping very well. But it isn't Clit Hero that appears in his dream. What however was nearly banished from my life. shouldn't have been thinking about. Bottom of the page, and again this is maybe a critique of The Enlightenment. Waldegrave like other men early devoted to mediation and books had adopted a different [inaudible] different systems of opinions on topics collected with religion and morals. His earliest creeds tended to efface the impressions of his education to deify necessity and universalize matter. But when Edgar looks for these letters, on page 128, he finds that they're missing.

ROUGE-1: 22.42, ROUGE-2: 20.84, ROUGE-L: 18.60
BERTScore: 58.63

==============================================
==================== [53/100] ====================
Summary:
Markus Klute: We're starting a new chapter in which we look at tests and implications of special relativity. One experimental test is stellar aberration, which we discussed can be explained by special relativity and by velocity addition. In all of this experimentation and experimental verification, it's important to understand the importance of uncertainties in the scientific process overall, Klute says. He says one needs to be open minded and scientifically open minded to study and grow to the question of whether or not Einstein's theory is correct.

ROUGE-1: 17.82, ROUGE-2: 14.03, ROUGE-L: 15.91
BERTScore: 60.23

==============================================
==================== [54/100] ====================
Summary:
In the colonial era, most American women of European descent lived lives much like those of their European counterparts: They were legally and socially subservient to men. Lower and working class women were actually more equal to men of their own classes, but only because they were, like, equally poor. In the 19th century, the movement for women’s rights was the most visible manifestation of woman’S suffrage, raised most eloquently at the Seneca Falls Convention of 1848. domesticity decreed that a woman’s place was in the home, so rather than making stuff, the job of women was to enable their husbands to make stuff. “Woman is to win everything by peace and love; by making herself so much respected, esteemed and loved, that to yield to her opinions and to gratify her wishes, will be the free-will offering of the heart,” said Catharine Stowe, sister of Harriet Beecher Stowe. But it wasn’t just men who bought into the Cult of Domesticity. The idea of true equality between men and women was so radical that almost no one embraced it. women found work in that most disreputable of fields, teaching, but the cult of domesticity held that a respectable middle class woman should stay at home. Many of the most famous advocates for legally prohibiting the sale of alcohol in the US were women. Many women were also important contributors to the anti-slavery movement, although they tended to have more subordinate roles. Like, abolitionist Maria Stewart was the first African American woman to lecture to mixed male and female audiences. Harriet Beecher Stowe wrote the terrible but very important Harriet Stowe.

ROUGE-1: 18.68, ROUGE-2: 17.53, ROUGE-L: 17.45
BERTScore: 58.33

==============================================
==================== [55/100] ====================
Summary:
We're really kind of bridging the gap between the victorian and the modern we're really transitioning. These are the last authors that we'll focus on in this time period some of their philosophy some of the writing styles and things that motivated them. Most of his poetry especially towards the end if you look on the right there the grief and poetry era. The death of his mother was pretty traumatic and so you can see this kind of as the lover's lament which we haven't really done much of since the sonnets. loss and help you know uh um hopefully other people and and help spread emotion uh through and through to an athlete dying young on page 1000 for us to an athletes dying young. There's an article that i'm going to have you take a look at later uh it might even be active already some of you may have read it. So it's a piece that's uh you know to a young athlete young you already know what it's kind of about um but really in essence the main thing is the reflection of the past and and kind of sad but written in a lyrical kind of way. heart away give pearls away and rubies but keep your fancy free but i was one in twenty no use to talk to me. When i was 1 and 20 i heard him say again the heart out of the bosom was never given in vain just paid with size aplenty and sold for endless rue. i'm 21 i know what's best right you guys have all experienced that your folks you know at this age until you do something you're like no way i'm not going to do that.

ROUGE-1: 29.94, ROUGE-2: 28.70, ROUGE-L: 27.60
BERTScore: 66.34

==============================================
==================== [56/100] ====================
Summary:
In quantum mechanics, the spin of a particle with a vector is quantized. You calculate the length of the spin vector. helicity is defined as the spin dotted with the momentum of the particle. An electron is a left-handed particle, so its helicity will point in this direction. For a fermion, which has a spin 1/2, you get plus 1/ 2 if the spin points in the momentum direction and minus 1/1 if it points in opposite direction. There's no right and wrong in this discussion. If you want to get an eigenvalue with the physical state of particles, which axis are the right ones to choose-- or, sensible? Let me motivate this. The components, and along any axis, actually-- and in this case here, the d-axis-- have eigenvalues. And we find that there is 2s plus 1 possible values. So I'll pick here, just arbitrarily, the z axis. But the question-- it's an obvious question-- which axis is a sensible choice for this problem?

ROUGE-1: 38.78, ROUGE-2: 35.93, ROUGE-L: 19.39
BERTScore: 73.75

==============================================
==================== [57/100] ====================
Summary:
In the 5th Century BC Athens was a direct democracy that encouraged wide participation through the principle of ho boulomenos, or anyone who wishes. This meant that any of its approximately 30,000 eligible citizens could attend the ecclesia, a general assembly meeting several times a month. Some ancient philosophers, including Plato, disparaged this form of democracy as being anarchic and run by fools. Could reviving election by lottery lead to more effective government through a more diverse and representative group of legislatures?

ROUGE-1: 23.45, ROUGE-2: 22.38, ROUGE-L: 23.45
BERTScore: 62.66

==============================================
==================== [58/100] ====================
Summary:
The Fourier transform is of any function that's either a product of those or a convolution of those kind of base functions. We're going to talk about the convolution theorem, noise and filtering Shannon-Nyquist sampling theorem and spectral estimation. Next time, we're going on to spectrograms and an important idea of windowing and tapering, time bandwidth product, and some more advanced filtering methods. And there may be, if there's time, I'll talk about a little trick for removing the line noise from signals. using this Matlab function FFT. In order to do this properly, you should first circularly shift. You have to take the time series. And actually, the FFT algorithm is expecting the first half of the data in the second half of that data vector. So that's what this looks like. Here is a cosine at 20 hertz. And you can see if you take the fast Fourier transform of that, you can. see that what you see is the real part as a function of frequency. Fourier transforms have an interesting property about scaling in time and frequency. If a signal had 10 times as much amplitude, the power would be how much larger? If you have a signal like this that's periodic at about 5 hertz, you can see a series of peaks. If you take that same function and you make it go faster-- so now, it's at about 10 hertz-- the Fourier transform is exactly the same. It's just scaled out. So the faster something moves in time, the more stretched out the frequencies are. stretching it out by that same factor. OK? All right, so that was just a brief review of what we covered last time. And here's what we're going to cover today in a little more detail. These are functions where you have a function. You take the Fourier transform of it. You get a different function. OK, so there are pairs of functions that are essentially for transforms of each other. A square wave like this has a Fourier transforms that's this funny function a set of peaks. If you take the four transform of that, you would get this square wave. Shannon-Nyquist theorem: Any signal that has discrete components and frequencies is periodic in time. As you make the width in time narrower, the bandwidth in frequency gets bigger. Wiener-Khinchin theorem: Time is sampled discretely at regular time intervals. The full width at half max of that peak is 12 hertz. The Fourier width of this in time is just 1 over the width of [AUDIO OUT] So you have to take the full width. transform of a Gaussian is just a Gaussia. If I make that Gaussian pulses in time narrower, then the Gaussian in frequency gets wider. And inversely, if I make the pulse in time wider, than the Gaussia in frequency space gets narrower. So this concept of time bandwidth product in the physical world is what gives us the Heisenberg uncertainty principle. The power spectrum of a signal is just the Fourier transform of the autocorrelation of the signal. could calculate the Fourier transform of that. And that's capital Y of omega. And then we have some other function, x of t, And its Fourier transforms, X of omega, and another function g of tau. So remember, we can write down the convolution of this time series, x with this kernel g as follows. So y of t equals this integral d tau g of Tau X of t minus tau, integrating over all tau -- that's a convolution. topic, let's talk about Gaussian noise. The Fourier transform of noise and the power spectrum of noise. And we're going to eventually bring all these things back together. OK? All right, so what isGaussian noise? So first of all, Gaussian Noise is a signal in which the value at each time is randomly sampled from a Gaussian distribution. So here's what that sounds like. Sounds noisy, right? OK, I just wanted to show you what the autocorrelation function of this looks like, which I think we saw before. Using these methods, you can pull tiny signals out of noise at a very bad signal to noise ratio, where the signal is really buried in the noise. So it's a very powerful method. And we're going to spend more time talking about how to do that properly. All right, so let me spend a little bit more time talk about the power spectrum of noise, so that we have a better sense of what that looks like. So remember, I told you if you take a sample of noise like this and you estimate the spectrum of it, you compute thePower spectrum of one sample of Noise. problem with that? Why might that be a bad idea? Yeah. But there's sort of a general principle that we just learned that you can apply to this problem. You can take a signal like this, Fourier transform it, multiply it by a square window to suppress high frequencies. What is that equivalent to? What would be the corresponding temporal kernel that that would correspond to? It would be convulsing your function with a sinc function. It turns out that's-- the reason you wouldn't normally do that is that it mixes the signal across all time. spectrum of that we can correctly read out from the power spectrum how much variance there is per unit frequency in the signal. All right, let's talk about filtering in the frequency domain. So we're going to talk about how to smooth things in frequency domain with kernels. What would a high-pass filter look like? So would it pass high frequencies suppress low frequencies? You've probably heard of it, but what would a band filter like? It would just be big in the middle and then go to 0 at higher frequencies. that the signal is periodic in time. Discretely sampled in time means that the Fourier transform is periodic. There's another copy of that spectrum sitting up here at 1 over the sampling rate. The sampling rate needs to be greater than twice the bandwidth of the signal. If you sample a signal at too low a sampling rate, you see that it has a different part of the spectrum contaminating the top of your Fourier transforms. That overlap is called aliasing. And now, if I do this zero-padding trick, I can perfectly reconstruct the signal I'm sampling at every time point. of zeros between and make it a longer vector. And then when we inverse Fourier transform this, you can see that you have a longer array. So you can essentially increase the sampling rate of your signal after the fact. Pretty cool, right? Again, it requires that you've sampled at twice the bandwidth of the original signal. Yes. And that kind of kind of thing. It's pretty cool. And it's very useful in a lot of applications. And you can do that from nearly all applications.

ROUGE-1: 22.84, ROUGE-2: 21.78, ROUGE-L: 20.13
BERTScore: 70.42

==============================================
==================== [59/100] ====================
Summary:
The science of classical mechanics establishes an important principle of cause and effect. Newton's Laws of Motion established the scientific principle of analyzing observed phenomenon through the use of clearly articulated mathematical models rather than through intuition. Developing a command of mechanics is a powerful tool for understanding the world around us. 8.01 assumes a strong background in high school level physics and mathematics, so a previous course in calculus is not a prerequisite. It is a rigorous and technically challenging course aimed at MIT undergraduates. In each of these lessons, you will find a series of short lightboard videos that will help you understand concepts.

ROUGE-1: 49.32, ROUGE-2: 46.33, ROUGE-L: 44.75
BERTScore: 74.77

==============================================
==================== [60/100] ====================
Summary:
In this video, we'll look at an application of the probabilistic method to graph theory. An independent set in a graph is a subset of vertices with no two adjacent. Caro-Wei's theorem says that every graph G contains a large independent set of size at least the following quantity: summing over all v among vertices G, 1 over the degree of v plus 1. We'll see an application and some ways to interpret this result in graph theory as well as the corollary of Turan's theorem.

ROUGE-1: 12.56, ROUGE-2: 11.61, ROUGE-L: 12.15
BERTScore: 67.12

==============================================
==================== [61/100] ====================
Summary:
Professor: "The games that we'll end up playing today are games from last year" "I want you to keep in mind that you're not constrained to building games that look exactly like those" "It's possible that in the entire game, maybe you don't get that many meaningful decisions. That doesn't necessarily make it not a game" "For kids or adults, making it fun and doing something that makes it fun is more important than making it meaningful" "If you look at the copyright on the cards, it actually still talks about the psychology of winning" Lecturer: "Who's the most important person when it comes to actually playing figure it out on their own" "There's a long term consequences, so a lot of immediate consequences" "If you expect players to pick it up and play it three times in their life and then move on, then it probably won't help them that much" "You have to be careful that it doesn't have too much complexity or some sort of unanticipated change that there's too steep of a learning curve to actually enjoy it" Norman Brathwaite says games are a series of interesting decisions, not necessarily meaningful but interesting-- the decisions. A decision in a game can be meaningful-- what does it mean? What does it means to be meaningful? Couple hands? When you make that decision, the game's status changes or code that meanders the change. You have to react on the fly. You can't just plan out all of your moves. This-- reactive play. If they can see the game state changing and know that they're effective but don't know how they're affecting it, that would frustrate you. In games like Yahtzee, the die roll is like something not Monopoly. It's not really a decision. You have to do it every turn. If you don't let the player know what changed the game state, even though the gameState might have changed, is it that meaningful a decision anymore? You did something. Some numbers changed inside the system. But you don’t actually know what happened. It just means that these things aren't games. And-- AUDIENCE: It's seems okay actually from three identical dice. I believe Candy Land was invented to keep kids from getting polio from each other. That might be urban legend. With polio, you can't do much of anything. There's a huge inversion from the get out and get some exercise. It's a serious game. It has health benefits. Let's play in the land of Candy. I would like to go back a little bit to this idea about changing the game state, right? You make a decision in a game, and you've changed a game state. There are many games where you can do very poorly or very well. And it takes a while to understand exactly why you're doing very poorly. In a digital game, usually people have to take their turns and have to play out more slowly. Charades and Pictionary-- usually your teammates are the ones who are guessing. In Scotland Yard, the player positions is almost never known, so you get some feedback about what he's doing, sort of what he'm doing. There is a computer battleship. It's not as fun. often enough, you will always be successful. And that's an interesting strategy that game designers use. In a digital game, save some random number seed at the time when the save is made so that outcome's always the same. It makes a lot of sense to people who are game designers or computer scientists but may not make a lotof sense to a lot-of-players. So this brings me to the second reading, which is Don Norman's first chapter in the design of everyday things. Mapping, instead of the actual operations of the system, actually has to do it for what you can see. So there are affordances and there are constraints. I think affordances is introduced in this reading. What's an example of an affordance? What does a handle on a door allow you to do? What are the affordances of cards? I'm sure it's already in the syllabus. How many people are playing Carcassonne? We should be able to get a chance to play this game this semester. the Edison plug. It's actually-- the design with the ground on the bottom is a bad idea because if something starts falling out that are too exposed to it, it will not be the ground one. So it would be better to flip it. British plugs, actually, have the pin usually on top and-- AUDIENCE: The British just have the eyes [INAUDIBLE] pin. There's a little bit smaller than it needs to be in order for you to hold it comfortably. Pit is one of those games where exchange is a real time thing. It's easy to have a deck and then draw from it. You don't know what tile you're going to draw. The title of the game suggests things. It isn't like a commodity trading game. On the side, it says, "Corner the Market." And it's got a bell. This is like, this game back in the time when stock market are run with bells. I'd love to see Pit done on some sort of updated 21st century thing. might make it appropriate. The rounded corners actually make it much easier for you to do things like this. If it wasn't around the corner, it's actually pretty uncomfortable to do a fan. It's really, really hard to do it consistently when using your hands, by the way. So it could take away the whole information hiding things. But oh, the one that was badly punched, that's the joker. Let's see what else I talk about? So mapping, so back to the idea of mapping. fact base, I guess. They look like little people. There's probably enough in that for everyone to grab one or two. And then you can just take a look. I want them all back, but you can take aLook at them. Whoops, and a bunch of tiles that I will also hand out. I'll hand out half to that table. AUDIENCE: Something you can look at. [INAUDIBLE]. Terrain? PROFESSOR: Terrain, all right, something to do with land. What else? that are low when you're feeling depressed. And you can't pick yourself off the ground, making you sad or bad. A lot of these metaphors are actually arbitrary. But that means that these might be things that you can play off. And we're going to do a little bit more detail. I'll give you a couple of more examples in about two weeks when we revisit the idea of user design. He talks about things like single control, single function where if you've got something that does something, you might not want to make it do yet another thing on top of it because that starts to get really, really confusing. is this game plays. When it comes to Carcassonne, there is actually a deep, deep problem with this game despite how popular it is. And that scoring is actually pretty difficult to do. It's a math intensive problem. It does largely map on to how many of the meeples that you have of your own color on large patches of things. But how much those things are worth, those patches of thing is worth, requires a lot of counting, a much of counting. That's why you need a scoring track. Pit feedback is the feedback that you get when you hit the bell, right? There's a huge ding sound. And there's a couple of board games where you have to put pieces together, and that gives you an idea of maybe those two pieces don't go together. In Catan, you can see if somebody has massive roads or a lot of settlements or cities. In Monopoly, if you look at the board, whoever has the most houses can usually tell that they're doing pretty well. Designer explains why it can take five or six times to get a product right. If it's not good by the second time, people just won't buy it. Market forces push you to add things, to do additive design in order to distinguish yourself from the competition. And that naturally leads to complexity in interface, he says. He says design starts off as being very clunky. But it can eventually become something that works well, communicates well, or something that people can learn and maybe even enjoy. what he's talking about only, I doubt that actually imagined that this was something to be possible at the time when he wrote it. That was 1980. And cell phones obviously have gone through a lot of criticism. But it is it has been successful through a number of different reasons. Don't discount marketing as being something that the does sell. But what I want you to think about now is actually the process of prototyping. It will probably make more sense once I've actually got the prototyping materials out.

ROUGE-1: 26.27, ROUGE-2: 24.56, ROUGE-L: 21.08
BERTScore: 59.99

==============================================
==================== [62/100] ====================
Summary:
The two-level problem is one that's exactly solved. It's one of our favorite exactly solved problems, although it doesn't seem to have any physical relevance. So we can take a two by two Hamiltonian and exactly diagonalize it. And that's done using a unitary-- or actually, in the case that we looked at, orthogonal-- transformation. And so we're going to talk about that for a two-mode molecule. And we've got to do some work on this perturbation theory in H12-- and we weren't even going to do that here. Learn how to use the t matrices, or the t dagger matrix. These things enable you to solve basically any problem in time independent quantum mechanics. Non-degenerate perturbation theory is really powerful because it's also incredibly ugly. This is a whole new way of looking at spectroscopy, which is about understanding how things things transition and how things are affected by quantum fluctuations in the environment. It's a way of taking the totality of observations that you're going to make and saying, yes, I have looked inside this molecule. And I've determined everything that I'm allowed to determine. Perturbation theory is a tool that you can use to solve any problem involving molecules. We're not allowed to determine the wave function by any experiment. But we are able to observe the energy levels and properties of the Hamiltonian. And today's lecture is mostly going to be on the interactions between normal modes of a polyatomic molecule. It's a small number of energy levels that we're sampling in an experiment. And it tells you how to get rid of the wave functions that is just contaminates the other stuff. put parentheses around things. So this is now what we were calling h tilde, and this is c tilde. This is now an equation that says, OK, we can transform the Hamiltonian into diagonal form-- E1, En, zeros. For any eigenvalue, we have an eigen vector. Now what we'd really like to know is, well, how do we get these eigenvectors from the unitary transformation that diagonalizes H. We don't calculate this unitary-- yes? to know how it's evolving. But often that initial state is an eigenstate of one of the exactly solved problems. And so you want to be able to re-express that in terms of the eigenstates of the real problem. So here we have non-degenerate perturbation theory. And it is a mind numbing, formal derivation. So we start out with this rotary equation. And we say, well, let us expand the Hamiltonian. And let's put a little thing here. first-order correction to the energy. And we can continue. The algebra isn't beautiful, but we end up getting the following equations. We have E n 1 is H 1 nm. And then we get the second-order corrections to theenergy, which is m not equal to n h n m 1 H m n 1 over En 0. That's all we need. Now it does say non-degenerate perturbation theory. And so it's subject to the requirement that H1 n m over E n minus E m. than quadratic terms. The quartic terms are 100 times smaller than the cubic terms. And so you don't need to go much further. So these are the couplings between modes 1 and 2 that are cubic. And then we have 1/4 k 112 Q 1 squared Q 2 squared. There's also a Q1 Q2 cubed, and those terms usually are not important because they mostly are dealt with under here. And you could have Q1 cubed. But we already deal with that in the single-mode problem. So we won't worry about that. discovered and understood by Fermi. And it has to do with CO2. Omega 1 in CO2 is approximately twice omega 2. The symmetric stretch and the bend are in FermI resonance. So what happens then? Suppose we have a level that involves V1, V2, V3. And nearby there is a level V1 minus 1 V2 plus 2 V3, which are nearly degenerate. So the levels repel because they're interacting and they're out of position. names that any educated physical chemist will know to say, oh, that's the Bixon-Jortner. And they're still alive. They're still doing beautiful stuff. But anyway, that is all I want to say today. I will do details on one mode and Morse oscillator in other sorts of things next time. Back to Mail Online home. back to the page you came from. Back To the pageyou came from, Back to thepage you came From. Back into the page.

ROUGE-1: 24.11, ROUGE-2: 21.99, ROUGE-L: 19.92
BERTScore: 66.19

==============================================
==================== [63/100] ====================
Summary:
If certain amount of output can be produced by given amount of inputs, then the same combination of input puts can also be used to produce less. The key word is feasibility technology represents the feasibilities that the combination of inputs and outputs, that can be achieved in this world even the current level of technology fine. The second is no free lunch. At least 1 input is required to produce some output, or more than you know more than 1 kind of output. Third is non reversibility, what it means is that a production process cannot be reversed. A production plan is a combination of input and output that is feasible, not just this y does not represent just amount of output. If it exhibits convexity then t y bar and 1 minus t ybar dash is also feasible. If we have reversibility in the production process, then it would violate the laws of thermodynamics. So, in that sense all the production and processes are irreversible. We are going to talk about additivity that we talked about earlier and divisibility fine. say that there is such definition it requires either 2 man hour, and this is also not a good example, let me think of something else. To produce 1 kg of rice ok, you need either let us say 100 grams of fertilizer. So, 2 and 25 liters of water of course, we need land, but that is fixed those are fixed, we are not talking about it only these 2 are variables. So Student: Should not, has be 2 like 1kg of rice gives 1 and 2. comma 2 1 minus t 2 comma 1 will give us again 1 unit of output. So, what we are saying basically that ok, we start with y bar and y bar dash then if these 2 are feasible, then t y bar plus t ybar dash is also feasible here. The combination is all given here and this exhibits convexity. This is taking care of not only inputs but also output, y is taken care of here. It is possible to combine the two together.

ROUGE-1: 21.75, ROUGE-2: 20.52, ROUGE-L: 17.97
BERTScore: 66.47

==============================================
==================== [64/100] ====================
Summary:
The Great Wall began as multiple walls of rammed earth built by individual feudal states during the Chunqiu period to protect against nomadic raiders north of China. Under the Han Dynasty, the wall grew longer still, reaching 3700 miles, and spanning from Dunhuang to the Bohai Sea. After the Ming dynasty gained control in 1368, they began to refortify and further consolidate the wall using bricks and stones from local kilns. The wall was formidable but not invincible. Both Genghis and his son Khublai Khan managed to surmount the wall during the Mongol invasion of the 13th Century. main body and expanding this remarkable monument to human achievement. Main body is made up of three parts: the head, the torso, and the legs. The main body of the main body is the most important part of the body. The second part is the lower body, which includes the legs, the arms, the legs and the feet. The third part is made of the lower torso, which is the largest body of its kind. The last part is called the lower legs, which are made of marble and marble.

ROUGE-1: 36.14, ROUGE-2: 27.54, ROUGE-L: 26.49
BERTScore: 66.09

==============================================
==================== [65/100] ====================
Summary:
Peter Solovits was invited to speak at a meeting of the National Academy of Science, Engineering, and Medicine. He was on a panel that discussed AI and decision making, privacy and informed consent in an era of big data, science curricula for law schools, emerging issues, and science, technology, and law. SolovITS: One of the things that I was very surprised by is somebody raised the question of shouldn't Tatel hire people like you guys to be clerks in his court? interesting to me. He said, no, he wouldn't want people like that, which kind of shocked me. And so we quizzed him a little bit on why, and he said, well, because he views the role of the judge not to be an expert but to be a judge. So by formalizing it, you might win. However, conversely, the use of technology to determine whose liberty is deprived and on what terms raises significant concerns about transparency and interpretability. example, if you are two identical people but one of you happens to be white, the chances of you getting bail are much lower if you're black. If historically, judges have been less likely to grant bail to an African-American than to a Caucasian-American, then the algorithm will learn that that's the right thing to do. And then the second problem, which I consider to be really horrendous, is that the algorithms are developed privately by private companies which will not tell you what their algorithm is. Peter Zolovich: What do you mean by fairness? What characteristics would you like to have an algorithm have that judges you for some particular purpose? He says it's impossible to pin down one specific definition, but for the pre-trial success rate for example, I think having the error rates be similar across populations is a good start. Zolovic: There are three criteria that appear in the literature. One of them is the notion of independence of the scoring function from sensitive attributes. In the ICU population, men have more substance abuse problems, women have more depression. In the psychiatric population, white patients have more topics enriched for anxiety and chronic pain, whereas black, Hispanic, and Asian patients have higher topic enrichment for psychosis. In psychiatry, when you look at the models that we're building, there is a racial bias in the data that we have in building the models. These models are particularly simple in psychiatry, particularly when youLook at the huge gap in the public insurance patients with a huge difference in anxiety and depression. is that if you count O as the optimal possible model over all possible model families, and you count L as the best model that's learnable by a particular learning mechanism, then the bias is essentially O minus L. The variance is like L minus A, it's the error that's due to the particular way in which you learned things. You can estimate the significance of differences between different models by just permuting the data, randomizing, essentially, the relationships in the data. If yours lies outside the 95% confidence interval, then you have a P equal 0.05 result that this model is not random. Until 1967, it was illegal for an African-American and a white to marry each other in Virginia. If you went to get a marriage license, you were denied, and if you got married out of state and came back, you could be arrested. This happened much later. Trevor Noah, if you know him from The Daily Show, wrote a book called Born a Crime, I think, and his father is white Swiss guy and his mother is a South African black. He had to pretend to be-- his mother was his caretaker rather than his mother in order to be able to go out in public. off the 50th floor of a building that's under construction, and that's probably a reasonable defense. Now, how do you demonstrate disparate impact? Well, the court has decided that you need to be able to show about a 20% difference in order to call something disparate impact. So the question, of course, is can we change our hiring policies or whatever policies we're using to achieve the same goals, but with less of a disparity in the impact? That's the challenge. attribute given the outcome. So that says, can we build a fair scoring function that separates the outcome from the protected attribute? And usually, there are knobs in these learning algorithms, and depending on how you turn the knob, you can affect whether you're going to get a better classifier that's more discriminatory or a worse classifier. So you can do that in pre-processing. And so in this story, the optimal score is basically going to depend on whether you have a computer science degree or not. of scoring them than we do of scoring group B. So you might wind up with a situation where you wind up hiring the same number of people, the same ratio of people in both groups. Or alternatively, it could be caused by malice also. There's also a technical problem, which is it's possible that the category, the group is a perfect predictor of the outcome, in which case, of course, it's a bad idea. The outcomes are likely to be better for a group A than for group B, which means that you're developing more data for the future. they can't be separated. Now, how do you achieve independence? Well, there are a number of different techniques. One of them is to do a Gann-like method where you say, I want to train my classifier, let's say, not only to work well on getting the right answer, but to work as poorly as possible on identifying which data set my example came from. So this is the same sort of idea. It's a representation learning idea. And then you build your predictor, R, based on this representation, which is perhaps not perfectly independent of but it's exactly the right drug for you. that's another popular way of looking at this. So for example, if the scoring function is a probability, or the set of all instances assigned the score R has an R fraction of positive instances among them. If it turns out that R is not well-calibrated, you can hack it and you can make it well-Calibrated by putting it through a logistic function that will then approximate the appropriately calibrated score. These guys in the tutorial also point out that some data sets actually lead to good calibration without even trying very hard. A new study suggests that gender influences whether you're a programmer or not. It turns out that visiting Pinterest is slightly more common among women than men. And then visiting GitHub is much more commonamong programmers than among non-programmers. So what they say is, if you want an optimal predictor of whether somebody's going to get hired, it should actually take both Pinterest visits and GitHub visits into account, but because those go back to gender, which is an unusable attribute, they don't like this model. the database, but the type of insurance you have correlates pretty well with whether you're rich or poor. So we did that, and then we looked at the notes. We wanted to see not the coded data, but whether the things that nurses and doctors said about you as you were in the hospital were predictive of readmission, of 30-day readmission. So these are some of the topics. And so we said, what happens when you look at the different topics, how often does it happen? The eICU data set we've mentioned, it's a larger, but less detailed data set, also of intensive care patients. And there, we see, again, a separation of mechanical ventilation duration roughly comparable to what we saw in the MIMIC data set. On the other hand, if you look at the use of vasopressors, blacks versus whites, at the P equal 0.12 level, you say, well, there's a little bit of evidence, but not strong enough to reach any conclusions. standard or remote supervision notion of a larger population that has a tendency to be mistrustful according to our model. And so if you look at chart events in MIMIC, for example, you discover that associated with those cases of obvious mistrust are features like the person was in restraints. If a person is in pain, that correlated with these mistrust measures as well. And conversely, if you saw that somebody had their hair washed or that there was a discussion of their status and comfort, then they were probably less likely to betrustful of the system.

ROUGE-1: 25.70, ROUGE-2: 24.19, ROUGE-L: 22.69
BERTScore: 59.25

==============================================
==================== [66/100] ====================
Summary:
Professor: Today we're going to talk about partial differential equations. Professor Swan is going to do a review on Friday. Next time I'll be lecturing to you will be a week from Friday. The homework following the quiz will be posted and so you guys can get started on that. For those of you who had trouble with COMSOL, might take some extra time, since it's the first time you're doing it, so I would suggest you at least give it a try early. When you have three or more dimensions in your PDE, you have, like, electrochemical shocks in some of his systems. You have to use special mathematics to correctly handle that. In the Navier-Stokes world, I'll show you what people do to try to deal with that. There's many other problems. If you get into those things, you should take the PDE class for hyperbolic equations. They'll tell you just all about the solvers for those kinds of things and special tricks. in practice, there's a big problem if the number of unknowns gets to be so large. That's the main problem. 10,000 unknowns are a lot harder to solve for than 100 unknowns. In fact, it's even going to have trouble with a 100 millionunknowns. And so, how would I solve this normally, is I would take the Jacobian of. and I'd take an initial guess and do this out. You have to provide some initial. guess. This is actually pretty hard if you have 100 million. The best solver in the world discretizes in space and then does ODE IVP using explicit method for marching it. So they'll have like 100 million mesh points and then 100 species. So that's a pretty big set of variables and you need to know all these numbers at each point. And they use, like, 40% of one of the national supercomputer centers will be running at one time for one job like this. But it works. You never have to store anything that's too big. system, you're looking up special tricks. How do people in this field deal with this particular PDE? Special ways that are good for that kind of PDE. All kinds of problems have this kind of thing. You can have just regular chemical reactors, like in multiple steady states, little tiny fluctuations can make it jump from one steady state to another one. A lot of these things, how you do a lot of combustion problems is that things work out. But in some situations, this is true. The methods that we're going to talk about mostly are ones for solving parabolic and elliptic problems. Do you problem you're trying to solve, for example, might be a steady state problem like this. What's happening downwind is very much affected by what happened upwind, but not vice versa. And if you think that your time dependent problem really converges to the steadyState problem, then you could put any random initial guess in and march it along in time enough, and eventually, it should end up at the solution. Time and space are so related by the velocity of the flow. As you slow the velocity down, then the diffusion will start to fight the velocity more and more. We'll have similar kinds of phenomenon then. It's almost like a parabolic time system that, if the flow is fast enough, the fusion backup, anything moving upstream is negligible. And so then, you'll have the same kind of issues. In fact, you may even try to solve it by just starting it at the upstream end and competing stuff and then propagating what that output from that first one does. There's a thing called local Peclet number, which is-- OK? If you make the local PEClet number too large, actually anywhere bigger than 2, and you try to solve this equation, what you get is oscillations, unphysical oscillations. If you take your delta z too big, then it's a terrible approximation to do this. So I would do better to change from this center difference formula to what's called the upwind difference formula. Just use that instead of using this formula. Now, you wouldn't think that that would make much difference, but it makes a gigantic difference. So if you look at the solution, the analytical solution of this problem, it's like this. Where this has a very, very thin layer at the end. It's like the flow has pushed all your stuff to one side. And if you choose your delta z to be from here to here, that's how big yourDelta z is, and then you're computing the derivative of this point halfway along from here. You compute your derivative like that. It is not a very good, not avery accurate representation of what the real derivative is here. before. In that case, there was a region of the boundary here that had some concentration c naught, and then there was, over here, the concentration was zero. Here, it's some number, 17, whatever number it was. And if you think of how to model what's going on, one way to look at it is, I have a diffusive flux coming in from the drug patch, diffusing the drug into the flow. And the flow gets slower as I get closer to the wall because of the friction with the wall. Normally you would do this with Gaussian elimination and huge steps. It would solve it perfectly, right? Right, backslash. You guys have used it a few times. It's really nice. But you're going to give up that because you can't afford it because the memory, it's consuming too much memory and your cheapo department didn't buy enough RAM in your computer for you. So you're instead trying a direct method which is, so it's trading off CPU time versus RAM. And because it only has to do a forward multiplication, you don't have to actually store j. time accuracy. In those cases, you might instead want to use-- So this is time accurate. There's another method. I don't know if I should call it time inaccurate. If you don't care, if the solution is really, what the solution yft is, all you care about is where you get to, then you can do other methods. One of them is the backward Euler. This is what's actually used a lot. However, this is now an implicit equation. So we may have to solve it with some method like Newton-Raphson.

ROUGE-1: 23.50, ROUGE-2: 22.54, ROUGE-L: 18.88
BERTScore: 64.01

==============================================
==================== [67/100] ====================
Summary:
elizabeth the first is considered one of the greatest rulers in england's history if not the greatest for women. She never married can you blame her i mean look at what she grew up with around her dad and all of those moms and stepmoms she never had any kids which that causes a problem come her end time. She eventually does set on a cousin up in scotland which is the country on the same island of england but right above it uh james king james ofscotland she names him as the heir. and his son and and we'll talk more about that when we get to uh the next unit and so on the king james bible page 414 415. The bible was the bible commissioned in english for the english speakers and english readers this was the first time. Without that if you didn't know how to read latin or french you were stuck by solely getting your material this is what the bible says solely from your clergy solely from the church. Now that something is in english and if you can read english you are able to read it over and over get your own theories.

ROUGE-1: 46.67, ROUGE-2: 44.70, ROUGE-L: 46.44
BERTScore: 74.88

==============================================
==================== [68/100] ====================
Summary:
The average velocity depends on the time interval t to t plus delta t while the person has displaced a certain amount of vector delta r. So, as a vector, we have delta x over delta t i hat. This component here is what we call the component of the average velocity. And, again as before, this component can be positive, zero, or negative depending on the sine of delta x. And now what we want to do is consider what happens in the limit as delta t becomes smaller and smaller. And that will enable us to introduce our concept of instantaneous velocity.

ROUGE-1: 55.17, ROUGE-2: 53.33, ROUGE-L: 55.17
BERTScore: 82.08

==============================================
==================== [69/100] ====================
Summary:
Last week we looked at the we first saw our first magic formula which you've probably forgotten because it was a wonderful weekend so I'll remind you. Then from there we will uh resolve the spider problem and then uh we're going to look at uh something called the super magic formula. The magic formula is simply a compaction of a lot of math that's it okay so I'm this time d by DT of u. A1 Plus v. A2 plus L dob1 plus Theta do L B2 that's why we Tak. to solve pretty much any kinematics problem because it's based purely on Geometry angular velocity is a kind of a madeup concept that just simplifies the math and it's kind of intuitive stuff is rotating Theta Dot multiplied by you know the Le lever arm right gives you a velocity. If a frame's rotating some point different points will have different velocities so yeah a frame can have a velocity but doesn't really mean much and you tell me which point in the frame you're referring to right look this thing is a frame what's the velocity at any given point in time of the frame. The simple thing is angles don't add up but infinite decimal angles do add up. If I rotate about one axis a certain amount 30° and then rotate about another axis 30° right if I reverse the sequence they don'tadd up. But if I rotate one axis like a tenth of a degree and then rotated about anotheraxis a 10° then the sequence actually comes pretty close all right. Infini dismal rotations do and because angle of velocity is D Theta by DT right and it's a vector if because it is about some direction and there's a d Theta. Kinematics is called the can's method of kinematics Kane K an is a professor at Stanford he I think he's still there retired. Kane K's method is used to analyze the Dynamics of robots complex robots and Space Systems so anyone who works in complex Aero Astro space system things like space shuttles with robots moving around Etc they use this method they use the symbology a b and all that right very few undergraduate courses on the planet uh learn teach the mechanisms that you've learned and understood right. we'll do let's prove it let's consider a frame a by the way you notice I will always show frames with this little circle a saying that it's a frame and with a little wiggly line pointing to it. Let's assume that you have a vector written in terms of frame B's unit basis vectors let's let's assume n is equal to sum U1 which is not a constant time B1 + U2 * B2 now what I'm trying to prove is this formula how would I go about doing it. here so will you please expand it for me what does that come out to be this is the left hand side of that formula right and we'll do the right hand side separately and show that the two are the same. I will never ask you to prove something like this in an exam right this is almost like a solved example I want you to understand that it all works out that's it. If you're confused that is fantastic because that's a symptom of something else and I'd like to surface it and help you figure it out. The room the world the Earth is a frame the basketball is another frame the kind of the U you know you guys need to be like Neo in Matrix have you seen that movie right it's very very important if you understand Matrix algebra you need to see Matrix all right so go see Matrix and you know how Neil kind of you know he when he's totally you know after Lawrence fishburns you know kind of trained him Etc he goes down and he can see he stops seeing buildings and he sees these characters you know flowing instead of the building. some other point right get it so look if I do this if I'm spinning it you know the center is staying stationary so that is the instantaneous Center of rotation but this Frame there's no kind of special point because with respect to someone who's flying overhead with a constant velocity the instantaneous cental rotation is going to be some other point the frame has an angular of velocity it's not an angular velocity about a point get it. There might be a point which is instantaneously not in motion and that's the instantaneous Central rotation but that's just a special thing. look this is just in formal okay I'll formalize this ston right uh actually that's right we're using four thank you we're use four. We need to capture the location of the spider but we also need to figure out where the frisbee is so that we can do the math with respect to the Frisbee Etc right so we're capturing it in terms of these parameters I just want to say this leave it for the time being we'll come back and I'll you know in the context of Dynamics it'll become more clear when we talk about degrees of freedom and parameters and things like that. can kind of do it that way but the simple way to do it is to simp well you know it doesn't matter we can just write this as B take the whole term d by DT of lb1 plus what quick hm I'm hearing a hesitant murmur not the yeah I was mumbling something a moment ago I'll tell you what I was trying to say uh cross lb1 what is a Omega B so we can rewrite a Omega b as Theta dot B3 and so this whole thing comes down to I'm going to write the answer here to save a Blackboard. start with the um Med go round and then the other one right so this is it I'll tell you when to start uh which on this one right okay. If you as I said if one of you invents a new term we'll name it after you okay and so far I've used these words coris and all that I kind of called them out without telling you a lot more about them right so what we're going to do now is take a break in the following way I'm going. The Coriolis effect is a phenomenon that occurs when objects are deflected by the Earth's rotation. An observer on the rotating Earth the path of an object appears to be deflected. This exemplifies the Corola Force and what happens with things like hurricanes and we'll show you I'll show some videos later if we have time is so now you're you're on the North Pole right you're sitting in the North pole and this is the Earth this is North Pole okay you're looking down a particle when it's low pressure. Coriolis effect is when something is moving that is translating with respect to frame which is itself rotating then there is a deflection in the B2 Direction in the way we've written it now I'll give you the general formula and you know B2 what it is ETC it become more clear any questions about coris by the way in the last class I told you this anecdote I said that artilleries artillery guns right in the 1700s 1800s um they would have to compensate for directionality because of the coris effect it turns out that that is true but I made one mistake. it turns out that when the Coro effect was discovered or when someone formulated it coris right the range of guns wasn't high enough for that that to have been you know really valid so it is true that in artillery you need to correct for the coris effect but that wasn't the reason the corus Effect was discovered so I take it back right. In the in World War I when the British when a British Fleet near Faulkland ran into a German Fleet they spend they shot 1,000 shells uh taking the Coriolis effect of the northern hemisphere well they were in the Southern Hemisphere and missed. What is the force that the astronaut feels you know or what force must the astronaut exert in order to move in a straight line with respect to a spaceship or a space shuttle or something? That's the question we seek to answer. What we're going to do is basically generalize the Frisbee problem. We will Define as usual and you'll find this all repetitive because really it's all the same stuff over and over again just getting more and more compacted with more insights kind of teased out of it. yeah sorry and plus this thing okay we'll end now because Professor Socrates needs to take over this is it this is correct acceleration of P acceleration of Q with respect to B Oiler coris centripedal and you know why it doesn't go to zero because I'm not taking a Omega B cross a omeg B. I'm taking a omega B cross this whole term which is at 90° to a OmegaB got it got it? Yeah sorry and Plus this thingOkay we'llend now because professor Socrates need to takeover.

ROUGE-1: 27.88, ROUGE-2: 27.10, ROUGE-L: 25.94
BERTScore: 67.04

==============================================
==================== [70/100] ====================
Summary:
Al shalot is the CEO of GT Sports. He is also a member of the Olympic commission the international Olympic Comm. He has been involved in the conception and Advising the I on what's not public the Olympic sports games. He says it's a huge moment because we now know there is going to be an official OlympicsEsports event. He also says there's room for new events in addition to the current Olympic events. He believes there's a clear opening for mobas games like League of Legend. There might have been some people who were quite hard to persuade I did nothing the they they came to us with the intent with the the decision with the motivation to do something special for gaming and orts. It's part of the agenda strategic agenda 2020 plus 5 to connect with the audience the youth audience where they are and to leverage gaming and Esports to do that so it was strategically cited and we're just part of a discussion about how to implement it. For me it's a very satisfying solution to believe that now Esports will be at the same level as winter and summer olympics. The Asian Games officially added League of Legend and other games on their list. The local authorities in in Saudi are very passionate and very supportive for eorts. PC always brought the idea to bring a property and events to a country to create some bridges. There's been a lot of criticism about Saudi Arabia as a host. Some LGBT players uh women saying that they don't feel like they'd be welcome. Are you expecting to have those kind of conversations with some of the national Olympic committees and at the very least is that something that came up? Competition and as you can as you probably know poetry is no longer on the official program so I think uh it's totally possible that in the future uh finding the balance as always between popularity relevancy and a certain fit with the values that the Olympic Committee wants to project this kind of game will be futureed but it we we have we have advanced so fast in the last few years to come to there's no way Sports would be ever with Olympics to we're GNA have a dedicated competition. far we have 50% of the players globally that happens to be women when they play video games and then when you go to the pro stage and the high level of competition and World Cup and things like that less than 0.00001% happen to be wound so we failed them maybe because we we didn't do all job in terms of education and they have a lot of like like toxicity. Maybe because we don't give them enough chances at G2 we created teams we provide them the same support from a staff point of view. Olympic opening ceremony will be similar to the summer and winter games. There will be something you know with a flag ceremony probably at the beginning. I think it's going to make a huge difference of like touching emotionally people that have not been touched so far by easts. I'm French so I I will cheer for the the national team. I don't know the kind of details of what opening ceremony there's Going to be um but if it's like the summer games there'll be something like that. of League of Legend uh potentially with one of my players on representing my country uh playing against the Spanish team we have been dreaming of this like France against Spain with the best player of the World for years. I think you know to see some of the players that we selected when they were underage that we help become the best version of them themselves as athletes being selected to represent their country wherever the country. We've got lots of events in Esports already what makes this one different I think the main thing is this idea of national teams like it's in the DNA of Olympic games. "Everybody wants to see what a dennish team will do against Corin I I would love to see it too you know so um I've seen a lot of players making the the guess or what the team could look like so his aspiration to be part of is is clearly part of the discussion," he says. "You follow a bit like the conversation about but uh rocket league and OverWatch back in the days at the at the World Cup for for their game and and and League of Legend also often have this conversation about creating the Euro Cup or World Cup"

ROUGE-1: 34.63, ROUGE-2: 32.49, ROUGE-L: 29.98
BERTScore: 65.55

==============================================
==================== [71/100] ====================
Summary:
Norvin Richards: A word like "unlockable" can mean either it's possible to unlock it, or it is not possible to lock it. Richards: We can account for this kind of ambiguity in the following way. He says there are two "un-"s, which can go before the "un" that can go after the "-able" And so we get to attribute it to basically the fact that there are 2 "un-s" Richards: There's a suffix that changes verbs into adjectives, and then there's an "un"-s. "Un-" combines with adjectives and makes adjectives that mean more or less "not (adjective) " "Un-" is kind of picky about what it can combine with. Joseph: Does the final-- after you "un-" something, is that going to be able to be redone? So if I untie a shoelace by cutting it up, now-- NORVIN RICHARDS: Yeah. It can't be tied again. Well, let's see, does it have to be possible to do the operation again? I don't know, maybe.  syntax is the study of how words are assembled to make sentences, words, sometimes things smaller than words, as we'll see. We're going to see that it's useful to think of sentences as being put together in a bunch of operations more or less the way "unlockable" is-- that we take pairs of words and put them together to form larger objects the way I just did for "unLockable" Just as with "unlockedable," we were taking pairs of things and putting them together in pairs to create these larger and larger structures. Vince Richards: syntacticians have to care passionately about the difference between one sentence, which is complete gibberish, and another sentence. "Up the stairs" is a prepositional phrase, "up the cats" is not. "I will walk them up." What are you going to do with those stairs? You have to say I will walk up them. You can't say, 'I will wake up them.' You must say 'I'll wake them up.' So these "up"s are different. CNN's John Sutter gives three sentences to slay a hypothesis that you might be entertaining. The existence of the second class of sentences shows that that's hopeless, Sutter says. Sutter: What if it's not particular sentences that we're remembering, but structures of sentences we've built up over time? "Stop entertaining that hypothesis. Make it go home. It's not a good hypothesis. It won't do you any good," he says. "This is the theory that covers "cough, hack, splutter," "sudden heart attacks," "flies" In many languages, including Latin, you have to say, "About what are you talking?" in order to ask a question. In English, you always bring the question word to the beginning of the sentence. "To whom are youtalking?" sounds fancy and snobbish, but still right. We're not going to be talking about prescriptive studies of English grammar, we're just going to talk about how you do it. The only point of these few slides has been it's possible to study syntax independently of meaning, where by independently I just mean the facts of syntax don't just reduce to facts about meaning. Noam Chomsky: We're capable of distinguishing grammaticality, even in sentences that don't mean anything. "Colorless green ideas sleep furiously" is meaningless if you don't mess with the meanings of the sentences. If the words meant something else, the sentence would be fine. We can have English sentences that consist of two adjectives modifying a noun, and then there's a verb, but "Big green monsters snore loudly" that would be OK. "We're class going to" is a meaningless sentence, but the words are fitting together the way they should. really, really desperately trying to end the conversation, I bring out these kinds of pairs. So it's not about meaning. We have this intuition that there are sentences that are OK and sentence that are bad. I've just been asserting things about our feelings about these sentences. Do people have this feeling about these sentence? First, that they're meaningless, and second, that the first one is OK and the second one is bad? Yeah, Raquel? AUDIENCE: I can't remember the word for this, but you were saying that certain types of words are categories that you can add more words to. English speakers end sentences with prepositions every day. In English, there's a distinction between the examples where leaving a preposition behind is what you prefer. "About what are you talking?" is worse than "To whom are youtalking?" "Despite what did you leave?" and "What did you avoid?" are both OK. "We left despite her warnings" is a bad example, but there are other examples that are really quite bad, too, he says. He says we should be proud of the fact that we can say this, because, as he says, it's rare. grammar. So what we are doing in this class is trying to figure out what people actually say, what the rules are for putting sentences together in English. We're not going to talk about that stuff, except to mock it the way I did just now. So prescriptive grammar is the study of rules that your teachers might have taught you in school about how to speak, some of which, just to stop mocking it for a second, might have tried to improve the quality of your writing. Inhale a fly. So I say, "This is the--" and then I stop. And then imagine that this experience is so traumatizing for me and also for the fly, that I just I never complete that sentence. That's the sentence I uttered. And I'm a native speaker of English. So there are two kinds of things we could say. One would be to say, we're developing a theory of all of the kinds of sentences that Native English speakers can say. that says English sentences can be arbitrarily long. Nobody ever performs an infinitely long sentence, but we're competent to produce them. There is no question to which the answer is "The red book," like "What will you find?" "Find the red" is a substring that has certain privileges, can be used for these various types of phenomena, as opposed to "find the red," which you can't do those things with. "I will find the red I will book, leave the blue I will pencils." Can't do this with just any random three-word string. In a sentence like, "I will find the red book" for example, we'll see that syntax treats that string, "The red book," as a unit. It's OK to take a substring like that and put it together with another similar substring conjoined with the word "and" It is OK to use "thered book" as a possible answer to a question. This is like the stuff we were Because this is syntax and not semantics, we won't worry too much about what it means. say, "The red book?" But if I tell you that I will find the red book and you're amazed, you're not going to say, "Find the red?" I think. Do you think that's true? But yeah, I take your point about-- to the extent that you can use "the red" as shorthand for " the red one" It's red acting as the noun, not red acting-- NORVIN RICHARDS: Yeah. Several people have points about that point. this part is the verb, the part that determines that that's the kind of phrase that can go in that position. "Find the red book" we're going to give that the label verb, because having a verb is the important part for that. "In the garage" is a unit, it's a constituent. And again, if I say, "I will find the book in the garage," and you're amazed, you can say "in the garage?" If I want to, I can topicalize "inThe garage" It's a prepositional phrase. just contain a preposition, like, "I will look up," where, again, if I say "I would look up" and you're amazed, you can say, "Up?" (Why will you look up?) "I said I would lookup, and up I will look"-- Maybe. "I think you might be right that this is an adverb in the sense that it modifies the verb. I think I might also be right in thesense that it's a prepositional phrase that's being used as adverbially" don't want to think of "up the cats" as a unit that has the cats as an object. We want "up" to not be a preposition that's combining with the cats. "I will walk her up" is modifying "to her room," but I don't think [? so? ?] NORVIN RICHARDS: It doesn't have to, does it? No. I think you also have to walk the student up. You can't walk the stairs up. But for "walk up them" we want "up" and "them" to combine to be a propositional phrase. But for these other two, we want something else. We're going to want to circle around and try to find out what that other thing is. Does anybody else-- yeah? AUDIENCE: I was going to suggest something else [INAUDIBLE] Even though you can't say, walk the-- "walk the stairs up to your room." NORVIN RICHARDS: "I walk--" "I want to ruthlessly squelch everything else you guys want to say," he says. "I have a triumphant slide that I want to show you. And then maybe we can unsquelch you and come back" "We want to construct a structure for this that's sensitive to all of the tests for structure that we've been developing, and it's going to involve putting things together via pairwise merge" "When we merged "the book" or "the garage," we're going to create something we're Going to give the label "noun" to," he adds. construct syntactic trees for strings of words. It's often the case that we get ambiguities like the "unlockable" ambiguity. There is more than one way to combine things. And what we'll do is develop tests that allow us to see which way we've combined words in different ways. And we'll find cases where, depending on in what order you combine things, you get different meanings, and our tests will combine with that. All right. We will do this again on Tuesday.

ROUGE-1: 23.29, ROUGE-2: 21.86, ROUGE-L: 19.38
BERTScore: 64.19

==============================================
==================== [72/100] ====================
Summary:
William Wordsworth was a very famous individual along with the next person we talked about Samuel Samuel Taylor Coleridge probably the two of the most influential of this particular era. He defied the conventions of his time by insisting that poetry should be should express deep feelings about everyday experiences and that's important because if you can recall back to the introduction of this unit and the big ideas and such you know it's about that emotional feelings and that powerful feelings. He was one that really was pushing pushing and pushing for those to be in his writings and that why he stands out that's why you've such a visionary of the time and the building background on the next page gives a little bit more insight. This is probably one of the most difficult readings that we have throughout the semester I don't think it's impossible but you're going to have to you're gonna have to work a little bit ok so just kind of grab onto those moments those moments of nature and such but also as this is kind of like a monologue where he's going on all of a sudden we find out at the end that he's actually talking to somebody and based on his history can we guess who that might be and why this moment is interesting. different than the typical every line it's kind of its own little thing and so when it's read in a different way it's it might see a little Jill at times button jamming is something very easy to visually identify a once you understand oh that's in JAMA it's a piece of cake okay. It might give you a little bit of reservation here and there but it shouldn't okay what I want to talk about first is a 786 lines composed a few miles above Tintern Abbey on Abbey you know a religious building. he talks a lot I mean just describing you know leaning against the tree and looking out amongst the field on looking at the orchard with the unripe fruits are clad in one green hue. He almost has kind of a teleportation back to some degree of when he was a child okay he goes into great details about when I found through around the river like a like a row like a deer and I would play and when I was little I was having a great time but when you're a kid playing outside you really appreciate how beautiful it is. This is the place one of my earliest memories was I was like two or three and I went to Disneyland on California I remembered nothing throughout my life except I remember visually what it looked like. I remember standing there in the front of Disneyland in California and watch the train go across and was like this is the same spot I stood when I was three you know and so it's kind of a nice little Wow think about how I've changed in my life think aboutHow I've grown up things that I've experienced. "It's a lot more difficult than reading those pulpme to expect you to get out of this because this is one I probably say the top three or four most difficult pieces," he says. "It's not that it's crazy hard but it's just it's a much more difficult to read than it looks," he adds. "I don't know what you're going to do with your life after you get through this," he admits. "This is one of the most difficult things I've ever done"

ROUGE-1: 26.03, ROUGE-2: 24.71, ROUGE-L: 21.59
BERTScore: 65.31

==============================================
==================== [73/100] ====================
Summary:
Professor: We're approaching the end of our fundamental section, but we still have a few important lectures to get through. After we get through the fundamentals, we'll be in a good position to understand the different technologies and finally the cross-cutting themes. The goal is to, at least for system efficiency in blue is somewhere, depending on the plant, somewhere around 1%, maybe as high as 7% or 8%, depending on very specialized plants that are experts at converting sunlight into usable chemical energy. Solar cell conversion efficiency is the ratio of output to input energy. For most solar cells, this breaks down to the following progression, from the solar spectrum to charge collection. The efficiency of the solar cell can be broken down to this little pie chart up here. The total are a bunch of two-letter or three-letter acronyms with some subscripts here that we'll get to know and we'll become very familiar with over the next few lectures. We're going to be focusing on charge separation, incorporating elements of either side. The energy band diagram shows the chemical potential, also called the Fermi energy, throughout the entire device. To transfer this into what we've seen so far with the solar cell devices facing up toward the sun, you'd have to rotate this by 90 degrees. As the barrier height decreases, we have a decrease of the built-in electric field. And that's why we have this diffusion current dominating in the dark, and a current flowing from right to left. In the illuminated case, we'll have all of our carriers traveling from left to right. In a reverse bias condition, electrons will be able to tunnel through the p-type into the n-type right here. The drift current is larger because of the larger electric field. That will be under reverse bias, and that could present problems if the solar cell can't withstand the reverse bias. If you have a strong enough p-junction, you could have a solar cell that is not low enough, in other words, if the pjunction is not enough, the current at reverse bias begins dropping. dark, this is the only case in which we have carriers traveling from right to left. And it's happening because we're using that battery in the dark to change the chemical potential on either side, which, in effect, reduces this barrier height and allows carriers to diffuse from the n-type into p-type. So you can think about it as forcing carriers up the junction. And this is a very useful technique because in a real device, the current will travel through the weakest point of that pn-junction. Professor: What does forward and reverse bias mean when there's no battery? This is a very interesting question. So let's make an estimate of what we think should happen, and then I'll confer some notes. And then we'll measure what actually does happen under illumination. And we, through superposition, shift this entire curve down, we'd be operating somewhere in this quadrant right there, right? So this we call IV quadrant, typically I, II, III, IV, IVquadrant. This is where power is coming out of the solar cell device. illuminated case is the only case in which power is coming, usable power coming out of our solar cell that we can use. The illuminated IV curve is, to the first order, is just your dark IV curve with a superposition, which we call the illumination current, I sub L. Now we can take the product of the voltage and the current to determine the power, and we obtain a curve that looks very much like this blue curve right here that you can see. That is the maximum power point of the solar cell device. The light itself is biasing that particular solar cell device. But that solar cell might be connected in series with a bunch of other solar cells in a module. Other solar cells might be biased that one solar cell. The biasing is because you have a shift in the chemical potential of this side up relative to the n-type side. That's a bias. Whether that's generated by light, whether it's generated. by a battery, right? Whether it's the energy input to create that bias is coming from the sun. or if it's coming from an external battery, that's a matter of detail. The lecture will be focused on the broader general topics, but if somebody is interested in learning more, I'm happy to kind of dive into there. I have interacted with several of you. Joe has interacted with probably 3n, n being the number of people I've interacted with so far, and really tried to impart the wisdom of pn-junctions. Please, please, please come to us if still things are going over your head. You should be able to explain to your roommates exactly what is going on. There's no power flowing through that external circuit because there's no potential to be dropped across the external resistor. The opposite happens over here at this point called Voc, which we'll call open-circuit voltage. This is the point at which the solar cell is producing the maximum amount of power output. The voltage at the maximum power point is almost the Voc, in a good device. And the current at the Maximum Power Point is almost Jsc, but not quite, all right? and Vmp, which is the voltage at the maximum power point. So, so far, we've learned essentially four variables here. We have our Jsc, our Voc, and our Jmp. And our Vmp at that data point right there. So the two need to be matched to each other. And that's where some of the power electronics come into play. Yeah? AUDIENCE: So in the last problem set, where [INAUDIBLE], we assumed that output voltage would be [INAudIBLE] volts. corner of my box is going to end at the maximum power point. And that box looks like this blue one right here. The area of that box is Jmp times Vmp. And notice I have another box around here. I have this clear box that starts at the Voc point and the Jsc point. The bigger one has an area of Jsc times Voc. And I'm going to define a parameter called fill factor, which will be the ratio of these two areas. but something that you could buy from a supplier, you'd need five times that area. Whereas if you had a 10% efficiency module, which is more approaching the area of some relatively inexpensive solar cells, you would need 10 times that. So if you say, OK, I'm willing to pay more for a high-efficiency cell because I'm using less area, you can use this type of calculation to get to the answer quickly. This is a really back-of-the-envelope envelope engineering approach to estimating costs of a solar system.

ROUGE-1: 23.57, ROUGE-2: 22.70, ROUGE-L: 19.94
BERTScore: 64.81

==============================================
==================== [74/100] ====================
Summary:
The world woke up this morning to Global chaos massive Tech outages are impacting Airlines businesses offices thousands of flights grounded globally long cues frustrated passengers. Banking and health care has been affected including the NHS and some TV channels have been taken off air millions of people have been affected the problems are first reported in Australia before spreading across the world including here where there were delays and big cues at airports. The boss of the cyber security firm responsible has said it could be some time before all systems are back up and running. The firm behind the outage crowd strike has held up its hands but admitted that it'll take time for things to get back to normal. The chief executive today has said he's deeply sorry for what happened but as to why a routine software update caused so many problems the question the answer to saying well actually if we have lots of smaller companies doing this stuff then are we leaving ourselves more open to vulnerabilities to weaknesses to attack so it is a real dilemma I think but nobody has seen anything of this size. There's no appointments available and to go to see the chemist I can't get my sick note um updated and unfortunately I was about to be sanctioned by the um Social Security office only written prescriptions are available. We're working with colleagues across government to get things back up and working as quickly as possible hospitals say urgent and Emergency Care has not been affected and if people have got appointments booked they should come in as normal. There has been an impact on some routine day-to-day activities at NHS trusts unfortunately due to the global IT issues we're going to need to ccel.

ROUGE-1: 26.43, ROUGE-2: 25.67, ROUGE-L: 21.40
BERTScore: 63.54

==============================================
==================== [75/100] ====================
Summary:
In the year 2000, four-year college grads actually earned more with their entry jobs than they're earning today. Changing technology has made wages rise more at the top, but has held wages down for a lot of other jobs. A third set of factors has to do with slower economic growth, slower productivity growth and slower dynamism in the U.S. economy, says Tyler Cowen. He says there is less demand for new labor and that makes it harder for a new college graduate to get the job he or she wants.

ROUGE-1: 13.01, ROUGE-2: 11.60, ROUGE-L: 12.44
BERTScore: 58.40

==============================================
==================== [76/100] ====================
Summary:
So, the first we learned rationality, rationality axioms - completeness, reflexivity and transitivity. And when our when someone’s preference exhibit these three properties, what does it mean that he is able to completely rank all his choices all his potential choices, with also possibility, with possibility that there are more than one bundle at some ranks this is a possibility. One thing also I should add that he will be able to rank only if he has finite consumption set and also what we have learned that this will translate into a utility function. So, from here we learned that indifference curve has to be downward sloping.

ROUGE-1: 17.11, ROUGE-2: 16.81, ROUGE-L: 17.11
BERTScore: 72.10

==============================================
==================== [77/100] ====================
Summary:
If you have a complex polyhedron, we found general unfoldings. We proved one of them. If you want an edge unfolding, that's the centuries-old open problem. For non-convex polyhedra, we know this is too much to hope for. Even if it's topologically convex, there's not always an edge unfolds. But for general unfolding, we don't know. So today's lecture is actually mostly about these two open problems and different variations of it that we know how to solve. The claim is facet-paths always exist for any triangulated, connected surface. Once you have this facet- path, you're basically golden because you can lay it out without overlap. The hard part is really getting this path. It's a little bit nontrivial with obtuse triangles. They don't necessarily just lie along the horizontal line. You have to set things up so that those guys are on the boundary, but you can always rotate it so that that's possible. Theory works for any polyhedron, any manifold, any sort of locally, two-dimensional thing. Professor: "I would love to see an implementation of this algorithm" in a computer. If you wanted to unfold a band, obviously, a band can just unfold straight. It's like a nice, long strip. But just keep cutting edges until cutting an edge would cause it to disconnect. If it's inset, I'm cutting with every-- I maybe didn't mention that-- through every vertices, I'll slice with a y-plane. In the case of a triangulated polygon, we usually call them ears. So cut edges until you can't anymore, so until cutting would disconnect. So this means what you're left with will be sort of a tree of faces. There'll be no cycles because if there was a cycle, you could cut one of the edges, and it wouldn't fall apart. So obviously, there's a tree here. Now, trees have leaves. They have at least two leaves. In reality, we're thinking about a dual graph which looks something like this. Picture, or if you just have two maybe. Well, I guess-- Yeah, because then, these are both first-level ears. Or maybe just a Mickey Mouse because those are all-- Well, it probably works. But for these cases, I just need to check that I can find a facet-path. In this one, in the base cases, not always. Like this guy, hard to make a cycle. But if I find two ears or three ears, I can just draw that in. So imagine those guys as being done. I'm left with these four triangles, actually, a little boring because I don't get the Mickey Mouse case. it's a bunch of cycles. You could actually connect some of them together, like these two. But these cycles are not even attached to these cycles, so it's kind of a problem. That's step five is we're going to fix all the problems. We've probably seen this trick once or twice before, I think in the Mountain Valley assignment stuff. So for a triangulated cube, I can splay out those triangles, get a non-overlapping chain like this. Bands are like a stack of orthogonal polygons, a bunch of bands, basically. As long as I get the band to do this, y-faces can hang up and down. It's not going to intersect anybody. So 1 by 2 refinement is enough for orthostacks. With towers, I could have multiple towers here. I really only want one tower built slab by slab. These things we don't know how to grid unfold. That's an open problem, but if you refine just in z by a factor of 2, that's enough. Professor: If you visit faces multiple times and kind of weave around in a clever way, you can do it. You can do stuff, and then, at the end, you should still be facing up. In fact, I basically just zigzagged. This would also work if it was rotated 90 degrees. It's really powerful. It also works if t is on the other side of s. You could do sort of the mirror image traversal. Once you have these sort of paths that visit everything, you just fatten out. left-left-right-right. Then, you will preserve your orientation. You tell each of the children which way you're initially going, and they can deal with it. It's basically telling you whether s and t is like this or the other way around. So now, we end up way up there where the lavender edge is at t10. Now what? We want to come back here, and I'm not allowed to sort of intersect myself. That would be the paper going into two parts of this unfolding. But I have all this space, so natural thing is to just wander from there back down to here, using up the space. So it's going to look like this. If your tree is ugly like something like this, you'll start with something nice and small down here, maybe constant number of terms. Then, you will double again, and it will be exponential. On the other hand, if your tree happens to be nice and balanced, doubling is not so bad because here you'll have constant. This a double everything below, but there's only log n levels. So is that linear? It should be about linear. It's certainly 2 to the theta log n. My conjecture's, in general, you need to omega n refinement. Where they're overlapping, and one on the right. Good question. I forgot to mention the subdivision at the beginning. This leads to another notion which we call grid unfolding. The opposite of cutting is gluing. If you want to make something convex, and therefore, sphere-like, you have to get rid of all the boundary, so you've got a glue every edge. We'll be more formal about defining gluing, I think, next lecture. But first question is suppose I gave you one of these pictures, and I give you agluing. by the same authors, like a year before the general result. So this algorithm has been implemented, at least in some simple examples. And it kind of nicely unrolls. You can see a 5 by 4 refinement in that little staircase. It's, again, to make everything keep going to the right, but here, they find a clever way to visit all the faces without having to revisit, basically, at all. And then, we can zoom out, and you get the unfolding. other open problems. This was genus 0. Interesting question is can you do genus higher than 0? Orthogonal polyhedra. I would guess so, but I'm not sure. The biggest question is, can you make this non-orthogonal? But then, the bands get messy. Haven't been able to do that. All right. Any questions? I'm going to take a break from unfolding now and switch the other direction of folding. So with folding, we're imagining we're given some polygon, and we'd like to make a polyhedron out of it. It's exactly the reverse of what we've been thinking about. When is this possible? This is Cauchy's rigidity theorem. If you have two convex polyhedra and they are combinatorally equivalent, they have the same way that things are connected together, and they have congruent faces, then, they are actually the same thing, the same polyhedron. If I want to make something convex, there's only one thing it could possibly make. Finding out what that one thing is is quite a challenge, but at least, we can prove that it's unique. Geometries of the faces. I know what these edge lengths are in the sphere. What I don't know are these angles of the polygon. If p and p prime are supposed to be different, then there must be two angles that differ. So what I want to show is actually all the angles are equal. Therefore, the polyhedra will be identical. There's no flexibility there, but it could be there are pluses and minuses. That's the proof by contradiction. a sphere. Think of it is as almost flat. What that would mean is there's some other way to draw this thing. Basically, there's a way to flex this linkage so that all of these angles increase and this one stays the same. How could I get a polygon where all the angles increase  and still be convex? Ain't possible. Why is it not possible? It's not possible by something called the Cauchy Arm Lemma. This is the part that Cauy got wrong. In a planar graph, the number of alternations is at most 2 k. The number of edges is half the sum of the degrees of the vertices. Cauchy's rigidity theorem would tell you that there's a unique realization of a convex polyhedron, and there will be a unique gluing realization of it, and that's what we're going to do here. We know it must make a little convex polygons, so we've got lots of polygons here. edge twice, once from each side, so this is half the number of faces. So now, things are starting to look similar, and I want to get some cancellation going on. I'm going to rewrite this formula as V equals 2 plus E minus F. OK? All I did here was decrease by-- well, because there's a half out here, I decrease each coefficient by 2. Whew! So I took E, I subtracted F, just took away one of each. Now, I have this formula. unique set of edges from the shortest paths that actually realize it. The next class will be all about how to actually find those gluings and know that they actually will fold into some convex shape. But that's it for today. We'll be back in a week or so with a new set of questions and a new lesson on how to find the right shape for the right size of an edge. We hope to see you in the next class. Back to the page you came from.

ROUGE-1: 25.46, ROUGE-2: 24.08, ROUGE-L: 21.89
BERTScore: 68.77

==============================================
==================== [78/100] ====================
Summary:
For a particle traveling in a circle, we've seen that the velocity can be written as r d theta dt. Let's now look at rotation in an arbitrary plane. And so what we're going to do is use the right hand rule to define a direction that tells you both the direction it defines the plane and it also tells you what the positive direction of rotation is for that plane. Now, in the k hat direction, in this case, I'm going to call it in the arbitrary n hat direction.

ROUGE-1: 19.13, ROUGE-2: 18.37, ROUGE-L: 18.73
BERTScore: 73.16

==============================================
==================== [79/100] ====================
Summary:
This week's lecture is about machine translation. In the second half of the week, we take a break from learning more and more on neural network topics, and talk about final projects, but also some practical tips for building neural network systems. So first of all, I'm going to introduce a new task, machine translation, which is a major use case of a new architectural technique. And then there's a crucial way that's been developed to improve sequence to sequence models. And so that's what I'll talk about in the final part of the class. In the early 1950s, there started to be work on machine translation. It was hyped as the solution to the Cold War obsession of keeping tabs on what the Russians were doing. Claims were made that the computer would replace most human translators. But despite the hype it ran into deep trouble. And this idea was largely canned. In particular, there was a famous German words we've translated. And so we explore forward in the translation process. And in the process, I'll go through in more detail later when we do the neural equivalent. When do you hope to be able to achieve this speed? I our experiments go well, then perhaps within five years or so. US government report in the mid 1960s, the ALPAC report, which basically concluded this wasn't working. Work then did revive in AI at doing rule based methods of machine translation in the 90s. But when things really became alive was once you got into the mid 90s, and when they were in the period of statistical NLP. And then the idea began, can we start with just data about translation i.e. sentences and their translations, and learn a probabilistic model that can predict the translations of fresh sentences? original piece of parallel data that allowed the decoding of Egyptian hieroglyphs. In the modern world, there are fortunately for people who build natural language processing systems quite a few places, where parallel data is produced in large quantities. And we can use that to build models. So how do we do it though? All we have is these sentences. And it's not quite obvious how to build a probabilistic model out of those. Well, as before, what we want to do is introduce an extra variable, which is an alignment variable. one translations, where one French word gets translated as several English words. You can get the reverse, where you can have several French words that get translated as one English word. So mis en application is getting translated as implemented. And you can get even more complicated ones. So here we sort of have four English words being translated as two French words. But they don't really break down and translate each other well. These things don't only happen across languages. They also happen within the language when you have different ways of saying the same thing. we have is based on the translation model. We have words or phrases that are reasonably likely translations of each German word, or sometimes a German phrase. And so then inside that, making use of this data, we're going to generate the translation piece by piece kind of like we did with our neural language models. So there's a search process. But one of the possible pieces is we could translate "er" with "he", or we could start the sentence with "are" translating the second word. So we could explore various likely possibilities. In neural machine translation we are directly calculating this conditional model probability of target language sentence given source language sentence. And so at each step, as we break down the word by word generation, that we're conditioning not only on previous words of the target language, but also each time on our sourcelanguage sentence x. So this is what the picture of a LSTM encoder-decoder neural machinetranslation system really looks like. Next week, John is going to talk about transformer based networks. They're typically much deeper. But we'll leave discussing them until we get on further.  Sequence to sequence models have been an incredibly powerful, widely used work force in neural networks for NLP. Machine translation was the first big use of them, and it's sort of the canonical use. So you can do summarization. You can think of text summarization as translating a long text into a short text. But you can use them for other things that are in no way a translation whatsoever. So the encoder will encode the previous two utterances, say. And then you will use the decoder to generate a next utterance. arc, right arc, shifts like the transition system that you used for assignment 3. Feed the input sentence to the encoder and let it output the transition sequence of our dependency parser. These models have also been applied not just to natural languages, but to other kinds of languages, including music, and also programming language code. So you can train a seq2seq system, where it reads in pseudocode in natural language, and it generates out Python code. And if you have a good enough one, it can do the assignment for you. looked at so far are already deep on one dimension then unroll horizontally over many time steps. But they've been shallow in that there's just been a single layer of recurrent structure about our sentences. We can also make them deep in the other dimension by applying multiple RNNs on top of each other. And this gives us some multilayer RNN. And having a multilayers RNN allows us the network to compute more complex representations. So simply put the lower Rnns tend to compute lower level features, and the higher RNN's should compute higher level features. Stuck with it. And you have no way to undo decisions. So we'd like to be able to explore a bit more in generating our translations. And well, what could we do? Well, I sort of mentioned this before looking at the statistical empty models. Overall, what an example to see how it works. So in this case, so I can fit it on a slide. The size of our beam is just 2. And the blue numbers are the scores of the prefixes. So these are these log probabilities of a prefix.  beam search's idea is that you're going to keep some hypotheses to make it more likely that you'll find a good generation. So that's most of the algorithm. There's one more detail, which is the stopping criterion. So in greedy decoding, we usually decode until the model produces an end token. And when it produces the end token, we say we are done. In beam search decoding, different hypotheses may produce end tokens on different time steps. So this is a heuristic method. It's not guaranteed to find the highest probability decoding. But at least, it gives you more of a shot than simply doing greedy decoding. n complete hypotheses. And then we'll look through the hypotheses that we've completed and say which is the best one of those. And that's the one we'll use. OK. So at that point, we have our list of completed hypotheses and we want to select the top one with the highest score. Well, that's exactly what we've been computing. But it turns out that we might not want to use that just so naively. Because that turns out to be a kind of a systematic problem, which is not as a theorem. But in general, longer hypotheses have lower scores. In the early 2010s, the big hope that most people had. application of deep learning and natural language processing, neural machine translation was the huge big success story. In the last few years, when we've had models like GPT2 and GPT3, and other huge neural models like BERT improving web search, it's a bit more complex. But this was the first area where there was a neural network, which was hugely better than what had preceded, and was actually solving a practical problem that lots of people in the world need. Even our best multilayer LSTMs aren't that great of capturing sentence meaning. For languages that have lots of inflectional forms of nouns, verbs, and adjectives, these systems often get them wrong. There are problems of agreement and choice. Many languages don't distinguish gender. So when that gets translated into English by Google Translate is that the English language model just kicks in and applies stereotypical biases. So if you want to help solve this problem, all of you can help by using singular they in all contexts when you're putting material online. For languages for which there isn't much parallel data available, commonly the biggest place where you can get parallel data is from Bible translations. Cherokee is not a language that Google offers on Google Translate. So we can see how far we can get. But we have to be modest in our expectations because it's hard to build a very good MT system with only a fairly limited amount of data. There is a flipside, which is for you students doing the assignment. The advantage of having not too much data is that your models will train relatively quickly. source sentence more flexibly to allow us to generate a good translation. I'll finish this off by going through the actual equations for how attention works. Next time I'll go through how attention is generated in the language of the brain and how it works in the brain of the human brain. Click here to read the rest of the article and follow me on Twitter @jenniferjames1 or on Facebook @JennyJames1. Back to the page you came from.

ROUGE-1: 26.64, ROUGE-2: 25.48, ROUGE-L: 23.66
BERTScore: 64.84

==============================================
==================== [80/100] ====================
Summary:
Nuclear materials is the biggest research field in the department, and yet, it's not talked about in 22.01. The basic mechanism of radiation damage is like you might imagine. The movement in clustering of those defects can be quite strongly influenced by the presence of all those other defects. And that's what we really need to know is what is the full population of every single type of defect in an irradiated material? That's what I mean by damage. It's the stuff that happens here that can tell us will our materials fail in nuclear reactors. curve that we were looking at before we started watching videos of crushing things, to remind you about the different material properties and what they actually mean. The strength of the material is how much stress you can put in until it starts to either plastically deform or it hits its UTS, ultimate tensile strength, where it will just fail. How about ductility? What do we mean by that? Either intuitively or on the curve, yeah? AUDIENCE: How much you can stretch it? MICHAEL SHORT: Exactly. The strain to failure would be a good measure of activity. A DPA measures the number of times that each atom has moved out of its original site. It's a simple measure of how many times has every atom left. The DPA only measure is what we call the ballistic stage of radiation damage. Damage is some of the wrong atoms at the end of the game, and they operate in very different timescales. It can take for these different defects to cluster up, and form these super structures and actually end up causing the damage that can lead to material degradation. So what sort of factors would affect the speed at which these different defects end up finding each other? What could you vary about a material or its environment to change the speed of these atomic diffusion jumps? MICHAEL SHORT: Temperature. Temperature determines diffusivities. It also can change phases or crystal arrangements, like for the case of anything iron-based. The dose rate, the rate at which those neutrons come in can change the rates at which the defects cluster up. And then micro structure. Things that are bigger than on the order of atoms. Nuclear materials data looks something like this. We predict that it stores about 2% of its energy in radiation defense. We did some of these measurements on a piece of steel from the nuclear reactor, and we got a whole bunch of interesting looking peaks for the red curve compared to the blue curve in the other irradiated conditions. So we think there's something there. And to jump through to the whole idea of differential scanning calorimetry, it's like what I drew here, but a lot more legible. When you make something colder, it tends to be more brittle. Normally you'd be able to bend a coin, or if it's a one yen coin, you can bite through it. The best we can do right now is to stick them in a reactor called BOR-60 in Russia. If your reactor is going to go to 500 DPA, you have to know whether or not your materials will survive, and it could take 20 years for the answer to be known. In that case, you could have a sudden pressure wave from steam explosion or whatever. Cluster of vacancies or interstitials is more stable for them to find each other and make larger defects. So there is a thermodynamic driving force bringing them together, and then as those defects build up, then what Jared said could happen. You could crack the material because it could get weaker, less ductile, less tough. Weaker is the opposite of strong, and what's the other one? Toughness-- oh, and harder, actually. So the origins of swelling are the humble vacancy. of vacuum, it could then combine to a bigger pocket, becoming a bigger and bigger void. The other problem too is that most materials generate helium when you irradiate them with neutrons. Did we go over the what's called the N alpha cross-section? Does that sound familiar to anyone? All right, I'm going to pull up Janice like I do pretty much every class. Let me show you what's going on. So among the millions of cross sections that we've gone through, there is this one right here called Z alpha. So what-- Yeah? When you apply radiation to a material, you can just create dislocations. Normally, you would have to deform a material to create and move dislocation. You create what's called this network forest of disLocations that makes things a lot harder to deform. What you want to happen is for dislocated things to move on the easiest planes, or slip plains, or the easiest directions for things to deform, like 45 degree angles. And without going into any of the math or atomistics, I just want to show you some examples pulled out of the fusion reactor. The way you test ductile-brittle transition temperature is what's called a Charpy impact test. It's probably the highest tech, lowest tech test I've seen. You simply hit things with a hammer. A very well calibrated, precise hammer. To measure by actually turning this dial and letting the hammer turn it as it moves through the material, you can see how much energy was absorbed by the material as the hammer comes back up at a certain temperature. By extrapolating, or say, integrating the area under those curves, you get the energy in each type of defect. a crack in the vessel. Boric acid actually ate through a whole chunk of the pressure vessel, leaving the stainless steel intact. The properties of the vessel are highly dependent on, not just its composition, but the heat treatment that went to make it. If you heat the vessel too much, it's no longer a code stamp vessel. Pretty tricky spot that we're in, huh? But we're trying to science our way out of it. But it's a couple minutes after. I'll open on Thursday with a little story. Michael Short: We can take out tiny pieces of the vessel and get the same information as you would from a Charpy test but on the nanoscale. The heating is so fast that the defects don't even have time to find each other, annihilate, and release their stored energy. We're doing this process on nanograms of material and seeing if you can irradiate something and measure its stored energy because if you could, you could take a tiny little razor blade, take out the smallest sliver of the vessels-- smaller than a grain of sand. Michael Short: Snipes are real. They don't exist, right? They actually do. You pretty much have to be British to know it, because they hunt them there for sport, and apparently, they're delicious. That's actually where we get the term snipe because the actual size of the sniper compared to the sniper is about that. If you can shoot that bird with a gun, you are an expert marksman and deserve the delicious and tiny treat that you've then blown apart with your bullet. Researchers used a nano a DSC, or nano differential scanning calorimeter, that can heat about 10,000 times faster than a traditional DSC. DSC induces a lot of artifacts in the signal that we couldn't separate from the noise. We were able to see very clearly, the first time we heated the sample, this extra area corresponds to some sort of energy release. We then heated that same sample a whole bunch of times, and made sure that it was always the same, which meant we had a fully relaxed material. about how mass attenuation coefficients can get you out of apartheid South Africa. And then we'll move into dose and biological effects. I'm serious. I mean it. I really am serious. We're going to talk about this for a while. It's not just a joke. It can be used to help you get out of a bad situation. We'll talk about it for a long time. We've got a long way to go, but it's a start. I'll be in touch.

ROUGE-1: 24.32, ROUGE-2: 22.79, ROUGE-L: 18.54
BERTScore: 66.89

==============================================
==================== [81/100] ====================
Summary:
In Western astrology, it's a constellation determined by when your birthday falls in the calendar. But according to the Chinese zodiac, your shēngxiào is the animal assigned to your birth year. Of the many myths explaining these animal signs and their arrangement, the most enduring one is that of the Great Race. The first twelve animals to make it across the river would earn a spot on the zodiac calendar in the order they arrived. Each year is associated with one of the animals in this order, with the cycle starting over every 60 years.

ROUGE-1: 25.00, ROUGE-2: 23.79, ROUGE-L: 24.49
BERTScore: 67.07

==============================================
==================== [82/100] ====================
Summary:
Professor: In almost all cases when you address atoms, you do two photon courses because a photon is scattered. Professor: If you have any doubts about some subtleties about how is light absorbed and emitted, the correct answer is always obtained from the two-photon picture. He says 99% of the research in our field is actually in the form of Raman processes, but pedagogical reasons for having a single photon are actually in our favor. The course is part of MIT OpenCourseWare, a free online education program. Professor: How many terms do you get if you write down the Hamiltonian? No approximation without. How many do we get? Professor: Four. We have four terms because we have combinations of omega 1 and omega 2. Professor: We have only one term, which dominates out of those four. It's one term-- the near-resonant term-- which dominates, he says. The only difference is that instead of having a coupling between one photon and another, we have one photon going up and down. be written as the sine squared divided by this, and it turns into a delta function times t. And when we divide the probability, the amplitude squared by t, we get a rate. And this is Fermi's Golden Rule. It is exactly the same you have seen probably more than 100 times. So therefore, we have now a transition rate. This is the delta function. I called it the function f because I want to discuss the spectral profile a minute. Any questions? Yes. because we have arranged our two levels a and b in such a way that the near-resonant process is that one. If you use a fully quantized picture with a and a degas, the a for the quantized description becomes an e to the i omega t. In the semi-classical description of the a dega has a minus sign. So therefore, if you look at all the combinations between plus and minus, for this Raman process we want to select e. In general, I would say if you have more than one process, there is no interesting interference term. You just get two different rates. One is the two-photon Raman rate and the other one is two- photon absorption rate. And you just have both simultaneously. They're not leading to the same final state. And this may have some detrimental effect, depending what you want to do. But we'll discuss some of those things in our next chapter on coherence. The energy of this state is where the dashed line is. It's not the energy of the real state k. The stepping stone is created with the first photon. What is its character? Spatial bay function and such? Well, it is exactly the intermediateState k. And what is the population? If you had a two-level system, we sort of start with 100% amplitude in state 1. But here, our population is diminished by the probability at which we have it admixed the state. The first form of coherence I want to discuss is the coherence involved in exciting atoms and the atom emitting light. It's related to the spontaneous emission and scattering problem. There is a certain randomness in spontaneous emission when we go to the laboratory and look at the spontaneously emitted photons. This is what is really the information-- the phase information-- which we have in a photon, which has been spontaneously emitted. There may be situations where we have a laser beam which has a well-defined phase and we just cannot retrieve the laser beam by looking at the photons. This kind of spontaneous Raman process-- has been very important historically. Before the advent of lasers, all you had is light bulbs, strong light bulbs. You couldn't really resonantly, you know, stack up two light bulbs and have enough spectral power to excite to a certain state. But look here-- you could still, with a strong light bulb, create an admixture of the excited state. This virtual line was terribly broad because of the width of the light bulb-- spectral width-- but then this spontaneous photon was just compensating for it. atom which has no motion. We are now saying, OK, now we allow the atom to move. And we can deal with that by simply saying when the atom has a velocity v, we can transform-- we can The momentum transfers are the same. And if we arrange for the two photons to be absorbed from opposite directions, we reach the situation where the Doppler shift is really zero. So this is the way where we do two-photon spectroscopy. This is one of the handful of methods of practical importance for eliminating the first order Dopple shift. just use the Galilean transformation and say OK, the physics is the same. However, the atom, due to its velocity, sees a slightly different frequency. But now, we calculate our Lorentzian by using the frequencies perceived by the atom. The different signs-- plus minus-- are, of course, distinguishing whether we have two-photon absorption or Raman process. And the frequency shift, going into the frame of the atoms, gives us Doppler ts k1v and k2v.  helium has not replaced hydrogen. It's still hydrogen. But this sort of tells you what the choices if you want to test fundamental physics or determine fundamental constants. In all atoms other than helium and hydrogen, you would be limited by the infeasibility of many electron calculations. And the one thing we have to discuss on Wednesday is the whole of the phase phi. Phi-- we started out by phi being the phase of the laser. And if the laser is in a coherent state, we can measure phi to any accuracy you want. you had the second spontaneous emission, I just want to clarify. So in the past, all this time when you were talking about off-resonant single photon scattering, was this actually, really, the more descriptive picture or was there actually a different physical-- PROFESSOR: No, this is off resonance scattering. And if you ask me, when do you have a situation where you first absorb the photon and then emit it, I would say, I'd like to know that. Coherence exists if there is a well-defined phase between two or more amplitudes, but we can only observe it if those amplitudes interfere. Coherence is important because coherence photons can't come out of a unitary time evolution. What is the randomness or this loss of phase of spontaneously emitted photons? Is it due to performing sort of a partial trace ever-reaching over certain states? So if you're interested in the photon, maybe tracing out the states of the atom. Or ever-reach over modes of the electromagnetic field. And question C is both is both. atomic wave function is this. The excited state-- let's just assume the photons are in resonance. We know how to deal with off resonant lasers. So therefore, the phase evolution is e to the i omega naught t. But now, and this is what coherence is about, there is a very specific phase. And this phase phi comes from the laser. If you excite the atom with a laser beam but it has a phase shift, then the atomic wavefunction is phase shifted. over, the photonic part of the wave function is a vacuum. We have no photon in our cavity. And now we wait and we allow spontaneous emission. And spontaneous emission is nothing else than the time evolution with the operator I just discussed with you. So after spontaneous emission, well, one is we know for sure the atom is in the ground state. And so we have now our knowledge from the vacuum Rabi oscillation that this part ofthe wave function does nothing. And the excited state with zero photon will actually do Rabiscillation. it appears mapped into a two-level system for the photons. But if we are now doing a measurement either on the atomic system or on the photonic system, we are limited in the accuracy at which we can retrieve phi. This is what I referred to as the fundamental limit of spontaneous emission because we have not lost any coherence here. It's just if the phase is only imprinted in one particle-- one particle quantum physics sets us a limitation. OK. Any question? To be continued on Monday.

ROUGE-1: 25.18, ROUGE-2: 23.93, ROUGE-L: 21.47
BERTScore: 67.96

==============================================
==================== [83/100] ====================
Summary:
Lecture eight is about deep learning software. This is a super exciting topic because it changes a lot every year. We're going to talk about some of the nuts and bolts about writing software and how the hardware works. And a little bit, diving into a lot of details about what the software looks like that you actually use to train these things in practice. And we'll talk about several of the major deep learning frameworks that are out there in use these days. The midterm will be in class on Tuesday, five nine. Last time we talked about fancier optimization algorithms for deep learning models including SGD Momentum, Nesterov, RMSProp and Adam. And we saw that these relatively small tweaks on top of vanilla SGD, are relatively easy to implement but can make your networks converge a bit faster. We also talked about transfer learning where you can maybe download big networks that were pre-trained on some dataset and then that you know, your model is maybe sitting on the GPU, but your big dataset is sitting over on the right on a hard drive or an SSD or something like that. of my computer at home that I built. And you can see that there's a lot of stuff going on inside the computer, maybe, hopefully you know what most of these parts are. And the CPU is the Central Processing Unit. That's this little chip hidden under this cooling fan right here near the top of the case. The GPUs are these two big monster things that are taking up a gigantic amount of space in the case, and they have their own cooling. They're quite large. So the question is what are these things and why are they so important for deep learning? Well, the GPU is called a graphics card. In deep learning we kind of have mostly picked one side of this fight, and that's NVIDIA. So if you guys have AMD cards, you might be in a little bit more trouble if you want to use those for deep learning. NVIDIA has gone in there and released their own binaries that compute these primitives very efficiently on NVIDIA hardware. So in practice, you tend not to end up writing your own CUDA code for deeplearning. You typically are just mostly calling into existing code that other people have written. The NVIDIA Titan XP which is the current top of the line consumer GPU has 3840 cores. That's like way more than the 10 cores that you'll get for a similarly priced CPU. The downside of a GPU is that each of those cores, one, it runs at a much slower clock speed. And two they really can't do quite as much as a CPU.at once. But it should give you the sense that due to the large number of cores GPUs can sort of, are really good for parallel things. up when running the exact same computation on a top of the line GPU, in this case a Pascal Titan X, versus a top-of-the-line CPU. Although, I'd like to make one sort of caveat here is that you always need to be super careful whenever you're reading any kind of benchmarks about deep learning. And you kind of need to know a lot of the details about what exactly is being benchmarked in order to know whether or not the comparison is fair. This was sort of out of the box performance between just installing Torch, running it on a CPU, and just running Torch on a PC. than a year ago, TensorFlow was relatively new. It had not seen super widespread adoption yet at that time. But now I think in the last year Tensor Flow has gotten much more popular. It's probably the main framework of choice for many people. So that's a big change. We've also seen a ton of new frameworks sort of popping up like mushrooms. So in particular Caffe2 and PyTorch are new frameworks from Facebook that I think are pretty interesting. Paddle, Baidu has Paddle,. Microsoft has CNTK, Amazon is mostly using MXNet. In TensorFlow you would typically divide your computation into two major stages. You'll first have code that builds the graph and then you'll go and us build our model without doing all these explicit bookkeeping details ourself. There's like a lot of different higher level libraries that people build on top of Tensor Flow. For kind of simple, kind of feed forward neural networks, like PyTorch, you can do a similar thing with Keras where we're actually building up this new thing on every forward pass. choose to use frameworks rather than writing your own stuff from scratch. So as kind of a concrete example of a computational graph we can maybe write down this super simple thing. Where we have three inputs, X, Y, and Z. And this code looks exactly like the two layer net code that you wrote in Numpy on the first homework. And now we're doing a manual update of the weights and computed gradients using our PyTorch tensors. But the major difference between the Py Torch tensor and Numpy arrays is that they run on GPU so use one weight matrix so if Z is negative we want to use a different weight matrix. TensorFlow has this magic line that just computes all the gradients for you. The other nice thing about TensorFlow is you can really just, like with one line you can switch all this computation between CPU and GPU. In PyTorch again, it's really easy to switch to GPU, you just need to cast all your stuff to the CUDA data type before you rung your computation. So the next layer of abstraction is the variable. So this is, once we moved from tensors to variables now we're able to take gradients automatically and everything like that. TensorFlow lets you run computations on top of a graph. The idea is that you can feed in data and put it in through these input slots in the graph. These input slots are now kind of like symbolic variables and we're going to perform different TensorFlow operations on these symbolic variables. This kind of basic way of working could be a little bit unwieldy and it could be really annoying to make sure you initialize the weights with the right shapes and all that sort of stuff. So as a result, there's a bunch of sort of higher level libraries that wrap around Tensor Flow and handle some of these details for you. of basic Tensor operations to compute our Euclidean distance, our L2 loss between our prediction and the target Y. Then we have this magical line where after we've computed our loss with these symbolic operations, then we can just ask TensorFlow to compute the gradient of the loss with respect to w1 and w2 in this one magical, beautiful line. But again there's no actual computation happening here. This is just sort of adding extra operations to the computational graph. And then you might think that this would train the network, but there's actually a bug here. So, if you actually run this code, and you plot the loss, it doesn't train. TensorFlow is smart and it only computes the parts of the graph that are necessary for computing the output that you asked it to compute. So when updates evaluates it just returns none. But because of this dependency we've told it that updates depends on these assign operations. These assign operations live inside the computational graph and all live inside GPU memory. So then we're doing these update operations entirely on the GPU and we're no longer copying the updated values back out of theGraph. So whenever you're working with TensorFlow you have this funny funny thing that happens. With these fake data dependencies and we just say that this dummy node updates, has these data dependencies of new w1 and new w2. And now when we actually run the graph, we tell it to compute both the loss and this dummy nodes. This dummy node doesn't actually return any value it just returns none, but because of this dependency that we've put into the node it ensures that when we run the updates value, we actually also run these update operations. So the question is why is loss a value and why is updates none? That's just the way that updates works. The layer is using this xavier initializer object to set up an initialization strategy for those. And you can see here, we're also passing an activation=tf.nn.relu so it's even doing the activation, the relu activation function inside this layer for us. And in fact if you run this code, it converges much faster than the previous one because the initialization is better. Question is does the xavierInitializer default to particular distribution? I'm sure it has some default, I'm not sure what it is. But it seems to be a reasonable strategy, I guess. Keras is very popular, so you might consider using it if you're talking about TensorFlow. We build some optimizer object and we call model.compile and this does a lot of magic in the back end to build the graph. Of layers. And now we can calls model.fit and that does the whole training procedure for us magically. So I don't know all the details of how this works, but I know Keras isvery popular. So there's actually like this whole large set of higher level Tensor Flow wrappers that you might see out there in the wild. And it seems that like even even even Keras can do this. but Tensorboard you can add sort of instrumentation to your code and then plot losses and things as you go through the training process. TensorFlow also let's you run distributed where you can break up a computational graph run on different machines. That's super cool but I think probably not anyone outside of Google is really using that to great success these days, but if you do want to run distributed stuff probably Tensorflow is the main game in town for that. A side note is that a lot of the design of Tensor Flow is kind of spiritually inspired by this earlier framework called Theano from Montreal. in PyTorch a lot is define your own nn modules. So typically you'll write your own class which defines you entire model as a single new nn module class. And a module is just kind of a neural network layer that can contain either other other modules or trainable weights or other other kinds of state. So in this case we can redo the two layer net example by defining our own nN module class and then storing them inside of our own class. In the forward pass we can use both our own internal modules as well as arbitrary autograd operations on variables to compute the output of our network. PyTorch provides pretrained models. And this is probably the slickest pretrained model experience I've ever seen. You just say torchvision.models.alexnet pretained=true. That'll go down in the background, download the pretrained weights for you if you don't already have them, and then it's right there, you're good to go. PyTorch also has, there's also a package called Visdom that lets you visualize some of these loss statistics. has convolution and relu operations kind of one after another, you might imagine that some fancy graph optimizer could go in and actually output, like emit custom code. So I'm not too sure on exactly what the state in practice of TensorFlow graph optimization is right now, but at least in principle, this is one place where static graph really, you can have the potential for doing this optimization in static graphs. Another kind of subtle point about static versus dynamic is this idea of serialization. So with a static graph you can imagine that you write this code that builds up the graph and then once you've built the graph, you have this data structure. Python control flow. And the problem is that because we only build the graph once, all the potential paths of control flow that our program might flow through need to be baked into the graph at the time we construct it before we ever run it. In TensorFlow this becomes a little bit uglier. And again, because we need to construct the graph all at once up front, this control flow looping construct again needs to be an explicit node in the Tensor Flow graph. awkward to work with than the sort of native dynamic graphs you have in PyTorch. So then, I thought it might be nice to motivate like why would we care about dynamic graphs in general? So one option is recurrent networks. So you can see that for something like image captioning we use a recurrent network which operates over sequences of different lengths. So I think that there's a lot of opportunity for doing cool, creative things with dynamic computational graphs. And maybe if you come up with cool ideas, we'll feature it in lecture next year. can train on data without writing any of your own code. Cafee has a model zoo with a bunch of pretrained models, that's pretty useful. Caffe has a Python interface but it's not super well documented. You kind of need to read the source code of the python interface to see what it can do. But it does work. So, kind of my general thing about Caffe is that it's maybe good for feed forward models, it'smaybe good for production scenarios, because it doesn't depend on Python.

ROUGE-1: 26.27, ROUGE-2: 25.06, ROUGE-L: 21.78
BERTScore: 59.66

==============================================
==================== [84/100] ====================
Summary:
Part 3 in our series on distributed word representations. We're going to be talking about vector comparison methods. To try to make this discussion pretty intuitive, I'm going to ground things in this running example. On the left, I have a very small vector space model. We have three words, A, B, and C. And you can see graphically that B and C are pretty close together. And A is kind of lonely down here in the corner, the infrequent one. Euclidean distance is capturing the first perspective that we took on the vector space, which unites the frequent items B and C as against the infrequent one A. As a stepping stone toward cosine distance, which will behave quite differently, let's talk about length normalization. Given the vector u of dimension n, the L2 length of u is the sum of the squared values in that matrix. And then the actual normalization of that original vector u involves taking each one of its elements and dividing it by that fixed quantity. in your reading or research. The first class are what I called matching based methods. They're all kind of based on this matching coefficient. And then Jaccard, Dice, and Overlap are terms that you might see in the literature. These are often defined only for binary vectors. Here, I've given their generalizations to the real valued vectors that we're talking about. And the other class of methods that you may see come up are probabilistic methods which tend to be grounded in this notion of KL divergence. to correct this. We're going to start to massage and stretch and bend our vector space models. And we will see much better results for these neighbor functions and everything else as we go through that material. It will be a big step forward for us in terms of our understanding of how the universe works, and how it can be improved for the benefit of the human body. It's going to be a very exciting time for us, and I'm looking forward to it. I think we'll see a lot more progress in the coming years.

ROUGE-1: 28.35, ROUGE-2: 25.60, ROUGE-L: 25.01
BERTScore: 67.76

==============================================
==================== [85/100] ====================
Summary:
liyan blake a little bit about him he's kind of out there some of his contemporaries thought he was insane you know nowadays maybe he would be heavily medicated. People think that he's out there and there was that who was William Wordsworth who said that there is something in the madness of this man which interests me more than the sanity of Lord Byron and Walter Scott. These individuals Blake and will do Wordsworth later and Coolridge and all of these people these are answers on Jeopardy so when you're flipping through and you just stop because oh there's an English question. and your parents don't know just throw out William Blake you might be right okay very very famous individual so we'll be covering in the next couple days the lamb by William Blake. okay this is probably the sweetest of the poems that we have to some degree okay looking at it at face value one they have a little picture and these are the the etchings that he does okay you know with the picture that they give us to to help place things you know we have a shepherd talking to his flock okay he's out there alone he's probably has conversations with them. the field but somebody that is passionate about their religion and their faith you know the nature and the common man the shepherd person of the flock or even a child in a sense of children so it's a very short one and it's one that I believe is pretty easy to to comprehend and understand okay. "I believe it is one of the easiest to understand and understand," he says. "It's one of those things that is very, very easy to understand. It's very simple to understand"

ROUGE-1: 37.60, ROUGE-2: 34.65, ROUGE-L: 34.48
BERTScore: 64.18

==============================================
==================== [86/100] ====================
Summary:
Cú Chulainn, hero of Ulster, stood at the ford at Cooley, ready to face an entire army singlehandedly. The army in question belonged to Queen Meadhbh of Connaught. Enraged at her husband’s possession of a white bull of awesome strength, she had set out to capture the fabled brown bull of Ulster at any cost. Cú Chulpainn invoked the sacred rite of single combat in order to fight the intruders one by one.

ROUGE-1: 20.74, ROUGE-2: 19.66, ROUGE-L: 20.74
BERTScore: 69.71

==============================================
==================== [87/100] ====================
Summary:
The Underworld is actually a lovely place to "live" It boasts historic charm and eccentric neighbors with eternal ties to the area. Tartarus is reserved for a select few who some might call the greatest sinners of all time. Elysium is the Underworld’s exclusive VIP section— and your permanent home. The Underworld also features four other waterways: Acheron, the river of woe; Cocytus, river. of wailing; Lethe, river of oblivion; and Phlegethon, a. great source of natural light.

ROUGE-1: 21.21, ROUGE-2: 19.65, ROUGE-L: 15.30
BERTScore: 65.12

==============================================
==================== [88/100] ====================
Summary:
Fluorescence is the absorption of light energy by a molecule. Luminescence is the general term. There are different types of luminescence. Fluorescence is a key point. So what is fluorescence? It's the absorption. of light. And I'm going to redefine all these properly in a moment, Professor Lambda. Lambda Lambda: Fluorescence, F-L-U-O-R-E-S. What are you guys whispering about? Audience: [INAUDIBLE]. Luminescence is the interaction of a chemical with another chemical to give luminescence. In bioluminescent marine organisms, ATP is combined with another molecule through the action of an enzyme that ends up kicking out light energy. Fluorescence is a more specific term, and you may wonder why I'm putting this in capital letters. The first thing to learn about fluorescence is how to spell fluorescence. And these things are going to come up. And it's not the last time you're going to see them. a particular wavelength so it's called its Lambda of excitation. Once it's excited, it just drops its energy back out and goes back to the ground state. And the most important thing that you want to remember is the wavelength, which is given in nanometers. This wavelength is higher energy. Lower energy are longer wavelengths. And that is a rule for fluorescence. When you excite a molecule, in this region. And we would see emission in wavelengths in this area. But a cardinal rule is that we excite with a shorter, higher energy wavelength and observe emission at a longer, lower energy wavelength. When you mix ethidium bromide with DNA, it will intercalate into the DNA. And it will light up as a bright orange dye. Now, there's a big problem here, because you can't use DNA ethidiumbromide. It's pretty toxic. And so these are toxic dyes that can only be used in fixed cells to do observations of cells. So there's been quite a revolution in the work done with DNA binding agents that bind a little differently and are way less toxic. DNA microarrays can be used for profiling genetic material or for profiling not just DNA, but RNA. Fluorophores are great, but you've got to worry about them. We'll see how at the beginning of the next class, in order to probe for particular stretches of DNA that might be disease related and have single nucleotide polymorphisms. And the DNA microarray experiments show you how you can use fluorophores attached to DNA sequences, OK. And I'll see you in the nextclass. cyan blue. So that's at a shorter wavelength from the ethidium bromide. We know intercalation is perpendicular to the axis of the DNA. So where, looking at this, do you think these bind? A while ago when I was talking about the structure of DNA, I like to think of DNA as having two grooves, two places where things combine to it. And so these compounds would be known to bind in the minor groove. And in fact, they bind in particular regions of DNA where there's AT, not GC. them. So nucleic acids seem to be something that we can definitely pinpoint with fluorescence. We can see where it is. We could follow cell division. For example, upon adding things to a cell, can you see-- remember very, very early on, we showed you movies of cells dividing. You could do that with this kind of dye because it's a non-toxic dye. So, great, so far, so good. So the key thing, though, about biology is we have so many other entities within a cell that we want to be able to track and monitor. Professor Martin: I'm going to focus completely on the technological side of antibodies and how they are useful reagents to study biology. Now, what you will learn from Professor Martin in two or three lectures time is much more about the nuts and bolts of the immune cells. And how it mounts a response to disease and other features. But for the time being, I'm just going to talk about B cells, which are cells that produce antibodies. And I'mgoing to talk to you about how they recognize their targets. There are hundreds of thousands of different antibodies in the human system. If we had a gene for every single different light chain and every heavy chain, you know, our DNA would be completely swamped by being dedicated to the genetic material for antibodies. Instead there is a particular system which provides little portions of the DNA structure that are in little pieces of variable components that can get zipped together through transcription and slicing events. And this is what's known as the BDJ system. And you'll hear more about that from Professor Martin. mouse. If you want to study rabbit cells, you could produce it in different organisms. You're not going to produce the antibodies in the same host. There's a lot of work being done now with antibodies from different organisms, in fact, you'll see them from camels. But then you collect the antibodies through an affinity method that's directed just at antibodies. Then you've got your population. And you can throw a fluorophore dye at it and chemically label it. So it's a good point there. Antibodies can't float into cells. They're too large. So you have to fix the cells on a glass side and permeabilize them, for example, with methanol so that the antibodies can gain. What do you think? Can they cross the plasma membrane to get into the cell to label a target? Who say yes? Who says no? Good. OK. You guys don't say much, but I know you know the answer here. But I know what the answer is.

ROUGE-1: 24.91, ROUGE-2: 23.71, ROUGE-L: 20.88
BERTScore: 60.45

==============================================
==================== [89/100] ====================
Summary:
Rafael JARAMILLO: Today we're going to discuss the many D's of thermodynamics. He says lowercase d, lowercase Greek d, and uppercase Greek D are the most physical, unstated assumptions. Jaramillo: To illustrate the concept of transformation quantities, it helps to draw state function surfaces. The most common transformations that we talk about are transformations that take place at constant pressure and constant temperature, he says. The transformation entropy between phase alpha and beta at a given pressure and temperature. of two independent variables. So, for example, we could have the entropy of phase alpha drawn as a function of temperature and pressure. So this is a 3D visualization of how the entropy for a given phase might vary with pressure and temperature at a fixed composition. So to illustrate transformation quantities, I want to draw the transformation entropy for an isobaric, isothermal phase transformation. We're going to be two different state function services, one for each phase. We'll label this beta. And I'll draw another state function service for phase alpha.

ROUGE-1: 30.39, ROUGE-2: 28.50, ROUGE-L: 26.33
BERTScore: 66.55

==============================================
==================== [90/100] ====================
Summary:
In this video I'm going to be going over lung oscilation specifically the sites of where you osculate. We're going to go over normal breath sounds versus abnormal breath sounds. You can access the quiz and the notes over here or in the description below. In the next video I'll be performing an assessment on a patient and show you how to listen with your stethoscope to these sides. We'll go over some audio clips ofnormal breath sounds and where you should hear them at. osculation sides so first let's cover the right lung your right lung is made up of three loes very important so you have your right upper lobe, right middle lobe and right here here you have the horizontal Fisher and then the oblique Fisher and this just separates the middle loow from your upper and your lower. We're going to start from top to bottom and compare sides and work our way down below the scapula right where the Apex is and then go over to the side and compare and listen. remember to get the best sound quality so you can hear so you're not listening over the shoulder blades because that will muffle your sound and you won't be able to hear. Have your patient put their arms in their lap or just separate those shoulder blades from each other then what we're going to do we want to assess first your upper loes so from C7 to T3 your cervical and thoracic spine that is where your Upper loes are. That will allow us to assess our lower loes and we'll start around T3.  abnormal breath sounds that you could hear that may be thrown in with those normal breath sounds are separated into continuous pitch monophonic wi sounds [Music] like stri spider. Strider is a high pitch whistling or gasp with a very harsh quality to it and patients like pediatric patients if they get the cro or cute epiglottis or you have a patient who has an airway struction you will hear this sound. This is an extra sound that you're hearing that is La lasting less than2 seconds okay first type um is coarse crackles crackles for nor Al has been known as rails. and discontinuous now first let's go over continuous what does continuous mean this is a extra sound that you're hearing that is lasting more than 2 seconds with a full respiration okay the first type is called a high pitch polyphonic whe let the name help you okay so what is it it is mainly heard in expiration so when the patient's breathing out but it can be in Inspiration as well. The second type of discontinuous abnormal sound is called Fine crackles this is heard on inspiration and it is a high Pitch crackling sound compared to the coarse crackle this is like low pitch like a bubbling noise this fine crackles is high pitch.

ROUGE-1: 26.49, ROUGE-2: 24.92, ROUGE-L: 22.91
BERTScore: 67.84

==============================================
==================== [91/100] ====================
Summary:
Professor Donald Kagan: Sparta is the most important, I think, of the early poleis. In Sparta, the only decisions they made were questions of whether to go to war, whether to make peace. The only people who spoke at those assemblies were the kings, the gerousia, or a group of people I haven't mentioned to you yet, the ephors, the five ephor. The idea was to have a representation of the ordinary Spartan to carry on the Spartan way. The ephors were given the responsibility of seeing that the kings were in line. They had various techniques or various policies and processes which had them make judgments. They could go to Delphi and ask the god, if they were right in thinking something was wrong. If they came back the kings would be put on trial. Kings were brought to trial in this way frequently in the history of Sparta and very often they were convicted, and often exiled, and in other ways punished. The Spartans have done something that is similar to what the Romans would do centuries later. The gerousia was, by far, the most significant council in the state, most able to have the necessary prestige and yet to be small enough truly to discuss what needed to be done. So, if the ephors wanted to do something, it would be damn foolish not to clear it with the Gerousia first; although if they wished to be reckless, they could do otherwise. It's important though to realize that in these early days of the polis they probably had very different leaders. Around 570 B.C. the Spartans started taking on other Greek states trying to establish their domination. They defeated the powerful and important state of Argos. In the process they took away a piece of land that is between the area ofArgos and Sparta, the name of it is Cynuria and they took it away, next to their own state. That's interesting, because the Argives never forgave that and never gave up on the idea of getting it back. The Spartans normally will like to see other states oligarchically governed. Sparta was the first state to be in command, or in control, or to be the leaders of a coalition of states. Ancient Greeks referred to the Spartans and their allies as the Peloponnesian League. How did it come to exist? The beginnings are shrouded in legend and are not absolutely clear. If you want your allies to come and fight with you it's better to have them to do so willingly than under orders, says Thucydides, the ancient Greek historian. Sparta's strategic importance is very great. They gained control of Tegea where they claimed to have discovered the bones of the great Homeric hero, Orestes. Also, there was a legend that maybe they propagated that showed up in some poetry we have, that Agamemnon had moved from his home base in Mycenae to Sparta. To that is added the permanent enmity of Argos, which never gives up the idea of returning to the great days of Pheidon with Argos as the dominant state in the Peloponnesus. rather than reality question. Scholars bat it around both ways; my prejudices are that the leagues' constitution, whatever it may have been, was less important than reality. That is to say, not all states in the Spartan Alliance were equal. The closer you were to Sparta the smaller you were, the weaker you were and the more you did what the Spartans told you. And vice versa. The stronger, the more distant, the wealthier you were,. the more independent you were of the Spartans. the Helots. It's not just that the Helots were so numerous compared to the Spartans, but I want to remind you again of their tremendous dissatisfaction with their situation, their backs may have been broken, but their spirits were not. They always were hoping to have a rebellion in which they could undue this extraordinarily heavy burden that they carried, and somehow in spite of the hundreds of years in which this has been going on, they did not lose sight of their nationality, of the fact that they were Mycenaeans. Their feelings towards the Spartans were as you might expect. no exercises of the arts, such as existed before this system was created. No luxuries legally in Sparta. There are few creature comforts. Why? Because in a way, necessity becomes a virtue. They said, of course, we gave that up, because that's what makes us the great people we are. That's what the Spartans did. Their way of life was imposed upon them by the decision to maintain their command of the Helots, after that it all makes perfect sense. Athens is located in the southeastern portion of the Greek peninsula. The city is Athens; the region in which they live is Attica. The people are Athenians and that's an important point I think I made too. Some of the most important and most aristocratic Athenians traced their ancestry not to the Athenians who were there before the moon was created, but to people who had come in this flight sometime after the end of the Bronze Age. The Athenians will be looking very carefully over the shoulders of aristocratic archons whenever they are in power. Attica contained silver mines in the south of the peninsula, and that gave the state a source of income that was very, very unusual among the Greek city states. One reason why the soil wasn't so great for agriculture was that a lot of it is red clay, but that turns out to be wonderful for making pottery. Another natural resource of great value and great blessing to those of us who can still see the remains of the Athenian experience is the marble that comes from Mount Penteli. There is no set of local rebellions against the major city, no need to go to war. But in neighboring Boeotia, the chief city of Thebes, traditionally was at war trying to subdue the other major cities. So, Boeotian is torn, to some degree, by this internal conflict, which makes it harder for Thebes to achieve the kind of power in its own home territory that the Athenians are able to achieve. Thebes is now the most powerful city in the world. Aristocratic means ruled by the best, and best in that time means simply best by birth. Nobody in Athens holds an office at this time, or as far as I can tell, at any time. The only thing in town that has continuity, that can develop power and influence over a period of time is the council of the Areopagus. They are very nervous about individuals who acquire too much power, popularity, influence which begins to engage in commerce to a greater degree than before. tribes contained, according to this tradition, three subdivisions that were called phratres. An easy way to translate phr atres is brotherhood. Notice it's again about family and birth. You are in phratre; you're in that phratrs, because so is your father and you inherit it. These phratmes were very important. I should have mentioned that the tribes had important religious functions that also the army consisted of four regimens, one for each tribe. These tribes had great reality for the Spartans. Little to decide and very little to do. Most of the real life of the state in the earliest days would have been out in the countryside where the overwhelming majority of the people lived. These noblemen would typically have held a lot of land and have been well to do, have had all the powers I've described, and were looked up to and were listened to. They would have led the military units into battle when that was necessary. One of the things that they did was to serve as the source of justice in the state. archon who was actually the most important archon, the one who gave his name to the year. So, if you wanted to know when did a thing happen, somebody would say it was in the archonship of so and so. A third archon was known as the King archon,. the archon basileus. His responsibilities were mainly religious, but I should point out that every one of them also did justice, that is, they had courts to which people could come to get their quarrels settled. will threaten the character of the aristocracy. Aristocracies love equality; equality among aristocrats, and then tremendous inequality between them and everybody else. An aristocratic republic is what we have, not a monarchy, but a republic. Yalies are very nervous about anybody sticking his head up above the crowd, because the question is always why not me? You have high expectations of yourself and so sometimes unless you're invaded by later religious ideas that the Greeks didn't have, you're not humble. Zeugitai means 'yoke fellows' or 'hoplite fellows' in Greek. They were well off enough to own a team of oxen who were yoked together to pull the plow. This new class of independent family farmer has arrived in Athens, says Stavrias. He says they are not satisfied with their position in the state, as his own importance to the state becomes greater and greater. We will find out more about this new breed of Greek farmer in our next episode. A young big shot of extraordinary character named, Cylon, attempted a coup d'état trying to establish a tyranny in Athens. He couldn't, he was resisted by enough of his to be attached to the family, and as we get to the last end of the last third of the fifth century and the Peloponnesian War is about to break out, the enemies of Pericles will pull out the curse of the Alcmaeonidae to use against him. Here's the first sign that we see of trouble in paradise. Nice, calm, happy synoecisized Athens has got trouble right here in River City. opponents that he was defeated. The leader of the resistance was the family known as the Alcmaeonidae. They went up there, locked up Cylon and his supporters in the Acropolis, in a temple. You couldn't go into the temple for the purpose of killing somebody, that would be sacrilegious. Still if you're inside that temple and trying to avoid being killed, you still need food and drink. So, how could they manage it? Well, they took a cord, tied it to the temple, held onto the cord, and went down to the well and got their water.

ROUGE-1: 31.05, ROUGE-2: 29.09, ROUGE-L: 23.98
BERTScore: 59.60

==============================================
==================== [92/100] ====================
Summary:
Ka-Yen: Today, I'll be teaching you guys a little bit about nuclear energy. He'll talk about the functionality and the benefits and the problems associated with nuclear. Ka-yen: Most of this development happened between 1939 and 1945. He says he walked into MIT not knowing a single thing about nuclear power. He gives a brief history of nuclear energy and explains some of the different types of reactors that can be used to produce electricity. He ends the lesson with a list of questions for you to answer. In 1951, the first nuclear reactor to produce electricity was the experimental breeder reactor, the EDR1. The first nuclear powered submarine, the USS Nautilus, was launched in 1954. The real heyday of nuclear was between 1960 to 1975, during the span of 15 or so years, this was the real commercial energy boom. Currently, China, India, and South Korea, they are the main players in this game. The main reason why we're not using more nuclear power is because of safety issues. Nuclear creates 75 times less carbon emission than coal does, and 35 times less than natural gas does. Nuclear power is able to provide a good baseload source of energy. Building reactors takes billions of dollars and tons of time. Nuclear is not nearly as economic of a source of electricity generation as any other of these ones I mentioned, unless you talk to the UK, which thinks it's OK, but everyone else is saying that it's not as money efficient. If you guys have any questions about any of these topics, I recommend going to NRC piqued your interest. Most reactors in the United States are boiling water reactors, or BWRs. They are the cheapest option out there for creating nuclear power. The next kind of reactor that falls under the light water reactor category is the pressurized water reactors. Heavy water reactors use heavy water instead of light water, which has a much lower cross-section of neutrons than light water. It's actually kind of a bad thing because the whole point of nuclear reactors is to create fission to create heat and power. The fuel core is basically just a bunch of rods of uranium, sometimes it's clad in something like zirconium. Neutrons can simulate other fissions, and the control rods are there to make sure that there's not too many fissions happening in the fuel core at a certain time. The heat that gets guys calculated in pset 1 eV is 0.025 eV, which is super low energy. If one of the neutrons-- so imagine-- if you're looking at the little fuel core, there's a lot of neutrons flying around. Deuterium has a chance of absorbing less neutrons as it undergoes its processes, so you're able to use a lower enriched uranium. But deuterium is really expensive. It's about 1,000 or so dollars per kilogram, which is kind of ridiculous because a kilogram of water is really not much at all, you know? So even though you're counteracting the lower fuel costs with higher water cost, you actually have to change out your fuel more often. really bad happens. You'd have to have breaches in both the loops, which is very unlikely to happen. This is a very similar problem, as you can see that in all these instances of the reactor incidents, it's just kind of like the fuel core was misbehaving and we weren't able to get enough coolant water to it. So following the earthquake, these coolant pumps broke. They're like, oh, that's OK. What we can do is we have backup generators to continue running the pumps. Three Mile Island, Chernobyl, Fukushima are some of our biggest nuclear accidents. After a nuclear accident you can see a pretty steep decline in the amount of nuclear reactors that are being commissioned. People think that nuclear reactors are incredibly dangerous, which is why we aren't able to get government funding and government loopholes to jump through. Another issue with nuclear power is nuclear waste, and the main thing in nuclear waste is made out of uranium oxide, which gets transformed into uranium particles after a bunch of fissions. their equipment and their instrumentation wasn't able to detect that. So they continued to operate again but this valve was open, so there was actually water that was getting leaked out of this primary loop. So because the water wasgetting leaked, they noticed that, oh, shoot, the pressure is dropping. Well, what do you do when the pressure's dropping? Apparently you have to make sure that there's not too many vibrations that could damage the reactor. So now there's water leaking out so the core is getting hotter, but then they also took out the water that is usually used to cool the reactor core. So this combination of events led to a core meltdown. So the core melted down. No one really lives near Chernobyl at the moment. It's kind of been deemed unlivable because these radioactive isotopes literally went everywhere in this environment. Luckily we see that there are animals coming back now now. If you look on NationalGeographic.com there's like little deer roaming around Chernobyl. But it's been about-- how long has it been, like 30, 40 years? People aren't advised to live here still. So Chernobyl was terrible. So the you can see, it looks like it's sitting in a parking lot outside the reactor. So this is an OK solution. escaping because water is a really great neutron moderator. It turns out it's actually fairly safe. Apparently you can go swimming on the top of the reactor spent fuel pool and you'll be OK and not be exposed to too much radiation if you want. So yeah. This is the main solution that people have been using for years, but they realize that this isn't super sustainable. We have way too much spent fuel to be able to just continue to store it in these spent fuel pools. So the next solution was something called dry cask storage. Yucca Mountain is the primary push by the U.S. to find a deep geological repository somewhere in the United States. People in Nevada weren't happy about this, they're like, why are we getting tossed on nuclear waste? We don't even have nuclear reactors in Nevada. There was a lot of opposition. And because of the social opposition there was government opposition and many loopholes we had to jump through, and so it was just becoming a huge disaster. It's been abandoned, as you can see from this lovely Google picture, it's permanently closed. can I have a billion dollars to build this nuclear reactor? It's going to take five years to get your profit back. No investor is going to be like, yeah, that's a good idea. That's the main reason why we can't get nuclear up and running. If for some reason something happens, you have to stop building your reactor. Like, there's no turning back, right? If you look at this chart over here, which is breaking up the cost of nuclear energy per kilowatt hour, I believe-- gigawatt hour? Kilowatthour.

ROUGE-1: 23.36, ROUGE-2: 21.29, ROUGE-L: 18.36
BERTScore: 55.89

==============================================
==================== [93/100] ====================
Summary:
Professor Steven Smith: I want to talk today about Aristotle's discovery of America. Smith: For Aristotle, as it is for every student of politics, the most serious issue one confronts is the problem of faction. He says the best regime is one where the best men rule, he says, a kind of aristocracy or an aristocratic republic. Smith says Aristotle's proposal for a mixture of oligarchy and democracy seems, in many ways, to anticipate, 2,000 years before the not and cannot deliver. Aristotle's Politics is a book about the kind of knowledge requisite for that kind of skill. Aristotle understands the mixed constitution as a balance of classes--the one, the few, and the many. He doesn't so much insist, as you will see in your reading, on the actual separation of functions of government. It is the skill possessed by the greatest statesmen, you might say, the fathers of the constitutions, as it were, who create the permanent framework in which allows later and lesser figures to handle change. Aristotle understood the importance of the separation of powers doctrine and the balance of factions, he says. He clearly understands, in many ways, the virtues of private property and of commerce, he writes. But the aim of the city is not wealth, is not the production of wealth, he tells us.that is the highest priority. Individual freedom may be, at best, a desirable byproduct of the Aristotelian mixed regime, but individual freedom is not its defining or principle goal. people par excellence--the Phoenicians would be the best regime. Aristotle could never endorse the view stated by a famous American president that the business of America is business. The political partnership, he says, must be regarded for the sake of noble acts performed well. These political clubs or parties use their influence to incense the populous, using their power to whip up dangerous passions that tend to make American politicians closer to demagogues than to statesmen. He would also regard the peculiar American practice of elections, rather than the Greek practice of appointing political offices by lot. Aristotle offers a serious challenge to existing Greek traditions and patterns of political education, says Berlin. Berlin: The core of political science is the concept of the regime, of the politea. The regime, for him, is not one branch of human activity among others, it is the fundamental principle or ordering principle that makes all the others even possible, he says. Berlin mentions Bismark, Disraeli, Franklin Roosevelt, Pericles, Lincoln, and Churchill. They provide a virtual education in statecraft, he writes. The great-souled person is the ideal recipient of a liberal education, says David Frum. He says the gentleman is not a philosopher in the strict sense, but a person of inherited wealth. Frum: The gentleman lacks the speculative intelligence of a Socrates, but he has practical rationality. He calls this kind of knowledge, of practical judgment, by the term phronimos, that he has on the blackboard.. The quality that I'm going to quote is that which distinguishes successful statesmen from others, Frum says. animal means first to possess speech or reason that allows us to participate in a community or a way of life governed by shared standards of justice and injustice. To be a political animal, for him, is to engage or to be engaged in this ongoing conversation and debate over the very nature of justice. To refuse to participate is either to be below humanity or above it, he says. is not a theoretical subject in the matter of physics or metaphysics or mathematics. Its purpose is not knowledge for its own sake. Clarity within the limits of its subject. How much precision does the subject allow? How do we know? There will always, he suggests, appear to be something ad hoc about the methods used in the study of politics. We will have to let the method fit the subject, rather than demanding the subject matter fit a kind of apriori method. To insist on that kind of methodological purity, he implies, would be to impose a false sense of unity, afalse sense of certainty or absoluteness. the regime, to be objective and disinterested, as if it were viewing human affairs from a distant planet. Aristotle takes his stand from within politics and the regime of which he is a part. But we all know, do we not, that most contemporary political scientists tend to be liberals. Their values are liberal values. This raises a question. Whether the relation between contemporary political science and liberalism is merely accidental or whether there is some intrinsic, some necessary connection between them. One might do well to ponder which political science is really more scientific--Aristotle's, which is explicitly and necessarily evaluative. the back door. On Friday, let me just remind you, Il Principe. We'll study Machiavelli. On this very partisan note I conclude. On Thursday, we'll study the life of the Italian statesman and play a game of chess with him. We're going to see how well he can play the game of poker. I'm looking forward to it. I hope you'll join us for the game on Friday night at 8 p.m. ET on CNN.

ROUGE-1: 26.83, ROUGE-2: 23.96, ROUGE-L: 21.10
BERTScore: 62.13

==============================================
==================== [94/100] ====================
Summary:
SNLI is the Stanford Natural Language Inference Corpus-- MultiNLI, and Adversarial NLI. SNLI is a big data set. It has over 550,000 training examples and 10,000 examples balanced across the three classes. MultiLNI sets up, you are forced to train on those training examples. and then test on entirely new genres, such as Berlitz travel guides from the "9/11 Report," says Christopher Potts, an NLP expert. to the fact that the genre is kind of restricted. We had about 60,000 examples that were additionally validated by four other annotators. They had high interannotator agreement. So given that validation, about 60% examples had a unanimous gold label. And we rate the overall human level of agreement at about 91.2% for the gold labels. That's the measure of human performance that's commonly used for SNLI. But down here at 78 is the original paper. That was from an era when deep learning systems were really not clearly the winners. in the Atlantic Ocean." You might ask, of course, those could be true together. They should be neutral, not contradiction. The reason we call them contradiction is because we make an assumption of event coreference, that we're talking about the same boat in the same event. And for that reason, they get the contradiction label. If a premise and hypothesis probably describe a different photo, then the label is contradiction. That's kind of anchoring back into our underlying domain that you might have in mind. Overall, that results in that large data set. And you can see that in subsequent rounds, the model is going to be expanded to include previous rounds of data, in addition, possibly, to other data resources. So with respect to the best model for each round, the test set is as adversarial as it could possibly get. The model has gotten every single example wrong. And Adversarial NLI is exciting because it's given rise to a whole movement around creating adversarial datasets. SNLI and MultiNLI into Turkish. XNLI is a bunch of assessment data sets that is dev-test splits for more than a dozen languages. Those are human-created translations that could be used to benchmark multilingual NLI systems. So there's a wide world of tasks you can explore, and I think that makes NLI a really exciting space in which to develop original systems, and projects, and so forth. And those could be interesting for seeing how well a model can grapple with variation that comes in very specific and technical domains.

ROUGE-1: 27.88, ROUGE-2: 26.58, ROUGE-L: 23.35
BERTScore: 62.79

==============================================
==================== [95/100] ====================
Summary:
In a perfectly competitive market, the firm is a price-taker. No matter how many units they produce, they're just going to be able to get that same market price. A firm in an imperfectly competitive market will have their own firm-specific demand curve. The price that they get in the market is higher than the marginal cost and the marginal revenue at that point. Folks are willing to pay more than that marginal cost, but you still have no motivation to produce more.

ROUGE-1: 21.16, ROUGE-2: 19.73, ROUGE-L: 21.16
BERTScore: 65.43

==============================================
==================== [96/100] ====================
Summary:
"Between the Folds" is on PBS Tuesday at 8 p.m. ET. Professor: Today we're going to talk about a variation of generic rigidity. He says the idea is we think about rigidity-- does a linkage move or not-- to the first order. "I just wanted to say goodbye to Eric Joisel, who died on Sunday, sadly," he says. "Between theFolds" will be on PBS at 9 p.M. ET on Tuesday. In the book, we take the derivatives. We work out all the details. But I'll just tell you what the condition is, because ultimately all we need is this condition. What we care about is the nullity. This is the space of motions-- infinitesimal motions. And this thing is the number of freedom. And it turns out these will always be equal to d times n in summation. And what this is telling us is the rank of a matrix. In two dimensions, there's three rigid motions. Those we're getting rid of. vertex v and vertex w on the graph. This is a vector C of v minus C of w. The right part has no real intuitive meaning, but it involves this thing-- some velocity vector for w-- and the velocity vectors for v. So this is a linear equation. The fancy way to write this is as a matrix equation, if you want. So we have a whole bunch of these constraints, one for every edge. And whenever you have a bunch of linear constraints, you can write them as R-- some big matrix-- times the velocity. The set of all infinitesimal motions is the null space of that matrix, also known as the kernel of the matrix. If you have a matrix, you take its rank-- which is another quantity-- and you add them together, it will be the number of columns in your matrix. In two dimensions, that is 2n minus 3. So this is telling us something we already knew, but essentially for free, which is kind of fun, if you know all this linear algebra. of generic, which was that you forbid all non-trivial polynomial or algebraic rational equations on your points, which is very messy and infinite and hard to think about. OK, for this definition we need the notion of a minor. So there's a whole bunch of minors, but it's only finitely many. It's exponential. You take all the minors of your rigidity matrix. And here I'm imagining the complete graph, so there's tons of edges. There's n choose two edges. So this'll be independent linkage. Tensegrity is a generalization of a linkage and where we allow three kinds of edges. We can have cables, which can decrease, but it can't increase, and there are struts, which are the reverse. All these struts are preventing is compression. They don't prevent expansion. So that's the thing we want to model. This is a tensegrity. It's a geometric condition, if you will, that each vertices is stationary. That is the notion of being in equilibrium. This is a tensegrity. What I want to prove is that this is flexible. If I can prove that it's always flexible, well, until everybody's straight or convex-- all the outermost are-- then I have proved this. So we're going to prove this theorem in, like, five minutes-- easy. It even gives you an algorithm for doing that, which we'll talk about next. We use just, is this thing rigid or flexible at all? And that's the power of all that's just telling whether something can move. dot product, and I claim this is really measuring-- it's a number, and it's measuring the sign's change in length of that edge. So if C of v minus C of w dot product with d of v plus d of w is greater than or equal to 0, that's the property I want for struts. That says it gets longer or stays the same to the first order. Now of course, in reality it might get slightly shorter, but not in the first derivative. All you need to know is it there are fast algorithms to solve this, also. The equilibrium condition is that if I take the sum over all vertices w, where v w is an edge, the weighted vector C of v minus C of w-- so I forgot to mention we're assuming we have some configuration of our tensegrity.bar, I don't care-- could be positive, or negative, or zero. Let me draw a picture. I'll draw a real example, how's that? Here's a rigid linkage. I'm not going to say yet which edges are struts and cables. In some sense, I can do that later. A spiderweb is something where every edge is basically a cable. All of the edges can have a positive stress, and it will be in equilibrium. In spiderwebs, in fact, this is the same-- these two-- rigidity and positive stresses are the same thing. Here's a little example of an origami tessellation, this fold by Jenny [? Ramsier ?], just the one I had hanging around in my office. Let me hold it up to the light. It looks pretty cool. origami tesselations since the mid '90s. Chris Palmer revitalized them, and then tons of people are looking at them. And there's this cool algorithm for building them. So you take some tessellation like these squares, and triangles, and hexagons. You shrink all of them a little bit, but all by the same amount. And then you just connect the dots. And that's your crease pattern. Now the question is, when does this work? This algorithm has been around forever. Professor: I can define what's called a polyhedral lifting. Polyhedral lifting is going to be an assignment of z-coordinates to the vertices. Because I forbid crossings, I end up decomposing space, and this only works in two dimensions. All I need to prove is that every polyhedron lifting is flat, so we're going to prove this theorem by contradiction. Student: What if you have one of those diagonals and switch it to be in relation to each other [INAUDIBLE] thing you can do-- there's some freedom here. If you have any lifting or, for example, you could not lift them at all. Set all the z-coordinates to zero. That's fine. You could also just lift everything onto some other plane, and generally have a rigid motion of freedom. Now, this is a lot like stresses-- in fact, it's identical to stresses. It's not a big connection, but it turns out they really are the same thing for non-crossing configurations. A linkage is locked if its configuration space is disconnected. If I have some configuration here, I can move to any other configuration here. But there's no way to get from this configuration to that configuration. They're disconnected from each other. So this would be kind of sad news if you're looking for motions. It means you cannot get from everywhere to everywhere. There are some locked configurations that cannot get back to start. The Carpenter's Rule Theorem is about one case of that. It says that if we have a linkage, and say we take a configuration of that linkage, then there's a motion of the linkage that straightens out. For convex cycles, it's a little less obvious, but it's also true. There's one catch which I didn't say here. I need to add outermost. When you have nesting like this, you're in trouble. This guy is not going to get straightened out. It could be super long. It may not have room to straighten out inside that convex chamber. So these guys will come along for the ride, but they won't actually getting straightened. The outermost guys will get straightening and convexified. why is it called the Carpenter's Rule Theorem? Because this is a carpenter's rule. So if you've never played with one, this is what people used before flexible measuring tape. They still use them a lot in Europe, but not so much in the US, but you can still buy them at the hardware store. That's it. So there you go. It's just like a carpenters rule. But as far as we know that doesn't make this theorem any easier to prove.

ROUGE-1: 22.93, ROUGE-2: 21.68, ROUGE-L: 19.23
BERTScore: 64.85

==============================================
==================== [97/100] ====================
Summary:
Chess has been known as a tool of military strategy, a metaphor for human affairs, and a benchmark of genius. The game was originally known as chaturanga– a Sanskrit word for "four divisions" With its spread to Sassanid Persia, it acquired its current name and terminology– "chess," derived from "shah," meaning king. Chess-playing computers had been developed for decades, but Deep Blue’s triumph over Garry Kasparov in 1997 was the first time a machine had defeated a sitting champion.

ROUGE-1: 21.34, ROUGE-2: 20.34, ROUGE-L: 21.34
BERTScore: 63.36

==============================================
==================== [98/100] ====================
Summary:
presenting okay share your screen that's what I'm doing oh you are okay hopefully it is yeah stop sharing yeah you should be sharing my screen under your camera until I can decide if I click on slideshow this is still show my camera uh it does I guess I can minimize it do screen sharing are you recording too yeah great baseball back yeah I mean it's my first time giving my lecture so I'm as good as I can be do you want that cheers I mean I have to write something to hear me okay okay you know that it works. Deep learning is a type of machine learning that combines feature extraction and output prediction. It allows you to automate the entire process from feature extraction to training. Post-processing is a way to make the process of learning more efficient. Deep learning has been used in the field of Vision and Vision Vision is not as used as machine learning is in other areas. It gives you some context as to why deep learning is being used and why it's different from classical machine learning. It's a way of taking the entire machine learning process off of one step and combining it with another. Getting these features is pretty easy because you can just take each row directly right or you can also maybe take a column depending on however the data is arranged in this tabular format. What if your input is something complex like it can be text audio images right how do you extract all the relevant features from such a complex input? Since this is a CV class I'm going to go over the CV example and turns out that there are special feature extractors for images so this is sort of what classical machine classical CV look like. age right over here we have a neural network that whose goal is to sort of predict something about a car. As you go deeper down the network is trying to become more and more abstract. depth refines representations you start with course information like edges and you go all the way down to like find information like this mental model of a car so I know that this was a lot of information does anyone have any questions about any of the any of this uh in case nudge I'm gonna pass the my country Verona who will talk about transfer learning. Pre-trained networks are trained on very very large data sets and oftentimes again more data means better representations so if we're only given a little data using some sort of pre-trained network can be a great idea. There are also a lot of free trained networks that we can use immediately so you can use models that haven't trained on largeData sets. Pre-compute networks can also be used to fine-tune the whole pre- trained network rather dimensional 28 by 28 dimensional image. Transforming learning is especially huge in NLP. Without transfer learning we're basically trying to learn two separate tasks and train our model separately. Transfer learning allows us to apply the knowledge that we've learned from one pre-trained network and apply that to the second task. We've talked about two main techniques for doing that freezing some layers and fine-tuning our our Patriot Network. We'll go into some details and examples of this in action but especially in natural language where there's some semantic meaning so you already know the meaning. have more confidence that we won't overfit if we fine-tune so we've briefly talked about overfitting in the past and this is a topic that will continue to come up when we train um uh an hour and so um on the other hand if we have a smaller data set even though it's similar to the original model it's not a good idea sometimes to fine tune because you can definitely overfit. In practice it's mostly beneficial toinitial your weights from the pre-trained patient model okay so um before we sort of get into embeddings um just some practical advice. of your pre-trained models um think about what sort of tasks it was sort of trying to accomplish in the beginning um the data that it was trained on and then also consider your learning rates so we'll talk a little bit more about learning rates and greater depth again next week when we talk about some of these more particular models. No network embeddings are useful because they can reduce the dimensionality of your categorical variables for example and meaningfully represent them in the transform space. In this photo example they've decided to use teasney for thedimensionality reduction so taking the embedding Vector dimensions and mapping them to a 2d space in this case. part of our neural network for our Target task so this sort of allows us to get in a bedding that's nicely customized for our particular task but it may take longer than training the embedding separately. The idea here is that we can basically reuse our embeddings for other tasks again embeddins they're sort of just like what we talked about earlier the broad ideas that we're trying to represent our data in meaningful ways. So here's sort of an example of how you can sort of see how there's different ways like we have um one hot Target probabilities. of words once you've pre-trained you can understand syntax so you can imagine in tasks for natural language processing it takes a lot of time if you want to retrain an entire network so using pre- trained networks and using transfer learning is a really really good idea. Not only does that apply to NLP it applies to almost other domains every other domain xcp yeah okay so we will transition to the next part of the lecture which is going to be on self-supervised free training. Even without any labels we can still learn something meaningful about the structure of the raw data. We can do unsupervised representation learning and indeed we can. The valency provides representation learning is done is to play to something called self-supervised learning. If you want to label a billion images you would have to hire manual labor. The process is called supervised learning like the name suggests and some examples of supervised learning can be your typical classification problem like the one that we just showed where you classify digits. hope is that if the model can learn to predict the original order like what patch goes in the let's say the top left corner versus the top right corner it is trying to learn something meaningful about the image. It's going to learn that an image can be made up of different parts and those parts are going to be related to each other. There are some more technical details on the slides um I won't go into those but something that the authors actually did I actually go mention this is when they sample the patches and they divide it into a grid instead of taking the grid directly they actually generate each patch a bit. this can be done is you could predict a word from its surrounding context so say if you have the sentence the dog with the man you could try to predict the word bed from dog and dog because a dog should kind of imply that the word bit is associated with it so this approach is called a continuous bag of words model. There are many other ways to train virtual back models one example is um skip Ram so instead of predicting aword from a context you instead predict the context from a word so you like kind of like flip the model upside down.

ROUGE-1: 26.26, ROUGE-2: 24.51, ROUGE-L: 22.51
BERTScore: 66.06

==============================================
==================== [99/100] ====================
Summary:
The third lecture on Foundation mulative AI will cover chat GPT. Chat GP was the the tool or the the AI that really made people understand this is different now. Next time we'll talk about stable diffusion image generation and then we will talk about emerging Foundation models basically Foundation models generative AI in the commercial space. The lecture on AI ethics and regulation as well as a panel will end with the lecture onAI ethics andregulation. The final lecture will be on the bet that open Ai and Ilia the head researcher did in terms of what actually would lead to CHP and how in hindsight it might be quite easy. GPT relies on a lot of tricks and Engineering insights and breakthroughs that we're not going to cover here. We have a robot or AI model here that's able to take a prompt and an answer and then give it a score like let's say between one and five and say how good is this. What we solve now is that we we know what's good or bad dialogue. We spent 99% of our time in computer on on this preaching this Transformer to predict the next word based on previous words. make very high level um statement but of course the nuances matters and I think it's quite interesting uh I took this quote from a general from the 18 and 1700s and he says this quote that P Theory which sets itself in opposition to the mind and what he meant was that he's a general so he fights in battles and War and at the time people loved to come up and theorize around War like we should have certain rules and how soldiers should behave in fighting and stuff like that but he's like well I've been in War uh and Wars don't comply to rules first off. Genty pre-trained is the self supervised step of how you train this and arrive at this model and then the Transformer is the basically the engine behind it in some sense. Transformer has much less and has to relearn a lot of this data but since we have so much data and we can afford that we can train on a scale that we haven't seen before so that's also why this works so well so now we have a language model that can train this model. We're just going to directly incorporate information from I and when to the Target. know we know or the computer knows somehow by just downloading the text that what this whole sequence is but when it trains this AI model it hides part of it right so it just inputs I to the AI model and then it's supposed to do something with it. So you you basically would allow the model to uh guess and then maybe it's off right and then you can give some negative feedback uh and then when it gets it right you can given some positive feedback right so this is the high level uh what we want to accomplish. We're going to create scores or predictions for all words in the L like in the human you know vocabulary in the English vocabulary that sounds extremely expensive and it is quite expensive and so I have different tricks to make this work. The first version it was using 175 billion parameters and just training the the final model cost around $5 million just in in Compu electricity bills right that's how huge and much compute they spent on this and uh again so it's a very very simple approach but it's  a certain scale that's that's never been seen before. a good job you also wants to able to incorporate the previous word and the features from there so you kind of also processes and includes into the second Vector both the previousword and the current word to create a new representation of the whole sentence so far. Then it uses that to to kind of predict the the next Target okay we go on and we do the same thing um and uh of course we do this for whole sent and I think what's this sounds maybe trival but the thing that's important to notice here is that for every step here that's label with the same uh digit you know they can all be done in parallel. these are extremely extremely popular and a version of them called uh lstm long short-term memory networks um it performs really really well and some people say it performs you know almost better than Transformers a lot of times but they just take them longer to train because we're going to realize why it one a point but they work very really well. For new networks memory is very hard so it's hard for networks to remember things so let's say you know that that if you read a book or you watch a movie if you want to understand the end part it might be good to kind of go back and look at the the start starting part of the book or something. in parallel it's the same step and this is true for all of these steps yes this sense when we compute a output distribution over all the words later like as a prediction we talked about the first thing. The training is the most expensive part uh because then you op you optimize and and you do back propagation to update update your parameters which is very very expensive when You' when you kind of uh when it's done you freeze it and you don't update things more and it's going to be much faster to run. more data to start doing a good job but there's also another thing that we're uh you know really forgetting here right so in a recuit network things are processed one at a time so the the model can figure out that you know Financial comes after the because it sees the first and then Financial but in a Transformer you know in the below here like if the only thing you see is the word and they're all F to you know for for if you look at the prediction we going to do at at step number nine if you see all these words the same time right there's some kind of comp like you can permute all the words and you basically see the same thing so there's no sequ. The model now has been trained on a vast amount of data from you know any Source on internet you can imagine right so novels Wikipedia Facebook posts. How you how users are going to use this is through some chat bot right so it's dialogue like human dialogue is what they call it. We want to be able to hone in and focus and adjust itself a little bit by training only on human dialogue so we going we going to go and collect the best data we have of human dialogue from whatever source that we have. about is to give the most likely next word but if you want to generate a sentence right you don't really care about optimizing the likelihood of the next word you you care about the accumulated likelihood of your whole sentence. So somehow we're too greedy we should be a little bit more long-term optimizing. The more the longer sequence and the more context or the longer prompt you have the more specific prompt the more Peak your distribution will be because the more information the model have around your specific context in use case the more it knows how to collapse into a space. is where we're going to do reinforcement learning from Human feedback uh and that's what open AI does on chtp and this was very very hyped for a long time but now people talk less about it uh okay so what do we do well we have a great model that's been fine tune on dialogue and it's able to generate really good answers still uh to different prompts so uh we'reGoing to run this model now on a collection of prompts and we're Going to generate four. delayed feedback so uh in reinforcement learning we're going to have our our starting point of a really good model but we'regoing to allow it to start generating things right it generates a word puts it in its own input and it reruns itself so it becomes a longer longer sequence one over at a time so we start off with this I and we now have a probability distribution and we decide what to go for next and we go with went and then we have a few options again and we we take a next step two and again there's no at this point there're no feedback we don't know if we're doing a good job. gratification actually leads to very non- GRE and and independent robustness so these are the consequence of applying reinforcement learning where you only get feedback at the very end so there's less you know supervision right you're more on your own and you have to H deal with an uncertainty of not having constant feedback. Okay so we've solved our problems uh we used human I mean we actually paid human beings for label data which is not maybe that goes against our principles here but but but uh we. Collaboration with neuroscientist in deep learning is is quite quite rare actually. If you look at Transformer Works uh if you mask a word uh then you will like then you'll only um okay I if you do this you know if only predict the last word based on previous words.and then it it just like oh this works now and they are not prbly completely conscious themselves about what they were inspired by you know so does that mean that we not have collaborations neuroscientists yeah. Lang models are very humanik even it's in its mistake they're like well they're somehow they're biased and they have stereotypes around things. They suffer from wishful thinking and some type of imagination where they rather be you know make you happy than being completely truthful. They do like more expensive than or like this is like the best um yeah I think so what are some what areSome challenges of these large Lang models well uh one challenge is to uh make them behave like we want them to. Reinforcement learning techniques of how to do planning well so how to incorporate planning is something that people talk about a lot and then of course multimodalities. It's not hard to see that this idea of predicting next word based on previous words corresponds really well to videos just to kind of predict the next frame based onPrevious frames. Of course it's more expensive again because it has to run for longer and stuff but it's very very useful. of course it is more expensive because it's a high dimensional picture or image but clearly you can learn a lot about the World by looking at videos. the text part and get a multimodality model that's able to do both in a really really sophisticated way uh also something that I think these these people are working on all right thank you thank you for your time. The next step is to get a model that can do both the text and the video part in a way that's really sophisticated, I think that's something that we're working on right now. Thank you for all your time and I'll see you next week.

ROUGE-1: 28.68, ROUGE-2: 27.49, ROUGE-L: 24.02
BERTScore: 65.54

==============================================
==================== [100/100] ====================
Summary:
The EM algorithm is an unsupervised version of the k-means GMM algorithm. In GMM, you guess randomly an assignment of every point to the cluster, the probability. You guess where they're probabilistically linked, that is, what's the probability of these points belong to cluster one, this point belongs to cluster two. And then once you have that, you then solve some estimation problem that looks like a traditional supervised learning. The decomposition is quite important. And we're going to try and kind of abstract that away. CNN's John Sutter takes a look at an algorithm that maximizes a likelihood function. The algorithm is called EM and is based on convexity and Jensen's inequality. Sutter: "If you understand this in the simplest way, then understanding the algorithm will make a lot of sense" "This is the rough algo on the thing I'm just giving you," Sutter says. "I'm not giving you enough math because otherwise, the math is kind of bizarre-looking" In the next lecture, we're going to see a different example, which is called factor analysis, where z has a very different form. And it's meant to constrain the problem in a different way. We went through in this lecture a couple of different steps. We started with that convexity piece so we could get an intuition for what these functions look like. And then we went through the EM algorithm, in the near future. And I think that's all I want to say. Any last questions before we head out? A function is going to be convex if its graph is, OK? As I said. OK, so let's draw an example of this. So here's 0. Here's minus 1, and I draw this character. So this is f equals x squared. It's a parabola, kind of a here. I pick b here, and the line between them is below the set. This is not convex. So let's look at this in symbols. Does that make sense? Just translating the definitions directly. Amitai Etzioni: I'd much rather answer these than badly draw the pictures that come next. We're going to get to those pictures no matter what, so there's really no saving us. All right, so I'll try and leave this more or less on the screen. Here's the algorithm. This is a picture, which maybe won't make perfect sense to start with, but we'll get there. I hope this is OK for people to see. If not, let me know. Great question. So what's going on here if you remember Taylor's theorem is you can keep expanding, and then you have the last term, which is the remainder term. The remainder term says, there exists some point that lives in a to b such that this holds with equality. By the way, this is really not important for your conceptual understanding. The real reason we want to go through the derivative thing is, otherwise, this thing, which we actually do care about, is strongly convex. Jensen's inequality says that no matter how I pick the parameters of that curve, anywhere that lives on this thing, that's a probability distribution, a bunch of numbers that sum to 1 in the discrete case. And that's going to allow me to build a lower bound for my function, and I'm going to hillclimb using it. We're going to use that in a second to draw some curves of a likelihood function that will hopefully be easier to optimize than the original function. Now, everything is defined in the literature traditionally for convex. If you take convex analysis, it's the way we define things. We actually don't want to use a convex function here because we're maximizing likelihood. And this is just notational pain, right? Like, if we were-- maybe we should have minimized unlikelihood. So we need concave functions. And what are concave Functions? g is concave if and only f minus g is convex, all right? So we flip it upside down.  EM algorithm has max likelihood-- I'll actually put MLE. MLE is a way for us to compare different parameters. We have some data, i from 1 to n. These are our data points. We take a log of the probability that we assign to the data given our parameters. And recall, these are the parameters, params, and this is the data. So far, so good? All right, now, we're working with latent variable models. P(x; theta)-- this is a generic term, right? This is just one of the i terms-- says the function factors. what is probability of x parameterized by theta actually represent in this case, in that photon example? Yeah, exactly. So P(x; theta), we've been using forever. We used that from the supervised days. We just inserted z and said, well, there's this wild z that we can't observe, but it somehow constrains x. It means that x-- like, the relationship between theta and x. And that's what the model does. The algorithm starts with an initial guess. It's always below the loss, so it's kind of a surrogate that I'm not overestimating my progress, and it's tight. Then we maximize that, and this is formalizing the back and forth. We take that new maximum that we have, which is our new best effort of parameters, optimize that thing in a second, pick our new theta(t) then repeat and do another curve that's present. It kind of looks like a supervised thing. So you're picking these concave kinds of functions, which are easy to maximize. curve, L of t. And then the M-step, and together, EM, given L of T, set phi t plus 1 equal to argmax phi Lt(phi). Cool. Just could you reiterate? Like, why are we not using gradients on the original turbulence? Right, so we could imagine doing some kind of gradient descent here, but it's not clear how to deal with this marginalization that happens in the middle. So if we did some marginalization or some sampling, we could do something that looked like that. LZ: How do we construct L of t? LZ: I claim we know everything else. So let's look at a single term in our equation, OK? So I'm going to grab one of these characters, just one, and work with that. So I want to pick Q such that log P of x, z; theta over Q(z) equals C. Now, before, I had all kinds of freedoms so on? You could also-- we'll see later-- replace it with an integral if you had something really fancy. have some theta at some time. You know the data point that you're looking at. And you say, what are the most likely values of the cluster linkage-- as we were talking about before, the source linkage-- for this particular point? You get a probability distribution over those. You set them to Q(i)z. That's really what's going on. It's your estimate of how likely that is. Then you take an M-step. Theta t plus 1 equals argmax over theta of Lt(theta), which equals-- so Lt( theta) equals this ELBO sum. This is an x. This is a Q, this is a theta. Do you use that equation [INAUDIBLE]?? Right. So it's just as before. t starts at 0. We have that initial guess, and then we go from there. Theta is our current guess. Cool. All right, why does this terminate? And it's basically for something that's kind of not very interesting or satisfying, but it does. This gives you a sequence that is monotonically increasing or nondecreasing. we wanted to understand-- what was the probability? This says, the probability that i, the i-th component, belongs in j given what we've observed about x(i) and what we know about the cluster shapes and their frequencies. So if you remember, we had this diagram that I drew quite poorly the last time that said we had these two bumps, which were our two Gaussians, let's say, in one dimensions that looked like this. This is a Gaussian distribution with center j. And the question is, you give me a point here. How likely is it to belong to 1 or 2, to cluster1 or 2? Right? That's basically what we're asking. phi 2 was hugely bigger than phi 1, right-- a billion points came from the second source. So to automate this, this is Bayes' rule. It just weighs those two probabilities and tells us what should happen. That's it. We ran through exactly those calculations last time. All right, let's take a look at the M-step now. We have to compute derivatives. So we're maximizing here over all the parameters, phi and mu and sigma, sigmas. always use brackets for this. It's historical, and I would love to beat it out of myself if it were possible. Does the covariance depend on j? Right now, the covarianance does not depend on z. So it depends on j. All right, so now we can compute some fun derivatives, OK? So let's compute mu j of fi of theta. We have to estimate the mean, right? Now. Let's do it-- actually, I'm going to do something slightly harder. It'll be just one extra line because it's all linear. Yeah, w doesn't have anything to do with lambda j. So it should be crossed out. Is that true? Let me show-- let me see something crazy here. But it will be-- sorry, I see what's going on. This is 1/2, and this is a minus. w(i)j is taken the derivative of the log. So when can we pull this thing out that's repeated? Because it's full rank, we can pull it out, and it's linear. So we want to set this to 0 and use that sigma j inverse. you what happens in phi j. Would you mind showing [INAUDIBLE] scrolling up? Sure. No, wait. I just want the last one. OK, sure. Also I would say, ahead of time, I do post all the notes online. Please feel free to take our reference to those notes too. They will have potentially fewer typos than me trying to answer questions, draw, and generally be distracted. Can't focus that long. They still do have typos, though, so always look at the notes. It's along the line, OK? So the question is, how do you encode that information that you want to kind of screen off information that's orthogonal to the line? And I'll write up a little note to show this whole thing. What you do is you introduce this thing called Lagrange multipliers. And if you haven't seen them, don't worry. These are super easy to teach. Just say this-- it's just an extra term here. And this multiplier is basically the thing that screening off things that are orthogonally to these constraints. It's just doing the average, which was weird to look at before. But that allows us to compute all the things in the way we would expect. Here, it's totally natural. In this case, it makes total sense, though, because these numbers have to sum to 1. So if you don't have a normalization constant here, you're adding up a bunch of numbers which individually sum up to n, right? The sum over all of them is n. And this is just the principle that tells you, you have to normalize them by this n factor, OK? which we formalized as kind of back and forth with using these curves over time. Once we had those curves, what was happening was we would pick and optimize on those curves. The Q(i)'s played a starring role. Those became our w's here, and they kind of add nastiness to all of the equations. They just add little weights and expectations everywhere. And then we ran exactly the kind of standard supervised machine learning, if you like, or the stuff that you've been doing for MLE for the entire quarter.

ROUGE-1: 23.97, ROUGE-2: 23.08, ROUGE-L: 21.48
BERTScore: 63.38

==============================================
