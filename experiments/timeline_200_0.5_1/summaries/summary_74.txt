This lesson will first dive into some signal Theory and then move on into things that we're more familiar with things like deconvolutions and using Transformers for next note prediction. The first thing we want to talk about is how can we sample and quantize a continuous time signal. We'll then go into some geometric signal theory and with Transformers and finally how how we can kind of generate sounds using these. The lesson will end with a soft introduction to digital signals and then we'll move on to the next lesson. Not very easy for a computer to do given that every operation needs to be in on a continuous time signal. The way that we we can fix that is through the process of discretization or to to make a an analog signal a digital signal for us to be able to process. The two kind of main ways that we can make our signal easier to process is one by taking samples at certain time periods and two by quantizing our level so instead of dealing with A continuous scale we can quantize at certain levels. voice and Pitch it up very fast right what quantization level do we do we want there. Can we do a lossy pitch up with a uh with by filling in the the blanks in some intelligent way through prediction or kind of note fitting which is an interesting consideration I think given the fact that audio is a continuous time signal um the digitization process and the choices you make matter a lot and because of that this field is so interesting and there's a lot of really didactic work around how we can take these continuous signals. The aliasing phenomenon is incredibly interesting this happens both visually and um auditorily. aliasing is the byproduct of poor sampling. A lower wave resolution will result in a modified output signal as compared to the original input that we're trying to process. There's a lot of literature about um aliasing effects including spatial illnessing. The effect of aliasing on digital signal processing can be seen in a number of different ways including in the way that the image is compressed and the way it's displayed. We're able to gain information through this uh this unit process so the way that this works is um our down sampled waveform is is initially sent through eight down sampling blocks. At each layer the number of filter Banks is doubled. We're not losing information but we're developing a new representation of these images. These kinds of techniques are used in a variety of ways to reproduce music from bands in the 1900s. We want to use Transformers for audio we're going to kind of transition now into the next generation of audio we can use Transformers to predict the next note for example. the better your model will be and the more generalizability you have in your Transformer the better it'll perform right we talked about Occam's razor. A generalized Transformer a generalized solution can fit the Goldilocks of what we want in a model. It's easier for machines to predict keys without flats and Sharps um which has you know similar to what humans do it's easier to focus on the regular keys on piano um so this specific example was trained on on those um however the glass andSharps with specific augmentations and specific training processes can also be added to our vocabulary. example um this specifically I believe this uses like a music 21 framework but it's able to you know tokenize a certain item and then you're able to transpose this to a certain amount of notes or a certain key. Just by transposing you'reable to increase your your training sample size which is very cool and can improve model performance by a ton. The next thing to consider is positional beat encoding right we want to include some metadata to feed into our model to give it a better sense of musical timing because the position of the token in our our tokenized representation it doesn't correspond to its actual position in time. Then it won't really learn a a very good representation of the data that you're providing right we want to be able to mask information that it previously had as well as mask info that it'll have in the future. We want to apply an attention mask to keep the model from peaking and essentially leaking information at the next token it's supposed to predict we can do this by kind of observing this model right um so here we at each at each step the model can only see itself right at the first step where zero is a token you can see one is a tokens you can't see. things a try yourself uh yeah thank you guys for tuning in have a good one. Things you might want to try yourself. Things that you may want to give a try. uh yeah. things you might be interested in trying yourself. things that you might like to give yourself. uhYeah. things a try yourselves. uh Yeah. Things a try themselves. Things your might like yourself.things you may like to do yourself. thanks you guys. for tuning into this week's episode of The Daily Discussion.