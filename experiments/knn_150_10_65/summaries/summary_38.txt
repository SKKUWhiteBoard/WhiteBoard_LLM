So I think I just spend like five minutes, just briefly review on the backpropagation last time. So I didn't have time to explain this figure, which I think probably would be useful as a high level summary of what's happening. This is how you define network and the loss function. So you start with some example x, and then you have some-- I guess, this is a matrix vector multiplication module, and you take x inner product multiplied with w and b. And then get some activation, the pre-activation. And you get some post activation. one of those three lemmas. If you know how to compute the derivative with respect to the output of some module, suppose this is a module, tau is the output. And now you can see what this does kind of like lemma are for. Those lemma, basically, are saying that if you know dg over the d tau, how do you computedg over da? And there's another lemma which says that ifYou know how. to compute dgover da, howDo you compute dG over dz? All of those lemma is about this kind of relationship. Some of the practical viewpoint of ML, like how do you relate to your model, what you have to do in this whole process. So far, we only talk about training. We have some examples, which we have seen when they are training data sets. And often, this is called test distribution. And then you evaluate what's the expected loss on this new test example. So this is a typical situation of overfitting. In the next 20 minutes, I will spend 20 minutes to talk about a new picture that is actually challenging. time. When you test, you find that your model is not as good as you thought before on the training set. Sometimes, it's probably a little worse. But generally, you shouldn't expect that your test performance is dramatically better than the training performance. So we're going to discuss what will happen if you change your model complexity, and whether in what cases, you may underfit. In what cases you may overfit, and what is the best response to that? So then what you-- In some sense, you care about two quantities. You care about the training loss and the gap. You want both of these two to be small. And typically, when l theta is big, there are two failure mode in some sense. One of the failure mode is called overfitting. The other is called underfitting. And whether you are in the overfitting regime or the underfitting regime, depends a lot on different things. And one kind of decision we are trying to discuss today is that what is the right model complexity. Bias is the best error or loss, you can get with even infinite data. Mathematically, one way to define a bias is that you can say this is the-- So bias is-- I guess, actually, there's some approximation here, depending on what exactly your model is. So the bias is something like this. And now let me talk about the variance. And here, there is -- I'll come back to the variance for this model. But here, the variance is, in some sense,you can say, it's not very important. Only the bias. OnlyThe bias is the way if I were to prove that test is equal to a bias plus variance. Bias is the distance from the [INAUDIBLE] model or something like that? I think that's pretty much-- so for this case, they pretty much are the same. So the bias, the trade-off, depends on, for example, how many data you have as well. So this bias and various is not don't think it's required for the exam or anything, but it's a relatively simple word if you're interested. But the intuition is still kind of fun. So if you don't care about what exactly definition of bias is. In the lecture notes, actually, there are some visualizations of the real models you're going to fit. So for linear models, I guess, you can see a bunch of properties. There's a large training error, training loss or training-- let's call it loss just for consistency. What's your prediction on cannot be mitigated by more data, as I said. And actually, it can also not beMitigated by less noise, even though there is-- and by less data data. the training data set. This is your prediction for this x. And you look at the distance between the prediction and the true label. The training error is pretty big. So this is underfitting, by our definition of underfitting because the tuning is already big. And now let's think about so what you should blame. Why the training is big? What's the culprit? The culprit, I would argue, is that it's just because no any linear model can fit your data. It's not just-- no anylinear model can work. And it's not because you don't even have enough data. well. So when in this kind of settings things happens, like you have the bias. So the bias is basically like it's saying that the reason why-- I don't know exactly why people call it bias in the very first time. But I think you can-- see kind of the relationship. So you are imposing a linear structure, but the true data is not linear. So it doesn't matter how many data you see, as long you insist that I just believe that this thing is linear, you're going to fail. So suppose you have a million data, roughly. There's a little bit fluctuation, of course. So now you want to fit a fifth-degree polynomial. What happens will be that this is probably not entirely obvious-- OK. What you really will fit, like if you minimize the error on the training data with this so many training examples, then what you will get is probably something like this. Maybe there are still some small fluctuations. It's not like necessarily matching exactly minima [INAUDIBLE] spurious patterns are the fluctuations in some sense. And so in other words, I think you are explaining the noise instead of the ground truth. How do I formulate this? Like one way to kind of formulate this a little bit more mathematically is that you can consider to redraw the samples. So you redraw some new samples with different spurious patterns. They are spurious because they are noise. And that's a good question. That's exactly what I'm going to talk about next. Same distribution. From the same distribution. Yeah. So like if you collect more data from-- yeah. So how do you know that you are having a large bias? You cannot really exactly know. When you don't know ground truth, so all of these are so far are for analysis purpose. When we don't knows I didn't even tell you what this is. I'll go back to come back to this. Are we [INAUDIBLE] For highly imbalanced data set? Maybe let's discuss this offline. of bias and variance first change, did you use different type of model [INAUDIBLE] So I think this figure, so this is the-- OK. You ask a good question. So probably, the best thing is to use quadratic. Quadratic is, in principle, expressive enough to express our data. So that's why quadRatic has small bias. And also, quadratics is probably, among all models, the most expressive of all models. But where the trade-off comes from, where the sweet spot is would depend on the ground truth. The phenomenon is called double descent. This phenomenon actually dates back to something like 1990. People realize that if you increase your model number of parameters even more, at some point, you will see that it will be like this. And it turns out that, actually, in many cases, what happens is that the test error will look like this, or increase, and it will decrease again. This is the so-called double descent phenomenon. It's about less mysterious these days like after people have studied this in the last five years very carefully. For most of the vision experiments, you are in this regime, where you have more parameters than a data point. This is something that is really like empirically irrelevant. So that's why stochastic gradient descent for linear models. So the existing algorithms underperform dramatically when it's close to d. So both these two peaks are basically like this. It's just that you are changing the axis in some sense. So this is also when n isclose to d, when the number of data points is close to number of features. people really care about it. And what I mean by that is that even within linear models, you can try to change the model complexity. So what that means is that you try to decide how many features you use. So you can start with only using one feature or two features like for example, in the house price, where you can use the square foot as the single feature, or you can collect a bunch of other features. So keep adding more and more features. That means you have more. The peak is caused because the algorithm was suboptimal. The reason is that some random matrix is not well behaved when n is close to d. So regularization would mitigate this to some extent. But here, it really just means that you don't only care about training loss, but also you try to find a model with small norms. And you have some kind of balance between them. So you can sacrifice a little bit of training error, but you insist that your norm is small, then you can see this rise. and this model. So this model seems to have less parameter than this. That's by definition. The norm is actually very big. So in some sense, if you use the norm as the complexity, actually, these peaks have large complexity. So what is the right measure for complexity? So this is a very difficult question. Like for different situations, you have different answers. But there is no universal answer. But norm could be one complex measure. And for linear model, it just happens that for mathematical reasons, I think l2 norm behaves really nice. going to discuss this more next time. So the high level thing is just that something else is driving the norm to be small. Thanks. Going to talk more about this in the next few days. Back to Mail Online home. back to the page you came from. Back To the pageYou came from: Back to thepage you camefrom. Back into the page You came from was from: The Daily Mail. Back onto the pageyou came from, the DailyMail.com page you were from.