this week will probably go live tomorrow uh not quite sure yet but you'll try to get it up as soon as possible so I guess like without further Ado let's Jump Right In. I'm gonna go somewhat into detail into what representation learning is and I think this should sort of cap out the last few weeks of deep learning um and probably give you a more comprehensive understanding of what deep learning actually is doing. So I guess before we jump into deep learning let's talk a bit about shallow learning. In terms of neural networks embeddings are pretty important so they are often described as lower dimensional learns continuous Vector representations of discrete variables. Embeddings are useful because they can reduce the dimensionality of your categorical variables for example and meaningfully represent them in the transform space. The idea is that if we can directly work with the significant somehow find a way to represent this line using just one variable instead of three that's going to be better because a high dimensional data can be very complex. and that will have a homework so if you ever need to review uh the topics from this lecture so to work on that homework this slide deck should be up on the website again if you feel free to do that. That is it for today a second pause. Back to the page you came from. The slide deck for this lecture is now available on our website again. Click through the slide deck to see the rest of the lecture. The lecture will be available again on the site later this week. Aryan is presenting today's lecture on pre-training with Verona. Aryan is one of the facilitators for this decal. The lecture will be on Advanced Techniques and as software CV. There is a homework for this entire cluster which is high Crush notebook that should be due next Tuesday even though this lecture doesn't have any homework. There will also be a lecture on SSL for CV and that will be that there will be a slide deck for that. The slide deck should be up on the website again if you feel free to do that.