"Chat GP" was the tool or the the AI that really made people understand this is different now. So hopefully after this lecture you'll you understand kind of the basic idea and also somehow understand the BET right the bet that open Ai and Ilia the head researcher did in terms of what actually would lead to CHP and how in hindsight it might be quite easy but it was a really daring bad not obvious at all at the time that this would actually work out. "Chat GPT" is the third lecture on Foundation mulative AI. of fun and just to quickly go through our course schedule as well a little bit right so today is January 16 uh and next time we'll talk about stable diffusion image generation and then we'll talks about emerging Foundation models basically Foundation models generative AI in the commercial space H we'll have two guest speakers and thenWe'll end with the lecture on AI ethics and regulation as well as a panel okay so what have we talked about before we started off H with an introduction a short high level intuitive answer to what is foundation M generativeAI. GPT relies on a lot of tricks and Engineering insights and breakthroughs that we're not going to cover. We apply this self-supervised learning where we learn without label data so we can get as much data as we want because there's no human being in the loop. What we get from this you know by learning from observation and learning from the data directly is a very contextual and relational understanding of meaning. We'll talk about something that's extremely engineering heavy in you know chat GPT. be useful for you without getting into all the engineering details but of course in real life those engineering details really really matters and are very very hard to get right and that's something that we won't really dive into in this lecture because that's just when you bring something up certain scale and you have to paralyze a lot of machines Etc and think about high parameters it's a whole science so it's not trival at all but it's kind of hard to teach in a course like this. Force things to comply to Simple Rules right it kind of abandons our ability to understand and compress what we're seeing and deals with that chaos directly. That's why AI is so powerful and so humanlike um so also like when I talk about this in CHP we try to make very high level um statement but of course the nuances matters. I think it's quite interesting uh I took this quote from a general from the 18 and 1700s and he says this quote that P Theory which sets itself in opposition to the mind. in War uh and Wars don't comply to rules first off so you know everybody has a plan before they get hit in face basically. So you know as people start shooting at you and you have this fog of War of you don't know what's going on there's no simple rules to help you there and also what he says this in terms of the mind he says like well actually he's realized by working with soldiers that soldiers and human beings our mind we're not good at acting according to rules that we try to memorize. Chat GPT stands for generative pre-trained Transformer and this is a I mean a good description of what this uh actually actually means. GPT can basically solve a really wide array of tasks for us anything that can be phrased in terms of text language it can. it can basically solved and now as well when with gp4 ET becomes uh it's able to handle multi modalities but it's it's extremely powerful so let's try to break this apart well first off what does this name actually stand for. is um and I think also if you look at the the two different three different concepts here they're also almost corresponding length in terms of how important and influential they are in making chat GPT work so chat part we we'll cover last it's the kind of the least important one in some sense H the Genty pre-trained is the self supervised step of how you train this and arrive at this uh model and then the Transformer is the basically the engine behind it in somesense and so let's start with this generative pre-train. so we have a sequence of words and and then we're just going to try to predict uh the next word based on previous words so let's say we have uh we start with i here as input. Then we want to someh predict the Target right so we know we know or the computer knows somehow by just downloading the text that what this whole sequence is but when it trains this AI model it hides part of it right so it just inputs I to the AI model and then the AI models supposed to do something with it. you can give some positive feedback right so this is the high level uh what we want to accomplish so the first thing actually that you start thinking about is well there's multiple you know giving a sequence there's just one ground Tru correct prediction there's still in this example just one single word that actually will follow but there's tons of words that are not the correct guesses so maybe you want you know you want to allow the model to make the best use of this example as possible so you can basically make a lot of guesses and you can you can give them information. then it kind of gets it right and then you give some positive feedback back. We're going to create scores or predictions for all words in the L like in the human you know vocabulary in the English vocabulary that sounds extremely expensive and it is quite expensive and so I have different tricks to make this work but you'regoing to make a guess and a score like a prediction for every word in English. Only one would be correct but it gives a lot of feedback as well because there's a lot. information knowing which ones are not correct. The model is in i12 it creates a distribution of all possible English words and then the score corresponsive likel of this word coming after the what is seen so far. You give this feedback to to the model it's called back propagation so you give some feedback through model it should push the score or the probability distribution for the correct one to be bigger or larger and they all sum to one so they actually corresponds to the the models guess at the likelihood of like the likely of a word coming next. then reduce all other ones so you know the next time it sees uh the same example or a similar example it actually does better and you know this is just one single example but you accumulate all of these directions and information across a batch of examples that you see at the same time. So it takes small small steps to getting a a better and better distribution and a more itic distribution of what word will come next given previous words and you do this in a batch on tons of examples and of course you know we have unlimited amount of data. like a a specific token that says we're happy until we complete a complete sentence for example. Like a a a token that say we'rehappy until we finish a sentence. You have to generate one thing at a time but of course training is is much faster because then you can just you don't need to generate and run on your own input. You just run look at the input that you get yet but here you actually have to look at. the prompt generate the next word add it and run it again so it's kind of expensive. we talked about this a little bit before and now you know if it's really good at predicting the next word based on previous words we can give it interesting prompts and it can start solving interesting tasks for us. If you try this for CHP right it does so it basically has killed a lot of different research Labs that focus on a specific task because now it does all of this really really well and I mean this is basically from a modeling perspective this is chdp in a nutshell okay. CTP was trained at a scale with an amount of data and parameters that we never seen before. Just training the final model cost around $5 million just in in Compu electricity bills right that's how huge and much compute they spent on this. It's a very very simple approach but it's it's a certain scale that's that's never been seen before and really that's what a big part of open eyes is to open eyes to new ways of looking at the world and seeing the world. bet and and the research there is like well you know we've been doing this language modeling for quite some time trying to understand Language by predicting the next word based on previous words and we're using it for certain things but I mean know very few thought and were convinced that if you just scale this up big enough it will become a multitask Sol and show humanlike Intelligence and that this actually really will work. People talk a little bit of this emerging abilities because also it's not linear right like you start adding and putting putting more and more compute and parameters. well we're just going to go all in and just make this bigger and bigger and big and and and then like in hindsight like maybe it makes sense but it could have been a case like it wouldn't work and then people like oh that's a stupid bet like why would you think such a simple idea and approach would lead to to such sophisticate intelligence but it did okay so we covered thetive pre-train part right so uh you know we've now said how we basically are going to uh train our model but how does this model look like like how did this kind of engine look like. Transformer part is extremely extremely important so there's a debate a little bit what was the most influential part of making uh CHP and large language model possible uh Transformer is definitely a significant part of it and and I'll let you judge for yourself but uh I think it's less important than the actual modeling perspective that we've we've come up with. In order to understand the Transformer we're going to start to thinking about how we can process sequences so text is just sequence of words. some intermediate embedding or feature here one and and then it uses the that in a second step to uh predict the next word okay we go on and thenIt uh looks at the second word so I I lookas went but of course to be able to do a good job you also wants to able to incorporate the previous word and the features from there so you kind of also processes and includes into the second Vector both the previousword and the current word to create a new representation of the whole sentence so far. is that for every step here that's label with the same uh digit you know they can all be done in parallel. This is key because in in deep learning we use this uh um computer is called gpus. If we can make multiple steps into single step in parallel this is a single cost. We want to run things in parallel as much as possible so here basically you know this be a cost of four because all these different numbers can be run in parallel uh so this would just be acost of four and then of course processing this whole sequence will be a costs of nine. these are extremely extremely popular and a version of them called uh lstm long short-term memory networks um it performs really really well and some people say it performs you know almost better than Transformers a lot of times. But they just take them longer to train because we're going to realize why it one a point but they work really really good and also notice here somehow that uh this was very very intuitive for researchers to say like well text we read text from left to right we process words one at a time and therefore our models should be able to learn effectively from them. things flow forward this in kind of this sequential way right so to get from uh you know for the information from I to go to the information prob being processed step number nine basically right when you want to predict the period has to travel eight or nine steps here to to to uh be used so let's think about this a little bit start we start off now in a Transformer which basically starts off the same way so we we let the first we know we just process the first word uh and we prict the next Target based on that. when to the Target so we kind of we're we're not going to enforce this quential structure we're just going to directly let the information flow from the previous word to the current word Etc right what we've seen so far. The important thing to notice here right before when we were processing things sequentially you had to wait for the previous step to finish to do the next step. Here you don't because here everything is processed independently. Every Target somehow has a a node or sorry an edge to the previous words so they can all be run in parallel they don't need to. to wait for anything they basically kind of re redoing all the work somehow for every step because they all have this added to the previous words so all of these steps now like none of the steps have to wait for each other right there's no independency they can just go Direct to the the the source and use that information and of course you know we do this for the whole sequence and um again to reiterate right so for the last Target all ofThese computations can be done in parallel right they they're somehow aggregated at the Target. in parallel it's the same step and this is true for all of these steps yes this sense when we compute a output distribution over all the words later like as a prediction we talked about the first thing know oh yeah yeah sorry sorry so exactly this this works only during training now right okay and I'll come to that actually later so this is onlyDuring training where we can optimize this way okay but that's also yeah that's a great question butthat's also like something that's uh in uh in deep learning we basically almost I mean the training is the most expensive part uh. and it's going to be much faster to run so it's much less uh uh well that's a modification but uh it's a little bit less sensitive in a sense. We care about both being fast uh and yeah I mean but somehow H this is going toBe much much faster. to train than a recurr network so you're going to get much much better performance and then the difference in deployment is less uh significant okay but during training we can do this because we not upend the words we just see them in the sequence. I mean what's the biggest difference somehow right one of them the top Rec Network looks very structured right this has kind of a strong bias of of processing things sequentially the bottom looks very chaotic and it looks like Ah that's just a lot of connections it probably is pretty hard to make sense of the sequence given that it's all FedEd you at the same time uh so it's may be surprising that one works better another one you know one that definitely kind of needs more data to start learning useful things. sees the first and then Financial but in a Transformer you know in the below here like if the only thing you see is the word and they're all F to you know for for if you look at the prediction we going to do at at step number nine if you see all these words the same time right there's some kind of comp like you can permute all the words and you basically see the same thing so there's no sequ there'sNo sequential structure and force and Transformer whatsoever. figure out by a small you know number how things are actually oriented so like if you go to the movies and you think in terms of of frames you can sit down and digest the whole movie in one second it's like super efficient but you're seeing all the frames you know flash at the same time and then after like in your own head you have to put them in a sequential order if it's useful as you know understand the plot of the movie which typically is right but there not like the transformer has to learn that implicitly. if you read a book or you watch a movie if you want to understand the end part it might be good to kind of go back and look at the the start starting part of the book or something you know or or it's good if you remember that information but probably you know if you don't remember you have to go back to look it up. In a Rec Network Rec Network because we're processing things uh sequentially here so to for something to be used like to for for information about the first uh word in the sequence. like since now we have St super plus learning we can train by just downloading text from the internet and there's no human being Loop the scale of data is so much you know so big that we can learn a lot by you know we can learning. Transformer has much less structure and has to relearn a lot of this structure but since we have so much data and we don't need to have labeled data we have we can afford that right. We can afford a train on a scale that we haven't seen before so that's also why this works so well. be efficient and work well so now we're just going to look at the F last part which is the chat part um so you know you you you train this model now that you call DPT 3.5 or something and now you want to turn turn into chat GPT right so we have a really uh good model basically we've done 99% of all the the work that's that's required and a lot of people still kind of debate how important this last step is but open is it does a difference. we we we're going to do is that the model now has been trained on a vast amount of data from you know any Source on internet you can imagine right so novels Wikipedia Facebook posts uh you know anything basically but how you how users are going to use this is through some chat bot right so it's dialogue like human dialogue is what they call it and now we're just going to say we want to be able to hone in and focus and adjust itself a little bit by training only on human dialogue so we going we going to go and collect the best data we have of human dialogue. we're going to find T the parameters only on human dialogue data so it can hone in its parameters and focus on a specific use case okay so we do that and we're even one step closer and so now we are even working we work even better but then opening I wants take it one step further and say like well there are some observed problems in this model we'regoing to address um and uh one one thing is that right now when we've trained on on this text data right each each uh Target is worth the same somehow and we don't really separate good or bad dialogue. would really good if you understood what's not helpful dialogue and what's helpful dialogue so you can just give us helpful dialogue. Another problem is that we're somehow too greedy so when we train things to predict the next word based on previous words all we care about is to give the most likely next word. If you want to generate a sentence right you don't really care about optimizing the likelihood of the nextword you you care about optimize the accumulated likelihood of your whole sentence. A lot of times you can sacrifice you know short-term profit for optimal long-term profits. you know the best step in at every you know every step of the way for example here right if you go down a little bit at bank you can you can reach out reach a much higher optimal score at the end of the sentence and that's of course what we care about so somehow we're too greedy we should be a little more long-term optimizing. The third Nuance or or kind of difficult with the current model is that somehow we would like this model to be alittle bit more robust let's say. are going to use this and interact with it in ways that maybe it doesn't really correspond perfectly to its training data so it's going to uh see things I haven't seen before so there might be a kind of a distributional shift between how people use it and what it's been trained on. No AI model is is perfect so maybe it accumulate some error as it start adding words and it's just going to go off a little bit and it will add a word after word so it adds aword after word. just goes off a little bit like here for example when you say you know I went to the financial and then just you know some small error happens and it goes off the road to restaurant like somehow you know they started seeing that okay now it's basically go Haywire because it went off and it's in a different space than it's been trained on. So somehow we want to be able to say like well if you find yourself you know alittle bit off the the the path you should be can to find your way back to be as as robust as possible. possible to be as useful as possible okay so uh these are the the the three different things we want to address right what's good and bad dialogue. We want to be more robust and learn to solve correct and this is where we're going to do reinforcement learning from Human feedback. Open AI does on chtp and this was very very hyped for a long time but now people talk less about it uh okay so what do we do well we have a great model that's been fine tune on dialogue and it's able to generate really good answers. to any prompt that we have so let's say we and this is very cheap to do so we have you know a million prompts that we found online now we run our model four times on each prompt with different random seeds we we sample four different answers so now we have 1 million prompts with four uh candidate answers okay and then we're going to say that we're pretty rich so we's going to pay people to actual human beings to label these they're going. to rank this this uh prompt or the answers that these models produce to these prompts. good or bad dialogue so we've solved that okay and the last two problems we are going to solve by using reinforcement learning so what is reinforcement learning well we talked about this a little bit before but uh something is very important and characteristics of reinforcement learning is this delayed feedback so uh in reinforcement learning we're going to allow it to start generating things right it generates a word puts it in its own input and it reruns itself so it becomes a longer longer sequence one over at a time so we start off with this I and we now have a probability distribution. There is no instant feedback so we're on our own and only when we you know we reach some uh predefined token like a pier for example then we stop and then we give uh our sequence that we produced to this robot and then it tell robot like hey is this good or bad. Only at the very end when we're like Hey we're done we give it to the robot and he scores it then we get the feedback okay. Why is this why is this difficult well it's difficult because let's say we do this again so I mean when we produced.two and again there's no at this point there’s no feedback we don't know if we're doing a good job or not. I went to a walk period I mean uh at least it's a pretty good sentence it's like medium score at least but let's say we now generate I went to lip I row row period. That's not not a very good sentence. It doesn't make any sense basically it's is a very very bad score uh but you know a big part of reinforcement learning now is how do you make sense of this information you have two signals you start off doing the the same Step at the first step and then they diverge what would actually caus one sentence to be better than other one. is about like how do you figure out what actually helps you reach your goal and optimizing your score function even if it's delayed um okay so another thing in doing this that's very very important it's exploration versus exploitation so let's say now basically that our model has seen these two different cases and have received two feedbacks right. In one of these you will got a pretty good score and in one uh you know in the lower here you got apretty bad score. Let's say we rerun this model again and it it went from I to went and then it kind of it remembers a little bit what we've seen. far so if we go down this path to I went to a walk then at least we know that going to do a decent job and better than this alternative that we've seen. But the problem with this is that if we do this you know we're not going to see anything new we're just going to explore and explore the the things that already have received feedback on they we already know St pretty well. We're not we're nevergoing to do really good to do better because we are just only going to exploring the sequence that's we already have seen H. example you might again find a much better solution that's much more optimal and that that's what you want to accomplish you you know. And also what I think is quite important to to emphasize here is that and this exploration cannot be completely random right let's assume that you would just generate a sequence you know of 50 random words I mean that would be completely nonsense and you would you wouldn't be able to get good feedback on this at all right. You just be complete nonsense and random. it would be very very hard to improve so in order to generate you know this exploration you you know you want to be a do a very very targeted exploration around language is still kind of make sense so the robot gives you good feedback and you actually can start you know making progress so that's also why you uh open is able to use reinforcement learnings because they have a really good model and language already and they're only really exploring the fringes of the knowledge this model model already has so like they basically only explore good prompt or good answers. gratification actually leads to very non- GRE and and independent robustness so these are the consequence of applying reinforcement learning where you only get feedback at the very end so there's less you know supervision right you're more on your own. H deal with an uncertainty of not having constant feedback you have to figure out things by yourself which leads to you being more robust and also again in reinforcement learn here the only thing we care about is the signal at the end so we don't care about making the best next step. We care about optimizing the whole output so we're now addressing these things. just constantly doting on them H it leads to kind of more robust uh people Okay so we've solved our problems uh we used human I mean we actually paid human beings for label data which is not maybe that like goes against our principles here but but but we still did it because open AI is Rich so they paid people to label things but then they want you know they didn't want to spend too much money so they created an AI to replicate the the job of the human beings H but then at least they got a you know robot or computer model and now is able to say what's good or bad dialogue okay. well because if you now have a better model you kind of want to you want to go to the human beings and get more feedback that's more relevant to this model because this model now is is doing better than the previous one. I think open a runs this two or three times um okay cool. We can just run this step uh and do it all again and you know you can done you can do this as much as you want of course uh maybe with some uh you know decreasing returns I don't know exactly. so to summarize what's the big fuss well just predict the next word based on previous words that's basically it. Who knew that this is going to work at the scale that uh you know and reach reach this kind of intelligence that we're seeing uh was quite hard. Transformers allows us to leverage more data and train quickly because we can paralyze paralyze all these steps in during training and uh then uh when we've done this we have a really really sophisticated model and we spent 99% of our time in computer on on this Transformer. incorporate human feedback and reinforcement learning to get even uh a little bit more of performance out of this okay um again uh obviously self super learning in Foundation models are at the core of of CHP um uh also maybe I mean uh generative a versus self-supervised learning U maybe it's useful to kind of um I mean the difference between entive Ai and self superlearning is not clear uh and people use it typically I mean there is more the difference is more clear when it comes to uh the research space but people say that CH P generative AI or ex Strang uses s super learning. and self Suess learning care more about both aspects somehow somehow. Next time we will talk about uh we'll do a similar Deep dive into stable diffusion there will be self supervised learning and Foundation mod an AI but uh I think it's going to be slightly more conceptually interesting um so should be a lot of fun and yes please go to the website for more information Etc and if you have any questions feel free yeah can I something can I assume that probability on that after there changes based on the subject of the totally yes yes yes great question okay. train to generate the distribution. The more the longer sequence and the more context or the longer prompt you have the more specific prompt the more Peak your distribution will be. So the more information the model have around your specific context in use case the more it knows how to collapse into a space that you want to know about right? So if you just say you know if you start sampling the model with no prompt it's going to generate so the most common starting points on the Internet or something just like random text. it's going to be able to collapse and create distribution that's much more targeted uh so is if it doesn't have data run a context or about you and your interest as a person it it won't be able. to tailor to you right they cannot create Magic out of thin air it can only do the best of The Prompt and knowledge has so far. It's also why data is so important to have around you right and good prompt Etc and it's alsowhy this prompt engineering to be. able to create the best prompts to get what you want. model generating is reinforc learning because itates the response yeah that's a great Point actually okay so yes in this what we talk about right now basically we had a few models involved and and we talks about reinforcement learning supervised learning and self-supervised learning right so exactly the robot that just H try to replicate the the human beings putting scores on the answers that we generated prompts yes that's train using self uh supervised learning right because you have these labels now that you want to replicate. have a starting point from superv learning right so you already have a world model that you can leverage and then you you can use your your labels more efficiently right so the first step of preaching the next word based on previous words that's selfs supervised learning right that's 99% of the work. Then we have supervised learning what we learn from the human beings but we already we have a starting start already so it's easier and then we do reinforcement learning. But again the starting point of the reinforcement learning is this self-supervised learning. The Transformer is based on how a normal person learns right when you're learning a new content you to Rel it with everything else you know yeah so I'm just I mean this yeah okay uh so the question is basically is the Transformer inspired by research about how we our kids learn right how the brain works yeah I mean you're going to find a lot of work around you know making those connections and uh then there's a huge debate in in like the Deep Learning Community is that is that actually true is it kind of wishful thinking and in hindsight we make this connection right. I do too right you for around similarities to how our own brain works and some people are like oh that's not you know we have to be very very careful because we kind of compare. I think that there's strong connections right but like what came first I actually think that uh people have some intuition they tinker and they try things and then suddenly something work and they work on intuition and actually theory is very much hindsight so some engineer played around with things actually theer was from Google right so we build a Transformer. well this reminds me of this and this uh so there was definitely no strong like this is how kids learn and then we replicate that it the more I would say the more likely description of how the Transformer came about was just Engineers tinkering out and trying things and then it it just like oh this works now and they are not prbly completely conscious themselves about what they were inspired by you know so does that mean that we not have collaborations neuroscientist yeah it means that the collaboration with neuroscientists in deep learning is is quite quite rare actually. word mask it and try to predict it based on the surrounding words um yeah the answer to that is we used to do that way and it works better uh but due to engineering you can Bas basically kind Transformer you can maybe this can be like a homework for you but if you look at Transformer Works uh if you mask a word uh then you will like then you'll only um okay I if you do this you know if only predict the last word based on previous words you can make this okay attention. so you can okay this maybe any like you have this tensor product you can Define this m Matrix High dimensional matrix product in a way that's efficient. So you can basically in a single goal predict the target like you can you know you have a sentence you can run the whole sentence and predict the next word based on previous word. If you do Mass language modeling you cannot do that then you basically have a full attention you can attend forward and backwards but you can only run the targets that you you masked. each Target by itself but if you mask then you maybe mask like 15% of the words and then it's it's only like that's only 50% then in terms of getting that feedback uh so it just in end of the day it means that uh when you try this empirically doing it aut agressively and this trick to be able to use the each you know each word is a target itself just leads to better performance at this task of generating export based on previous words uh so yeah so it's I guess it's engineering empirical okay it's not as in an Ideal World if you have unless compute you would have. oh actually works better given the same amount of compute yeah does that give you some sense of answer we can talk more about it offline as well. What are like the main challenges of these models nowadays and are there other like language models that are like better but they do like more expensive than or like this is like the best um yeah I think. I mean we want to able to rely on it as much as possible and if they if they don't behave like we want them to we don't want to make up things right. you like ask a factual question and they give us something that's wrong and they're confident about it maybe that's bad so like I think what we're starting to see is that they um are very humanik even it's in its mistake they're like well they're somehow they're biased are we going to talk about this like they have stereotypes around things. They also like we do I mean they suffer from wishful thinking and some type of imagination where they rather be you know make you happy than being completely truthful. The model will be able to digest some input generate some output and then you know digest that input generate more output and stuff like in this kind of planning step uh it becomes has much much better abilities so you can basically instead of having a single go at your prompt if you can you know have a few tries and improve on itself and only get give you the response after it's done this internal internal processing it will be to do much better and then like again if you throw in some tools there it can search internet to get more information. These are reinforcement learning techniques of how to do planning well so how to incorporate planning is something that people talk about a lot and then of course multimodalities. It's not hard to see that this idea of predicting next word based on previous words corresponds really well to videos just to kind of predict the next frame based onPrevious frames uh and why aren't people doing it well. videos are suddenly a next level in terms of compute what it needs because they have tons of videos a frame is very expensive because it's a high dimensional picture or image. the World by looking at videos right you can even sort to understand how human beings work even better because you can see people being upset or sad or happy whatever right in in a video and start picking these cues up. You can connect the vision part to the text part and get a multimodality model that's able to do both in a really really sophisticated way uh also something that I think these these people are working on all right thank you thank you for your time and good luck with your book.