most people in deep learning when we talk about GPUs, we're pretty much exclusively talking about NVIDIA GPUs. So to give you an idea of like what is the difference between a CPU and a GPU, I've kind of made a little spread sheet here. And there's a couple general trends to notice here. Both GPUs and CPUs are kind of a general purpose computing machine where they can execute programs and do sort of arbitrary instructions. But they're qualitatively pretty different. For GPUs we see that these sort of common top end consumer GPUs have thousands of cores. debate. So if you guys have AMD cards, you might be in a little bit more trouble if you want to use those for deep learning. And really, NVIDIA's been pushing a lot for deeplearning in the last several years. It's been kind of a large focus of some of their strategy. And they put in a lot effort into engineering sort of good solutions to make their hardware better suited forDeep learning. So in practice, you tend not to end up writing your own CUDA code for deepLearning. TensorFlow is a pretty safe bet for just about any project that you want to start new, right? Because it is sort of one framework to rule them all. However, you probably need to use it with a wrapper and if you want dynamic graphs, you're maybe out of luck. Some code ends up uglier but in my opinion, maybe that's kind of a cosmetic detail. If you're just writing research code, I think PyTorch is a great choice. But it's a bit newer, has less support out there, so it could be a bit of an adventure. Numpy is definitely CPU only. And you're never going to be able to experience or take advantage of these GPU accelerated speedups if you're stuck working in Numpy. So, kind of the goal of most deep learning frameworks these days is to let you write code in the forward pass that looks very similar to Numpy, but lets you run it on the GPU and lets you automatically compute gradients. TensorFlow has this magic line that just computes all the gradients for you. So now you don't have to go in and write your own backward pass yourself. different where we're actually building up this new computational graph, this new fresh thing on every forward pass. That's called a dynamic computational graph. For kind of simple cases, with kind of feed forward neural networks, it doesn't really make a huge difference, the code ends up kind of similarly. But I do want to talk a bit about some of the implications of static versus dynamic. And what are the tradeoffs of those two? So one kind of nice idea with static graphs is that because we're kind of building up one computational graph once, and then reusing it many times, the framework might have the opportunity to go in and do optimizations on that graph. is really great for research. If you're interested in production deployment, you should probably look at Caffe, Caffe 2 or TensorFlow. PyTorch is a great choice. But it's a bit newer, has less community support, less code out there, so it could be a bit of an adventure. So it's kind of unfortunately, there's not just like one global best framework, it kind of depends on what you're actually trying to do, what applications you anticipate. - Hello? Okay, it's after 12, so I want to get started. So today, lecture eight, we're going to talk about deep learning software. This is a super exciting topic because it changes a lot every year. But also means it's a lot of work to give this lecture. But as usual, a couple administrative notes before we dive into the material. So as a reminder the project proposals for your course projects were due on Tuesday. And again, when working on assignment two, remember to stop your Google Cloud instances.