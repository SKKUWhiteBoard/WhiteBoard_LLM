So, last time we were starting to talk about the sort of the general overview of what reinforcement learning involves. We introduced the notion [NOISE] of a model, a value, and a policy. Can anybody remember off the top of their head what a model was in the context of reinforcement learning? So what we're gonna do today is, sort of, um, build up for Markov Processes, up to Markov Decision Processes. And this build, I think, is sort of a nice one because it allows one to think about what happens in the cases where you might not have control over the world. from sort of a probability distribution. So, if we have a finite number of states in this case R can be represented in matrix notation which is just a vector because it's just the expected reward we get for being in each state. Right now we're still in Markov Reward Processes so there's no action. The ways you could define rewards would either be over the immediate state or state and next state. Um, but it often we think about the case where, um, an agent might be acting forever or this process might be going on forever. The definition of a return is just the discounted sum of rewards you get from the current time step to a horizon and that horizon could be infinite. If Gamma is equal to one, then that means that your future rewards are exactly as beneficial to you as the immediate rewards. If you're only using discount factors for mathematical convenience, um, if your horizon is always guaranteed to be finite, it's fine to use gammaequal to one in terms of from a perspective mathematical convenience. But in the general case, we are gonna be interested in these stochastic decision processes which means averages will be different than particularly runs. There, one could try using other participant is certainly the most common one and we'll see later why it has some really nice mathematical properties. So, if we go back to our Mars Rover here and we now have this definition of reward, um, what would be a sample return? So, let's imagine that we start off in state s_4 and then we transitioned to s_5,. s_6, s_7 and we only have four-step returns. And of course we could define this for any particular episode and these episodes generally might go through different states even if they're starting in the same initial state. The question was was if it's possible to have self-loops? Um, could it be that this is sort of circulator defined [NOISE] in this case. Is it ever actually possible for, uh, that matrix not to have an inverse or does like the property that like column sum to one or something make it not possible? It's a good question. I think it's basically never possible for this not to has an inverse. I'm trying to think whether or not that can be violated in some cases. Markov Decision Processes are the same as the Markov Reward Process except for now we have actions. So, the agent is in a state they take an action, they get immediate reward, and then they transition to the next state. And as was asked before by Camilla I think, the reward can either be a function of the immediate state, the state and action to the state action and next state for most of the rest of today we'll be using that it's the function of both theState and action. An observation you'd see something like this s, a, r, and then transition to state s' And so a Markov Decision Process is typically described as a tuple which is just the set of states, actions, rewards, dynamics, model, and discount factor. Because of the way you've defined that dynamic model, is the case that if you take a specific action that is intended for you to move to your state s', you won't fully successful move to that state? Like I guess I'm curious about why there's a- why there is a probability at all? If you have an MDP plus a policy then that immediately specifies a Markov Reward Process. So, in order to learn what is the value of a particular policy we instantiate the reward function by always picking the action that the policy would take. And then for whatever state I end up by next continuing to follow this policy. So that's what the V^pi_k-1 specifies. What would happen if the expected discounted sum of rewards we get by continuing to. follow policy from whatever state we just transitioned to. you do this computation. Just to quickly check that the Bellman equation make sense. So in this case, um, because p is a stochastic matrix, its eigenvalues are always going to be less than or equal to one. And that's back to my original question which is you seem to be using V_k without the superscript pi to evaluate it. Oh, sorry this should, yes. This should have been pi. That's just a typo. And this is just an example of how you would compute one Bellman backup. before we do this let's think about how many policies there might be. So there are seven discrete states. In this case it's the locations that the robot. There are two actions. I won't call them left and right, I'm just going to call them a_1 and a_2. Then the question is how many deterministic policies are there and is the optimal policy for MDP always unique? So kind of right we just take like one minute or say one or two minutes feel free to talk to a neighbor. Policy iteration is a technique that is generally better than enumeration. So, in policy iteration what we do is we basically keep track of a guess of what the optimal policy might be. We evaluate its value and then we try to improve it. If we can't improve it any more, um then we can halt. If you have a lot of compute, you might just want to and this might be better if you really care about wall clock and you have many many many processors. But if you don't have kind of infinite compute, it's generally more computationally efficient if you have to do this serially to do policy iteration. a Markov Reward Process. And then we do policy improvement. A state action value says well, I'm going to follow this policy pi but not right away. So, that defines the Q function and what policy improvement does is it says okay you've got a policy, you just did policy evaluation and you got a value of it. The next critical question is okay why do we do this and is this a good idea? So, when we look at this, um let's look through this stuff a little bit more. The key question of whether or not value iteration will converge is because the Bellman backup is a contraction operator. The distance between, in this case we're gonna think about it as two vectors, doesn't get bigger and can shrink after you apply this operator. So how do we prove this? Um, we prove it- for interest of time I'll show you the proof. And then the next thing we can do is we can bound and say the difference between these two value functions is diff- is, um, bounded by the maximum. between the two value functions can't be larger after you apply the Bellman operator than it was before. There has to be a unique solution. It's also good to think about whether the initialization and values impacts anything if you only care about the result after it's converged. All right. Class is basically over. There's a little bit more in the slides to talk about, um, the finite horizon case, and feel free to reach out to us on Piazza with any questions.