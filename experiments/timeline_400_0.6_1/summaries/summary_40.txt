Danqi Chen is one of the foremost researchers in question answering. She is the professor at the Princeton University. Danqi once upon a time was the head TA of CS224N. She's quite familiar with the context of this class. So today I'm very happy to introduce to you some of the fundamentals in this field, as well as on cutting edge and state of the art topics. So here's my plan for this lecture. I would give a brief introduction of what question answering is. And then today, delighted to have our first invited speaker. The goal of question answering is to build systems that can automatically answer questions posed by humans in our natural language. Question answering, or, let's say QA in short, is one of the earliest NLP tasks, and the early systems can even date back to the 1960s. IBM Watson has been shown to beat two national champions in Jeopardy answering questions at least at least once in the past. The question and answer has enabled a lot of really useful real world applications, for example, today if you just put your question in a search engine. Stanford Question Answering Dataset is a supervised reading comprehension dataset. It consists of 100K annotated passage question and answer triples. The questions are crowd-sourced, basically like from Mechanical Turking. Each answer is a short segment of text, or we called it span in the passage. The state-of-the-art already exceeds estimated human performance on the SQuAD dataset, according to the Stanford researchers. The evaluation metrics to evaluate how well the system can do on this dataset are exact match and F1. BERT is basically a deep bidirectional transformer encoder pre-trained on large amounts of text. It is trained on the two training objectives, including masked language modeling and the next sentence prediction. The BERTbase has 110 million parameters and the BERT-large model has 330 million parameters. And if you use a stronger pre-training models, like a stronger BERT models, they can even lead to better performance on SQuAD and BiDAF. And you can see that this is a huge jump from the standard BERT model to the BiF model. pre-training has been so important. So next I will quickly talk about-- OK, a question here is that can we actually even design better pre-training objectives for reading comprehension or question answering? And the answer is actually yes. So this is actually a work I did with Mandar Joshi and other folks one year ago called SpanBERT. So for SQuAD and other a lot of extractable reading comprehension datasets, the goal is trying to predict the answer span from the passage of the question. a large collection of documents. So one example is just taking the whole Wikipedia, which has five million articles. And we're going to return the answer for any open-domain questions. So this problem, there isn't any single passage, so we have to answer questions against a very largeCollection of documents or even the whole web documents. This is actually a much more challenging and also more practical problem. So if you look at the example of Google example I showed at the beginning, so these techniques will be very useful in the practical applications. unsupervised question answering so by using some kind of approach like some form of unsupervised machine translation, this kind of idea. That can be borrowed-- can borrow the idea from that and can also work pretty well, reasonably well in unsuper supervised question answering. So my question is I guess it's kind of interesting that there's not really that strong of a transfer effect between data sets that are kind of ostensibly similar. So have there been any research done on how close I guess the formatting and the semantic content of these question answering datasets actually adheres to the data that BERT is pre trained on? And if so, has there been sort of any effect found between those similarities or differences? Is the question asking whether there has been like somekind of-- OK, maybe I can just try to clarify a little bit why the current models cannot really generalize well from one data set to another data set. the answer? So why do you think the nearest neighbor-- I mean, you always can find something, right? The question is that whether it's close enough or not. So the question is what if in the datasets that the answer is not close enough? Yeah, that's a good question. If you really come up with something that is really very far away from all the questions that we have been seeing in the training set, that could be possible. Basically it depend on how the text are formatted. Next question is about the future of NLP. Do you think that in order to solve NLP in the sense that you can perform on par with humans on all NLP tasks it's sufficient to only interact with text data? Or do you think we'll eventually need the sort of experiences and common sense that we get only from seeing and viewing the world and having a set of interactions that we as humans have? The answer is definitely yes. Next is-- Hey, how is it going? Thanks so much for the lecture. The key difference between the generative model and the extractive model is that for generative models, you can actually leverage more input passage together and do the generation. So for this model, there isn't any retrieval. So you can always find the answer from the question, right? So this model really has you relying on all the parameters you memorized, all the encodings. And then Generative models are you're remembering the whole question and you try to retrieve the memory when you answer the question. The model is very large, like 11 billion parameters. So the parameters are basically trying to memorize a lot of information that has been.information. So by just taking this input, it has to just rely on the parameters to infer this answer. So it's actually very hard to-- yeah, it's a definite balance between memory and the generalization from it. All right, thanks. Do you want to call it a night or do you want one more question? Either way, yeah.