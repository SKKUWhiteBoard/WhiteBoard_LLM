The EM algorithm is an unsupervised version of k-means GMM. In GMM, you guess randomly an assignment of every point to the cluster, the probability. You guess where they're probabilistically linked, that is, what's the probability of these points belong to cluster one, this point belongs to cluster two. And then once you have that, you then solve some estimation problem that looks like a traditional supervised learning. The decomposition is quite important. And we're going to try and kind of abstract that away. These weird pictures? The technical detail is I want to make sure that you understand this key result. And the reason is-- I'll refer to this thing as we go through. We're going to use it. It's not like I'm just teaching you something for your health. Like, this is actually going to be used in the next step. It will actually, in some sense, be the entire algorithm. So it'll become a clear point where we apply Jensen's inequality, where we make it tight. A function is going to be convex if its graph is, OK? As I said. OK, so let's draw an example of this. It's a parabola, kind of a bowl-shaped function, right? Now, no matter how I pick the points-- and clearly, I should really only have to worry about picking on the edge. So this is now z, f of z. This is f ofZ. Does that out a Taylor series for this f double prime-- see the a, a minus z square? If f is twice differentiable and, for all x, f double prime of x is greater than 0, then f is convex, OK? So this says these functions really are bowl-shaped, right? Second derivative being positive means that they have this kind of positive curvature that looks like the U's. Their first dimension-- first derivative goes up and down, but they're kind of always trending. That first derivative is always getting more positive. It's negative on the left-hand side, positive on the right-handside. That's what it means by bowl- shaped. conceptualize it is we solve for some hidden parameter. We solve, and that gives us an entire family of possible solutions. We take a log of the probability that we assign to the data given our parameters. The best way to understand it is just to run it through a couple of the different examples. EM and the next one-factor analysis-- and by the end, you'd be like, oh, OK, that makes sense. It's a lot of notation because we're abstracting out a huge number of things that we're doing. structure. P(x; theta)-- this is a generic term, right? This is just one of the i terms-- says the function factors this way. Looks like a sum over z, where z is our hidden or latent variable. Q(z) has to be related in some way to P x of z for this to work. What is c? Ah, what is z or c? Oh, c is some constant independent of the-- just for some constant c. We don't care what its value is. We just care that it doesn't depend on z. The algorithm is based on the fact that Lt of theta is always below the loss, so it's kind of a surrogate that I'm not overestimating my progress, and it's tight. So if I did happen to have the actual optimal value, it would meet at that point. So I wouldn't think and get fooled that there was a higher loss function somewhere else. But right now, I have a way of going term by term from the likelihood function and getting lower bounds at a particular spot. But I have to pick a certain Q to make this operational. That's the piece. Curve, L of t. And then the M-step, and together, EM, set phi t plus 1 equal to argmax phi Lt(phi). Cool. Just could you reiterate? Like, why are we not using gradients on the original turbulence? Right, so we could imagine doing some kind of gradient descent here, but it's not clear how to deal with this marginalization that happens in the middle. So we'll come back to that claim in a second. be thinking because I told you that Jensen's will have something to do with this. What we're going to do to put it in the form where Jensen's could be used looks wholly unmotivated, OK, totally unmotivation. But it's to shoehorn into what we're doing, and there's some motivation, but it's kind of opaque, let's say. So I'm going to pick Q as a probability distribution. I'll write that in a different color. OK, why? Because now I can make my argument one line. and x. So we're going to have this notation, Q(i) of z because it depends on each different value. So each data point is going to get its own different Q, which is the log of how likely this thing is, OK? And we picked those for each i. So the ELBO of x, Q, z equals x,Q, z. OK? This thing has a very famous name, so I'll write that while I kind of stall for more questions. What we've defined here is called the Evidence-based Lower BOund. sequence that is monotonically increasing or nondecreasing, OK? So it's possible that it would grind to a halt. But eventually, it has to be strict. And so to derive a counterexample, you would just find a likelihood function that had those two bumps. And you would run it in that particular lower bound setting. And what it will do is it will gradually hillclimb. And this is actually not great. Like, it can't go back downhill, right? It's got to just continue to go up. phi 2 was hugely bigger than phi 1, right-- a billion points came from the second source, and only one point from the first source. We'd probably say that it's more likely that it would go to this, right? It would certainly boost its probability. So to automate this, this is Bayes' rule. It just weighs those two probabilities and tells us what should happen. That's it. We ran through exactly those calculations last time. All right, let's take a look at the M-step. equal to the sum over j-- because now I'm summing over the cluster centers, right? The z(i) notation was still very abstract-- wj(i), which was summing. over this part here, log-- and help us all, 1 over 2 pi-- this is a covariance, 1/2. This is the exp of Oh, I decided to write this in four general things. Why do I care about that? Oh,I see why. OK. Transpose sigma inverse x( i) mu j times phi j. Oh, that hurts. When you have a constrained probability distribution, you have to use a Lagrange multiplier. In this case, it makes total sense, though, because these numbers have to sum to 1. So if you don't have a normalization constant here, you're adding up a bunch of numbers which individually sum up to n, right? The sum over all of them is n. And this is just the principle that tells you, you must normalize them by this n factor, OK? So all I care that you take away, if you've seen this a thousand times, is that you understand. do this when you have something that sums to 1. It's not more complicated than what I wrote here, but make sure independently you go through it and ask questions. In the next class, as I said, what we're going to see is this notion of factor analysis. And that is going to tell us how to apply EM to a different kind of setting, which, at first glance, will look kind of impossible to do without a latent variable model. And I think that's all I want to say.