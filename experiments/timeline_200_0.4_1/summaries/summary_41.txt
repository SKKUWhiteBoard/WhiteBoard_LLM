homework two is out now. This weekend sessions will be having some more background on deep learning. We're also gonna be reaching, uh, releasing by the end of tomorrow, what the default projects will be for this class. Um, and those proposals will be due, um, very soon, er, in a little over a week. The assignments, [inaudible] are they limited to TensorFlow? asked the question if, if the assignment is limited toTensorFlow. I'm pretty sure that everything relies that you're using Tensor Flow. Last week, we were discussing value function approximation. Today we're gonna start to talk about other forms of value function approximations using deep neural networks. We're mostly not gonna talk so much about enormous action spaces, but we are gonna think a lot about really large state spaces. And so, this could be things like a laser range finder for our robot, which told us how far away the walls were in all 180 degree directions. And the key thing was that we don't know what the true value of a policy is. So, the two ways we talked about last time was inspired by the same. There are some limitations to use the linear value function approximation, even though this has been probably the most well-studied. One alternative that we didn't talk so much about last time is to use sort of a really, really rich function approximator class. Some of those are Kernel based approaches. If you take a machine learning you've heard of k-nearest neighbors, those are sort of these non-parametric approaches, where your representation size tends to grow with the number of data points. Deep neural networks can be useful but they generally don't scale so well. What we're talking about today is thinking about deep neural networks which also have very flexible representations but we hope we're gonna scale a lot better. These are- happen a lot in unsupervised learning like predicting whether or not something is a cat or not or, you know, an image, uh, of a particular object, um, or for regression. So, we're just gonna think of com- making a function approximator which is a composition of a number of functions. how we could use these type of approximations for Atari. And then we had the results that were kind of happening around like 1995 to maybe like 1998, which said that, "Function approximation plus offline off policy control, plus bootstrapping can be bad, can fail to converge." So I think for a long time after that, the, the community was sort of backed away from Deep Neural Networks for a while. And so, perhaps it was natural that, like, around in like 2014, DeepMind and DeepMind combined them and had some really amazing successes with Atari. are the important things that they, um, did in their paper, this is a nature paper from 2015, is they use the same architecture and hyperparameters across all games. Now just to be clear, they're gonna then learn different Q functions and different policies for each game. But their point was that they didn't have to use totally different architectures, do totally different hyperparameter tuning for every single game separately. It really was the sort of general architecture and setup was sufficient for them to be able to learn to make good decisions for all of the games. DQN, deep Q-learning addresses these is by experienced replay and fixed Q-targets. Experienced replay, prime number if you guys have heard about this, if you learned about DQN before is we're just gonna stroll data. So, even though we're treating the target as a scalar, the weights will get updated the next round which means our target value changes. So this is nice because basically it means that you reuse your data instead of just using each data point once, you can reuse it and that can be helpful. Learn to model, a dynamics model and a word model, and then the planning for that which is pretty cool. But we don't wanna do that all the time because there's a computation trade-off and particularly here because we're in games. Um, I'm trying to strike the right balance between continuing experience like new data points versus re-flagging it. Can we use something similar to like random probability just decide to re-flag [inaudible]. The question is about how would we choose between getting new data and how much to replay. This is just reducing the noise and the target that we're trying to sort of, um, if you think of this as a supervised learning problem, we have an input x and output y. The challenge in RL is that our y is changing. If you make it that you're- so your y is not changing, it's much easier to fit. This is really just about stability and that's- that's true for the experience replay too. Experience replay is just kinda propagate information more- more effectively and this is just gonna make it more stable. Lemme just to clarify, if we set alpha to zero, what's the rule for selecting tuples among the existing tuples? It's a great question. Lemme just clarify. If you are fixing, um, uh, your w minus, then, if you were looking at our case that we had before, then you wouldn't be able to continue propagating that back, because you hadn't update yet, yet, that's exactly right. So there's gonna be this tension between, when you fix things versus her propagating information back. It's very computationally, expensive or impossible in some cases to figure out. It could be super tempting to try to start, by like implementing Q learning directly on the ATARI. Highly encourage you to first go through, sort of the order of the assignment and like, do the linear case, make sure your Q learning is totally working. Even with the smaller games, like Pong which we're working on, um, it is enormously time consuming. It's way better to make sure that you know your Q Learning method is working, before you wait, 12 hours to see. whether or not, oh it didn't learn anything on Pogge. So, that, that- there's a reason for why we, sort of build up, the way we do in the assignment. Um, another practical, to a few other practical tips, feel free to, to look at those, um, and then we were on Thursday. Thanks. Back to Mail Online home.back to the page you came from. Back from the page where you come from. back to MailOnline home.