Lecture eight is about deep learning software. This is a super exciting topic because it changes a lot every year. We're going to talk about some of the nuts and bolts about writing software and how the hardware works. And a little bit, diving into a lot of details about what the software looks like that you actually use to train these things in practice. And we'll talk about several of the major deep learning frameworks that are out there in use these days. The midterm will be in class on Tuesday, five nine. Last time we talked about fancier optimization algorithms for deep learning models including SGD Momentum, Nesterov, RMSProp and Adam. And we saw that these relatively small tweaks on top of vanilla SGD, are relatively easy to implement but can make your networks converge a bit faster. We also talked about transfer learning where you can maybe download big networks that were pre-trained on some dataset and then that you know, your model is maybe sitting on the GPU, but your big dataset is sitting over on the right on a hard drive or an SSD or something like that. of my computer at home that I built. And you can see that there's a lot of stuff going on inside the computer, maybe, hopefully you know what most of these parts are. And the CPU is the Central Processing Unit. That's this little chip hidden under this cooling fan right here near the top of the case. The GPUs are these two big monster things that are taking up a gigantic amount of space in the case, and they have their own cooling. They're quite large. So the question is what are these things and why are they so important for deep learning? Well, the GPU is called a graphics card. In deep learning we kind of have mostly picked one side of this fight, and that's NVIDIA. So if you guys have AMD cards, you might be in a little bit more trouble if you want to use those for deep learning. NVIDIA has gone in there and released their own binaries that compute these primitives very efficiently on NVIDIA hardware. So in practice, you tend not to end up writing your own CUDA code for deeplearning. You typically are just mostly calling into existing code that other people have written. The NVIDIA Titan XP which is the current top of the line consumer GPU has 3840 cores. That's like way more than the 10 cores that you'll get for a similarly priced CPU. The downside of a GPU is that each of those cores, one, it runs at a much slower clock speed. And two they really can't do quite as much as a CPU.at once. But it should give you the sense that due to the large number of cores GPUs can sort of, are really good for parallel things. up when running the exact same computation on a top of the line GPU, in this case a Pascal Titan X, versus a top-of-the-line CPU. Although, I'd like to make one sort of caveat here is that you always need to be super careful whenever you're reading any kind of benchmarks about deep learning. And you kind of need to know a lot of the details about what exactly is being benchmarked in order to know whether or not the comparison is fair. This was sort of out of the box performance between just installing Torch, running it on a CPU, and just running Torch on a PC. than a year ago, TensorFlow was relatively new. It had not seen super widespread adoption yet at that time. But now I think in the last year Tensor Flow has gotten much more popular. It's probably the main framework of choice for many people. So that's a big change. We've also seen a ton of new frameworks sort of popping up like mushrooms. So in particular Caffe2 and PyTorch are new frameworks from Facebook that I think are pretty interesting. Paddle, Baidu has Paddle,. Microsoft has CNTK, Amazon is mostly using MXNet. In TensorFlow you would typically divide your computation into two major stages. You'll first have code that builds the graph and then you'll go and us build our model without doing all these explicit bookkeeping details ourself. There's like a lot of different higher level libraries that people build on top of Tensor Flow. For kind of simple, kind of feed forward neural networks, like PyTorch, you can do a similar thing with Keras where we're actually building up this new thing on every forward pass. choose to use frameworks rather than writing your own stuff from scratch. So as kind of a concrete example of a computational graph we can maybe write down this super simple thing. Where we have three inputs, X, Y, and Z. And this code looks exactly like the two layer net code that you wrote in Numpy on the first homework. And now we're doing a manual update of the weights and computed gradients using our PyTorch tensors. But the major difference between the Py Torch tensor and Numpy arrays is that they run on GPU so use one weight matrix so if Z is negative we want to use a different weight matrix. TensorFlow has this magic line that just computes all the gradients for you. The other nice thing about TensorFlow is you can really just, like with one line you can switch all this computation between CPU and GPU. In PyTorch again, it's really easy to switch to GPU, you just need to cast all your stuff to the CUDA data type before you rung your computation. So the next layer of abstraction is the variable. So this is, once we moved from tensors to variables now we're able to take gradients automatically and everything like that. TensorFlow lets you run computations on top of a graph. The idea is that you can feed in data and put it in through these input slots in the graph. These input slots are now kind of like symbolic variables and we're going to perform different TensorFlow operations on these symbolic variables. This kind of basic way of working could be a little bit unwieldy and it could be really annoying to make sure you initialize the weights with the right shapes and all that sort of stuff. So as a result, there's a bunch of sort of higher level libraries that wrap around Tensor Flow and handle some of these details for you. of basic Tensor operations to compute our Euclidean distance, our L2 loss between our prediction and the target Y. Then we have this magical line where after we've computed our loss with these symbolic operations, then we can just ask TensorFlow to compute the gradient of the loss with respect to w1 and w2 in this one magical, beautiful line. But again there's no actual computation happening here. This is just sort of adding extra operations to the computational graph. And then you might think that this would train the network, but there's actually a bug here. So, if you actually run this code, and you plot the loss, it doesn't train. TensorFlow is smart and it only computes the parts of the graph that are necessary for computing the output that you asked it to compute. So when updates evaluates it just returns none. But because of this dependency we've told it that updates depends on these assign operations. These assign operations live inside the computational graph and all live inside GPU memory. So then we're doing these update operations entirely on the GPU and we're no longer copying the updated values back out of theGraph. So whenever you're working with TensorFlow you have this funny funny thing that happens. With these fake data dependencies and we just say that this dummy node updates, has these data dependencies of new w1 and new w2. And now when we actually run the graph, we tell it to compute both the loss and this dummy nodes. This dummy node doesn't actually return any value it just returns none, but because of this dependency that we've put into the node it ensures that when we run the updates value, we actually also run these update operations. So the question is why is loss a value and why is updates none? That's just the way that updates works. The layer is using this xavier initializer object to set up an initialization strategy for those. And you can see here, we're also passing an activation=tf.nn.relu so it's even doing the activation, the relu activation function inside this layer for us. And in fact if you run this code, it converges much faster than the previous one because the initialization is better. Question is does the xavierInitializer default to particular distribution? I'm sure it has some default, I'm not sure what it is. But it seems to be a reasonable strategy, I guess. Keras is very popular, so you might consider using it if you're talking about TensorFlow. We build some optimizer object and we call model.compile and this does a lot of magic in the back end to build the graph. Of layers. And now we can calls model.fit and that does the whole training procedure for us magically. So I don't know all the details of how this works, but I know Keras isvery popular. So there's actually like this whole large set of higher level Tensor Flow wrappers that you might see out there in the wild. And it seems that like even even even Keras can do this. but Tensorboard you can add sort of instrumentation to your code and then plot losses and things as you go through the training process. TensorFlow also let's you run distributed where you can break up a computational graph run on different machines. That's super cool but I think probably not anyone outside of Google is really using that to great success these days, but if you do want to run distributed stuff probably Tensorflow is the main game in town for that. A side note is that a lot of the design of Tensor Flow is kind of spiritually inspired by this earlier framework called Theano from Montreal. in PyTorch a lot is define your own nn modules. So typically you'll write your own class which defines you entire model as a single new nn module class. And a module is just kind of a neural network layer that can contain either other other modules or trainable weights or other other kinds of state. So in this case we can redo the two layer net example by defining our own nN module class and then storing them inside of our own class. In the forward pass we can use both our own internal modules as well as arbitrary autograd operations on variables to compute the output of our network. PyTorch provides pretrained models. And this is probably the slickest pretrained model experience I've ever seen. You just say torchvision.models.alexnet pretained=true. That'll go down in the background, download the pretrained weights for you if you don't already have them, and then it's right there, you're good to go. PyTorch also has, there's also a package called Visdom that lets you visualize some of these loss statistics. has convolution and relu operations kind of one after another, you might imagine that some fancy graph optimizer could go in and actually output, like emit custom code. So I'm not too sure on exactly what the state in practice of TensorFlow graph optimization is right now, but at least in principle, this is one place where static graph really, you can have the potential for doing this optimization in static graphs. Another kind of subtle point about static versus dynamic is this idea of serialization. So with a static graph you can imagine that you write this code that builds up the graph and then once you've built the graph, you have this data structure. Python control flow. And the problem is that because we only build the graph once, all the potential paths of control flow that our program might flow through need to be baked into the graph at the time we construct it before we ever run it. In TensorFlow this becomes a little bit uglier. And again, because we need to construct the graph all at once up front, this control flow looping construct again needs to be an explicit node in the Tensor Flow graph. awkward to work with than the sort of native dynamic graphs you have in PyTorch. So then, I thought it might be nice to motivate like why would we care about dynamic graphs in general? So one option is recurrent networks. So you can see that for something like image captioning we use a recurrent network which operates over sequences of different lengths. So I think that there's a lot of opportunity for doing cool, creative things with dynamic computational graphs. And maybe if you come up with cool ideas, we'll feature it in lecture next year. can train on data without writing any of your own code. Cafee has a model zoo with a bunch of pretrained models, that's pretty useful. Caffe has a Python interface but it's not super well documented. You kind of need to read the source code of the python interface to see what it can do. But it does work. So, kind of my general thing about Caffe is that it's maybe good for feed forward models, it'smaybe good for production scenarios, because it doesn't depend on Python.