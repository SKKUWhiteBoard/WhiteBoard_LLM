Machine learning is about how to acquire a model, from data and experience. In the end, we want to build good systems. Where do you get accurate systems? You get them from good models. Good models come from good data, and we're going to look at that last part now. What we are going to do today is we're Going to start with model-based classification, and, as an example of that, we'reGoing to work through some details of how the Naive Bayes models work. takes on machine learning that are going to highlight different subsets of the big ideas on this topic. We're going to have a couple running examples. One of them is that spam classifier that pulls out all the emails you don't want from your email. And something like digit recognition, we'll start to give you a window into how other vision tasks work. We'll see more in-depth examples and more structured examples of these kinds of problems later on when we talk about applications. "To be removed from future mailings, simply reply to this message and put Remove in the subject" "99 million email addresses for only $99" "I know it was working pre-being stuck in the corner, but when I plugged it in, hit the power, nothing happened" "Had an old Dell Dimension XPS sitting in the corners and decided to put it to use." "I'm beginning to go insane. I know this is blatantly off-topic, but I'm beginning-to-go insane" very much an individual question which emails you want to receive or not. The boundary between what is actually spam, unsolicited commercial email, whatever, and emails you just don't want, this can be a fuzzy boundary. What is it about the top two emails that let you conclude that they're spam, and how could we automate this? Well, machine learning is going to do some amount of work, but something has to power this. There has to be something about those first emails that's going to give you the clues that something's fishy here. In practice, for actual spam detection, a lot of the evidence of spam versus harm comes not from the word or even the content of the email in any way, but rather, its relation to other things in the ecosystem. For example, is the sender of this email in your contacts? Well if it is, this is probably not spam, even if it's got some sort of marginal contents. Has this email been widely broadcast within a short amount of time? Your email account can't tell this, but your email account provider can. these features, and then some match is going to happen in the middle where we're going to build a model and make predictions. We want to be able to predict labels of new images that are not the ones we've already seen, OK? So that's actually subtle, but it's super-important. We are not, like this is not-- This is not the sort of the Pokemon collection task here. We have to collect every digit, every image, right? Every image you see of a digit. is going to be unique. It's going to have to be at least one pixel off of something else you've seen. So you can't just collect all the data. You can get data that is similar, but then, in the end, you're going to need to generalize. What features might you use to detect digits? Well, somebody puts a grid of numbers. Your eyes and your visual processing system is already doing all kinds of processing. People will think about computer vision, replicate some of that processing. really noisy, and you're training set, they might be hard, expensive to label, because they're noisy. And then at test time, you're going to make mistakes because machine learning is not perfect. So people think about computer vision, think about invariances. What are better? We're just not all even going to agree on what the heck that's supposed to be. We need to find a way to make computer vision more accurate and more reliable, and that's where machine learning comes in. representations that if the thing gets tilted or it's a little bit lighter. It's not the exact pixels being on that we care about. But the pixels are something we could use. We could look at how many connected components of ink there are. What's the aspect ratio? How many loops are there? It's increasingly the case, especially for problems like this, that we feed in low-level features like pixels, and higher level features like edges. We'll talk about that in a couple weeks when we talk about narrow nets. some account activity and you want to red flag accounts that are suspicious. Automatic essay grading, auto grading, this can be a machine learning problem. Customer service email routing. You'd like to automate the routing of that. Review sentiment. Here's a bunch of reviews of my product. Which ones are good and which ones are bad? Have they gotten better in the past 10 days since the new announcement? And so on. You can do that with classification. You gotta do that before you can do things like translation. In model-based classification, rather than directly learning from errors that you make in the world from experience, think like reinforcement learning model-free. The Naive Bayes assumption is an extremely radical assumption as far as probabilistic models go, but for classification, it turns out to be really effective and it goes something like this. You build a model where the output label and the input features are random variables. There's going to be some connections between them, and maybe some other variables too that might help you build a better model. A general Naive Bayes model places a joint distribution over the following variables, y, which is your class, and some number of features, which you get to define. You're going to have to write code which extracts them from your input. So if your spam feature is, have more than 10 people received this email in the past hour? The machine learning will do the work to connect the probability of that taking on a certain value up to the class. And that means when you go to make a prediction, it decomposes into a product of a bunch of different feature conditional probabilities. The Bayes net is a model that assumes all the features are conditionally independent, given the label. For each feature-- which is each kind of evidence-- we're going to need to compute a bunch of conditional probabilities for each class. As I change the parameters of the model, different things are going to blink in and out as I think this is spam now, or ham now. So those numbers, collectively, will determine which predictions it makes. So it has to come from data, which parameters we want. 1 to 0 is equally likely. If you're looking at lots of round numbers, maybe it's 0. You can think about why that might be. So these come from data. And this actually underscores the point that depending on the data, depending on where you are, it depends on what you are looking at. And that's what we're trying to figure out here in this article. We want to know what you think is most common in real data. Or are they all equally common? So 0 might be common. How you collect the data can shape the distributions that you are imagining are going to exist at test time. In addition to the prior probability of each label, we can compute things like, what is the probability that pixel 3 comma 1 is on, given each class? This isn't a distribution over on or off. These are just the probability of that pixel for each class. And it's going to be some number. So for example, the pixel in that position might be pretty likely for the number six, but pretty rare for thenumber one. that comes in is going to get a probability for each class. And there's going to be a race to see which class wins in terms of probability. So what am I going to do? I'm going to have to see the words that come in, but I'mgoing to, in the end, compute two things. Whichever one is bigger, that's going-to be my prediction. If I want to know the probability that I assign, I have to take these two numbers and divide them by their sum. In this example, 2/3 of the emails are ham, and 1/3 are spam. This underscores that just because there's some distribution that your data reflects, and then there's the real world. And if there's major, major systematic ways in which your data and the distribution it was drawn from in its construction do not match the distribution in thereal world, you're going to have issues. One major issue you can have is reduced accuracy. And in spam, the most likely word is the. word free. predict 2/3 chance of ham. All right. The terms that are going to show up on the left here are. going to be the evidence terms for each word as it comes in. The thing on the right here is the product of all of these terms. What, minus 1.1? So, so far, we think it's ham because the log of that product is higher. Allright, next word came in. Gary. Remember, Gary is not particularly likely under either distribution. model, the way they're aggregated is multiplying their conditional probabilities. Gary, would you like to lose weight while you sleep? And if you look at this now, it thinks it's spam. Somewhere in there, somewhere around lose weight it changed its mind. You can see, weight is a pretty strong indicator. Apparently so is sleep. OK. So this is what it's like to be a Naive Bayes model. Features come in, you aggregate all of the weak evidence, and then you output. be weighed, and that's what's going on here in the conditional model. It's actually very common when you're multiplying probabilities to just add log probabilities instead. In the end, when you want to turn it into probabilities, you do need to sum them. And summing the logs won't do that. You need to do a sort of log sum, which one way to do that is to convert them back to probabilities by taking exponentials. That's actually not the way you would do it. You would sort of shift them by their minimum or their maximum as appropriate. word depends on the class and also the previous word. This Is a better model of language. If you started, if you did prediction in this, and you cranked out a pretend document, it would look significantly more like a real email than if you just did a bag of words. Will it be more accurate for classification? It really depends. In general, it will be a little more accurate, but at a cost of having significantly more complicated conditional probabilities to estimate. All right. Let's take a break now. We are now into the machine learning section which means we are done with the spooky Halloween time ghost-busting section. But I have candy. Come up and grab some, please. I'm not allowed to take it home. [NO SPEECH] All right, we're going to get started again. Let me say, your candy consuming I would rate as middle of the road. You can come back up to the-- at the end of the class if you would like to grab more. Allright, so let's talk about training and testing. So we talked a little bit about we want to build classifiers. We're going on the basis of data. How the heck am I supposed to know what pixel 7 comma 3 is for the number eight? I've got to get that from the data. Machine learning theory is based on trying to say something precise about the connection between what's going on in your data and this future used to which you're going to put the classifier to. The main worry is that in picking the parameters of your model, you do a really good job of capturing that training data, but it doesn't generalize. This is like you download all the exams from past years, and you optimize. You learn all those answers, and then you go to the final exam and it's totally, totally different questions that look nothing like those. You go through an experimentation cycle, it looks something like this. Get your model and learning already, and then you're going to learn the parameters. parameters are things like what's the probability of pixel 73 for the number eight? Then there's hyper-parameters, like, do I want to have features for the lowercased version of the words in case I've seen the word, but never uppercased? Right? These are questions about, is this or this orthis going to work better? You always know you're training data. The question is, do you generalize? This can happen to your classifier too, so you always want to test your performance on data that was not used to train it. And there can be a slow leak of your test data into your training data if you're not careful. So you try not to peek at the test set, and that's another reason why I say don't test your classifiers on the test data.to see that today. we have held-out data, which gives you something you can peek at. I ran 20 experiments. How did they go? Am I doing well? Is this thing good enough to release? You need to have some metric, and there's a lot of possible metrics. An easy one is accuracy. For how many of these emails did I make the correct decision? Fraction of instances predicted correctly, but actually, that's actually not a great metric for spam detection. Any ideas why? What's wrong with accuracy? cost-- of different kinds of mistakes may not be the same. And so accuracy isn't always what you want. What you really want, is you want a utility here. You want to know what was my utility, and you should have different costs for these things. There are also cases like machine translation, where you're always going to be a little bit off, a little word here or there, but there's a difference between being completely off and a tiny bit off. And again, we're going talk a lot today and next time about over-fitting and generalization. Spam detection is, in some ways, a very poor example of a canonical classification problem. The problem here is not that you're test accuracy is low, but your training accuracy was also low because you didn't learn anything. We'll investigate these things formally in a few lectures. I had a really good question during the break, which I want to answer for everybody, which is, couldn't you just defeat this Naive Bayes spam classifier by pasting the word Gary 100 times to the end of your offer to lose weight while you sleep? Spam is being generated by people who are trying to defeat spam filters. Spammers are going to double down on what's working. And so if you have features that are like, did the same email get sent to a lot of different people? What do spammers do? They're going to start modifying that email in some templated way. Now you have some feature that detects templates. Now there's sort of an arms race here. and so in that sense, in a sense, there's a spam arms race. over time, spam classification doesn't actually look like a standard classification problem because it's adversarial. OK, so in these images, you want to fit the hat right. You don't want it to be too small, because if you over-fit, you're not going to be able to generalize. Here's an example of this tradeoff. In general, we're going to do discrete classification. But for this example, let's imagine the thing we're trying to do is to fit a curve to this data. the data points of the squared distance or something. So what is my constant approximation to this? Does anybody want to hazard a guess? Let's call it five. OK, did I fit something about this data? Yeah, I felt something about the data. I fit basically it's mean. Did I capture the major trends? No. All right, let's try again. Let's fit a linear function. It's close, right? It's a better fit than the constant function. Notice that when I went to linear function, the space of hypotheses grew. than the quadratic. It's about fitting your data to the point where the patterns that you are capturing are ones which generalize to test, and that's a tricky balance. Over-fitting shows up not just on these continuous functions. It also shows up on discrete functions. And so, you can't basically just judge by your training accuracy. You need some measure of whether you've gone too far in the fitting process. And in this case, we talked about hyperparameters. A hyperparameter could be something like, what's the maximum degree of polynomial I'm allowed? In Naive Bayes, the numbers are the numbers two and three, let's say, are equally likely. The. probability of southwest, which occurs once in ham and zero in spam, is not zero in ham. It's really dangerous to give things probability zero. Words like Gary, except when I look at my data, it's actually a mess. It turns out, there are a bunch of words in this data which occur in spam once, and it could occur in once and occur in Spam zero. of over-fitting, where the exact details of which sample points you drew when you collected your data get captured in a way that doesn't generalize. In Naive Bayes probabilistic models, over- fitting usually shows up as sampling variance. For other methods, it's going to show up in totally other ways. OK. To do better, we need to smooth, or regularize, our estimates. So let's figure out some ways to do that, to just illustrate what it would look like to limit over-fits. you shrink a hypothesis space, you fit less. Using it too much, you under-fit. So let's take a look at the distribution of a random variable, just to sort of show why we need to do these kinds of things. We can do elicitation, right? You can ask a human. You can go to a doctor and say, hey, I'm building a classifier. What fraction of people with meningitis will present with a fever? And a doctor can give you a guess. You could also do that empirically. of patient treatment or something like that. And this is basically what learning does. You take your training data, you take the trends out of the training data. The simplest version of this is for each outcome to look at the empirical rate. So, for example, if I am a jelly bean-counting robot, and I am trying to figure out in this vat of jelly beans, how many reds versus blues there are, and it's two reds and two blues, well, what can we do? In practice, you need some smoothing. But we want no surprises to our model. We want our model to assign probability to events it's never seen. So that one errant pixel or word that is rare doesn't completely torpedo an otherwise very nuanced balancing of evidence. All right, so what was the maximum likelihood estimate? You have to work this out, right? Maybe we can just go back and do it real quick. OK. So let's say r is my probability of red, and one minus r isMy probability of blue. What is the probability of D.for red. of this data? Well, it's basically I got an r, and then I got another r. And then I've got the other thing, which is one minus r. So as I change the probability of red, this term, the likelihood of the data, is going to go up and down. And the balance, the point where that's going to be maximized you can sort of, if you set it up carefully, take derivatives, find the extreme point, you'll get the relative frequency answer out. two reds, there's three. There's those ones, plus my pretend red. And instead of one blue, there're two, because I have my pretend blue. Now what do I get? I get 3/5 and 2/5. Red is still winning, but this distribution has gotten flatter. And if there had been zero blues it would no longer be given probability zero. So pretty reasonable. We can do better. And so if I add zero, if I take Laplace's extended method and I addzero, then I just get my 2/3, 1/3 estimate from red, red, blue. there's 100 blues. Now how many reds do I have? Well, I do my computations as if I had 102 reds and 101 blues. And suddenly, even though there are still more reds than blues, in my posterior estimate here, it's pretty close to 50-50. So as I crank up k, I have a stronger prior, and I fit less. If I crank downk, I fit more, and so I now have a dial which can trade off the amount of fitting against generalization. example, I can go into my spam, and instead of computing odds ratios on the maximum likelihood-- or empirical relative frequency estimates-- I can instead do some smoothing. And suddenly things that only occurred once, they don't percolate to the top, because they haven't occurred enough to overwhelm that flat prior that I'm associating them with. So this is the top of the odds ratios for ham on the left, and favoring spam on the right. Some of these maybe make sense. Like, there it is. Free is probably in there somewhere. If you see money, that's a good sign that it's spam. But you might be wrong. Sometimes things surprise you, and that's why it's always good to like actually look into your model and see what has been learned here? Is there something that I can learn about this problem from what the machine has learned about the problem? All right. We talked about tuning. So let's say I build my Naive Bayes model for spam, for digits, whatever. I've got my features. Let's say they're mostly words and pixels. On your projects, you'll see you can do better. And I have some tuning to do. In general, your model is going to make errors. So we learn our parameters from the training data. We tune them on some different data, like some held-out data, because otherwise, you'll get crazy results. And then eventually, you're going to take the best value, do some final tests, test run. We're talking a bit more about features, because I think it's important for when we start to get to neural nets, where the stories here are going to change. In general, in general, you're going to need more features. In spam classification, we found out that it wasn't enough to just look at words. For digit recognition, you do sort of more advanced things than just looking at pixels, where you look at things like edges and loops and things like that. Try to do things that are more advanced than just pixels, like looking at loops and edges and edges, and try to look at other metadata from the ecosystem as well as just words. are invariant to rotation and scale and all of that the vision folks think about. You can add these as sources of information by just adding variables into your Naive Bayes model. We'll also talk in the next few classes about ways to add these more flexibly, and also ways to induce these. All right, I'm going to stop there for today, and as you go, please come up and grab some more candy. Thank you. Back to the page you came from.