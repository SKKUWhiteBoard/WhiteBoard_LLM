Andrej Karpathy: In this module, I'm going to briefly introduce the idea of differentiable programming. Differentiable programming is closely related to deep learning. It allows you to build up an increasingly more sophisticated model without losing track of what's going on. So let's suppose you want to do image classification. We need some way of representing images. To fix this problem, we introduce convolutional neural networks which is a refinement of fully connected neural networks. So here is an example of ConvNet in action. In NLP, words are discrete objects and neural networks speak vectors. So whenever you're doing NLP with neural nets, you first have to embed words, or more generally, tokens. So we're going to define an EmbedToken function that takes a word or a token x and maps it into a vector. And all this function is going to do is it's going to look up vector in a dictionary that has a static set of vectors associated with particular tokens. But the meaning of the words and tokens depends on context. So this representation of the sentence is not going to be a particularly sophisticated one. A simple RNN works like this: I take a hidden state, multiply by a matrix, take the input and multiply by the matrix. And then I add these two and I apply an activation function. LSTMs, or long short term memory, were developed to solve this problem. So now we have our sequenced model on RNN which produces a sequence of vectors, and the number of vectors depends on how long the input sequence is. So suppose we want to do classification, we need to somehow collapse that into a single vector. And the core part of a transformer is the attention mechanism.