Peter Solovits: I got a call from a committee of the National Academy of Science, Engineering, and Medicine. The committee is chaired by David Baltimore, who used to be an MIT professor until he went and became president of Caltech. Solovitz: They convened a meeting to talk about the set of topics that I've listed here. He says the group of us that talked about AI and decision making, I was a little bit surprised by the focus because Hank really is a law school professor. Trevor Noah: Algorithmic technologies may minimize harms that are the products of human judgment. Noah: We know that people are in fact prejudiced, and so there are prejudices by judges and by juries that play into the decisions made in the legal system. He says we should look for people with broad educations, like Trevor's father is white and his mother is African-American and his father is a Swiss guy. Trevor: What do you like about being able to defend a marriage that they were not allowed to marry six years ago? In California, the decision of whether you get bail or not is going to be made by a computer algorithm, not by a human being. The critique of these bail algorithms is based on a number of different factors. The data collection system is flawed in the same way as the judicial system itself, Peter says. Irene: If we're going to define the concept, what is fair? What characteristics would you like to have an algorithm have that judges you for some particular purpose? Yeah? When I was an undergraduate at Caltech, the Caltech faculty decided that they wanted to include student members of all the faculty committees. In those days, Caltech only took about 220, 230 students a year. So one day, one of the professors said here's what we ought to do. We ought to take the 230 people that we've just offered admission to and we should reject them all and take the next 230 people, and then see whether the faculty notices.look like they're a better bet. Peter Zolovits: Disparate treatment and disparate impact are really in conflict with each other. He says we don't have a nicely balanced set where the number of people of European descent is equal to people of African-American, or Hispanic, or Asian, or whatever population you choose. Zolovich: The good news is that there's a terrible piece of news, which is that you can prove that it's not possible to achieve any of these pair of conditions jointly. In the US, there were tests of this sort done, but the problem was that a lot of African and African-American populations turned out to have this genetic variant frequently without developing this terrible disease. And it was only after years when people noticed that these people who were supposed to die genetically weren't dying that they said, maybe we misunderstood something. And what they misunderstood was that the population that was used to develop the model was a European ancestry population and not an African ancestry population. So you go, well, we must have learned that lesson. The study looked at how machine learning models can identify disparities in general medical and mental health. It found a racial bias in the data that we have and in the models that we're building. In psychiatry, when you look at the comparison populations, you see a fair amount of overlap. The models are not going to give us as accurate predictions, but you still see, still, a huge gap in the confidence intervals between them. The study was published in the American Medical Association's Journal of Ethics, which I didn't know existed. is choose some family of models to try to fit, and then I'm going to use some fitting technique, like stochastic gradient descent or something, that will find maybe a global optimum, but maybe not. And then there is noise. And so his observation is that if you count O as the optimal possible model over all possible model families, then the bias is essentially O minus L. The variance is like L minus A, it's the error that's due to the particular way in which you learned things. can't define a universal notion of what it means to discriminate because it's very much tied to these questions of what is practically and morally irrelevant in the decisions that you're making. And so it's going to be different in criminal law than it is in medicine. And it's feature-specific as well, so you have to take the individual features into account. The government has tried to regulate these domains, and so credit is regulated by the Equal Credit Opportunity Act, education by the Civil Rights Act and various amendments. scoring function is independent of the protected attribute. So that says, can we build a fair scoring function that separates the outcome from theprotected attribute? So here's some detail on those. If you look at independence-- this is also called by various other names-- basically, what it says is that the probability of a particular result, R equal 1, is the same whether you're in class A or class B. That tells you that the scoring function has to be universal over the entire data set. And then the final criterion is sufficiency, which flips R and Y. Hiring is based on a good score in group A, but random in B? So for example, what if we know a lot more information about group A than we do about group B? Or alternatively, it could be caused by malice also. There's also a technical problem, which is it's possible that the category, the group is a perfect predictor of the outcome, in which case, of course, they can't be independent of each other. And so they say, well, we could use an optimal separated score. which is some combination of X and A, and you do this by maximizing the mutual information between X and Z. So this is an idea that I've seen used in machine learning for robustness rather than for fairness, where people say, the problem is that given a particular data set, you can overfit to that data set. One of the ideas is to do a Gann-like method where you say, I want to train my classifier, let's say, not only to work well on getting the right answer, but also to work as poorly on identifying which data set my example came from. does in other populations, and the FDA has actually approved the marketing of that drug to those subpopulations. And if you think about the personalized medicine idea, which we've talked about earlier. The populations that we're interested in becomes smaller and smaller until it may just be you. And so there might be a drug that works for you and not for anybody else in the class. But it's exactly the right drug for you, and we may get to the point where that will happen and where we can build such drugs. degree of calibration will give you a good approximation to this notion of sufficiency. These guys in the tutorial also point out that some data sets actually lead to good calibration without even trying very hard. So for example, this is the UCI census data set, and it's a binary prediction of whether somebody makes more than $50,000 a year if you have any income at all and if you're over 16 years old. It's almost exactly along the 45 degree line without having done anything particularly dramatic in order to achieve that. The probability that you visited the Grace Hopper Conference is dependent on your gender. Computer scientists are much more likely to be programmers than non-computer science majors. The optimal score is going to depend basically on whether you have a computer science degree or not. If you're a historian, you're not likely to have been interested in going to that conference. It's a really cool conference. Grace Murray Hopper invented the notion bug or the term bug and was a really famous computer scientist starting back in the 1940s. We used LDA, standard topic modeling framework. And the topics, as usual, include some garbage, but also include a lot of recognizably useful topics. And so what we found is that, for example, white patients have more topics that are enriched for anxiety and chronic pain, whereas black, Hispanic, and Asian patients had higher topic enrichment for psychosis. And we were speculating on how this relates to sort of known data about underdiagnosis of COPD in women. So again, stereotypes of what's most common in these different groups. Public insurance patients often have multiple chronic conditions. Public insurance patients have atrial fibrillation, pacemakers, dialysis. Private insurance patients has higher topic enrichment values for fractures. The error rates on a zero-one loss metric are much lower for men than work here and embarrassing myself. So this is modeling mistrust in end-of-life care, and it's based on Willie's master's thesis and on some papers that came as a result of that. It could be any of a lot of different factors, but that's the case. The study looked at trust between patients and their doctors. It found that black patients were more likely to be distrustful of the medical system than white patients. The study also found that mistrust was related to severity, not to social differences. The researchers are still trying to understand what the correlation is between mistrust and severity, and how it's related to the patient's ability to be trusted by the medical care system. They are also trying to find out how to make the study more useful for the general public.