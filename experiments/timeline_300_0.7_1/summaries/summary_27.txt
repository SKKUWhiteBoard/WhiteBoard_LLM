James Swan: I hope everybody saw the correction to a typo in homework 1 that was posted on Stellar last night and sent out to you. The TAs gave a hint that would have let you solve the problem as written. But that's more difficult than what we had intended for you guys. So maybe you'll see the distinction between those things and understand why one version of the problem is much easier than another. So we've got two lectures left discussing linear algebra before we move on to other topics. We ran out of time a little bit at the end of lecture on Wednesday. There were a lot of good questions that came up during class. We saw that reordering is important. In fact, it's essential for solving certain problems via Gaussian elimination. If you don't, well-- my PC had-- I don't know-- like, 192 gigabytes of RAM. The elimination on that matrix will fill the memory of that PC up in 20 minutes. And you'll have to reorder in order to minimize fill-in. be stuck. It won't proceed after that. So it's the difference between getting a solution and writing a publication about the research problem you're interested in and not. So how do you do reordering? Well, we use a process called permutation. There's a certain class of matrix called a permutation matrix that can swap rows or columns. So if I want to swap columns, I multiply my matrix from the right, IP transpose. If I swap the rows and then I swap them back, I get back what I had before. This is a form of preconditioning. We discussed sparse matrices and a little bit about reordering and now permutation. There are lots of variants on Gaussian elimination that we can utilize. There's a sparse matrix that describes how the probability of finding the Plinko chip in a certain cell evolves from level to level. It works the same way the cellular automata do. It's always done via Gaussian Elimination if we want an exact solution. But you don't do elimination on an entire full matrix. You do it on a sparse Matrix whose structure you understand. model I showed you last time works. If the chip is in a particular cell, then at the next level, there's a 50/50 chance that I'll go to the left or I'll going to the right. And there's some sparse matrix A which spreads that probability out. And we'll see the simulation that tells us how probable it is to find the Plinko chip. Yes? AUDIENCE: [INAUDIBLE] JAMES W. SWAN: Yeah. So in diffusion in general? Eigenvectors of a matrix are special vectors that are stretched on multiplication by the matrix. So they're transformed. But they're only transformed into a stretched form of whatever they were before. For a real N-by-N matrix, there will be eigenvector and eigenvalues, which are the amount of stretch. And finding the eigenvalue of a complex number is the subject of today's lecture. The lecture will be on the topic of complex numbers, or complex matrices, and how they relate to each other. eigenvectors and eigenvalues are non-linear because they depend on both the value and the vector, the product of the two, for N plus 1 unknowns. We can never say what an eigen vector is uniquely. We just care about its direction. The amount of stretch, however, is unique. It's associated with that direction. And you have a direction. That describes the eigenvector-eigenvalue pair. Let's go with this idea that w belongs to the null space of A minus Lambda I. do an example. Here's a matrix, minus 2, 1, 3. And it's 0's everywhere else. Can you work out the eigenvalues of this matrix? Let's take 90 seconds. You can work with your neighbors. See if you can figure out that matrix. Nobody's collaborating today. I'm going to do it myself. OK. What are you finding? Anyone want to guess what are the eigevalues? AUDIENCE: [INAUDIBLE] JAMES W. SWAN: It's OK. roots of the secular characteristic polynomial. They are the eigenvalues. It turns out the diagonal elements of a triangular matrix are eigen values, too. If there is a complex eigenvalue, then necessarily its complex conjugate is also an eigen value. So the determinant of a matrix is the product of the eigenevalues and the trace of the matrix, which is the sum of its diagonal elements. We'll come back to that idea in a second. a matrix is also the sum of the eigenvalues. These can sometimes come in handy-- not often, but sometimes. Here's an example I talked about before-- so a series of chemical reactions. We want to know how the concentrations of A, B, C, and D vary as a function of time. And our conservation equation for material is here. This is a rate matrix. The eigen values of that matrix are going to tell us something about how different rate processes evolve in time. eigenvalues of the rate matrix? AUDIENCE: 0. JAMES W. SWAN: 0's an eigenvalue. Lambda equals 0 is a solution. Minus k1 is another solution. What is this eigen Value 0 correspond to? What's that? What physical process does that represent? It's something evolving in time now, right? So that's the transformation of A into B. And the eigenvector should reflect that transformation. We'll see what those eigenvectors are in a minute. We want to know the eigenvector of the rate matrix having eigenvalue 0. This should correspond to the steady state solution of our ordinary differential equation. Can you do that? Can you find this eigen vector? Try it out with your neighbor. See if you can do it. And then we'll compare results. Are you guys able to do this? Sort of, maybe? Here's the answer, or an answer, for the eigenector. It's not unique, right? It's got some constant out in front of it. James Swan: Can you find the eigenvalues and some linearly independent eigenvectors of this matrix? And if you find them, what are the algebraic and geometric multiplicity? Well, you guys must had a rough week, you're usually much more talkative and energetic than this. JAMES W. SWAN: OK. Can you give me eigenavectors of the matrix? Can you Give me linearlyindependent-- yeah? AUDIENCE: [INAUDIBLE] JAMESW. SWan: Good. So this is a very ambiguous sort of problem or question, right? The problem of finding the eigenvectors is as hard as the problem of solving a system of equations. We want to be able to do a type of transformation called an eigendecomposition. It's useful for solving systems of equations or for transforming systems of ordinary differential equations. The Lanczos algorithm is a method that takes products of your matrix with certain vectors and from those products, infer what eigenvalues are. That's the way a computer's going to be going to do it. W lambda W inverse-- and I define a new unknown y instead of x, then I can diagonalize that system of equations. Each of them satisfies their own ordinary differential equation that's not coupled to any of the others, right? And it has a simple first-order rate constant, which is the eigenvalue associated with that particular eigendirection. So this system of ODEs is decoupled. And it's easy to solve. It's an exponential. And that can be quite handy when we're looking at different sorts of chemical rate processes. triangular form for this matrix. We'll talk next time about the singular value decomposition, which is another sort of transformation one can do when we don't have these complete sets of eigenvectors. You'll get a chance to practice these things on your next two homework assignments, actually. So it'll come up in a couple of different circumstances. I would really encourage you to try to solve some of these example problems that were in here. Solving by hand can be useful.