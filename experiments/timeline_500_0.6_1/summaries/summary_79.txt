So, last time we were starting to talk about the sort of the general overview of what reinforcement learning involves. We introduced the notion [NOISE] of a model, a value, and a policy. Can anybody remember off the top of their head what a model was in the context of reinforcement learning? So what we're gonna do today is, sort of, um, build up for Markov Processes, up to Markov Decision Processes. And this build, I think, is sort of a nice one because it allows one to think about what happens in the cases where you might not have control over the world. The question was was if it's possible to have self-loops? Um, could it be that this is sort of circulator defined [NOISE] in this case. Is it ever actually possible for, uh, that matrix not to have an inverse or does like the property that like column sum to one or something make it not possible? It's a good question. I think it's basically never possible for this not to has an inverse. I'm trying to think whether or not that can be violated in some cases. Markov Decision Processes are the same as the Markov Reward Process except for now we have actions. So, the agent is in a state they take an action, they get immediate reward, and then they transition to the next state. The advantage of this is that each of the iteration updates are cheaper and they'd also will be some benefits later when we start to think about actions. Now, if we think about our Mars Rover MDP, let's just define there being two actions being A1 and A2. before we do this let's think about how many policies there might be. So there are seven discrete states. In this case it's the locations that the robot. There are two actions. I won't call them left and right, I'm just going to call them a_1 and a_2. Then the question is how many deterministic policies are there and is the optimal policy for MDP always unique? So kind of right we just take like one minute or say one or two minutes feel free to talk to a neighbor.