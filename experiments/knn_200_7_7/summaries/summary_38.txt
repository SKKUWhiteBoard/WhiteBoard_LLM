So I think I just spend like five minutes, just briefly review on the backpropagation last time. So I didn't have time to explain this figure, which I think probably would be useful as a high level summary of what's happening. So you start with some example x, and then you have some-- I guess, this is a matrix vector multiplication module, and you take x inner product multiplied with w and b. And then get some activation, the pre-activation. And you get some post activation. This is how you define network and the loss function. done by one of the lemma that we discussed last time. Next lecture, we're going to talk about the concept of regularization. And next lecture we also talk about some of the practical viewpoint of ML. Any questions? This is just an extension of the last five minutes of the previous lecture. OK. So good. So now in this lecture, and the lecture afterwards, we are talking about, I guess, a few concepts. One concept is called generalization, which is the main point of this lecture. The test loss is defined on unseen examples. So suppose, say, you draw some new example, x comma y, from some distribution D. And then you evaluate what's the expected loss on this new test example. And typically, when l theta is big, there are two failure mode in some sense. So one of the failure mode is called overfitting. So a typical situation of overfitting is that the training loss, j, is small, but the test lost is big. In some sense, you care about two quantities: the training loss and the gap. You want both of these two to be small. An underfitting, basically, just means that you face something like this. So you can see that there's a large training error, training loss or training-- let's call it loss just for consistency. And now let's think about so what you should blame. Why the training is big? What's the culprit? The culprit, I would argue, is that no any linear model can fit your data. So suppose you have some x and some y. You have some data set. The data are approximately quadratic. So given x, you want to predict y. And you observe some-- so you have aData set. For example, you have four points. You want to fit a line to it or fit some curve to it. And the question is what curve you are going to fit? So suppose, you fit something crazy like this. Let me try to see what color I'm using for this. Sorry. The bias is going to be a decreasing function as the model complexity. The variance is, in some sense, you can say, it's not very important. Only the bias is the culprit. And now, I'm going to show cases where the variance is the. culprit to blame for. So any questions so far? Why is the bias [INAUDIBLE]. Why is bias this crazy? Oh, squared, I mean. Oh, this is just because it's kind of a unit thing. The bias is basically like it's saying that the reason why-- I don't know exactly why people call it bias in the very first time. So this is just because the linear model is not expressive enough. And this is called bias. And you can kind of see that it's probably important for bias to be small because if bias is large, even with infinite data, you cannot do anything. And by that, reasons. One is like you have lack of data, and the other is you have too expressive models. If you don't have enough points, and your degree is high enough, then you can always make the training error 0, literally 0. And the thing is that this is overfitting. So basically, you are looking at-- you are kind of like overfitting to the spurious patterns, but instead of the big pattern. The big pattern is this. The spurious patterns are the fluctuations in some sense. And so in other words, I think you are explaining the noise instead of. the ground truth. When you draw the same number of samples with similar ground truth-- the same ground truth and the solution. But just their randomness are different. And that's a good question. That's exactly what I'm going to talk about next. OK. When we don't know the ground truth, you cannot really exact like-- let me think. How do you know that you are having a large bias? You cannot really exactly know. When you don't have infinite data, there is no way to evaluate the bias. So typically, what you do is you say, "Underfitting means you have a large training error" bias square [INAUDIBLE] what's the third one? The third one is the sum of them. This is the test error. And the bias is the total of them? Bias is this one. I'll discuss that in a moment. I think I do have something to say about the variance, and then I'll come back to the trade-off. OK. Are we [INAudIBLE] for highly imbalanced data set? So maybe let's discuss this offline. it means that it's somewhere in the middle. So basically, when you see the training error is big, you kind of see your biases. You kind of believe that your bias is too high, so that's why you should increase the model complexity. And at some point, you find that you are in other regime, where the variance is toohigh, then you should stop. Basically, you increase theModel complexity to some extent until your bias and variance has a right trade-off. of bias and variance first change, did you use different type of model [INAUDIBLE] So I think this figure, so this is the-- OK. You ask a good question. So probably, the best thing is to use quadratic. Quadratic is, in principle, expressive enough to express our data. That's probably the best solution. And if you really run the algorithm, the quadratics, you would probably recover something very close. But if you're going to go with a different model, there's always a trade-off. Double descent is the second descent of the test error. In this regime, typically, the number of parameters is larger than the numbers of data points. This peak is often happening when-- it's roughly equal to d. This is basically mostly kind of correct for linear models. But for nonlinear models, whether this is exactly equal to 2 or not is less clear. In some sense, the norm is also a way to describe how many, like you have a small ball-- you have fewer choices to fit your data. that, you have more data. It actually helps. I saw some questions. So the original double descent, does that like continue to decrease or does it eventually increase again? So in the first. figure. This is, again, more than this function. This was like this has been for a while. For this one? Yeah. This phenomenon. This one, I think, is also-- actually, the paper that first systematically discussed this is like 2020. About that peak, when was that discovered? The peak? This is discovered in the same paper, the peak. people really care about it. Even within linear models, you can still change the complexity, just to clarify that. And most of this theoretical study, I think, are for linear models. And they are pretty precise these days. And I'm going to try to kind of roughly summarize the intuition from the study. And what I mean by that is that you can try to change the model complexity. So you can start with only using one feature or two features like for example, in the house price. That means you have more and more parameters. The peak is caused because the algorithm was suboptimal. The norm of the theta, the linear models you learned, is very big when n is roughly equals d. So regularization would mitigate this to some extent. But there's one more question, which is there is no peak, but why there's no ascent? So suppose you just see this. Actually here, you will also see this, something like this. So this one, let's say, we are OK with it. We are happy if you see just a peak. I guess we are not going to go into that. But at least, the immediate reason is that when n is close to d, somehow, this algorithm is producing a very large norm on classifier theta. You can argue that if the norm is too big, then your model is too complex. So very complex on [INAUDIBLE] to the norm. So this model, it seem it doesn't have a lot of parameters compared to, for example, this model. That's by definition. The norm is actually very big.