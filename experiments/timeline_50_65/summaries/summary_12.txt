Welcome back to week three of CS224N. This week in week three, we're actually going to have some human language, and so this lecture will be about that. Okay. Let's get started again. So we- we've got a bit of a change of pace today after week two. So, um, this week in weeks three and four, we'll be talking about human language. We're going to talk about the human mind. We'll talk about how it works. has no partial derivative signs in it. And so we'll be moving away, um, from sort of working out the so technicalities of doing, um,. new networks and back propagation, and a sort of math heavy week two. So then, this week, what we actually want- well, in today's Daily Discussion, we want to talk about what we want in the future of the Internet. We'll be talking about that in this week's Daily Chat, and we hope you'll join us for the next two weeks. lecture, we want to look at, well, what kind of structures do human language sentences have, and how we can build models that, um, build that kind of structure for sentences that we see. Um, then going particularly focusing on dependency grammars, and then gonna present a method for doing dependency structure, dependency grammar parsing called transition-based dependency parsing. And then talk about how you can make linguistics in 20 minutes or something. So, that's kind of like, uh, linguistics In 20 minutes. neural, um, dependency parsers. In assignment three, what you're doing is building a neural dependency parser. The other thing that happens in assignment three is that, we start using a deep learning framework PyTorch. So, for doing assignment 3, instruction zero, is to learn about neural networks, and then jump straight right in to building a Neural dependency Parsers. That's assignment three. And assignment two was due one minute ago, so I hope everyone's succeeded in getting assignment two out of the way. and this is in the PDF for the assignment, is to install PyTorch as a Python package, and start using that. Um, so we've attempted to make assignment three sort of be a highly scaffolded tutorial, where you can start to learn how to do things inPyTorch by just, just, installing it. And this is how you do it in the first place. It's a very simple tutorial. It doesn't require a lot of knowledge, just some basic Python knowledge. um, writing a few lines of code at a time. Hopefully that works out for people. Um, if you have any issues with, with that, um, well, obviously, you can send Piazza messages, come to office hours. I mean, the one other thing you could think of doing is that, you know, send me a message on Twitter or something like that. If you want to send a message to me on Twitter, I'll send it to you. there's sort of a one hour introduction to PyTorch on the PyTorCh site, where you down- where you're directed for installingPyTorch, and you could also look at that if that was maybe helpful. Um, now the final mentions, yes. So, um, final projects, um,. you know, we're going to be going to, we'll be working on some more projects in the next few days. We'll be talking about some of the things that we've been working on. sort of focus on those more in week five, but if it's not bad to be thinking about things you could do, if you're under a custom final project. You're certainly encouraged to come and talk to me or the TAs. We have under the sort of office hours page page for people who want to talk to us about custom projects. We'll be focusing on that more in the next few weeks, but it's always good to think about what you can do. on the website, a listing of the expertise of some of the different TAs. Um, since I missed my office hours yesterday, I'm gonna have a shortened office hour tomorrow from 1:00 to 2:20. um, that's at the same time as the, um, normal CS224N, Um, office hours, so you can join in on the fun. I'll be in the office from 1-2:20, so if you want to join in, you can. can kind of come for any reason you want, but it might be especially good to come to me if you want to talk about, um, final projects. Okay. So, let's leap in and start talking about the structure of sentences. And so, I just sort of want to explain, Ijust sort of wants to explain how sentences work. Okay, so that's what I'm going to do. And then we'll go on to other things. And that's the way it goes. something about human language sentence structure, and how people think about that structure. What kind of goals then people in natural language processing have of sort of building structure to understand the meaning of sentences. Um, all of the examples I'm going to give today are in English, um, and they're all in English. They're just examples of sentences that we can use in a natural language context to understand what's going on in a sentence. So, they're just English sentences, but they're written in different ways. because that's the language that you're all expected to have some competence in. This is meant to be sort of ideas of how you can think about the structure of human language sentences that are applied to all. But this really isn't meant toBe sort of facts about English. It's a way of thinking about how we think about human language and how it can be applied to a variety of situations and situations. And it's not just about English, it's about how to think about language in general. sorts of languages. So in general, there are two different ways that linguists have thought about the structure of sentences, though there are some relations to them. One of them is called phrase structure, or phrase structure grammars. And if you vaguely remember from CS103 if you did that, when you did it, when did you do it? You did it when you were in the middle of a conversation. You were talking about a sentence, and you were trying to make a point. Phenomenal structure grammars are using the tools of context-free gramMars to put structures over sentences. So, I'm first of all going to just briefly introduce that, so you've seen it, but actually the main tool that we're going to use in this class is the phrase structure Grammars. You spent about the lecture on context- free grammar, um, phrase structuregrammars, are using those tools to create sentences. We'll use those tools in the next class. this class and for assignment three, is to do put dependency structures over, um, sentences, so I'll then go about that. So, the idea of phrase structure is to say that sentences are built out of units that progressively nest, so, we start off with words that, cat, cuddly, et. And then, we go on to sentences that are based on those words, such as cat and cuddle. And so on and so on, until we get to the end of the class. cetera, and then we're gonna put them into bigger units that we call phrases, like "The cuddly cat by the door", and then you can keep on combining those up into even bigger phrases. Um, [NOISE] Okay, that's that. So, how does this work? Well, you can combine phrases like "cuddly" and "cat" into bigger phrases, such as "the cuddy cat" or "by the door" and so on. work? Well, so the idea of it, and this is sort of the way linguists thinks, is to say, "Well, here's this language, which, you know, might not be English. It might be Oaxacan or some other language. What kind of structure does it have?" And well, we could look at it. And we could find out what kind of language it is, and how it's structured. And so on and so on, and on. at lots of sentences of the language. And so the linguist is gonna think, "Well, I can see, um, patterns, like the cat, a dog, the dog, a cat, et cetera" So, it's sort of seems like there's one word class here, which linguists often referred to as determiners. Um, that's what I'm trying to get at, is that there's a word class in the English language that's called a "determiner" they're also referred to as article sometimes in English. There's another word class here of nouns. And so, what I- to capture this pattern here, it seems like we can make this unit, um, that I see all over the place in language, which is made of a nouns, a word class, and a word group. I think that would be a good way to capture it. I don't know if that's possible, but that's what I'm trying to do. a determiner, followed by a noun. So, I've write, um, a phrase structure grammar role, a context-free grammar role of- I can have a noun phrase that goes to a determiner and a noun, and that's not the only thing I can do. Okay. But, you know, that is not the other thing that I can, uh, see. So,. I've written, um,. a phrase Structure Grammar Role, which is a grammar role that allows me to write phrases that go to determiners and nouns. can also see, um, other examples in my language of the large cat, or a barking dog, or the cuddly cat. So, that seems that I need to put a bit more stuff into my grammar. Maybe I can say from my grammar that a noun is a noun? I don't know. I'm not very good at grammar. I think I need more practice with it. I've got a bit of a bad habit of saying things like 'cuddly' instead of 'cat' phrase goes to a determiner, and then optionally, you can put in an adjective. And then I poke around a little bit further and I can find examples like the cat in a crate, or a barking dog by the door. And I think that's a good way to start. I think it's a very good way of starting a sentence. I don't know why it's so hard to start a sentence with a noun. It's a bit of a mystery to me, but I think there's a reason. can see lots of sentences like this. And so I want to put those into my grammar. But at that point, I noticed something special, because look, here are some other things, and these things look a lot like the things I started off with. So, it seems like, which seems like a good way to start a sentence. So I started with that, and then I went on to the next sentence. And then the next one, and the next, and so on until I got to the end. sort of having a phrase with the same expansion potential that's nested inside this bigger phrase, because these ones can also be, um, expanded, right? I could have something like the green door something in here. So, I just wanna capture that in some way. Maybe I could say "the green door" or something like that. I don't know. I'm just trying to think of things that can be expanded into something else. I just want to think about it and see what comes out of it. that a noun phrase goes to a determiner, optionally an adjective, a noun, and then a something else, which I'll call a prepositional phrase. And then I'm gonna write a second rule saying that aPrepositional phrases go to a preposition, that's gonna be these words here, um, followed by a noun. And that a Prepositional phrase goes to a preposition following a noun phrase, and so on. a noun phrase. So then I'm reuse- [NOISE] I'm reusing my noun phrase that I defined up here. I can sort of say, "The cat by the, the large door." Or indeed I could say, 'The catby the large crate." Um, that's a good one. That's a great one. And then I could immediately generate other stuff. And I could do that all the time. So that's what I'm doing right now. "The cat by the large crate on the table", or something like that, because once I can have the prepositional phrase includes a noun phrase, I've already got something that I can kind of recursively go back and forth between noun phrases, he says. "Once I have a prepositionsitional phrase, and a noun phrases includes aPrepositions," he says, "I have something I can go back to again and again. I can do that with noun phrases and Prepositions" and I can make infinitely big sentences, right?Yeah? Yeah? So, I could write something like, yeah, "The cat by the large crate on the, um, large table, by the door." Right. I can keep on going and make big sentences. And I could say, well, I've got a- And I can say,  "I've got a large table by the door" Right. And I can keep on going and make big sentences. Right. I don't have space to fit it on this slide, but I've got an analysis of this according to my grammar, where that's a noun phrase. The prepositional phrase goes to a preposition, and a noun phrases, and this noun phrase go to a determiner noun prepositionally phrase. This noun phrase is called a "prepositional" phrase, and it goes to the noun phrase "preposition" The noun phrase then goes to "postpositional," and so on. a determiner, adjective, noun prepositional phrase. And that goes to a preposition, and another noun phrase, and I keep on going and I can produce big sentences. Okay. You know, that kind of then continues on, because, um, you know, I can then start seeing more bits of grammar. So, yeah, that's kind of what I'm doing. That's how I'm going to go on with it. I'm just going to keep going. I could say, "Well, I can now talk to the cat." Um, and so if I wanna capture, um, this talking to a cat here, well, that now means I've got a verb, because words like talk and walk are verbs. And then talk to  the cat, it seems like. It seems like that's what I'm going to do. I think that's the way to do it. I don't know if it's going to work, though. after that, it could become a prepositional phrase. And so I could write another rule saying that a verb phrase goes to a verb followed by a preposition. And then I can make more bigger sentences like that. And I could look at more sentences of the language and change the way they're written. I could do that with more and more sentences, I think. I think that's a good way to go. I don't know how to do it yet, but I think it would be a good start. start building up these, these context-free grammar rules to describe the structure of the language. And that's part of what linguists do, and different languages, um, have different structures. So, for example, like in this, uh, little grammar I've had and in general in English, in general, what you do, is you do the same thing in English as you do in other languages. And so that's what we're trying to do in this little grammar lesson. what you find is that prepositional phrases following the verb. But if you go to a different language like Chinese, what you finds is the prepositions come before the verb and so on. And so, we could say okay, there are different rules for Chinese, um, and I could start writing a Chinese novel. But I could also write a novel in any language that has different rules. And I could do that in English, for example, or in any other language, for that matter. context-free grammar for them. Um,so that's the idea of context-free grammars, and actually, you know, this is the dominant approached linguistic structure that you'll see if you go and do a linguistics class in the linguistics department, people make these kinds of Phrase Structure Grammar trees. Okay, beauty. But that's not what we're going to talk about here. We're not going to get into that right now. We'll talk about it later. just to be contrary, no, it's because this alternative approach has been very dominant in computational linguistics. What I'm going to show you instead, um, is the view point of dependency structure. So, the idea of dependency Structure is rather than having these these structures, we have this structure of rules and rules that can be added to each other. That's what dependency structure is, and that's what we're going to look at in the next section of this article. Back to Mail Online home. back to the page you came from. sort of phrasal categories, like, noun phrases and prepositional phrases, and things like that. We are going to directly, um, represent the structure of sentences by saying, how words, how arguments or modifiers of other words in a recursive faction. Which is sort of another way of saying how the word structure is expressed in the language of a language. We will be using this language in a number of ways in the next few weeks, including in a series of articles. dependence on other words. So, we have a sentence, ''Look in the large crate in the kitchen by the door''. And if we want to we can give these word, words word classes, so we can still say this is a verb, and this is preposition. And this is dependent on others. And we can say, 'This is a preposition, this is dependence on others', and so on. And so on, and on, until we get to the end of the sentence. a determiner, and this is an adjective. And this is a noun. But to represent the structure, what we're going to say is, "Well, look here is the the root of this whole sentence." So, that's where things start. Um, and then, well, where are we going to look is a determiner and an adjective, and a noun, and so on. And that's the structure of a sentence. And so that's what we are going to do. in the large crate, so that is a dependent of look. And well, if we- then we have for the crate, it's got some modifies its a large crate. So, that's adependent of crate. And in this system of dependencies, that is also a dependence of the size of the crate. That's a dependency of the type of crate that is being used. And so on and so on, until we get to the point where we have a system that works for us. I'm going to show you, we've got in as kind of, um, a modifier of crate in the large crate. And well, then we have this next bit by the door. And as I'll discuss in a minute, well, what does the by-the-door modifying? It's still modifying the crate, it saying, ''It's the crate by thedoor.'' Okay. And so that's then, um,. the structure you get may be drawn a little bit more neatly when I did that in. advance like this. And so we call these things, uh, dependency structure. What we're doing here, um, is that we're- sorry, I had two different examples. [NOISE] Different examples. Um,Um, what we're saying is saying, what, what words modify other words? And so, that allows us to do things like this, you know, like advance like this and so on. So, that's what we are doing here. We're saying what words modifying other words allow us to advance. us to sort of understand how the different parts of the sentence relate to each other. And so, overall, you know, then- let me just so say here, you might want to why do we need sentence structure? You know, the way, um, language seems to work when you're talking. And that's kind of what we're trying to do here, I think, is to get people to understand what's going on in a sentence. So that's sort of the first step. to your friends is that you just blab of something, and I understand what you're saying, and, um, what goes on beyond that is sort of not really accessible to consciousness. But well, to be able to have machines that interpret language correctly, we sort of need to understand, he says. He says that's what we need to do to make sure we're not all talking to each other at the same time, and that we're talking to one another in different ways. the structure of these sentences, because unless we know what words are arguments and modifiers of other words, we can't actually work out what sentences mean. And I'll show some examples of that as to how things go wrong immediately, because actually, a lot of the time there are different ways of saying the same thing that are not always clear to the reader at the same time. And so that's what we're trying to get out of the way here, so that we can get to the root of what's going on. possible interpretations you can have. And so, in general, our goal is, you know, up until now we've sort of looked at the meaning of words, right? We did word vectors, and we found that words there was similar meaning, and things like that. Um, and you can get somewhere, and I think that's what we're trying to do here. I think we're going to try to get to a point where we're able to do that. in human languages with just saying words. You can say, "Hi", and friendly, um, and things like that, but you can't get very far with just words, right? The way human beings can express complex ideas and explain and teach things to each other, is you can put words on top of each other. That's how languages work, and that's the way languages work. It's not just about saying 'Hi', it's about saying, 'I love you' or 'I'm happy to see you' together words to express more complex meanings. And then, you can do that over and over again recursively to build up more and morecomplex meanings. By the time you're reading the morning newspaper, you know most sentences are sort of 20-30 words long, and they're saying, um, "I'm going to go to the bathroom" or "I am going to get a drink of water" Or "I want to go for a walk," or " I'm going for a run." some complex meaning, like you know, "Overnight Senate Republicans resolve that they would not do blah blah blahblah.'' And you understand that flawlessly, by just sort of putting together those meanings of words. And so, we need to be able to know what is connected to what in order to understand what is going on, he says. "We need to know ... what's going on," he says, "in order to make sense of what's happening in the world" to be able to do that. And one of the ways of saying, um, that's important is saying, ''What can go wrong?'' Okay. So here, is a newspaper article. Uh, ''San Jose cop kills man with knife''. Um, now, this has two meanings and the two meanings, um,. depend on, "How do you stop a cop from killing a man with a knife?" "What do you do when a cop kills someone with a gun?" "Do you do anything to stop a police officer from killing someone?" well, what you decide depends on what, you know, what modifies what? So, what are the two meanings. Meaning one. The cop stabs the guy. Right. So, meaning one is the officer stabs that guy. So,. what we've got here is, we've Got the cop stabbing that guy, right? Right. We've got the cop stabing that guy,. right? We've Got The Cop Stabbing That Guy, Right. Right? So. What do you think? cops that are killing. So, this is what we'll say is the subject of kill, is the cops, and I'll just call them the San Jose cops here. And well, there's what they kill which say that, the man is an object of killing. Um, and then while one person is killed, another person is also killed. And then while another person dies, the other person is still alive. And so on and so on. And that's the way it goes. is the, the cop using knife to kill the person. And so that's then that this is, um, modifier and here if we complex we call it an instrumental modifier to say that the cops are killing people with a knife. That's one possible analysis. Okay. Then, there's a second analysis. And that's the one that we're going to go with here. And we'll go with that analysis. We'll go back to the start of this article. We're just going to start with the first one. meaning sentence can have. The second meaning the sentence can has is, that's the man has a knife. So, um, in that case, what we wanna say is, well, you know, is this word man, and this man has, uh, noun modifier, [NOISE] Okay. That's what we want to say. So that's what the second meaning of that sentence can be. The word man is a noun, and the word knife is a verb, so that's how the sentence works. um, which is sort of saying something that the man possesses, and then this dependency is the same, and it's a man with a knife. Okay. And so, the interpretations of these sentences that you can get depend on putting different structures over the sentences in terms of who is- and who has the knife. That's what's going on here. And that's what we're going to try to get at in the next few minutes. We'll see if we can do it. what is modifying what? Um, here is another one that's just like that one. Um, scientists count whales from space. Okay. So again, this sentence has two possible structures, right? [LAUGHTER] That we have, the scientists are the subject that are counting and the whales are the object. [LAUGHLY] Okay, so that's the first one. The second one is the second one, which is the third one. That's the fourth one. and, well, one possibility is that this is how they're doing the counting, um, so that they're counting the whales from space using something like a satellite. Um, but the other possible is that these parts are the same, this is the subject, and this are the object, but these are the whales. And that's the way they count them. And so that's how they count the whales, I guess, from space. I don't know how they do it, but that's what they do. are whales from space which, you know, we could have analyzed as a noun phrase goes to, um, and now, on a PP. But its dependency grammar we saying, "Oh, this is now a modifier of the whales, and that they are whale from space," he says. "It's a constituency grammar, but it's also a dependency grammar," he adds. "And it's a way of saying that whales are from space, that they're from space. And that's what we're trying to say." um, that are starting to turn up as in the bottom example." Right? So, obviously what you want is this one is correct and thisOne is here wrong. Um, and so this choice is referred to as a prepositional phrase attachment ambiguity, and it's one of the most common. It's called "prepositional phrases attachment ambiguity" It's a phrase that can be used to refer to a person, a place, a thing, or an object. It can also be used as a way to describe a person or a place. ambiguities in the parsing of English, right? So, here's our prepositional phrase from space. And so in general, when you have prepositions, and before it you have verbs, and noun phrases, or nouns, that the prepositionally phrase can modify either of the things that come beforehand,right? And so, in the case of space, it's the first word in the phrase. So that's what we're going to use here. This is a crucial way in which human languages are different from programming languages. In programming languages, we have hard rules as to how you meant to interpret things that dangle afterwards, right? So, in Programming languages, you have an else is always construed with the closest if. Well, in human languages, it's not always possible to do that. So we have to be able to do something else, and then interpret it in a different way. That's a very different way of looking at it. if that's not what you want, um, you have to use parentheses or indentation or something like that. I guess, it's different in Python because you must use indentation. But if we think of something like C or a similar language, right? Um, if you haven't used braces, you don't know what you're doing. If you're writing in Python, you're probably writing in a different language. You're probably using a different type of code than you're used to. to indicate, it's just deterministically, the else goes with the closest if. Um, but that's not how human languages are. Human languages are, um, this prepositional phrase can go with anything proceeding, and the hearer is assumed to be smart enough to work out the right one. And, you know, this is a good way to say, "I don't know what to do with this," and so I'm going to do it for you. that's actually a pa- large part of why human communication is so efficient, right? Like, um, we can do such a good job at communicating with each other because most of the time we don't have to say very much, and there's this really smart person on the other end, he says. That's actually part of the reason we're so good at communicating. He says we're able to do it because we know there's someone who's really smart on the end of the line. um, who can interpret the words that we say in the right way. Um, so, that's where if you want to have artificial intelligence and smart computers, we then start to need to build language understanding devices who can also, um, work on that basis. That they can just decide to do what they want to do. That's where we need to start to build those devices. We need to be able to talk to them in a way that they can understand us and work with us. what would be the right thing for form space to modify. And if we have that working really well, we can then apply it back to programming languages, and you could just not put in any braces in your programming languages. And the compiler would work out what you meant, and it would be able to work out how to use the form space in a way that makes sense to you. And that would be a really big step forward for the future of programming languages in the future, I think. This is a real example of a sentence from The Wall Street Journal. It's sort of seems maybe not that hard there, but you know, it, it gets worse, I mean, this isn't as fun an example, but it's a real examples. The board approved this. It is a prepositional phrase attachment. It was used to attach a word to a word. The word is called a word attachment. This is an example of such an attachment. And it was used for a word called "prepositional phrases" acquisition by Royal Trustco Limited of Toronto for $0.27, $27 a share at its monthly meeting. Boring sentence, but, um, what is the structure of this sentence? Well, you know, we've got a verb here, and we'veGot exactly the same subject, and for this noun,Um, object coming after. And so on and so on, until you get to the end of the sentence, when you say, "I've got it, I've got the answer," and then, "What do I do?" it. Here, we've just got a see four prepositional phrases in a row. And so, well, what we wanna do is say for each of these prepositionally phrases what they modify, and so on. But then what happens after that? Well, here, we're going to go through a series of different phrases and see how they modify each other. And then we're gonna go on to a different phrase and see what that phrase does to the next one in the series. starting off there only two choices, the verb and the noun proceeding as before. Um, so once we start getting further in there'll be more possibilities. But it's gonna get more complicated as we go in, because look, there's another noun here, and another nounHere, and a noun here. And there's also a verb here, so there's a verb there. And a noun there, so that's the verb. And the noun here is the noun. And that's a noun. So that's what we're gonna do. Okay. So, um, by Royal Trustco Limited, what's that modifying? [NOISE] Right. You see acquisition, so it's not the board approved by Royaltrustco Limited. It's an acquisition by Royal trustco. Okay. So,. this one is a dependent of Royal trust co. It is not a board approved acquisition, it's a Royal trust acquisition. Okay, let's see if we can, uh, work it out. We're going to try and work out what that means. the acquisition. Um, now, we went to of Toronto, and we have three choices, that could be this, this, or this. Okay. So, of Toronto is modifying. [NOISE] Its acquisition of Toronto? [LAUGHTER] No, I think that's a wrong answer. Um. Is there another guess for what? No, that's the wrong answer, too. I'm sorry, I don't know what it is, but I think it's something like that. Royal Trustco is a noun phrase, so it can also have modifiers by prepositional phrase. So, this of Toronto is a dependent of Royal Trustco Limited, right, that's this again, sort of this noun phrase. Okay. Royal Trust co.of Toronto is modifying? Royal Trust Co. Limited of Toronto. That's Royal TrustCo Limited. of Toronto, right? So, it's Royal trustco Limited ofToronto, right. It's a dependent. of Royaltrustco Limited. Of Toronto. For $27 a share is modifying acquisition, right? [NOISE] So now, we leap right back. Now, is now the acquisition that's being modified? And then finally, we have at its monthly meeting is modifying? [Noise] Approved. Well, the approved, Well,  Well, the approval,   “Moody’s” is the name of the company that is being “modified.”    “ Moody's’ name is “S&P 500”. right? It's approved, yeah. It's approval that its monthly meeting. Okay. [NOISE] I drew that one the wrong way around with the arrow. Sorry, it should have been done this way. I'm getting my arrows wrong. Um, um. So that we've got this. Right? It'm approved,Yeah. It't approved that its Monthly Meeting. Right. Okay, that's what we're going to do. That's what's going to happen. pattern of how things are modifying. Um, [NOISE] and so actually, you know, once you start having a lot of things that have choices like this, you stop having- if I wanna put an analysis ac- on to this sentence I've to work out the, the right structure, I have to have. And so that's what I'm trying to do here. And that's why I'm going to try to make it a little bit more complex, I think. to potentially consider an exponential number of possible structures because, I've got this situation where for the first prepositional phrase, there were two places that could have modified. For the second prepositions, there are three places that can be modified, and for the fourth one, there's five places that are possible. I think that's a good way to look at it. I don't think it's a bad way to think about it. It's just a different way of looking at it, though. could have modified. That just sounds like a factorial. It's not quite as bad as the factorial, because normally, once you've let back that kind of closes off the ones in the middle. And so, further prepositional phrases have to be at least as far back in terms of what they could have modified, he says. He says it's a good idea to start with the beginning of a sentence and work your way up. The end of the sentence should be followed by the end. they modify. And so, if you get into this sort of combinatorics stuff, the number of analyses you get when you get multiple prepositional phrases is the sequence called the Catalan numbers. Ah, but that's still an exponential series. And it's sort of one that turns up in a lot of research. It's one of the things that we find out about when we look at the language. We find out a lot about the language when we study it. We also find out that it can be used to predict the future. of places when they're tree-like contexts. So, if any of you are doing or have done CS228, where you see, um, triangular- triangulation of, ah, probabilistic graphical models and you ask how many triangulations there are, that's sort of like making a tree over your variables. And that's, again, gives you a sense of what a tree looks like in a particular context. So that's what we're trying to do. you the number of them as the Catalan series. But- so the point is, we ha- end up with a lot of ambiguities. Okay. So, that's prepositional phrase attachments. A lot of those going on. They are far from the only kind of ambiguity. So,. I wanted to tell you the number. of them that we have in the series. You can read more here: http://www.cnn.com/2013/01/29/science/science-fiction/science_fiction/cnn-science-film-science.html#storylink=cpy. you about a few others. Uh, okay, shuttle veteran and longtime NASA executive Fred Gregory appointed to board. Um, why is this sentence ambiguous? What are the different reading of this statement? [NOISE]. Yes? Uh, it's a better [inaudible] Okay. So, um, right answer. That is either that there's somebody who's a shuttle veteran. and a long time NASA executive, and their name is Fred Gregory, and that they've been appointed to the board. and whoops, and longtime NASA executive. Or we can say, well, we're doing appointment of a veteran and the longtime NASA exec, Fred Gregory. And so, we can represent by dependencies, um, these two different structures. Okay. Um, that's,Um, one. That one is not very funny again. So- So, we have to go back to the drawing board. We have to come up with another way to say it. So, that one is, uh, one, too. so, here's a funnier example that illustrates the same ambiguity effectively. Um, so, there isn't actually an explicit, um, coordination word here. But effectively in, um,. a natural language or certainly English, you can use kind of the word 'coordination' [LAUGHTER] The doctor: No heart, cognitive issues. The patient: Yes, I'm fine. The physician: No, I don't think there's anything wrong with me, but you'll have to wait and see. just comma of sort of list intonation to effectively act as if it was an "And" or an "Or", right? So, here, um, we have again two possibilities that either we have issues and the dep- and the dependencies of- the dependency of issues is that there are no issues, right? Or we have no issues. Or we don't have issues at all. Or, we just don't know what the issue is. We have no idea what the problem is. So, that's actually a determiner, ah, no issues. So, heart is another dependent. It's sort of a non-compound heart issues. And so, we refer to that as an independency, and then it's heart or, um, cognitive. Um, so that heart or cognitive is a conjoined phrase inside of this "No heart" or "Cognitive issues". But there's another possibility,. um, which is, um,. that the coordination is at the top level that we have "No. heart" and "C cognitive issues". And,Um, at that point, we ha- have the "C Cognitive" as an adjective modifier of the "Issues" and the "No heart", the determiner is just a modifier of "Heart", and then these being conjoined together. So, um, "Heart" has a depend- has a coordinated dependency of "Issue". Okay. That's one one. Um, I've got more funny ones. Susan gets- [NOISE]Susan gets- Susan gets - Susan gets to be a little bit of a troublemaker. She's a bit of an troublemaker, but she's a good one. "First" is an adjectival modifier of "First hand" and it's firsthand experience. The person who wrote this intended to have is that there- we- Here we've got an adjective modifier ambiguity. So, so, the " first hand" is a first-hand experience, not a firsthand experience, and it is not an experience of first hand, it's a first hand experience of firsthand experience of someone who has had that experience. That's the way it was intended to read. modifier of "Experience" and the "Job" is also a modifier of " experience" And then we have the same kind of subject, object, um, reading on that one. But unfortunately, this sentence has a different reading, where you change the modification relationships. Um, and you have it's "experience" and "job" as the subject and "object" in the same sentence. And then you have to change the relationship between the two to make it work. the first experience and it goes like this. Um, "Mutilated body washes up on Rio beach to be used for Olympics beach volleyball." Um, wha- what are- [LAUGHTER] what are the two ambigui- What are the the two readings that you can get for that? [NOISE] One more example. Okay. [LAUGHLY] Okay, one more example, and then we'll go to the next one. The next one is the first experience. this one? [NOISE] We've got this big phrase that I want to try and put a structure of to be used for Olympic beach volleyball, um, and then, you know, this is sort of like a prepositional phrase attachment ambiguity but this time instead of it's a prePOSitional phrase that's attachment ambiguity, this time it is attachment ambiguity. This time it's attachable ambiguity. this one? This one? this one is attachmentambiguity. This one is attachmentsambig ambiguity. being attached, we've now got this big verb phrase we call it, right, so that when you've sort of got most of a sentence but without any subject to it. That's sort of a verb phrase to be used for Olympic beach volleyball which might be then infinitive form. Sometimes it's used to refer to a person or a group of people who are in the middle of a conversation. It can be used to describe a group or a person who is talking to each other. it's in part of CPO form like being used for beach volleyball. And really, those kind of verb phrases they sort of just like, um, prepositional phrases. Whenever they appear towards the right end of sentences, they can modify various things like verbs or nouns. Um, so, here, uh, we are. We are in the United States of America. And we are in Washington, D.C. and we are at the White House. We would like to thank you for your time. have two possibilities. So, this to be used for Olympics beach volleyball. Um, what the right answer is meant to be is that that is a dependent of the Rio beach. So,. it's a, um, modifier of Rio Beach. But, but the funny reading is that instead of. instead of beach volleyball, it could be called "beach volleyball" or "beachy volleyball" Or it could even be "beaches volleyball" instead of "beather volleyball" for the first time. that, um, we can have here is another noun phrase muti- mutilated body, and it's the mutilatedBody that's going to be used. Um, and so then this would be, uh, a noun phrase modifier [NOISE] of that. Okay. So knowing the right structure of sentences is important, and knowing how to structure a sentence is important as well. So that's what we're going to do here. Okay, so that's the first part of the sentence. to understand the interpretations you're meant to get and the interpretations that you're not meant to. Okay. But it's, it's sort of, um, okay, you know, I was using funny examples for the obvious reason, but, youknow, this is sort of essential to all the things that we'd like to do. Okay? So, yeah, that's what I was going to say. That's why I used funny examples, but this is essential to the whole thing. to get out of language most of the time. So, you know, this is back to the kind of boring stuff that we often work with of reading through biomedical research articles and trying to extract facts about protein-protein interactions from them or something like that. "So, youknow, this was a good way to start the day," he says. "It was a great way to kick off the day. It was great to start it off with a little bit of a laugh." is, um, the results demonstrated that KaiC interacts rhythmically with SasA Ka- KaiA and KaiB. Um, and well, [NOISE] I turned the notification's off. um, so, if we wanna get out sort of protein-protein interaction,Um, facts, you know, well, we have this KaiC that's interacting with these other KaiBs and SasAs and KaiAs. [NOise] I turn the notifications off. The way we can do that is looking at patterns in our dependency analysis, and so that we can sort of, um, see this repeated pattern where you have a noun subject here interacts with a noun modifier, and then it's going to be.proteins over there. And well, the way we do that in this case is by looking at the relationship between the noun subject and the noun modifier in the dependency analysis. And so we can see a repeated pattern of this. these things that are beneath that of the SasA and its conjoin things KaiA and KaiB are the things that interacts with. So, we can kind of think of these two things as essentially, um, patterns. [NOISE] I actually mis-edited this. Sorry. This should also be nmod:with. Um, we're talking about the world of KaiB and KaiA, which are essentially the same thing, but in different ways. We're not talking about KaiA or KaiB in the same way. can kind of think of these two things as sort of patterns and dependencies that we could look for to find examples of, um, just protein-protein interactions that appear in biomedical text. Okay. Um, so that's the general idea of what we wanna do, and so the total we want is that we want to look for patterns anddependencies that we can look for in biomedical texts. That's what we're trying to do. We want to find patterns and Dependencies. We're looking for examples of protein- protein interactions. to do it with is these Dependency Grammars. I just want us to sort of motivate Dependencies Grammar a bit more, um, formally and fully, right? So, Dependency Grammar postulates the what is syntactic structure is is that you want to use it in a certain way. And so, I've sort of shown you some Dependency grammars, and I want to show you how to use them in your own work, too. have, um, relations between lexical items that are sort of binary asymmetric relations which we draw as arrows, because they are binary and asymmetric, and we call dependencies. And there's sort of two ways, common ways, of writing them, and I've sort of shown both now. One way is you say you have relations between Lexical Items and Lexical Objects. The other way is to say that Lexical items and lexical Objects have relations that are Binary and Asymmetric. sort of put the words in a line and that makes it. He see, let's see the whole sentence. You draw this sort of loopy arrows above them and the other way is you sort of more represent it as a tree, where you put the head of the whole. It's a very different way to represent it, but it's the same idea. It makes it more like a tree. It has a lot of loops and arrows and it's a bit like a sentence. sentence at the top, submitted and then you say the dependence of submitted, uh, bills were in Brownback. Um, so, it was bills on ports and immigration. So,  the dependence of bills and were submitted words, the dependent of each of those. And then, um, then, you say, "The dependence of each" of those words, and then, "the dependence" of each one of those, and so on. And so, that's what you do. submitted and you're giving this kind of tree structure. Okay. Um, so, in addition to the arrows commonly what we do is we put a type on each arrow which says what grammatical relations holding them between them. So, is this the subject of the sentence? Is it the object. Is it a subject or an object? What is the relationship between the two? What are the grammatical relationships between them? And what are the relations between the subjects and the objects? And so on. of the verb? Is that a, um, a conjunct and things like that? We have a system of dependency labels. Um, so, for the assignment, what we're gonna do is use universal dependencies, which I'll show you more, a little bit more in a minute. And if you think, "Man, I can't figure out how to do this," well, you're not alone. You can do it. You just have to know how to use it. this stuff is fascinating. I wanna learn all about these linguist structures. Um, there's a universal dependency site, um, that you go and can go off and look at it. But, if you don't think that's fascinating, um,. for what we're doing for this class, we're going to have to talk about it a little bit later in the class. We'll have to go back to the beginning of the class and talk about how we got to this point. we're never gonna make use of these labels. All we're doing is making use of the arrows. And for the arrows, you should be able to interpret things like prepositional phrases as to what they're modifying. Just in terms of where the prepositionally phrases are connected and whether that's right. That's what we're trying to do here. We're not trying to make any sense of the labels. We just want to make some sense of what the arrows are doing. or wrong. Yes. So formally, when we have this kind of Dependency Grammar, we've sort of drawing these arrows and we sort of refer to the thing at this end as the head of a dependency. And the things at the other end are the dependent of the dependency. Okay. And that's what we're going to do in this article. We'll be using that language in the next article, which is going to be called the "Dependency-Grammar" edition. as in these examples are normal expectation and what our policies are gonna do is the dependencies form a tree. So, it's a connected acyclic single, um, rooted graph at the end of the day. Okay. Dependency Grammar has an enormously long history. The famous first linguists, basically, the famous first language, were called dependency grammars. They were used by the French, Germans, English, and other European languages. They used them to write the French language. that human beings know about his Panini who, um, wrote in the fifth century before the Common Era and tried to describe the structure of Sanskrit. And a lot of what Panini did was working out things about all of the morphology of Sanskrit that I'm not gonna touch at right now, but he did work out some interesting things about the structure and morphology of the language. That's what I'm going to focus on today. I'll be back in a few minutes with the next episode. the moment. But beyond that, he started trying to describe the structure of Sanskrit sentences. And, um, the notation was sort of different but, essentially, the mechanism he used for describing theructure of Sanskrit was dependencies of sort of working out these,Um, what are arguments in modifies of modifies in Sanskrit? And so that's what he used to do. He used that mechanism to do his analysis of the language. And that's how he came up with the idea of the word 'soul' what relationships like we've been looking at. And indeed, if you look at kind of the history of humankind, um, most of attempts to understand the structure of human languages are essentially Dependency Grammars. Um, so, sort of in the later parts of the first millennium, there was a ton of interest in Dependency grammars, and that's what we're looking at in this study. So, we're going to be looking at the relationships between Dependency and Dependency in a number of ways. of work by Arabic grammarians and essentially what they used is also kind of basically a Dependency Grammar. Um, so compared to that, you know, the idea of context-free grammars and phrase structure Grammars is incredibly incredibly new. I mean, you can basically, um, totally date it. There was this huge amount of work done on Arabic grammar in the 19th century and early 20th century, and it's still going on today, I think. guy Wells in 1947 who first proposed this idea of having these constituents and phrase structure grammars, and where it then became really famous is through the work of Chomsky, um, which love him or hate him is by far the most famous, um,. linguist and also variously contributed to many other areas of linguistics. He was the first person to suggest the idea of a phrase structure grammar, and it was his work that led to the creation of the Chomskyan system of grammar. He died in 1994, but his work continues to be influential. The Chomsky hierarchy was not invented to torture beginning computer science students. It was invented because Chomsky wanted to make arguments as to what the complexity of human languages was. The hierarchy was invented by Chomsky to show how complex human languages were. It is now the subject of a new book, The Hierarchy of Linguistic Complexity: A Theory of Complexity in Human Language. The book is published by Oxford University Press and is available in hardback and paperback. For confidential support call the Samaritans on 08457 90 90 90, visit a local Samaritans branch or click here for details. um. So, in modern work, uh, there's this guy Lucie Tesniere. Um, and he sort of formalized the kind of version of dependency grammar that I've been showing you. And you know it's- it's long-term being influential and you know, it's a long way from one person to the next, but it's important to keep in mind that it's not just one person's work that's influential. It's a whole group of people's work. computational linguistics. Some of the earliest parsing work in US Computational Linguistics was dependency grammars. But I won't go on about that um more now. Okay. Um, just one, two little things um, to note. I mean, if you somehow start looking at other papers where their dependency Grammars, people are using them. People are using dependency gramMars in a way that's very different from what we're used to in the U.S. aren't consistent on which way to have the arrows point. There's sort of two ways of thinking about this um, that you can either think okay, I'm gonna start at the head and point to the dependent. Or you can say I'm going to start. at the dependent and say "I" am "the dependent" or "I'm the head" Or "I am the head of the dependent" is the "head" of the "dependent" of "the head" what its head is, and you find both of them. The way we're gonna do it in this class is to do it the way Tesniere did it, which was she started the head and pointed to the dependent. Uh, sorry. I'm drawing that wrong. Whoops, um because discussion is what we're doing here. We're going to do this in a different way, so that we can get to know each other a little bit better. That's the way it's going to be. of the outstanding issues. So, really um, the dependent is sort of discussion. Um, okay. We go from heads to dependence. And usually, it's convenient to serve in addition to the sentence to sort of have a fake root node that points to the head of the whole sentence. So that's what we're going to do with this one. We're just going to use it as an example of what we can do with a sentence that's already been written. And we'll go from there. we use that as well. Um, so to build a dependency pauses or to indeed build any kind of human language structure finders including kind of constituency grammar pauses, the central tool in recent work, where recent work kind of means the last 25 years has been this idea. We use that to build dependency pauses and to find human language structures. We also use it to find language structures that are related to each other. We've been doing this for 25 years, and it's been a big part of our work. of tree banks. Um, and the idea of tree banks is to say we are going to get human beings to sit around and [NOISE] put grammatical structures over sentences. So, here are some examples I'm showing you from Universal Dependencies. Here are some um, English sentences. I think I think that's a good way to go. I don't think it's a bad way to do it. It's just a way to get people to think about things. Miramar was a famous goat trainer or something. And some human being has sat and put a dependency structure over this sentence and all the rest. Um, and with the name Universal Dependencies, this is just an aside. It's actually project I've been strongly involved with. But it's just a bit of fun to play around with, I'm sure you'll get used to it. I'll be back in a week or so with a new version of the app. I'm looking forward to seeing what you've come up with. precisely what the goal of universal dependencies was is to say what we'd like to do is have a uniform parallel system of dependency description which could be used for any human language. So, if you go to the Universal Dependencies website, it's not only about English. You can find Universal Dependency analyses of you know, French, or German, or Finish, or Carsac, or Indonesian, lots of languages. Of course, there are even more languages which there aren'tUniversal Dependencies analyses of. Dependencies um, treebank, um, you can get in touch. Um, but anyway. So, this is the idea of treebank. You know, historically, tree banks wasn't something that people thought of immediately. This so- an idea that took quite a long time to develop, right? That um, people started thinking about in the '70s and '80s. That's when tree banks began to take off. And they're still going strong today. grammars of languages even in modern times in the fifties, and people started building parses for languages in the 19, early 1960s. So, there was decades of work in the 60s, 70s, 80s, and no one had tree banks. The way people did this work is that they wrote grammars, and they wrote them in a way that made sense to them. And that's the way they did it in the '60s, '70s and '80s. that they either wrote grammars like the one I did for constituency of noun phrase goes to determiner, optional adjective noun. Noun goes to goat um, or the equivalent kind of grammar and a dependency format. They hand built these grammARS and then train, had parsers that could parse them. That's what I did with this one. It's a little bit like that, but it's a lot more complex. I think that's the way to do it. these sentences. Having a human being write a grammar feels more efficient. Because if you write uh, a rule like noun phrase goes to determiner optional adjective noun. I mean, that- that describes a huge number of phrases or actually infinite number of phrase. Um, so that's what we're going to do here. We're just going to go with the flow of the language. That's what's going to happen. And we'll see how it goes from there. We'll see what happens. you know, this is the structure of you know, the cat, the dog, or cat or dog. All those things we saw at the beginning. So, it's really efficient you're capturing lots of stuff with one rule. Um, but it sort of turned out that in practice, in practice it's not very efficient. It's a lot of work to get all of these things to look like the way they do. But it's a really efficient way to get them. that wasn't such a good idea, and it turned out to be much better to have these kind of treebank supporting structures over sentences. It's often a bit more subtle was to why that is because it sounds like pretty menial work um, building tree banks, and in some sense it is. Once you have a treebank, it's reusable for all sorts of purposes that lots of people build parsers format. But also other people use it as well like linguists now often used tree banks to find examples of different constructions. we want to do machine learning, we want to have data that we can build models on. In particular, a lot of what our machine learning models exploit is how common are different structures. So, we wants to know about the commoners and the frequency of things. Um, but then, but than, we then want to look at the patterns in the data that are common to the different structures that we are trying to learn about. We want to find out how common these common structures are, and how common they are. treebanks gave us another big thing which is, well, lots of sentences are ambiguous, and what we want to do is build models that find the right structure for sentences. If all you do is have a grammar you have no way of telling what is the right Structure for a sentence. We want to build a model that finds the right structures for sentences that are ambiguous. We call this model 'treebanks' and it is based on the idea that ambiguous sentences are more likely to be ambiguous. ambiguous sentences. All you can do is say hey that sentence with four prepositional phrases after it that I showed you earlier, it has 14 different parsers. Let me show you all of them. Um, but once you have um, treebank examples, you can say this is the right structure. Once you have Um, tree bank examples, You can say This is the Right Structure. You can use this to help you with your sentence structure. If you have any questions, please contact us at [email protected] and we'll try to get back to you. for this sentence in context. So, you should be building a machine learning model which will recover that structure, and if you don't that you're wrong. [NOISE]. Okay. Um, so that's treebanks. Well, how are we gonna do build dependency parsers? Well, somehow we want models that can kind of recover that. For example, we want to build a model that can sort of recover the structure of a tree. If you want to do that, you need a tree bank. of capture what's the right parse. Just thinking about abstractly, you know, there's sort of different things that we can pay attention to. That's a reasonable thing. So, it's reasonable to think that we should be paying attention to the actual words, right? Discussion of issues. That’s a reasonablething to do, right?" "That's areasonable thing," he said. "That’d be a good thing to do. That would be a great thing." have issues as dependent of discussion um, where you know, discussion of outstanding. That sounds weird. So, you probably don't want that dependency. Um, there's a question of how far apart words are. Most dependencies are fairly short distance. They not all of them are. There's a questions of what's a dependency, and what's not. And how much is too much, and how much isn't too much? And what's too much is not too much. And so on. in between. Um, if there's a semicolon in between, there probably is an a dependency across that. So, and the other issue is sort of how many arguments do things take? So, here we have was completed. If you see the words was completed, you sort of expect that there'll be a dependency between that and the rest of the story. If there's not a dependency, then there's probably not a story at all. And if there is, then the story is probably incomplete. be a subject before of the something was completed, and it would be wrong if there wasn't. So, you're expecting an argument on that side. But on the other side, hand it won't have object after it. You won't say the discussion was completed the goat. Um, that's not a subject, it's just a goat. That's what happens when you have a goat in the room. It's not the end of the world, it just happens to be the beginning of the end. good sentence, right? So, you won't have ah, um, an object after it. So, there's sort of information of that sort, and we want to have our dependency parsers be able to make use of that structure. [NOISE] Okay. Um, so effectively what we do when we build a dependency is that we create a dependency structure. And then we use that dependency structure to build a new dependency. So that the dependency structure can be used to build new dependency structures. parser is going to say, for each word is- is going- to be the dependent of some other word or the root. So, this give here is actually the head of the sentence. It's a dependent of root, the talk is adependent of give, 'll is a dependent, and so on. The end of a sentence is the end of the word that is dependent on the beginning of that word. This is the final word in the sentence, and it is the head. For each word we want to choose what is the dependent of. We want to do it in such a way that the dependencies form a tree. So that means it would be a bad idea if we made a cycle. So, if we sort of said, Bootstrapping, was a dependent of, um, talk, but then we had things sort of move around, that's bad news. We don't want cycles, we want a tree, and there's one. And so,this goes to here, butthen talk is a dependent that, and so I'm gonna cycle that's good news. Final issue is whether we want to allow dependencies to cross or not. So, most of the time, um, dependencies don't cross each other. Uh, but sometimes they do, and this example here is an example of this. Final issue is we don't want things that, um,. is whether you want to let them cross. um, or if you do want them to cross, then you need to allow them to do it. um. That's the last issue, and that's what we're trying to do. is actually an instance for that. So, I'll give a talk tomorrow, um, on bootstrapping. We actually have another dependency here that crosses, um,. that crosses into this talk. We're giving a talk that's the object, and when it's being given is tomorrow, but this talk has a modifier that's on bootstripping. So that's what we're doing here. That's what's going to happen in the talk, and that's why we're giving it tomorrow. dependency. And that's sort of rare, that doesn't happen a ton in English, but it happens sometimes in some structures like that. And so, this is the question of whether, um, what we say is that the positive sentence is projective if there no crossing dependencies and it's non-projective ifthere are crossing dependencies. And when it's not is when you kind of have these constituents that are delayed to the end of the sentence, right? You could've said, I'll give a talk on bootstrapping. tomorrow, and then a [inaudible] have a projective parse, but if you want to, you can kind of delay that extra modifier and say I'll give a talk tomorrow on bootstrapping and then the parse becomes non-projective. So, that's that. Um, there are various ways of, um, doing this, and we'll talk about some of them in the next few minutes. Um,. okay, that was that. We'll talk to you in a little bit. dependency parsing, but basically what I am gonna tell you about today is this one called transition-based or deterministic dependency parsing. This is, um, the one that's just been enormously influential in practical deployments of parsing. So, when Google goes off and parses every web page, what they're using is what they've been using for a long, long time. It's a very, long way from what you see on the web today, but it's a key part of what we're doing. is a transition based parser. It was popularized by Joakim Nivre, who is a Swedish computational linguists. It's sort of inspired by shift-reduce parsing. So, probably in- in our case, it's probably in the middle of the way through the parsing process. But it's not the end of the world, because it's a tool that can be used to do other things as well. It can be applied to a lot of different situations. CS103 or compilers class or something, you saw a little bit of shift-reduce parsing. And this is sort of like a shift- reduce parser, apart from when we reduce, we build dependencies instead of constituent. Um, and this has a lot of very technical description that doesn't help you at all. It's a very, very long way of saying that this is not a very good way of working with the language. It doesn't really help you much at all, but it's a starting point. to look at in terms of understanding what, um, a shift-reduce parser does. And here's a formal description of a transition-based shift- Reduce Parser, which also doesn't help you at all. Um, so, instead we kinda look at this example, uh, [LAUGHTER] because that will hopefully help you. So, what do you think? Let us know in the comments. Back to Mail Online home. back to the page you came from. I wanna to do is parse the sentence "I ate fish". And yet formally what I have is I have a why I start, there are three actions I can take and I have the finished condition for formal parse, parse. Um, and so here's what I do. So, I start by saying, "I want to do a formal parse of this sentence" and then I go through and do it. And then I say, "Parsing this sentence is a formal way of saying that I ate fish" and I do that. And it works. have a stack which is on this side and I have a buffer. Um, so, the stack is what I have built, and the buffer is all the words in the sentence I haven't dealt with yet. So, I stop the parse, and that's the sort of instruction here, by default. By default, when I start a new sentence, the first word is the first thing that comes to mind. And so on, and so on. So that's what I do here. putting route, my root for my whole sentence onto my stack, and my buffer is the whole sentence, and I haven't found any dependencies yet. Okay, and so then, the actions I can take is to shift things onto the stack or to do the equivalent of a Reduce where where I put my root on the stack and then my buffer on my stack. And then, I can do a Reduce on my buffer. And I can then do a reduce on the root of my buffer to shift it onto the Stack. I build dependencies. So, starting off, um, I can't build a dependency because I only have root on the stack, so the only thing I can do is shift, so I can shift I onto the stack. Um, now, I could at this point say, let's build adependency, but I don't want to do that. I just want to make sure that I'm not doing anything that's going to break the system. I want to be able to do what I need to do. is a dependent of root, but that would be the wrong analysis, because really the head of this sentence is I ate. So, I'm a clever boy and I shift again. And now I have root I ate on the stack. Okay, and so, at this point,  I'm in a position where I can say that I ate root. I can then say that root is in a state of change. I am now in the state of a change. And so, I am in a situation where I am able to eat root. position where, hey, what I'm gonna do is reductions that build structure, because look, I have I ate here and I want to be able to say that I is the subject of dependency of ate, and I will do that by, um, by doing a reduction. And so, what so I'm in a position where I'm able to do that, and that's what I'll do. I'll reduce the amount of food that I'm eating, and so I'll be in a.position where I can do that. I'm gonna do is the left-arc reduction, which says, look, I'm gonna treat the second from top thing on the stack as a dependent of the thing that's on top of the stack. And so, I do that, and so, when I doThat, I create the secondFromFrom. When I did that, I created the second FromFrom. I created a second from from from the top of a stack. That's what I call the leftarc reduction. head thing as a subject dependent of ate, and I leave the head on the stack ate, but I sort of add this dependencies as other dependencies I've built. Okay, um, so, I do that. Um, now, I could immediately reduce again and say ate is a dependent of root, and so on. But I don't want to do that, I want to be able to say that ate is dependent on root. So I reduce it to root, ate, root, root. but my sentence's actually I ate fish. So, what I want to do is say, "Oh, if it's still fish on the buffer," so what I should first do is shift again, have root ate fish in my sentence, and then I'll be able to say, Look, I Want to eat fish. "I want to. eat fish," he says. "That's what I'm going to do. I'm not going to say "I ate fish." I'm just going to eat root. That's all I'm doing." now build, um, the thing on the top of this stack as a right dependent of the thing that's second from top of the stack, and so that's referred to as a Right-Arc move. And so, I say Right Arc, andSo, I do a reduction where I've generated a Right Arc. and so, i.e. I say 'Right Arc' and then I generate a 'Left Arc' which is a 'Right-Arc' move. So that's what I mean when I say, 'RightArc' and 'Left-Arc.' New dependency and I take the two things that are on top of the stack and say, um, fish is a dependent of ate, and so therefore, I just keep the head. And I generate this new Arc. And then I keep the hit on the stack. and the- and I generate a new Arc, and I always just keep. the hit and the Arc. and then I generate another Arc, which is the new dependency. And so I keep that Arc and I keep this Arc. The Arc is the Arc that I generate. so, at this point, I'm in the same position I want to say that this ate is a right dependent of my route. So, again I'm again going to do Right Arc, um, and make this extra dependency here. Okay. So then my finished condition of having successfully parsed the route is that I have successfully used Right Arc. So I'm going to use Right Arc to do that. Okay, so now I have a route that I can use to get to my destination. sentence is my buffer is empty and I just have root left on my stack because that's what I sort of said back here. So that worked well but, you know, I actually had different words for it. Okay. So, I've parsed the sentence. Now, I'm going to go ahead and try to do the same thing with the second sentence. I'm just going to try and do it with a different word for it and see what happens. Okay, so I've done that. Now I'm off to do my third sentence. choices of when to pa- when to shift and when to reduce. And I just miraculously made the right choice at each point. And well, one thing you could do is say, well, you could have explored every choice and, um, seen what happened and gone different, he says. He says he wishes he had explored all the options and gone through the whole process differently. But he says he's glad he made the choices he did and is happy with what he's achieved. parsers. And I could have, but if that's what I'd done, I would've explored this exponential size tree of different possible parsers. If that was what I was doing, I wouldn't be able to parse efficiently. And indeed that's not what people did in the 60s, 70s and 80s. Uh, rather than doing a crummy search here, we can come up with clever dynamic programming algorithms and you can relatively efficiently explore the space of all possible Parsers. But that was sort of the mainstay of parsing in those decades. when Joakim Nivre came along, he said "Yeah, that's true, um, but hey, I've got a clever idea, uh, because now it's the 2000s and I know machine learning." Um, so, what I could do instead, is say I'm at a particular position in the parse and I'm gonna build a machine learning system. That's what we're doing. We're building a machine-learning system that can predict what a person is going to do. a machine learning classifier. It's gonna tell me whether to shift, um, with left arc or right arc. So, if we're only just so talking about, well, how to build the arrows, they're just three actions, and they're all the same. They're just different ways of doing it. So that's what we're going to do. We're just going to use the same three actions. And that's it. That's the way it's going to be. shift, left arc or right arc. Um, if we also wanted to put labels on the dependencies, and we have our different labels, um, there are then sort of 2R plus actions because she is sort of left arc subject or left arc object or something like that. But anyway, anyway, but anyway, that's the way to go. That's how we do it. And then there are other ways to do it, too, if you want to do something different. Joakim Nivre shows you can predict the correct action to take with high accuracy. He used machine learning to build a classifier which will predict the right action. "In the simplest of terms, it's like a set of actions and so you gonna build aclassifier with machine learning somehow," he says. "And so, um, in the simplest way possible, you're gonna build the right classifier withmachine learning somehow which will Predict the right Action" "It's the sort of slightly surprising fact that actually you could actually do that," he adds. version of this, um, there's absolutely no search. You just run a classifier at each step and it says "What you should do next is shift" and you shift, and then it says 'What youshould do is left arc' and you left arc, and you run that through and you get the result. It's a very simple system, and it's very easy to use, and there's no need to know how to do it. It just gives you an idea of what to do, and that's it. he proved, no, he showed empirically, that even doing that, you could parse sentences with high accuracy. Now if you wanna do some searching around, you can do a bit better, but it's not necessary. Um, and we're not gonna do it for our, um, assignment. But so if you're interested in learning more about this, go to CNN.com/soulmatestories and search for "SoulMatestories" on the site. doing this just sort of run classify, predict action. We then get this wonderful result. Which you're meant to explain a bit honest on your assignment 3, is that what we've built is a linear time parser. Right? That because we are gonna be sort of sort of- and we then get the result we were looking for. And that's what we're trying to do here. That's why we're doing this in the first place. And we're going to do it in the second. as we chug through a sentence, where we're only doing a linear amount of work for each word. That was sort of an enormous breakthrough. Because although people in the 60s hadn't come up with these dynamic programming algorithms, dynamic programminggorithms for sentences were always cubic or worse. It was a huge breakthrough for computer science in the '60s and '70s. It's still a big part of the field today, and it's been around for a long time. And that's not very good if you want to parse the whole web, whereas if you have something that's linear time, that's really getting you places. Okay. So this is the conventional way in which this was done. Was, you know, we have a stack, we might have already built. And that's the way we did it in this case. And it's the same way that we do it in the rest of the world. It's just a different way of doing it. some structure if we hadn't working out something's dependent of something. We have a buffer of words that we don't deal with and we want to predict the next action. So the conventional way to do this is to say well, we Want to have features. And well, the kind of features we want are dependent on each other. We want to be able to predict what's going to happen in the future. We don't want to have to wait and see what the next step will be. of features you wanted was so the usually some kind of conjunction or multiple things so that if the top word of the stack is good, um, and something else is true, right, that the second top word is also good. And it's part of speech is verb, so the second word has to be a verb, and it's usually a conjunction or a multiple thing so that it's a good combination of things. So that's the way to do it. It's a little bit like a stack of words. then maybe that's an indicator of do some action. So ha- had these very complex binary indicator features and you'd build- you literally have millions of these binary indicators. And you'd feed them into some big logistic regression or support vector machine or something like that and you would see what happened. And that's what we did. We used that to see what would happen. And it was very successful. It was very, very successful for us. We were able to get a lot of work done. build parses. And these parses worked pretty well. But you sort of had these sort of very complex hand engineered binary features. Um, so in the last bit of lecture I want to show you what people have done in the, um, neural dependency parsing world. But before I do that, let's look at some of the work that's been done in this area of computer science in the past. And it's pretty exciting. It's a lot of work, but it's very exciting. do that, let me just explain how you, um, how you evaluate dependency parses. And that's actually very simple, right? So, what you do is well, you assume because the human wrote it down, that there is a correct dependency parse for a sentence. She saw the video lecture. She thought it was very interesting, so she went back and watched it again. And it was even more interesting. It was like, "Oh, my God, this is so cool!" like this. And so these are the correct arcs and to evaluate our dependency parser, we're simply gonna say, uh, which arcs are correct. So, there are the gold arcs, so there's a gold arc, um, from two to one, She saw subject, and there's an arc from zero to two, the root of the sentence. And we're just going to count up how many of them are correct, treating each arc individually. And there are a lot of gold arcs. are two ways we can do that. We can either, as we're going to do, ignore the labels and that's then, uh, referred to as the unlabeled attachment score. So here in my example, my dependency paths, I've got most of the arcs right but it got this one wrong. So I say my unlabeling attachment score is 80 percent or we can also look at the labels. And so we can just count up the number of dependencies and how many there are. we get correct. And that's in our accuracy and in the assignment, you're meant to build a dependency parser with a certain accuracy. I forget the number now is saying, some number 80 something or something that you're supposed to get to. Okay. Um, maybe I'll skip that. Okay, I'm sorry, I've got to go. I'm going to go to the bathroom. I'll be back in a few minutes. Oh, by the way, we've got a few more questions for you. so, now I wanted to sort of explain to you just a bit about neural dependency parses and why they are motivated. So I'd mentioned to you already that the conventional model, uh, had these sort of indicated features of, um, on the top of the stack is the word. So, now,  I wanted to kind of explain to you just a bit about neural dependency  parses and why they are motivated to do what they do. good and the second thing on the stack is the verb has or on the top of the Stack is some other word. And the second top is of some part of speech. And that part ofspeech has already been joined with the dependency of another part of Speech. Good is a word that has a dependency on another word that is a good word. Good means that it is good or that it has a good quality or a good sound to it. Good can also mean that the word has a positive connotation or a negative connotation. People hand-engineer these features. And the problems with that, was these features were very sparse. Each of these features matches very few things. Um, they match some configurations but not others so the features tend to be incomplete. And there are a lot of them, they're are commonly millions of people. And they're very, very difficult to work with. So that's one of the reasons why it's so hard to get them working right now. But we're working on it. We're going to get there. of features. And so it turned out that actually computing these features was just expensive so that you had some configuration on your stack and the buffer and then you wanted to know which of these features were active for that stack and buffer configuration. So you had to be very, very careful about what you were doing with the data you were trying to get from your computer. And that was a big challenge. And it was a huge challenge for a lot of people to figure out how to do it. compute features format. And it turned out that conventional dependency parsers spent most of their time computing features, then went into the machine learning model rather than doing the sort of shifting. And so that seemed like it left us with a pure parser operation. And, which you're are seeing, are just a pure Parser operation, which is what we are seeing here. We are seeing a pure parsers operation, and that's what we're seeing here, which we are calling a pure parsing operation. open the possibility that, well, what if we could get rid of all of this stuff and we could run a neural network directly on the stack and buffer configuration. And so that was a project that Dan Chi Chen and me tried to do in 2014, uh, we used to build a neural dependency parser. And, you know, effectively what we found, is that that's exactly what you could do. So, here's sort of the sort of how-to guide on how to do it. of a few stats here. So these are these same UAS and LAS. Uh, so Malt Parser was Joakim Nivre's Parser that I sort of, uh, we started showing before. And they've got, um, a UAS on this data of 89.8. But everybody loved that. And the reason they loved it was because it was so easy to use. It was so simple to understand. And it was just so fun to play with. is it could parse at 469 sentences a second. There had been other people that have worked out different more complex ways of doing parsing with so-called graph-based dependency parsers. So it was actually, you know, a bit more accurate. So this is another famous dependency parser from the 90s. It was actually a bit less accurate than the other one, but it was still a lot more accurate than other parsers that had been developed at the same time in the 80s and 1990s. but it was a bit more accurate at the cost of being two orders of magnitude slower. And, you know, people have worked on top of that. So, here is an even more complex graph-based parser, uh, from the 2000s. It's a little more accurate, but it's still not as accurate as the old one. But it's a lot more accurate than the old version, which was a lot of work to get to where it is today. And that's a good thing, because it means we don't have to do it all the time. again but it's gotten even slower. Um, okay. So, what we were able to show is that using the idea of instead using a neural network to make the decisions of Joakim Nivre Style shift-reduce parser, we could produce something that was almost as accurate as the very best parsers. We could produce a parsers that was nearly as good as the best. But it was slower. It was still faster than the best, but it wasn't as accurate. available at that time. I mean, strictly we won over here and we are a fraction behind on UAS. Um, but, you know, it was not only just as fast as Nivre's Parser. It was actually faster than Nivr's parser, because we didn't have to spend as much time on. UAS was faster than UAS because it didn't require as much work to understand. It wasn't as good as UAS, but it was better than it was before. feature computation. And that's actually almost a surprising result, right? It's not that we didn't have to do anything. We had to do matrix multiplies in our neural network, but it turned out, um, you could do the matrix multiplie more quickly than the feature computation that he was doing. It turned out that we could do it faster than he could do his feature computation. It was almost almost a surprise. It wasn't that we Didn't Have to Do Anything. We Had To Do Something. even though at the end of the day, it was sort of looking at weights that went into a support vector machine. So that was kind of cool. And so the secret was we're gonna make use of distributed representations like we've already seen for words. So for each word, we're going to represent it as a word embedding. But well, if we're interested in distributed representations, it's a good idea to use word vectors as the starting representations of words in our Parser. seem to us like maybe you should only have distributed representations of words. Um, maybe it also be good to have distributedrepresentations of other things. So we had parts of speech like, you know, nouns and verbs and adjectives and so on. Well some of those parts ofspeech have been taken out of the language. So maybe it would be good if we didn't have to use them as representations for other things as well. We'll have to wait and see. more to do with each other than others. I mean, [NOISE] in particular, um, most NLP work uses fine-grained parts of speech. So you don't only have a part of speech like noun or verb, you have parts ofspeech like singular noun versus plural noun and you have different types of noise. That's what NLP is all about. It's all about how to make people talk in a way that makes sense to them, I think. parts of speech for, you know, work, works, working, kind of the different forms of verbs are given different parts of speech, um, as well. So there's sort of sets of parts ofspeech labels that kind of clusters. So maybe we could have distributed representations, a part of speech. I don't know if that's possible, but it would be a good idea, I think. I'm not sure if it's possible to do that, but I'm thinking about it. that represent their similarity. Why not? Um, well if we're gonna do that, why not just keep on going and say the dependency labels. They also, um, have a distributed representation. And so, we built a representation that did that. So the idea is that we have in our stack, we have a stack, that represents their similarity, and a dependency label that represents the similarity. That's the dependency label. And that's what we use to represent the similarity of the two. the sort of the top positions of the stack, the first positions of. the buffer and for each of those positions, we have a word and a part of speech. If we've already built structure as here, we kind of know about a dependency that's already been built. And if we're not sure what we're doing, we can try to figure it out by looking at the top of the structure. And that's what we do here. We look at the stack and the first position of the buffer. And then we look at each of the words that we have. so we've got a triple for each position and we're gonna convert all of those into a distributed representation, um, which we are learning. We're gonna use those distributed representations to build our parsers. Okay. Now for- so, you know starting from- starting from the next lecture forward, we're going to learn how to build parsers with a number of different types of characters. We'll use those different types to help us understand how to write parsers in the future. we're gonna sort of s- start using a more complex forms of neural models. But for this model, um, we did it in a sort of a very simple straightforward way. We said, well, we could just use exactly the same model, exactly thesame parser structure that Nivre used, and that's what we did. And that's the way we came up with this model. It's very simple, it's very straight-forward, and it's a lot of fun to use. Right? Doing those shifts and left arcs and right arcs. Um, the only part we're gonna turn into a neural network. We're gonna have the decision of what to do next, um, being controlled by our neuralNetwork. So our neural network is just a very simple classifier of the data. Right? DoingThose shifts andleft arcs andright arcs.right? doing those decisions. Um,. the only thing we're going to turn into is a Neural Network. So, our neural Network is just an extremely simple classifiers of data. the kind that we are talking about last week. So based on the configuration, we create an input layer which means we're sort of taking the stuff in these boxers and turn- and looking up a vector representation for each one and concatenating them together to produce an input representation. The kind of thing that we're talking about this week is the sort of thing we're going to be talking about next week. We'll be looking at some of the things that we've been talking about in the past. that's sort of similar to when we were making those window classifiers and then we can concatenate a bunch of stuff together. So that gives us in our input layer. [NOISE] Um, so from there, we put things through a hidden layer just like last week. We do Wx plus Wx, and that's what we do in our output layer. That's what's in the output layer, so that's where we put the data. And then we put it in the hidden layer, just like we did in the previous week. b and then put it through a ReLU or a non-linearity to a hidden layer. And then on top of that, we're simply gonna stick a softmax output layer. So multiplying by another matrix, adding another, um, bias term, and then that goes into the softmax which is gonna give the output.b and the hidden layer, that's what we're going to do with it. It's gonna be a little bit different, but it's basically the same thing. a probability over our actions as to whether it's shift left arc or right arc, or the corresponding one with labels. And then we're gonna use the same kind of cross entropy loss to say how good a job did we do at guessing the action that we should have taken. We're gonna do the same thing to see how good we were at guessing our actions. We'll use that to see if we did as well as we thought we did, and then we'll see how well we did. taken according to the tree bank parse of the sentence. And so each step of the shift-reduce parser, we're making a decision as what to do next and we're doing it by this classifier. And we're getting a loss to the extent that we don't give probability one to the next. We're doing this by using the classifier to make a decision on the next step in the process. We don't want to give a probability of 1 to the previous step. We want to make sure that we're giving probability of 0. right action. Um, and so that's what we did using the tree bank. We trained up our parser, um, and it was then able to predict the sentences. The cool thing- the cool thing was, um,. that this had all the good things of Nivre's parser but, you know, it was a little bit different. It was a bit of an experiment, but it worked. It's a good way to start, I think, a new project. know, by having it use these dense representations, it meant that we could get greater accuracy and speed than Nivre's parser at the same time. So here is sort of some results on that. I mean, I already showed you some earlier results, right? So this was showing, um, the results of using dense representations. So this is showing some of the results that we got from using dense representation. And we're going to show some more of them in the next few days. fact, um, that, you know, we're outperforming these earlier parsers basically. People at Google said, "Well, this is pretty cool. Um, maybe we can get the numbers even better if we make it even better," he says. He says Google is now working on a new version of the software that will make the results even better. "We're going to be able to do this for a long time to come, I think, and I'm really excited about it," he adds. our neural network, um, bigger and deeper and we spend a lot more time tuning our hyper-parameters. Um, sad but true. All of these things help when you're building neural networks and when you are doing your final project. Sometimes the answer to making the results better is to make it bigger. Sometimes, the answers to making it bigger is to increase the number of neurons in the network. That's the way to get the most out of a neural network. And that's what we're going to do. bigger, deeper and spend more time choosing the hyper-parameters. Um, they put in Beam search as I sort of mentioned. Beam search can really help. Rather than just saying, "Let's work out what's the best next action, do that one and repeat over", Beam search, you know, rather than just say, "let's do this one and then that one" can help you find the best action to take. So in BeamSearch, you can do that and then do it again. you allow yourself to do a little bit of search. You sort of say, "Well, let's consider two actions and explore what happens." Um, quick question. Do humans always agree on how to build this trees and if they don't, what will be the [inaudible] or agreement of humans relative to each other? You can read the rest of the interview on CNN iReport.com, or click here to read the full interview. Back to the page you came from. to [inaudible] [OVERLAPPING] [NOISE] So that's a good question which I haven't addressed. Um, humans don't always agree. There are sort of two reasons they can't agree fundamentally. One is that, uh, humans, um, sort of mess up, right? Because human work is doing this aren't perfect. And the other is that they're trying to do something that they don't like, and they're not perfect at it. one is they generally think that there should be different structures. Um, so, you know, it depend- varies depending on the circumstances and so on. If you just get humans to parse sentences and say, "Well, what is the agreement and what they produced?" You know, maybe you're only getting half of the story, I think. I don't know if that's true, but that's what I'm trying to get out of it. I'm not sure if it's true. something like 92 percent. But, you know, if you then do an adjudication phase and you say, "Um, look at these differences, um, is one of them right or wrong?" There are a lot of them where, youknow, one of the person is effectively saying, "Oh yeah, I goofed" There are also cases where a person says, "I don't remember what I did," but they do remember it. There are cases where they don't recall it. There's a residual rate in which people can disagree about possible parses. "I think that's sort of more around three percent," he says. "There certainly are cases," he adds, "and that includes some of the prepositional phrase attachment ambiguities" "I wasn't paying attention or whatever," the author adds. "That's the residual rate," he concludes. "And that's the rate that people can actually disagree about possibly parses" "That is the residual rates in which, um, people can Actually disagree about Possible parses," the writer adds. Sometimes there are multiple attachments that sort of same clause although it's not really clear which one is right even though there are lots of other circumstances where one of them is very clearly wrong. Um, yeah. There's- there's still room to do better. I mean, at the unlabeled. I think that's what we need to focus on. We need to make sure that we don't have multiple attachments. We don't need to have multiple clauses. We want to be able to say that one of the attachments is right and the other is wrong. attachment score, it's actually starting to get pretty good. But there's still room to do better. Um, yeah. So Beam search, the final thing that they did was- that we're not gonna talk about here, is the sort of more global inference to make sure that it's sensible. um, yeah, that's what we're doing. That's what they're doing with Beam search as well. So that's the next step, and we'll talk about that later. Google developed these models that they gave silly names to, especially the Parsey McPa- parseFace, um, model of parsing. So that then- that's sort of pushed up the numbers even further so that they were sort of getting better at it, he says. "So, yeah. That then led to Google developing these models," he adds. "And so, yeah, that led to them being able to get better at what they were doing," he says, "and so that then led them to be able to do what they're doing now" close to 95 percent unlabeled accuracy score from these models. And actually, this work has kind of, you know, deep learning people like to optimize. But, um, the numbers are sort of getting a bit higher again, but, But, the work has continued along in the intervening two years and the numbers have continued to get higher and higher. And that's kind of what we're trying to do here. We're not trying to get to the top of the heap, but we're getting there. you know, so this actually, um, led to ah sort of a new era of sort of better parsers because so effectively this was the 90's- the 1990's era of parsers that was sort of where around 90 percent and then going into this sort of new generation of,Um, parsers. You know, this actually led to a new age of better Parsers, and so this led to sort of an era of better parsers and so on and so on. neural transition based dependency parsers. We sort of have gone down that we've halve that error-error rate. And we're now down to sort of about a five percent error rate. Yeah. I'm basically out of time now but there is further work including, you know, at Stanford. It's more accurate than 95 percent, right? So we- we're still going on but I think I'd better stop here today, um, and that's neural dependency parsing. [NOISE].