Andrej Karpathy: In this module, I'm going to briefly introduce the idea of differentiable programming. Differentiable programming is closely related to deep learning. It allows you to build up an increasingly more sophisticated model without losing track of what's going on. So let's suppose you want to do image classification. We need some way of representing images. To fix this problem, we introduce convolutional neural networks which is a refinement of fully connected neural networks. So here is an example of ConvNet in action. In NLP, words are discrete objects and neural networks speak vectors. So whenever you're doing NLP with neural nets, you first have to embed words, or more generally, tokens. There's a lot of processing that needs to happen and it's hard to kind of specify in advance. So what we're going to do is going to define an abstract function. An abstract function is something that has an interface but not an implementation. I'm going to talk about two implementations of the sequence models. One is recurrent neural networks and one is transformers.