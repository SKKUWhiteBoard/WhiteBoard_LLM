The EM algorithm is an unsupervised version of k-means GMM. In GMM, you guess randomly an assignment of every point to the cluster, the probability. You guess where they're probabilistically linked, that is, what's the probability of these points belong to cluster one, this point belongs to cluster two. And then once you have that, you then solve some estimation problem that looks like a traditional supervised learning. The decomposition is quite important. And we're going to try and kind of abstract that away. This is convexity and Jensen. So this is a classical inequality. And what I want to show you is that Jensen's inequality really is like conveXity in another guise, OK? And it's a key result, so i want to go through it. And then we almost certainly will not have time for this today, but I combine the notes, and we'll go continue to gothrough them on Wednesday. We'll go through what we call factor analysis, OK?" And factor analysis is another model. The reason I want it to be different is it's different than GMMs, so it occupies a different space, and it will kind of force you to look at the kind of decisions you're making, right? What are you modeling here? A set is convex if for any a and b element of omega, the line between them is in omega, OK? So what does that mean? So let's draw the picture first, and we'll draw the math. Here's the convex set. Now, no matter how I pick the points-- and clearly, I should really only have to worry about picking on the edge. So if I pick a point here and I picked b here, yep, the straight line is in the set, but that doesn't prove it's convex. If f is twice differentiable and, for all x, f double prime of x is greater than 0, then f is convex, OK? So this says these functions really are bowl-shaped, right? Second derivative being positive means that they have this kind of positive curvature that looks like the U's. Their first dimension-- first derivative goes up and down, but they're kind of always trending. That first derivative is always getting more positive. That's what it means by bowl- shaped. out a Taylor series for this. f double prime-- see the a, a minus z square. OK? And this a to a is just something in a, b. So maybe you remember this from your Taylor series. All I'm saying is I can write f of a at some point f of z plus some first derivative information. And then I'm using the second derivative remainder. This isn't super important for your conceptual understanding, by the way. Like, this is just to show that you can do what you want to do here, that this makes sense to you. Jensen's inequality says that the expected value of f of x is greater than f of theexpected value of x so long as f is convex. This inequality holds, and that's going to allow me to build a lower bound for my function, and I'm going to hillclimb using it. We actually don't want to use a convex function here because we're maximizing likelihood, and this is just notational pain, right? Like, if we were-- maybe we should have minimized unlikelihood, but this is where we are. conceptualize it is we solve for some hidden parameter. We solve, and that gives us an entire family of possible solutions. Let me draw the picture after I give you the formal set, OK? Oops. So EM algorithm has max likelihood-- I'll actually put MLE. All right, now, we're working with latent variable models. P(x; theta), says the function factors this way. Looks like a sum over z, where z is our hidden or latent variable. The algorithm is called GM, which is a supervised supervised learning algorithm. It works by taking an initial guess and mapping it to an easy-to-optimize function. The algorithm is then used to create a new curve, Lt of theta, based on that point. Jensen: "It's kind of a surrogate that I'm not overestimating my progress, and it's tight. So if I did happen to have the actual optimal value, it would meet at that point" Jensen's inequality can be used to turn a discrete function into a lower bound that works no matter how you pick it up. The key holds for any Q satisfying star, OK? No matter how I pick the probability distribution, this chain of reasoning goes through. So we have this character-- copy-- in here. This can also be written as an expected value, where z is distributed like Q of this weird-looking quantity. This is just symbol pushing. There is nothing deep going on. But it's important symbol pushing because it means Jensen's applies. Each data point is going to get its own different Q, which is the log of how likely this thing is, OK? And we picked those for each i. So as long as this term is a constant-- that is, it doesn't depend on z-- I'm in business, all right? So what that means is I want to pick Q such that log P of x, z; theta over Q(z) equals C. So what we've defined here is called the Evidence-based Lower BOund, or the ELBO. sequence that is monotonically increasing or nondecreasing, OK? So it's possible that it would grind to a halt. But eventually, it has to be strict. And so to derive a counterexample, you would just find a likelihood function that had those two bumps. And what it will do is it will gradually hillclimb. And this is actually not great. Like, it can't go back downhill, right? It's got to just continue to go up. If it gets locked inside one of those bumps, it's kind of toast. entire curve because we wanted to optimize it. So it wasn't enough to find a point in a lower bound. We needed to find the whole thing that was underneath it so we could run our argmax step. It's a lot of notation because we're abstracting out a huge number of things that we're doing. But in the end, it's not so bad, right? You take the Q(i)'s, set the thetas, do a descent on them, or ascent in this case. see if the loss of the likelihood function is not changing too much. What does too much depend on? Depends on your data, depends on the problem. Sometimes if you have a huge amount of data, and you're averaging over billions of examples, you want to get to machine precision and 10 to the minus 16. And so that's the way you decide when to do it. This just says that it's not going to oscillate wildly. It's a very weak statement I'm making. sense? Yeah, yeah. Thank you for the question. Why is this tight? Which one? Oh, it's tight because we went through this small piece here, which was that if we selected it as a constant in this particular way, then actually this line was no longer an inequality but was actually exact equality. And it depended, though-- that selection of Q depends on theta and x. [INAUDIBLE] Awesome. Great question. Yeah, and that's just making sure that the picture in your head is exactly right. EM is very general. You can instantiate it, right? So what does it mean here? Now, what actually happened here when we wanted to understand-- what was the probability? This says, the probability that i, the i-th component, belongs in j given what we've observed about x(i) and what we know about the cluster shapes and their frequencies. Now, remember, if we just looked at this, and these two distributions were-- or the phis were equal-- then we would say, oh, it's probably much more likely it belongs to this function, cluster 1. phi 2 was hugely bigger than phi 1, right-- a billion points came from the second source, and only one point came from first source. We'd probably say that it's more likely that it would go to this, right? It would certainly boost its probability. So to automate this, this is Bayes' rule. It just weighs those two probabilities and tells us what should happen. That's it. We ran through exactly those calculations last time. All right, let's take a look at the M-step now. P z(i) j-- to compute that, remember, we expanded it by Bayes' rule. We had, if you knew you were in a cluster, how likely is the data point? And then we had a term that said, howlikely is the cluster? And those were the two functions that we put in and broke down. It's exactly the same. You've got it perfectly. Yeah. All right, so let me write out this monstrosity just because it will be potentially-- it has been in the past educational. Who knows if it's educational in the future? equal to the sum over j-- because now I'm summing over the cluster centers, right? The z(i) notation was still very abstract-- wj(i), which was summing. over this part here, log-- and help us all, 1 over 2 pi-- this is a covariance, 1/2. This is the exp of Oh, I decided to write this in four general things. OK, I see why. Transpose sigma inverse x( i) mu j times phi j. All right, let me scoot. Sigma j is full rank. Because it's full rank, we can pull it out and it's linear, and it doesn't change anything. We get here sigma j inverse times sum, which is an unfortunate collision. i equals 1 to n w(i)j x(i), minus mu j equals 0, OK? But then because this is full. rank, the only way that this thing is 0 is if it's identically 0, right? If this were non-full rank-- sorry, the j is in the wrong spot. In calculus, phi j is constrained, and I just want to remind you of something that you probably learned in high school or freshman year in calculus. If you haven't seen this before, this will trip you up in some way. What you do is you introduce this thing called Lagrange multipliers. And it says, if you're going off in a direction that would not change any of their values, that's OK. And I'll just post a one-page write-up for you. the same j's. No, but [INAUDIBLE]. Which line? Just says phi j i. I see. I was talking while I was saying. Thank you for the clarification. Apologies for that. Yes, it's this constraint here. Sorry, this is the constraint that was in our head. Yeah, and it just makes a mysterious reappearance here, all right? All right, awesome. OK, so what is the message that I want you to take away from this? Two things. First message is GMM is an EM algorithm. MLE for the entire quarter on those properties. Then we introduced a ton of typos to keep you on your toes. In the next class, as I said, what we're going to see is this notion of factor analysis. And that is going to tell us how to apply EM to a different kind of setting, which, at first glance, will look kind of impossible to do without a latent variable model. And I think that's all I want to say. Thanks so much for your time and attention.