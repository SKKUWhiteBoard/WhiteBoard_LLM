homework two is out now. This weekend sessions will be having some more background on deep learning. We're also gonna be reaching, uh, releasing by the end of tomorrow, what the default projects will be for this class. Um, and those proposals will be due, um, very soon, er, in a little over a week. The assignments, [inaudible] are they limited to TensorFlow? asked the question if, if the assignment is limited toTensorFlow. I'm pretty sure that everything relies that you're using Tensor Flow. Deep Q Learning is a form of reinforcement learning. It uses deep neural networks to learn to make complex decisions. We'll be covering the basics of Deep Q Learning in this tutorial. You can look at the tutorial, or you can reach out to us on Piazza with any other questions. We hope to see you in the next class on November 14th and 15th, at 10:30am and 11:00am. Back to the page you came from. Click here for more information about the class. when we thought about doing this, we're gonna focus on function approximations that are differentiable. Um, and the nice thing about differentiable rep-representations is that we can use our data, and we can estimate our parameters, and then we can take use gradient descent to try to fit our function. And that information now could be [NOISE] in the form of episodes or it could be individual tuples. When I say a tuple, I generally mean a state-action reward next state tuple. is the same as the full gradient update. And the key hard thing was that we don't know what this is. So, the two ways we talked about last time was inspired by a work on Monte Carlo, or on TD learning. We could either plug-in the return from the full episode. This is the sum of her words. Or we could put in a bootstrapped return. Where we look at the reward, the next state, and the value of our next state. all for linear value function approximation, but there are some limitations to use the linear valuefunction approximation, even though this has been probably the most well-studied. So, if you have the right set of features, and historically there was a lot of work on figuring out what those rights set offeatures are. They often worked really well. And in fact when we get into, I think I mentioned briefly before. When we start to talk about deep neural networks you can think of, a deep neural network is just a really complicated way to get out features. Non-parametric approaches, where your representation size tends to grow with the number of data points. These have a lot stronger convergence results compared to linear value function approximators. But they're not gonna scale very well and in practice you don't tend to see them, though there's some really cool work by my colleague, Finale Doshi-Velez, over at Harvard who's thinking about using them in the future. The number of points you need to tile that 180 degrees space, generally scales exponentially with the dimension. These for things like, um, health care applications and how do you sort of generalized from related patients. So, they can be useful but they generally don't scale so well. What we're gonna talk about today is thinking about deep neural networks which also have very flexible representations but we hope we'll be able to scale a lot better. Um, now, in general we're going to have almost no theoretical guarantees for the rest of the day, and- but in practice they often work really really well. Deep neural networks are artificial connections with artificial neural networks inside our brain. They are used in unsupervised learning like predicting whether or not something is a cat or not or, you know, an image, uh, of a particular object, um, or for regression. They combine both linear and non-linear transformations, and they need to be differentiable if gradient descent is to be used to fit them. In the last 5 to 8 years, there's auto differentiation. So, you don't have to derive all of these gradients by hand instead. Convolutional neural networks are used extensively in computer vision. They can represent any function with the deep neural network. You can use exponentially less nodes or parameters compared to using a shallow net which means not as many of those compositions, um, to represent the same function. The final thing is that you can learn the parameters using stochastic gradient descent in like five seconds. It's very likely that we're going to want to use similar sorts of input on our- our robots in our artificial agents. for really high dimensional input and kind of average and slow down until we can get to, um, a low dimensional output. So, the final layer is typically fully connected. We're kind of computing this new feature representation of the image, and at the very end, we can take some fully connected layer, where it's like doing linear regression, and use that to output predictions or scalars. So these type of representations, both Deep Neural Networks and Convolutional Neural Networks, are both used extensively in deep reinforcement learning. how we could use these type of approximations for Atari. And then we had the results that were kind of happening around like 1995 to maybe like 1998, which said that, "Function approximation plus offline off policy control, plus bootstrapping can be bad, can fail to converge" So I think for a long time after that, the, the community was sort of backed away from Deep Neural Networks for a while. And so, perhaps it was natural that, like, around in like 2014, DeepMind and DeepMind combined them and had some really amazing successes with Atari. I think there's also a couple algorithmic ideas that we're gonna see later in this lecture, that help the performance kind of avoid some of those convergence problems. So with the Atari case specifically, did you- did you avoid that problem? Well, sort of, that if you tried it by having on policy control? I just don't know. Um, and it's a great question for me. We'll see how it works here. Anyone else? Okay, cool. So, we'll- we'll see an example for breakout shortly, um, of what they did. are the important things that they, um, did in their paper, this is a nature paper from 2015, is they use the same architecture and hyperparameters across all games. Now just to be clear, they're gonna then learn different Q functions and different policies for each game. But their point was that they didn't have to use totally different architectures, do totally different hyperparameter tuning for every single game separately. It really was the sort of general architecture and setup was sufficient for them to be able to learn to make good decisions for all of the games. DQN, deep Q-learning addresses these is by experienced replay and fixed Q-targets. Experienced replay, prime number if you guys have heard about this, if you learned about DQN before is we're just gonna stroll data. So this is nice because basically it means that you reuse your data instead of just using each data point once, you can reuse it and that can be helpful. And what we'll look at that more in a minute. [inaudible] It's a great question which is this equivalent to keeping more frames in our representation? It's not that interesting. I am in the world, I'm now in state four. It's like I suddenly pretend that I'm back in s1, took a1, got r1, and went to s2 and I'm gonna update my weights again. The reason that that update will be different than before is because I've now updated using my second update and my third update. So, it'll cause a different weight update. In general, one thing we talked about a long time ago is that if you, um, uh, do TD learning to converge it, which means that you go over your data mu- like, like, an infinite amount of time. learn to model, a dynamics model, and then the planning for that which is pretty cool. But we don't wanna do that all the time because there's a computation trade-off and particularly here because we're in games. There's a directtrade-off between computation and getting more experience. Can we use something similar to like exploitation versus exploration. Um, essentially like with random probability just decide to re-flag [inaudible]. The question is about how would we choose between getting new data and how much to replay et cetera. based on the experience replay versus getting, um, putting new samples into there. So, generally right now is really heuristic trade-off. Could certainly imagine trying to optimally figure this out but that also requires computation. Um, but if, you know, your agent thinking about how to prioritize its own computation which is a super cool problem. Which is what we solve all the time. Okay. The second thing that DQN does is it first has- it first keeps route this old data. minus. I'll call it minus because, um, well there might be other conventions but in particular it's the older set of weights, the ones we're not updating right now. Those are the ones that we're using them as target calculation. So, when we compute our target value we, again, can sample and experience tuple from the dataset from our experience replay buffer, compute the target value using our w minus, and then we use stochastic gradient descent to update the network weights. help, um, in terms of stability? In terms of Stability, it helps because you're basically reducing the noise in your target. If you kept your target fixed forever, you would learn the weights that- that minimize the error to a constant function. That would then be stable because you always have the same target value that you're always trying to predict. And eventually you'd learn that, and that would eventually be stable. And that's what we're trying to do with GT. This is just reducing the noise and the target that we're trying to sort of, um, if you think of this as a supervised learning problem, we have an input x and output y. The challenge in RL is that our y is changing. If you make it that you're- so your y is not changing, it's much easier to fit. This is really just about stability and that's- that's true for the experience replay too. Experience replay is just kinda propagate information more- more effectively and this is just gonna make it more stable. It stores the transition in this sort of replay buffer, a replay memory, um, use sample random mini-batches from D. So, normally sample in mini-batch instead of a single one. You do your gradient descent given those. Um, you compute Q learning using these old targets and you optimize the mean squared error between the Q network and Q learning targets, use stochastic gradient descent, and something I did not mention on here is that we're typically doing E-greedy exploration. of what the agent is doing. So, remember the agent's just learning from pixels here how to do this. Um, and one of the interesting things about it is that as you'd hope, as it gets more and more data, it learns to make better decisions. So this is really cool that sort of it could discover things that maybe are strategies that people take a little while to learn when they're first learning the game as well. Yeah, so, um, you might see, uh, I think she is talking- she is referring to the fact that the paddle was moving a lot. clearly sort of an inexperienced player to do that. That would be a strange thing but from the agent's perspective, that's completely reasonable. Um, and it does not give him positive or negative reward from that. So, it can't distinguish between, you know, stay in stationary versus going left or right. If you put it in a cost for movement that could help. This might become a little bit of [inaudible] but is there a reason to introduce a pulling layer? Puling layer? There might be one in there. uh, they're not talking about how long it took them or their agent to learn and as you guys will find out for homework two, it can be a lot of experience. So, they did very well on some domains. Some domains, they doing very poorly. Um, I think that it's clear that the really important feature is replay. We'll probably talk- uh, we'll talk a lot more about exploration later on in the course. so, what was critical? So, I- I like the, uh, there's a lot  lovely things about this paper and one of the really nice things is that they did a nice ablation study. Replay is hugely important and it just gives us a much better way to use the data. Double DQN is kind of like double Q learning, which we covered very briefly at the end of a couple of classes ago. Greedy Policy is where we average networks, average reward networks, and then use one of the Qs as the target for the other network. Then we update Q2 with 50 percent probability, we pick the next action from the next network, this is a pretty small change. back to the Mars Rover example. So, let's say you get to choose two replay backups to do. Vote if you think it matters which ones you pick, in terms of the value function you get out. If you pick backup three, so what's backup three? It is, S2, A1, 0, S1. So that means now you're gonna get to backup and so now your V of S2 is gonna be equal to one. So you've got to back-propagate from the information you're already [NOISE] have on step one to step two. The number, of, um, updates you need to do until your value function converges to the right thing can be exponentially smaller. If you update carefully and you, you could have an oracle tells you exactly what to do. But you can't do that. You're not gonna spend all this. It- it's very computationally, expensive or impossible in some cases to figure out exactly what that. oracle ordering should be. But it does illustrate that we might wanna be careful about the order that we do it. The idea is that, if you want to, make decisions in the world, they're working some states are better or worse, but they're just gonna have higher value or lower value, but- figure out what the right action is. One method basically takes these priorities, raises them to some power alpha, um, and then normalizes. And then that's the probability, of selecting that tuples. So you prioritize more things that are weights. Doesn't freezing. in the old ways were a counter to propagating back the information there? It could be super tempting to try to start, by like implementing Q learning directly on the ATARI. Highly encourage you to first go through, sort of the order of the assignment and like, do the linear case, make sure your Q learning is totally working. Even with the smaller games, like Pong which we're working on, um, it is enormously time consuming. It's way better to make sure that you know your Q Learning method is working, before you wait, 12 hours to see. whether or not, oh it didn't learn anything on Pogge. So, that, that- there's a reason for why we, sort of build up, the way we do in the assignment. Um, another practical, to a few other practical tips, feel free to, to look at those, um, and then we were on Thursday. Thanks. Back to Mail Online home.back to the page you came from. Back from the page where you come from. back to MailOnline home.