Today we're gonna talk about learning in the setting of games. Can you still be optimal if you reveal your strategy? It's actually not the size that matters. It's the type of strategy that you play that matters, so just to give you an idea. And then, er, towards the end of the lecture, we wanna talk a little [NOISE] bit about variations of the game- the games we have talked about. So, uh, how about if you have- how about the cases where we have simultaneous games. In the game of chase- che- and chess, you have this evaluation function that can depend on the number of pieces you have, the mobility of your pieces. Maybe the safety of your king, central control, all these various things that you might care about. And, and potentially, a designer can come in and say, "Well, I care about about nine times more than I Care about how many pawns I have." So the hand-designer can actually hand- design these things and write down these weights. can actually, like, roll two dice and based on the outcome of your dice, you move your pieces various, various amounts to. Uh, there are a bunch of rules. So your goal is to get all your pieces off the board. If you have only one piece and your opponent gets on top of you, they can push you to the bar and you have to start again. So, so what are some features that you think might be useful? Remember the learning lecture? Yes. How did we come up with feature templates? Currently, still bound with the [inaudible]. So maybe like the location of the X's and O's. somewhere. So, so one idea that we can use here is we can try to generate data based on our current policy pi agent or pi opponent, which is based onOur current estimate of what V is. Right. So currently, I might have some idea of what this V function is. It might be a very bad idea ofwhat V is, but that's okay. I can just start with that and starting with, with that V function that I currently have, what I can do is I can, I can call arg max of V over successors of s and a to get a policy for my agent. So, what you have here is you have a piece of experience; s, a, r, s prime, we can try to learn something from each one of these pieces of experience. And then we had a target that you're trying to get to. And my target, which is kind- kind of acts as a label, is going to be equal to my reward, the reward that I'm getting. So, so my target the thing I'm trying to like get to is the reward plus Gamma V of s Prime, w. is the TD learning algorithm. This is all it does. temporal difference learning, what it does is it picks like these pieces of experience; s, a, r, s prime, and then based on that pieces of. experience, it just updates w based on this gradient descent update, difference between. prediction and target times the gradient of V. There are very minor differences that you'll talk about actually at the end of this section, comparing it to Q learning. All right. So I wanna go over an example, it's kind of like a tedious example but I think it helps going over that and kind of seeing why it works. If you use like, uh, initialize rates do not be zeros which you update throughout instead of just to the end. Yeah. Okay and section two, so S4 and S9 are the same future of activities but you said S4 is S9 [OVERLAPPING]. Uh, this is a made up example, [LAUGHTER] so don't think about this example too much though. Well, is it that possible to have, an end state and not end state have the same feature vector, or no? As one, uh, entry that's always isn't [inaudible] like instead of 1, 2, we have 1, 0 leading to the, the final weight then the weight corresponding to that. Is going to- [OVERLAPPING] Yeah. It will never converge. And that kind of tells you that that entry in your feature vector, you don't care about that. If it is always 0, it doesn't matter what the weight of that entry is. So in general, you wanna have features that are differentiating and, and you're using it in some way. on the relationship between the features and the weights. Uh, they always have to be the same dimension, and what should we be thinking about that would make a good feature for updating the weights specifically, like- So, uh, okay so first off, yes, they need to be always in the same- in dimension cause you are doing this, um, dot-product between them. Um, the feature selection, you don't necessarily think of it as, like how am I update the weights, you think of the featureselection as is it representative of how good my board is. to 0.25 and 0.75 then it kind of stays there, and you are happy. All right so, so this was just an example of TD learning but this is the update that you have kind of already seen. And then a lot of you have pointed out that this is, this is similar to Q-learning already, right? This is actually pretty similar to update, um, it's very similar, like we have these gradients, and, and the same weight that we have in Q- learning. The idea of learning in games is old. People have been using it. In the case of Backgammon, um, this was around '90s when Tesauro came up with, with an algorithm to solve the game. And he was able to reach human expert play. And then more recently we have been looking at the game of Go. So in 2016, we had AlphaGo, uh, which was using a lot of expert knowledge in addition to ideas from a Monte Carlo tree search and then, in 2017, we have AlphaGo Zero, which wasn't using even expert knowledge. Minimax sca- strategy seemed to be pretty okay when it comes to solving these turn-based games. But not all games are turn- based, right? Like an example of it is rock-paper-scissors. You're all playing at the same time, everyone is playing simultaneously. The question is, how do we go about solving simultaneously, okay? So let's start with, um, a game that is a simplified version of rock- paper-sc scissors. This is called a two-finger Morra game. So, so you have things like pure strategies, uh, pure strategies. We have also this other thing that's called mixed strategy which is equivalent to, to stochastic policies. So, so pure strategies are just actions a's. And then you can have things that are called mixed strategies and they are probabilities of, of choosing action a, okay? All right. So how do we evaluate the value of the game? Well, I'm gonna use my payoff matrix, right? So 1 times 1 over 2 times the value that we get at 1, 1, is equal to 2. Someone tells me it's pi A and pi B, I can evaluate it. I can know how good pi A is, from the perspective of agent A. But with the challenge here is we are playing simultaneously, so we can't really use the minimax tree. So what should we do? So I'm going to assume we can play sequentially. So that's what I wanna do for now. So right now I'm gonna focus only on pure strategies. I will just consider a setting- very limited setting and see what happens. In a more general case, I'm gonna make a lot of generalizations in this lecture. So I'll show you one example I generalize it, but if you're interested in details of it, like, we can talk about it offline. So, so, so setting is for any fixed mixed strategy Pi A. What I should do as Agent B is I should minimize that value. I should pick Pi B in a way that minimizes that value, and that can be attained by pure strategy. The key idea here is revealing your optimal mixed strategy does not hurt you which is kind of a cool idea. The proof of that is interesting. If you're interested in looking at the notes, you can use linear programming here. So next 10 minutes, I want to spend a little bit of time talking about non-zero-sum games. In real life, you're kind of somewhere in between that, and, and I'm going to motivate that by this idea of Prisoner's dilemma. different players. So the von Neumann's minimax theorem doesn't really apply here because we don't have the zero-sum game. But do you actually get something a little bit weaker, and that's the idea of Nash equilibrium. So a Nash equilibrium is setup policies Pi star A and Pi star B so that no player has an incentive to change their strategy. So, so what that, that means is if you look at the, the value function from perspective of player A. Nash's existence theorem says if any finite player game with a finite number of actions, then there exists at least one Nash equilibrium. In a collaborative Two-finger Morra game, it's not a zero-sum game anymore and, and you have two Nash equilibria. And then Prisoner's dilemma is the case where both of them testify. And yeah, there are other types of games still like Security Games and or resource allocation games that have some characteristics that are similar to things we talked about.