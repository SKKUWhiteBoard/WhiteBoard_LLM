Differentiable programming is closely related to deep learning. It allows you to build up an increasingly more sophisticated model without losing track of what's going on. The FeedForward function takes in an input vector x and produces an output vector which could be of a different dimensionality. So now we can write our three layer neural network using FeedForward and the way I'm going to do this is score is equal to you take x or distribute phi of x. And you can write this as FeedForward cubed as to be more compact. This is a very compact way of writing something that would otherwise be quite complicated. you want to do image classification. We need some way of representing images. The FeedForward function that we just introduced, takes a vector as input and we can represent an image as a long vector by, for example, adding all the rows. But then we would have this huge matrix that we would need to be able to transform this vector resulting in a lot of parameters which may make life difficult. To fix this problem, we introduce convolutional neural networks which is a refinement of a fully connected neural network. first is Conv. Conv takes an image and the image is going to be represented as a volume which is a collection of matrices, one for each channel, red, green, blue. Each matrix has the same dimensionality as the image, height by width. The way that Conv is. going to compute this volume is via a sequence of filters, and intuitively. what it's going to do is try to detect local patterns with [AUDIO OUT] So here is one filter and how it works is I'm going to slide this filter across the image. going to slide a little max operation over every 2x2 or 3x3 region. So the max over these four numbers is going to be used to build this [INAUDIBLE] and so on. That's all I'm going to say about MaxPool. If you want to go into the details, you can check out this demo or you can learn more in 231. But again, I want to highlight that there's these two modules. One for detecting patterns and one for aggregating. And with these two functions along with FeedForward, now we can define AlexNet. In NLP, words are discrete objects and neural networks speak vectors. So whenever you're doing NLP with neural nets, you first have to embed words, or more generally, tokens. We're going to define an EmbedToken function that takes a word or a token x and maps it into a vector. And all this function is going to do is it's going to look up vector in a dictionary that has a static set of vectors associated with particular tokens. So this representation of the sentence is not going to be a particularly sophisticated one. is something that has an interface but not an implementation. So a SequenceModel is going to be something that takes a sequence of input vectors and produces a corresponding sequence of output vectors. I'm going to talk about two implementations of the sequence models. One is recurrent neural networks and one is transformers. So an RNN, or a recurrent neural network, can be thought of as reading a sentence left to right. So the intuition again is reading left-to-right, updating the hidden state as you go along. Collapse takes a sequence of vectors and returns a single vector. There's three common things you can do. If you're doing text classification, you probably want to pick the average to not privilege any individual word. But as we'll see later if you're trying to do language modeling, you want to take the last. These types of functions where the input and output have the same type signature are really handy because then you can compose them with each other and get multiple steps of computation. The core part of a transformer is the attention mechanism. The attention mechanism takes in a collection of input vectors and a query vector and it outputs a single vector. So in contrast with the RNN, you have representations that have to kind of proceed step by step. And the number of steps is the length of a sequence which causes these long chains which prevents kind of fast propagation, whereas attention solves this problem. So there's two other pieces I need to talk about before I can talk about the transformer. was this complicated thing that I mentioned at the beginning. So BERT is this large unsupervised pretrained model which came out in 2018 which has really kind of transformed NLP. And what BERT does on a sequence of tokens is it's going to embed the tokens, and then it's just going to apply the TransformerBlock 24 times. So we can take language models and we can build on top of them to create what is known as a sequence-to-sequence model. So this is by and large how a lot of the state of the art methods for, for example, machine translation works. encourage you to consult the original source if you want kind of the actual, the full gory details. Another thing I haven't talked about is learning any of these models. It's going to be using some variant of stochastic gradient descent, but there's often various tricks that are needed to get it to work. But maybe the final thing I'll leave you with is the idea that all of differentiable programming is built out of modules. Even if you kind of don't understand or I didn't explain the details, I think it's really important to pay attention to the type signature of these functions.