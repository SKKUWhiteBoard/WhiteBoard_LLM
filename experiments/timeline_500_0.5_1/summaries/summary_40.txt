Danqi Chen is one of the foremost researchers in question answering. She is the professor at the Princeton University. Danqi once upon a time was the head TA of CS224N. She's quite familiar with the context of this class. So today I'm very happy to introduce to you some of the fundamentals in this field, as well as on cutting edge and state of the art topics. So here's my plan for this lecture. I'm going to spend the most of this lecture focused on one type of question answering problems called reading comprehension. like 10-ish minutes to talk about a more practical, and in my opinion, more exciting problem called open domain question answering. Question answering, or, let's say QA in short, is one of the earliest NLP tasks, and the early systems can even date back to the 1960s. And, the question and answer has enabled a lot of really useful real world applications. For example, today if you just put your question in a search engine like Google, you can actually click on the correct answer, which is actually our concise answer. The key difference between the generative model and the extractive model is that for generative models, you can actually leverage more input passage together and do the generation. So for this model, there isn't any retrieval. So this model really has you relying on all the parameters you memorized, all the information. So by just taking this input, it has to just rely on the parameters to infer this answer. So it's actually very hard to-- yeah, it's a definite balance between memory and the generalization from it, yeah.