homework two is out now. What the default projects will be, uh, for this class. Um, and you guys will get to pick whether or not you wanna do your own construction project or, um, the default project. The assignments, [inaudible] are they limited to TensorFlow? asked the question if, if the assignment is limited toTensorFlow. If you have any questions about getting setup without feel free to use the Piazza channel. We also released a tutorial for how to just sort of set up your machine last week. That's a great place to get started. "We wanna be able to deal with really complex, um, information about customers, or patients, or students, where we might have enormous state and our actions spaces. And so, when we started talking about those, I was arguing that we either need representations of models. T, T or R or a state-action values Q, or V, or our policies," he says. "With the idea being that we may in fact never encountered the exact same state again. You might never see the exactsame image of the world again, um" as, um, a mean squared error. So, we can define our loss j, and we can use gradient descent on that to try to find the parameters w that optimize. And just as a reminder stochastic gradient descent was useful because when we could just slowly update our parameters as we get more information. And that information now could be [NOISE] in the form of episodes or it could be individual tuples. When I say a tuple, I generally mean a state-action reward next state tuple. the last layer being a linear combination of those features. For most of the time when we're talking about deep RL with, um, a deep neural networks represent the Q function. So, linear value function is often really works very well if you're the right set of features, but is this challenge of what is the rightSet of features. Um, and there are all sorts of implications about whether or not we're even gonna be able to write down the true p- um, value function. to linear value function approximators. The intuition is that if you want to have sort of an accurate representation of your value function, um, and you're representing it by say, uh, local points around it. For example, with the k-nearest neighbor approach. then the number of points you need to have everything be close like in an epsilon ball scales with the- the dimensionality. Um, but they haven't far been used for in a very widespread way. Deep neural networks are very flexible representations but they don't scale very well. In practice they often work really really well. They become an incredibly useful tool in reinforcement learning and everywhere else really in terms of machine learning. So, what we're gonna talk about today is thinking about deep neural networks which also have very flexible. representations but we hope we'll be able to scale a lot better. We'll talk in a second about what these type of functions are and then we'll talk about how to use them. Instead of having our full x input, we're just gonna take in- we're gonna direct different parts of the x input to different neurons which you can think of just different functions. We think that often, the brain is doing this. It's trying to pick up different sort of features. So, we want to sort of extract features that are relevant for deciding whether or not a face, for example, is a face or not. This means also that rather than computing this sort of variance in translation, you can do this all the way across the image. Deep Neural Networks and Convolutional Neural Networks are both used extensively in deep reinforcement learning. In 1994, we had TD backgammon which used Deep Neural Networks. And then we had the results that were kind of happening around like 1995 to maybe like 1998, which said that, "Function approximation plus offline off policy control, plus bootstrapping can be bad, can fail to converge" So I think it sort of really changed the story of how people are perceiving using this sort of complicated function approximation. In a paper from 2015, a group of researchers used the same architecture and hyperparameters across all 50 games. They didn't have to use totally different architectures, do totally different hyperparameter tuning for every single game separately. It really was the sort of general, um, architecture and setup was sufficient for them to be able to learn to make good decisions for all of the games. And the nice thing is that, I think this is actually required by nature. They, they released the source code as well. DQN, deep Q-learning addresses these is by experienced replay and fixed Q-targets. Experienced replay, prime number is we're just gonna stroll data. So, even though we're treating the target as a scalar, the weights will get updated the next round which means our target value changes. And so, um, you can sort of propagate this information and essentially the main idea, is just that we're gonna use data more than once, and that's often very helpful. Learn to model is a dynamics model, a word model, and then the planning for that which is pretty cool. But we don't wanna do that all the time because there's a computation trade-off and particularly here because we're in games. So, the experienced replay buffer has like a fixed size. Um, is it the most recent and- and how do you remove items from it? It's a really interesting question. Different people do different things. But you can make different choices and there's interesting questions of what thing that you should kick out. use in that value of S prime for several rounds. So, instead of always update- taking whatever the most recent one is, we're just gonna fix it for awhile and that's basically like making this more stable. Because this, in general, is an approximation of the oracle of V star. What this is saying is, don't do that, keep the weights fixed that used to compute VS prime for a little while and that just makes the target, the sort of the thing that you're trying to minimize your loss with respect to, morestable. Our w minus, and then we use stochastic gradient descent to update the network weights. So, intuitively, why does this help, um, in terms of stability? In terms of. stability, it helps because you're basically reducing the noise in your target. Do you every update the- Minus at all, or is that [inaudible] Great question. Di- Dell? Dian. How is this really, not grade- gradient descent, like, all those assumptions? Researchers have trained an agent to play Atari games as well as humans. The agent can learn to exploit the reward function to maximize its chances of winning. The team is now looking at whether there is a pulling layer in the network to help the agent learn how to play the games. The next step for the team is to train the agent to be able to play a variety of different Atari games at the same level of performance as a human. The first step is to teach the agent how to move the paddle around. Replay is hugely important and it just gives us a much better way to use the data. Double DQN is kind of like double Q learning, which we covered very briefly at the end of a couple of classes ago. Greedy Policy is where we average networks, average reward networks, and then use one of the Qs as the target for the other network. Then we update Q2 with 50 percent probability, we pick the next action from the next network, this is a pretty small change. So, let's say you get to choose two replay backups to do. Maybe it doesn't matter if you can just pick any of these, you're going to get the same value function no matter what you do. So, it absolutely matters which two you pick in terms of the resulting value function. It matters the order in which you did, do it. If you had done S3, a1, 0, S2, your S3 wouldn't. be in this case. Lemme just to clarify, if we set alpha to zero, what's the rule for selecting tuples among the existing tuples? It's a great question. Lemme just clarify. If you are fixing, um, uh, your w minus, then, if you were looking at our case that we had before, then you wouldn't be able to continue propagating that back, because you hadn't update yet, yet, that's exactly right. So there's gonna be this tension between, when you fix things versus her propagating information back. It's very computationally, expensive or impossible in some cases to figure out. could be super tempting to try to start, by like implementing Q learning directly on the ATARI. Highly encourage you to first go through, sort of the order of the assignment and like, do the linear case. Even with the smaller games, like Pong which we're working on, um, it is enormously time consuming. It's way better to make sure that you know your Q Learning method is working, before you wait, 12 hours to see whether or not, oh it didn't learn anything on Pogge.