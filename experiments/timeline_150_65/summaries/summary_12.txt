This week in week three, we're actually going to have some human language, and so this lecture has no partial derivative signs in it. In today's lecture, we want to look at, well, what kind of structures do human language sentences have, and how we can build models that, um, build that kind of structure for sentences that we see. And then talk about how you can make neural,Um, dependency parsers. And so we hope that you can put together a neural dependency parser. what you learned about neural networks last week and the content of today, and jump straight right in to building a neural dependency parsers. Um, the other thing that happens in assignment three is that, we start using a deep learning framework PyTorch. So, for doing assignment three, and this is in the PDF for the assignment, is to install Py Torch as a Python package, and start using that. If you have any issues with, with that, um, well, obviously, you can send Piazza messages, come to office hours. there's sort of a one hour introduction to PyTorch on thePyTorch site. We're going to sort of focus on those more in week five, but if it's not bad to be thinking about things you could do, if you're under a custom final project. We have under the sort of office hours page on the website, a listing of the expertise of some of the different TAs. Since I missed my office hours yesterday, I'm gonna have a shortened office hour tomorrow from 1:00 to 2:20. can kind of come for any reason you want, but it might be especially good to come to me if you want to talk about, um, final projects. So, let's leap in and start talking about the structure of sentences. And so, I just sort of want to explain something about human language sentence structure, and how people think about that structure. Um, all of the examples I'm going to give today are in English, because that's the language that you're all expected to have some competence in. But this really isn't meant to be sort of facts about English. Linguists have thought about the structure of sentences in two ways. One of them is called phrase structure, or phrase structure grammars. The idea of phrase structure is to say that sentences are built out of units that progressively nest. So, we start off with words that, cat, cuddly, et cetera, and then we're gonna put them into bigger units that we call phrases. And then you can keep on combining those up into even bigger phrases, like, "Thecuddly cat by the door" they're also referred to as article sometimes in English. There's another word class here of nouns. And so, what I- to capture this pattern here, it seems like we can make this unit, um, that I see all over the place in language, which is made of a determiner, followed by a noun. So, maybe I can say from my grammar that a noun is a phrase structure grammar role, a context-free grammar role of- I can have a noun phrase that goes to aeterminer, and a noun, and so on. phrase goes to a determiner, and then optionally, you can put in an adjective. And then I poke around a little bit further and I can find examples like the cat in a crate, or a barking dog by the door. And so I want to put those into my grammar. But at that point, I noticed something special, because look, here are some other things, and these things look a lot like the things I started off with. So, it seems like, which sort of having a phrase with the same expansion potential that's nested inside this bigger phrase. that a noun phrase goes to a determiner, optionally an adjective, a noun, and then a something else, which I'll call a prepositional phrase. And then I'm gonna write a second rule saying that a prePOSitional phrase goes. to a preposition, that's gonna be these words here, um, followed by a. noun phrase. So then I could immediately generate other stuff. I can sort of say, "The cat by the, the large door." Or indeed I could say, 'The catby the large crate on the table' what you find is that prepositional phrases following the verb. But if you go to a different language like Chinese, what you find are the prepositions coming before the verbs. And so, we could say okay, there are different rules for Chinese, um, and I could start writing a context-free grammar for them. Um,so that's the idea of context- free grammars. This is the dominant approached linguistic structure that you'll see if you do a linguistics class in the linguistics department. sort of phrasal categories, like, noun phrases and prepositional phrases, and things like that. We are going to directly, um, represent the structure of sentences by saying, how words, how arguments or modifiers of other words in a recursive faction. Which is sort of another way of saying how the dependence on other words. So, we have a sentence, ''Look in the large crate in the kitchen by the door''. And if we want to we can give these word, words word classes, so we can still say this is a verb. In this system of dependencies I'm going to show you, we've got in as kind of, um, a modifier of crate in the large crate. And well, then we have this next bit by the door. And as I'll discuss in a minute, well, what does the by thedoor modifying? It's still modifying the crate, it saying, ''It's the crate by the doors.'' Okay. So, the structure you get may be drawn a little bit more neatly when I did that in advance like this. to your friends is that you just blab of something, and I understand what you're saying, and, um, what goes on beyond that, is sort of not really accessible to consciousness. But well, to be able to have machines that interpret language correctly, we sort of need to understand the structure of these sentences. Unless we know what words are arguments and modifiers of other words, we can't actually work out what sentences mean. And I'll show some examples of that as to how things go wrong immediately, because a lot of the time there are different possible interpretations you can have. "What can go wrong?" is a way of saying, ''What can't go wrong?'' Okay. So here, is a newspaper article. Uh, ''San Jose cop kills man with knife''. Um, now, this has two meanings and the two meanings, um, depend on, well, what you decide depends on what, you know, what modifies what? "The cop stabs the guy. The second meaning the sentence can have is, that's the man has a knife" what is modifying what? Um, here is another one that's just like that one. Um, scientists count whales from space. Okay. So again, this sentence has two possible structures, right? [LAUGHTER] That we have, the scientists are the subject that are counting and the whales are the object.Um, and, well, one possibility is that this is how they're doing the counting, um, so that they're counting the whales fromspace using something like a satellite. A prepositional phrase attachment ambiguity is one of the most common ambiguities in the parsing of English. In programming languages, you have an else is always construed with the closest if.um, that are starting to turn up as in the bottom example. In human languages, we have hard rules as to how you meant to interpret things that dangle afterwards, right? And so this is a crucial way in which human languages are different from programming languages. It's a crucial difference between human languages and Programming languages. if that's not what you want, um, you have to use parentheses or indentation or something like that. Human languages are this prepositional phrase can go with anything proceeding, and the hearer is assumed to be smart enough to work out the right one. And, you know, that's actually a pa- large part of why human communication is so efficient, right? Like, we can do such a good job at communicating with each other because most of the time we don't have to say very much. um, who can interpret the words that we say in the right way. So, that's where if you want to have artificial intelligence and smart computers, we then start to need to build language understanding devices. That they can just decide what would be the right thing for form space to modify. And if we have that working really well, we can then apply it back to programming languages. And you could just not put in any braces in your programming languages, and the compiler would work out what you meant. acquisition by Royal Trustco Limited of Toronto for $0.27, $27 a share at its monthly meeting. Boring sentence, but, um, what is the structure of this sentence? Well, you know, we've got a verb here, and we'veGot exactly the same subject, and for this noun,Um, object coming after it. But then what happens after that? well, here, we's got a prepositional phrase. Here, we're gonna get more complicated as we go in, because look, there's another noun here. Okay. So, um, by Royal Trustco Limited, what's that modifying? [NOISE] Right. You see acquisition, so it's not the board approved by Royaltrustco Limited. It's an acquisition by Royal trustco. Okay. Now, we went to of Toronto, and we have three choices, that could be this, this, or this. This one is a dependent of the acquisition. Okay, so, of Toronto is modifying. acquisition of Toronto? [LAUGHTER] No, I think that's a wrong answer. For $27 a share is modifying acquisition, right? [NOISE] So now, we leap right back. And then finally, we have at its monthly meeting is modifying? [Noise] Approved. Well, the approved, right. It's approved, yeah. Okay. [NOise] I drew that one the wrong way around with the arrow. Sorry, it should have been done this way. I'm getting my arrows wrong. Um, um. So that we've got this pattern of how things are modifying. to potentially consider an exponential number of possible structures because, I've got this situation where for the first prepositional phrase, there were two places that could have modified. And so, if you get into this sort of combinatorics stuff the number of analyses you get when you get multiple prepositions is the sequence called the Catalan numbers. Ah, but that's still an exponential series. And it's sort of one that turns up in a lot of studies of the human brain. of places when they're tree-like contexts. So, if any of you are doing or have done CS228, where you see, um, triangular- triangulation of, ah, probabilistic graphical models and you ask how many triangulations there are, that's sort of like making a tree over your variables. And that's, again, gives you the number of them as the Catalan series. But- so the point is, we ha- end up with a lot of ambiguities. Right? That is either that there's somebody who's a shuttle veteran and a long time NASA executive, and their name is Fred Gregory, and that they've been appointed to the board. Or, um, the other possibility is that there're a shuttle vet and a longtime NASA executive. And so, we can represent by dependencies, um,. these two different structures. Okay. That one is not very funny again. Um, that's,Um, one. That's, um. One. In English, you can use kind of just comma of sort of list intonation to effectively act as if it was an "And" or an "Or", right? So, here, um, we have again two possibilities that either we have issues and the dep- and the dependencies of issues is that there are no issues. Um, and then it's sort of like no heart or cognitive issues. So, "Heart" has a depend- has a coordinated dependency of "Issues". That's one one. modifier of "Experience" and the "Job" is also a modifier of " experience" And then we have the same kind of subject, object, um, reading on that one. Um, but unfortunately, this sentence has a different reading where you change the modification relationships. [NOISE] One more example. "Mutilated body washes up on Rio beach to be used for Olympics beach volleyball." Um, wha- what are- [LAUGHTER] what are the two ambigui- being attached, we've now got this big verb phrase we call it, right, so that when you've sort of got most of a sentence but without any subject to it, that's sort of a verb phrase to be used for Olympic beach volleyball which might be then infinitive form. Sometimes it's in part of CPO form like being used for beach volleyball. And really, those kind of verb phrases they sort of just like, um, prepositional phrases. Whenever they appear towards the right end of sentences, they can modify various things like verbs or nouns. that, um, we can have here is another noun phrase muti- mutilated body, and it's the mutilatedBody that's going to be used. Um, and so then this would be, uh, a noun phrase modifier [NOISE] of that. Okay. So, you know, this is back to the kind of boring stuff that we often work with of reading through biomedical research articles and trying to extract facts about protein-protein interactions from them or something like that. is, um, the results demonstrated that KaiC interacts rhythmically with SasA Ka- KaiA and KaiB. Um, and well, [NOISE] I turned the notification's off. [NOise] I actually mis-edited this. This should also be nmod:with. [Noise] Um, we have this KaiC that's interacting with these other proteins over there. We can kind of think of these two things as essentially,Um, patterns. So, we can sort of see this repeated pattern. can kind of think of these two things as sort of patterns and dependencies that we could look for to find examples of, um, just protein-protein interactions that appear in biomedical text. Okay. So, so that's the general idea of what we wanna do, and so the total we want to do it with is these Dependency Grammars. And so, I've sort of shown you some Dependency grammars, and I just want us to sort of motivate them a bit more formally and fully. sort of put the words in a line and that makes it. He see, let's see the whole sentence. You draw this sort of loopy arrows above them and the other way is you sort of more represent it as a tree. So, the dependence of bills and were submitted words, the dependent of submitted and you're giving this kind of tree structure. Okay. Well, in addition to the arrows commonly what we do is we put a type on each arrow which says what grammatical relations holding them between them. So,. is this the subject of the sentence? Is it the object? of the verb? Is that a, um, a conjunct and things like that? We have a system of dependency labels. Um, so, for the assignment, what we're gonna do is use universal dependencies, which I'll show you more, a little bit more in a minute. And for the arrows, you should be able to interpret things like prepositional phrases as to what they're modifying, just in terms of where the prepositions are connected and whether that's right. But, if you don't think that's fascinating, for what we's doing for this class, we're never gonna make use of these labels. All we're doing is making use of the arrows. The Dependency Grammar has an enormously long history. The famous first linguists that human beings know about his Panini who, um, wrote in the fifth century before the Common Era. And a lot of what Panini did was working out things about all of the morphology of Sanskrit that I'm not gonna touch at. Okay. So, it's a connected acyclic single, um,. rooted graph at the end of the day. Or, it could be a tree. In the later parts of the first millennium, there was a ton of work by Arabic grammarians and essentially what they used is also kind of basically a Dependency Grammar. So compared to that, you know, the idea of context-free grammars and phrase structure Grammars is incredibly incredibly new. I mean, you can basically, um, totally date it. There was this idea of this Dependency grammar. And indeed, if you look at kind of the history of humankind, most of attempts to understand the structure of human languages are essentially Dependencygrammars. guy Wells in 1947 who first proposed this idea of having these constituents and phrase structure grammars, and where it then became really famous is through the work of Chomsky. So, in modern work, uh, there's this guy Lucie Tesniere. Um, and he sort of formalized the kind of version of dependency grammar that I've been showing you. And you know it's- it's long-term being influential and influential and long term being influential. Okay. Who's head of the Chomsky hierarchy? Do people remember that 103? Yeah. computational linguistics. Some of the earliest parsing work in US Computational Linguistics was dependency grammars. There's sort of two ways of thinking about this um, that you can either think okay, I'm gonna start at the head and point to the dependent. Or you can say I'm going to start. at the dependent and say what its head is, and you find both of them. Uh, sorry. I'm drawing that wrong. Whoops, um because discussion. of the outstanding issues. So, really um, the dependent is sort of discussion. We go from heads to dependence. And usually, it's convenient to serve in addition to the sentence to sort of have a fake root node that points to the head of the whole sentence. Um, so to build a dependency pauses or to indeed build any kind of human language structure finders including kind of constituency grammar pauses, the central tool in recent work, where recent work kind of means the last 25 years has been this idea of tree banks. Universal Dependencies is actually project I've been strongly involved with. The goal of universal dependencies was is to have a uniform parallel system of dependency description. So, if you go to the Universal Dependencies website, it's not only about English. You can find Universal Dependency analyses of you know, French, or German, or Finish, or Carsac, or Indonesian, um, lots of languages. Of course, there are even more languages which there aren't universal analyses of. So if you have a- a big calling to say I'm gonna build a Swahili Universal, I'll be happy to help. Dependencies um, treebank, um, you can get in touch. So, this is the idea of treebank. You know, historically, tree banks wasn't something that people thought of immediately. This so- an idea that took quite a long time to develop, right? That um, people started thinking about grammars of languages even in modern times in the fifties, and people started building parses for languages in the 19, early 1960s. There was decades of work in the 60s, 70s, 80s, and no one had tree banks. these sentences. It's often a bit more subtle was to why that is because it sounds like pretty menial work um, building tree banks, and in some sense it is. But it turned out to be much better to have these kind of treebank supporting structures over sentences. So, it's really efficient you're capturing lots of stuff with one rule. Um, but it sort of turned out that in practice that wasn't such a good idea, and it turns out that it's better to be more subtle. Treebanks are very reusable. People who started about building a parser invented their own notation for grammar rules which got more and more complex. Once you have a treebank, it's reusable for all sorts of purposes that lots of people build parsers format. But also other people use it as well like linguists now often used tree banks to find examples of different constructions. This sort of just became necessary once we wanted to do machine learning. So that if if you if you want to do a machine learning algorithm, you can use a tree bank. we want to do machine learning, we want to have data that we can build models on. In particular, a lot of what our machine learning models exploit is how common are different structures. We want to know about the commoners and the frequency of things. Um, but then treebanks gave us another big thing which is, well, lots of sentences are ambiguous, and what we wanted to do is build models that find the right structure for sentences. If all you do is have a grammar you have no way of telling what is the right Structure for ambiguous sentences. All you can do is say hey that sentence with four prepositional phrases after it that I showed you earlier, it has 14 different parsers. Let me show you all of them. for this sentence in context. So, you should be building a machine learning model which will recover that structure, and if you don't that you're wrong. Um, there's a question of how far apart words are. Most dependencies are fairly short distance. They not all of them are. There's a questions of what's the right parse for this sentence, and what's a reasonable thing to look for in a discussion of issues. It's reasonable to have issues as dependent of discussion um, where you know, discussion of outstanding. in between. If there's a semicolon in between, there probably is an a dependency across that. Um, and the other issue is sort of how many arguments do things take? So, here we have was completed. You sort of expect that there'll be a subject before of the something was completed, and it would be wrong if there wasn't. So, there's sort of information of that sort, and we want to have our dependency parsers be able to make use of that structure. For each word we want to choose what is the dependent of. We want to do it in such a way that the dependencies form a tree. So that means it would be a bad idea if we made a cycle. So, if we sort of said, Bootstrapping, was a dependent of, um, talk, but then we had things sort of move around. And so I'm gonna cycle that's bad news, we don't want cycles, we want a Tree. And there's one. Final issue is whether we want to allow dependencies to cross or not. Most of the time, um, dependencies don't cross each other. But sometimes they do, and this example here is actually an instance for that. So, we actually have another dependency here that crosses that dependency. And that's sort of rare, that doesn't happen a ton in English, but it happens sometimes in some structures like that. You could've said, I'll give a talk on bootstrapping tomorrow, and then a [inaudible] have a projective parse, but if you want to, you can kind of delay that extra modifier. And then the parse becomes non-projective. is a transition based parser. This was a notion of parsing that, um, was mainly popularized by this guy, walk him Joakim Nivre, he is a Swedish computational linguists. Um, and what you do it's- it's sort of inspired by shift-reduce parsing. So, probably in- in our CS103 or compilers class or something, you saw a little bit of shift- Reduce Parser. And this is sort of like a shift-Reduce parser, apart from when we reduce. I wanna to do is parse the sentence "I ate fish". And yet formally what I have is I have a why I start, there are three actions I can take. So, I stop the parse, and that's the sort of instruction here. By putting route, my root for my whole sentence onto my stack, and my buffer is the whole sentence, and I haven't found any dependencies yet. Okay, and so then, the actions I could take is to shift things onto the stack or to do the equivalent of a Reduce. I build dependencies. So, starting off, um, I can't build a dependency because I only have root on the stack, so the only thing I can do is shift, so I can shift I onto the stack. And so, at this point, I'm in a position where, hey, what I'm gonna do is reductions that build structure, because look, I have I ate here and I want to be able to say that I is the subject of dependency of ate, and I will do that by doing a reduction. sentence is my buffer is empty and I just have root left on my stack because that's what I sort of said back here. So, I've parsed the sentence. So that worked well but, you know, I actually had different choices of when to pa- when to shift and when to reduce. And I just miraculously made the right choice at each point. And well, one thing you could do at this point is say, well, you could have explored every choice and, um, seen what happened and gone different parsers. But that's not what people did in the 60s, 70s and 80s. Joakim Nivre came along with the idea of a machine learning classifier. The classifier would tell the user whether to shift with left or right arc. The arrows are just three actions that the classifier can take on a given position in the parse. It's a very simple way to think about how to use machine learning in a search engine, he says. He says it could be used in a variety of ways, including in the future in the form of an online search engine. shift, left arc or right arc. Um, if we also wanted to put labels on the dependencies, and we have our different labels, um, there are then sort of 2R plus actions because she is sort of left arc subject or left arc object or something like that. But anyway, there's a set of actions and so you gonna build a classifier with machine learning somehow which will predict the right action. Joakim Nivre showed the sort of slightly surprising fact that actually you could predict the correct action to take with high accuracy. he proved, no, he showed empirically, that even doing that, you could parse sentences with high accuracy. Now if you wanna do some searching around, you can do a bit better, but it's not necessary. Um, and we're not gonna do it for our, um, assignment. But so if you're doing this just sort of run classify, predict action, run classify and predict action. We then get this wonderful result which you're meant to explain a bit honest on your assignment 3. And that's not very good if you want to parse the whole web, whereas if you have something that's linear time, that's really getting you places. Okay. So this is the conventional way in which this was done. Was, you know, we have a stack, we might have already built some structure if we hadn't working out something's dependent of something. We have a buffer of words that we don't deal with and we want to predict the next action. And well, the kind of features you wanted was so the usually some kind of conjunction or multiple things so that if the top word of the stack is good, um, and something else is true, right, that the second top word is verb. then maybe that's an indicator of do some action. So ha- had these very complex binary indicator features and you'd build- you literally have millions of these binary indicators. And you'd feed them into some big logistic regression or support vector machine or something like that and you would build parses. And these parses worked pretty well. But you sort of had these sort of very complex hand engineered binary features. Um, so in the last bit of lecture I want to show you what people have done in the, um, neural dependency parsing world. like this. And so these are the correct arcs and to evaluate our dependency parser, we're simply gonna say, uh, which arcs are correct. So here in my example, my dependency paths, I've got most of the arcs right but it got this one wrong. So I say my unlabeled attachment score is 80 percent or we can also look at the labels and then my parser wasn't very good at getting the labels rights, so I'm only getting 40 percent. And that's in our accuracy. good and the second thing on the stack is the verb has or on the top of the stack are some other words. And that part of speech has already been joined with the dependency of another part ofspeech. People hand-engineer these features. And the problems with that, was these features were very sparse. Each of these features matches very few things. Um, they match some configurations but not others so the features tend to be incomplete. And so it turned out that actually computing these features was just expensive. Compute features format. And it turned out that conventional dependency parsers spent most of their time computing features, then went into the machine learning model rather than doing the sort of shifting. And so that seemed like it left open the possibility that, well, what if we could get rid of all of this stuff and we could run a neural network directly on the stack and buffer configuration. And, you know, effectively what we found, is that that's exactly what you could do. So, here's sort of what we did. of a few stats here. So these are these same UAS and LAS. Uh, so MaltParser was Joakim Nivre's Parser that I sort of, uh, we started showing before. And they've got, um, a UAS on this data of 89.8. But everybody loved that. And the reason they loved it is it could parse at 469 sentences a second. There had been other people that have worked out different more complex ways of doing parsing with so-called graph-based dependency parsers. again but it's gotten even slower. Um, okay. So, what we were able to show is that using the idea of instead using a neural network to make the decisions of Joakim Nivre Style shift-reduce parser, we could produce something that was almost as accurate as the very best parsers available at that time. I mean, strictly we won over here and we are a fraction behind on UAS. Um,. but, you know, it was not only just as fast as Nivrse's parser, but it was actually faster than Nivirse's. even though at the end of the day, it was sort of looking at weights that went into a support vector machine. So that was kind of cool. And so the secret was we're gonna make use of distributed representations like we've already seen for words. So for each word, we're going to represent it as a word embedding. And in particular, um, we are gonna use word vectors and use them as the represent- the starting representations of words in our Parser. But well, if we're interested in distributed representations, it seem to us like maybe you should only have distributed representation of words. the sort of the top positions of the stack, the first positions of. the buffer and for each of those positions, we have a word and a part of speech and if we've already built structure as here, we kind of know about a dependency that's already been built. And so we've got a triple for each position and we're gonna convert all of those into a distributed representation, um, which we are learning. Okay. Now for- so, you know starting from- starting from the next lecture forward, we're going to use a more complex forms of neural models. Our neural network is just a very simple classifier of the kind that we are talking about last week. So based on the configuration, we create an input layer which means we're sort of taking the stuff in these boxers and turn- and looking up a vector representation for each one and concatenating them together. So that gives us in our input layer. Um, so from there, we put things through a hidden layer just like last week, we do Wx plus Wx. b and then put it through a ReLU or a non-linearity to a hidden layer. And then on top of that, we're simply gonna stick a softmax output layer. So multiplying by another matrix, adding another, um, bias term, and then that goes into the softmax which is gonna give a probability over our actions as to whether it's shift left arc or right arc, or the corresponding one with labels. And so each step of the shift-reduce parser, we's making a decision as what to do next and we're doing it by this classifier. Google developed a new way to train parsers using a tree bank. It was able to get greater accuracy and speed than Nivre's parsers at the same time. The results were published in a paper by Weiss and Andor, and people at Google said, "Well, this is pretty cool. Um, maybe we can get the numbers even better if we make our parsers even better," he says. "This was showing the fact, um, that, you know, we're outperforming these earlier parsers," he adds. our neural network, um, bigger and deeper and we spend a lot more time tuning our hyper-parameters. Um, sad but true. All of these things help when you're building neural networks. Sometimes the answer to making the results better is to make it bigger, deeper and spend more time choosing the hyper- parameters. Do humans always agree on how to build this trees and if they don't, what will be the [inaudible] or agreement of humans relative? to [inaudible] [OVERLAPPING] [NOISE] So that's a good question which I haven't addressed. Um, humans don't always agree. There are sort of two reasons they can't agree fundamentally. One is that, uh, humans, um, sort of mess up, right? Because human work is doing this aren't perfect. And the other one is they generally think that there should be different structures. So, you know, it depend- varies depending on the circumstances and so on. There's still room to do better. I mean, at the unlabeled attachment score, it's actually starting to get pretty good. Um, and so then, what's the residual rate in which, um, people can actually disagree about possible parses? I think that's sort of more around three percent. But there certainly are cases and that includes some of the prepositional phrase attachment ambiguities. Sometimes there are multiple attachments that sort of same clause although it's not really clear which one is right. Google developed these models that they gave silly names to, especially the Parsey McPa- parseFace, um, model of parsing. So that then- that's sort of pushed up the numbers even further so that they were sort of getting close to 95 percent unlabeled accuracy score from these models. And actually, this work has kind of, you know, deep learning people like to optimize. So this actually led to ah sort of a new era of sort of better parsers because so effectively this was the 90's era of parsers. neural transition based dependency parsers. We sort of have gone down that we've halve that error-error rate. And we're now down to sort of about a five percent error rate. Yeah. I'm basically out of time now but there is further work including, you know, at Stanford. It's more accurate than 95 percent, right? So we- we're still going on but I think I'd better stop here today, um, and that's neural dependency parsing. [NOISE].