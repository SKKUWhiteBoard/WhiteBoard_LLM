Micheal FEE: Today, we're going to continue with our plan for developing a powerful set of tools for analyzing the temporal structure of signals. And so this was the outline that we had for this series of three. We're looking at a periodic structure and signals. We'll be back next week with the next in the series, on the temporal structures of signals and signals and the timing of their peaks and troughs. Back to the page you came from. Click here for the next installment. lectures. Last time, we covered Fourier series, complex Fourierseries, and the Fourier transform. And we started talking about the power spectrum. And in that section, we described how you can take any function and write it as a-- or any periodic function andwrite it as. a Fourier function. And now, we are going to talk about Fourier Fourier transforms. We will start with the discrete Fouriertransform. And then we will go on to the complex Fouriers transform. a sum of sinusoidal component. So even functions we can write down as a sum of cosines. And odd functions can be written as a sums of sines. Today, we're going to talk about the convolution theorem, noise and filtering Shannon-Nyquist sampling theorem. We're also going to take a look at some of the effects of noise on the Fourier transform. We'll end the show with a look back at the last episode of our series on Fourier transforms. and spectral estimation. And next time, we're going to move on to spectrograms and an important idea of windowing and tapering, time bandwidth product, and some more advanced filtering methods. So last time, I gave you this little piece of code that allows you to compute the discrete Fourier transform. Now, let's go to the next part of the series, where we look at some of the more advanced methods for filtering and spectrographic analysis. We'll be back in a few days with the next installment. using this Matlab function FFT. And we talked about how in order to do this properly, you should first circularly shift. You have to take the time series. And actually, the FFT algorithm is expecting the first half of the data in the second half of that data vector. Don't do it! Don't try to do it. You're not going to get it right. It's just a way to get a better look at a data set. It doesn't mean you have to use the same data set every time. ask me why. But we just do a circular shift, run the FFT and then circular shift again to get the negative frequencies in the first half of the vector. And then you can plot the Fourier transform of your function. This shows an example where we took a cosine and took a negative frequency in the second half of it. It's a very simple way to plot a Fourier Transform of a function, but it can be used to get a more complex result. For example, we could have taken a positive frequency and taken a negative one in the third half of our cosine. as a function of time. At some frequency, here, 20 hertz. We compute the Fourier transform of that and plot that. So that's what this looks like. Here is a cosine at 20 herhz. And you can see if you take the fast Fouriertransform of that, you cansee if you look at it at a different frequency, you get a different result. That's what the cosine looks like at 20Hertz. And that's how it works at different frequencies. that what you see is the real part as a function of frequency. It has two peaks, one at plus 20 hertz and one at minus 20Hertz. And the imaginary part is 0. So we have two peaks. One produces a complex exponential that goes around the unit circle like this. The other peak produces a real part that goes back and forth at 20hertz. So that's what those two peaks are doing. Here is the Fourier transform of a sine wave at 20 herhz. This is phase shifted so the cosine is a symmetric function. The sine is an odd function. function, symmetric functions, even functions, are always real. The Fourier transform of even functions is always real, as is the Fourier series of the even part of the function into the real part. The same applies to the odd part of a symmetric function, as well as the odd parts of even and even parts of the same function. This is called the "Fourier series" of the odd and the even portions of a function, and it is the same for symmetric and even functions. the function into the imaginary part of the Fourier transform. The power spectrum of the sine and cosine function is just a single peak at the frequency of the sine or cosine. For the cosine, it's real. And for the  sine, It's imaginary. But the square magnitude of that Fourier transform is real. OK? And you can see why that is because the sines and cosines have a peak at plus 20 hertz. both of those is 1 at that frequency. OK, any questions about that? Feel like I didn't say that quite as clearly as I could have? OK. Let's take a look at another function that we've been talking about, a square wave. In this case, the square wave has a frequency of 1.1. The square wave is a wave with a frequency that is equal to the square of the frequency of the wave that is at the center of it. The frequency of this wave is 1.0. case, you can see that the square wave is symmetric or even. And the Fourier transform of that is all real. You can see the imaginary part in red is 0 everywhere. OK? If you look at the power spectrum of thesquare wave, youCan see again it's got multiple peaks at regular intervals. The peaks are in the real part of the Fouriers transform. YouCan see the real parts in red are equal to the frequency of this square wave. One thing that you can do with this is to make it symmetric and even. often find when you look at power spectra of functions is that some of the peaks are very low amplitude or very low power. So one of the things that we often do when we're plotting power Spectra is to plot not power here but log power. OK? And so that's what we're doing here. We're plotting the log power of the power spectrum of a function. And so we're looking at a function that has a low power peak. And we're trying to figure out what that peak is. we plot power in log base 10. A difference of an order of magnitude in two peaks corresponds to a unit called a bel, b-e-l. So you can see this peak here is about 1 bel lower than the next peak. So 1 bel is a factor of 10 difference in power. This peak is 1 bel higher than the previous peak. This is a difference of 1 bel less than the second peak. It is a 1 bel difference of 2 power differences. This means that this peak has a power of 1.2. that peak, right? And more commonly used unit is called decibels, which are 10 decibles per bel. So decibel are given by 10 times the log base 10 of the power of the square magnitude of the Fourier transform. Does that make sense? Yes. So [INAUDIBLE] square magnitude [INAudIBLE]square magnitude. Do you know what a decibel is? If you do, please send it to us. We'd like to hear it. so just like [INAUDIBLE] MICHALE FEE: No, so you take this square magnitude because-- OK, remember, last time we talked about power. So if you have an electrical signal, the power in the signal would be voltage squared over resistance. Power, when you refer to signals, is often kind of like a voltage squared. So just like that, you take the square magnitude of the electrical signal and multiply it by the square of the resistance. That's how you get power. used synonymously with variance. And variance is also goes as the square of the signal. Now, because the Fourier transform is a complex number, what we do is we don't just square it, but we take the squared magnitude. So we're measuring the distance from the origin in the complex in the Complex Fourier Transform. And we're also measuring the Distance from the Origin in thecomplex Fourier Transient (Fourier Transform), which is also known as the Square of the Signal. plane. All right, any questions about this and what the meaning of decibels is? So if a signal had 10 times as much. amplitude, the power would be how much larger? If you had 10. times as many amplitude, how much increased power would there be? 100. OK? Good question. OK, good question. If a signal has 10 times. as much amplitude,  the power would be how much larger? How much larger would it be if it had 100 times the amplitude? A factor of 10 in signal is a factor of 100 in power. Log base 10 of 100 is 2.times. How many decibels? AUDIENCE: 22? MICHALE FEE: No, it's 10 decibel per bel. Deci just means a tenth of, right? Remember those units? So a factor. of. 10 in signals is a Factor of 100. in power, so a factor-of-10 in power is a. factor of. 100 in signal. which is 2 bels, which is 20 decibels. All right, now I just want to show you one important thing about Fourier transforms. There's an interesting property about scaling in time and frequency. So if you have a signal like this that's periodic at about-- I don't know, it's about 2.5 bels or 2.6 decibells. That's about a signal that's about to go off at about 2,000 times the speed of light. looks like-- OK, there it is-- about 5 hertz. If you look at the Fourier transform of that, you can see a series of peaks, because it's a periodic signal. Now, if you take that same function and you make it go faster-- so now, it's at about 10 hertz, instead of 5Hertz, youcan see that the Fouriers transform is exactly the same. It's just scaled out. So the faster something moves in time, the more stretched out the frequencies are. Does that makes sense? show you the Fourier transform of it, you can immediately write down the Fouriers transform of any scaled version of that function. If this goes faster, if this same function but at a higher frequency, you could write down a Fourier transforms of that same function just by taking this Fouriertransform and taking the same function and writing it down as the transform of this function. You can do this with any function you want to write down as long as it has a high frequency. stretching it out by that same factor. OK? All right, so that was just a brief review of what we covered last time. And here's what we're going to cover today in a little more detail. So we're Going to talk some more about the idea of Fourier transform pairs. We're Going To Talk About Fourier Transform pairs is on tonight at 10 p.m. ET on Science Friday and Science Saturday on Science Sunday at 10 a.m., Science Monday at 11 a. m. ET. These are functions where you have a function. You take the Fourier transform of it. You get a different function. If you take the transform of that, you go back to the original function. OK, so there are pairs of functions that are essentially for transforms of each other. That's the basic idea of Fourier transforms. And that's what we're trying to do here. We're not trying to get at the exact same thing. We just want to get the general idea of how to do it. A square wave like this has a Fourier transform that's this funny function a set of peaks. If you take the four transform of that, you would get this square wave. OK? We're going to talk about the convolution theorem, which is the same thing as the Fourier Transform. We'll talk about it in more detail later on in the show. Back to the page you came from. Follow us on Twitter @CNNOpinion and @jennifer_cnn. a really cool theorem. Convolution in the time domain looks like multiplication in the frequency domain. And it allows you to take a set of Fourier transform pairs that you know, that we'll learn, and figure out what we'll do with them. It's a really cool idea. It lets you do something with Fourier transforms that you don't even know how to do. It is a really neat idea. And we're going to use it a lot in the next few years. Fourier transform is of any function that's either a product of those or a convolution of those kind of base functions. It's a very powerful theorem. We're going to talk about the Fourier transform a Gaussian noise and this power spectrum ofGaussian noise. We'll talk about how to use it to make a Fourier Transform of a Gaussian noise. And we'll also look at how to turn it into a power spectrum for a Gausian noise, too. do spectral estimation. And we'll end up on the Shannon-Nyquist theorem and zero padding. And there may be, if there's time at the end, I'll talk about a little trick for removing the line noise from signals. OK, so let's start with Fourier transform pairs. So one of the most important Fourier transforms pairs is the Fourier Transform Pair (FTP) The FTP is a function of the sign of a FourierTransform (FPR) function. The Fourier transform of a square pulse is a function called the sinc function. It looks a little bit messy, but it's basically a sine wave that is weighted so that it's big. The Fourier Transform of a Square Pulse is a Function of Time. It's 0 everywhere, 0 everywhere. But it's 1 if the time is within the interval plus delta t over 2 to minus delta tover 2. And let's just take the case where delta t is 100 milliseconds. in the middle and it decreases as you move away from 0. So this is frequency along this axis. It's just imagine that you have a sine wave that gets smaller as you go away from the origin by an amount 1 over f. That's frequency of the sine waves in the middle. It is the same as the frequency of a wave in the center of a circle. It can be seen in the image below. The frequency of this wave is 1/f. It decreases as 1/ f. all it is. The Fourier transform of a square wave, of this square wave of with 100 milliseconds, is a sinc function. So once you know that, you can take a function of time. Now, really important concept here, remember, we talked about how you can taking a function. of time, remember to take that into account. You know what the Fourier transforms of asquare wave is? That's what it's called. It's a Fourier Transform of aSquare Wave. is of a square pulse that's longer. If you take this pulse and you make it narrower in time, then the Fourier transform just stretches out. So you can see that here if it's 10 milliseconds, it's the same sinc function, but it's just stretched out in the frequency domain. So if we take that pulse and we make it narrowing, 25 milliseconds, then you cansee that the sincfunction is the same, just in a different frequency domain, which is the time domain. the width of this is 12 hertz. If this is 4 times narrower, then this width will be 4 times wider. What happens if we have a pulse here that is 500 milliseconds longer? So 5 times longer, that's the full width. The full width at half max of that peak is 12Hertz. That's the width of the fullwidth of the peak. That is the width that we are talking about. We're talking about a full width of 12hertz at the half max. That would be a width of 4 times larger. what's the width of the sinc function here going to be? It'll be five times narrower than this, so a little over 2 hertz. OK? Does that makes sense. So you should remember this Fourier transform pair, a square pulse and a sincfunction. And there's a very important concept that you need to understand. It's called the 'sinc function' and it's a function of a Fourier Transform Pair. It is a function that is five times smaller than this. called the time bandwidth product. You can see that as you make the width in time narrower, the bandwidth in frequency gets bigger. And as you making the pulse in time longer, thewidth gets smaller. And it turns out that the product of thewidth in time and the frequency is called the bandwidth of time. The bandwidth product is the result of the width and frequency of the time pulse, which is the width of the pulse and thefrequency of the frequency. For more information, go to: http://www.cnn.com/2013/01/29/technology/technology-news/how-to-make-a-broadband-pulse-in-time. width in frequency is just a constant. And for this square pulse sinc function, that constant is 1.2. So there's a limit. If you make the square pulse smaller, the sincfunction gets broader. All right, let's look at a different Fourier transform pair. It turns out that the Fourier. width in frequency of a square pulse is a constant, so there's no limit to how wide it can get. It gets broader if you make it smaller and smaller. transform of a Gaussian is just a Gaussia. If I make that Gaussian pulses in time narrower, then the Gaussian in frequency gets wider. And inversely, if I make the pulses wider, the frequency gets narrower. So, here, this Gaussian pulse is 50 milliseconds long. The Fourier transform of that is a Gaussian pulse that's 20 hertz wide. That's a Gausman pulse that is 20 herz wide. And that's a wider Gaussman pulse than a normal Gaussian. make the pulse in time wider, then the Gaussian in frequency space gets narrower. Yes. AUDIENCE: I just have a question [INAUDIBLE] MICHALE FEE: Yes. So I'm trying-- it's a little bit unclear here, but I'm measuring these widths at the half height. OK, and then I'm going to do the same thing with the length of the pulse. Yes, that's what I'm doing. So that's the way to do it. so you can see that for a Gaussian, this time bandwidth product, delta f times delta t is just 1. So there's a time bandwidth Product of 1. Who here has heard of the Heisenberg uncertainty principle? Yeah. This is just the Heiskenberg principle. It's not quantum mechanics, it's just the uncertainty principle. And it works for all Gaussian distributions. It works for any Gaussian distribution. It doesn't work for every Gaussian. It just works for this one. uncertainty principle. This is where the Heisenberg uncertainty principle comes from, because wave functions are just-- you can think of wave functions as just functions in time. So the spatial localization of an object is some-- the wave function is just some position of space. And the momentum of a wave is just the movement of the wave in that position. And that's what we're trying to do with the wave equation in this case. It's a wave function in time, but it's also in space. that particle can be computed as the Fourier transform of the wave function. And so if the particle is more lo-- [AUDIO OUT] in space, then if you compute the Fouriers transform of that wave function, it's more dispersed in momentum. OK, so the uncertainty in momentum is larger. So if the particles is more scattered in momentum, then the uncertainty is larger, too. So that's what we're trying to do with the particle, which is a wave. this concept of time bandwidth product in the physical world is what gives us the Heisenberg uncertainty principle. It's very cool. Actually, before I go on, you can see that in this case, the Fourier transform of this function is the same function. Does anyone remember what a [AUDIO OUT] is? If you do, please email it to jennifer.smith@mailonline.co.uk. If you don't want to email it, please send it to iReport.com. function is that the Fourier transform of it is another version of that same function? We talked about it last time. A train of delta functions has a Fourier transforms that's just a train ofDelta functions. And 1 over 1 is the same as a Pulse train. That's what we're trying to get at with Fourier Transform. We're going to try to get to the heart of the matter with this next segment. We'll let you know what we find out after the break. spacing in the time domain is just equal to 1 over the spacing in the frequency domain. So that's another Fourier transform pair that you should remember. All right? OK, convolution theorem. Imagine that we have three functions of time, y of t, like this one, t of t. We have a convolution of the time function with the frequency function. We call this the y of time convolution. We then have a y of frequency. We can see that the convolution is the same in both cases. could calculate the Fourier transform of that. And that's capital Y of omega. And then we have some other function, x of t, And its Fourier transforms, X of omega, and another function g of tau and its Fouriers transform, capital G of Omega. So remember, we can write down all of these functions in terms of omega and tau. And we can also write them down for other functions, such as tau, t, and t, for example. the convolution of this time series, x with this kernel g as follows. So y of t equals this integral d tau g of tau x of t minus tau, integrating over all tau. So in this case, we're defining y as the convolved of g with x. So that's the convolution. The convolution is the sum of the convolutions of g and x. That's the kernel of the time series. And that's what we're looking at here. a convolution. The convolution theorem tells us that the Fourier transform of y is just the product of the Fouriers transform of g and x. So that, you should remember. All right? I'm going to walk you through how you derive that. I don't expect you to be able to derive it. But the derivation is kind of cute, and I enjoyed it. So I thought I'd show you how it goes. So here's the definition of the convolved. here's how you calculate Fourier transform of something. Capital Y of omega is just the integral over all time dt y of t e to the minus i omega t. So we're going to substitute this into here. Now, you can see that-- OK, so the first [AUDIO OUT] is [Fourier transform] [F Fourier Transform] is a Fourier transforms of something, and it's an integral over time. It's the first part of the Fourier transformation of a thing. The second part is the second part of it. actually reverse the order of integration. We're going to integrate over t first rather than tau. Then what we are going to do is we can move the g outside the integral over t, because it's just a function of t Tau. So we pull that out. So now, we have an integration over t rather than over tau, and we can integrate over g instead of over t. We can now have an integral over g, which we can then integrate over the g integral, which is over the t integral. integral dt x of t minus tau e to the minus i omega t. So you can say if you multiply those two things together, you just get back to that. And what do you think that is? What is that? What would it be if there were no tau there? If you just cross that, it's a function of tau, we can pull that out of the integral. And now we have integral dt X of tminus tau. Fourier transform is just the Fourier transform of x. We're integrating from minus infinity to infinity. Shifting the inside of it by a small amount tau isn't going to do anything. The Fourier transforms of y is just y. And what is this? Fouriertransform of g. Out and that, what would that be? Anybody know? What that's? Audience: The Fouriers transform. Out. And that would be g.out and that. Fourier transform of g times the Fouriertransform of x. All right, that's pretty cool. Kind of cute, but it's also really powerful. OK, so let me show you what you can do with that. First, let me just point out one thing that there is a convolution theorem that that is true for all convolutional transformations. That is, all convolutions are equal to a Fourier transform that is equal to the convolution of g and x. relates convolution in the time domain to multiplication in the frequency domain. You can do exactly the same derivation and show that convolution of two functions in thefrequency domain is the same as multiplication in time domain. So that's the convolution theorem. So let's see why that's so. Let's see how it works and why it's so important for us to be able to use it in the future. Back to Mail Online home. back to the page you came from. powerful. So I just showed you the Fourier transform of a Gaussian. So what we're going to do is we're gonna calculate transform of Gaussian times a sine wave. So if you take a. Gaussian, some window centered around 0 in time-- this is a function of time. This is a result of taking a Gaussian and some time-varying sine waves. We're gonna take that Gaussian and a time-Varying Sine Wave. We'll see what happens. now right? So there's a little Gaussian pulse in time. We're going to multiply that by this sine wave. And when you multiply those together, you get this little pulse of sine. OK? [WHistles] Sorry, constant frequency. Boy, that's harder to do than I thought. [WHISTLES] OK, just a little. OK,just a little, just one more time, okay? Just a little more, OK? Just one more. pulse of sine wave. So what's the forehead transform of that? Well, we don't know, right? We didn't calculate it. But you can actually just figure it out very simply, because you know the Fourier transform of a Gaussian. What is that? A Gaussian, of course. You know. the Fouriers transform of. aGaussian. The Fourier transforms of a. Gaussian is the same as that of a sine waves. a sine wave. What is that? Yeah. Hold it up for me. What does it look like? Yes, sine waves, thank you, like this. OK. And so what do you know-- what can you tell me right away what the Fourier transform of this is? You take the Fouriers transform and you make it into a sineWave. That's what the sine Wave looks like. And that's what a Fourier wave looks like, too. of that and can involve it with the Fourier transform of that. So let's do that. If this is 200 milliseconds wide, then how wide is this? AUDIENCE: [INAUDIBLE] MICHALE FEE: It's 1 over 200 milliseconds, which is what? 5 hertz, right? Right? So there's the Fouriers transform of this Gaussian. And we can see how wide it is. And then we can find out how much time it takes to do it. The Fourier transform of the sine wave is 5.1 over 0.2. I think I made it a cosine instead of a sine. Sorry, that's why I was going like this, and you weregoing like this. So I madeIt a Cosine function. The Fourier Transform of the Sine Wave is 5, over 0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 30. cosine function has two peaks. This is 20 hertz. So one peak at 20, one at minus 20. Fourier transform of this is just the convolution of this with that. You take this Gaussian and you slide it over those two peak. You essentially smooth this withThat. It's called a Gaussian convolution. It is a smooth convolution that smooths the two peaks of the cosine function. It’s called a Fourier transforms of this. It was created by taking this and smoothing it. The product of a Gaussian and a sine wave, or cosine, is the Fourier transform of that. So you didn't actually have to stick this into Matlab to compute that. You can just know in your head that that's the product of that and the Gaussian. Does that makes sense? Cool. Back to Mail Online home. Back into the page you came from. Back To the pageyou came from, back to the pageYou came from,. Back to thepage you came From. the convolution of a Gaussian with these two peaks. And there are many, many examples of interesting and useful functions in the time domain that you can intuitively understand what their Fourier transform is just by having this idea. It's very powerful. Here's another example. We're going to calculate the Fouriertransform of aGaussian with two peaks, and we'll use the same idea to do the same thing for this Gaussian. We'll use this idea to calculate a Fourier Transform of a gaussian with the two peaks of the Gaussian, and it will be the same. Fourier transform of this square windowed cosine function. So it's a product of the square pulse with this cosine to give this. What is that? So what is the Fourier transforms of that? AUDIENCE: The sinc function. MICHALE FEE: It's the sync. It's a sync function. What's that? What is it? What's it called? The Fourier transform. It is the product of this and this. It gives this. function. It's that kind of wiggly, peaky thing. And so the Fourier transform of this is just like two peaks-- yeah-- with wiggled stuff around them. That's exactly right. All right, any questions about that? OK. Allright, change of phrase. OK. Change of change ofphrase. OK, changes ofphrase, OK. OK,. Change ofchange offunction.function. function, function.function, function,function,function.Function.function,. function. function,. function,.function. topic, let's talk about Gaussian noise. The Fourier transform of noise and the power spectrum of noise. And we're going to eventually bring all these things back together. OK? All right, so what is Gaussian Noise? So first of all, Gaussian. noise is a signal in which the value at. The value at is the value of the noise's Fourier. transform, or power spectrum, of the signal. The power spectrum is the Fourier Transform of noise, or Fourier transforms of noise with a range of values. each time is randomly sampled from a Gaussian distribution. So you can do that in Matlab. That's a very simple function. This returns a vector of length N, sampled from the normal distribution, with variance 1. So here's what that sounds like. [STATIC NOISE] Sounds noisy, right? OK, I just want to show you what it looks like when you run the function in a different way. It's very simple. It just looks like a vector with a different shape. wanted to show you what the autocorrelation function of this looks like, which I think we saw before. So if you look at the distribution of all the samples, it just gives you a distribution that it has the shape of a Gaussian. And the standard deviation of that Gaussian is the same as that of a standard deviation in the whole study. So that's what we're looking at here. We're going to use that for the rest of the study, and then we'll see what the results are. Is there any relation between the value of this function at any time t and another time t plus 1? So they're completely uncorrelated with each other. There's zero correlations between neighboring samples. What about the correlation of the signal at time t with the signalAt time t? Well, that's perfectly correlated, obviously. So we can plot the correlation between the two values.is 1. The value of time t is uncor related with the value with time t + 1. correlation of this function with itself at different time lags. And if we do that, you get a 1 at 0 lag and 0 at any other lag. So that's the autocorrelation function of Gaussian noise. All right, now, the Fourier transform of Gaussian noise is called the "Fourier transform" of the noise. The Fourier Transform of Gausian Noise is known as the " Fouriertransform of the Noise" It's a function of noise and noise intensity, and it can be used to measure noise levels. noise is just Gaussian noise. It's another kind of interesting Fourier transform pair. And it's a Gaussian distribution in both the real and the imaginary part. So you can see that the blue and red-- the red here is the imaginary parts-- are both just Gaussian noise. And the blue is the real part, and the red is the imagined part, so it's just Gausian noise in both parts of the Fourier Transform. The real part is called the real noise. The imaginary part is known as the imaginary noise. Gaussian noise. All right, now what is the power spectrum? So we can take this thing-- and, remember, when we plot the power [AUDIO OUT] just plot the square magnitude of just the positive frequencies. Why is that again? Why do we only have to plot thesquare magnitude? OK, so we can do that. We can do it. OK, now we have a power spectrum. OK? All right. Now we can go to the next step. of the positive frequencies? AUDIENCE: [INAUDIBLE] Gaussian, so they're all [INAudIBLE] MICHALE FEE: Yep. So it turns out that the Fourier transform in a positive frequency is just the complex conjugate of the Fouriers transform in the negative frequency. So the square magnitude is identical. The power spectrum on positive frequencies is the same as the power spectrum of negative frequencies. The square magnitude of positive frequencies in negative frequencies is identical to negative frequencies in positive frequencies. this side is equal to the power spectrum on that side. So we just plot half of it. The power spectrum of noise is very noisy. We're going to come back, and I'm going to show you what that power spectrum looks like. We'll be back on Monday at 9 p.m. ET. For confidential support call the Samaritans in the UK on 08457 90 90 90, visit a local Samaritans branch or see www.samaritans.org. In the U.S. call the National Suicide Prevention Line on 1-800-273-8255. you that on average, if you take many different signals, many copies of this, and calculate the power spectrum and average them all altogether, it's going to be flat. But for any given piece of noisy signal, the power Spectrum is very noisy. Any questions about that? OK. All right, all right, we'll get to the answers to those questions in a minute. Back to Mail Online home. back to the page you came from. The power spectrum is flat for most signals, but noisy for noisy signals. so now let's turn to spectral estimation. How do we estimate the spectrum of a signal? So let's say you have a signal, S of t. And you have  short measurements of that signal. You have some signal from the brain, like you record some. So you have some spectrum of the brain. You can then use that spectrum to get a spectrum of that brain signal. And so on and so on, until you get to the point of a spectrum that you can use to estimate a signal. local field potential or some ECG or something like that. And you want to find the spectrum of that. Let's say you're interested in measuring the theta rhythm or some alpha rhythm. Or some other periodic signal in the brain. What you could do is you could have a bunch of tests that look at the same thing at different times of the day. And then you can see how the brain responds to each of those different tests. It's a very powerful tool, and it can be used for a lot of things. of independent measurements of that signal. Let's in this case call them four trials, a bunch of trials. What you can do is calculate the power spectrum, just like [AUDIO OUT] for each of those signals. So this is a little sample y of t, y1 of t,. y2 of t and y3 of t. For each of these signals, we can get a power spectrum of y1, y2, y3 and y4. For example, we could get a sample of y3, y4, y5 and y6. t. You can calculate the square of magnitude of the Fourier transform of each one of those samples. And you can estimate the spectrum of those signals by just averaging together those separate, independent estimates. Does that make sense? That's the simplest form of spectral estimation. It's like if you want to estimate the average height of a population of people. You just average together all of your different samples. That's what we did here. We have a little bit of signal. We Fourier transforms it, take the square magnitude. And now, you average together your different signals. take a bunch of different measurements. You randomly sample. And you average them together. OK? Now, you can apply that same principle to long signals. What you do is you just take that signal and you break it. That's all we're doing here. We're just taking a signal and we're breaking it. And we're taking a sample of that signal. And that sample is taken from a long signal. That is what we're trying to do here. It's just a simple experiment. into short pieces. And you compute the power spectrum in each one of those windows. And again, you average them together. Now, extracting that little piece of signal from this longer signal is essentially the same as multiplying that long signal by a square pulse. 0 everywhere, but 1 in for each long signal. And that's essentially what we're doing here. We're taking the long signal and dividing it by a short signal to get the short signal. That's what we do here. here, 1 here. 0 everywhere else. So that process of taking a long signal and extracting out one piece of it has a name. It's called windowing. Sort of like you're looking at a scene through this window, and that's all you can see. OK, so one way to do it is to take a signal and extract out one part of it. That's what we call windowing, and it's the process of extracting a piece of information from a signal. estimate the spectrum of this signal is to take the signal in this window, compute the FFT of that, take this power spectrum. And then apply this window to the next piece and compute the spectrum and average them all altogether. What's the average spectrum? That's what we're trying to figure out here. The average spectrum is the average of the power spectrum and the signal spectrum. That's the spectrum we're looking at here. And we're going to use that spectrum to get a better idea of what's going on. problem with that? Why might that be a bad idea? Yeah. That's a very good example. But there's sort of a general principle that we just learned that you can apply to this problem. What happens to the Fourier transform of the signal? That's what we're trying to figure out. We're going to try to find a way to get it to work. We'll let you know what we come up with in the next few minutes. Back to Mail Online home. back to the page you came from. when we multiply it by this square pulse? AUDIENCE: Convolving. MICHALE FEE: We're convolving the spectrum of the signal with a sinc function. And the sincfunction is really ugly, right? It's got lots of wiggles. And so it turns out this process of windowing a piece of data with a square pulse is very, very useful. It's a very simple way to look at a signal and get a sense of what's going on. this square pulse actually does really horrible things to our spectral estimate. And we're going to spend a lot of time in the next lecture addressing how you solve that problem in a principled way and make a good estimate of the signal by breaking it up into little pieces. It's a very difficult problem to solve, but it can be solved if you have a principled approach to it. And that's what we'll be talking about in our next lecture on how to solve the problem. We'll also be looking at how to break the signal down into pieces. But instead of just taking a square window we do something called tapering. So instead of multiplying this signal by square pulses, we sample the signal by applying it by little things that look like little smooth functions, like maybe a Gaussian, or other functions that we'll talk about later in the show. We're going to talk about how this works in the next section of the show, which will focus on the Fourier transform. We'll also talk about some of the other techniques used in Fourier transforms. an even better job. OK, so that process is called tapering, multiplying your data by a little [AUDIO OUT] paper that's smooth, unlike a square window. Computing spectral estimates from each one of those windowed and tapered pieces of data gives you a very good estimate of how much light there is in a room. OK? All right. So that's called computing spectral estimates. That's called compute spectral estimates, and that's what we're going to do here. the spectra. And we're going to come back to that, how to really do that right, on Thursday. OK? All right, let me point out why this method of spectral estimation is very powerful. So, remember, we talked about how you can see-- remember, if you have a high-quality image of the spectrum, you can get a very good idea of what you're looking at. And so, that's what we're trying to do in this study. We're looking for a way to get a better idea of how the spectrum works. a noisy signal that has a little bit of underlying sine wave in it, we talked about in class, if you take the autocorrelation of that function, you get a delta function and then some little wiggles. So there are ways of pulling periodic signals, periodic structure out of noisy. signals, out of the noise, into a periodic structure. That's what we're trying to do here. We're not trying to create a new type of signal. We just want to get rid of the noisy part of it. signals. But it turns out that this method of spectral estimation [AUDIO OUT] did the most powerful way to do it. I'm just going to show you one example. This blue function here is noise, plus a little bit of a sine wave at, I think it's 10 hertz. OK, OK, that's a good way to look at it. That's a pretty good way of looking at it, too. It's a very powerful method. It can be used to get a lot of information. yeah. Anyway, I didn't write down the frequency. But the blue function here is noise plus the red function. So you can see the redfunction is small. And it's buried in the noise, so that you can't see it. But when you do this process of spectral estimation that that can be seen. It's a very, very small function, but it's very important. It tells you how loud the noise is, and how long it is. And so on and so on. we're learning about, you can see that that signal buried in that noise is now very easily visible. So using these methods you can pull tiny signals out of noise at a very bad signal to noise ratio, where the signal is really buried in the noise. So it's a very good way to learn about the signals that we're seeing in the environment. It's also a good way of learning more about how to detect signals in a noisy environment, such as in a car or a house. very powerful method. And we're going to spend more time talking about how to do that properly. All right, so let me spend a little bit more time talk about the power spectrum of noise, so that we have a better sense of what that looks like. So remember, I remember, remember, that noise is a very powerful method, and that it can be used in a variety of ways. So let's talk about that a bit more. Back to the page you came from. The power spectrum, really, you should think about it properly as a power spectral density. There is a certain amount of power at different frequencies in this signal. In order to estimate what the spectrum of noise looks like, you have to take many examples of that and average them together. And when you do that, what you find is that the power spectrum ofnoise is a constant. It's flat. And for Gaussian noise, that means it's extremely noisy. And that's what we're looking at here. power spectral density is flat. It's constant as a function of frequency. And the units here have units of variance per unit frequency-- variance per frequency. OK? Or if it were an electrical signal going through a resistor, it would be power per unitfrequency. So you can see that here the value here is 0.002. The bandwidth of this signal is 500 hertz. And we started with a Gaussian noise that has variance 1, that when we calculate the power spectrum of that we can correctly read out how much variance there is per unit Frequency in the signal. So you know formally what it is that you're looking at when you look at a spectral estimate of a noisy signal. All right, let's talk about filtering in the frequency domain. So remember, we learned how to smooth a signal. How to filter a signal, either high pass or low pass. How do you filter a high-pass or low-pass signal? We're going to talk about that in a minute. We'll talk about how to filter in thefrequency domain. low pass, by convolving a signal with a kernel. So when you convolve, that's the kernel for a low pass. And for a high pass, anybody remember what that looks like? AUDIENCE: [INAUDIBLE] MICHALE FEE: Yep. So you remember that the kernels for aLowPass was something like this. And aHighPass was a highPass. And that's what you do for a HighPass. You convolve a signal to a kernel, and the kernel is the signal that's convolved. The kernel for a high-pass filter is a delta function that reproduces the function. And then you subtract off a function. So that's the kernels for a low-pass and high- Pass filters. The kernel for the low-Pass filter is the delta function, and the high-Pass Filter is the Delta function, which is the function that replaces the function with a new function. That's what we're looking at in this example. We're going to remove the Delta Function. We'll put in a new Delta function. low-pass filtered version of the signal. OK? So that's the kernel for a high pass. OK, so this was how you filter a signal by convolving your signal with a function, with a linear kernel. We're going to talk now about how you do filtering in the frequency domain. So that was how we filtered a signal in the low-pass domain. And now we'll talk about how we filter in the high-pass area of the spectrum. We'll show you how to do that. if filtering in the time domain is convolving your [AUDIO OUT] with a function, what is filtering in. the frequency domain going to be? MICHALE FEE: It's going to. be multiplying the Fourier transform of your signal times what? AUDIENCE: The Fouriertransform [INAUDIBLE] MICHale Fee: The frequency domain. It's a function of the time and frequency domain, and it's a way to get a more efficient filter. Fourier transform of things like that. All right, so let's do that. So this is what we just talked about. This was actually a neural signal that has spikes up here and local field potentials down here. And we can extract the localField potentials from the neural signal. We introduced the idea before. This is actually an idea that we can use to transform neural signals into Fourier transforms. We can use Fourier transform to transform the neural signals from neural signals to Fourier Transform. by smoothing this [AUDIO OUT] by low pass filtering it, by convolving it with this kernel here. So this is what we just talked about. And you can see that what this does to the power spectrum is just what you would expect. The power spectrum of the filtered signal is just the power Spectrum of your original signal times the powerSpectrum of the kernel. So if filtering in the time domain is convolving your data with a signal, then filtered in the frequency domain is multiplying the Fourier transform. the kernel. All right, so here's an example. So in blue is the original Gaussian noise. In green is the kernel that I'm smoothing it by, filtering it by. Convolving the blue with the green gives you the red signal. What kind of filter is that called again? High pass. High pass is a filter that filters the noise by the kernel. The kernel is the filter that smoothes the noise and filters it by the filter. The filter is called a high pass filter. or low pass? AUDIENCE: Low pass. All right, so let me play you what those sound like. So here's the original Gaussian noise. [STATIC] Good. AndHere's the low pass Gaussian Noise. [LOWER STATIC] It got rid of about the high frequency parts of the noise. It got. rid of the high frequencies of the Gaussian. noise. The low pass noise is much quieter than the original. It's a lot quieter. It gets rid of a lot of noise. noise. OK, so here's the power spectrum of the original signal in blue. We're going to multiply that by the magnitude squared Fourier transform of this. What do you think that looks like? So this is a noise.noise image in blue and a filtered noise image in red. In order to get the power Spectrum of the filtered signal in red, we're going To multiply that and this to get a power spectrum in red and this is the filtered power spectrum. In blue is the original power spectrum, in red is the filter power spectrum and in red the filteredpower spectrum. little Gaussian filter in time. What is the Fourier transform of that going to look like? Audience: [INAUDIBLE] MICHALE FEE: The Fourier transforms of a Gaussian is a Gaussia. So the power spectrum of that signal is going to just be aGaussian. Now, how would I plot it? It's a little Gaussian. It's not a big deal, it's just a little bit bigger than a big Gaussian, but it's not that big. peaked where? The Fourier transform of a Gaussian is peaked at 0. We're only plotting the positive frequencies. So this, we're ignoring. And it's going to be like that, right? So that's the Fouriertransform, squared magnitude Fourier transforms. So it was going to look like this. It was centered at 0, and it's like that now. It's aGaussian here centered at 1, and this is centered at 2, and so on. It looks like this now. of that Gaussian. And now if we multiply this power spectrum times that power spectrum, we get the power spectrum of our filtered signal. Does that makes sense? So convolving our original blue signal with this green Gaussian kernel smooths the signal. It gets rid of the blue. It's just another Gaussian, and it's just like a normal Gaussian in the same way. So it's like a regular Gaussian with a green kernel. And it smooths our signal. high frequencies. In the frequency domain, that's like multiplying the spectrum of the blue signal by a function that's 0 at high frequencies and 1 at low frequencies. Does that makes sense? So filtering in the frequency Domain means multiplying the power spectrum of a signal by the frequency of the signal it is being filtered against. It's a very complex process, but it's possible to do it in a way that makes the difference between high and low frequencies seem obvious to the human eye. For more information on how to do this, go to: http://www.cnn.com/2013/01/30/technology/filtering-in-the-frequency-Domain.html. your signal by a function that's low at high frequencies and big at low frequencies. So it passes the frequencies and suppresses the high frequencies. It's that simple. Any questions about that? Well, yes, yes-- AUDIENCE: So why is it that like-- you need to like-- of I guess when you like your signal, you have to like it more than other people do? "Well, yes," he says, "I guess when I like my signal more than others, I like it better" multiply in the frequency, could you theoretically multiply by anything and that would correspond to some other type of filter. So why don't we just like throw away high frequencies? Or something like multiply by a square in thefrequency domain and correspond to  some different filter we don't know. We don't want to know what the filter is, we just want it to work. We just want to get rid of the high frequencies. We want to have a filter that works at different frequencies. MICHALE FEE: You can do that. You can take a signal like this, Fourier transform it, multiply it by a square window to suppress high frequencies. What is that equivalent to? What would be the corresponding temporal kernel that that would correspond to? AUDIENCE: [INAUDIBLE] MICHALEFEE: Good. It It's good. It's a good thing you're with me. I'm going to take a break. I'll be back in a few minutes. would be convulsing your function with a sinc function. It turns out that's-- the reason you wouldn't normally do that is that it mixes the signal across all time. The sincfunction goes on to infinity. So the nice thing about this is when you smooth a signal with a sinc function, it smooths it to infinity as well. It's a nice way to get rid of some of the noise in the signal. It also gives you a way to see how the signal has changed over time. Gaussian, you're not adding some of the signal here that were over here. Convolving with a sinc function kind of mixes things in time. So normally you would smooth by functions that are kind of local in time, local in frequency, but not having sharp edges. Do that makes sense? Do you know how to do that? If so, please send us a picture. We'd like to hear from you. Please send a picture of your work to jennifer.smith@mailonline.co.uk. High-pass filter would pass high frequencies and suppress low frequencies. So we're going to talk about how to smooth things in frequency with signals with kernels that are optimal for that job. That's Thursday. What would a high- pass filter look like in the frequency domain? So high-pass filters would look like this: A filter with a high pass filter and a low pass filter. Do you know what a highpass filter looks like? If so, email us at jennifer.smith@cnn.com. A band pass filter would just pass a band. So it'd be 0 here. It would be big somewhere in the middle and then go to 0 at higher frequencies. Does that makes sense? Any questions? Send them to jennifer.smith@mailonline.co.uk. Back to the page you came from. Follow us on Twitter @MailOnlineLive and @JillSmith. Back To the page You came from, click here for the latest from MailOnline Live. On a log plot, a Gaussian, which is e to [AUDIO OUT] like f squared, is minus f squared. If we plot this on a log Plot in decibels, you can see that on a Log Plot this would be the case. That's why on alog plot this would look like this. About that? OK.about that? Well, that's why it looks like this on the log Plot. It looks like that on thelog Plot. look like an inverted parabola. So that's the same plot here, but plotted on a log scale. Any questions about that? I want to tell you about a cool little theorem called the Wiener-Khinchin theorem. It relates the power spectrum of a signal with the autocorrelation of a signals. So the same thing can be done with a TV signal, or a radio signal, for example. It's a very simple idea, but it's very powerful. In blue, that's our original Gaussian noise. In red is our smooth Gaussian signal. If you look at the correlation of neighboring time points in the blue signal, you can see they're completely uncorrelated with each other. But what about neighboring timepoints on the smooth signal? Are they correlated? Are you sure? Share your thoughts in the comments below. Back to Mail Online home. Back To the page you came from. Back into the now. Click here to go to the original page. with each other? If we look at for the red signal, y of i and y of 1 plus 1, what does that look like? They become correlated with each other, right? Because each value of the smooth signal is some sum over the blue points. The red signal is the sum of the red and blue points over each other. The smooth signal, the sum over blue points, is the blue signal over the black signal. The black signal is the sum over black points over the white signal. So neighboring points here will be similar to each other. That's what smoothness means. So if you look at the correlation of neighboring time points in the smooth signal, it looks like this. It has a strong correlation. It's a smooth signal. It doesn't have to be perfect. It can be very, very close to being perfect. And that's what we're looking for. We want it to be smooth. We don't want to have to worry about it being perfect all the time. We just want to be able to see it. at the autocorrelation of the original Gaussian noise, it has a delta function at zero. And the width to that autoc orrelation function tells you the time [AUDIO OUT] this signal was smoothed. Right? OK. Now, how does that relate to the smoothed function? Well, the width of the smoothing function has some width to it. And that width tells you how long it took to smoothen the signal. OK, now, how do we know when it's been smoothed? The power spectrum of a signal is just the Fourier transform of the autocorrelation. Anybody remember that? So it turns out that the power spectrum is the magnitude squared of the Fouriers transform of a delta function. So it's the power of the delta function that makes up the spectrum of the signal, not the magnitude of the transform. So that's what we're looking at, right? That's what's happening in the signal. That's why it's called a power spectrum. And that's the way it works. MICHALE FEE: So if you have a signal that you have some sense of what the power spectrum is. And how about our smoothed? Our smooth signal has a power spectrum that's a Gaussian in this case. What's the transform of aGaussian? AUDIENCE: [INAUDIBLE] MICHALE MARTIN: It's a constant. MICHale MARTINS: It is a constant, and it's the same for both smooth and smoothed. is you immediately know what the autocorrelation is. You just Fourier transform that and get the Autocor correlation. What's the width of this in time? How would I get that from here? How are the width in time and frequency related to each other for-- AUDIENCE: [INAUDIBLE] MICHALE FEE: Right. The autoc orrelation is what you get when you Fourier transforms a time series into a frequency series. That's what we're trying to do here. width of this in time is just 1 over the width of [AUDIO OUT] So you have to take the full width. Does that makes sense? OK. Wiener-Khinchin theorem, very cool. All right, let's talk about the Shannon-Nyquist theorem. Any anybody heard of the Nyquist limit? Anybody heard of this? All right. All the time in the world, we're going to be talking about the Wiener Khinchin  theorem. Anyone who's acquiring signals in the lab needs to know the Shannon-Nyquist theorem. All right, so remember that when we have discrete Fourier transforms, fast Fourier transform, our frequencies are discretized and so is our time. But the discretization is not the same as the frequency, it's the time that is discreted. It's a very important theorem, and it's very important for scientists to know. It means that we don't have to worry about how much time we have left. Any signal that has discrete components and frequencies is periodic in time. In these signals, time is sampled discretely at regular time intervals. Discretely sampled in time means that the Fourier transform is periodic. In time, a signal is periodic if its frequencies are integer multiples of each other. It's also periodic if the signal is discrete in frequency at regular intervals. And it's periodic if it's discrete in time, too, and it's also discrete infrequency at regular interval. And I've been showing you the Fourier transforms of those signals. In fact, really be thinking that those discreetly sampled signals have a Fourier transform that's actually periodic. There's another copy of that spectrum sitting up here at 1. But I've only been showingYou this little part of it. But you can see the rest of it in the next section of this article. The next section will be about the next phase of the project, which will take place over the next few months. over the sampling rate and another copy sitting up here. Remember, this is like a train of delta functions. So there are copies of this spectrum spaced every 1 over delta t. The Fourier transform of that is like another train of Delta functions. It's kind of a strange concept, but it's a good one to work with, I think, because it gives you a good idea of what's going on in the system at the time you're looking at. I think that's the most important thing to understand. So the separation between those copies of the spectra in the frequency domain are given by 1 over the sampling rate. It's a little strange. But we'll push on because I think it's going to be more clear. So what this says is that if you if you have two sets of spectra, you can get the same result. Any questions about that? It's not very clear, but we're going to push on and get to the bottom of it. We'll get back to you later. want to properly sample this signal in time, you need these [AUDIO OUT] copies of its spectrum to be far away so they don't interfere with each other. The higher the sampling rate is, the further these spectra are in time. The sampling rate needs to be greater than twice the bandwidth of the signal. That means delta t is too big. These copies of the spectrum are too close to each other and they overlap. That overlap is called aliasing-- a-l- i-a-s-i-n-g. spectrum of the signal, you see that it has-- like you'll see this part of the spectrum, but you'll also see this other part of. the spectrum kind of contaminating the top of your Fourier transform. Does that makes sense? OK. So let me just say it again. If your signal has two parts, you can't see one and the other part at the same time. Do you know what I'm talking about? If you do, email me at jennifer.smith@mailonline.com. signal has some bandwidth B that in order to sample that signal properly, your sampling rate needs to be greater than twice that bandwidth, 1, 2. Actually, there was actually recently a paper where somebody claimed to be able to get around this limit. And they were mercilessly treated in the responses to that paper. So don't make that mistake. Now, what's really cool is that if the sampling rate is greater than two times the bandwidth, something is going to happen. OK? amazing happens. You can perfectly reconstruct the signal. Now that's an amazing claim. Right? You have a [AUDIO OUT] time. All right, it's wiggling around. What this is saying is that I can sample that signal at regular intervals and completely ignore what's happening between those samples, have no knowledge of what's going on in the background. That's what I'm trying to do. I can't explain it to you, but it's amazing. of what's happening between those samples. And I can perfectly reconstruct the signal I'm sampling at every time point, even though I didn't look there. So how do you do that? Basically, your sampled signal, you're regularly sampled signal. has this spectrum-- has this Fourier transform with repeated copies of the same signal. And so I can reconstruct the signals at every point in the spectrum, even if I'm not looking at them at the same time. So that's how you get the Fourier Transform. the signal, repeated copies of the spectrum. So how would I recover the spectrum's original signal? Well, the spectrum of the original signal is just this piece right here. So all I do is in the frequency domain I take that part. I keep this, and I throw away all the rest of the signal. And that's how I get back to the original spectrum. It's a very simple process, but it takes a long time to get it back to its original state. Fourier transfer sampled signal in the frequency domain is multiplied by a square pulse that's 1 here and 0 everywhere else. When I inverse Fourier transform that I've completely recovered my original signal. What is multiplying this spectrum by the square pulse? Do that makes sense? If so, I've recovered my signal. If not, I'm not sure what I'm doing. I've got to figure out what's going on. I'm trying to get back to the original Fourier transfer. square wave in the frequency domain equivalent to in the time domain? MICHALE FEE: Yeah. So it's amazing, right? It's cool. So let me just what I was going to ask-- MICH ALEXANDER: So why do you want to do that? AUDIENCE: SoWhy do you Want to Do That? Michale Fee: So it’s amazing, so it”s amazing. It’ll be cool. It will be amazing. it is. And then we can marvel at how that could possibly be. Multiplying this spectrum by this square wave, throwing away all those other copies of the spectrum and keeping that one is multiplying by a square wave in the frequency domain, which is like doing what? AUDIENCE: Convolving. It is. It's like doing. what? It is like multiplying. by asquare wave in a frequency domain. And it is. like multiplying by this. square wave. And that's how it works. Micheal FEE: Convolving the time domain sinc-- that regular train of samples, convolving that with a sinc function. Here, we have a function that we've regularly sampled at these intervals. If we take that function, which is a bunch of delta functions here, here, and here, we get a time-varying time function. It's a time function, but it's also a time domain function, and it's convolved into time. here, here, just samples, and we can evolve that with a sinc function, we perfectly reconstruct the original signal. So that's the Nyquist-Shannon theorem. What it says is that we can perfectly. reconstruct the signal we've sampled as long as we sample it at a sampling rate that's. pretty wild. It's the same as saying that if we sample a signal at a rate of 1,000,000Hz, we can reconstruct it at that rate. greater than twice the bandwidth of the signal. OK? All right. So there's this cute trick called zero-padding, where you don't perfectly reconstruct the original signal, but basically you can interpolate. So you can extract the values of theOriginal signal times between where you actually sampled it. OK, good. So here's how to do it. You don't have to be a genius. You just have to know how to use it. And you can do it with a little bit of help. OK? And basically the trick is as follows. We take our sampled signal. We Fourier transform it. And what we do is we just add zeros. OK? So we just take positive frequencies and the negative frequencies, and we just stick a bunch of zeros on them. We pad that Fourier transforms with zeros, OK? OK? We just take the positive and negative frequencies and we stick a lot of zero on them, and that's how we do it. of zeros between and make it a longer vector. When you inverse transform, inverse Fourier transform, what you're going to have is your original samples back, plus a bunch of samples in between. And then when we inverse Fouriers this, you can see that you have a longer array of zeros and zeros. The result is a much longer array that can be used to make a larger array. For more information on how to do this, go to: http://www.cnn.com/2013/01/28/technology/how-to-make-a-long-array-of-zeros-and-zers-inverse-fourier-transforms. that interpolate, that are measures of the original signal at the times where you didn't measure it. So you can essentially increase the sampling rate of your signal after the fact. Pretty cool, right? Again, it requires that you've sampled at twice the bandwidth of theOriginal signal. Yes. Yes, it does. It does. You can do that, too, if you have the bandwidth to do it. You just have to be able to get it to work. You have to get the bandwidth. You've got to get to it. Like how do you know the bandwidth of the original signal if you don't have samples? MICHALE FEE: Good question. How might you do that? AUDIENCE: Can you like [INAUDIBLE] different sampling lengths to get [INAudIBLE] MICHALESTER: You could do that. From nearly all applications, you have a pretty good idea of what the bandwidth is. You have to be able to do that from nearly all  applications. good sense of what the frequencies are that you're interested in. And then what you do is you have to put a filter between your experiment and your computer that's doing the sampling that guarantees that it's suppressed all the frequencies above some point. OK? And that kind of thing. And that's what we're doing here. We're trying to get a signal that we think is of interest to us. And we're getting that signal through a filter that's suppressing the frequencies that we want to see. of filter is called an anti-aliasing filter. So in that case, even if your signal had higher frequency components, the anti-Aliasing filter cuts it off so that there's nothing at higher frequencies. Does that makes sense? Let me give you an example of aliasing. Let's say I had this signal and I wanted it to be at a higher frequency. So I would use a filter that would cut off the aliasing at that frequency. And the filter would cut it off at a lower frequency as well. like this. I need to do regular intervals. And I sample it here, here,Here, here,. Here, here. Here. And here. So you can see that if I have a sine wave that is close in frequency to the sampling rate, you can. see that when I sample the signal, I'm going to be more likely to get a signal that looks like this. It's a very simple technique. It doesn't take a lot of time to do it. see something at the wrong frequency. That's an example of aliasing. OK? OK, so here is an example. We have a 20 hertz cosine wave. I've sampled it at 100 hertz. So I'm, you know, 5-- so what frequency would I have to sample this in order to reconstruct the cosine? That's what I'm trying to do. I'm not trying to make you do it. I just want to know what frequency I would need to do it at. I'd have to sample at least 40 hertz. Here, I'm sampling at 100. The delta t is 10 milliseconds. So those are the blue points. And now, if I do this zero-padding trick, I Fourier transform. I do zero- padding by a factor of 4. That means if I take the take the delta t to 10 milliseconds, that's the blue point. The blue points are the red points. The red point is the blue line. The black line is the black line. Fourier transform signal and I'm now making that vector 4 times as long by filling in zeros, then I inverse Fourier transform. You can see that the red points show the interpolated values of that function after zero-padding. OK? So it can be a very useful trick, you can also use this trick to get a more accurate Fourier Transform signal, for example. If you want to try it out, click here to go to the source code of this article. sample the signal in the time domain and then add a bunch of zeros to it before you Fourier transform. And that gives you finer samples in the frequency domain. And I think that's-- so zero-padding in theTime domain gives you fine spacing in the Frequency Domain. And so that's what we do. And we use that technique to do Fourier transforms. And it's a very good way to get better results in the Fourier Transform. So that's the way we do it. I'll show you in more detail how to do this after we talk about tapering. It's very simple code actually. Matlab has built into it the ability to do zero-padding right in the FFT function. OK, let's actually just stop there. I feel like we covered a lot of the basics. I'll be back in a few minutes to talk about the next part of the show. Back to the page you came from. Click here for the next episode. stuff today. Read more at CNN.com/soulmatestories. Follow CNN Living on Facebook and Twitter. Share your photos and videos with us at iReport.com. For the latest from CNN, check out our gallery of iReport photos. Follow us on Twitter at @CNNLive and @CNNOpinion. For more from CNN iReport, visit www.dailymail.co.uk/news/features/2013/01/30/cnn-lifestyle-features-soul-matters.