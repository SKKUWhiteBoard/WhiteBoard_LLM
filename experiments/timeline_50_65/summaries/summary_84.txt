This is part 3 in our series on distributed word representations. We're going to be talking about vector comparison methods. To try to make this discussion pretty intuitive, I'm going to ground things in this running example. On the left, I have a very small word representation, and on the right, a very large word representation. We'll talk about the difference between the two later in the week, when we'll be looking at the different ways of representing words in the same way. Back to the page you came from.  vector space model. We have three words, A, B, and C. And you can imagine that we've measured two dimensions, dx and dy. You could think of them as documents if you wanted. There are two perspectives that you might take on this vector spacemodel. The first is just to think of it as a document. The second is to think about it as an object. It's not just an object, it's an object in the sense that it can have multiple dimensions. at the level of raw frequency, B and C seem to be united. They are frequent in both the x and the y dimension. Whereas A is comparatively infrequent along both those dimensions. You might just observe that if we kind of correct for the overall frequency of the individual words, then it's actually A and B that are united. Because they both have a bias in some sense for the dy dimension. By comparison, C has a bias for the x dimension, again, thinking proportionally. Both of those are perspectives that we might want to capture and different notions of distance will key into one or the other of them. One more preliminary, I think it's very intuitive to depict these vector spaces. And in only two dimensions, that's pretty easy. You can imagine that this is a vector space in which you can see the world from two different perspectives. It's a very intuitive way to depict the world in two dimensions. It is very easy to do in only 2D and 3D. the dx dimension along the x-axis. And this is the dydimension along the y- axis. Then I have placed these individual points in that plane. And then you can see graphically that B and C are pretty close together. And A is kind of lonely down here in the middle of all of them. And that's the way I like it to be. It's a little bit like a game of Frogger, but with a lot more fun. I like to play Frogger. corner, the infrequent one. We can measure the Euclidean distance between vectors u and v if they share the same dimension n by just calculating the sum of the squared element wide differences, absolute differences, and then taking the square root of that. Let's look at that in terms of this space. So here we have our vector space depicted graphically, A, B, and C. And you can see that Euclideans distance is capturing the first perspective that we took on the vector space, which unites the frequent items. B and C as against the infrequent one A. As a stepping stone toward cosine distance, which will behave quite differently, let's talk about length normalization. Given the vector u of dimension n, the L2 length of u is the sum of the squared values in that matrix. And then, the length of a vector is the product of its squared values and the square root of the vector's length. This is called the "L2 length" of a matrix, or the "length of the matrix" we take the square root. That's our normalization quantity there. And then the actual normalization of that original vector u involves taking each one of its elements and dividing it by that fixed quantity, the L2 ranks. Let's look at what happens to our little illustrative example. On the left, on the left of the image, we have the normalization function for a vector u. And on the right, we've got the normalisation function for the vector u, which is the same thing. here, I have the original count matrix. And in this column here, I've given the L2 length as a quantity. And then when we take in that quantity and divide each one of the values in that vector to get its L2 norm, you can see that we've done something. We can then see that when we divide that quantity by each of the other values in the matrix, we get the norm of that quantity. That's the norm. And we can then use that norm to get the length of the matrix. significant to the space. So they're all kind of united on the same scale here. And A and B are now close together. Whereas B and C are comparatively far apart. So that is capturing the second perspective that we took on the matrix, where A and A have something to do with each other, and B and B have something not todo with one another. And that is what we're trying to capture with this project. It's a new way of looking at the world. in common as against C. And that has come entirely from the normalization step. If we measured Euclidean distance in this space, just the length of the lines between these points, we would again be capturing that A and B are alike and B and C are comparatively different. And if we measured the distance between the points in the same way, the same thing would happen. But that's not what we're doing here. We're trying to show that A, B, and C have the same distance. Cosine kind of does that all in one step. So the cosine distance or approximately the distance, as you'll see, between two vectors u and v of shared dimension n. This calculation has two parts. This is the similarity calculation, cosine similarity. And it is the dot product of the two vectors divided by the product of their L2 lengths. And then to get something like the distance we just take 1 and subtract out that similarity. Again, let's ground this in our example. is essentially measure the angles between these lines that I've drawn from this origin point. And so you can see that cosine distance is capturing the fact that A and and B are close together as measured by this angle. Whereas B and C are comparatively far apart. So again with cosine, we're abstracting away from frequency information and keying into that abstract notion of similarity that connects A and B as against C. Another perspective that you could take is just observe that if we first normalize the vectors via the L2 norm and then apply the cosine calculation. we changed the space as I showed you before. So they're all up here kind of on the units here. And notice that the actual values that we get out are the same whether or not we did that L2 norming step. And that is because cosine is building the values out of the space that we're using in the model. And we're getting the same values regardless of whether we did the L2 step or not. We're just using a different way to get the same results. effects of L2 norming directly into this normalization here in the denominator. There are a few other methods that we could think about or classes of methods. I think we don't need to get distracted by the details. But I thought I would mention them in case they come up in the future, in case there's a need to change the way we look at the code in the first place. For example, we could look at how to use the L2 method to get the same result as the L1 method. in your reading or research. The first class are what I called matching based methods. They're all kind of based on this matching coefficient. And then Jaccard, Dice, and Overlap are terms that you might see in the literature. These are often defined only for binary vectors. Here, I've given them to you as examples of how they can be used in your research or reading. For more information, go to: http://www.cnn.com/2013/01/30/science/science-and-technology/how-to-use-jaccard-dice-overlap-in-your-reading-or-research. their generalizations to the real valued vectors that we're talking about. And the other class of methods that you might see come up are probabilistic methods which tend to be grounded in this notion of KL divergence. KL divergence is essentially a way of measuring the distance between two probability distributions. It is a way to look at the difference between the probabilities of two different events that occur at the same time in the same place. It can be used to calculate the probability of different events happening at the exact same time. distributions. To be more precise, from a reference distribution p to some other probability distribution q. And it has symmetric notions, symmetric KL, and Jensen-Shannon distance, which is another symmetric notion that's based in KL divergence. Again, these are probably appropriate measures to choose if the quantities that you're thinking are not what you want to think about when you're looking for a distribution. It's a good idea to start with a reference, then move on to a probability distribution, and so on. The cosine distance measure that I gave you before is not quite what's called the proper distance metric. To qualify as a properdistance metric, a vector of are appropriately thought of as probability values. The vector of is a function of the length of a line. The length of the line is the distance between two points. The distance between the two points is the ratio of the number of points to the distance that the line spans. The ratio of these two numbers is called the cosine ratio. comparison method has to have three properties. That is, it needs to be symmetric. It needs to give the same value for xy as it does to yx. It must assign 0 to identical vectors. And crucially, it must satisfy what's known as the KL divergence rule. KL divergence actually fails that first rule. It fails the second rule, and the third, that it must assign zero to an identical vector. It also fails to satisfy the third rule, that is, that the vector must be identical. called the triangle inequality, which says that the distance between x and z is less than or equal to the distance Between x and y and then y to z. Cosine distance, as I showed it to you before, fails to satisfy the triangleequality. And this is just a just a small example of what happens when you add up all the distances between two points in a circle. It's a big problem. It can be solved by adding up all of the distances from two points to the center of the circle. simple example that makes it intuitive. It just happens that this distance here is actually greater than these two values here, which is a failure of the statement of the triangle inequality. Now this is relatively easily corrected. But this is also kind of a useful framework. Of all the examples that make it intuitive, this is one of the best. It's a very simple example, but it shows how complex the problem can be. And it's also a very good example of how complex problems can be solved. different choices that we could make, of all the options for vector comparison, suppose we decided to favor the ones that counted as true distance metrics. Then that would at least push us to favor Euclidean distance, Jaccard for binary vectors only, and Jensen-Shannon distance if we were talking about binary vectors. If we wanted to compare binary vectors to each other, we could use the following methods: Jaccard for binary vectors, or Euclidean distance for binary vectors. probabilistic spaces. And we would further amend the definition of cosine distance to the more careful one that I've given here, which satisfies the triangle inequality as well as the other two criteria. And by this kind of way of dividing the world, we would also reject matching Jaccard, Dice, and Dice, for example, for the world as we know it today. We would also amend our definition of the world to include the whole world, not just certain parts of it. Overlap, KL divergence, and symmetrical KL divergence as ones that fail to be proper distance metrics. And so that might be a useful framework for thinking about choices in this space. One other point in relation to this. This is obviously a more involved calculation than the one that I did earlier. But it might be useful to think about this as part of a larger set of choices. It might be helpful to think of this as a series of choices that we make in the future. We'll have to wait and see. gave you before. And in truth, it is probably not worth the effort. Here's an example of just a bunch of vectors that I sampled from one of our vector space models. And I've compared the improper cosine distance that I showed you before on the x-axis with the proper one on the y-axis. It's not pretty, but it's better than what you'd get if you did it on the z-axis or y- axis. And it's not as bad as it used to be. cosine distance metric that I just showed you. And the correlation between the two is almost perfect. So there is essentially no difference between these two different ways of measuring cosine. And I think that they are probably essentially identical up to ranking, which is often the quantity that we are most concerned with. That is, we are concerned with the quality of the cosine, not the quantity of it. And so we want to make sure that we measure it in a way that makes sense to us. care about when we're doing these comparisons. So probably stick with the simpler and less involved calculation would be my advice. Let's close with some generalizations and relationships first. Euclidean, as well as Jaccard and Dice with raw count vectors will tend to favor raw frequency over other distributional patterns, so don't compare them to each other too much. For example, in the example above, we're comparing the number of points in a row to the number in a square root. like that more abstract one that I showed you with our illustrative example. Euclidean with L2 norm vectors is equivalent to cosine when it comes to ranking, which is just to say that if you want to use Euclideans and you first L2norm your vectors, you're probably just doing it wrong. It's just a way to get a more abstract example of how to rank a set of vectors. It doesn't have to be very abstract. It can be as simple as that. something that might as well just be the cosine calculation. Jaccard and Dice are equivalent with regard to ranking. And then this is maybe a more fundamental point that you'll see recurring throughout this unit. Both L2 norming and also a related calculation which which may as well be called the L2-Dice calculation. That's something to keep in mind. It's a very simple formula. It doesn't have to be complicated. It just has to be based on the fact that it's based on a number. will just create probability distributions out of the rows. They can be useful steps, as we've seen. But they can obscure differences in the amount or strength of evidence that you have, which can in turn have an effect on the reliability of, for example, cosine, non-Euclidean, or KL divergence. It can also have a negative impact on the amount of evidence you have that you can use to make a decision about whether or not to use a particular method. Right, these shortcomings might be addressed through weighting schemes though. But here's the bottom line. There is valuable information in raw frequency. If we abstract away from it, some other information might come to the surface. But we also might lose that important frequency information in distorting the space in which it is stored. It's a trade-off that we have to make if we want to get the most out of the data we have at our disposal. But it's worth it. that way. And it can be difficult to balance these competing pressures. Finally, I'll just close with some code snippets. Our course repository has lots of hand utilities for doing these distance calculations and also length norming your vectors and so forth. It also has this function called Neighbors. That function is used to do distance calculations in the course repository. It can also be used to calculate length of a line or a vector. It's also used to compute distance between two points in a course. in the VSM module. It allows you to pick a target word and supply a vector space model. And then it will give you a full ranking of the entire vocabulary in that vector space with respect to your target work, starting with the ones that are closest. So here is the list of words that are the closest to my target word. And here is a list of the words that were the most likely to be the target word in the first place, starting from the word that was the closest. the results for "bad" using cosine distance in cell 12 and Jaccarddistance in cell 13. And I would just like to say that these neighbors don't look especially intuitive to me. It does not look like this analysis is revealing really interesting semantic information. But don't worry, we're going to go back and do it again in the next section. The next section will be about the "good" neighbors in cell 11. The third and final section is about cell 12. The fourth and fifth are about cell 11 and 13. to correct this. We're going to start to massage and stretch and bend our vector space models. And we will see much better results for these neighbor functions and everything else as we go through that material. It will be a big step forward for us in terms of our understanding of how the universe works, and how it can be improved for the benefit of the human body. It's going to be a very exciting time for us, and I'm looking forward to it. I think we'll see a lot more progress in the coming years.