In week four, I'm going to talk about machine translation related topics. And then in the second half of the week, we take a little bit of a break from learning more and more on neural network topics. I'll talk about final projects, but also some practical tips for building neural network systems. I'm just checking everyone's keeping up with what's happening. So first of all, assignment 3 is due today. And really today's lecture is the primary content for what you'll be using for building your assignment 4 systems. In the early 1950s, there started to be work on machine translation. It was hyped as the solution to the Cold War obsession of keeping tabs on what the Russians were doing. Claims were made that the computer would replace most human translators. But despite the hype it ran into deep trouble. The experiments did not go well. And so in retrospect, it's not very surprising that the early work did not work out very well. But as regards poetry and novels, no, I don't think we'll ever replace the translators of that type of material. using to translate. And so effectively, what you were getting were very simple rule based systems and word lookup. But that just didn't work well because human languages are much more complex than that. So if instead we had a probability over English sentences p of y, and then a probability of a French sentence given an English sentence, that people were able to make more progress. And it's not immediately obvious as to why this should be because this is sort of just a trivial rewrite with Bayes' rule. That allowed the problem to be separated into two parts which proved to be more tractable. CS228 is a statistical machine translation system. It uses a language model and a translation model to pick out the most likely why there's the translation of the sentence. It then breaks things down into pieces and explores it in what's called a decoding process. In the process, I'll go through in more detail later when we do the neural equivalent.off and see CS228 if you want to know more about that. And that's what we're not really expecting you to understand the details here. But I did then want to say a bit more about how decoding was done. In the period from about 1997 to around 2013, statistical machine translation was a huge research field. Google Translate launched in the mid 2000s. And people thought wow, this is amazing. But that was chugging along well enough. And then we got to 2014. And really with enormous suddenness, people then worked out ways of doing machine translation using a large neural network. And these large neural networks proved to be just extremely successful, and largely blew away everything that preceded it. So for the next big part of the lecture, what I'd like to tell you something about neural machine translation. model end to end on parallel sentences. And it's the entire system rather than being lots of separate components as in an old fashioned machine translation system. So these neural network architectures are called sequence to sequence models or commonly abbreviated seq2seq. And they involve two neural networks. So if we have a source sentence here, we are going to encode that sentence. And what we know about a way that we can do that, we can start at the beginning and go through a sentence and update the hidden state each time. Each step, as we break down the word by word generation, that we're conditioning not only on previous words of the target language, but also each time on our source language sentence x. Because of this, we actually know a ton more about what our sentence that we generate should be. And so in the same way that we saw last time for language models, we can work out our overall loss for the sentence doing this teacher forcing style, generate one word at a time, calculate a loss relative to the word that you should have produced. Multi-layer or stacked RNNs are more powerful. So we've got this multilayer LSTM that's going through the source sentence. And so now, at each point in time, we calculate a new hidden representation that rather than stopping there, we sort of feed it as the input into another layer. And the output of it, we feed into a third layer, and so we run that right along. And then that we use to then feed in as the initial, as theInitial hidden layer into then sort of generating translations, or for training the model. stuck with it. And you have no way to undo decisions. So if these examples have been using this sentence about, he hit me with a pie going from translating from French to English. So we'd like to be able to explore a bit more in generating our translations. And well, what could we do? Well, I sort of mentioned this before looking at the statistical empty models. Overall, what we'd want to do is find translations that maximize the probability of y given x, and at least if we know what the length of that translation is. Far too expensive. So beyond greedy decoding, the most important method that is used. And you'll see lots of places is something called beam search decoding. So beam search's idea is that you're going to keep some hypotheses to make it more likely that you'll find a good generation. So what we want to do is search for high probability hypotheses. So this is a heuristic method. It's not guaranteed to find the highest probability decoding. But at least, it gives you more of a shot than simply doing greedy decoding. an example to see how it works. So in this case, so I can fit it on a slide, the size of our beam is just 2. And the blue numbers are the scores of the prefixes. So we start off with our start symbol. And we're going to say, OK. What are the two most likely words, to generate first according to our language model? And there are the log probabilities. Then what we do next is for each of these k hypotheses, we find what are likely words to follow them? Neural translation has proven to be much, much better. It has many advantages. It gives better performance. The translations are better. In particular, they're more fluent because neural language models produce much more fluent sentences. And then the technique of optimizing all parameters of the model end to end in a single large neural network has just proved to be a really powerful idea. The models are also actually great in other ways. They actually require much less human effort to build. There's no feature engineering, there's in general, no language specific components. In 2014, the first modern attempt to build a neural network from machine translations and encoded-decoder model. Within two years' time, Google had switched to using neural machine translation for most languages. Does that mean that machine translation is solved? No. There are still lots of difficulties which people continue to work on very actively. But there are lots of problems with out of vocabulary words. And domain mismatches between the training and test data. And hopefully, you'll even get a sense of this doing assignment 4. Even our best multilayer LSTMs aren't that great of capturing sentence meaning. For languages that have lots of inflectional forms of nouns, verbs, and adjectives, these systems often get them wrong. So here's just sort of quick funny examples of the kind of things that go wrong, right? So if you asked to translate paper jam. Google Translate is deciding that this is a kind of jam just like this. And so this becomes a jam of paper. There are problems of agreement and choice. If you have many languages don't distinguish gender, the sentences are neutral between things masculine or feminine. NMT is a flagship task for NLP and deep learning. It was a place where many of the innovations of deep learning NLP were pioneered. We've got a new version of the assignment, which we hope will be interesting. But it's also a real challenge. So for assignment 4 this year, we've decided to do Cherokee English machine translation. Cherokee is an endangered Native American language that has about 2000 fluent speakers. It's an extremely low resource language. So it's just there isn't much written Cherokee data available. For languages for which there isn't much parallel data available, commonly the biggest place where you can get parallel data is from Bible translations. So this is a piece of parallel data that we can learn from. Cherokee is not a language that Google offers on Google Translate. So we can see how far we can get. But we have to be modest in our expectations because it's hard to build a very good MT system with only a fairly limited amount of data. There is a flipside, which is for you students doing the assignment. The advantage of not too much data is that your models will train relatively quickly. In the U.S. in the.1840s, the percentage of Cherokee that were literate in Cherokee written like this was actually higher than the. percentage of white people in the southeastern United States at that point in time. So we had this model of doing sequence to sequence models such as for neural machine translation. And the problem with this architecture is that we have this one hidden state, which has to encode all the information about the source sentence. So it seems like we would do better, if somehow, we could get more information from the source.