foreign and I should turn off the zoom background blur Ry options like this oh it does show up uh yeah there's like speaker notes on your screen but there's be careful because I accidentally just put something else in the first longer okay. I apologize that was kind of a rough introduction that was uh that was me making a couple of last minute edits that probably hurt more than they helped so I want to just apologize. I just think I was just too ready to go I usually uh yeah as our slides were they and put which is the product describing to replace the names. review um convolutions and and the architecture of a CNN to make this more clear and put it put it into perspective how it relates to just standard um dense neural networks I think it's fine um so we talked I think I think most people felt okay about um the actual mechanics of doing a convolution um and I just wanted to sort of clarify that when we do a convolutions operation we treat it like a layer like with standard dense neural network. There were more questions on on this on the sort of mechanics of like what does CNN is and what it what it looks like mechanically. and we also have a bias term that gets added to the output of moving each window on each location of our input we refer to it as a volume simply because it sort of looks like a cube. If you don't want to do a striving convolution a very simple way to do it is to just look at individual little squares and just take the max in this whole area in each one of these areas and just spit that out. If there are one by one convolutions can actually be used as a form of padding and dimensionality addition and reduction. a little timeline here starting from uh alexnet and moving forward. Laneet is something that was created quite a while ago. Alexnet is like the first kind of visible Improvement in this field. We want higher accuracy and a simpler architecture and these are two things that Inception Nets are able to give us in some form um but residents bring to another level as well. We're only about like halfway through the network with this network with it's super easy to learn. We don't need the rest of these layers they're just like okay we're only like okay they're in the way to make a good classification. soft Max will scale more logarithmically um and it'll give you a final like probability map um are there any questions on Alex now actually before uh we move on yeah what's up yeah so uh if you don't specify a certain type of padding valid padding is going to be applied to make sure that as you're sliding your kernel across an image you're left with the same dimension is there anything you want to add Jake or no that's I I should have mentioned adding two yeah I mean you did a good job we basically just had a bunch of zeros on the outside. discrimination in lower stages um increase the gradient signal that gets propagated back and provide additional regularization. Vanishing gradients is a common problem as you add a bunch of layers stacked together and that the learning signal or the gradient computation becomes extremely weak the model struggles to learn. The depth of the matrix multiplication that you're doing without like by losing form of the identity is the reason that stacking a lot of layers doesn't result in like better performance or strictly better performance even like equivalent performance. relief activation function um as your X goes through a weight layer the function is applied you go through another weight layer this f of x kind of encompasses that process this is the function that you've applied to X now your output is whatever f of X is the motivation behind residuals is that after your f ofx has been applied you add X back into your network. Adding residuals will increase the time to convergence because you're increasing the number of backwards considering computations that you have. A 34 layer residual will have jumps between every two layers. event like a low dimensional projection s yeah yeah this is like probably like really important thing for today but like this idea of like why it'd be important to sort of be able to learn the identity like it's sort of a weird thing um are there any questions or comments or concerns about that yes yeah for sure right so like if you have a dent snail Network like like let's just ignore convolutions right now if you like a dense neural network trivially you have the identity Matrix which is just ones along the diagonal and it spits out the exact same thing that it took in. generally work I'm running a thin one by one by three layer here so this is 64 times three it's 192. and this is what is being multiplied by the 256 and added to our previous product which is 74 times 64. so these two are being added together to end up with your your final computation for how many I guess multiplication parameters you have other questions about this yeah. So mobilenet has a lot fewer parameters which results in a lot faster convergence time um and it matches Inception of D3 accuracy just by using depth and point wise convolutions and combining. so these are some things that these models wanted to optimize over time accuracy performance and model size um model size is something that has a trade-off if you get too big you lose out on other metrics like accuracy. performance is something directly corresponds to depthwise convolutions and mobile nuts for Edge Computing and things like that. You want to drastically reduce the number of computations that you want to do yep that is basically everything for today thank you guys for coming oh and there will also be a quiz.