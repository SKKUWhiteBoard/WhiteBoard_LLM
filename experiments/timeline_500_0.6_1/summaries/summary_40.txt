Danqi Chen is one of the foremost researchers in question answering. She is the professor at the Princeton University. Danqi once upon a time was the head TA of CS224N. She's quite familiar with the context of this class. So today I'm very happy to introduce to you some of the fundamentals in this field, as well as on cutting edge and state of the art topics. So here's my plan for this lecture. I'm going to spend the most of this lecture focused on one type of question answering problems called reading comprehension. like 10-ish minutes to talk about a more practical, and in my opinion, more exciting problem called open domain question answering. Question answering, or, let's say QA in short, is one of the earliest NLP tasks, and the early systems can even date back to the 1960s. And, the question and answer has enabled a lot of really useful real world applications. For example, today if you just put your question in a search engine like Google, you can actually click on the correct answer, which is actually our concise answer. Question answering is probably one of those fields that we have seen the most remarkable progress in the last couple of years driven by deep learning. So in this lecture, I will be mostly focusing on the text based, or textual question answering problems. Another class, bigger class of the question Answer problems is called visual question answering. So if you have interest in these type of problems, I encourage you to check out those problems, but I'm not going to dig into these problems today. So next, I'm going to start with a part 2, reading comprehension. Reading comprehension has been viewed as a very important test bed for evaluating how well computer systems understand human language. This is really just similar to how we humans actually test the reading comprehension test to evaluate how well we actually understand one language. So this is also the way that we actually post questions to test the machine's language understanding ability. It actually has been formally stated back in 1977 by Wendy Lehnert in her dissertation. She says that, since questions can be devised to query any aspect of text comprehension, the ability to answer questions is the strongest possible demonstration of understanding. is actually called a semantic role labeling. So basically try to-- given one sentence, given one word, "finish" trying to figure out who did what to whom and when and where. By converting all these kind of semantic role relations, we can also just apply the reading comprehension problem and give you the correct answer. So next, I'm going to introduce this Stanford Question Answering Dataset called SQuAD. So if you are going to develop for the final projects, you will need to use this dataset. If you remove the context-to-query attention, the performance will drop to 67.7 F1 score. And then if you remove this part, it will drop a 4-point F 1 score. So, OK, we can actually use BERT for our reading comprehension. So you just have this input attention into BERT. And BERT can give you the hidden vector hi that can actually represent the hidden. vector that's corresponding to the context word ci. OK, so next, I'm going to talk about BERT, how to use the BERT model to solve this problem. yeah, I guess I'm just a little worried about who comes up with the test cases? Who determines what the right answer is? I mean, we will have more discussion of toxicity and bias coming up very soon, including actually Thursday's lecture as well as a later lecture, not specifically about QA though. OK. Next person is-- Thank you for the lecture. Yeah, my question is also related to the open domain question answering. So I was just wondering how much of the learning side of domain sort of generalization or domain alignment techniques can be combined with language level, like question answering? The key difference between the generative model and the extractive model is that for generative models, you can actually leverage more input passage together and do the generation. So for this model, there isn't any retrieval. So you can always find the answer from the question, right? So this model really has you relying on all the parameters you memorized, all the questions you've answered. And then generative Models are you're remembering the whole question and you try to retrieve the memory when you answer the question. The model is very large, like 11 billion parameters. So the parameters are basically trying to memorize a lot of information that has been.information. So by just taking this input, it has to just rely on the parameters to infer this answer. So it's actually very hard to-- yeah, it's a definite balance between memory and the generalization from it. All right, thanks. Do you want to call it a night or do you want one more question? Either way, yeah.