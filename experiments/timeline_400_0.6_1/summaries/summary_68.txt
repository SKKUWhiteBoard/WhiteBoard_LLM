Today we're gonna talk about learning in the setting of games. What does learning mean? How do we learn those evaluation functions that we talked about? And then, er, towards the end of the lecture, we wanna talk a little [NOISE] bit about variations of the game- the games we have talked about. So, uh, how about if you have- how about the cases where we have simultaneous games or non-zero-sum games? And an example of that is rock, paper, scissors. Can you still be optimal if you reveal your strategy? In the game of chase- che- and chess example is, you have this evaluation function that can depend on the number of pieces you have, the mobility of your pieces. Maybe the safety of your king, central control, all these various things that you might care about. So we're gonna talk about how learning is applied to these game settings. And specifically the way we are using learning is to just get a better sense of what this evaluationfunction should be from some data. And, and that kind of introduces to this, um, temporal difference learning. It's very similar to Q-learning. can actually, like, roll two dice and based on the outcome of your dice, you move your pieces various, various amounts to. Uh, there are a bunch of rules. So your goal is to get all your pieces off the board. If you have only one piece and your opponent gets on top of you, they can push you to the bar and you have to start again. The idea is, we want to come up with features that we would care about in this game of backgammon. So a feature template- set of feature templates could look like this. "We generate episodes and then from these episodes, we want to learn. These episodes look like state action reward states and then they keep going until we get a full episode" "The reward is going to be 0 throughout the episode until the very end of- end of the game. Until we end the episode and we might get some reward at that point or we might not" "We go over them to make things better and better. So that's, kind of, the key idea" So, so what do we try to do usually, like when you are trying to do learning? We have prediction, we have a target, what do I do? Minimize the- your error. So, so my target the thing that I'm trying to like get to is the reward plus Gamma V of s prime, w, okay? So we're playing games, in games Gamma is usually 1. And then the update is just this, this particular update where we move in the negative direction of the gradient. This is, this is what you guys have seen already, okay. to 0.25 and 0.75 then it kind of stays there, and you are happy. All right so, so this was just an example of TD learning but this is the update that you have kind of already seen. And then a lot of you have pointed out that this is, this is similar to Q-learning already, right? This is actually pretty similar to update, um, it's very similar, like we have these gradients, and, and the same weight that we have in Q- learning. The idea of learning in games is old. People have been using it. In the case of Backgammon, um, this was around '90s when Tesauro came up with, with an algorithm to solve the game. And he was able to reach human expert play. And then more recently we have been looking at the game of Go. So, in this section we're gonna talk a little bit about AlphaGo Zero too. So if you're attending section I think that will be part of that story.