The cost function and the reward function are really the same thing they're just negatives of one another and the reason that we see both sometimes is the same kind of a cultural distinction that I alluded to before remember I mentioned that we have S a which comes from the study of dynamic programming that's where the reward comes from in optimal control. In optimal control it's it's a bit more common to deal with costs I don't know if there's a cultural commentary here well you know optimal control originated in Russia maybe it's more common in America. thing you actually want like reaching your destination or avoiding a car accident and then use those with more the more powerful reinforcement learning algorithms. In future weeks, we'll cover more of the powerful algorithms that we'll be covering in the next few weeks. We'll also cover how to use these algorithms to help you get what you want in the real world. Back to the page you came from. Follow us on Twitter @CNNOpinion and @cnnOpinION. We'd like to hear from you. The dagger algorithm aims to provide a more principled solution to the imitational and distributional Shi problem. The idea in dagger is to actually run the policy in the real world see which states it visits and ask humans to label those States. The goal is to collect data in such a way that P Pi Theta can actually learn from the data it's trained on. The basic version of dagger works like this and that's the version that you will all be implementing in your homework. It's a very simple algorithm to implement if you can get those labels. It can actually get up to fly pretty reliably through a forest dodging trees.