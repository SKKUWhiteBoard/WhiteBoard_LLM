The final contest, which is due tonight, is to design an agent that plays together with another agent to try to collect food pellets while not getting eaten by ghosts. submissions for that, your last chance to submit are tonight at midnight. And on Thursday in lecture, we'll discuss the results. What else is left? I think there is a project due next week. There is still a section this week. And I think that homework is all wrapped up, but you would still have a self-assessment of your last homework that will be due next. And then there's a final exam the week after that. be mostly on advanced applications. We will not quiz you on these application lectures, on the final exam, or anything like that. It's more meant to give you more perspective rather than extra study materials for the final. So far, I've looked at foundational methods for search, for acting adversarial environments, for learning to do new things, and for dealing with uncertainty, noisy information. We'll look a bit more at AlphaGo, arguably the most prominent result in gameplay in the past few years. For a game like Tic-Tac-Toe, you will find out that you can force a draw, and that means fully solving the game. For Go, this is actually pretty hard to do, and it's even much harder than chess. And why? Let's take a look at chess. It's a 19 by 19 board, so there's 19 times 19 positions to choose from in the first move. And then one less, of course, next move. But the branching factor is enormous. So if you tried to run an exhaustive search through this kind of game tree, it's not going to work. DeepMind's AlphaGo is a computer program that can play the game of Go. It uses a deep neural network to learn how to search for the best moves. It can also learn to predict who is likely to win from a given situation. DeepMind is now working on a system that can learn to play Go without human help. It's called AlphaGo Zero, and it's being developed by DeepMind's Sergey Brin and Yann Leibovitz. It will be unveiled at the World Economic Forum in Davos this week. AlphaGo Zero has no prior knowledge. It starts at a pretty, actually, negative elo rating. Loses all the time initially. It's playing itself, but then gets tested against a set of players. After 21 days, it goes past where AlphaGo Lee Sedol was. And then it was still creeping up after 40 days. Once you reach that level, essentially, there's no further to go, because you solved the game. With reasonable compute power, it traverses the whole tree, even with alpha beta pruning. Berkeley student Woody Hoburg took charge in seeing how far we can get without human experts for some simpler things, not for all the maneuvers here. He was able to have it learn to hover reliably with the only human input being shut it off when it looks like it might start doing something dangerous. We did not push that further to flying those maneuvers. There is some work. If you look at Woody, Woody was shutting things off.to this behavior. Then recently at OpenEye, there's been some work on robots learning to do back flips. it has more time, and if it already has a recovery controller, then you can imagine that. And Claire Tomlin's group here at Berkeley has done some work in that direction, where they have a safe controller and a learned controller. And the learning controller is learning on its own while the safe controller keeps things in check so the helicopter doesn't crash. So what we used there is a model-based reinforcement learning method. So we learned a dynamics models for simulator from data that was collected. To learn a good controller in the simulator, we used something called iterative LQR. is a separate linear feedback controller for each time slice. If there is no wind, you can actually just run the linear feedback control. It will be fine. But if there's some wind gusts that could throw you off, you want to use the value functions and the two second look ahead against those value functions to do the controls. Training a unified policy across the entire space might work. It might take some work, exactly, figuring out how to do it. It could be interesting to revisit that now and see what the current understanding of how to train these networks. In 2015, there was the Doppler Robotics Challenge, which was held in Pomona, just east of Los Angeles. The robot had to, essentially, drive a car or walk, but driving the car was recommended. It turned it's very complex to get a robot to do that. The thing is modeling these situations proved even harder than modeling helicopters, because your sensing needs to understand whether or not you're already making contact, and making contact or not. You can be very close, but not have contact. It's a very subtle thing. The Dartmouth Grand Challenge is the first time autonomous vehicles have been put to the test. Stanley is about to become the first vehicle in history to drive 132 miles by itself. Just five robots remain on the course, and to finish, they must wind through a treacherous mountain pass. The first Grand Challenge, where many faltered within sight of the start, and no robot went beyond seven miles, was held in 2005. It was won by a Berkeley entry. Only motorcycle was allowed in the race. tail of special events that can happen when you're driving. You can measure progress by just demo videos, which is one way, and it gives you some kind of feel for what's going on. But the 2013 video is already very impressive. So another way to measure progress is to see how are these cars doing relative to human drivers. So left and right are the same plots, but the ride is on a log scale so you can see more detail. It's a number of events per 1,000 miles driven. Red there is human fatalities. Then yellow is human injuries. is between 10 and negative 2 and 10 negative 3 per 1,000 miles of human driving. In green is the Google slash [? wave ?] mode disengagement. It's when the driver decides they want to take control because they don't trust the autonomous system right now to avoid an accident. And we see that it's going down how often that needs to happen, but still a bit removed from where humans are at. Where does this data come from? If you test in California, you have to report this data to the DMV. so many decisions. If they're gigantic, use a lot of power. That's a problem. Let's see what we can do to build smaller networks to make decisions. What else did we not cover yet? Personal robotics. I want to spend a little more than two minutes on that, so let's keep that for Thursday. that's it for today. Bye. [SIDE CONVERSATIONS] [Side CONversation] [sideconversation.com: Do you know more about this topic? Email us at jennifer.smith@cnn.com].