Robotics is a really cool and important direction for the future. We are moving towards a world where so many routine tasks are taken off your plate. I really believe that in the future we will have AI assistance whether they are embodied or not to act as our guardian angels to ensure that we maximize and optimize our lives to live well and work effectively. It turns out it's 19. Can you count how many robots there are in this in this image? Anybody wants to want to take a guess how many Robots do I have in this Image? Okay that's so that's that's really close! cognitive and physical work. And so today we can say that with AI we we will see such a wide breadth of applications for instance these technologies have the potential to reduce and eliminate car accidents. These technologies have potential to keep your information private and safe to transport people and goods more effectively and faster and cheaper to really make it easier to communicate globally by providing instantaneous translations. To develop education to everyone to allow human workers to focus on big picture tasks with machines taking on the routine tasks and so this future is really enabled by three interconnected fields. is about learning from and making predictions on data. This kind of application of machine learning is Broad it it applies to cognitive tasks and physical tasks. We can characterize how machine learning works as using data to answer questions that are either descriptive predictive or prescriptive. So when we think about these questions in the context of a robot we have to kind of get on the same page about what a robot is and um and so think of a Robot as a programmable mechanical device that takes input uh with its sensors reasons about this input and then generates an action. can do so the robot can only do what its body is capable of doing a robot on wheels will not be able to do the task of climbing stairs so we have to think about that body and we have at T Cell we have a lot of machine learning based research that allows us to examine how to design optimally a robot body for a particular task. In the context of robots we have three types of learning and you have seen different aspects of these methodologies throughout the course we have supervised learning, unsupervised learning and reinforcement learning. Road and we do this so that when the system when the car sees a new image for example this one the car could say oh this is a this is ducks on road now. In order to in order to actually provide solutions for this object classification problem we have to employ multiple algorithms. The exciting thing is that we already have very good algorithms that that can segment images very fast so we can we can do this. We can take an image and we can find the object in the image very fast we don't know what the objects are. of people to label the objects that we have and so this is exciting but it but labeling is a very labor intensive activity and a significant challenge to machine learning. The most popular Benchmark for measuring the accuracy of image classification is imagenet and we see performance of various variations of of image Classification algorithms that perform well into 90 90 accuracy. But if those algorithms were to run on a car that's not good enough because the car is a safety critical system and in fact we cannot afford to have any errors in how images get recognized in the car. Boris Katz did an experiment a few years ago where they took regular objects and they put them in a different context. With this significant change in context the performance of the top performing imagenet algorithms dropped by as much as 40 to 50 percent. So keep this in mind as you think about deploying or building and deploying deep neural network Solutions. There's another thing um that is very critical for for autonomous driving and and for robots you have heard a beautiful lecture on adversarial attacks well it turns out you can attack very well. The images that get fed from the camera streams of cars are fed to the decision-making engine of the car. Machine learning is very powerful for building perception systems for robots but as we employ machine learning in the context of robots it's important to keep in mind the scope when they work when they don't work and then it'simportant to think about what my what kind of guard rails we might put in place at the decision time so that we have robust Behavior. "With all small perturbations you can turn the stop sign into a yield sign and you can imagine whatkind of chaos this would create on a on a physical Road" to do given input reinforcement learning is causing a huge revolution in robotics. We have built fast simulation systems and simulation methodologies that allow us to run thousands of simulations in parallel in order to train a reinforcement learning policy. We are also decreasing the gap between the hardware platforms and the simulation engines so you have seen reinforcement learning earlier in the in the boot camp and so reinforce learning is concerned with how intelligent agents ought to take action in an environment inorder to maximize the notion of a cumulative reward. error and and eventually the positive rewards dominate the negative rewards and that that directs the the agent towards the the best action and so here is an example where we have a reinforcement learning agent that is trying to drive on a race track and you can see that it starts off and it initially it makes mistakes but eventually it learns how to how to take the turns at high speeds. It's very it's really a very exciting area reinforcement learning much like deep learning has been invented decades ago but it works well now because of the Advent of of computation. years ago and so these techniques that did not do so well back then all of a sudden are creating extraordinary possibilities and capabilities in our agents now this is a simple simulation in order to get the simulation to drive a real robot we actually need to think about the Dynamics of the robot and so in other words we have to take into account what the vehicle looks like what are its kinematics and its Dynamics and so here is a vehicle that is running the policy learned in simulation so it's really cool because really we are now able to train in simulation. they get the position of the other vehicles on the track but only only so they get this position from an external localization system but they only know where the vehicles within their field of view are. I think that these these advancements in robotics are really enabling the possibility that you saw in the first Slide the possibility of creating many robots that can do many tasks and much more complicated tasks than what we see here. What I want to talk about next is the autopilot how do we take these pieces together to enable a self-driving vehicle.  Carnegie Mellon project called nav lab built a car that was driven by a machine learning engine called Alvin and Alvin drove this car all the way from Washington DC to Los Angeles. The car was in autonomous mode for a large part of the highway driving but there was always a student there right ready to um to take control and the car did not did not drive inonomous mode when there were when it was raining or when there was a lot of congestion or when the car had to had to take exits. In 1986 German engineer Ernst Dickman started thinking about how he could turn his van into an autonomous vehicle. He put computers and cameras on the van and began running tests on an empty section of the German Autobahn which had not been open for for public driving. He was able to actually get his van to drive on that empty road but interestingly when he started developing the MIT autonomous vehicles he was not able to do the same thing. It's extraordinary to think about what is needed uh in terms of of advancement in terms to get from where we were back then to the point where we can actually see deployed autonomous vehicles. This work um computers needed about 10 minutes to analyze an image can you imagine okay so how do you go from that to enabling an autonomous vehicle to drive at 90 kilometers an hour well um what they did was they they developed some very fast solutions for paring down the image to only the the the aspects that they needed to look at. They assumed that there were no obstacles in the world which made the problem much easier because all the car had to do was to stay on on the road so it's really super interesting to think about how visual processing improved from one frame per 10 minute to 100 frames per second. vehicle we deployed and in fact we had the public ride our vehicle in 2014 we have vehicles at MIT we have a lot of other groups that are developing these vehicles now before we had lidar we had sonar and nothing worked when we had Sonar. With lidar that problem went away so all of a sudden a powerful accurate sensor made a huge difference all the algorithms that were developed on Sonar and didn't work started working when the later was introduced it's really exciting um okay now when we think about autonomous driving there are several key parameters that emerge. of these systems are one one question how complex is the environment where the car t Road like in the German case then the problem is much easier then we have to ask ourselves how how complex are the interactions between the car and the environment. We have very effective and Deployable solutions for robot cars that move safely in Easy environments where there aren't many static nor moving obstacles and you can you can see from this example this is this is an example of the MIT car and it'sYou can see this this car operating autonomously without any any obstacles. issues at Fort Devens where there aren't too many obstacles and the car is perfectly capable of avoiding the obstacle by the way that's my car. The sensors don't work well in weather and the uncertainty of the perception system increases significantly if it rains hard or it snows. The uncertainty also increases in the case of extreme congestion where you have erratic driving with vehicles with people with scooters even with cows on the road and this is a video I took during a taxi ride in Bangalore there come the cows so um there are so many important preconditions. them follow a very simple solution which you can adopt and turn your car into a self-driving car. Here's what you have to do you take your car you extend it to drive by wires so that your computer can talk to um to the steering and the acceleration the throttle controls. Then you'll further extend this car with sensors and most of the sensors we use are cameras and lidars. Then there are Suite of software models modules and this includes a perception module that provides support for making maps and for detecting a static and dynamic obstacles. country on a country road or in the city on a city road or you're on a road with no Lane markings so these are these are really challenging things that that the first solutions for autonomous driving had to had to reason through now in Alexander's PhD thesis his idea was to utilize a large data set to learn a representation of what humans did in similar situations. The solutions that we employed build on things we have already talked about we can use deep learning and reinforcement learning to take us from from images of ofroads onto steering and and throttle onto what to do so this is really great because you can train on certain kinds of Roads. completely different driving environments and driving situations and you don't need new parameters you can go exactly directly to what the car has to do so in other words we can learn a model to go from raw perception and here you can think of this as pixels from a camera. The other thing we feed the vehicle is noisy Street View maps so these are not the high definition maps that are usually created by autonomous driving labs and companies and so you can do this to directly infer a full continuous probability distribution over the space of all control. from this from this data this data is processed and it's from thisData we can learn to maximize the likelihood of particular control signals for particular situations. The solution also allows us to to localize the the vehicle so it's really super exciting okay so we can we can get this human-like control but assuming light control requires a lot of data. It'd be pretty expensive to take a car and crash that car in order to generate the data so instead what we do is we do the training in simulation and so Alexander developed the Vista simulator. The Vista simulator has been recently open sourced you can get the code from vista.csel.mit.edu and a lot of people are already using the system. What we get from Vista are is the ability to simulate different physical sensing modalities that means including 2D cameras 3D lidar event cameras and and so forth. You can also simulate different types of interactions so here's how we use Vista we can take one high quality a data set taken from a human-driven vehicle we can turn it into anything we want. very realistically into a new simulated trajectory that is erratic and that now exists that's part of our training set in Vista and so we can we can do this and we can use this data and then we can learn a policy we can evaluate this policy offline and ultimately deploy it on the vehicle. Here you can see the results of comparing what happens in Vista with the existing simulators in the state of the art so the Top Line shows crash locations in red and the bottom line shows mean trajectory variation in color. The decision engine has about a hundred thousand neurons and about a half a million parameters and I will challenge you to figure out if there are any patterns that associate the state of neurons with the behavior of the vehicle. So it turns out this vehicle is looking at the bushes on the roads in order on the road in order to make decisions still it seems to do a pretty good job but we asked ourselves can we do better can we have more reliable uh learning based Solutions and so yesterday Ramin introduced liquid networks and introduced neural circuit policies. have a better understanding of how liquid networks work and what their properties are now we can take this model and apply it to many other problems. Here is a problem we call Canyon Run where we have taken a liquid Network and we have implemented it on a task of flying a plane with one degree of Freedom. Here's another task we call drone dodgeball where the objective is to keep the a drone at a specified location and the Drone has to protect itself when balls come its its a very complex task. way and you can see a two degree of Freedom solution to drone dodgeball and that's the network. You can really associate the the function of of this controller of this learning based control with activation patterns of the neurons and so very excited because in fact we're able to extract decision trees from these kinds of solutions. These decision trees provide human understandable of human understandable explanations and so this is really important for safety critical systems all right um so um let's see um Ramin told you that these liquid networks are Dynamic causal models and I want to show you some more examples that explain how these models are Dynamic ozone models. standard deep neural network and we have asked this network to solve this problem and the attention map of the network is really all over the place you can see that the network the the Deep neural network solution is very confused but check out something else the data that we collected was summertime data and now it's fall so the background is no longer green we have we don't have as many leaves on trees and so the context for this task has completely changed by comparison the the liquid network is able to focus on the task is not confused. tree lines and the environment looks much much different than than the environment where we trained. This kind of this kind of ability to transfer from one set of training data to completely different environments is is truly transformational for the capabilities of machine learning. We see other examples where we take our solution and we deploy it to find the same object the chair just outside of the stata building and this is uh the the Deep neural network solution that gets completely confused and here is the liquid Network solution that has the exact same input and has no problem. by hop we're actually searching the object and doing multi-step Solutions and in fact in fact we can if I can get to my next video I'm sorry so um I am the next one is it shows you that we can actually do this forever so here is an infinite hop demo that was done just outside on the baseball field and we we placed uh three of the same objects that we trained on and we placed them at unknown locations. We can see that liquid networks generalize very well whereas if we take an lstm solution it gets confused and goes to the wrong object. that yields models that generalize to unseen scenarios essentially addressing a challenge with today's neural networks that do not generalize well to unseen test scenarios. I think it's so exciting to use machine learning to study nature and to begin to understand the nature of intelligence and we in in our lab here at C cell we have one project that is looking at whether we can understand the lives of whales and so what do I mean by this so here is an example where we have used a robotic drone to find whales and look at what they do. The system is able to use machine learning to identify the whales and then once you have identified the whale we can actually Servo to the center of the whale essentially tracking the whale along the way. Here's a here's Sophie our soft robotic fish um which Joseph who is with us today has participated in in building and here is this beautiful beautiful very natural moving robot that can get close to aquatic creatures that can move in the same way aquatic creatures do without without disturbing them when you put thruster-based robots in Ocean environments they behave differently than than the fish do and they they tend to scare the fish. Machine learning can be used to look for the presence of language which is a major sign of intelligence. We are trying to understand the phonetics the semantics and the syntax and the discourse for whales. We have a big data set consisting of about 22 000 clicks. Using machine learning we can identify coded types. We can identify patterns for Coda exchanges and we can we can begin to really ask ourselves how is it that that that Wales exchange information and if you're interested in this problem please come see us. that model to run on edge devices or on huge devices uh you have seen that many of our Solutions are Black Box Solutions and sometimes we have brittle function we have we have easily attackable models you have also seen some alternative models like liquid networks which attempt to address some of these questions. There is so much opportunity for developing improved machine learning using existing models and inventing new models and if we can do this we can create an exciting work world where machines will really Empower us will really augment us. Just-in-time Holograms could be used to make the virtual world much more much more realistic much more connected and so here they're discussing the the design of a new flying car and let's say we have these flying cars and then we can integrate these cars with the it infrastructure and the cars will know your needs so that they can tell you for instance that you can buy the plants you have been wanting nearby by Computing.your dimensions and can create a bespoke shoe just for you. Robots can help us with cognitive and physical work. There's the garbage ban the garbage bin that takes itself out. After a good day when it's time for a bedtime story you can begin to enter the story and control the flow and begin to interact with the characters in the story. These are some possibilities for the kind of future that machine learning artificial intelligence and robots are enabling. I'm personally very excited about this future with robots helping us with Cognitive and physical Work but this future is really dependent on very important new advancements that will come from all of you. Thank you very much and uh come come work with us.