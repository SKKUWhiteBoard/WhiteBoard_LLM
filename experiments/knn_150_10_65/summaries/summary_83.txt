Lecture eight is about deep learning software and how the hardware works. We'll talk a little bit about CPUs and GPUs and then we'll talk about several of the major deep learning frameworks that are out there in use these days. This is one way that you can attack a lot of problems in deep learning, even if you don't have a huge dataset of your own. We're in the process of assigning TA's to projects based on what the project area is and the expertise of the TA's. It's kind of hard to most people in deep learning when we talk about GPUs, we're pretty much exclusively talking about NVIDIA GPUs. For all of the several notebooks it's just in Python and Numpy so you don't need any GPUs for those questions. The downside of a GPU is that each of those cores, one, it runs at a much slower clock speed. And two they really can't do quite as much. You can't really compare CPU cores and GPU cores apples to apples. The midterm will be in class on Tuesday, five nine. It'll be sort of pen and paper working through different kinds of, slightly more theoretical questions to check your understanding of the material that we've covered so far. And I think we'll probably post at least a short sort of sample of the types of questions to expect. So just, Yeah, yeah, so that's what we've done in the past is just closed note, closed book, relatively just like want to check that you understand the intuition behind most of the stuff we've presented. About fancier optimization algorithms for deep learning models including SGD Momentum, Nesterov, RMSProp and Adam. And we saw that these relatively small tweaks on top of vanilla SGD, are relatively easy to implement but can make your networks converge a bit faster. We also talked about dropout, where you're kind of randomly setting parts of the network to zero during the forward pass, and then you kind of marginalize out over that noise in the back at test time. So this is kind of out of the box performance, but it's not really like peak, possible, theoretical throughput on the CPU. The CPU is the Central Processing Unit, that's this little chip hidden under this cooling fan. The GPU is called a graphics card, or Graphics Processing Unit. These were really developed, originally for rendering computer graphics, and especially around games and that sort of thing. So if you're writing programs that run directly how you want it to work, these are the basic ideas even if you want to write it yourself. This is the kind of thing that you can find more details on GitHub, but my findings for things like ResNets, ResNet 19, Resnets 16 and VGG 16 were all very similar. This is kind of the prototypical type of problem that like where a GPU is really well suited, where a CPU might have to go in and step through sequentially and compute each of these elements one by one. So you could imagine that for a GPU you can just like blast this out and have all of this elements of the output matrix all computed in parallel and that could make this thing computer super super fast on GPU. Another thing to point out here is that these lines of code are not actually computing anything. There's no data in the system right now. We're just building up this computational.  Torch is actually in Lua, not Python, unlike these other things. Torch is also older, so it's more stable, less susceptible to bugs. In PyTorch it's in Python which is great, you've got autograd which makes it a lot simpler to write complex models. In Lua Torch you end up writing a lot of your own back prop code sometimes, which is a bit of a turn off for some people. They're about the same speeds, that's not really a concern. that can be really bad and slow you down. Some solutions here are that like you know if your dataset's really small, sometimes you might just read the whole dataset into RAM. You can also make sure you're using an SSD instead of a hard drive, that can help a lot with read throughput. Another common strategy is to use multiple threads on the CPU that are pre-fetching data off RAM or off disk, buffering it in memory. And finally you want all this stuff to run efficiently on GPUs so you don't have to worry too much about these low level hardware details about cuBLAS and cuDNN and CUDA. The first generation of deep learning frameworks that really saw wide adoption were built in academia. Now industry is giving us these big powerful nice frameworks to work with. So today I wanted to mostly talk about PyTorch and TensorFlow 'cause I personally think that those are probably the ones you should be focusing on for a lot of research type problems. I'll also talk a bit about Caffe and Caffe2. But probably a little bit less emphasis on those. Although I think Caffe is still pretty commonly used in industry again for production. In Numpy, you can just kind of write down in Numpy that you want to generate some random data. And in TensorFlow, we actually need to construct some concrete values that will be fed to the graph. So here we're just creating concrete actual values for X, Y, w1 and w2 using Numpy and then storing these in some dictionary. In PyTorch you can define your own new autograd functions by defining the forward and backward in tensors. And it has some other implications that we'll get to in a bit. of these frameworks. TensorFlow has this magic line that just computes all the gradients for you. So now you don't have go in and write your own backward pass and that's much more convenient. The other nice thing about Tensor Flow is you can really just, like with one line you can switch all this computation between CPU and GPU. So here, if you just add this with statement before you're doing this forward pass, you just can explicitly tell the framework, hey I want to run this code on the CPU. TensorFlow lets you build a graph and then run it over and over again. You can then feed data into the graph to perform whatever computation you want it to perform. In this example we're only feeding in the data and labels X and Y and the weights are living inside the graph. And here we've asked the network to compute the loss for us. And then you might think that this would train the network, but there's actually a bug here. So we've had to tell TensorFlow that the loss and updates is not actually a real value. In the forward pass we can use both our own internal modules as well as arbitrary autograd operations on variables to compute the output of our network. So here we receive the, inside this forward method here, the input acts as a variable, then we pass the variable to our self.linear1 for the first layer. And now the rest of this code for training this thing looks pretty much the same. Where we build an optimizer and loop over and on ever iteration feed data to the model, compute the gradients with loss.backwards, call optimizer.step. TensorFlow is smart and it only computes the parts of the graph that are necessary for computing the output that you asked it to compute. So that's kind of a nice thing because it means it's only doing as much work as it needs to, but in situations like this it can be a little bit confusing and lead to behavior that you didn't expect. So the solution in this case is that we actually need to explicitly tell TensorFlow to perform those update operations. There's a little trick you can do instead. The question is why is loss a value and why is updates none? That's just the way that updates works. So here after you run updates, then the output is none. So it's kind of some TensorFlow magic that's going on there. Maybe we can talk offline if you're still confused. So but now we've kind of got this, again we've got this full example of training a network in Tensor Flow and we're kind of adding bells and whistles. In the previous example we were computing the loss explicitly using our own tensor operations. TensorFlow gives you a bunch of convenience functions that compute these common neural network things for you. So in this case we can use tf.losses.mean_squared_error and it just does the L2 loss for us so we don't have to compute it ourself. And in this example we've actually not put biases in the layer because we're not using biases. So another kind of weirdness here is that we had to explicitly define our inputs and define our weights and then like chain them together in the forward pass using a matrix multiply. TensorFlow is an open-sourceensorFlow framework. It's used by Google to train neural networks. The code example shows how to use TensorFlow to train a neural network. It uses the xavier initializer object to set up an initialization strategy for the data and labels. It also sets up variables for those with the right shapes that are kind of inside the graph but a little bit hidden from us. And in fact if you run this code, it converges much faster than the previous one because the initialization is better. There's like a lot of different higher level libraries that people build on top of TensorFlow. When we're working with neural networks we have this concept of layers and weights and some layers have weights associated with them. So that's what these various packages are trying to help you out and let you work at this higher layer of abstraction. There's these three different ones, tf.layers, TF-Slim and TF.contrib.Learn that all ship with Tensorflow, that are but Tensorboard. can compose together these modules to build big networks. The PyTorch variable is similar to the TensorFlow tensor or variable or placeholder, which are all sort of nodes in a computational graph. Rather than using torch.FloatTensor, you do torch.cuda.Floattensor, cast all of your tensors to this new datatype and everything runs magically on the GPU. You should think ofPyTorch tensors as just Numpy plus GPU. That's exactly what it is, nothing specific to deep learning. a chance to play around with this myself so I can't really speak to how useful it is. Tensorboard actually lets you visualize the structure of the computational graph. And Visdom does not have that functionality yet. PyTorch is kind of an evolution of, kind of a newer updated version of an older framework called Torch which I worked with a lot in the last couple of years. It is pretty much better in a lot of ways than the old Lua Torch, but they actually share a much of the same back end C code for the back end. different where we're actually building up this new computational graph, this new fresh thing on every forward pass. That's called a dynamic computational graph. With a static graph you can imagine that you write this code that builds up the graph and then once you've built the graph, you have this data structure in memory that represents the entire structure of your network. And now you could take that data structure and just serialize it to disk. And then you could later rear load that thing and then run that computational graph without access to the original code that built it. The problem is that because we only build the graph once, all the potential paths of control flow that our program might flow through need to be baked into the graph at the time we construct it before we ever run it. In this case this tf.cond.kind of needs to be an explicit operator in the TensorFlow graph. But what this basically means is that you have this sense that Tensor Flow is almost building its own entire programming language, using the language of computational graphs. have loops. We can just kind of use a normal for loop in Python to just loop over the number of times that we want to unroll. Now depending on the size of the input data, our computational graph will end up as different sizes. But that's fine, we can just back propagate through each one, one at a time. Now in PyTorch this is super easy. We just want to compute this same recurrence relation no matter the length of our sequence of data. Super new paper that's being presented at ICLR this week in France. Initial impression was that it does add some amount of dynamic graphs to TensorFlow but it is still a bit more awkward to work with than the sort of native dynamic graphs you have in PyTorch. So one option is recurrent networks. So you can see that for something like image captioning we use a recurrent network which operates over sequences of different lengths. In this case, the sentence that we want to generate as a caption is a sequence and that sequence can vary depending on our input data. inner product, we compute some loss and the whole structure of the graph is set up in this text file. One kind of downside here is that these files can get really ugly for very large networks. So for something like the 152 layer ResNet model, which by the way was trained in Caffe originally, then this prototxt file ends up almost 7000 lines long. So people are not writing these by hand. People will sometimes will like write python scripts to generate these prototext files.