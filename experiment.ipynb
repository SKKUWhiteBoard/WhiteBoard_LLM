{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================= [built-in modules] =======================\n",
    "import os\n",
    "import time\n",
    "\n",
    "# ====================== [third-party modules] =====================\n",
    "import yaml\n",
    "from box import Box\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from bert_score import score as bert_score\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# ======================= [custom modules] =========================\n",
    "from utils.eval_similarity import *\n",
    "from utils.utils import *\n",
    "from utils.segment_embedding import *\n",
    "from utils.concat_functions import *\n",
    "from utils.summarizer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    config = Box(config)\n",
    "print('Experiment name:', config.experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading data... \", end=\"\", flush=True)\n",
    "if config.data.source == 'opensource':\n",
    "    datasets = load_dataset(config.data.opensource)\n",
    "    indices = np.load(f'data/gov_indices{config.data.index_set}.npy')\n",
    "    datasets = datasets['train'].select(indices)['report']\n",
    "\n",
    "elif config.data.source == 'youtube':\n",
    "    datasets = load_dataset(config.data.youtube)\n",
    "    indices = np.load(f'data/ytb_indices{config.data.index_set}.npy')\n",
    "    datasets = datasets['train'].select(indices)['content']\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_score = 0\n",
    "best_summary = \"\"\n",
    "\n",
    "evaluation_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for di, text in enumerate(datasets):\n",
    "    print(f\" ----------------- [{di+1}/{len(datasets)}] ----------------- \")\n",
    "    init_s = time.time()\n",
    "\n",
    "    # ========================== [Segmentation] ========================\n",
    "    print(\"Segmentating... \", end=\"\", flush=True)\n",
    "    s = time.time()\n",
    "    segments = segmentate_sentence(text, **config.segment.args)\n",
    "    e = time.time()\n",
    "    print(\"Done\", f\"{e-s:.2f} sec\")\n",
    "    \n",
    "    # ========================== [Embedding] ===========================\n",
    "    print(\"Embedding...    \", end=\"\", flush=True)\n",
    "    s = time.time()\n",
    "    embeddings = encode_segments(segments) # model fixed\n",
    "    e = time.time()\n",
    "    print(\"Done\", f\"{e-s:.2f} sec\")\n",
    "\n",
    "    # ========================== [Clustering] ==========================\n",
    "    print(\"Clustering...   \", end=\"\", flush=True)\n",
    "    s = time.time()\n",
    "    concat_indices = globals()[config.concat.method](embeddings, **config.concat.args)\n",
    "    e = time.time()\n",
    "    print(\"Done\", f\"{e-s:.2f} sec\")\n",
    "\n",
    "    max_group_size = max([len(group) for group in concat_indices])\n",
    "    print(f\"Num. of Cluster: {len(concat_indices)}, Max group size: {max_group_size}\")\n",
    "\n",
    "    # ========================== [Ready to summarize] ==================\n",
    "    batch_clusters = [\n",
    "        \" \".join([segments[gi] for gi in group]) for group in concat_indices\n",
    "    ]\n",
    "\n",
    "    # ========================== [Summarize] ===========================\n",
    "    print(\"Summarizing...  \", end=\"\", flush=True)\n",
    "    s = time.time()\n",
    "    if config.mini_batch.size > 0:\n",
    "        mini_batch_size = (len(batch_clusters)\n",
    "                           if len(batch_clusters) < config.mini_batch.size else\n",
    "                           len(batch_clusters) // config.mini_batch.size)\n",
    "\n",
    "        batch_summaries = []\n",
    "        for i in range(0, len(batch_clusters), mini_batch_size):\n",
    "            mini_batch_summaries = summarizer(batch_clusters[i:i+mini_batch_size], **config.summary.args)\n",
    "            batch_summaries.append(mini_batch_summaries)\n",
    "        batch_summaries = \" \".join(batch_summaries)\n",
    "    else:\n",
    "        batch_summaries = summarizer(batch_clusters, **config.summary.args)\n",
    "    e = time.time()\n",
    "    print(\"Done\", f\"{e-s:.2f} sec\")\n",
    "\n",
    "    # ========================== [Evaluate] ============================\n",
    "    print(\"Evaluating...   \", end=\"\", flush=True)\n",
    "    s = time.time()\n",
    "    score = 0\n",
    "    e = time.time()\n",
    "    print(\"Done\", f\"{e-s:.2f} sec\")\n",
    "    print(f\"=> Score: {score:.4f}  \")\n",
    "\n",
    "    # ========================== [Post-process] ========================\n",
    "    if score > max_score: # score는 대소비교 가능한 1가지 방식을 이용\n",
    "        max_score = score\n",
    "        best_summary = batch_summaries\n",
    "        # 원본 텍스트의 index는 indices[di]로 찾을 수 있음\n",
    "    \n",
    "    evaluation_results.append(score)\n",
    "    print(f\"Total: {time.time()-init_s:.2f} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save experiment result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving evaluation results... \", end=\"\", flush=True)\n",
    "\n",
    "save_dir_path = os.path.join('experiments', f'{config.experiment_name}')\n",
    "if not os.path.exists(save_dir_path):\n",
    "    os.makedirs(save_dir_path)\n",
    "\n",
    "# Copy config file\n",
    "os.system(f'cp config.yaml {save_dir_path}')\n",
    "\n",
    "# Make README.md\n",
    "with open(os.path.join(save_dir_path, 'README.md'), 'w') as f:\n",
    "    f.write(f'# {config.experiment_name}\\n')\n",
    "\n",
    "# Save evaluation results\n",
    "...\n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
