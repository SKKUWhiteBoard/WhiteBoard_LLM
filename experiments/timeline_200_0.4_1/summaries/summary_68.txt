Today we're gonna talk about learning in the setting of games. What does learning mean? How do we learn those evaluation functions that we talked about? And then, er, towards the end of the lecture, we wanna talk a little [NOISE] bit about variations of the game- the games we have talked about. So, uh, how about if you have- how about the cases where we have simultaneous games or non-zero-sum games? And an example of that is rock, paper, scissors. Can you still be optimal if you reveal your strategy? somewhere. So, so one idea that we can use here is we can try to generate data based on our current policy pi agent or pi opponent, which is based onOur current estimate of what V is. Right. So currently, I might have some idea of what this V function is. It might be a very bad idea ofwhat V is, but that's okay. I can just start with that and starting with, with that V function that I currently have, what I can do is I can, I can call arg max of V over successors of s and a to get a policy for my agent. So, what you have here is you have a piece of experience; s, a, r, s prime, we can try to learn something from each one of these pieces of experience. And then we had a target that you're trying to get to. And my target, which is kind- kind of acts as a label, is going to be equal to my reward, the reward that I'm getting. So, so my target the thing I'm trying to like get to is the reward plus Gamma V of s Prime, w. Is it possible to have, an end state and not end state have the same feature vector, or no? If you use like, uh, initialize rates do not be zeros which you update throughout instead of just to the end. If there were kind of the same and have same sort of characteristics, it's fine to have feature that gives the same value. If it is always 0, it doesn't matter like what the weight of that entry is. So in general, you wanna have features that are differentiating and you're using it in some way. to 0.25 and 0.75 then it kind of stays there, and you are happy. All right so, so this was just an example of TD learning but this is the update that you have kind of already seen. And then a lot of you have pointed out that this is, this is similar to Q-learning already, right? This is actually pretty similar to update, um, it's very similar, like we have these gradients, and, and the same weight that we have in Q- learning. The idea of learning in games is old. People have been using it. In the case of Backgammon, um, this was around '90s when Tesauro came up with, with an algorithm to solve the game. And he was able to reach human expert play. And then more recently we have been looking at the game of Go. So, in this section we're gonna talk a little bit about AlphaGo Zero too. So if you're attending section I think that will be part of that story. The key idea here is revealing your optimal mixed strategy does not hurt you which is kind of a cool idea. The proof of that is interesting. If you're interested in looking at the notes, you can use linear programming here. So next 10 minutes, I want to spend a little bit of time talking about non-zero-sum games. In real life, you're kind of somewhere in between zero-sum and collaborative games. So, let's motivate that by this idea of Prisoner's dilemma.