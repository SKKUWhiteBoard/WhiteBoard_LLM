The 24th lecture of 188, before last one. The idea behind these two lectures is to look at advanced applications, where we have covered a good amount of the material in the ideas behind those applications. We will not quiz you on these application lectures, on the final exam, or anything like that. It's more meant to give you more perspective rather than extra study materials for the final. So far, I've looked at foundational methods for search, for acting adversarial environments, for learning to do new things, and for dealing with uncertainty, noisy information. DeepMind's AlphaGo can predict who will win from a certain situation in Go. The game is much harder to solve than chess or Tic-Tac-Toe. The branching factor in Go is much larger than in chess. A neural network can be trained to evaluate the value of a position. This gives you a lot of data that can be used to decide who is likely to win in a given situation. The first thing being learned is a policy network, which is deciding which moves to play against. AlphaGo Zero has no prior knowledge. It starts at a pretty, actually, negative elo rating. Loses all the time initially. It's playing itself, but then gets tested against a set of players. After 21 days, it goes past where AlphaGo Master, was which was an improved version of AlphaGo Lee Sedol. And then it was still creeping up after 40 days. Once you reach that level, essentially, there's no further to go, because you solved the game. even longer. It could be that by using human knowledge, you're in some kind of based enough attraction. I don't know if that would be the case or not, but that's a possibility. It also depends on how much randomness you have in your exploration. If you have enough randomness, then initialization will have much less effect than if you have limited randomness. How good can this system get? Is it even possible to learn good Go players by just playing against yourself? That's something people did not have an answer to until this experiment was run. The MDP has four control channels, two in each joystick. A collective is the action for the main rotor collective page. It's the average angle of attack as the blade goes through the air, which modulates how much vertical thrust you generate. The tail rotor has a variable pitch also, and that pitch allows you to modulate how much thrust you get from the tail rotor. You cannot directly ask it to fly forward or sideways. If you want to fly. forward, you've got to rotate nose-down and then you can accelerate vertically. In the RL lecture, we saw this example of a helicopter reliably hovering, which is a very hard problem. Upside down is harder. How do you keep yourself upside down? Well, that main rotor can have a negative angle of attack. If you're flying upside down, it keeps you up in the air. And it's actually more efficient because when you pull in air, you're accelerated. And vertically, the helicopter frame will now be partially forward and partially up, and up compensates for gravity. We use something called dynamic time warping to align two trajectories. Then we re-infer, through probabilistic inference, what the hidden state might be. Keep repeating this till we reach some fixed point, and that will be our target for our helicopter to fly. And then we can run reinforcement learning in simulation to find a good controller and run it on the real helicopter. The controller we learn in simulation is still a little optimistic about really following that path. We can now penalize our award, penalized for deviating from the target. able to look ahead only two seconds, rather than needing to look further. A value function tells us, OK, how good is it to end up here? We also have a reward at each time tick. The algorithm's only this big, so it's pretty fast for something of this size. The fastest we flew this helicopter was close to 55 miles per hour, so almost highway speeds. And ending an inverted hover. So with this methodology, it was possible to fly this helicopter at the level of the best human pilots. would be there, shutting it off if it started doing something weird. So essentially, he would shut it off whenever it starts tilting itself, so it lands on some pretty wide landing gear so it's more stable. But that was the only human input required. He was able to have it learn to hover reliably. We did not push that further to flying those maneuvers. There is some work. If you look at Woody, Woody was shutting things off. Then recently at OpenEye, there's been some work on robots learning to do back flips. And that was kind of one step further. gets a little more power. Maybe a question related to that is, how much power does this thing actually have? This helicopter had inverted slide, where it has more power, 3 Gs. So it can generate three times the power of gravity. Of course, you need to generate one of them to even stay up in the air, but it still had two G's left to do other things with. And regular flight had about 2.5 G's maximum acceleration. OK, let's take a short break here. And after the break,let's do legged locomotion and manipulation. In 2008, a professor and his students tried to teach a robot to walk and drive. The robot fell a lot, which is indicative of how hard it is to do walking with robots. In 2015, there was the Doppler Robotics Challenge, which was held in Pomona, just east of Los Angeles. Now, what's changed recently in the past few years is that through advances in learning, it's been possible to map from raw sensory information to controls, says Andrew Keen. autonomous car drive a desert race. It's a time trial type race. There's no other cars that you have to overtake, but you're kind of on your own and try to do it as fast as possible, 150 mils off road. Well, it's on a road, but it's this kind of road that is not like a regular road. So it's pretty hard to distinguish road from non-road, and if you steer off the road, you might lose your car if you go down some kind of ravine. Four cars finished the 150-mile Berkeley autonomous car race in 2005. What goes onto the cars? There is IMU, like right on a helicopter, a lot of computers. Lasers, where you shoot out laser beams. Cameras, radar, control screen, steering motor. How do you decide with path to follow? Often, your sensor readings will tell you if there might be obstacles or not.. A camera will be better at that than a LIDAR. Somebody needs to tell you what is road, what is not road. The devil is really in the details, in the long tail of special events that can happen when you're driving. In urban environments, there's even more need to recognize, not just road versus not road. A lot of progress has been made this is video from 2013. This was before deep neural networks were heavily used for this kind of thing. It's only getting better to recognize what's in scenes, thanks toDeep neural networks. Instead of classifying into which categories in the image, you would classify each pixel, as to what is in each pixel. That way, you get a semantic segmentation. is between 10 and negative 2 and 10 negative 3 per 1,000 miles of human driving. In green is the Google slash [? wave ?] mode disengagement. It's when the driver decides they want to take control because they don't trust the autonomous system right now to avoid an accident. And we see that it's going down how often that needs to happen, but still a bit removed from where humans are at. Where does this data come from? If you test in California, you have to report this data to the DMV.