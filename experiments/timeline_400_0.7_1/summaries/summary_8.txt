The final contest, which is due tonight, is to design an agent that plays together with another agent to try to collect food pellets while not getting eaten by ghosts. So submissions for that, your last chance to submit are tonight at midnight. And on Thursday in lecture, we'll discuss the results. So today's lecture, as well as Thursday's lecture,. will be mostly on advanced applications. We will not quiz you on these application lectures, on the final exam, or anything like that. It's more meant to give you more perspective. Today's state-of-the-art in Go is that there are computer players better than the best human players. But actually, if you went back to March 2016, that was not the case yet. So how do you make an AI for Go? Let's go back to what we were looking at in lecture on games, MiniMax. MiniMax is about solving games in adversarial environments. And in what it did, we had to update this graph that you see at the very beginning of the course. DeepMind's AlphaGo is a computer program that can play the game of Go. It uses a deep neural network to learn how to search for the best moves. It can also learn to predict who is likely to win from a given situation. DeepMind is now working on a system that can learn to play Go without human help. It's called AlphaGo Zero, and it's being developed by DeepMind's Sergey Brin and Yann Leibovitz. It will be unveiled at the World Economic Forum in Davos this week. AlphaGo Zero has no prior knowledge. It starts at a pretty, actually, negative elo rating. Loses all the time initially. It's playing itself, but then gets tested against a set of players. After 21 days, it goes past where AlphaGo Lee Sedol was. And then it was still creeping up after 40 days. Go, even though it's a master game, in principle, has a solution. Once you reach that level, essentially, there's no further to go, because you solved the game. With reasonable compute power, it traverses the whole tree. Even with alpha beta pruning, I don't think that'll happen anytime soon. It could be that by using human knowledge, you're in some kind of based enough attraction. For a local [? optim, ?] that maybe not as good as another one that might be out there. It also depends on how much randomness you have in your exploration. If you have enough randomness, then initialization will have much less effect than if you have limited randomness. Researchers have created an RC helicopter that can fly autonomously. The system uses a hidden Markov model to learn from scratch. The researchers hope to use the system to teach people how to fly helicopters in the future. They hope the system will also be used to train people to be better Go players, among other things, by teaching them how to play the game without knowing any prior knowledge of the game or how to do it well.. The project is funded by the National Institute of Standards and Technology (NIST) In the RL lecture, we saw an example of a helicopter reliably hovering. Upside down is harder. How do you keep yourself upside down? Well, that main rotor can have a negative angle of attack. If you're flying upside down, it keeps you up in the air. And it's actually more efficient because when you pull in air, you're accelerated. In the video we watched, the helicopter had already been flipped. The pilot flipped the helicopter, toggled autonomous control for inverted flight, and that's what we saw. saw it making these wild motions, overcompensating. It pushed the controls so hard that the engine died. The engine just couldn't push it, died. You lose control over your helicopter, more or less, at that point. Then what happened is our human pilot took back control to try to save the helicopter. And believe it or not, they actually saved this. It landed a little harder than you want to land, but it landed on its feet and it could be recovered from that. the helicopter to follow a path that's not flyable. So what if we collect paths from a human pilot and then ask the helicopter to fly those paths? Well, we could learn the trajectory from these as noisy observations. Hidden Markov models. If we have something we don't know that evolves over time, but we have some noisy measurements of it, we can run an HMM to recover what we actually want. We see here is something that's better than any single one of the demonstrations. We want to take a closer look. able to look ahead only two seconds, rather than needing to look further. A value function tells us, OK, how good is it to end up here? We also have a reward at each time tick. And our search over those two seconds is what results in the control we apply. The fastest we flew this helicopter was close to 55 miles per hour, so almost highway speeds. The algorithm's only this big, so it's pretty fast for something of this size. It was possible to fly this helicopter at the level of the best human pilots. Berkeley student Woody Hoburg took charge in seeing how far we can get without human experts for some simpler things, not for all the maneuvers here. He was able to have it learn to hover reliably with the only human input being shut it off when it looks like it might start doing something dangerous. We did not push that further to flying those maneuvers. There is some work. If you look at Woody, Woody was shutting things off.to this behavior. Then recently at OpenEye, there's been some work on robots learning to do back flips. it has more time, and if it already has a recovery controller, then you can imagine that. And Claire Tomlin's group here at Berkeley has done some work in that direction, where they have a safe controller and a learned controller. And the learning controller is learning on its own while the safe controller keeps things in check so the helicopter doesn't crash. So what we used there is a model-based reinforcement learning method. So we learned a dynamics models for simulator from data that was collected. To learn a good controller in the simulator, we used something called iterative LQR. is a separate linear feedback controller for each time slice. If there is no wind, you can actually just run the linear feedback control. It will be fine. But if there's some wind gusts that could throw you off, you want to use the value functions and the two second look ahead against those value functions to do the controls. Training a unified policy across the entire space might work. It might take some work, exactly, figuring out how to do it. It could be interesting to revisit that now and see what the current understanding of how to train these networks. In 2015, there was the Doppler Robotics Challenge, which was held in Pomona, just east of Los Angeles. The robot had to, essentially, drive a car or walk, but driving the car was recommended. It turned it's very complex to get a robot to do that. The thing is modeling these situations proved even harder than modeling helicopters, because your sensing needs to understand whether or not you're already making contact, and making contact or not. You can be very close, but not have contact. It's a very subtle thing. Stanford robot becomes first vehicle in history to drive 132 miles by itself. Five robots remain on the course. To finish, they must wind through a treacherous mountain pass. After months of tireless effort, there's a lot at stake. The first time it's ever been done, autonomous vehicles. A vision they all share will now be put to the test. The Dartmouth Grand Challenge is held every year at Dartmouth University in Dartmouth, Dartmouth, New Hampshire, and Dartmouth College in Hanover, New York. Four cars finished the 150-mile Berkeley autonomous car race in 2005. What goes onto the cars? There is IMU, like right on a helicopter, a lot of computers. Lasers, where you shoot out laser beams. And based on how long it takes them to get back, you know how far away the nearest obstacle is in that direction. Cameras, radar, control screen, steering motor, usually, you would have a high level planner choosing a path and then a low level controller following that path. A camera will be better at that than a LIDAR. Self-supervision is a trick that's very widely used to reduce labeling efforts. In urban environments, there's even more need to recognize, not just road versus not road. A lot of progress has been made this is video from 2013. This was before deep neural networks were heavily used for this kind of thing. It's only getting better to recognize what's in scenes, thanks toDeep neural networks. The devil is really in the details, in the long term. tail of special events that can happen when you're driving. You can measure progress by just demo videos, which is one way, and it gives you some kind of feel for what's going on. Another way to measure progress is to see how are these cars doing relative to human drivers. If you test in California, you have to report this data to the DMV. It's a number of events per 1,000 miles driven. Red there is human fatalities. Then yellow is human injuries. In green is the Google slash [? wave ?] mode disengagement. so many decisions. If they're gigantic, use a lot of power. That's a problem. Let's see what we can do to build smaller networks to make decisions. What else did we not cover yet? Personal robotics. I want to spend a little more than two minutes on that, so let's keep that for Thursday. that's it for today. Bye. [SIDE CONVERSATIONS] [Side CONversation] [sideconversation.com: Do you know more about this topic? Email us at jennifer.smith@cnn.com].