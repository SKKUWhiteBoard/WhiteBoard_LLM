SNLI is the Stanford Natural Language Inference Corpus-- MultiNLI, and Adversarial NLI. SNLI is a big data set. It has over 550,000 training examples. And it has dev and test sets. Each have 10,000 examples balanced across the three classes. We estimate human performance at 92% for SNLI, because Sam Bowman has been curating that data. We can mark progress on SNLI because we can mark back into our underlying domain that you might have in mind. SNLI and MultiNLI into Turkish. XNLI is a bunch of assessment data sets that is dev-test splits for more than a dozen languages. Those are human-created translations that could be used to benchmark multilingual NLI systems. So there's a wide world of tasks you can explore, and I think that makes NLI a really exciting space in which to develop original systems, and projects, and so forth. And those could be interesting for seeing how well a model can grapple with variation that comes in very specific and technical domains. unanimous gold label. And we rate the overall human level of agreement at about 91.2% for the gold labels. And the overall Fleiss kappa measured interannotator agreement was 0.7, which is a high rate of agreement. And then for the leaderboard, you can check out this link here. Sam has been good about curating all the systems that enter. It's clear at this point, for example, that ensembles of deep learning methods are the best for this problem.