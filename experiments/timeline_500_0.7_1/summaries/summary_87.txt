Differentiable programming is closely related to deep learning. It allows you to build up an increasingly more sophisticated model without losing track of what's going on. The FeedForward function takes in an input vector x and produces an output vector which could be of a different dimensionality. And the way to interpret what people are doing is performing one step of processing. In particular what that processing is, is taking this input vector, multiplying it by a matrix, adding a bias term and applying an activation function. Convolutional neural networks are a refinement of a fully connected neural network. ConvNets have two basic building blocks. The way that Conv takes an image and the image is going to be represented as a volume which is a collection of matrices, one for each channel, red, green, blue. Each matrix has the same dimensionality as the image, height by width. The convolutional network can also be applied to text or sequences which are 1D or videos which are [INAUDIBLE].. Conv is going to compute this volume is via a sequence of filters, and intuitively what it's going to do is try to detect local patterns with [AUDIO OUT] MaxPool takes an input volume and then it produces a smaller output volume. And with these two functions along with FeedForward, now we can define AlexNet which was the seminal CNN from 2012 that won the ImageNet competition and really transformed [INAUDIBLE] So in one line I have AlexNet. Now let's turn our attention to natural language processing. In NLP, words are discrete objects and neural networks speak vectors. So whenever you're doing NLP with neural nets, you first have to embed words, or more generally, tokens. So we're going to define an EmbedToken function that takes a word or a token x and maps it into a vector. And all this function is going to do is it's going to look up vector in a dictionary that has a static set of vectors associated with particular tokens. But the meaning of the words and tokens depends on context. So this representation of the sentence is not going to be a particularly sophisticated one. A simple RNN works by taking an old hidden state, an input, and a new hidden state of the same dimensionality. LSTMs, or long short term memory, were developed to solve this problem. So now we have our sequenced model on RNN which produces a sequence of vectors, and the number of vectors depends on how long the input sequence is. So suppose we want to do classification, we need to somehow collapse that into a single vector. So you can intuitively think about this as summarizing the collection of vectors as one. There's three common things you can do. Transformers are a way of combining different types of networks into one. They can be used to solve problems in language modeling. They use something called self attention, which means that the query is actually going to output the input vectors. So if self attention takes a sequence of input vectors, then it's going to stick the first vector into the query vector for y and then compute the attention, x2 and x4. So in other words, I've basically generated a sequence where all n squared of all the objects, all squared of the vectors, where I've allowed them to communicate with each other. input sequence of vectors and I spit out the corresponding set of contextualized vectors. And the intuition behind this is I'm going to apply f to x safely. So AddNorm of f of x is equal to-- I'm first going to take x and apply f. OK. So now we have enough that we can actually build up to BERT which was this complicated thing that I mentioned at the beginning. BERT is this large unsupervised pretrained model which came out in 2018 which has really kind of transformed NLP. The basic building block for generation is, I'm going to call it GenerateToken. And you take a vector x and you generate token y. And this is kind of the reverse of EmbedToken which takes a token and produces a vector. Then we can take language models and we can build on top of them to create what is known as a sequence-to-sequence model. So this is by and large how a lot of the state of the art methods for, for example, machine translation works. We're generating a translated sentence, given the input sentence.