Today is the day that you have to have done the mid-quarter survey by. Hundreds of people have, but if you haven't, this is your last chance to get the half-point for that. Final project proposals are due. We really encourage you to try and hand them in on-time or nearly on- time. And then today, delighted to have our first invited speaker. And there is a reaction paragraph talking about something that the speaker talks about. There is also assignment 5, which we're giving you one extra day for. Danqi Chen is one of the foremost researchers in question answering. Once upon a time she was the head TA of CS224N. She's quite familiar with the context of this class. So here's my plan for this lecture. I'm going to spend the most of this lecture focused on one type of question answering problems called reading comprehension. So I know that many of you are going to do a different project on the Stanford Question Answering Dataset. So understanding this part will be very crucial for your final project. short, is one of the earliest NLP tasks, and the early systems can even date back to the 1960s. So this system is built to try to find some kind of text matching between the question, and somekind of text segments. And there are many different types of question answering problems. And we also have categories of these question answer problems, based on the information source, or the type of the questions. So for the question part, we can also build systems that can answer factoid questions or non-factoid questions. The question and answer has enabled a lot of really useful real world applications. For example, today if you just put your question in a search engine like Google, it can find a short snippet of text. And then you can actually click on the correct answer, which is actually our concise answer. And those kind of systems are also in use today in a number of other industries. So just have in mind there's many different types of questions in problems and all these problems may require very different techniques or different data. able to handle more complex questions like how-to questions. People actually really like to ask questions on these digital assistants. Ask a question is actually the second most used case. Only ranks after listening to music and before the check the weather and set up. The best way to prevent illness is to avoid being exposed to this virus. And to help prevent the spread of COVID-19, you can do the following. If you just click this link and read through the article. So this is also one type of the question answering problems. timer. So question answering has been really useful in digital assistants. And another very famous example of the question answering system is this IBM Watsonquestion answering system. So in 19-- in 2011, so this IBMatson QA system has been shown to beat two national Jeopardy champions in answeringJeopardy questions. So this is a part of a historical event, at least in the NLP history. So if you look at the inner working of this kind of system more closely, so you can see that it is actually a very complicated and highly modularized system. NLP modules that have included. BERT. And now this system has been over 10 years older, actually, exactly 10 years now. And this is actually representative of state of the art 10 years ago at that time. So we know that this class is about deep learning. So today different has completely really transformed the landscape of the question answering systems. So in this lecture, I will be mostly focusing on the text based, or textual question answering problems. So basically, we are trying to answer questions based on text. the unstructured text. There are many other really big question answering problems. And each of them can be really like a subfield in NLP and they actually have very different challenges and also model designs. So basically we wanted you to question build answering systems to answer questions that can answer questions over a very large database. So to solve this problem, some approaches need to take this question and convert this question into some kind of logic forms. And another class, bigger class of the question Answer problems is called visual question answering. So this problem basically requires both understanding of the questions and also images, and is actually a very active field between the computer vision and NLP. dig into these problems today. So next, I'm going to start with a part 2, reading comprehension. I just want to quickly check if there are any quick questions I can answer before I start us on part 2. OK. So let's talk about the reading comprehension then. So reading comprehension is a basic problem that we want to comprehend a passage of text and answer questions about the content. So here is one example. So basically to answer this question, so you need to find this sentence, like, in 1861, Tesla attended this school where he studied. he studied German, arithmetic, and religion, and only German is a language. So the answer to this question should be German. OK, here is another example, OK? Another passage of text. And the question is, which linguistic minority is larger, Hindi or Malayalam I think 5 seconds. OK. So next I'm going to talk a little bit so why do we care about this problem? So why do you care about the reading comprehension problem? It has actually many useful real-world practical applications. Reading comprehension has been also viewed as a very important test bed for evaluating how well computer systems understand human language. So this is really just similar to how we humans actually test the reading comprehension test to evaluate how well we actually understand one language. And also there is another interesting and important reason that reading comprehension is important. So in recent few years, some researchers actually found that, OK, well, there are many other NLP tasks. So we also reduce them to a reading comprehension problem. personal subject, Barack Obama. So we want to fill in what is-- fill in this question mark and figure out, OK, where Barack Obama was educated at. So one way to solve this problem is basically trying to convert this relation into a question. So where did Barack Obama graduate from? And taking all of that relevant piece of text and then by applying a reading comprehension problem. Then basically, we can find out-- the correct answer should be Columbia University. That is also the output of this information extraction system. Reading comprehension can be very universally useful to many other tasks. So next, I'm going to introduce this Stanford Question Answering Dataset called SQuAD. So if you are going to develop for the final projects, you will need to use this dataset. It consists of 100K annotated passage question and answer triples. So by converting all these kind of semantic role relations, we can also just apply the reading comprehension problem and give you the correct answer. So here is one example from this data set. sets have been also collected, basically runs this size around 100K. So 100K is actually very important to train these neural models. So for these datasets-- so the passages is like a single paragraph selected from the English Wikipedia, which usually consists of 100 to 150 words. And the questions are crowd-sourced, basically like from Mechanical Turking. And this is a very important property of the dataset, is that each answer is a short segment of text, or we called it span in the passage. Stanford, so it's called Stanford Question Answering Dataset. Today, after four or five years now, so SQuAD still remains the most popular reading comprehension data set. So it's actually very clearly a high quality dataset, but is also not a very difficult dataset. So today, basically the SQuad dataset itself has been almost solved, and the state-of-the-art already exceeds estimated human performance. So basically for the development and testing set, there will be three gold answers collected because for some questions, there could be multiple plausible answers. articles and also the punctuation excluded. And basically you can compare the exact match score and also F1 score by comparing the predicted answer to the gold answer. And there are many different examples in the dev or test set. And then finally, we just take the average of all the examples for both the exactmatch and the reference score. So by using this evaluation metric, the estimated human performance estimated by the researchers at the time is. 82.3% and the F1 scores is 91.2. the F1 score would be taking the max. Danqi, one question you might answer is, so if you can do other tasks like named entity recognition or relation extraction by sticking something on top of BERT and fine tuning for it or do it as a question answering, does one or the other method work better and by how much? That's an interesting question. So I haven't really seen the-- OK. So there has been some claim, OK, that all the tasks can be converted into question answering task. But I'm not sure if there is really a very fair comparison. answer to that. So next, I'm going to talk about how to build neural models for reading comprehension. And in particular, how we can build a model to solve the Stanford Question Answering Dataset. I also to just quickly mention that because there are many different papers that actually use different notions to refer to the same thing, so starting from-- so I'mgoing to use the passage, paragraph, and context, and also question and query basic interchangeably. OK. The answer must be a section of text in the passage. So the output can be just written this way. We are going to predict a start and end. So start and then end would be within the range between the 1 and the N. So it's basically just two checkpoints-- sorry, two end points of the answer. So SQuAD has been collected beginning late 2016. So after 2016, there have been-- there are two families of neural role models to solving this SQuad data set. So pre-trained language models for these kind of reading comprehension problems. So here are the two-- the illustration of these two families of the models. So on the left is an LSTM-based models with attention, and on the right is the BERT model. And then we need to fine-tunel this model for the reading comprehension task. So I'm going to walk us through hard to build this model step by step, and hopefully with that, you'll have a good understanding of how this model works. comprehension problem because they really share a lot of similarities. Just like some sequence to sequence model, we need to model the attention between the source sentence and the target sentence. So we only need to train the decoder to generate the target sentences. OK. Next, I'm going to talk about this model called BiDAF. So it stands for Bidirectional Attention Flow for Machine Comprehension. It was proposed by Minjoon Seo and other folks in 2017. It remains one of the most popular reading comprehension models. this model from the bottom to the top, it actually can be decomposed into many different layers. So the idea here is that OK, let's take the context part of the passage in question. We need to encode them separately. So to do this, so this model basically proposed to use a concatenation of the word embedding as well as the character embedding for each word in the context and the query. And then we just concatenate them and pass this to a highway network. LSTM model from another direction. So we just need to concatenate the two hidden representation in two directions. And finally, we can get a contextualized representation for each single word in the context. And we can do a similar thing for the question representation. I also want to quickly mention, because I mentioned the sequence to sequence model, we cannot really do these bidirectional LSTMs for the two sequences because the decoder is an autoregressive model. But because here we don't really care about the generation, so we can just use two bid Directional L STMs to represent the representations. called context-to-query attention. The idea is for each context word, can we find the most relevant words from the question for the query words. And then the second type of attention is called the query to context attention. So here the idea is to choose some context words that are most relevant to one of the queryWords. And this is also why this model is called a bidirectional attention flow because there's a context- to- query attention and there is also a query-to.-context attention. similarity score for every pair of the contextualized vector ci and then for everypair of the question with qj. So this is actually the output from an encoding layer. So the way they do this is basically just taking these metrics, the similarities for sij, for each row. Each row basically correspond to one context word. So here, i actually enumerates over all the context words. And this can give us another attention score of beta i, which captures how important this context word is relevant to this question. Is there a reason why you use both query-to-context and context-to the query attention? Is it sometimes advantageous or OK to use just one? Difficult question. Yeah. So I'm going to show us some operations from this figure, so way we just find both directions can really help. So there'll be some operation studies. So by using one set is useful, but it's sometimes advantageous to use both sets. I hope this answers your question, yeah. It's a good question, yes. just not with us using the both directions. In the bottom right, we sum over i, so why does the i remain in bi? Is that correct or is that a typo there? This is another typo. So the output of gi will actually range from the 1 to N, which is the number of the context words. There are lots of questions about this. What is the rationale for the expression of the gi? How does one come up with such an expression? OK, OK, I'm sorry.  query-to-context attention is trying to measure the importance of these context words with respect to some question words. So by taking the max for each row in this matrix, so it's basically trying to see, OK, which question word is actually most relevant to this context word? And then you can just apply your softmax. And then this will give you a probability that OK, what is the probability of this condition i, would be based on the start position of the final answer string. the dot product between w end and this vector, and this can produce all the probability over all the conditions which predict how likely this position will be the end the position of the answer. So by passing the mi to another bedirectional LSTM, their reasoning is that they're trying to capture some kind of dependence between the choice of the start and end. OK. I'm done with this part, describing the BiDAF model. Any quick questions I can answer? I think you can actually go on. BiDAF model achieved a 77.3 F1 score on SQuAD data set. They found that both attentions in two directions are actually important. If you remove the one direction, the performance will actually drop quite a bit. And the whole model can be just trained in an end to end way from the encoding layer to attention layer to modeling layer and to output layer. And basically all of the models that account at that time between 2016 and 2018 are BiDAF models. the models are actually a very similar ballpark. So numbers range from the highest number here, 79.8 until after the ELMo was introduced, the numbers have actually improved quite a bit. Each model actually improved the previous model by one point or two points. And now here is our attention visualization to show that how this smorgasbord of attention actually can capture the similarity between the question words and the context words. So it show the actual question word here. And each column's matrix basically indicates the attention score, the similarity score that has been learned by this model. those negative scores pretty well, yeah. OK, so next, I'm going to talk about BERT, how to use the BERT model to solve this problem. So BERT is basically a deep bidirectional transformer encoder pre-trained on large amounts of text. And it is trained on the two training objectives, including masked language modeling and the next sentence prediction. So, OK, we can actually use BERT for our reading comprehension. So it's actually very easy and very straightforward. question is how many parameters does BERT-large have? So you can see that they basically just take the questions here and then take the passage here. And also for the questions that we just need to pass the A to a segment embeddings. And then finally, the training loss is also the same. So you basically just try to maximize the sum of the negative log likelihood of both the start and end positions. Well here, the way that they compute thestart and end probability is slightly different. L.hi is the output from the BERT encoder, and then we are training this two Wstart and Wend for these two probability distribution Pstart and Pend. If you just take this BERT model, and by just optimizing all the parameters together, it can give you a very high performance. I will show you BERT in a minute. And even if you use a stronger pre-training models or modern, like a-- stronger models than BERT models, they can even lead to better performance on SQuAD. BERT models are pre-trained while BiDAF models only builtd on top of the GloVe vectors. So here it is very clear that pre-training is a game changer here. Pre-training basically can just change everything and it also gives you a very, very large boost in terms of the performance. But I also want to raise another question. So if we first think of this pre- training, these BiDAf models and BERT model are really fundamentally different? I don't think so, so this is actually my argument. So let's try to see how these two models are actually connected, especially in termsof model design. BERT models can do really well on this kind of reading comprehension data set. But you probably cannot really build a model as big as 110 million parameters or 330 million parameters models. So as we train the model between these family of LSTM models and BERT models. They're called QANet from Google. So that the model actually can perform better than the BiDAF models and other models. But it actually under-performs Bert models a little bit. So just check it out on QANnet.is. pre-training has been so important. So next I will quickly talk about-- OK, a question here is that can we actually even design better pre-training objectives for reading comprehension or question answering? And the answer is actually yes. So this is actually a work I did with Mandar Joshi and other folks one year ago called SpanBERT. So think about this. So for SQuAD and other a lot of extractable reading comprehension datasets, the goal is trying to predict the answer span from the passage of the question. The idea here is that can you try to compress the two end points of answer span to predict all the words in the middle? So this is why it is called SpanBERT. So I encourage you to check out our paper. And this actually really helps a lot, at least for the question answering data set. So as you can see from this figure-- so this is SQuAD 1.1 and this isSQuAD 2.0. And these are many other question answeringData sets. And our BERT is actually just the-- is actually our re-implementation of the BERT model using the same data where we have been trying to train this model for slightly longer. So SpanBERT actually greatly outperformed Google BERT and other BERT basically across all of the datasets. This number has already exceeded even the human performance on SQuAD. Does this means that reading comprehension is already solved? The answer is of course not. So in the recent last couple of years, there's been a lot of work on reading comprehension. a lot of evidence showing that the current systems still perform poorly on adversarial examples or the examples from out of domain distributions. So here is a very classical example proposed by Robin Jia and Percy Liang in 2017. They're trying to just insert a random sentence to the end of the paragraph. And this sentence actually has some overlap with the question. It's actually very similar to this question, but actually the numbers have been changed. And they found that these kind of adversarial example are actually very easy to fool the current system and makes the system to predict the answer to be Jeff Dean. is another paper that actually just came out in 2020. So there has to be a lot of evidence showing the similar things. So today we compute a very good reading comprehension data set on the individual data sets. But these systems trained on one dataset basically cannot really generalize to other datasets. And all the other numbers in this table basically shows that if you train one system on one datasets and then evaluate on another dataset, the performance will drop quite a lot. So it basically really cannot generalize from one dataset to another dataset. Open-domain question answering is a problem that-- so it's different from reading comprehension that we don't assume a given passage. So here, we use the assumptions that we only have access to. And they found that a BERTlarge model cannot solve and basically failed these type of test cases 100% of the time. So I have 10 minutes left. Chris, is there any question I should answer at this point here? I think you can go on. OK. So in the last 10 minutes, I'm going to give you a very, very brief introduction of what is open- domain question answering and what we have been trying to do in the past couple of years. a large collection of documents. So one example is just taking the whole Wikipedia, which has five million articles. And we're going to return the answer for any open-domain questions. So the term here open domains is just in contrast to closed domains that deal with questions under a specific domain. OK, so how can we solve this type of problem? Because for the reading comprehension problem, we just make sure to answer questions based on a simple passage. So this is actually a much more challenging and also more practical problem. by using a retrieval and also a reader framework. So the idea is that let's take a question-- OK, so here, the article is trying to answer questions using a very large collection of documents such as the Wikipedia. So we can just decompose this problem into, as I just mentioned, in our retrieval and the reader component. The retrieval is basically trying to take a large collection. of document D and Q, and is trying. to return a set of documents or set of passages. From let's say 5 million documents. And finally the reader basically takes the question and takes this type of the passages and finally returns the answer. So the idea is very simple, but it's trying to bridge with two things, how to have a bridge as the retrieval and also the reader to do this kind of open-domain question answering. So I'm just going to quickly go over some really exciting ideas that has been happening in the last two years basically. So here is actually-- so this idea has been first proposed in Kenton Lee's paper in 2019 called Latent Retrieval for Weakly Supervised Open Domain Question Answering. This model kind of works really well, and it can largely outperform the traditional IR retrieval models. So here is actually a really nice demo. So again, the database ferries the whole Wikipedia. And you can see that if you ask a question of who tells Harry Potter that he is a wizard in the Harry Potter series. And the system had really found out the correct article should be Harry Potter films series, and then finally gave you the correct answer. OK. So by looking at all these different curves basically using a different number of training examples, so it actually largely greatly outperforms thetraditional IR models. answer, which is exactly what you have seen here from the Google example here. So the answer would be the Rubeus Hagrid, who is actually the person who told tells Harry Potter that he's a wizard. OK, I'm going to skip this slide. And then finally, very quick. Some researchers have demonstrated that maybe you don't even need this retrieval stage. If you just use a very large language model, you can also just use all open-domain questions answering. So this kind of QA model is also called Coastal QA systems. OK. So this is one direction that personally, I'm very excited about. This is actually a new direction that basically shows that maybe for the open-domain question answering, maybe this reader model is not necessary anymore. So instead we can just record all the phrases in Wikipedia using some kind of dense vectors. And by taking a question, you can just encode this question in a single vector and then we canjust do the nearest neighbor search. And then it can directly give you the answer. that for the BERT reader model, you essentially have to run the Bert model at the inference time. This is actually very expensive. Well, you just get a read off the reader model. You can just do the similarities search, you can justDo the nearest neighbor search without running a BERT model. So this could be very fast and they can even run on the CPUs without needing to run a very expensive deep neural network and then it can still run very well, perform very, very well. Today, because she doesn't have a Stanford login, we're going to do questions inside Zoom. So if you'd like to ask a question, if you use the raise hand button, we can promote you. And if you hang around and don't leave the Zoom for more than a few minutes, maybe we'll just promote everybody who's still there into people in the regular Zoom for some bits of discussion. There are now four people who've been promoted. OK. Should I read those questions? Should I look at the chat or? No. Out: How can we train the reading comprehension model using only a small number of training examples? Out: If we can leverage a very large and very powerful pre-trained language model, there is a possibility that we can actually do the question answering well. Out: And also there are some other promising directions, including some other models that could be used in the future. OK. So thank you so much for the lecture today.OUT: Maybe he could start by asking a question, and then the other people that we've promoted. OUT: How small can your training dataset be for you to get reasonable results? unsupervised question answering so by using some kind of approach like some form of unsupervised machine translation, this kind of idea. That can be borrowed-- can borrow the idea from that and can also work pretty well, reasonably well in unsuper supervised question answering. Yeah, also I have seen a lot of works like [INAUDIBLE] showing that synthetic Pure-DSS can also help a lot in boosting the performance if you don't have enough supervised examples. So my question is I guess it's kind of interesting that there's not really that strong of a transfer effect between data sets that are kind of ostensibly similar. Most existing question answering datasets or reading comprehension datasets have been collected from Mechanical Turk. So it is very difficult to avoid some kind of artifact though, like a simple clues or superficial clues-- let's say not superficial but some simple clues that are for the machines to pick up. Natural Questions was a dataset that Google put out about a year and a half ago maybe where they were actually taking real questions from Google search logs and then trying to find answers for them in web documents. But there's some other issues that people like to ask some common questions. in the training set that is not really generalization, right? Yeah, but this is more on the open-domain setting not in the really [INAUDIBLE] here. Do you want to ask a question? Yes. So you mentioned in the last part of the presentation that the reader model may not be necessary and you presented the DensePhrases which also work well on CPUs. So do we know how well it performs on the question and answering datasets compared to other models including BERT and those on computer of course. The goal of this project is trying to ingest all the phrases in the Wikipedia. So these conversations are built using the training set of the question answering datasets. So if we use say a different data set that does not present the information using the structure presented in Wikipedias, this model may not work as well as.kind of the intuition behind the dense phrases apart from the answers will probably be in close proximity. And what if the datasets has answers to a specific question like very far from the actual information? Say, the answers to your question may not resided in close. proximity to the words in the question. the answer? So why do you think the nearest neighbor-- I mean, you always can find something, right? The question is that whether it's close enough or not. So the question is what if in the datasets that the answer is not close enough? Yeah, that's a good question. I don't know. If you really come up with something that is really very far away from all the questions that we have been seeing in the training set, that could be possible. It depend on how the text are formatted. The nearest neighbor search may not work as well as other models. while asking a question if you want. So next person is [AUDIO OUT]. All right. Thank you for taking the time to teach this. My question is kind of quick. So you mentioned work, they brought up a set of relatively simple questions that show how brittle or poor the current models can be, right? I'm curious if that-- yeah, exactly. Did that kind of change the community to improve how to evaluate the models? Because they're actually doing pretty poorly on some of those. thank you for bringing this one up. It's really interesting. Next is-- Hi, thanks for taking the time. In what extent can in context learning help models to be more robust with respect to different domains? Can you tell me what you mean by in context? So like basically you provide the template generated by BERT. And then instead of directly predicting the classes of text classifications, you just use some word to represent that class or predict the wordings there. OK. Thanks. Do you think that in order to solve NLP in the sense that you can perform on par with humans on all NLP tasks it's sufficient to only interact with text data? Or do you think we'll eventually need the sort of experiences and common sense. that you get only from seeing and viewing the world and having a set of interactions that we as humans have? Yeah. I mean, common sense is a very difficult-- even in the context question answering, commonsense is aVery important topic that I think still remains unresolved. solve the easy problems. So all these things need to be resolved. Do you think that the current sort of benchmark data sets are maybe a little bit too easy for- [INTERPOSING VOICES] Or just like that. Yeah. One final thought is that having a lot of transversely trying to have a lot. of humans in the loop of the frameworks to evaluate these kind of systems. Just try to break the current system, come up with some harder questions. OK. Are you still game for a couple more questions? Sure. encountered this paper called Learnable Quantizers, which essentially learns baseless representation for the quantizers jointly with the leads of the network. And while this would be extremely effective if you were to just like, say, train from scratch, I was just sort of curious, do you think there is some way to do this say a pre-trained BERT model or something like that? I had a few ideas with like beam search for instance, but I don't see a very clear way of doing that. Next question is from Danqi, who was one of the co-organizers of the EfficientQA task. Danqi: How concerned should we be about potential encoding sort of biases into these record labels or how we evaluate them, or is that just more of a concern for more open ended questions? John: I'm not sure I have a good answer to that. Some people do a de-biasing of the pre-trained language models, all these things are very important. yeah, I guess I'm just a little worried about who comes up with the test cases? Who determines what the right answer is? I mean, we will have more discussion of toxicity and bias coming up very soon, including actually Thursday's lecture as well as a later lecture, not specifically about QA though. OK. Next person is-- Thank you for the lecture. Yeah, my question is also related to the open domain question answering. So there is some very specific designs like domain server alignments and efficient level disentanglement techniques that has shown some interesting performance on other tasks. people also leveraged similar things for question answering. So I was just wondering to what extent these kind of techniques can work on one group of tasks, not just limited to question answering, but mainly question Answer. I have seem some work sort of trying to learn some kind of disentangled representations that can better generalize to the different domains on adversarial examples. Yeah, I'm not sure. I think we have to try that. But it's a little bit more specific for-- so there's a paper called domain. definitely an interesting point. At least for the work that I have seen so far, it all applied or operated at a very simple sentence classification task. I feel like QA is a more structured task and also handles longer bar sequences. Yeah, so I don't know if it works unless people have tried that. OK then we've got-- and maybe we should call this the last question. Hi, I'm just wondering what is the intrinsic difference between solving question answering with generative models like T5 versus encoders like BERT. So if the retrieval returns like, say, 100 passages, so they have to extract the answer from each of the passages and then finally figure out which one has the highest score. But for the generative model, essentially they are trying to aggregate all the 100 passages and the dense representations together and do the generation jointly. So essentially, you're taking the 100 representations together through the joint generation. So I think that is actually the key difference. So that's why thisGenerative model can do really well compared to the extractive models. These numbers are a little bit confusing. So it's actually basically really on par. Their base perform similarity. So the key difference between the generative model and the extractive model is that for generative models, you can actually leverage more input passage together and do the generation. Is that clear or not? Yes. Thanks. Yeah, otherwise, you should just check out this paper here. So this paper actually explains it pretty well why this model works better than the previousGenerative model. The model is very large, like 11 billion parameters. So the parameters are basically trying to memorize a lot of information that has been.information. So by just taking this input, it has to just rely on the parameters to infer this answer. So it's actually very hard to-- yeah, it's a definite balance between memory and the generalization from it, yeah. All right, thanks. Do you want to call it a night or do you want one more question? It's up to you. Japanese, or Arabic, or some other languages? Another followup question maybe not exactly in your domain expertise is there's a lot of interest in modelling user behavior. And then you can use that to predict how users-- like embed users, so that you can can predict user's actions. How promising do you think that would be? I just want get your thoughts on that. OK. The first question is whether these techniques can be generalized to other languages. I think the answer is yes. collect so many examples for other languages. And there has been also a lot of work trying to do cross-lingual question answering and stuff like that. If we have, actually, I think that the techniques can be generally applied to other languages, he says. "I think that we have a lot to learn from each other," he adds. "We have a long way to go, but I think we're making progress." "We've got a lot more to learn," he says, "and we're getting better at it."