then we are going to continue with the second part of the lecture today which focuses on the problem what actually happens if the gaussian assumption that i have about my constraints doesn't hold you can have multiple reasons why this doesn't holding. As you will see in some small examples having this outliers in your optimization problem is something which hurts dramatically which actually screw up your solution so already a few outliers can lead to a environment model which is completely unusable for doing any navigation task. Where the geometry of what you computed doesn't fit to the real world geometry anymore and one of the questions actually how to handle that.  places may look identical although they are not identical if we hear this lecture room and we're in the next lecture room next door they look very very similar it may be very hard for a robot to distinguish that we are here whatever in room 18 and all in room 16. Having the ability to take into account multi-modal beliefs is actually helpful there's another real world example so this is the intel research lab data set that you have also experienced and if you look to those poses over here in the poses down here how those individual structures match. The original number of constraints i don't know how large it was but i think it's around three thousand something like this along those lines. If you put already ten wrong ones in there it's quite likely to screw up it was one hundred which is still a very small number compared to 3000 or just a small fraction. It will actually end up in dramatic mapping errors so the system is unusable so having good data cessations is really important so already screwing up a small number of places is something which can hurt your optimization. the first attempt to to solve this problem this would be our probability distribution what is the problem with this probability distribution so i say okay some of my constraints are these multiple multimodal constraints i will simply go ahead and implement that. What's the problem that you're going to experience if you make this is some so this is we know how to solve that right this is what we did so far let's say i want to go from here to here what's the Problem that shows up if you do it exactly that way you start coding your stuff. done you know he said oh damn i can't do that what would be the ugliest trick that you can do in order to make that work even worse no that's not quite what you're going to do i mean the sum is kind of the the the bad thing what can i do with the sum instead of the sum i can i can get rid of thesum in some nice way sorry oh the integral of some to the integral actually makes our life typically worth um so that's that's going what's going to fly. individual constraints so when you evaluate the error the evaluation of the error is somewhat more expensive because every constraint can have can be multimodal or bimodal in this case. There's not no big difference in the operation of those systems when you um if you use the red or the. or the or the blue plot this is exactly the trick that is used. If you just want to deal with outliers so the the red one is kind of the inlier and the blue function is the one which is the outlier. that's actually a nice thing so um another thing is it can handle both things at the same time data station errors as well as multimodal constraints. So the combination of outlier rejection and dealing with wrong data associations is actually kind of nice we also can do this obviously in 3d so this is again this data set with the sphere that we have seen before robot moving in a virtual sphere with constraints. It's hard to identify the sphere here and if he increases to 100 outliers this is just whatever a big mess whereas this one still is able to solve those things quite nicely. actually compute these so the main changes we go to this formulation we have the scaling factor over here and we need a good way to compute the scaling Factor so how can we actually do that and there's actually closed form you can derive that under certain properties where you end up with this operation here. If the error increases if you're further away from the mode this is what your error function looks like so the the error is weighted down the further i'm away however we still have a linearization point so if i compute the jacobian over here i still has a jacsobian which drags the system into the right direction. and flatter and flatter gaussian distribution the further the point is actually away there's also one technique which you can find which is also quite easy to implement because you just need to compute the scaling factor and multiply that with your information matrix for every constraint also something you can do quite efficiently. If you have constraint which introduce large errors these are these outliers this can actually screw up the optimization when computing the minimal error configuration. There are different ways of uh kind of row function that we can actually in there and then we're then trying to minimize. here that by changing this function you can't get much better behaviors kind of deciding which function to use for the underlying optimization problem is not on it's not always an easy and easy choice so this requires some expert knowledge some good intuition on coming up with the way with one of those functions. Next week which is the last week of the term i will briefly talk about front ends and give kind of a short summary on what typical front ends exist obviously we're not going to all the details as we did that here.