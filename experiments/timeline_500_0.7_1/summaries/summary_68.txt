Today we're gonna talk a little about learning in the setting of games. Can you still be optimal if you reveal your strategy? It's actually not the size that matters. It's the type of strategy that you play that matters, so just to give you an idea. We'll talk about this in a lot of details towards the end of the class. Today is not Thursday. Tomorrow. For a second, I thought it's Thursday. Um, all right, let's co- Tomorrow. know what to play." But th- this has an unintuitive answer that we are gonna talk about towards the end of the lecture. So just more of a motivating example. Don't think about it too hard. All right. So, so let's do a quick review of games. We were playing for the agent, uh, and the agent was trying to maximize their utility. So we had this minimax tree and based on that, the utilities that are gonna pop up are minus 50, 1 and minus 5. of this weak estimate of your value is going to work well and give you an idea of what to do next. So, so instead of the usual recurrence, what we decided to add this D here, um, this D right here which is the depth that un- until which we are exploring. And then we decrease the value of depth, uh, after an agent and opponent plays. When depth is equal to 0, we just call an evaluation function. So intuitively if you're playing chess, for example, you might think a few steps ahead. how learning is applied to these game settings. And specifically the way we are using learning for these game. settings is to just get a better sense of what this evaluation function should be from some data. And, and that kind of introduces to this, this, um, temporal difference learning which we're gonna discuss in a second. It's very similar to Q-learning. Uh, and then towards the end of the class, we will talk about simultaneous games and non-zero-sum games. can actually, like, roll two dice and based on the outcome of your dice, you move your pieces various, various amounts to, to various columns. Uh, there are a bunch of rules. So your goal is to get all your pieces off the board. But if you have only one piece and your opponent gets on top of you, they can push you to the bar and you have to start again. So, so what are some features that you think might be useful? Remember the learning lecture? Yes. So, so that was my model. And now, the question is where do I get data? Like where and because if I'm doing learning, I got to get data from somewhere. So, so one idea that we can use here is we can try to generate data based on our current policy pi agent or pi opponent. And then from these episodes, we want to learn. These episodes look like state action reward states and then they keep going until we get a full episode. One thing to notice here is, is the reward is going to be 0 throughout the episode until the very end of- end of the game. as a function of w, okay? All right. So, so what do we try to do usually, like when you are trying to do learning? We have prediction, we have a target, what do I do? Minimize the- your error. So I'm gonna write one-half of prediction, minus target squared, this is my squared error. I want to minimize that. So now I have the gradient. What algorithm should I use? I can use gradient descent. All right, so I'm going to update my w. How do we update it? between the two? Yeah, so this is very similar to Q learning. There are very minor differences that you'll talk about actually at the end of this section, comparing it to Qlearning. All right. So, so I wanna go over an example, it's kind of like a tedious example but I think it helps going over that and kind of seeing why it works. Especially in the case that the reward is just equal to 0 like throughout an episode. So it kinda feels funny to use this algorithm and make it work but it work. So I want to just go over like one example of this. do these specific things that you would wanna do or these differentiating factors about it. So, so picking features, it's an art, right, so. [LAUGHTER] All right. So lemme, leMme move forward cause we have a bunch of things coming up. All right so, so this was just an example of TD learning but this is the update that you have kind of already seen. And then a lot of you have pointed out that this is, this is similar to Q-learning already, right? The idea of learning in games is old. People have been using it. In the case of Backgammon, um, this was around '90s when Tesauro came up with, with an algorithm to solve the game. And then more recently we have been looking at the game of Go. So in 2016, we had AlphaGo, uh, which was using a lot of expert knowledge in addition to ideas from a Monte Carlo tree search and then, in 2017, we have AlphaGo Zero, which wasn't using even expert knowledge. between our prediction and our target and try to minimize that error and, and find better W's as we go through. So, um, all right so that was learning and,. and games. Uh, so now I wanna spend a little bit of time talking about, uh, other variations of games. So the setting where we take our games to simultaneous games from turn-based. And then, theSetting where we go from zero-sum to non-zero-sum, okay? All right. This value function, uh, over, um, over our state here. Now, we have this value function that is- do we- we shall use here, I'll just use here. That is again from the perspective of agent A. So, so I'm trying to like get good things for A. In this case it's not at the end [inaudible] ? Uh, yeah. And then this is like a one-step game too, right? So like you're just playing and then you see what you get. So whatever the agent A gets, agent B gets negative of that. stochastic policies. So, so pure strategies are just actions a's. And then you can have things that are called mixed strategies and they are probabilities of, of choosing action a, okay? All right. So here is an example. So if, if you say, well, I'm gonna show you 1, I's gonna always show you1. Then the- if you can, you can write that strategy as a pure strategy. With probability 0 show you 2. I could also come up with a mixed strategy. You could only pull it out to like you're in the si- simultaneous game, you could just bring chance in. time show one, half the time show, show two. And then the question is, what is the value of, of these two policies? How do we compute that? [NOISE] Well, I'm gonna use my payoff matrix, right? So, so 1 times 1 over 2 times the value that we get at 1, 1, which is equal to 2. Ah, so you might be interested in looking at what happens in repeated games. So that opens up a whole set of new questions that you're not discussing in this class. think minimax. So agent B should be min- minimizing this. agent A should be maximizing this. That's, that's what we wanna do. But with the challenge here is we are playing simultaneously, so we can't really use the minimax tree. So, so I'm going to limit myself to pure strategies. So right now I will just consider a setting- very limited setting and see what happens. So then player B is going first, player A is minimizing and then player a is maximizing. If you are playing a mixed strategy, even if you reveal your best mixed strategy at the beginning, it doesn't matter if you're going first or second. For every simultaneous two-player zero-sum game, with a finite number of actions, the order of players doesn'tmatter. So remember, if you play mixed strategy,. your opponent is going to play pure strategy because this is like this the first point that we had before it. All right? So this is kind of the third thing that we just learned, which is von Neumann's Theorem. With probability p, like, if we're doing like ordering, like one of the two answers might- will come out, [inaudible] it'll be either one or two. So, uh, the thing is these two end up being equal. So no matter what your opponent does, like you're gonna get the best thing that you can do. So in expectation when- you're saying when you are choosing p? Yes, so I'm treating p as a variable that I'm deciding, right? The key idea here is revealing your optimal mixed strategy does not hurt you which is kind of a cool idea. The proof of that is interesting. If you're interested in looking at the notes, you can use linear programming here. The reason, kind of the intuition behind it is, is if you're playing mixed strategy, the next person has to play pure strategy and you have n possible options for that pure strategy. So that creates n constraints that you are putting in for your optimization. You end up with a single optimization with n constraints. the pay off matrix. Is that A or B? So, uh, so you have two players A and B. Each one of you have an option. You can either testify or you can refuse to testify. So, so what that, that means is if you look at the, the value function from perspective of player A at the Nash equilibrium at Pi star A and Pi star B is going to be greater than or equal to value of, of any other policy Pi A if you fix Pi B. There's a huge literature around different types of games, uh, in game theory and economics. If you're interested in that, take classes. And yeah, there are other type of games still like Security Games and or resource allocation games that have some characteristics that are similar to things we've talked about. And with that, I'll see you guys next time. All right. So summary so far is we have talked about simultaneous zero-sum games. We talked about this von Neumann's minimax theorem, er, which has like multiple minimax strategies and a single game value.