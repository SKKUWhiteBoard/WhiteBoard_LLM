PHILIPPE RIGOLLET: --124. The fluctuations that are due to the fact that I get different samples every time should somewhat vanish. And so what I want is to have a small bias, hopefully a 0 bias. If this thing is 0, then we see that the estimator is unbiased. So this is definitely a property of the model that I'm looking for. It's a very, very powerful tool. It can be used to improve the quality of education. that we are going to be looking for in an estimator, trying to find them to be unbiased. But we'll see that it's actually maybe not enough. So unbiasedness should not be something you lose your sleep over. Something that's slightly better is the risk, really the quadratics risk, which is expectation of-- so if I have an estimators, theta hat, I'm going to look at the expectation of theta n minus theta squared. And so for different thetas, some estimators are better than others. examples. So am I planning-- yeah. So if I do, for example, X1, Xn, there are iid Bernoulli. And I'm going to write it theta so that we keep the same notation. Then theta hat is the average of Xi's. So what is the bias of this guy? Well, to know the bias, I just have to remove theta from the expectation. So this thing is actually theta, which means that this isn't biased, right? for sum of independent random variables, now it's time to wake up. So we have the variance of something that looks like 1 over n, the sum from i equal 1 to n of Xi. So it's of the form variance of a constant times a random variable. We would like somehow to say that this is the sum of the variances. And in general, we are not allowed to say this, but we are because my Xi's are actually independent. And that's by independence, so this is basic probability. equal to theta, 1 minus theta divided by n. So this is just summarizing the performance of an estimator, which is the random variable. If I really wanted to describe it, I would just say risk, because this is the only risk we're going to be actually looking at. Yeah. This parenthesis should really stop here. So the risk of this guy is what? Well, it's the expectation of x bar n plus theta squared. And we know it's. the square of the variance, so it's 0 squared plus the variance. tell you the entire distribution of this random variable. In a way, this decomposition is really telling you that-- it is really accounting for the bias. And the other thing is actually the variance, so for finite number of observations, what are the fluctuations? All right. Then you can see that those things, bias and variance, are actually very different. So I don't have any colors here, so you're going to have to really follow the speed of the change in the data. the order in which I draw those curves. All right. So let's find-- I'm going to give you three candidate estimators, so-- estimators for theta. So the first one is definitely Xn bar. That will be a good candidate estimator. The second one is going to be 0.5, because after all, why should I bother if it's actually going to being-- right? So for example, if I ask you to predict the score of some candidate in some election, then since you know it's going to been very close to 0. 5, you might as well just throw 0.4. And it'sactually going to cost you 0 time and $0 to come up with that. very helpful if your prediction is telling you this. But if it was something different, that would be a good way to generate some close to 1/2. For a coin, for example, if I give you a coins, you never know. Maybe it's slightly biased. But the good guess, just looking at it, inspecting it, maybe there's something crazy happening with the structure of it, you're going to guess that it's 0.5 without trying to collect information. And let's find another one, which is, well, you know, I have a lot of observations. But I'm recording couples kissing, but I'm on a budget. The bias is 0 and the variance is equal to theta, 1 minus theta divided by n. The second number is better for the other guy, so I will definitely go for this guy compared to this guy. The bias is actually-- just for simplicity, I can think of it as being X1 bar, the average of itself. And I have the variance that's actually n times smaller when I use my n observations than when I don't. So this guy is gone. The risk of this guy is that it will depend on n. So I will just pick some n that I'm happy with just so that I can actually draw a curve. And we'll write it like this. So when theta is very close to 0.5, I'm very happy. When theta gets farther, it's a little bit annoying. And so now I need to plot the function. So this here is the risk of0.5. And at 1, it is going to be 0.25 as well. Theta, 1 minus theta divided by 10 is a curve that goes like this. It takes value at 1/2. It thinks value 1/4. That's the maximum. And then it's 0 at the end. So really, if n is equal to 1, this is what the variance looks like. The bias doesn't count in the risk. The risk of predicting the outcome of a presidential election is 0.5 if you know it's not going to go too far. Rigollet: When you have an unbiased estimator, it's simple. It's just telling you it's the variance, because the theta that you have over there is really-- so in the definition of risk. But if I look at the risk of Xn, all I'm doing is just crushing this curve down to 0.X1. So as n increases,. it's going to look more and more like this. And so now I'm going to have to be very close to 1/2 if I want to start saying that Xn bar is worse than the naive estimator 0.5. can be large. Or you have 0 bias-- you have a bias, but the variance is 0. So you can actually have this trade-off and you can find things that are in the entire range in general. So those things are actually-- those trade-offs between bias and variance are usually much better illustrated if we're talking about multivariate parameters. If I actually look at a parameter which is the mean of some multivariate Gaussian, then the bias is going to-- I can make the bias bigger by forcing all the coordinates of my estimator to be the same. a vector of parameters, a multivariate parameter, the bias increases when you're trying to pull more information across the different components to actually have a lower variance. So the more you average, the lower the variance. As n increases, the variance decreases, like 1 over n or theta, 1 minus theta over n. And so this is how it happens in general. In this class, it's mostly one-dimensional parameter estimation, so it's going to be a little harder to illustrate that. what a confidence interval is. And so we fixed a statistical model for n observations, X1 to Xn. Theta is a subset of the real line, and that's why I talk about intervals. An interval is just a one-dimensional conference region. And it has to be an interval as well. A confidence interval of level 1 minus alpha-- so we refer to the quality of a confidence intervals is actually called it's level. It takes value 1 minusalpha for some positive alpha. is between 0 and 1. The closer to 1 it is, the better the confidence interval. And so for any random interval-- so a confidence interval is a random interval. The bounds of this interval depends on random data. Just like we had X bar plus/minus 1 over square root of n, for example, or 2 over squareroot of n. This X bar was the random thing that would make fluctuate those guys. And now I have its boundaries, but now the boundaries are not allowed to depend on my unknown parameter. The probability that I contains theta is at least 1 minus alpha. So it better be close to 1, because it's really telling you that whatever random variable I'm giving you, my error bars are actually covering the right theta. But I want this to hold true for all possible values of the parameters that nature may have come up with from. So I don't know what my confidence -- my parameter of theta -- is, but I want it to be true. And I really want to emphasize that the randomness is in I. want this-- so there's theta that changes here, so the distribution of the interval is actually changing with theta hopefully. And theta is changing with this guy. So regardless of the value of theta. that I'm getting, I want that the probability that it contains the. theta was actually larger than 1 minus alpha. And so in particular, if it's equal, then I can put some larger than or equal to, which guarantees my asymptotic confidence level. you actually need to use things such as Hoeffding's inequality that hold for every n. So as a rule of thumb, if you use the central limit theorem, you're dealing with a confidence interval with asymptotic level 1 minus alpha. And the reason is because you actually want to get the quintiles of the normal-- the Gaussian distribution that comes from the centrallimit theorem. So this is the formal definition. It's a bit of a mouthful. But we actually-- the best way to understand them is to build them. belongs to the confidence interval is actually 1 minus alpha. Now, if I start plugging in numbers instead of the random variables X1 to Xn, I start putting 1, 0, 0. 1, 1,0, 1. And this guy, the probability that theta belongs to it is not 1 plus alpha. It's either 0 if it's not in there or it's 1 if it is in there. So the sample average, Xn bar, is a strongly consistent estimator of p. one of the properties that we wanted. Strongly consistent means that as n goes to infinity, it converges almost surely to the true parameter. That's the strong law of large number. It is consistent also, because it's strongly consistent, so it also converges in probability, which makes it consistent. We've actually computed its quadratic risk. And now what I have is that if I look at-- thanks to the central limit theorem, we actually did this. We built a confidence interval at level 1 minus alpha. as n goes to infinity to some standard normal distribution. This is by definition of the quintile of a standard Gaussian and of a limit in distribution. So the probabilities computed on this guy in the limit converges to the probability computed on that guy. And we know that this is just the probability that the absolute value of sum is greater than the sum of the squares of n and p1 minus p. That's exactly what I did last time. I'm actually going to use the same notation, limit as n go to infinity, this is the same thing. two ways of getting rid of this. Since we only need this thing-- so this thing, as we said, is really equal. Every time I'm going to make this guy smaller and this guy larger, I'm only going to increase the probability. So what we do is we actually just take the largest possible value for p1 minus p, which makes the interval as large as possible. And so now I have this. I just do one of the two tricks. I replace p1 plus p by their upper bound, which is 1/4. also to some standard Gaussian. We've seen that when we saw Slutsky as an example. And so those two things-- actually, just because I'm taking the limit and I'm only caring about the asymptotic confidence level, I can actually just plug in consistent quantities in there, such as Xn bar where I don't have a p. And that gives me another confidence interval. All right. So this by now, hopefully after doing it three times, you should really, really be comfortable with just creating this confidence intervals. not Slutsky, right? AUDIENCE: That's [INAUDIBLE]. PHILIPPE RIGOLLET: So Slutski's about combining two types of convergence. So if you actually have one Xn that converges to X in distribution and Yn that converge to Y in probability, then you can actually multiply Xn and YN and get that the limit in distribution is the product of X and Y, where X is now a constant. And here we have the constant, which is 1. is no. So here's how you would go about thinking about which method is better. So there's always the more conservative method. The only thing you're losing is the rate of convergence of the central limit theorem. Of course, the price you pay is that your confidence interval is wider than it would be if you were to use Slutsky for this particular problem. Actually, it is always wider, because Xn bar-- 1 minus Xn Bar is always less than 1/4 as well. as much from your data as you can. So it depends on how comfortable and how critical it is for you to put valid error bars. If they're valid in the asymptotics, then maybe you're actually going to go with Slutsky so it actually gives you slightly narrower confidence intervals. Now, if you really need to be super-conservative, then you're going to be going with the P1 minus P. So depends on-- I mean, there's a lot of data in statistics which is gauging. how critical it is for you to output valid error bounds or if they're really just here to be indicative of the precision of the estimator you gave from a more qualitative perspective. So here, there's basically a bunch of errors. There's a theorem called Berry-Esseen that quantifies how far this probability is from 1 minus alpha, but the constants are terrible. And then for Slutsky, again you're multiplying something that converges by something that fluctuates around 1, so you need to understand how this thing fluctuates. if this function is super-sharp, then small fluctuations of Xn bar around this expectation are going to lead to really high fluctuations of the function itself. So that's what the function here-- the function you're interested in is 1 over square root of X1 minus X. So what does this function look like around the point where you think P is the true parameter? Its derivative really is what matters. OK? Any other question. OK. So it's important, because now we're going to switch to the real let's do some hardcore computation type of things. When we do maximum likelihood estimation, likelihood is the function, so we need to maximize a function. And if I give you a function, you need to know how to maximize this function. Sometimes, you have closed-form solutions. You can take the derivative and set it equal to 0 and solve it. But sometimes, you actually need to resort to algorithms to do that. And we'll briefly touch upon it, but this is definitely not the focus of this class. OK. So we'll do a little bit of reminders on those things. estimator, what I'm going to try to do is to give you an insight for what we're actually doing when we do maximum likelihood estimation. So remember, we have a model on a sample space E and some candidate distributions P theta. And really, your goal is to estimate a true theta star, the one that generated some data, X1 to Xn, in an iid fashion. But this thea star is really a proxy for us to know that we actually understand the distribution itself. In a way, what does it mean to have two distributions that are close? It means that when you compute probabilities on one distribution, you should have the same probability on the other distribution pretty much. So what we can do is say, well, now I have two candidate distributions. So if theta hat leads to a candidate distribution, and this is the true theta star, it leads to the true distribution P thetaStar. That's my candidate. As a statistician, I'm supposed to come up with a good candidate. And what I want is that if you actually give me the distribution, then I want when I'm computing probabilities, I know what the probabilities for the other guys are. a probability under theta hat of some interval a, b, it should be pretty close to the probability under. theta star of a, a. And more generally, if I want to take the union of two intervals, I want this to be true. If I take just 1/2 lines, I wants this to is true from 0 to infinity. I want it to betrue for all of them at once. Does that sound like a reasonable goal for a statistician? The total variation distance between probability measures is central to probabilistic analysis. If the total variation between theta and theta prime is small, it means that for all possible A's that you give me, then P theta of A is going to be close to P theTA prime of A. The problem is that we don't know what the total variations is to something that weDon't know. The goal was to spit out a theta hat, which was close such that P thena hat was close to theta star. So here's the strategy. Just build an estimator. are you to compute this maximum over all possible events? I mean, it's just crazy, right? There's an infinite number of them. It's much larger than the number of intervals, for example, so it's a bit annoying. There's actually a way to compress it by just looking at the basically function distance or vector distance between probability mass functions or probability density functions. So I'm going to start with the discrete version of the total variation. Throughout this chapter, I will make the difference between discrete random variables and continuous random variables. integrals. But they're all the same thing, really. So let's start with the probability mass function. This is the function that tells me for each possible value that it can take, the probability that it takes this value. So what I want is, of course, that the sum of the probabilities is 1. And I want them to be non-negative. Otherwise, we can just remove this x from the sample space. And so then I have the total variation distance, I mean, it's supposed to be. the maximum overall sets of-- of subsets of E, such that the probability of A minus probability of theta prime of A-- it's complicated, but really there's this beautiful formula that tells me that if I look at the total variation between P theta and P theTA prime, it's actually equal to just 1/2 of the sum for all X in E. So that's something you can compute. But if I give you just the densities and the original distribution, the original definition where you have to max over all possible events, it is not clear you're going to be able to do that very quickly. what it is doing for you. It's controlling the difference of probabilities you can compute on any event. But here, it's just telling you, well, if you do it for each simple event,. It's actually simple. Now we have also the continuous ones, such as Gaussian, exponential. And what characterizes those guys is that they have a probability density. So the density, remember the way I use my density is when I want to compute the probability of belonging to a group. The probability of X falling to some subset of the real line A is simply the integral of the density on this set. That's the famous area under the curve thing. Since for each possible value, the probability at X-- so I hope you remember that stuff. But essentially, we know that the probability that X is equal to little x is 0 for a continuous random variable, for all possible X's. There's just none of them that actually gets weight. So what we have to do is to describe the fact that it's in some little region. probability that I belong to the interval a, b is just the area under the curve between A and B. If you don't remember that, please take immediate remedy. So this function f, just like P, is non-negative. And rather than summing to 1, it integrates to 1 when I integrate it over the entire sample space E. And now the total variation, well, it takes basically the same form. I said that you essentially replace sums by integrals when you're dealing with densities. Two Gaussian densities, exponential minus x squared. You could actually write it as an integral. Now, whether you can actually reduce this integral to some particular number is another story. So now, you have actually a handle on this thing and you could technically ask Mathematica, whereas asking MathematicA to take the max over all possible events is going to be difficult. All right. So the total variation has some properties. So let's keep on the board the definition that involves, say, the densities. And you have two Gaussians, one with mean theta and one withmean theta prime. This guy is computing the absolute value of the difference between f and f theta prime. You can check for yourself that graphically, this I can represent as an area not under the curve, but between the curves. So this thing here, this area, this is 2 times the total variation. The scaling is the same as the scaling of the Gaussians. It's just saying, well, if I have-- so think of two Gaussian. For example, I have one that's here and one that is here. 1/2 really doesn't matter. It's just if I want to have an actual correspondence between the maximum and the other guy, I have to do this. So this is what it looks like. And so we have a couple of properties that come into this. The first one is that it's symmetric. TV of P theta and P thea prime is the same as the TV between P theTA prime and P Theta. Well, that's pretty obvious from this definition. I just flip those two, I get the same number. If TV is equal to zero, then the two distributions, the two probabilities are the same. The only way I can integrate a non-negative and get 0 is that it's 0 pretty much everywhere. And so what it means is that the two densities have to be the samePretty much everywhere, which means that the distributions are theSame. But this is not the case. This is not a zero number. It's not a negative number. The fact that it is in an area tells me it's going to be non- negative. really the way you want to do this, because you have to understand what pretty much everywhere means-- which I should really say almost everywhere. But let's go to this definition-- which is gone. That's the one here. The max of those two guys, if this maximum is equal to 0-- I have a maximum of non-negative numbers, their absolute values. Their maximum isequal to 0, well, they better be all equal to0, because if one is not equal to 1, then the maximum is not 1. The total variation equal to 0 implies that P theta is equal to P theTA prime. If this thing being small implied that P. theta could be all over the place, that would not help very much. The fact that you need two definitions of the [INAUDIBLE],, is it something obvious or is it complete? PHILIPPE RIGOLLET: I'll do it for you now. So let's just prove that those two things are actually giving me the same definition. going to do is I'm actually going to start with the second one. And I'm going to write-- I'mgoing to begin with the density version. But as an exercise, you can do it for the PMF version if you prefer. I just don't want to have to write indices all the time. So think of this as being f sub theta, and Think of this guy as beingf sub thea prime. So I'm Going to Start with this thing, the integral of f of X minus g of X. PhilipPE RIGOLLET: Let me write the set A star as being the set of X's such that f of X is larger than g of X. So that's the set on which the difference is going to be positive or negative. Rigollet: This, again, is equivalent to f ofX minus g ofX is positive. Everybody agrees? So this is the set I'm interested in. So now I'm going to split my integral into two parts, in A, A star. RIGOLLET: It is. Just look at this board. So this is definitely at most the maximum over A of Pf of A minus Pg of A. Right? We agree with this? Because this is just for one specific A, and I'm bounding it by themaximum over all possible A. So that's clearly true. So now I have to show you that the max is actually this guy, A star. So why would that be true? Well, let's just inspect this. thing over there. We want to show that if I take any other A in this integral than this guy A star, it's actually got to decrease its value. So we have this function. I'm going to call this function delta. And what we have is-- so let's say this function looks like this. Now it's the difference between two densities. It doesn't have to be non-negative. But it certainly has to integrate to 0. And so now I take this thing. And the set A star is the set over which the function delta is non- negative. So that's just the definition. phillipPE RIGOLLET: The integral of f minus g-- the integral of delta is 0. So there's actually exactly the same area above and below. It's actually still be true, even if there was-- if this was a constant, that would still betrue. But yeah, you're right. You could go to the extreme cases. And that's true for all A. Yeah? AUDIENCE: [INAUDIBLE] could you use like a portion under the axis as like less than or equal to the portion above the axis? when I take A to be the set where it's positive. Just need to make sure that there is someplace where it is, but that's about it. So it's a distance. It's symmetric, non-negative, equal to 0, if and only if the two arguments are equal, then it satisfies the triangle. If it's not satisfying this thing, it's called pseudo-distance or quasi-distance. Or just metric or nothing at all, honestly. So we have this notion of distance between probability measures. estimator is. And I'm going to try to minimize this estimator now. And if the two things are close, then the minima should be close. That's a pretty good estimation strategy. The problem is that it's very unclear how you would build this. estimator of TV, of the Total Variation. So building estimators, as I said, typically consists in replacing expectations by averages. But there's no simple way of expressing the total variation distance as the expectations with respect to theta star. The surrogate for total variation distance is actually called the Kullback-Leibler divergence. It has some roots coming from information theory, which I will not delve into. But if any of you is actually a Core 6 student, I'm sure you've seen that in some-- I don't know-- course that has any content on information theory. All right. So the KL divergence between two probability measures, P theta and P theTA prime-- and here, as I said, it's not going to be the symmetric, so it's very important for. you to specify which order you say it is, between P theta and P thea prime. And so we denote it by KL. And the KL, if I use the same notation, f and g, is integral of f of X over g of X, dx. It's a weird function. OK. So this was what we had. That's the TV. And then we replace this absolute value of the distance divided by 2 by this weirdfunction. This function is P. theta, log P thena, divided by P theda prime. Thatâ€™s the function. bit different. And I go from discrete to continuous using an integral. Everybody can read this. Everybody's fine with this. Is there any uncertainty about the actual definition here? So here I go straight to the definition, which is just plugging the functions into some integral and compute. So I don't bother with maxima or anything. I mean, there is something like that, but it's certainly not as natural as the total variation. Yes? AUDIENCE: The total variation, [INAUDIBLE].. of differences between probability density function, at least for the probability density functions we're used to manipulate is actually a nightmare. So people prefer KL, because for the Gaussian, this is going to be theta minus theta prime squared. And so those things are much easier to manipulate. But it's really-- the total variation is telling you how far in the worst case the two probabilities can be. This is really the intrinsic notion of closeness between probabilities. So that's really the one-- if we could, that's the one we would go after. here, it has a very specific meaning. If I tell you the KL divergence is 0.01, it's not clear what it means. OK. So what are the properties? The KL divergence between P theta and P theTA prime is different from the KL divergences between theta prime and theta theta in general. Of course, in general, because if theta is equal to thetaprime, then this certainly is true. So there's cases when it is not true. The divergence is non-negative. compared to the convex function of the expectation of a random variable. If you know Jensen, have fun and prove it. What's really nice is that if the KL is equal to 0, then the two distributions are the same. And that's something we're looking for. Everything else we're happy to throw out. So they're not symmetric. It does satisfy the triangle inequality in general. But it's non-negative and it's 0 if and only if the two Distributions are the Same. The total variation distance is actually an expectation of something. The divergence is doing a pretty good thing for us. And this is what will allow us to estimate it and basically overcome what we could not do with the total variation. It's the integral of some function against a density. That's exactly the definition of an expectation, right? That's what we're trying to get at here. We're looking for a way to get a distance from P theta to P prime. We can get it by taking the average of the two possible values it can take. So this is the expectation of this particular function with respect to this density f. And I could actually replace the expectation by an average and try to minimize here. The problem is that-- actually the star here should be in front of the theta, not of the P, right? That's P theta star, not P star theta. But here, I still cannot compute it, because I have this P theTA star that shows up. I don't know what to do. it is. And that's now where the log plays a role. If you actually pay attention, I said you can use Jensen to prove all this stuff. You could actually replace the log by any concave function. That would be f divergent. That's called an f divergence. But the log itself is a very, very specific property, which allows us to say that the log of the ratio is the ratio of the log. If I change theta, this thing is never going to change. It depends only on theta. respect to theta star, log of P thetaStar of X. And then I have minus expectation with respect to thea star of log ofP theta of x. Now as I said, this thing here, this second expectation is a function of theta. When theta changes, thisthing is going to change. And that's a good thing. We want something that reflects how close theta and theTA star are. But this thing is not going to changed. This is a fixed value. theta star is and what P theta is. And so what I'm doing is I'm replacing any-- I can actually-- this is a very standard estimation method. You write something as an expectation with respect to the data-generating process of some function. And then you replace this by the average of this function. The law of large numbers tells me that those two quantities should actually be close. Now, it doesn't mean that's going to be the end of the day, right. The minimizer is always this guy, regardless of what the value is. Every time it's a translation on the y-axis of all these guys. And the value that I translated by depends on theta star. I don't know what it is. But it's with respect to theta, so without loss of generality, I can assume that it's this guy. OK? So when I say constant, it's an unknown constant. It's a constant with respectto theta. It might very well be this function or this function. this constant is 0 for my purposes, or 25 if you prefer. All right. So we'll just keep going on this property next time. And we'll see how from here we can move on to-- the likelihood is actually going to come out of this formula. Thanks. Back to Mail Online home. back to the page you came from. Back from the page where you came From. Click here to go to the show page where we'll be talking about the probability of winning the lottery.