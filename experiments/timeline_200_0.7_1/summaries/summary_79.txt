Reinforcement learning involves the idea of a model, a value, and a policy. A policy is a mapping from the state you're in to what is the action, um, to take. A model is a representation of the world and how that changes in response to agent's accident. A value is the expected discounted sum of rewards from being in a state and/or an action, and then following a particular policy. Markov Decision Processes is where we think about an agent interacting with the world. The Markov Process is to say that the state that the agent is using to make their decisions is a Markov state. The Markov process involves taking actions that affect the state of the world in some way, and then the agent receives back a state and a reward. Today, we're going to think of an agent, just focusing on the current state, um, so the most recent observation, like, you know, whether or not the robots laser range finders saying, that there are walls, to the left or right of it. decisions, is the sufficient [NOISE] statistic of the history. Um, but the idea is that, you might have a stochastic process that's evolving over time. And you could think of that as a Markov Process. And so essentially, it allows us to say that, the future is independent of the past given some current aggregate statistic about the present. The next time step. Here we're using t to denote time step, and the action that is taken a_t, is again the action. Markov Chain is a sequence of random states, where the transition dynamics satisfies this Markov property. If you have a finite set of states, you can just write this down as a matrix. So if we go back to the Mars Rover example that we talked about last time, we thought of a Mars Rover landing on Mars and there might be different sorts of landing sites. And then, it can go to the left or right, um, er, under the Markov process. of be, passively observing how the stock market for a particular, th- the stock value of a particular stock is changing over time. different actions or we could just think of those actions as being a_1 or a_2, where it's trying to act in the world. In this case, the transition dynamics looks like this, which says that, for example, the probability that I start in a particular state s_1, um, and then, I can transition to the next state on the next time step is 0.4. There is a 0.6 chance that I stay in the same state on  the next time step. [NOISE] that is independent of the start state, if you run it for long enough. So in this particular case, you could have it as, um, the transition of saying, "If you start in state, [NOise] uh, let me make sure that I get it right" So, you would have your initial state. So 1, 0, 0,. 0, 1, 2, 3, 4, 5, 6, and then times P, and that would give you your next state distribution. what are the probabilities computed of, like the rewards, I guess, the probability, based on the reward of going from state 1 to 2 [NOISE] or? Great question, so was, you know, one of this transition probabilities looking at Markov Chains. This is specifying that there's some state of the, uh, of the process. So it's as if you're, let's say your agent, um, had some configuration of its motors. And what this would say is, this is the transition probabilities of if that agent starts in state. this is how, yeah, this is how the world works. So we're assuming right now, the, this Markov process is a state of the world that you were, there is some the, the environment you're in is just described as a Markov Process, and this describes the dynamics of that process. We're not talking about how you would estimate those. This is really as if, this are how that world works, like the world of the fake little Mars Rover. So, let's say that your initial starting state is S four, and then you could say, well, I can write that as a one-hot vector. I multiply it by my probability. And that gives me some probability distribution over the next states that I might be in and the world will sample one of those. So, for example, if we were looking at state s_1, it has a 0.6 chance to abstain in s_ 1 or 0.4 chance of transitioning. It's like sampling from sort of a probability distribution. like before. But now we also have a reward function. So, again just like before, if we have a finite number of states in this case R can be represented in matrix notation which is just a vector because it's just the expected reward we get for being in each state. If we look at the Mars Rover MRP, then we could say that the reward for being an s_1 is equal to 1. The reward forBeing an s-7 isequal to 10 and everything else that reward is zero. rewards for the Markov Decision Process can either be a function of the state, the state in action, or state action next state. Right now we're still in Markov Reward Processes so there's no action. So, in this case, the ways you could define rewards would either be over the immediate state or state and next state, for example. Once we start to think about there being rewards, we can then think about returns and expected returns. We talked about those briefly last time, but it often we think about the case where, um, an agent might be acting forever. long time. The definition of a return is just the discounted sum of rewards you get from the current time step to a horizon and that horizon could be infinite. If the process is deterministic, these two things will be identical. But in general if theprocess is stochastic, they will be different. So, what I mean by deterministic is that if you always go to the same next state, no matter which if you start at a state if there's only a single next state you can go to, uh, then the expectation is equivalent to a single return. General case, we are gonna be interested in these stochastic decision processes which means averages will be different than particularly runs. So, for an example of that well, let me first just talk about discount factor and then I'll give an example. Discount factors are a little bit tricky. They're both sort of somewhat motivated and somewhat used for mathematical convenience. Uh, people empirically often act as if there is a discount factor. We weigh future rewards lower than, than immediate rewards typically. Businesses often do the same. convenience, um, if your horizon is always guaranteed to be finite, it's fine to use gamma equal to one in terms of from a perspective mathematical convenience. There, one could try using other participant is certainly the most common one and we'll see later why it has some really nice mathematical properties. Any other questions? Okay. So, what would be some examples of this? Um, if we go back to our Mars Rover here and we now have this definition of reward,Um, what'd be a sample return? So, let's imagine that we start off in state s_4 and then we transitioned to s_5, s_6, s-7. There's often a bounded number of time you know bounded length of course in many many cases that the horizon is naturally bounded. So, in this case you know what might happen in this scenario we start off in s_4. Um, and then on time-step s_7 we get a reward of 10. But that has to be weighed down by the discount factor which here is 1/2. And so the sample return for this particular episode is just 1.25. And of course we could define this for any particular, um, episode. other cases, um, it might go all the way to the left. So, if we then think about what the expected value function would be, it would involve averaging over a lot of these. And as we average over all of these, Um, then we can start to get different rewards for different time steps. Um, how would we compute this? One thing you could do which is sort of motivated by what I would just showing before, is that you could estimate it by simulation. And that would asymptotically converge to what the value function is. um, and there are mathematical bounds you can use to say how many simulations would you need to do in order for your empirical average to be close to the true expected value. The accuracy roughly goes down on the order of one over square root of N where N is the number of roll-outs you've done. So, it just tells you that, you know, if you want to figure out what the value is of your Markov Reward Process, you could just do simulations and that would give you an estimate of the value. roll out in the world then you can get these sort of nice estimates of really how the process is working. But it doesn't leverage anything about the fact that if the world really is Markov, um, there's additional structure we could do in order to get better estimates. So, the value function of a mark forward process is simply the immediate reward the agent gets from the current state it's in plus the discounted sum of future rewards weighed by the discount factor times the- and we can just express it with V, V(s'). to some state s' Um, and then you're going to get the value of whatever state you ended up in discounted by our discount factor. So, if we're in a finite state MRP we can express this using matrix notation. Um, we can say that the value function which is a vector is equal to the reward plus gamma times the transition model times V. And the nice thing is that once we've done that we can just analytically solve for thevalue function. The question was was if it's possible to have self-loops? Um, could it be that this is sort of circulator defined [NOISE] in this case? So, if one of the transitions can be back to itself, um wouldn't it be become a circular to try to express V in terms of V(s)? And if you have N states, it's fine if some of the states that you might transition back to the same state there's no problem. You do need that this matrix is well-defined. so let's say you have N states there's generally on the order of somewhere between N squared and N cubed depending on which matrix inversion you're using. Is it ever actually possible for, uh, that matrix not to have an inverse or does like the property that like column sum to one or something make it not possible? Question was is it ever possible for this not to has an inverse? Um, it's a it'sA good question. I'm trying to think whether or not that can be violated in some cases. The idea in this case is because of the Markov property, we've said that the value of a state is exactly equal to the immediate reward we get plus the discounted sum of future rewards. In this case, we can simply use that to derive an iterative equation where we use the previous value of the state in order to bootstrap and compute the next value. So, the advantage of this is that each of the iteration updates are cheaper and they'd also will be some benefits later when we start to think about actions. A Markov Decision Process is typically described as a tuple which is just the set of states, actions, rewards, dynamics, model, and discount factor. So, if you think about serve an observation you'd see something like this s, a, r, and then transition to state s'. And so a Markov decision process is typicallydescribed as a tuples which are just the sets of states,. actions, Rewards, dynamics and model. And so for most of the rest of today we'll be using that it's the function of both the state and action. in a state in K action, why is it deterministic what the next state is? Question is same like well why is this- why are there stochastic processes I think. Um, there are a lot of cases where we don't have perfect models of the environment. May be if we had better models then things would be deterministic. And so, we're going to approximate our uncertainty over those models with Stochasticity. So, maybe you have a robot that's a little bit faulty and so sometimes it gets stuck on carpet and then sometimes it goes forward. A Markov Decision Process policy specifies what action to take in each state. The policies themselves can be deterministic or stochastic, meaning that you could either have a distribution over in the next action you take. So, if we think about our Mars Rover MDP. Now, let's just define there being two actions A1 and A2. You can think about these things as the agent trying to move left or right but it's also perhaps easier just to think about in general them as sort of these deterministic actions for this particular example. If you have an MDP plus a policy then that immediately specifies a Markov Reward Process. So, in order to learn what is the value of a particular policy we instantiate the reward function by always picking the action that the policy would take. And then for whatever state I end up by next continuing to follow this policy. So that's what the V^pi_k-1 function is for a deterministic policy. And this is also known as a bellman backup for a specific policy. specifies. What would happen if the expected discounted sum of rewards we get by continuing to follow policy from whatever state we just transitioned to. So, imagine your policy is always to do action a_1 and your discount factor is zero. In this case, what is the value of the policy and this is just to remind you of what like the iterative way of computing it would be. Um, and I think that will be zero for everything except s-1 and s-7 where it's +1 and +10. computation. And it just requires plugging in what is the value of the reward. The value is and- and the particular numbers for the dynamics and the old value function. So as we do these iterations of policy evaluation, we start to propagate the information about future rewards back to earlier states. So if you think about looking at this, that's with information of the fact that state s_7 is good, is going to kinda flow backwards to the other states because they're saying "I might reach that really great +10 state" So what if you just uh, let's ask a question then we can all take a second to uh. I'm just wondering, er, if repeating the same process to find the value function. So what we've done here is we've said, we've initialized the valuefunction to be zero everywhere. That is not the real value function, that just sort of an initialization. And what this process is allowing us to do is we keep updating the values of every single state until they stop changing. And then that gives us the value. expected discounted sum of rewards. Now you might ask, okay well they- are they ever guaranteed to stop changing? And we'll get to that part later. The whole process is guaranteed to be a contraction so it's not going to go on forever. So the distance between the value functions is going to be shrinking. And that's one of the benefits of the discount factor. So if people don't have any more immediate questions, I suggest we all take a minute and then just compare with your neighbor of what number you get when you do this computation. equal to one. If your discount factor is less than one, then I which is the identity matrix minus gamma times P is always going to be invertible. So that's just an example of, um, how you would compute one Bellman backup. And that's back to my original question which is you seem to be using V_k without the superscript pi to evaluate it. Oh, sorry this should, yes. This should have been pi. That's just a typo. to be pi up there. Yes it was, thanks for catching. All right, so now we can start to talk about Markov Decision Process control. Control here is going to be the fact that ultimately we don't care about just evaluating policies, typically we want our agent actually be learning policies. And so in this case we're not going to talks about learning policies, we're just going to talking about computing optimal policies. So the important thing is that there exists a unique optimal value function. before we do this let's think about how many policies there might be. So there are seven discrete states. In this case it's the locations that the robot. There are two actions. I won't call them left and right, I'm just going to call them a_1 and a_2. Then the question is how many deterministic policies are there and is the optimal policy for MDP always unique? So kind of right we just take like one minute or say one or two minutes feel free to talk to a neighbor. This- what this how many policies there are and whether maybe- there maybe it looked like it was going to be linear and it's actually exponential. Um, the way that we're defining a decision policy here, um, a deterministic decision policy is a mapping from a state to an action. And so that means for each state we get to choose an action and so just as an illustration of why this ends up being exponential. So, you could either have action a_1-a_1, you can have action s_1 and s_2. simplicity, we're going to assume that all actions are applicable in all states. Um, in reality that's often not true. In many real-world cases, um, some of the actions might be specific to the state. Ah, for totally, there's a huge space of medical interventions. For right now, I think it's simple as just to think of it as there's one uniform action space and then they can be applied in any state. Okay. So, the optimal policy for an MDP and a finite horizon problem where the agent acts forever. is indexing which policy we're at. We evaluate the policy using the same sorts of techniques we just discussed because it's a fixed policy. A state action value says well, I'm going to follow this policy pi but not right away. So, that defines the Q function and what policy improvement does is it says okay you've got a policy, you just did policy evaluation and you got a value of it. Now I want to see if I can improve it. Noise. now I compute Q^pi which says if I take a different action, it could be the same. So, for all A and all S we compute this and then we're going to compute a new policy and this is the improvement step which maximizes this Q. Now, by definition this has to be greater than or equal to Q^πi(s, pi_i(a)), right, because either a is equal to pi_ i(a), sorry pi_I(s). So, either you the arg max is going to be same as that your previous policy π_i. maybe do some local monotonic improvement maybe, um but are we going to be susceptible to gain stuck. Um, in fact, ah for any of you that have played around with reinforcement learning and and policy gradient and stuff that is exactly one of the problems that can happen when we start doing gradient based approaches nicely in this case. So, we're guaranteed to converge to the global optima and we'll see why for a second. All right. So this is how it works. You do this policy evaluation and then you compute the Q function and thenYou compute the new policy that takes an arg max of the Qfunction. that quantity for each state. But then- so that's going to just define a new policy, right? Like I thought that might be the same or it could be a, a different policy than the one you've had before. Here's the weird thing. So, this is saying that if you were to follow that arg max A and then follow your old policy from then onwards, you will be guaranteed to be doing better than you were before. But the strange thing is that we're not gonna follow the old policy. We are going to follow this new policy for all time. that my value would be better than before. But what you really want is that this new policy is just better overall. And so the cool thing is that you can show that by doing this policy improvement it is monotonically better than the old policy. So, this is just saying this on a words, we're saying, you know, if we took the new policy for one action, then follow pi_i forever then we're guaranteed to be at least as good as we were before. strict inequality if the old policy was suboptimal. So, why does this work? So, it works for the following reasons. Let's go ahead and just like walk through the proof briefly. Okay. This is- what we've said here is that, um, V^pi_i(s), that's our old value of our policy. Has to be less than or equal to max a of Q#pi. Is equal to R(s, pi_i+1(s) The next questions that might come up is so we know we're gonna get this monotonic improvement, um, so the questions would be if the policy doesn't change, can it ever change again? And is there a maximum number of iterations of policy iteration? So, what do I mean by iterations? Here iterations is i.pi_i+1 value is by definition at least as good as the previous value function. So, why don't we take like a minute and just think about this maybe talk to somebody around you that you haven't met before and just see what they think. The idea in policy iteration is you always have a policy, that is- that you know the value of it for the infinite horizon. And then you incrementally try to improve it. Value iteration is an alternative approach. It says you always know what the optimal value in policy is, but only if you're gonna get to act for say k time steps. So they're just- they're computing different things, um, and they both will converge to the same thing eventually. It's useful to think about Bellman. work? The algorithm can be summarized as follows. You start off, you caninitial your value function to zero for all states. And then you loop until you converge, um, or if you're doing a finite horizon, which we might not have time to get to today. And basically, for each state, you do this Bellman backup operator. So you'd say, my value at k plus one time steps for that state is if I get to pick the best immediate action plus the discounted sum of future rewards. basically say what is the optimal immediate action you should take if you only get to take one action. So policy evaluation you can think of as basically just computing a fixed point of repeatedly applying this Bellman backup until V stops converging and stops changing. And so, if you see sort of a B with, um, ah, pi on top and saying, well, instead of taking that max over actions, you're specifying what's the action you're going to take. Wanna see if we can get to a little bit of that? on sort of the contraction operator. So, for any operator, um, let's let O be an operator and x denote a norm of x. So x could be a vector like a value function and then we could look at like an L2 norm or an L1 norm or L infinity norm. If an operator is a contraction it means that if you apply it to two different things, you can think of these as value functions. So just to, um- actually, I'll save examples for later. But this is the formal definition of what it means to be a contraction. doesn't get bigger and can shrink after you apply this operator. So, the key question of whether or not value iteration will converge is because the Bellman backup is a contraction operator. And it's a contraction operators as long as gamma is less than one. So how do we prove this? Um, we prove it- for interest of time I'll show you the proof. Again, I'm happy to go through it, um, I- or we can going through it in office hours et cetera. value functions and then we re-express what they are after we apply the Bellman backup operator. So there's that max a, the immediate reward plus the discounted sum of future rewards. And then the next thing we can do is we can bound and say the difference between these two value functions is diff- is, um, bounded by the maximum of the distance between those two. And so that means that theBellman backup as long as this is less than one has to be a contraction operator. between the two value functions can't be larger after you apply the Bellman operator than it was before. There has to be a unique solution. It's also good to think about whether the initialization and values impacts anything if you only care about the result after it's converged. All right. Class is basically over. There's a little bit more in the slides to talk about, um, the finite horizon case, and feel free to reach out to us on Piazza with any questions.