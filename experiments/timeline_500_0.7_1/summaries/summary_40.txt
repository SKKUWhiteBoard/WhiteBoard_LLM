Danqi Chen is one of the foremost researchers in question answering. She is the professor at the Princeton University. Danqi once upon a time was the head TA of CS224N. She's quite familiar with the context of this class. So today I'm very happy to introduce to you some of the fundamentals in this field, as well as on cutting edge and state of the art topics. So here's my plan for this lecture. I'm going to spend the most of this lecture focused on one type of question answering problems called reading comprehension. like 10-ish minutes to talk about a more practical, and in my opinion, more exciting problem called open domain question answering. Question answering, or, let's say QA in short, is one of the earliest NLP tasks, and the early systems can even date back to the 1960s. And, the question and answer has enabled a lot of really useful real world applications. For example, today if you just put your question in a search engine like Google, you can actually click on the correct answer, which is actually our concise answer. Question answering is probably one of those fields that we have seen the most remarkable progress in the last couple of years driven by deep learning. So in this lecture, I will be mostly focusing on the text based, or textual question answering problems. Another class, bigger class of the question Answer problems is called visual question answering. So if you have interest in these type of problems, I encourage you to check out those problems, but I'm not going to dig into these problems today. So next, I'm going to start with a part 2, reading comprehension. Reading comprehension has been viewed as a very important test bed for evaluating how well computer systems understand human language. This is really just similar to how we humans actually test the reading comprehension test to evaluate how well we actually understand one language. So this is also the way that we actually post questions to test the machine's language understanding ability. It actually has been formally stated back in 1977 by Wendy Lehnert in her dissertation. She says that, since questions can be devised to query any aspect of text comprehension, the ability to answer questions is the strongest possible demonstration of understanding. is actually called a semantic role labeling. So basically try to-- given one sentence, given one word, "finish" trying to figure out who did what to whom and when and where. So by converting all these kind of semantic role relations, we can also just apply the reading comprehension problem and give you the correct answer. So this is actually a very interesting perspective, that reading comprehension can be actually very universally useful to many other tasks. So next, I'm going to introduce this Stanford Question Answering Dataset called SQuAD. Stanford, so it's called Stanford Question Answering Dataset. Today, after four or five years now, so SQuAD still remains the most popular reading comprehension data set. So it's actually very clearly a high quality dataset, but is also not a very difficult dataset. So today, basically the SQuad dataset itself has been almost solved, and the state-of-the-art already exceeds estimated human performance. Danqi, one question you might answer is, so if you can do other tasks like named entity recognition or relation extraction by sticking something on top of BERT. method work better and by how much? That's an interesting question. So the kind of state-of-the-art AER systems still have time to just train or sequence tagger on top of the word. So I want to draw some connections between the machine translation problem and the reading comprehension problem because really share a lot of similarities. So next, I'm going to talk about how to build neural models for reading comprehension, and in particular, how we can build a model to solve the Stanford Question Answering Dataset. BiDAF stands for Bidirectional Attention Flow for Machine Comprehension. It was proposed by Minjoon Seo and other folks in 2017. It remains one of the most popular reading comprehension models and achieved a very good performance at that time, at least on the SQuAD data set. So next, I'm going to just dissect this model layer by layer and talk about what this layer is actually doing and how we can really build this model from the bottom layer to the top layer. the gi's input and the output will be on the mi, which is another 2H dimensional vector for each context word in the passage. OK. So the final is the output layers. So final output layers are basically just two classifiers just trying to predict the start and end positions. And they also have another classifier to predictions the end position of the answer. OK, so this model is actually achieved-- like on SQuAD data set, it achieved a 77.3 F1 score. They found that both attentions in two directions are actually important. If you remove the context-to-query attention, the performance will drop to 67.7 F1 score. And then if you remove this part, it will drop a 4-point F 1 score. So basically this theory tells us that these kind of attention scores can actually capture those negative scores pretty well. OK, so next, I'm going to talk about BERT, how to use the BERT model to solve this problem. BERT is basically a deep bidirectional transformer encoder pre-trained on large amounts of text. And it is trained on the two training objectives, including masked language modeling and the next sentence prediction. BERT models are pre-trained while BiDAF models only builtd on top of the GloVe vectors. Pre-training basically can just change everything and it gives a very large boost in performance. Even if you use a stronger pre-training models or modern, like a-- stronger models than the BERT models, they can even lead to better performance on SQuAD. And then finally, if you see even the latest pre- trained language models, including the XLNet or RoBERTa or Albert, these models can give you another like 3, 4 F1 score. is another paper that actually just came out in 2020. So there has to be a lot of evidence showing the similar things. So today we compute a very good reading comprehension data set on the individual data sets. But these systems trained on one dataset basically cannot really generalize to other datasets. And all the other numbers in this table basically shows that if you train one system on one datasets and then evaluate on another dataset, the performance will drop quite a lot. So it basically really cannot generalize from one dataset to another dataset. very useful in the practical applications. So the term here open domains is just in contrast to closed domains that deal with questions under a specific domain. The idea is that let's take a question-- OK, so here, the article is trying to answer questions using a very large collection of documents such as the Wikipedia. So we can just decompose this problem into, as I just mentioned, in our retrieval and the reader component. And this reader model is basically trying to read through all the documents that this retrieval return and try to find out the correct answer. Danqi can stay for a bit to answer questions, but not forever. Because she doesn't have a Stanford login, we're going to do questions inside Zoom. If you'd like to ask a question, if you use the raise hand button, we can promote you so that you appear in the regular Zoom window and can just ask questions and see each other. There are now four people who've been promoted. OK. So this is my answer, is that if we can leverage a very large and very powerful pre-trained language model, there is a possibility that we can actually do the question answering well with only a small number of examples. Most existing question answering datasets or reading comprehension datasets have been collected from Mechanical Turk. So it is very difficult to avoid some kind of artifact though, like a simple clues or superficial clues. So that's the reason that more specialized models that have been trained very well in one data set, it's very easy to pick up these kind of clues, and is very hard to generalize this kind of thing to another data set. Natural Questions was a dataset that Google put out about a year and a half ago maybe where they were actually taking real questions from Google search logs. The goal of this project is trying to ingest all the phrases in the Wikipedia. So these conversations are built using the training set of the question answering datasets. By using end to end, we apply this encoder to all the phrase, like 60 billion phrases in this region. So the model is definitely able to generalize from theTraining set to all of the Wikipedia phrases. And then we can use text as the representation, can actually generalize well for the unseen questions. It doesn't have to have seen the phrase then. Is this what you were asking? I see, OK. In context learning can help models to be more robust with respect to different domains. Do you think that in order to solve NLP in the sense that you can perform on par with humans on all NLP tasks it's sufficient to only interact with text data? Or do you think we'll eventually need the sort of experiences and common sense that we get only from seeing and viewing the world and having a set of interactions that we as humans have? Yeah. I mean, common sense is a very difficult-- even in the context question answering. The next question is from Danqi, who was one of the co-organizers of the EfficientQA task. Danqi is too modest to mention that she was also a member of the DensePhrases project. Her question is how concerned should we be about potential biases into these record labels or how we evaluate them, or is that just more of a concern for more open ended questions? Yeah, I'm not sure if I really have the answer, but I also want to quickly mention that quantization has been definitely a very useful technique to make the model smaller. yeah, I guess I'm just a little worried about who comes up with the test cases? Who determines what the right answer is? I mean, we will have more discussion of toxicity and bias coming up very soon, including actually Thursday's lecture as well as a later lecture, not specifically about QA though. OK. Next person is-- Thank you for the lecture. Yeah, my question is also related to the open domain question answering. So I was just wondering how much of the learning side of domain sort of generalization or domain alignment techniques can be combined with language level, like question answering? The key difference between the generative model and the extractive model is that for generative models, you can actually leverage more input passage together and do the generation. So for this model, there isn't any retrieval. So you can always find the answer from the question, right? So this model really has you relying on all the parameters you memorized, all the questions you've answered. And then generative Models are you're remembering the whole question and you try to retrieve the memory when you answer the question. The model is very large, like 11 billion parameters. So the parameters are basically trying to memorize a lot of information that has been.information. So by just taking this input, it has to just rely on the parameters to infer this answer. So it's actually very hard to-- yeah, it's a definite balance between memory and the generalization from it. All right, thanks. Do you want to call it a night or do you want one more question? Either way, yeah.