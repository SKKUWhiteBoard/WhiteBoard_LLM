This week in week three, we're actually going to have some human language, and so this lecture has no partial derivative signs in it. The idea of phrase structure is to say that sentences are built out of units that progressively nest. And so if I wanna capture, um, this talking to a cat here, well, that now means I've got a verb, because words like talk and walk are verbs. And then talk to the cat, it seems like after that, it could become a prepositional phrase. So I could write another rule saying that a verb phrase goes a verb followed by a phrase. what you learned about neural networks last week and the content of today, and jump straight right in to building a neural dependency Parser. Um, the other thing that happens in assignment three is that, we start using a deep learning framework PyTorch. So, um, final projects, we're going to sort of focus on those more in week five, but if it's not bad to be thinking about things you could do, if you're under a custom final project. We have under the sort of office hours page on the website, a listing of the expertise of some of the different TAs. Linguists can see patterns, like the cat, a dog, the dog, a cat, et cetera, and then we're gonna put them into bigger units that we call phrases, like "The cuddly cat by the door", and then you can keep on combining those up into even bigger phrases. So, I've write, um, a phrase structure grammar role, a context-free grammar role of- I can have a noun phrase that goes to a determiner, and a noun. phrase goes to a determiner, and then optionally, you can put in an adjective. And then I poke around a little bit further and I can find examples like the cat in a crate, or a barking dog by the door. And I can see lots of sentences like this. And so I want to put those into my grammar. But at that point, I noticed something special, because look, here are some other things, and these things look a lot like the things I started off with. what you find is that prepositional phrases following the verb in English. But if you go to a different language like Chinese, what you find are the prepositions come before the verb. And so, we could say okay, there are different rules for Chinese, um, and I could start writing a context-free grammar for them. Um,so that's the idea of context- free grammars, and actually, you know, this is the dominant approached linguistic structure that you'll see if you do a linguistics class. In this system of dependencies I'm going to show you, we've got in as kind of, um, a modifier of crate in the large crate. And well, then we have this next bit by the door. And as I'll discuss in a minute, well, what does the by thedoor modifying? It's still modifying the crate, it saying, ''It's the crate" And so, for each word we want to choose what is the dependent of and we want it in such a way that the dependencies form a tree. So, if we sort of said, Bootstrapping, was a dependent of,Um, talk, but then we had things sort of move around, then talk is a dependent that, and so I'm gonna cycle that's bad news. Knowing the right structure of sentences is important to understand the interpretations you're meant to get and the interpretationsyou're not meant toget. So, here, is a newspaper article. Uh, ''San Jose cop kills man with knife''. Um, now, this has two meanings and the two meanings, um, depend on, well, what you decide depends on what, you know, what modifies what? Okay. The second meaning the sentence can have is, that's the man has a knife. what is modifying what? Um, here is another one that's just like that one. Um, scientists count whales from space. So again, this sentence has two possible structures, right? [LAUGHTER] That we have, the scientists are the subject that are counting and the whales are the object. And really, those kind of verb phrases they sort of just like, um, prepositional phrases. Whenever they appear towards the right end of sentences, they can modify things like verbs or nouns. if that's not what you want, um, you have to use parentheses or indentation or something like that. Human languages are. this prepositional phrase can go with anything proceeding, and the. hearer is assumed to be smart enough to work out the right one. If we think of something like C or a similar language, it's just deterministically, the else goes with the closest if. If you just get humans to parse sentences and say, "Well, what is the agreement and what they produced?" you know, maybe you're only getting something like 92 percent. For $27 a share is modifying acquisition, right? [NOISE] So now, we leap right back. Now, is now the acquisition that's being modified? And then finally, we have at its monthly meeting is modifying? [Noise] Approved. Well, the approved, right. It's approved, yeah. Okay. So, yeah there are two possibilities, is, um, the results demonstrated that KaiC interacts rhythmically with SasA Ka- KaiA and KaiB. to potentially consider an exponential number of possible structures because, I've got this situation where for the first prepositional phrase, there were two places that could have modified. And so, if you get into this sort of combinatorics stuff the number of analyses you get when you get multiple prepositions is the sequence called the Catalan numbers. Ah, but that's still an exponential series. And it's sort of one that turns up in a lot of studies of the human brain. "We can represent by dependencies, um, these two different structures. That is either that there's somebody who's a shuttle veteran and a long time NASA executive, and their name is Fred Gregory, and that they've been appointed to the board. Or we can say, well, we're doing appointment of a veteran and the longtime NASA exec, Fred Gregory. And so, again, we can start to indicate the structure of that using our dependency. And we're simply going to count up how many of them are correct, treating each of them individually" can kind of think of these two things as sort of patterns and dependencies that we could look for to find examples of, um, just protein-protein interactions that appear in biomedical text. Okay. So, so that's the general idea of what we wanna do, and so the total we want to do it with is these Dependency Grammars. And so, I've sort of shown you some Dependency grammars, and I just want us to sort of motivate them a bit more formally and fully. sort of put the words in a line and that makes it. He see, let's see the whole sentence. You draw this sort of loopy arrows above them and the other way is you sort of more represent it as a tree. So, in addition to the arrows commonly what we do is we put a type on each arrow which says what grammatical relations holding them between them. Some of the earliest parsing work in US Computational Linguistics was dependency grammars. But I won't go on about that um more now. In the later parts of the first millennium, there was a ton of work by Arabic grammarians and essentially what they used is also kind of basically a Dependency Grammar. There was this guy Wells in 1947 who first proposed this idea of having these constituents and phrase structure grammars, and where it then became really famous is through the work of Chomsky. So, in modern work, uh, there's this guy Lucie Tesniere. He sort of formalized the kind of version of dependency grammar that I've been showing you. Treebanks are very reusable. Once you have a treebank, it's reusable for all sorts of purposes that lots of people build parsers format. But also other people use it as well like linguists now often used tree banks to find examples of different constructions. So that if right action is taken, the tree bank can be used to train a machine learning model to predict sentences. It can then be used by humans to build better parsers. It was used by Google to develop the Parsey McPa- parseFace model of parsing. for this sentence in context. Most dependencies are fairly short distance. There's a question of what's in between. If there's a semicolon in between, there probably is an a dependency across that. The other issue is sort of how many arguments do things take? So, here we have was completed. If you see the words was completed, you sort of expect that there'll be a subject before of the something was completed and it would be wrong if there wasn't. So, you should be building a machine learning model which will recover that structure. In the 60s, 70s and 80s, the mainstay of parsing was a crummy search for possible parsers. Joakim Nivre came along with a clever idea, he said "Yeah, that's true, um, but hey, I've got a clever ideas, because now it's the 2000s and I know machine learning" So, what he did was build a machine learning classifier and that classifier is gonna tell me whether to shift with left arc or right arc. shift, left arc or right arc. Um, if we also wanted to put labels on the dependencies, and we have our different labels, um, there are then sort of 2R plus actions because she is sort of left arc subject or left arc object or something like that. But anyway, there's a set of actions and so you gonna build a classifier with machine learning somehow which will predict the right action and Joakim Nivre showed the sort of slightly surprising fact that actually you could predict the correct action to take with high accuracy. For each word, we're going to represent it as a word embedding, like we've all what already seen. And in particular, um, we are gonna make use of word vectors and use them as the represent- the starting representations of words in our Parser. So we had parts of speech like, you know, nouns and verbs and adjectives and so on. Well some of those parts ofspeech have already been joined with the dependency of another part of speech. And so you had to even though at the end of the day, it was looking at weights that went into a support vector machine. MaltParser was Joakim Nivre's Parser that I sort of, uh, we started showing before. And they've got, um, a UAS on this data of 89.8. And the reason they loved it is it could parse at 469 sentences a second. There had been other people that have worked out different more complex ways of doing parsing with so-called graph-based dependency parsers. So, what we were able to show is that using the idea of instead using a neural network to make the decisions, we could produce something that was almost as accurate as the very best parsers available. There's still room to do better. I mean, at the unlabeled attachment score, it's actually starting to get pretty good. Um, and so then, what's the residual rate in which, um, people can actually disagree about possible parses? I think that's sort of more around three percent. But there certainly are cases and that includes some of the prepositional phrase attachment ambiguities. Sometimes there are multiple attachments that sort of same clause although it's not really clear which one is right.