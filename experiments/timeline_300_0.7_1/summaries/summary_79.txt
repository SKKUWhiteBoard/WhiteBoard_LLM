Reinforcement learning involves the idea of a model, a value, and a policy. A policy is a mapping from the state you're in to what is the action, um, to take. A model is a representation of the world and how that changes in response to agent's accident. A value is the expected discounted sum of rewards from being in a state and/or an action, and then following a particular policy. Markov Decision Processes is where we think about an agent interacting with the world. world. So the agent gets to take actions, typically denoted by a, those affect the state of the world in some way, um, and then the agent receives back a state and a reward. The Markov process is to say that the state that the agent is using to make their decisions, is the sufficient [NOISE] statistic of the history. And next states that we have seen up until the current time point. And so essentially, it allows us to say, the future is independent of the past given some current aggregate statistic about the present. The definition of a return is just the discounted sum of rewards you get from the current time step to a horizon and that horizon could be infinite. If Gamma is equal to one, then that means that your future rewards are exactly as beneficial to you as the immediate rewards. If you're only using discount factors for mathematical convenience, um, if your horizon is always guaranteed to be finite, it's fine to use gamma equal to 1 in terms of from a perspective mathematical convenience. Any other questions about discount factors? Yeah. The Markov Reward Process is a way to estimate sums of returns- sums up rewards. You could estimate it by simulation. There are mathematical bounds you can use to say how many simulations would you need to do in order for your empirical average to be close to the true expected value. The accuracy roughly goes down on the order of one over square root of N where N is the number of roll-outs you've done. And that would asymptotically converge to what the value function is. roll out in the world then you can get these sort of nice estimates of really how the process is working. But it doesn't leverage anything about the fact that if the world really is Markov, um, there's additional structure we could do in order to get better estimates. So, the value function of a mark forward process is simply the immediate reward the agent gets from the current state it's in plus the discounted sum of future rewards weighed by the discount factor times the transition model. State and multiplying that by the value function there. So, in this case we can express it just using a matrix notation. Um, and the nice thing is that once we've done that we can just analytically solve for thevalue function. And if you have N states, it's fine if some of the states that you might transition back to the same state there's no problem. You do need that this matrix is well-defined. That you can take that and take the inverse of it. so let's say you have N states there's generally on the order of somewhere between N squared and N cubed depending on which matrix inversion you're using. Is it ever actually possible for, uh, that matrix not to have an inverse or does like the property that like column sum to one or something make it not possible? Question was is it ever possible for this not to has an inverse? It's a good question. Um, but I'll double-check then send a note on a Piazza. the value of a state is exactly equal to the immediate reward we get plus the discounted sum of future rewards. And in this case, we can simply use that to derive an iterative equation. The advantage of this is that each of the iteration updates are cheaper and they'd also will be some benefits later when we start to think about actions. So, here are two different ways to try to compute the value of Markov Reward Process or three really one is simulation, the second is analytically and the third one is dynamic programming. A Markov Decision Process is typically described as a tuple which is just the set of states, actions, rewards, dynamics, model, and discount factor. So, the agent is in a state they take an action, they get immediate reward, and then they transition to the next state. Now we have a dynamics model that is specified for each action separately and we also have a reward function. And as was asked before by Camilla I think, the reward can either be a function of the immediate state, the state and action to the state action and next state for most of the rest of today we'll be using that. A lot of this class we'll be thinking about deterministic policies but later on when we get into policy search we'll talk a lot more about stochastic policies. So, if you have an MDP plus a policy then that immediately specifies a Markov Reward Process. Because you're only ever taking you've specified your distribution over actions for your state and so then you can think of sort of what is the reward, the expected reward you get under that policy for any state. And similarly you can define your transition model for Markov reward Process by averaging across your transition models according to the weight at which you would take those different actions. The Markov Decision Process for the Mars Rover is based on the Markov chain. The reward function is still that you either have for any action if you're in state one you get plus one and in any state any action for state s_7 you gets plus 10. Everything else is zero. And the reward for all actions and all other states is zero except for in state s-7 where it's always 10 no matter which action you take. So, in this case, what is the value of the policy and this is just to remind you of what like the iteration way of computing it would be. you do this computation. Just to quickly check that the Bellman equation make sense. So the immediate reward of this is zero plus gamma times [NOISE] 0.5. And that's just an example of, um, how you would compute one Bellman backup. So that's back to my original question which is you seem to be using V_k without the superscript pi to evaluate it. Oh, sorry this should, yes. This should have been pi. That's just a typo. to be pi up there. Yes it was, thanks for catching. All right, so now we can start to talk about Markov Decision Process control. Control here is going to be the fact that ultimately we don't care about just evaluating policies, typically we want our agent actually be learning policies. So the important thing is that there exists a unique optimal value function. So that's one really good reason why it's sufficient for us to just focus on deterministic policies, with a finite state MDPs in infinite horizons. There are two choices for every state and there are seven states. The number of policies is |A| to the |S| so we can be large, its exponential and the state-space but it's finite. Um, any one want to take a guess of whether or not the optimal policy is always unique? I told you the value function is unique. Is the policy unique? Yeah. I think there might be cases where it's not. It's not always unique but if there may be ties. So no. Depends on the process. In policy iteration what we do is we basically keep track of a guess of what the optimal policy might be. We evaluate its value and then we try to improve it. If we can't improve it any more, um then we can halt. In policy improvement we define something new which is the state action value. We're talking about like V^pi(s) which says if you start in state s and you follow policy pi what is the expected discounted sum of rewards. So, remember right now we're in the model where we know the reward model and we know what we can do. policy from then onwards. We are going to follow this new policy for all time. So, it should be at least a little unclear that this is a good thing to do [LAUGHTER]. Should be like, okay, so you're, you're saying that if I were to take this one different action and then follow my old policy, then I know that my value would be better than before. But what you really want is that thisnew policy is just better overall. And so the cool thing is that you can show that by doing this policy improvement it is monotonically better than the old policy. strict inequality if the old policy was suboptimal. So, why does this work? So, it works for the following reasons. And the next questions that might come up is so we know we're gonna get this monotonic improvement, um, so the questions would be if the policy doesn't change, can it ever change again? And is there a maximum number of iterations of policy iteration? Here iterations is i. It's a kind of how many policies could we step through? different, right? Because policy says you always have a policy and you know what its value is. It just might not be very good. Value iteration says youalways know what the optimal value in policy is, but only if you're gonna get to act for say k time steps. So they're just- they're computing different things, um, and they both will converge to the same thing eventually. So how does value iteration work? We're taking this max a over th- the best immediate already credit plus the discounted sum of future rewards. work? The algorithm can be summarized as follows. You start off, you caninitial your value function to zero for all states. And then you loop until you converge, um, or if you're doing a finite horizon, which we might not have time to get to today. And basically, for each state, you do this Bellman backup operator. So you'd say, my value at k plus one time steps for that state is if I get to pick the best immediate action plus the discounted sum of future rewards. So, if you see sort of a B with, um, ah, pi on top and saying, well, instead of taking that max over actions, you're specifying what is the action you get to take. So policy evaluation you can think of as basically just computing a fixed point of repeatedly applying this Bellman backup until V stops converging and stops changing. So, if an operator is a contraction it means that if you apply it to two different things, you can Think of these as value functions, then the distance between them shrinks after. doesn't get bigger and can shrink after you apply this operator. So, the key question of whether or not value iteration will converge is because the Bellman backup is a contraction operator. And it's a contraction operators as long as gamma is less than one. Which means that if you do- if let's say have two different Bell- er, two different value functions and then you did the Bell man backup on both of them. Then the distance between them would shrink. So how do we prove this? Um, we prove it- for interest of time I'll show you the proof. then the next thing we can do is we can bound and say the difference between these two value functions is diff- is, um, bounded by the maximum of the distance between those two. So you can pick the places at which those value functions most differ. And then you can move it out of the sum. And now you're summing over a probability distribution that has to sum to one. And so that means that the Bellman backup as long as this is less than one has to be a contraction operator.