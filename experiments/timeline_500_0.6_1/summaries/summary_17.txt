Machine learning is about how to acquire a model and acquire the parameters of a model, from data and experience. In the next few lectures, we're going to work through a sequence of different takes on machine learning that are going to highlight different subsets of the big ideas on this topic. We'll see today exactly how data kind of goes through the mill and gets turned into a model. And we'll see more in-depth examples and more structured examples of these kinds of problems later on when we talk about applications. is going to be unique. It's going to have to be at least one pixel off of something else you've seen. There's a lot of inputs that are really noisy, and you're training set, they might be hard, expensive to label, because they're noisy. And then at test time, you're going to make mistakes because machine learning is not perfect. We can look at other kinds of patterns. We could look at how many connected components of ink there are. What's the aspect ratio? How many loops are there? machine learning methods get better at doing that. We'll talk about that in a couple weeks when we talk about narrow nets. There's tons of classification tasks. It's probably the most widely-used application of machine learning. In model-based classification, rather than directly learning from errors that you make in the world from experience, instead we're going to learn by building a model from our data, and then doing inference in that model to make predictions. After today we'll look at the model-free methods. The Naive Bayes assumption is an extremely radical assumption as far as probabilistic models go, but for classification, it turns out to be really effective. The model itself might look something like this, where the class is the cause, and it independently causes each of those features. And that means when you go to make a prediction, it decomposes into a product of a bunch of different feature conditional probabilities, and we'll see examples of that and unpack the inference for that. Machine learning is something called empirical risk minimization. The main worry is that you-fit is that over-fit in picking the parameters of your model. Optimize your training set in the hopes that quantity will remain optimized. This is like downloading all the answers from past years, and you go and optimize all those answers. We'll see some concrete examples up now and this will be a little abstract, but we'll see more concrete examples in the next couple of lectures. All right, we're going to get started again. the thing we're trying to do is to fit a curve to this data. So you say, what is the fit? Is the fit getting as close as possible to the last dot? So what is my constant approximation to this? Does anybody want to hazard a guess? Let's call it five. OK, did I fit something about this data? Yeah, I felt something about the data. I fit basically it's mean. Did I capture the major trends? No. All right, let's try again. Let's fit a linear function. It's close, right? It's a better fit than the constant function. Notice that when I went to linear function, the space of hypotheses grew. Instead of just lines, now it's like lines with slopes and intercepts. In Naive Bayes probabilistic models, over-fitting usually shows up as zeros in your probability table. In other methods, it's actually going to show up in totally other ways. So let's figure out some ways to do that, to just illustrate what it would like to look like. We could take that polynomial and limit the degree of the polynomials. Using it under a hypothesis, you can also shrink the hypothesis space, so you can fit less under it. We already know one kind of over-fit to limit that. The maximum likelihood estimate, or relative frequency estimate, is used in machine learning. It says, OK, the probabilities are just the counts in the training data. For each probability I assign to red, and one minus that goes to blue, I can compute the probability of D. This is something you could try writing out for yourself. Of all of those probabilities, the one that matches the frequency of the data is the one. that maximizes the probability. of theData. But in practice, you need some smoothing. CS281A is an open-source computer program. It can be used to test computer models. CS281A uses a Bayes rule to find the parameters which maximize the product of this, which is what we were doing before. But there's this extra term, p of theta, which says, if I want to know what parameter or what probability is most likely, I need to weigh the likelihood of the data against how likely I think that parameter is in the first place. This is actually, due to Laplace, hundreds of years ago now, who's a philosopher who kind of worried about things. In a real classification problem, you have to smooth if you're going to use Naive Bayes. Instead of computing odds ratios on the maximum likelihood, I can instead do some smoothing and see after that smoothing, what has the biggest odds ratio? And suddenly things that only occurred once, they don't percolate to the top, because they haven't occurred enough to overwhelm that flat prior that I'm associating them with. So this is the top of the odds ratios for ham on the left, and favoring spam on the right. Some of these maybe make sense. Like, there it is. In general, your model is going to make errors. In spam classification, we found out that it wasn't enough to just look at words. For digit recognition, you do sort of more advanced things than just looking at pixels. You can add variables into your Naive Bayes model, but we'll also talk in the next few classes about ways to add these more flexibly and also induce these right, right? All right, I'm going to stop there for today and as you go, please come and grab some more candy.