A set is convex if for any a and b element of omega, the line between them is in omega, OK? So what does that mean? So let's draw the picture first, and we'll draw the math. Here's the convex set. So it means no matter how I pick a-- here's a-- and no matterhow I pick b, the straight line between. them, the geodesic between them, is in a set. OK? Now, we're going to apply this to functions. If f is twice differentiable and, for all x, f double prime of x is greater than 0, then f is convex, OK? So this says these functions really are bowl-shaped, right? Second derivative being positive means that they have this kind of positive curvature that looks like the U's. Their first dimension-- first derivative goes up and down, but they're kind of always trending. That first derivative is always getting more positive,right? It's negative on the left-hand side, positive on the right-handside. convex function here because we're maximizing likelihood. And this is just notational pain, right? Like, if we were-- maybe we should have minimized unlikelihood. So we need concave functions. And what are concave function? g is concave if and only f minus g is convex. So if I take a chord of this function-- that's a chord-- it's below. Which is what I should hope. If I flipped it upside down, the chord would be above. Cool. Jensen: The problem of optimizing over all those z's seems daunting, directly optimizing the l's. Instead, I'm going to come up with a local curve, OK, and I'll call this curve Lt of theta. It's another function. We're going to try and get that kind of easy-to-optimize function. And then, [MOUTH POP] we then do it again and create another curve. The whole algorithm is going to be Jensen's, and that's the whole algorithm. be theta t plus 1, OK? And we're going to, again, create some new curve, Lt plus 1 of theta, based on that point. And the key aspects of the point that I'll write in a second is this point is a lower bound. This curve is always below the loss, so it's kind of a surrogate that I'm not overestimating my progress, and it's tight. It meets at exactly that point, so I wouldn't think and get fooled that there was a higher loss function somewhere else. Jensen: Q is a probability distribution over the states such that the sum over Q(z) equals 1. Jensen: Q can also be written as an expected value, where z is distributed like Q of this weird-looking quantity. No matter how I pick the probability distribution, this chain of reasoning goes through, Jensen says. The key holds for any Q satisfying star, he says, and that's how we ground it into an example. The result is a curve that's always a lower bound everywhere. T sigma inverse j x(i) mu j. All right, and so just so you're clear what's going on here, the log turns these multiplications into additions. So we want to set this to 0 and use that sigma j inverse. Sigma j is full rank. And that will become clear in a second why that matters so much, because when we pull it out, what do we get? We get here s Sigma j inverse times sum, which is an unfortunate collision. that's OK. You get to screen that off. And I'll just post a one-page write-up for you. Please remind me in the thread, and I will definitely do that. If you don't do that, you'll get the wrong answer. That's also a motivation to learn it. And so what ends up happening here is you get something that says, I get sum i goes from 1 to n w(i)j over phi j plus lambda equals 0. And this implies that phi of j is equal to 1 over Lambda sum i equals 1. do this when you have something that sums to 1. It's not more complicated than what I wrote here, but make sure independently you go through it and ask questions. In the next class, as I said, what we're going to see is this notion of factor analysis. And that is going to tell us how to apply EM to a different kind of setting, which, at first glance, will look kind of impossible to do without a latent variable model. And I think that's all I want to say. The EM algorithm is an unsupervised version of k-means GMM. In GMM, you guess randomly an assignment of every point to the cluster, the probability. You guess where they're probabilistically linked, that is, what's the probability of these points belong to cluster one, this point belongs to cluster two. And then once you have that, you then solve some estimation problem that looks like a traditional supervised learning. The decomposition is quite important. And we're going to try and kind of abstract that away.