Peter Solovits: I got a call a year ago from the National Academy of Science, Engineering, and Medicine. He says the body is made up of old people with lots of gray hair who have done something important enough to get elected to these academies. SolovITS: They convened a meeting to talk about the set of topics that I've listed here. The issue of using litigation to target scientists who have opinions that you don't like, for example, is one of the topics. tenor of the times. So the group of us that talked about AI and decision making, I was a little bit surprised by the focus because Hank really is a law school professor at Stanford. Cherise Burdee is at something called the Pretrial Justice Institute, and her issue is a legal one which is that there are now a lot of companies that have software that predict, if you get bail while you're awaiting trial, are you likely to skip bail or not? And so that that was our panel, and we each gave a brief talk and then had a very interesting discussion. interesting to me. He said, no, he wouldn't want people like that, which kind of shocked me. And so we quizzed him a little bit on why, and he said, well, because he views the role of the judge not to be an expert but to be a judge. To be a balancer of arguments on both sides of an issue. And he was afraid that if he had a clerk who had a strong technical background, that person would have strong technical opinions which would bias his decision. Algorithmic technologies may minimize harms that are the products of human judgment. But the use of technology to determine whose liberty is deprived raises significant concerns about transparency and interpretability. The algorithms are developed privately by private companies which will not tell you what their algorithm is. And so you have no idea what's going on in that box other than by looking at its decisions. The state Supreme Court ruled against a particular person in Wisconsin, saying that knowledge of the algorithm's output was a sufficient level of transparency in order to not violate his rights. When I was an undergraduate at Caltech, the Caltech faculty decided that they wanted to include student members of all the faculty committees. In those days, Caltech only took about 220, 230 students a year. So one day, one of the professors said here's what we ought to do. We ought to take the 230 people that we've just offered admission to and we should reject them all and take the next 230 people, and then see whether the faculty notices.look like they're a better bet. Peter Zolovits: What do you mean by fairness? What characteristics would you like to have an algorithm have that judges you for some particular purpose? Irene: It's impossible to pin down sort of, at least might in my opinion, one specific definition, but for the pre-trial success rate for example, I think having the error rates be similar across populations, across the covariants you might care about, is a good start. And you'll see later Irene-- where's Irene? Right there. People who are similar should be treated similarly, says Peter Zolovits. But defining that function is a challenge, he says. He shows an example of a bias in the genetics used to determine whether somebody is at risk for cardiomyopathy, hypertrophic cardaomyopathy. "It's another of the classic sort of notions of fairness," he says, "that puts a lot of weight on the distance function" "You obviously don't want to use the sensitive characteristics, the forbidden characteristics in order to decide similarity" In the US, there were tests of this sort done, but the problem was that a lot of African and African-American populations turned out to have this genetic variant frequently without developing this terrible disease. And it was only after years when people noticed that these people who were supposed to die genetically weren't dying that they said, maybe we misunderstood something. And what they misunderstood was that the population that was used to develop the model was a European ancestry population and not an African ancestry population. So you go, well, we must have learned that lesson. descent do not improve the prediction of osteoporotic fracture and bone mineral density in Chinese populations. So it's the same story. Different disease, the consequence is probably less dire because being told that you're going to break your bones when you're old is not as bad as being told your heart's going to stop working. But there we have it. OK, so technically, where does bias come from? Well, I mentioned the standard sources, but here is an interesting analysis. This comes from Constantine Aliferis. the data, randomizing, essentially, the relationships in the data. And then you get a curve of performance of those models, and if yours lies outside the 95% confidence interval, then you have a P equal 0.05 result that this model is not random. So that's the typical way of going about this. Now, you might say, but isn't discrimination the very reason we do machine learning? Not Discrimination in the legal sense, but discrimination in the sense of separating different populations. Until 1967, it was illegal for an African-American and a white to marry each other in Virginia. Trevor Noah, if you know him from The Daily Show, wrote a book called Born a Crime. He had to pretend to be-- his mother was his caretaker rather than his mother in order to be able to go out in public. So here are some of the legally recognized protected classes, race, color, sex, religion, national origin, citizenship, age, pregnancy, familial status, disability, veteran status. is something not right, that there is some sort of discrimination. Now, the problem is, how do you defend yourself against, for example, a disparate impact argument? Well, you say, in order to be disparate impact that's illegal, it has to be unjustified or avoidable. So the question, of course, is can we change our hiring policies or whatever policies we're using to achieve the same goals, but with less of a disparity in the impact? So that's the challenge. and you can't square that circle easily. Well, there's a lot of discrimination that keeps persisting. There's plenty of evidence in the literature. And one of the problems is that, for example, take an issue like the disparity between different races or different ethnicities. It turns out that we don't have a nicely balanced set where the number of. people of European descent is equal to the number. of people of African-American, or Hispanic, or Asian, or whatever population you choose. descent. There are three criteria that appear in the literature. One of them is the notion of independence of the scoring function from sensitive attributes. Another notion is separation of score and the sensitive attribute given the outcome. And then sufficiency is the inverse of that. It says that given the scoring. function, the outcome is independent of the protected attribute. So that says, can we build a fair scoring function that separates the outcome from the protected. attribute? So here's some detail on those. And by the way, this relates to the 4/5 rule. because they have better outcomes. Or alternatively-- well, of course, it could be caused by malice also. There's also a technical problem, which is it's possible that the category, the group is a perfect predictor of the outcome. They can't be independent of each other. Now, how do you achieve independence? Well, there are a number of different techniques. One of them is-- there's this article by Zemel about learning fair representations, and what it says is you create a new world representation, Z. the protected attribute, but is as independent as possible. And usually, there are knobs in these learning algorithms, and depending on how you turn the knob, you can affect whether you're going to get a better classifier that's more discriminatory or a worse classifiers that's less discriminatory. So you can do that in pre-processing. You can do some kind of incorporating in the loss function a dependence notion or an independence notion. And again, there's a knob where you can say, how much do I want to emphasize misclassifications for the protected attribute? does in other populations, and the FDA has actually approved the marketing of that drug to those subpopulations. And if you think about the personalized medicine idea, which we've talked about earlier. The populations that we're interested in becomes smaller and smaller until it may just be you. And so there might be a drug that works for you and not for anybody else in the class. But it's exactly the right drug for you, and we may get to the point where that will happen and where we can build such drugs. to reduce the errors in all groups. So that issue about randomly choosing members of the minority group doesn't work here because that would suppress the ROC curve to the point where there would be no feasible region that you would like. So for example, if it's a coin flip, then you'd have the diagonal line and the only feasible region would be below that diagonal, no matter how good the predictor was for the other class. And then the final criterion is sufficiency, which flips R and Y. So it says that the regressor or the predictive variable can depend on the protected class, but the protectedclass is separated from the outcome. degree of calibration will give you a good approximation to this notion of sufficiency. These guys in the tutorial also point out that some data sets actually lead to good calibration without even trying very hard. Now, there's a terrible piece of news, which is that you can prove, as they do in this tutorial, that it's not possible to jointly achieve any pair of these conditions. So you have three reasonable technical notions of what fairness means, and they're incompatible with each other except in some trivial cases. A paper based on Irene's work says that gender influences whether you're a programmer or not. It also says that visiting Pinterest is slightly more common among women than men. The paper says that despite the fact that you have these two scenarios, it could well turn out that the numerical data, the statistics from which you estimate these models are absolutely identical. And so from a purely observational viewpoint, you can't tell which of these styles of model is correct or which version of fairness your data can support. work. We were really interested in socioeconomic status, but we didn't have that in the database, but the type of insurance you have correlates pretty well with whether you're rich or poor. So we wanted to see not the coded data, but whether the things that nurses and doctors said about you as you were in the hospital were predictive of readmission, of 30-day readmission. So yeah, this is what I just said. So the question we were asking is, is there bias based on race, gender, and insurance type? We used LDA, standard topic modeling framework. And the topics, as usual, include some garbage, but also include a lot of recognizably useful topics. And so what we found is that, for example, white patients have more topics that are enriched for anxiety and chronic pain, whereas black, Hispanic, and Asian patients had higher topic enrichment for psychosis. And we were speculating on how this relates to sort of known data about underdiagnosis of COPD in women. So again, stereotypes of what's most common in these different groups. The error rates on a zero-one loss metric are much lower for men than they are for women. We have much tighter ability to predict outcomes for private insurance patients than for public insurance patients with a huge gap in the confidence intervals. This indicates that there is, in fact, a racial bias in the data that we have and in the models that we're building. The last thing I want to talk about is some work of Willie's, so I'm taking the risk of speaking before the people who actually did the work here. but that's the case. The eICU data set we've mentioned, it's a larger, but less detailed data set, also of intensive care patients. And there, we see, again, a separation of mechanical ventilation duration roughly comparable to what we saw in the MIMIC data set. So these are consistent with each other. On the other hand, if you look at the use of vasopressors, blacks versus whites, at the P equal 0.12 level, you say, well, there's a little bit of evidence, but not strong enough to reach any conclusions. The author says black patients are more likely to be distrustful of the medical system than white patients. He says it's not race, but something that correlates with race because blacks are more distrustful. The author says mistrust is not just a proxy for severity, but a reflection of the fact that they're sicker. The book is being presented at conferences across the country. It's a very rich area for both technical and technical work for trying to understand what the problem is, he says. fairness popping up at different universities. University of Pennsylvania has the science of Data ethics, and I've mentioned already this fairness in machine learning class at Berkeley. This is, in fact, one of the topics we've talked about. I'm on a committee that is planning the activities of the new Schwarzman College of Computing. The college obviously hasn't started yet, so we don't have anything other than this lecture and a few other things like that in the works, but the plan is there to expand more in this area.