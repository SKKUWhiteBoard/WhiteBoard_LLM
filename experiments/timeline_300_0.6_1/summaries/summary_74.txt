This lesson will first dive into some signal Theory and then move on into things that we're more familiar with things like deconvolutions and using Transformers for next note prediction. The first thing we want to talk about is how can we sample and quantize a continuous time signal. We'll also talk about the different kinds of models that we've been talking about and how they can be used for generative audio. The last part of the lesson will be on how we can use Transformers to generate sounds using these models. periods and two by quantizing our level so instead of dealing with A continuous scale we can quantize at certain levels for example a frequency of like 2 4 6 8 Hertz what this allows us to do is come up with discrete points. The analog to digital converter uses something called the sample and hold circuit. The motivation behind this is that we we maintain a signal that's the pass band where we allow every signal to pass when we hit certain cutoff frequency we Wane our signal by a certain factor. voice and Pitch it up very fast right what quantization level do we do we want there. Can we do a lossy pitch up with a uh with by filling in the the blanks in some intelligent way through prediction or kind of note fitting which is an interesting consideration I think given the fact that audio is a continuous time signal um the digitization process and the choices you make matter a lot and because of that this field is so interesting and there's a lot of really didactic work around how we can take these continuous signals discretize them. is less than double of the highest frequency present aliasing will happen. This asserts that you need at least two samples per period. aliasing is the byproduct of poor sampling. A lower wave resolution will result in a modified output signal as compared to the original input that we're trying to process. There's a lot of literature about um aliasing effects including spatial illnessing. The aliasing phenomenon is incredibly interesting this happens both visually and auditorily. It's a very prevalent problem in any problem. familiar with perhaps in terms of how we can use those to reconstruct signals and ultimately how we Can use Those to generate audio right predict the best uh kind of next node um so looking at the next the next step here we want to use deep learning for reconstruction right where we are are reconstructing a low quality audio to high resolution audio right um and this is this is the kind of uh model framework um that we can used for this um you might notice it really closely resembles a unit which is something that we talked about during image segmentation. Transformers can help increase our sample of training data and generalize key scales and beats throughout a data set. A single song can be transformed into 12 songs of different Keys. The more data you have the better your model will be and the more generalizability you have in your Transformer the better it'll perform. Transformers will far outperform classical methods of of both computer vision and natural language processing. It's easier for machines to predict keys without flats and Sharps which has you know similar to what humans do. hidden state memory Transformer memory specifically to this model enables very fast inference for music prediction right we've done a lot of things to optimize for for our prediction we're including a beat embedding so that's not something it has to learn. We're able to get a sense of of relative position with Transformer XL whereas vanilla Transformers will use Absolution absolute position only. It's important for music models to know the position of each token relative to one another because positionality matters right the order that you're playing the notes really is is what matters the most. are the original Pachelbel's Canon um as you can see this does deviate a bit but honestly it sounds pretty good. The Transformer model is able to do this next note next sequence prediction pretty pretty well. So yeah there's a a lot to do in this field um a lot of really cool things happening um and yeah I hope you guys learned something about uh about generative audio today and are inspired to kind of give some of these things a try yourself. thank you guys for tuning in have a good one.