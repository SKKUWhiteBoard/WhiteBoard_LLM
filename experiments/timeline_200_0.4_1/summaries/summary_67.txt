So in the next portion of today's lecture we're going to talk about how we can modify the policy gradient calculation to reduce its variance. In this way we'll obtain a version of the policy gradients that can be used as a practical reinforcement learning algorithm. The first trick that we'll start with is going to exploit a property that is always true in our universe which is causality causality says that the policy at time t prime can't affect the reward at another time step t if t is less than t prime. The optimal baseline is not used very much in practical policy grading algorithms but it's perhaps instructive to derive it just to understand some of the mathematical tools that go to studying variants. In many cases when we just need a quick and dirty baseline we'll use average reward however we can actually derive the optimal baseline. In order to find the optimal b i'm going to write down the derivative d var db and solve for the best b so the derivative of the second part is 0 because it doesn't depend on b.