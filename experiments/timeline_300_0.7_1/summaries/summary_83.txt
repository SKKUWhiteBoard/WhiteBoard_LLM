Today, we'll talk about coded imaging. It's a form of a co-design between how you take a photo and how you recover the sharp detail afterwards in software. We'll see how-- we already have some projects that are inspired by biological vision. And I believe Santiago-- where's Santiago? Oh, yeah, his triangle-- the piston, kind of-- so some really great ideas. So I'm glad a lot of these concepts are coming together in the final projects. It is going to be very popular. capture the image and how you process the image. In a typical film camera, or even it is digital camera, you take the picture, and that's basically the end of the story. So when you try to recover this information, you start getting this banding artifacts. And we'll see it in the next slide, why that happens, and how to get rid of the banding and noise in the image you're trying to capture. Back to Mail Online home. back to the page you came from. have basically a 1D convolution that's converting this image into this image. But the Fourier transform has some zeros, so you cannot divide those frequencies by 0 and recover an image. So the culprit here is really this box function, which is equivalent to-- when you release the shutter, opening the-- release your shutter button-- opening the shutter and keeping it open for exposure duration and closing it. But that's not the most effective. So what if you change that? What if you open and close it in a carefully chosen binary sequence? frequencies-- they're all preserved. Of course, they're attenuated. It's not as high as-- it's not 1.0.0, it's reduced. Maybe it's 0.1 or so. So there is still some hope to recover this photo back from this because, in the denominator, we will not have seen. The problem is each of the frame will be extremely dark. So you are basically adding up a lot of noise. Every frame is dominated by noise. that's your 1010 inquiry. Instead of keeping the shutter open for the entire duration and getting a well-exposed photo, the shutter is open for only half of the time. The support for the representation of the Fourier domain of that function that you describe there is infinite, right? So you actually truncate this in order to-- RAMESH RASKAR: It's not infinite because you still have some width. AUDIENCE: Right, but you have infinite high frequencies there by the sharp conditions. but not 0. If you invert the process-- RAMESH RASKAR: From here, this is what you get. If your toy is moving or your taxi's moving really slow, then there is no need to-- in this case, the sequence was about 51-- actually, 52 vector long. So your 52 vector is going to stretch or shrink based on how fast the object is moving. And you're saying that it also depends on how far theobject is in space because faster-moving objects. think about image space motion because the speed in the real world and-- the distance are they get-- you divide to normalize by the distance. If you're in a dark room, you can just strobe the light, rather than opening and closing the shutter. So you get a reasonable result. But going back, what are the limitations of this method? Yes. You need to know the motion or the direction of the motion. If the car is moving from left to right versus right to left, the way your point spread function will be imposed on the scene will be different. If we did 100 milliseconds, it picks up speed, then your assumption that the 52-length vector will map to some stretched or shrunk version of 52 is not valid. Some parts will go faster and slower. You can either go to object space or you can come back to image space to make sure there is no acceleration. It's all linear. If two cars are partially overlapping during the exposure, it's possible, but it's more challenging because you don't know exactly how fast the two car are moving. your shutter? RAMESH RASKAR: No, when you're setting up your shutter, if the car is moving really slow, and you don't expect it to blur by 52 pixels, then using a 52 sequence is overkill. Maybe you should use a new sequence that's only about 10 long or 11 long, right? So it's just like-- AUDIENCE: OK, but that's just so you can get more light. RAMESHRASKar: No. That's so that it's most optimal for that setting. during that 52 window. In general, the technique works well when things are moving naturally. But if somebody wants to do this kind of an experiment, or move things behind an occluder and move out, those are very challenging scenarios. The basic assumption is that if you take any point in the scene, it's moving in a straight line, let's say. And if you have an object, and every point of that object moves in astraight line, OK. It doesn't matter which direction and what speed. blurred function is very critical. And this is what we want to study about half of the class. And the concept is very, very,very interesting because light is linear. So eventually, it's very linear. What happens to a point happens to the rest of the object. So if I have a car that's moving, and I tell you how exactly one point of the car is behaving in the image, I can tell you automatically how the other cars are behaving. All of it is going to have the same spread image. to engineer activity of the camera. So in this particular case, a point that was moving created a blur like this. And by engineering the time point spread function, it stops looking a bit like that. And then it just turns out that this one is easier to deal with than this one. So that that's why a computational camera is doing the computation not just in Silicon but also in optics. And the circuit is very, very simple. You just take the hot shoe of the flash, and it triggers. When you lose the shutter, it triggers the circuit. would you apply spatial coding? AUDIENCE: Coded aperture? RAMESH RASKAR: C coded aperture. So this is coded exposure, coded aperture-- very easy. And all you're going to do is put some kind of a code in the aperture of the lens. And this is how, actually, it started in the days of-- in scientific imaging, especially in astronomy, coded apertures are very well known. And so I thought, it must be useful for something in photography. An alligator came out of the water, and he lost his balance, and the boat flipped upside down. He managed to flip back in. But it completely damaged his camera that was with him, and it just wouldn't work. So he just took out his lens, which is a standard Canon lens. And he said, let's open it all the way. It had all the mud in it and so on. And then he just showed me this thing as is. And it was very fascinating because it was a standard film lens. When you think about a visual camera, you make this very simplistic assumption. That is a pinhole, and there's a sensor. And when you put a lens, we assume that the center of the lens is the central projection, that this always can be assumed to go to that point. So finding that plane is actually a tricky problem. And in retrospect, it's very easy. If the lens makers are putting everything there, we should put a recorded aperture also in the same plane. in the back. We tried all those things. But that creates a blur that's not constant all over the image. And it has a lot of issues. But placing it over there, it turns out you get the same blur. So what exactly happens if you take a picture of a point light, and everything is a sharp focus? Nothing changes. If you start moving away, the code will shrink. And eventually, when you put it here, we get another code. That's exactly what's happening here. When it's auto focus, we just see the code. digitally. In 1D, this is what we saw, right? Its Fourier transform is flat. So there are 52 entries here, and almost all of them are the same. Now we're saying, think about the problem in 2D. And what's the Fourier transforms of this? So first, for this one, the Fouriers transform is-- as we see, it's black. And then if you take that in 2 D-- so how is the code? I'll give you a hint. the values will be constant. So if we're placing a broadband code, certainly we have an opportunity to recover all the information. It's much easier to think about convolution and deconvolution in frequency domain than in primal domain. In communication theory, everything is [INAUDIBLE].. We think about carrier frequencies of radio stations in frequencies. And we think about guard bands and audio bands. And that's the same thing that's going on here. And convolution, deconvolved-- much easier than frequency domain. At the end, the solution is very easy-- just flutter the shutter. don't know whether this matters. But you're right. If you're looking at something that's-- we have bright lights in the scene. At a distance, take our false photo. They will all look like this. Or you could put hearts in it, or, like-- AUDIENCE: Right, yeah, I was thinking maybe, that's totally possibly. [LAUGHTER] RAMESH RASKAR: So an interesting art problem is how do you create-- how do we create a mask that visually looks aesthetic but is mathematically also invertible? the depth. When it comes into sharp focus, my edges, that must be the right depth. Unfortunately, it doesn't work out in this case. The main reason is that, because it's coded aperture, no matter where you refocus, it still looks like it has very high frequencies. So you need to find this 7-by-7 pattern or even the previous case, the 52 pattern. Take a Fourier transform to see if it's flat. If it's not flat, you go to the next one. code and do a gradient descent and so on. There's no good solutions for 2D. But for 1D, there are some really good solutions to come up with that. For 2D, for certain dimensions, they call it one more 4 or three more 4. Basically, when you divide by 4, the remainder can be 1 or 3. And there are certain sequences that are beautiful mathematical properties, of which sequences could have broadband properties and which may not. So it turns out you cannot-- there's a little bit of cheating going on here. filter to the beginning of the signal. This particular filter is actually not circular, but it's linear. So when you apply the filter here, when you start applying the filter at the end of the image, you don't go back to the front. It turns out, for circular convolution, the match is very clean and beautiful and smoother course work. Or for linear convolution,. there is no good mechanism. So we came up with our own code called RAT code, R-A-T, which is after three quarters. In astronomy, you have circular convolution because they use either two mirror tiles and one sensor or one mirror tile and two sensors. If you're tiling the mask at aperture, but you are using single-tiled aperture, you'll get horrible frequency response. In this photo, those frequencies are not lost because all the frequencies are preserved. But that's because our eyes are not very good at thinking about what the original image could be, given either this one or the previous one. It's just pure x equals b, x equals a backslash b. Ramesh Raskar: There's just one way of engineering the point spread function, one in motion and one in focus. He says for any continuous code, there is a corresponding binary code that will do an equally good job. RaskAR: Eventually, it's going to have a 2D projection. "It's amazing because motion is time, and the focus is space. They're completely orthogonal. So you can play with it," he says of motion and focus. A lot of it is actually being used in cell phone cameras. The wavefront coding, as they call it, preserves the spatial frequencies. It has the benefit that, no matter which steps you are at, you have the same defocus blur. The disadvantage of coded aperture was that you need to know what the depth was to be able to deblur. But now, because it's independent of depth, you can just apply the same deconvolution and get back a sharper image. has a short focus lens. But within this region, the thickness will be a bonus. So you can either think of it as adding small matchsticks on top of the main lens. And a face mask basically means you are changing the face of incoming light. That's why, as we learned about at the beginning, if you have something very far away, this slows down a little bit. And everything just works out with operations. But [INAUDIBLE] this extra piece of glass, you're saying, I'm going to speed up and slow down. the name. The solution is very similar. I'm sure they're fighting out in court right now. Same solution. Instead of putting this particular guy, that's just going to add some extra glass, but mostly in a minor form. It's just [INAUDIBLE] on that one. So basically the same solution but creating different focal length for different [? partners. ?] AUDIENCE: Yeah. Although you said, I mean, there's this portion there, where if you have another blur [INAudIBLE],, right? RAMESH RASKAR: Right. we're painting the rays, but the colors are, for all practical purposes, that'd be the same. RAMESH RASKAR: The effect is very low, though, remember. So maybe you have a pixel and get blurred by 10 pixels or [INAUDIBLE]. It's not a global effect. And then we also saw this one very early on, where the point spread function-- typically when something goes in and out of focus, it looks like a point. If it goes out offocus other ways, it still lookslike a disc. of this? AUDIENCE: Does [INAUDIBLE]? [LAUGHTER] RAMESH RASKAR: She would have used it by now. The goal here is exactly opposite. If you go slightly out of focus, you get a very different point spread function. So when you're looking with a microscope, depending on what the depth of your tagged particle is, the point spreadfunction will look very different. So you can estimate the depth by looking at the orientation of those two dots. These concepts.these concepts. OK? So let's very briefly look at compressed sensing because it's something you should be familiar with. OK, so here's an idea that received a lot of publicity. It was even "The 10 Emerging Technologies" by a very reputable magazine. It's a very cool idea, by the way. As a scientist, I really like it. But when somebody like Technology Review or Wired Magazine says, Top 50, Top 10, of course, I wish I'm listed among them. But at the same time, it has good side effects. Raskar: If you're on 2 megapixels, then you need to take 2 million [? pics] All right? So the claim this group made at Rice University was that if I wanted a million-pixel image, I don't have to really take a million readings. And that's where the concept of compressive sensing or compressed imaging comes up. You want to take something that is much higher resolution but recover it in a compressed way, where it's taking the picture and compressing the software. can transform the image and measure in [? your ?] measurements. And there are certain cases where it is really true. You have signals that can be compressed very easily. A very classic example is in communication, where, if you have a huge band of frequencies, and software radio-- instead of tuning it with electromagnetics, you just capture the whole signal. And by doing that, they're able to sample this effect of a software radio with a detector that doesn't have to measure 100-megahertz-wide signal. Rohit: Compressive sensing allows you to represent an image with fewer coefficients. Rohit: In general, this scheme doesn't work. But you will continue to see people who come to you and say, you know, I have this magical thing I just heard or compressive image something, and that will just solve a problem. Tom: The secret of success for film, of film photography, is that if somebody had given you this problem before the invention of film, that there is a scene. looks just like a cartoon does-- some whites clothes, some black clothes. And that's why compressive sensing works very well there. Compressive sensing allows you to take less measurements. But the problem is you need to actually have more information about the scene before you take the measurement, which is another measurement. But advantages are, at the time of capture, I just use this random basis or Fourier basis-- I mean, kind of a modified [INAUDIBLE] basis. of captures, that you're limiting yourself in what kind of scenes will be compatible with that capture. So for example, if I just had a scene that's all white, then just one captured would be enough. But that's because you know something about the scene. RAMESH RASKAR: No, the claim is that even if you don't know anything about theScene, you take very few measurements. All you know about the Scene is that once you take its transform, some transform, it's very sparse. which is how to write a paper and wishlist for photography. Which isHow to Write a Paper and Wishlist for Photography: How to Write A Paper and Write A Wishlist For The Camera. For more information on writing a paper or wishlist, go to: http://www.cnn.com/2013/01/30/photography/how-to-write-a-paper-and-wishlist-for-photography-how- to- Write-A-Paper-And-Wishlist.html.