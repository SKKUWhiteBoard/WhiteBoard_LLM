This lesson will first dive into some signal Theory and then move on into things that we're more familiar with things like deconvolutions and using Transformers for next note prediction. The first thing we want to talk about is how can we sample and quantize a continuous time signal. We'll also talk about the different kinds of models that we've been talking about and how they can be used for generative audio. The last part of the lesson will be on how we can use Transformers to generate sounds using these models. periods and two by quantizing our level so instead of dealing with A continuous scale we can quantize at certain levels for example a frequency of like 2 4 6 8 Hertz what this allows us to do is come up with discrete points. The analog to digital converter uses something called the sample and hold circuit the details of which aren't incredibly important. If you guys do want to learn more about Dax and adcs you can take 16b as well as signals classes at Berkeley I would highly recommend those. signal to an analog output we can kind of start by talking about the ADC circuit. This uses something called the SARS ADC algorithm essentially what this is is a binary search to figure out what is my best digital approximation of my analog signal. From here we take in our our input and our output is based on the amount of bits of precision that we want to have so depending on whether we want a two-bit approximation a three bit approximation this highly depends on the level of precision we want. Stars ADC algorithm. The level of quantization here correlates very directly to the dynamic range of the signal that you're quantizing. With a bit depth of two our approximation isn't very great with a big depth of three um it's definitely getting better our errors reduce significantly and our quantization is uh a lot more indicative of our sample bit Depth of five we're really adhering to our our curve now our error is almost zero and a bitdepth of 16 you have an almost perfect approximation um there is a trade-off though thetrade-off is compute power how much time do you have to process this if this is something where a lot of musicians they want to sample their. voice and Pitch it up very fast right what quantization level do we do we want there. Can we do a lossy pitch up with a uh with by filling in the the blanks in some intelligent way through prediction or kind of note fitting which is an interesting consideration I think given the fact that audio is a continuous time signal um the digitization process and the choices you make matter a lot and because of that this field is so interesting and there's a lot of really didactic work around how we can take these continuous signals discretize them. is less than double of the highest frequency present aliasing will happen. This asserts that you need at least two samples per period. aliasing is the byproduct of poor sampling right so a lower wave resolution will result in a modified output signal as compared to the original input that we're trying to process. Different frequencies approximate our input wave differently right this wave you wouldn't really think is indicative of uh the actual wave that we's trying toprocess this wave is one could argue even less indicative we're skipping a lot of points. from we take our low frequency and that becomes a product from taking our original frequency that's above half of the sampling rate and subtracting half of it from it so essentially you can think of it as down sampling and contributing to information loss by transforming higher signals into lower frequencies. There's a lot of literature out about aliasing um it's a problem that's very prevalent um in in any signals problem and something that a lot. Academia is is covering um so I'd highly recommend checking out some of these links. um this is is kind of uh and and add add-on um to this presentation it uh doesn't uh really contribute exactly to what we're talking about but I thought this was incredibly cool um with one line in C you're able to to generate Melodies um which is incredibly coolUm yeah so please check it out if you if you guys have a chance uh the next thing we're going to talk about briefly is geometric signal Theory um and specifically you know what a projection is how it can be used to reconstruct signals and finally how that can tie in to reconstructing signals. The idea behind the the last two sections here was to give you motivation for for how signals work and how kind of classical reconstruction can occur using math that we're all familiar with. This really covers any Vector in R2 um e0 uh we can Define as one zero so a horizontal Vector E1 is zero one a vertical Vector if we look at where we we're trying to project some Vector X onto uh the the e0 space right we can kind of see how the math works out here. familiar with perhaps in terms of how we can use those to reconstruct signals and ultimately how we Can use Those to generate audio right predict the best uh kind of next node um so looking at the next the next step here we want to use deep learning for reconstruction right where we are are reconstructing a low quality audio to high resolution audio right um and this is this is the kind of uh model framework um that we can used for this um you might notice it really closely resembles a unit which is something that we talked about during image segmentation. after this restacking step um and the the loss function used throughout this process specifically was was kind of a mean squared error loss function so by playing around with different kinds of loss functions you might be able to yield better performance. We're going to kind of uh kind of transition now into Transformers for audio generation right how can we use Transformers to predict the next note for example. We can use the Transformer architecture to predict music notes our goal is to build a sequence model for music where we take an input sequence and predict a given Target sequence. music realm does provide a different issue than it did for our other two representations um of of image and text we want to then build the model and train it to predict the next token right the first step takes the form of converting data which is music files into a token sequence which is individual notes. We can approximate this which is a series of notes in a piano roll right or a graph that has our offsets and our pitches right so our pitches span you know a certain uh pitch set um and we want our our information to be captured in multiple Dimensions. see here um this kind of adheres to you have you know two quarter notes and a half note and the tokenized form of this is you have C and then quarter D quarter e half right but the problem with this is that you have a large vocabulary size where you have to keep keep track of all of these sub tokens. And you have less control over predictions given that your vocabulary size will grow as you add nodes right um the other kind of thing is polyphony right taking many notes and mapping it for one specific kind of tokenized set where you play note sequentially. Transformers can be transformed into 12 songs of different Keys which can help increase our sample of training data and generalize key scales and beats throughout a data set. Data augmentation provides an amazing data sub multiplier to to get more data. Transformers will far outperform classical methods of of both computer vision and natural language processing the more information you have and the more generalizability you have in your Transformer the better it'll perform. The next thing to consider is positional beat encoding to give it a better sense of musical timing. zero is a token you can see one is atoken you can't see um which is a little backwards I know um but you're you're essentially masking the every token except for yourself right here. By the last step you're able to see everything right by applying an another mask a window of size 2 here at the first step you only see yourself right at the Second Step you don't even see what token you're on. You only see the previous token at the third step and you're getting more information about the the current token. hidden state memory Transformer memory specifically to this model enables very fast inference for music prediction right we've done a lot of things to optimize for for our prediction we're including a beat embedding so that's not something it has to learn. We're able to get a sense of of relative position with Transformer XL whereas vanilla Transformers will use Absolution absolute position only. It's important for music models to know the position of each token relative to one another because positionality matters right the order that you're playing the notes really is is what matters the most. are the original Pachelbel's Canon um as you can see this does deviate a bit but honestly it sounds pretty good. The Transformer model is able to do this next note next sequence prediction pretty pretty well. So yeah there's a a lot to do in this field um a lot of really cool things happening um and yeah I hope you guys learned something about uh about generative audio today and are inspired to kind of give some of these things a try yourself. thank you guys for tuning in have a good one.