presenting okay share your screen that's what I'm doing oh you are okay hopefully it is yeah stop sharing yeah you should be sharing my screen under your camera until I can decide if I click on slideshow this is still show my camera uh it does I guess I. I don't know if this works. I guess it does. I'm not sure if it does or not. If it doesn't work, stop sharing. If you're not sure, just stop sharing your screen. can minimize it do screen sharing are you recording too yeah great baseball back yeah I mean it's my first time giving my lecture so I'm as good as I can be do you want that cheers I mean I have to write something to hear me okay okay you. Can you hear me? Can you see me? Do you have a recording device that you can use to record a baseball game? If so, please share it with us. We'd love to hear from you. Please share your photos and videos of the game. know that it works I think like unless you get really close to it it doesn't just pull an ariancoil combustion a few more minutes about you foreign this was a good idea I should have done that it's fine I guess yeah I'm so nervous. I mean I am. I am so nervous I mean we are going to do this. We are doing this. It's going to be fine. We're going to get through it. I'm going to make it. We will get through this. a kid but I have like admins responsible because I don't need a sections foreign [Music] thank you okay let's see what's going to happen what are we supposed to be in the cereal and we are both presenting our place it's not happening are you going to be here. "I don't have a section of foreign" "I have a sections of foreign." "I'm not a kid. I'm a kid." "We are both presented our place" "We're both presenting ourselves. It's not going to work" and when you're presenting am I going to be here as well it's almost 7 10. maybe like maybe two minutes at least yeah yeah we will get started in a few minutes [Music] foreign foreign probably a good time to start so hey my name is Aryan I'm one. I'm Aryan and I'll be here for the next few minutes. I've got a few questions for you. If you want to talk to me about anything, please contact me at [email protected] of the facilitators for this decal and I'm going to be presenting today's lecture on pre-training with Verona today so I guess uh for important announcements uh I think homework one is due next Tuesday so probably start that if you haven't already. The quiz for the decal will be due on Tuesday. The decal is due on November 1st and the quiz will be on November 8th. The Decal is being designed by the team at Verona University in Verona, Italy. this week will probably go live tomorrow uh not quite sure yet but you'll try to get it up as soon as possible so I guess like without further Ado let's Jump Right In. So I guess before we start the official pre-training part of this lecture um I'm gonna. gonna. uh I'm going to get into the pre- training part of the lecture. This week's show will be up tomorrow or Wednesday, I'm not sure when yet but I'll try and get it live as early as possible. go somewhat into detail into what representation learning is and I think this should sort of cap out the last few weeks of deep learning um and probably give you a more comprehensive understanding of what deep learning actually is doing and yeah hopefully the last lectures will make more sense of what it is to be a deep learner and what it means to be an learner in the context of a big data environment. The last lecture will go into more detail about how deep learning works and how it can be applied to the big data world. sense after this context so I guess before we jump into deep learning let's talk a bit about shallow learning. So say you have the classical machine learning problem and the way it is set up is you have some input X and you extract the features from this input. The way this is done is called shallow learning and it is used to extract features from input X. This is a way of extracting features from the input X by extracting features in X. It's a very simple way to do it. X and you pass it into some model that is going to be parametrized by some data to get an output y. Keep in mind that this sort of theta is. going to contain all of your learned parameters so if you have a neural network this would contain. all of the learned parameters. X would contain X and y would be the data that was passed into the model to get the output. Y would contain the parameters that were learned by the model and the data it was fed into it. all of your weights biases any other things that you might want to learn if this is say a regression model this would just contain the weights and maybe a bias term if it's if if one is there but yeah this this data is is going to represent all the data that you have in your model. If you want to change the data, you can do so by changing the size of the data set. You can also change the type of data you have by adding or subtracting some of the variables. of your rates and uh instead of passing in the input X directly we would extract some features from it using this function Phi which is what we're going to call a feature extractor. Now why don't we push why don's we input the term X in directly it's it. It's it, it's the same thing, but it's a different way to do it. Now let's push it. We'll push it again. We're pushing it again, we're pushing this time it's X. might not be something that you can so let's say that you're working on a problem of predicting the price of a house from a house so your X can be a house you can really put that into your model right. You would have to extract some information about the house that you are trying to predict. You can't just predict a house's price just by looking at a house. You have to be able to see the whole house, not just the inside of it. You also have to look at the outside of the house to get the full picture. the house which is then something that you could input into a model so this information can be you know things like the number of rooms in the house. The size of the house how old it is that can be categorical variables like does it have a pool. The house can also be a categorical variable like does the house has a pool or is the house old or has it been built in the past. The information can then be input to a model to create a picture of a house. back here or a basement any of that so once you extract these relevant features you can get your output y or you canget a prediction Y and your goal is to learn these weights in such a way such that your predicted label your appropriate output is as close to the true level as possible. So just to sort of recap uh the machine learning pipeline you start with an input X you extract all the relevant features from it um and then you push those into a machine learning algorithm should get an output Y and you should get a predicted label Y. sort of optimize based on that so I guess now we defined this feature extractor or something that we need to we this is something thatWe need to Define right so and you might imagine that different kinds of problems will have different kind of feature extractors so if if you if you have a problem that you want to solve, you need to find a way to solve that problem. So that's what we're trying to do here. We need to figure out how to do that. your data is arranged in say a table so if we go back to the housing crisis example you could maybe say that each row is a single house and each column is one of the features. getting these features is pretty easy because you can just take each row and column and add them to the table. You can see the full list of features in the next section of this article. Click here to read the full transcript of the interview with John Defterios, head of software development at Google. row directly right or you can also maybe take a column depending on however the data is arranged in this tabular format. But what if your input is something complex like it can be text audio images right how do you extract all the relevant features from such a complex feature? How do you get all the information you need from a complex input? How can you get the most out of a complex data structure? What do you do with all the data that you have to work with? How are you going to use it? input um since this is a CV class I'm going to go over the CV example and turns out that there are special feature extractors for images so this is sort of what classical machine classical CV look like. You people would come up with all of these like different like different CV examples, I'm sure you can think of a lot of them. I'll be back in a few minutes with an example of what a CV looks like for you to work with. I'm back in my office. kinds of feature extractors one really common example is something called hog or histogram oriented gradients. It sort of captures the edge information in an image and based on that you can train some sort of like class fire model on top of that. So say this is this can say this can be a class fire fire model. It can be used to train a fire model as well. It's a very common example of a feature extractor that can be trained on an image. You can use it to train fire models for example. be an svm with learned weights for example. This feature extractor is something that you have to program yourself. This is not something that's being learned right now. It's something you come up with yourself based on your intuition about the situation. It is not a feature that is being taught right now, it is a feature you can program yourself to use. It can be used to help you with other problems with your svm. For example, it can help you deal with a problem with a large number of svm commands. problem whatever you think might be the most relevant features for this problem right and if you think about this this can be a very challenging task if you you can't really use the same feature extractor across multiple tasks right so if you want if you have a task you can use whatever you want to do with it right. If you want you can also use the feature extractors to help you with other tasks. If You want to help with another task, you can help with that too. You can also help with this by using the feature extraction tool. that has to do with the colors in an image this sort of these features won't really do anything because this gives you edge information right you have to define a different feature extractor does that make sense. This process of choosing the right features can get really complicated so this process of picking the right feature extractors can get very complicated. It can be really difficult to get the right information out of an image. You have to choose the features that give you the right edge information. This gives you the edge information you need to make the best use of the image. really fast and this is also kind of a compromise solution in the sense that you are learning the weights of your model but you're still hand programming the feature extractor yourself. Why don't we we want to make this whole process automatic right but as of right now only only use this feature right now. We want to made this wholeprocess automatic right. But as ofright now only use it right now as a way to test your models. We don't want to use it as a tool to test our models. the second half is automatic the learning of the weights we are still defining the features ourselves. This is sort of where deep learning comes in so deep learning says that hey we don't need to hand program feature extractors we can all we can learn those as well. The second half of the learning is automatic. The third half is manual. The fourth half is automatically learning the weights. The fifth half is learning the features themselves. The sixth half is automatic learning the weights. in fact we can learn the entire pipeline from feature extraction to um training and you could you just need to pass in this like raw image input and it will spit out an output and you won't have to hand program anything specific and uh in in either some of the pipeline. One example of this is you can use a something called a convolutional layer again don't worry about what a con layer is that is something that we will teach you guys next week but it is a neural network layer that can extract features from features. an image and this extractor has parameters that can be learned sensor of um something like something like hog which is sort of a very stationary in the sense that it doesn't really change you still have the same thing uh you can learn this feature extractor and you can use it to learn a feature of an image. An image can be extracted from an image by using a feature extracted from the image. The image can then be used to learn the parameters of the image and the parameters can be applied to the image to create an extractor. then pass these features into a learned algorithm so in in Post-its in both steps of this process you're learning something right and and you might think that okay we have two different steps and we're learningsomething but why keep those steps separate why decouple those two then what? So in in posts you're doing something right. Then you're going to do something wrong. Then what? You're doing it right. So you're not doing anything wrong. You're just doing something different. And so on and on. what if you combine them and that's exactly what a neural network is. It's a model that combines with feature extraction and output prediction and it learns everything from the data. So hopefully it just sort of gives you some context as to why deep learning has such a big impact on our society. It is a huge area of research and development in the field of machine learning. It has a huge impact on the way we see the world and how we interact with the world around us. It also has huge implications for the future of technology. been sort of taking off and classical machine learning is not as used and areas like Vision anymore because deep learning allows you to sort of automate this entire process from end to end. In a sense what you're really doing is you're learning a representation of your input and that's what deep learning is really good at. It's sort of like a super computer that can sort of learn from its environment. It can learn from the environment around it and learn how to respond to it in a way that makes it more efficient. right so your features are a way to represent what that input looks like because a model doesn't know what an image is. A model will know what the features of that image are because that's what it's receiving right so in a sense what deep learning is doing is doing that in this case. It's doing what a model would do if it didn't know exactly what it was receiving. It would know what features it wanted to have in an image. That's what a deep learning model is doing in this instance. it's allowing you to learn good abstract representations from the data itself and without having to manually do anything. The main idea is to sort of like relinquish all control to the model and let it learn whatever it needs to learn for whatever task it is trying to learn. It's a way of allowing people to learn abstract representations of data without manually doing any work with the data. It is a way to let people relinquish control of their data and let the model do the work for them. solve and in a sense you can view each layer in a neural network as a learned feature extractor. You are chaining these feature extractors on top of each other right so a single layer might receive some kind of input it learns how to best represent represent that. It's a very powerful tool to have in your toolbox. It can be used to improve the quality of data that is published on the Internet. It is also a powerful way to test the accuracy of your data. input and it can pass that representation along to another layer that comes after it so a deep neural network is basically learned representations that are stacked on top of each other. These representations are going to be kind of hierarchical so if you look at the the same input it will look the same to the next layer in the network. It can also be used to predict the future by looking for patterns in the data that are similar to what it has seen in the past. For more information, visit CNN.com/deep neural network. age right over here we have a neural network that whose goal is to sort of predict something about a car so then if you input in the image of a car you can see that the earlier layers what they're trying to do is they're try to look like a car. That's what we're doing here. We're looking at a car and trying to predict what it's going to do. We want to make sure that we don't end up with a car that doesn't look like us. for so like these like modules that you save like black and white well what this is doing is is trying to look for different kinds of edges in the car. So if you have a straight line it's trying to looking for a straight edge right so it's tried to find the right edge. For more information on how to get your hands on the car, visit www.cars.org.uk or call 1-800-273-8255. In the U.S. call the National Suicide Prevention Lifeline on 1-8457 90 90 90 or visit a local Samaritans branch or click here. to look for different kinds of patterns and like low level and likeLow level information like edges from the image. As you go further along instead of looking at these low level details you're trying to look at something more concrete so a later layer might be trying to do the same thing. You can also look for patterns and patterns in the background of an image. For more information on how to create your own video, visit iReport.com. For the latest from CNN iReport, visit CNN.com/Travel. to see if this image has say a veal maybe a door maybe a window something else that might be in a car. By the very end the final layers of trying to see if the input image had occurred in the first place now this model has a car in the background. The final layer is to check if the image has a door, window, or something else in it. The model has also had a door in the past and a window in the future. The last layer of the model is to try and see if there is anything in the image that could be a door or window. mental model of what a car looks like it's not something that we know but it's some sort of abstract model that only the model knows. As you can see that as you go deeper down the network is trying to become more and more abstract so I guess I guess that's what we're trying to do. I think we're going to see a lot more of this in the future. It's a very exciting time for the future of the car industry. I'm looking forward to seeing what the future holds. " depth refines representations you start with course information like edges and you go all the way down to like find information like this mental model of a car so I know that this was a good mental model," he says. "The thing I want you to take away from this is that depthRefines representations," he adds. "That's what we're trying to do here. That's what I'm trying to get out of the students. I want them to be able to understand what I've been trying to show them" lot of information does anyone have any questions about any of the any of this uh in case nudge I'm gonna pass the my country Verona who will talk about transfer learning okay yeah so now that we've sort of gone over an overview of what representation learning is we'll go into more detail about what transfer learning is and what it means to be a student of transfer learning. We'll go back to the start of the show and talk about how we got to this point in the program. talk a little bit about what transfer learning is and what the benefits might be. When we train a model from scratch which we don't usually do a lot of the times uh it takes a lot. of time compute and training data so to just to give you an idea of the benefits of transfer learning, we train our models from scratch. We don't do this a lot, but when we do we train them from scratch, we do it in a way that is very different from other models. an idea of how much data is often necessary to train a pretty good model even just a few thousand examples is often not nearly enough. It takes a lot of data for a model to learn properly and of course it's also very expensive to fit a model with all the data needed to make it work. A model needs to be able to learn from a large amount of data to be effective. It can also take a long time to learn how to use a particular data set to make a model more effective. but luckily huge models have already been trained before so the sort of question is can we leverage them in some way and the answer is yes absolutely. You might have heard of what we've called pre-trained models and many of these pre- trained models are frequently used all the time. Many of these models are often used in the production of our video games and we're looking at how we can use them in a similar way in the future. We're also looking at ways to use the data to create more realistic 3D models for our games. time and so we can take a look at how and why we might want to use them so if we train a model from scratch our model parameters or our weights are randomly initialized in the beginning. Then we update them gradually through an optimization algorithm such as an optimizationgorithm such as optimization.time. We can also look at the effects of different types of data and how we can use them to train our models. For example, we could use data that is more or less noisy for a particular type of data. stochastic radio descent or something like atom and so let's say for example that we have two separate tasks that we want to solve using some sort of deep learning technique. Without transfer learning we might have to train these two models separately from scratch which as I mentioned is a big problem. We might need to train the two models from scratch to solve the same task. This is called transfer learning and it is used to train new models. We use transfer learning to teach new models how to solve problems. earlier is very costly in terms of time compute and data so the idea is can we do better and so when we learn something we often find in general you know just like us ourselves what we can use what we've learned already and apply them our skills and skills and so on. The idea is that we can learn from each other and learn from our mistakes and improve our own skills and our own knowledge of the world around us. We hope this article has helped you in any way. our knowledge to other domains right so just to give a little example you know once we've taken a sort of intro programming course like 61a we don't continue to learn programming from scratch each time we have the program where we take another course. We don't learn from scratch every time we take a new course, we learn from previous ones. We learn from the previous ones that we've already taken. We can then apply that knowledge to a new area of our lives. We are able to apply our knowledge to new areas of our life. already know the high level important ideas and we only need to focus on the new stuff Factor so we can see basically apply the same idea to our neural networks so as an example let's say we have a convolutional neural network trained for a single or a single. We can see the same thing applied to a neural network that is trained for an image or a series of images. We also can see how this can be applied to other types of data, such as text and images. simple computer vision task like a cat versus dog classification or some sort of object detection and again no worries if a convolution is sort of familiar. We'll cover convolutions and CNN's in much greater depth very soon very soon um well notice that even though these tasks are slightly different from CNN's they are still very similar to CNN's. We will cover CNN's and convolutions in a lot more depth in the coming weeks and months. We hope to see you in the next few weeks. Each of these models should be able to capture how low-level features as Arya mentioned such as the general shape the edges the patterns and the patterns. For example um each of thesemodels should be. able to for example capture how high level features such as shapes, patterns and patterns are captured. Each of the models should. be able. to learn some sort of suitable representation of the respective inputs so for example um. each of them should be can to capture. how low level features are captured by the respective models. for example. the shape of the edges. colors um R for this data in the lower layer so the more earlier layers of our neural network. Then once we've moved on and we've gone towards the later layers they should be able to capture some higher level features such as the abstractions of the cats versus the dogs. We hope to use this technique in the future to help people understand more about the world around them. We'll be sharing the results in a blog post later this week. Click here for more information on how to use the technique. the dogs people's faces or some of the major objects in the later later layers. so here is a small example of some of these layers for some sort of object detection or classification task. In the first layer we can see that we have features like colors and colors and in the second layer we have some features like shapes and shapes. in the third and fourth layers we have objects like shapes, shapes, colors and other features such as shapes and colors. We can then use these layers to detect and classify objects. patterns that are similar uh tightly close to each other and then we have shapes and maybe some textures. Once we've gone to something like layer 3 usually just to give you an idea there are we don't usually train like a five layer they're all network but they are all network. We have shapes, textures, patterns and shapes and then once we have gone toSomething like Layer 3 usually. Just to giveYou an idea of what we're looking for. There are shapes and textures and then there are shapes, shapes, and textures. like a lot of the times they'll be like 100 layers for example depending on how deep we want it to be um so but in this example we start sort of abstracting in layer three um where we see objects and humans and then once we reach layer five we see humans and objects. Like a a lot a a times they will be like100 layers forexample depending onHow deep wewant it to go um so. But in this examples we start kind of abstracted in layer 3 and then we see people and objects in layer five. at the end we reach to a close to final level of the attraction that we fought and just for a slightly more concrete example let's say we train a residual neural network on the imagenet data set um so again don't worry about what a residual Neural Network is. We are now at the end of the first level of our story. We have now reached the second level of this story. The final level is the third level, which is the final level in the story. This is the last level in our story, and it will be shown in the next few hours. but basically we can train a deep residual neural network with many many layers or tasks like classification or output protection and good thing this has already basically been done for us. So how can we use this resnet imagenet classifier and transfer the knowledge that we've learned from this? We can use it to train our own classifier. We can also use this classifier to train other types of classifiers. It can also be used to train a classifier that can predict the outcome of an image. two other tasks so we want to figure out what aspects of this network is already shared among the other ones. We want to keep that shared information and then basically use that kept information for our other models. So how might we actually go about this um and how do we actually do this? And so on and so on. And so we go on to the next task. And then we go to the third task. and then the fourth. and the fifth. and finally the final task. so in other words like how do we actually transfer or do transfer learning um does anyone have any ideas perhaps of how we might want to keep certain information from a previous pre-trained model and transfer that knowledge yeah that's basically the right idea. What about you you're just. just a person. what about you? What do you think? Share your thoughts in the comments below or tweet us @dailymail.co.uk with the hashtag #dailymailcomentor. We would like to hear from you about your thoughts on transfer learning. copy the weights Maybe yeah so very similar um one of the more common ideas is that well basically our neural networks are just stacks of layers so the idea is can we keep certain layers and basically use that on our next model so there's an idea called freezing. Copy the weights is a way to get rid of layers in a neural network. The idea is to freeze the layers and then use them in the next version of the model. It's similar to the idea of freezing a computer program to make it more efficient. so that's basically what you guys both mentioned one idea is to freeze certain layers of our neural network so basically we use our pre-trained Network which is like the already trained resnet classifier this card a few of the layer layers and then freeze the remaining earlier layers and that's what we did. So that was basically what we were trying to do. We used the pre- trained Network and then we froze the remaining layers of the neural network and that was what we used to create the image. so we can add and train the later layers to basically customize them in a way that sort of satisfies the tasks that we want to perform on uh for the second task and also remember how the later layer are often the higher level abstractions and the earlier layers are the earlier ones. So we can basically customize the later Layer to sort of satisfy the tasks we want on uh. For example, we can train the layers to be able to perform the task that we need to do on the second layer. are the general shapes edges patterns um so we generally want to keep those General shapes textures patterns. But our abstractions might be a bit different depending on the type of tasks that we you know we want to solve on our second task so we can add and train. So we might want to start with the basic shapes and then work our way up to more complex shapes and patterns as we go. We might also want to work on the edges of the shapes as well as the corners of them. We can work on this as we progress through the tasks. our new custom layer later layers and the layer weights of a trained model are not changed when they are reused in a downstream task. By freezing layers you might imagine we're not really modifying the weights or parameters and so that backward pass that we talked about earlier is not changed. We're not changing the parameters or weights of the model. We are just freezing the layers and we are not modifying the layers or parameters of the trained model. This is a step in the right direction for the future of machine learning. in the past few lectures can be basically avoided so the speed of our model increases by a lot by doing this. um just sort of a note is when you're trying to freeze certain layers be careful of where you're freezing your your model so if you freeze layers, don't freeze them in the same place as the rest of the model. So that's one of the things that we can do to speed up our model a little bit. We can also do this in other ways to increase our speed. too early on um that's basically sort of useless like this can lead to pretty inaccurate predictions because you're not really understanding the low-level edges for example. So a CNN model with several several layers that's trained on can be very inaccurate. So that's a good example of what we're trying to do here. We're using the CNN model to try and predict what the future will look like. It's a very complex model. It has several layers. It can be used to predict the future. a pretty large image data set like imagenet it can be reused by removing the last few layers. Depending on the task that you're you're trying to do the second task you can either remove just like one or a few layers and then have the model classify some new images. The model can then be used to create some new image data sets. For example, the model could create a new set of images for a large image set like the image of the U.S. Capitol. image categories for example so instead of cats or dogs maybe let's say you want to I don't know understand like how to classify another animal um so you might want to just change the last few layers um or you can also use like a larger data set okay. So you might be able to use this to help you understand how to categorise another animal or something like that. OK. So that's what we're going to be looking at. We're looking at how you can use it to help people understand animals. so another common technique is called fine tuning so instead of freezing our layers we can fine tune our pre-trained neural network. Instead of discarding the later layers, we can basically train ourPre-trained model a bit more on some output layers and non-frozen layers from our original neuralnetwork. So instead of Discarding the Later layers, We can basically Train Our Pre-trained Model a bit More on Some Output layers and Non-f frozen layers from Our original neural network and so on. network so the more data that we generally have for our Downstream task the more that we can unfreeze the layers of our original data original model and then fine-tune them again for our specific second task. Sometimes we might want to fine-Tune the whole pre-trained network rather than just the pre- trained network rather then the original model. The more data we have, the more we can free up the layers to be fine-tuned for the specific task. than just the unfrozen layers um so essentially we sort ofinitial our new neural network to the pre-trained network the weights instead of initializing them at random. So again this speeds up our training process as the parameters of the neural network is taken from the pre,trained parameters. Okay so um one one we're initializing our weights initially in the very uh randomly in the beginning. So now we can take thepre-trained the weights from thePre- trained network and use those. other question that you might have is how do we sort of decide between whether we want to freeze or fine tune um so there are a few main factors that you Might want to consider but the two main important ones are the size of your new data set and how much data you want to keep. You might also want to think about how you're going to use the data in the first place and how it's going to be used in the future. You may want to look at how you use it in the past and how you'll use it now. um and the similarity of the new data set to the original data set. So for example in the case one where you have um a small small data set or a lot a large data set and it's pretty similar to theOriginal data set since it's larger we use this data set as a basis for comparing two data sets. We use the data set size as a proxy for the size of the data sets that we are comparing two sets of data to each other to see how similar the two sets are. have more confidence that we won't overfit if we fine-tune so we've briefly talked about overfitting in the past. This is a topic that will continue to come up when we train um uh an hour and so um on the other hand if we have a smaller data. If we have less data, we have more confidence we will not overfit. We've talked about this before and we'll continue to talk about it. It's a very important part of our training. set even though it's similar to the original model it's not a good idea sometimes to fine tune. "You can definitely overfit um and then in the third case in which you have like a small small data set and then it's pretty different from the first task um," he says. "It's not really a good thing sometimes tofine tune because you can definitelyoverfit um" "You have to be very careful that you don't overfit the data set," he adds. "Because if you overfit it, you're going to end up with a very different data set" you can probably um fix some of the initial layers but you don't want to really fine tune and then at the very end you have a very large second data set but it's very different from the first task so you can probably just change it from scratch. You can probably fix it but it will take a long time to get it right. You'll probably need to start from the beginning and work your way up to the end. You will probably need a lot of data to work with. in practice it's mostly beneficial toinitial your weights from the pre-trained patient model okay so um before we sort of get into embeddings um just some practical advice so just a few other things to keep in mind when you're performing transfer learning um so think about the constraints okay so think of the constraints and then think about how you're going to embed the data into the patient model. Okay so that's what we're doing here. We're embedding the patient into a patient model to learn about the patient. of your pre-trained models um think about what sort of tasks it was sort of trying to accomplish in the beginning um the data that it was trained on and then also consider your learning rates so we'll talk a little bit more about learning rates and greater depth again. Back to the page you came from. Follow us on Twitter @CNNOpinion and @cnnopinions. Follow CNN Living on Facebook and Twitter. We'll also be posting on CNN iReport and CNN Tech. next week when we talk about some of these more particular models okay um in terms of neural networks embeddings are pretty important so they are often described as lower dimensional learns continuous Vector representations of discrete variables. No network embeds are useful because they can reduce the dimensionality of a model. Next week we will look at some of the more specific models that we can use in this kind of analysis. Back to the page you came from. Click here to read the transcript of this interview. of your categorical variables for example and meaningfully represent them in the transform space. So in this photo example they've decided to use teasney for the dimensionality reduction so taking the embedding Vector dimensions and mapping them to a 2d space in this case um plotted they also plotted the categorical variable in the transformed space in the photo example. In this example we've taken the embeddings and mapped them to two 2d spaces. In the image below we've also taken the dimensions and plotted them in 2d and 3d. embeddings and then they color coded based on the genre of the book so they took like book data and then basically did some dimensionality reduction um so just as a summary like embeddings are pretty useful for like represent representing High dimensional data okay and very very closely linked to each other. The book was published by Simon & Schuster in the UK and the U.S. It was published in the United States by Random House and is available on Amazon.com and other retailers. to embeddings to something called a latent space. We often prefer to work with lower dimensional data so a common task is transforming them in from high dimensional data into lower dimensional instructors. We can capture this through something called latent space of features which basically encode all features of the data. To see more of our work on this topic visit our blog and Twitter accounts. For more information, visit: www.dailymail.co.uk/news/features/2013/01/30/how-do-we-turn-high-dimensional-data-into-lower-dimensional- instructors-instructors.html. of the important information that represents a high dimensional the high dimensional data. High dimensional data is embedded in a lower dimensional space latent space and a lot of the time latent space is sort of interchangeable. We often say that high dimensionalData is embedded into a lowerdimensional space latentSpace. Highdimensional data isembedded in aLowerDimensional Data is Embedded in LowerDimensionalData. High Dimensional Data embedded in Lower DimensionalData is Embedded in Lower dimensional Data. Low dimensional Data Embedding embedded in Low Dimensional data. with like an embedding space um and so something to note here is that what we offer is what we often call the curse of dimensionality um. One of the reasons that high dimensional data or high dimensional spaces can be bad is because if the data is naturally a high dimensional space, it can't be easily embedded into another space. So we offer what we call a 'high dimensional embedding' space with an embeddable embeddability space that can be used to embed data. lower dimensional structure it's going to be very sparsely spread out in your latent space as you can imagine um in high dimensional spaces so um a lot of the times you want to be careful about yeah I don't know you can sort of draw something yeah but like like like in a high dimensional space you can't draw something like that. Lower dimensional structure is going to have a very low dimensional structure so you have to make sure that it's very sparse and spread out. Lowerdimensional structure is a very high dimensional structure. you can imagine like if something is naturally a lower dimensional structure in high dimensional spaces it's going to be like very very sparse so this is an example that I came up with so say that you have the 3D space but your data is along this line now. You can imagine that if you have a 3D data set and you're in a high dimensional space you're going to have a very sparse data set. So say that your data set is in 3D but it's in a higher dimensional space. your data is basically has a one-dimensional structure right you don't need to work with the entire 3D space because you don’t need to. This sort of goes back to the idea of course dimensionality because you can see that all of these points occupy a very small region. It’s a very simple way of looking at data. It's very simple to see how to do it and how to use it in a way that makes it look more like 3D data. of space in 3D right so the idea is that if we can directly work with the significant somehow find a way to represent this line using just one variable instead of three and so here's let's say the XYZ coordinates that's going to be better because like a high. of space in 2D right. So the idea was to find out how to represent that line in a 3D world. So we can work with that in a 2D world and then work with it in a3D world to get the same result. dimensional data can be very complex we want to avoid that as much as possible thanks okay and so obviously one of the big questions is how do we actually get our embedding so well we can basically learn them and so one way is to learn and embedding as much of the data as possible. Thanks a lot for your time. Back to Mail Online home. back to the page you came from. Click here to go to the next part of the interview with David Wallis, the author of the book, The Art of Data Science. part of our neural network for our Target task so this sort of allows us to get in a bedding that's nicely customized for our particular task but it may take longer than training the embedding separately um and the idea here is that we can basically reuse our embeddings. The embedding is a part of the neural network that's trained for our target task. It's trained separately for the Target task and then used to train the rest of the network for the task. The training process can take up to 24 hours. for other tasks again embeddings they're sort of just like what we talked about earlier the broad ideas that we're trying to um represent our data in meaningful ways um so here's sort of an example um honestly it'ssort of like I don't know we talk about we don't. We don't talk about it. We just don't have the time to do it. So here's an example of one of the things we're doing. We're using it to sort of represent some of the data that we have in our system. really quite talk a lot about soft soft Max but um you can sort of see how there's different ways like we have um one hot Target probabilities um so it's like pretty sparse as we talked about earlier so um I didn't do this do my run this part this part. Really quite talkA lot aboutSoftSoftMax.com. Max is the author of SoftSoftMax: The Ultimate Guide to Soft Soft Max, published by Simon & Schuster, priced $39.99. For more information on softSoftMax, visit softsoftmax.com or go to www.softsoftMax.org. what's up the target Boston oh yeah um oh so this example uh it's an example about uh training something called an mnist classifier so feminist is a data set of hand uh ROM digits it's a 28 by 28 image which you can actually represent using a single Vector. What's up  Boston ohYeah um ohso this example is about training a classifier called anmnist. so Feminist is aDataSet of Hand uhROM digits it’s a 28By28 image whichyou can actually representation using asingle Vector. as a 70 784 dimensional Vector plus 25 to 784. so what you can do is you can train a model that would classify um what digit the image contains um using this and and it's going to be one of the 10 digits uh from zero to nine this is a good way to train the model. It's a very simple way to teach a model what a digit looks like in an image. It can be used to train models for other types of images as well, such as computer vision. is what those target class labels sort of um identify over there. You train this using a soft Max loss because soft Max is the loss that is used for classification. It's basically because it's a target class. Is what those targets sort of sort of identify over here. Is that what you're looking for? That's what we're trying to figure out. Is it a target or is it a class? Is that the target or the class that you're aiming for? Is it the class of the target that you want to identify? loss function don't worry too much about it so the idea is that once you train this model you can take um can you see my cursor you can. Once this model has been trained these three like set up neurons in the middle of the model can then be used to train a new model. You can then use this model to train your own computer models. You will then be able to use these three models to train computer models of your own. For more information on how to train models, see www.cnn.com/training. used as an embedding for this mnist image so once this model has been trained this the weights must have learned something meaningful right so this means that we could just take some of these layers in the middle take the output of that as a representation of this 20. We could then use some of this output to create the image we want to show. We can then use this image to show how the model would look if it had been trained on a different image. We then used this image as a model to create a new image using this new image. Here's an example of some of the results from training networks from scratch versus applying some sort of transfer learning um from a paper. In this example the authors compared pre-changed convolutional neural networks for audio classification using 28 by 28 dimensional image. This example is based on a paper by the authors of the University of California, San Diego. It's based on an image of a 28-by-28 dimensional by 28-dimensional image. It was trained using a convolutionalist neural network. The results of this experiment were compared to the results of training the network from scratch. transfer learning and they found that the retrained models with transfer learning applied actually achieved better accuracy classification accuracy than retraining the network from scratch. So not only is it let's computationally expensive but it also helps achieve better results so here's sort of another summary of some of the results of the study. The study was published in the journal Applied Machine Learning. It was published by the Proceedings of the National Academy of Sciences (PNAS) and the American College of computer science (ACCS). major advantages of pre-trained networks so a lot of the time pre- trained networks are trained on very very large data sets and oftentimes again more data means better representations so if we're only given a little data um using some sort ofpre-trained network can be a great idea. um using pre-training networks can be an great idea um. Back to Mail Online home. back to the page you came from. Back To the pageyou came from, Back to the pages you came From. there are also a lot of free trained networks that we can use immediately so you can use models that haven't trained on large data sets already. Just search them up on like hugging base or something or import them directly and use them. We can also pre-compute and use these networks. We use them to train our models on data sets that are too large to be trained on. We also use them for training models that are already trained on a large data set. We're also using them to test our models and train new models. store our embeddings instead of using the original High dimensional data which can again save a lot of time and Storage okay and so here's just a very very broad summary so without transfer learning we're basically trying to learn two separate tasks and train our model separately um but that's a pretty broad summary of what transfer learning is. So that's what we're trying to do with transfer learning. That's a very, very broad, summary of transfer learning, and that's how we train our models. then with transfer learning we basically apply the knowledge that we've learned from one pre-trained network and apply that to the second task. So we've talked about two main techniques for doing that freezing some layers and fine-tuning our our Patriot Network um yeah so very broad once again once again. It's a very broad topic. We're going to talk about it a little bit more in the next few minutes and then we'll move on to the next task. We'll talk about how we use the Patriot Network in the future. you basically just apply your knowledge um and try to transfer that to another task instead of relearning everything um and just a quick note we'll go into some details and examples of this in action but especially um I know this is not an NLP course we talk more about how to apply knowledge to a new task. You basically just applied your knowledge and tried to transfer it to a different task. It's a very simple way to do it, but it can take a lot of practice.  transforming learning is especially huge in NLP as you can imagine with natural language. Once you've trained on a large large amount of text there's no reason for you to relearn all of that um so you already know the meaning so there's like some semantic meaning.about CV but transforming learning can be a huge part of the NLP process. For more information on how to use NLP in your business, visit NLP.com or go to www.nLP.co.uk. of words once you've pre-trained you can understand syntax so you can imagine in tasks for natural language processing it takes a lot of time if you want to retrain an entire network. Pre-trained networks and using transfer learning is a really really good idea but not only for language but also for other types of data such as images and data on the web. It's a very good idea to use pre- trained networks and use transfer learning. It can be used for a variety of tasks such as image recognition and data analysis. does that apply to NLP it applies to almost other domains every other domain xcp yeah okay so we will transition to the next part of the lecture which is going to be on self-supervised free training. Let's clarify some terminology so before I tell you what self- supervised is let's clarifySome terminology so that we can get to the point of the talk. We are going to talk about the concept of self- Supervised Free Training, which is a form of free training for people who want to learn how to use NLP to improve their knowledge of the world. Machine learning usually works like this: you take some input X which is going to be your raw data and it gives you some output say y hat and you define this. If you think back to how we describe the machine learning setup to you, the way it usually works is you defines a model. You define a model and then it takes that model and turns it into an output. That's how machine learning works. It's a very complex process. It takes a lot of work to get it to work the right way. notion of a loss that sort of Compares how far apart this prediction is from the ground truth label y right and your goal is to optimize the network such that this error decreases and your model is trying to output something that is very close to the actual labels. Compares the prediction to how close it is to the prediction of the actual label y. If the prediction is too far away from the prediction, this is called an error. If it is too close, it is called a gain. y . so in a sense your training process is receiving supervision from the labels your super your labels are guiding what the model must learn and some examples of and and this whole process is called um supervised learning like the name suggests. Some examples of supervised learning and how to use it in your own training.y . So in asense your training. process is receive supervision from your super. Your labels are guides what the models must learn. and some example of and. this wholeprocess is calledUm supervised learning. can be your typical classification problem like the one that we just showed where you classify digits you're checking in some image of a handwritten digit and you have a label corresponding to that. It can also be something like regression if you have kick in say 16 a or 16 a. or 16 or 16. A problem like this can be solved by using a series of letters and numbers. For more information on how to solve this problem, go to www.cnn.com/questions. 16b I think you might have seen regression in those classes um there are other examples of object detection segmentation we will discuss those in the coming weeks. So now we know that we can learn if we have both the labels and the raw data do you think we could learn more from this? If so, please share your thoughts in the comments below or post a video on how you would like to learn more about this type of data. If you have any questions, please send them to jennifer.smith@mailonline.co.uk. can learn if we just have the raw data and it turns out we can so even without any labels we can still learn something meaningful about the structure of the rawData. How many have you guys taken 16b before so yeah you might have you might recall something. So yeah you may have. You might have. you might Recall something. You may have taken 16B before soYeah you might. remember something. If you have any questions, please feel free to contact us at jennifer.smith@mailonline.com. called PCA from the class of principle component analysis it's actually one of the most common unsupervised learning algorithms out there. If you remember correctly you just input some data Matrix into that algorithm and it splits out those principles common inductors right you don't you never feed in. It's actually a very simple algorithm that can be used by anyone to learn new skills. It can also be used to teach people how to use a computer to read a book. It was developed by computer scientists at the University of California, Berkeley. any labels into that algorithm you just feed in the data Matrix. um if you remember from the 16v car one thing that you did was to you you took the audio signals from the words that you would pass to the car and you would cluster them together again. You would cluster the words together again and so on. So that's what we do with the data. We put it into an algorithm and we feed it in to the Matrix. And that's how we get the data we get. there was there were no labeling involved you just took each audio signal um projected it down to two dimensions and clustered them with other points. So yeah it turns out that dimensional eruption with PCA clustering etc etc are common examples of unsupervised learning and this is sort of sort of an example of that. There was no labelinginvolved. You just tookeach audio signal and projected it into two dimensions. SoYeah it turnsout that dimensional eruptions with PC a clustering and etc arecommon examples ofUnsupervised Learning and this was sort of one of them. and this is sort of uh hopefully it will give you a clearer picture of what's going on so in the first picture your different points and they have plot labels associated with them. So in a classification task you're going to predict what the labels are and you can do that in this picture. And then you can go on to the next picture and do the same thing. And so on and so on until you get to the bottom of the page. And that's what this picture does. draw like decision boundaries based on that but even if you don't have any labels the model can still learn that okay these points are dripping up together they're forming clusters and this is still like meaningful information that the models can learn. So yeah hopefully it's sort of this. Hopefully it's kind of this sort of like decision-making process that we're going to be able to use in the future. We'll see how far we can go in the next few years. We're still a long way away from that. picture makes clear the difference between unsupervised and supervised learning. The examples that we have discussed so far when we were going about transfer learning was supervised pre-training. We take these large models that were trained on say something like imagenet and usually these models like a resnet. picture makes clear that the examples we have shown so far so far were supervised before-training so we can use them for transfer learning. We can also use them to teach people how to transfer learning to a new environment. are trained for the image net classification task the image classification task is where you train a CNN on the imagenet data set which has a million different images and a thousand different classes and it needs and the cluster has to learn to like classify those images correctly and it's a very complex task to train a network to do. The CNN is trained on a data set of 1.5 million images and 1,000 different classes. It is then trained on the data set to learn how to classify the images correctly. since this is classification is a supervised learning problem you have a label associated with each of the each images in the data. We mentioned before that large data sets are helpful for learning more generalizable representations. What if we take this idea further image not image not is a better way to learn about an image. We can use this idea to learn more about the shape of an image by looking at it from the perspective of a large data set. This is an example of how we can use a large set of images to learn how to classify an object. only has a million examples but you can find like a billion images on the web or even trillions right so what if you try to like harness all of them to learn representations and this goes beyond CV as well so a common data set that people use in is the one that you can use in your own work. You can use this data set to learn more about yourself and your job. It can also be used to learn about other people and help you improve your job performance. It's a great way to teach people how to work better. NLP is the English Wikipedia so and this usually has like hundreds of millions of text tokens in that but you can find like a trillion tokens on the internet of text right. So if you try to harness all of this information and maybe you can learn better representations, that's what NLP is about. It's about learning how to make better representations out of text. That's why it's called NLP and it's used in computer science and other fields. It can be used to learn how to better understand the world. using that and turns out this is a pretty good idea also but it turns out that these large data sets are usually not labeled you can you could like scrap text or image or images from the web but you can't really label them automatically right so a lot of the data is not labeled right so it's a lot harder to use than you might think. It's also a lot more expensive than you would think to do it on your own. It takes a long time to get the data ready to use. of the time you're working with unlabeled data so we want to see if we could use unsupervised learning techniques to the sum level data sets and learn representations using that. This is also appealing because labeling in general is a very time consuming and tedious process say that the researchers at the University of California, Los Angeles, are working on a project that uses unsuper supervised learning to learn representations from data sets. The project is called Unsupervised Learning for Data Structures (ULSD) you want to label a billion images you would have to hire manual labor. You would also have to pay for storage. It would be very time consuming it would also be very expensive and it just turns out that's how it is. It's a lot of work. It is a very, very big job and it's a very expensive job. It takes a long time to do a billion pictures. You have to start at the beginning and work your way up. It just happens that way. that Gathering good labels is simply a very very hard process and so that the question that research has asked is if it could do unsupervised representation learning. And indeed we can so before I delve into that I want to clarify one more term so the valency provides representation. So the valencies provides representation and so does the label. And so on and so on until we get to the bottom of it. And that is what we are going to do. And we will do it in the next few weeks. learning is done is to play to something called self-supervised learning now like the name suggests the data is receiving supervision from itself you still don't have any labels with the in the data set you still only have say a collection of images but what you can do is you can create labels from those images and train in a supervised manner now how does this typically done is uh I'll just read off the statement from the Facebook research blog is the general technique of self- supervised learning is to predict any unobserved or hidden. part or property of the input from any observed or unhidden part of the inputs. This information can be used to predict the hidden part of an image. For example, if you hide some information about the image from the model and the model has to predict this hidden part from the unhidden parts of the image. The model can then predict this information from this hidden information. For more information on how to use this information, go to: http://www.cnn.com/2013/01/23/technology/features/machine-learning-machine-intelligence/top-five-features-how-to- predict-an-image.html. hidden across time or space we will go over some examples soon and actually we will have an entire lecture dedicated to self-supervised learning for um Envision in a few weeks so we'll double deeper in that lecture Cube. Some more terminology before before I go so again some more slang before before we go. Back to the page you came from. Follow us on Twitter @CNNOpinion and @jennifer_eiken. We'll also be posting a weekly Newsquiz on CNN iReport. on to examples so during a discussion of transfer learning we have been referring to two different tasks as task one and task two which is you know not a very descriptive name. It turns out that in the context of self-supervised learning these two tasks actually have a special name. We have been using task one as an example of task two. We are now going to use task two as a test to see how well we are doing with task one. We will then move on to the next example. name so the task on which you train the representations you know what what we have been referring as to cast one for so long is also called a pretext task and the task tourist representations are transferred down to are also a downstream task. Different domains we different domains we use are also called downstream task now different domain we use is downstream task and downstream task is the same thing as downstream task, so we can use downstream task to train representations. We can also use downstream tasks to train tourist representations and tourist representations to train tourists. have different kinds of Downstream tasks so in computer vision this can be something like you learn the representations from some pretext tasks and you'll use those representations for image classification or object detection or semantic segmentation or whatever. For NLP this could be somethinglike text classification machine translation. This could beSomething like text classificationMachine translation for NLP. It could also be a way to teach people how to read text. It can also be used to teach them how to understand text. For more information on how to use NLP in your business, visit www.nLP-project.org. document summarization question answering any of that uh it turns out that this is also possible for RL you could pre-train something called a policy and Ro and then find him that later for different kinds of tasks again don't worry if you know if you don't know what that is. If you know what a policy is, then you can train Ro to do it for you. You can also train Ro for different tasks by training him to do a different kind of task. For example, he could do a job for you and then do it again for you later. Great training is a very broad topic and self-supervised learning algorithms can be applied to different domains so I guess we can delve into examples now so this was a paper that was published in 2017 and it was called jigsaw what.means but yeah just wanted to show that great training is very broad and can be used in a variety of ways. It's a great example of how to use a self- Supervised Learning Algorithm to train people to be better at a particular task. the authors do is they take an image they take some part of the image and divide it into a three by three grid so you get nine patches. They Shuffle the patches around and ask the model to predict the original order so the hope is that if the model can learn to predict it will try to learn something meaningful about the image so it's not just focusing on say all level features. The authors do this by taking an image and dividing it into nine patches and then shuffling them around. anymore but it's also trying to understand what's going on in the image it's it's going to learn that an image can be made up of different parts and those parts are going to be related to each other. um there are some more technical details on the slides um there's a lot more to come in the next few days as we go through the rest of the film. We'll be back on Monday with the next installment of "This Is Life" at 9 p.m. ET. I won't go into those but something that the authors actually did I actually go mention this is when they sample the patches and they divide it into a grid instead of taking the grid directly they actually generate each patch a bit so the patches are like not next. I won't going into those. But something that they actually did was when they samples the patches they divide them into a Grid. Instead of taking a grid directly, they actually generated each patches a bit. So the patches were like not the next next. to each other because if they are the model can just learn to see if the pixels along the edge match each other. It really won't learn it anything in that case because it will just be you know because like matching pixels is a very easy task it's very similar to matching pixels on a computer screen. It's just a very simple thing to do. It doesn't have to be very complex. It just has to be something that looks like an edge. It can just look at the edge and see if it matches the pixels. a very easy way to like cheat this process right so which is why you might see a non-perfect written image a any questions about this task before I move on yeah so what they did is they actually take they took the representations from this pre-text task and tested and that's how they got the images. So what they do is they take they take the representations and tested them and that’s how they get the images back. So that's what they got back. it on classification and protection um Downstream tasks um I think I have some numbers up there uh it turned out that this was sort of the best um pre-text task at the time and it actually and then it actually sort of bridged the gap between supervised and self-supervised. It actually was kind of the first step in the development of self- supervision. It was the first time that a computer program was able to do this sort of thing and it was a big step for the future of computers. learning in a sense um on these different uh classification and detection tasks so yeah another task is the rotation sorry go ahead yeah so I guess this task right like so the internet um remember correctly I think they took like most of the backbones from the original model from the internet model. I think that's what they did. So yeah I guess that's part of the task. So that's the rotation. That's what the rotation is. And then there's also the rotation of the internet. And that's another task. and I think they just took the thing that created that as like a frozen feature extractor representations. In that way another task is something called broadnet uh it's sort of a similar idea but instead of predicting let's say a shuffle order what you do instead is you take an image of a person and extract it from it. And that's kind of what they did with this task. They took an image and extracted it from the person and from the image of the person. And they took that image and made it into a broadnet image. image you you rotate it by some number of degrees and which is selected from zero 90 degrees 180 or 270 degrees. You ask the model to predict the rotation angle from the from this rotated image and the hope is that and then if you look at the image you can see the angle of the rotated image. The model is then able to predict how the image will look when it is rotated by a certain amount of degrees. For more information on how to use the model, go to www.cnn.com/science. sort of example over here even if you rotate this image of a bird the fact that it's beak and it's either close together doesn't really change that there's still going to be closer whether in each image and the fact of it's sort of claws are pushing on this. Even if you rotated this image over here over here it still doesn't change that the beak is either close or not close together. It's the same with each image. It doesn't matter how you rotate the image. branch is still going to be the same in each image so the hope is that the model can learn some of that information. A single image might have different kinds of objects and it might have to learn to focus on say something like the object's orientation. The hope is the model will be able to learn the orientation of an object in a single image. The project is funded by the National Science Foundation and the University of California, Los Angeles. For more information on the project, visit: www.nscf.org. location pose type etc etc instead of just focusing on again low level details. I think this sort of next example makes it more clearer so on the left hand side you have a model that was trained in a fully supervised manner and when you look at what is on the right hand side I think it makes it a lot more clear. I hope this helps you with your understanding of what a model is supposed to look like and how it should be used for your work. Thank you for your time. focusing on on a given image it's looking at a single part at at one time but on the right hand side you have this model Illustrated using the rotation prediction task. Instead of focusing on a single parts it's looked at multiple Parts at the same time so it can rotate multiple parts at once. The model is then able to predict the rotation of a given part of an image. It then rotates the image so that it can focus on that part of the image at that time. if you look at this image on the bottom right it's looking at the eye of the cat and sort of it's not at the same time to see if there's like a relationship between those two. In the image of a dog it'sLooking at both the body and the body. In this image of the dog it is looking at both of the body as well as the dog's head. It's trying to show the relationship between the two body parts. The image is of a cat and a dog looking at each other's eyes. of the dog and the face at the same time to see if it could like maybe discern some sort of relationship between the two so maybe this is a way to like qualitatively show that this also provides learning algorithms are trying to learn something much more Beyond supervised. The study was published in the open-source journal Computer Vision and Pattern Recognition (CVPR) and is published by the Association for Computing Machinery (ACM). For more information, visit www.acm.org. training turns out that this example is not constrained to just um CV you can also do SSL with NLP um one really common example in NLP is something called word2back which and the world of work to work is to learn embeddings for a single word now the way to go is to use NLP to do SSL for a word. training turnsout that thisexample is not constraints to justUm CV youcan also doSSL with N LP to doSSL for aWord2back. this can be done is you could predict a word from its surrounding context so say if you have the sentence the dog with the man you could try to predict the word bed from dog and dog. A dog should kind of imply that the word bit is because a dog is a dog. The word bed is a word that implies a bed or a bed. It can also be used to predict words from their surrounding context. For example, a dog with a man and a man is a man with a dog so the word dog is bed. associated with it so this approach is called a continuous bag of words model. There are many other ways to train virtual back models one example is um skip Ram so instead of predicting a word from a context you instead predict the context from a word so you like. The approach can be used to train a virtual back model to predict a word's context. It can also be used as part of a training set of words to predict the meaning of a word in a context. The model can be trained using a series of words that have a similar meaning to each other. kind of like flip the model upside down and there are many other approaches this is just these are just like two really common ones okay um there's also something called wave tubeck which is set up a generalization of virtue back for audio I'm not going to go into. Kind of like Flip the Model upside down. There's alsoSomething called Wave tubeck. Which is set. up aGeneralization of Virtue Back for audio. I'm gonna go into a little bit more detail on that.  SSL can also be applied to audio it's a very broad sort of paradigm and I think the currency of the art and audio classification is Wave 2 Vector Q which I think came out a few years ago okay. The details of this I just wanted to show that SSL can be used to analyse audio. It's very broad and it's not just about music. It can also apply to movies, TV, music, commercials, commercials. It has a very wide range of applications. It could also be used in the future to analyse video. so what if we go back to this idea of where to work we are predicting a single word from some surrounding context right and we're only considering a few words at a time. What if you predict a word from the entire sentence that it is a part of? What if we are only looking at one word in a whole sentence? What do you think? Share your thoughts in the comments below or post a video on our Facebook page, Twitter or Instagram account. We would love to hear from you. and as I think I think you might imagine that this white this might work better because the sentence will give you more context than just like those two surrounding words. and you can take this step even further and instead of predicting a single word from a sentence. instead of predicts a single words from a sentences. and and you could take thisStep even further by predicting a word from the beginning of a sentence instead of the end of the sentence. and. you can even predict a word in the middle of a sentences instead of at the end. you can try to predict multiple words from a sentence and what when what happens is that you call these like multiple words a masked word and your goal is to predict the masked word from the rest of the sentence. So if you have a sentence that says a, you can predict that it will say a. You can also predict that a word will be followed by a word that is followed by another word. This can be done by using a series of words or phrases. For more information on how to use this technique, visit CNN.com/Heroes. quick Dash fox jumps over the dash dog your goal is to predict the words Brown and lazy from this input. This was a really effective approach for learning what embeddings and there's a very famous model in an NLP called bird which takes us to the next level. The next step is to learn how to use this data to predict what the word Brown means and how it relates to the word lazy. This is the next stage of the NLP experiment. The final stage is to find out how to predict a word like lazy from the input. to the next level uh the bird is something called a Transformer model you don't need to know what Transformers are yet we'll have a lecture on that later in the course. Verge Texan these sentences that have like Mass birds and it tries to predict what those Mass birds will look like in the future. VergeTexan.com: What do you think? Share your thoughts in the comments below or post a video of your own on our Facebook page, @CinemaVerse. words are it actually goes a step further it actually takes in two sentences instead of one and it protects the best words for both of them. At the same time it also predicts the order of photosynthesis like you might imagine that if you sample two sentences from.words are. It also takes in the best word for each of the two sentences and protects them from each other. It takes in both words for each sentence and protects both words from one another. It then takes in all of the words from both sentences and then it predicts the Order of Photosynthesis. a paragraph one sentence comes before the other one right so it tries to do both at the same time so it learns a word level and a sentence level embedding now bird was a huge success in MLP and I think bird is sort of what kick-started the interest in the show. I think it's a great way to teach a character to read a paragraph. It's a really good way to get the feel for the different levels of a sentence. I've used it in a lot of my other books. The idea of SSL and CV kind of like died down a bit in 2015 2016 2017 but people started taking more and more interest after people saw that bird worked really well in NLP and like another reason I included self-supervised learning back in CV. In the past I've included SSL.in and CV.in together as part of my CV and NLP training. I've also included SSL in my NLP classes as well as in other areas of my life. I'm also working on a book about how to use bird as a tool for NLP. I hope to write a book on the subject. The current state of the art for CV is actually very similar to Burke so that's a teaser for the lecture that we discuss Advanced Techniques and as software CV. So any questions about any of these approaches because uh that's pretty much pretty much what we're going to talk about in this lecture. We'll be talking about advanced techniques and software CV in the next few weeks. We hope to hear from you in the weeks to come. We're looking forward to hearing from you. Thanks for your support. "We went through a whirlwind tour of representation learning transfer learning self-supervised learning and we discuss like different kinds of we discuss different concepts terminologies methodologies Etc" "I just want to point out that.it for the lecture so just to give a wrap up just toGive a summary of the lecture" "We discuss like. different kinds. of we discussed different concepts Terminologies methodology Etc. Etc." "We discussed different. concepts terminology methodologies" "Etc" this specific lecture doesn't have any homework but there is a homework for this entire cluster which is the high Crush notebook that should be due next Tuesday. I mentioned before that there will be a lecture on on Advanced SSL for CV next week. This specific lecture does not have homework but this lecture does have homework for the high crush notebook that is due next week which is a lecture for Advanced SSL. This lecture doesn’t have homework and there is no homework for that lecture so it doesn't count as homework. and that will have a homework so if you ever need to review uh the topics from this lecture so to work on that homework this slide deck should be up on the website again if you feel free to do that. That is it for today a second pause. Back to the page you came from. The slide deck for this lecture is now available on our website again. Click through the slide deck to see the rest of the lecture. The lecture will be available again on the site later this week.