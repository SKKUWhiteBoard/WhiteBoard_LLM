The dagger algorithm aims to provide a more principled solution to the imitational and distributional Shi problem. The dagger algorithm is actually something that you're going to be implementing in your homework and the dagger algorithm can be used to solve problems in math and science. We'll be back next week to talk about our homework topic of the day, the Dagger Algorithm. Back to the page you came from. Follow us on Twitter @CNNOpinion and @jennifer_cnn. We're on Facebook and Instagram. uh as a reminder the problem with distributional shift intuitively is that your policy makes at least small mistakes even close to the training data. When it makes small mistakes it finds itself in states that are more unfamiliar and there it makes bigger mistakes and the mistakes compound. The problem is that when it makes smaller mistakes it gets into a state that is more unfamiliar, and there the mistakes get bigger and bigger and the problems compound. It's a problem that can't be fixed by changing the way your policy is designed. more precisely the problem can be described as a problem of distributional shift. The distribution of States under which the policy is trained is systematically different from the distribution of states under which it's tested which is p Pi Theta. So far a lot of what so far has happened is that the data has been trained in a different way to the way that it is tested in the States it is trained in. This has led to problems with the way the data is collected and tested. The data is being trained in the same way as the States are tested, but in different ways. we talked about are methods that try to change the policy so that P Pi Theta will stay closer to P data by making fewer mistakes. Can we go the other way around can we instead change p data so that. P data better covers the states that the policy actually visits? We're going to be actually collecting more data than just the initial demonstrations and the question then is which one is better. We're introducing some additional assumptions so we'll be collecting moreData than just just theinitial demonstrations. The idea in dagger is to actually run the policy in the real world. So instead of being clever about Pi Pi Theta or about how we train our policy Let's Be Clever about our data collection strategy.data to collect and that's what uh dagger tries to answer. Instead of being Clever about PiPiTheta, let's be Clever about Our Data Collection Strategy. So the idea in Dagger is to actual run the Policy in the Real World so that we can collect the data we need. see which states it visits and ask humans to label those States so the goal is to collect data in such a way that P Pi Theta is used instead of P data. "We're going to do that by actually running the train and asking humans tolabel those States" says the project's founder. "It's going to be a very interesting project to watch," he says. "I'm excited to see what the future holds for the project" The project is being funded by the National Science Foundation and the University of California at Berkeley. our policy so here's the algorithm now we're going to need labels for all those States. We'll train our policy first on our training dat just on our demonstrations to get it started. Then we'll run our policy and we'll record the observations that the policy seats and then we'll ask a person to go through all of those observations and label them with the action that they would have taken okay and now we have a labeled version of the policy data set. Then, we'll aggregate the data set and then go back to Step One retrain the policy and repeat. those observations and then we Aggregate and it can actually be shown that eventually this algorithm will converge such that. eventually the distribution of observations in this data set will approach the distribution. of observations that the policy actually sees when it runs the intuition for why that's true. The algorithm is based on the idea that if you collect enough data, it can be used to predict the future direction of the direction of a country's policy. It can also be applied to other data sets, such as the world's population. of course is that uh eventually is that each time the policy runs you collect its observations but then you might label them with actions that are different from the actions it took. But that distribution is closer than the initial one so as long as you get closer each. each time you get to the end of the policy you'll get closer to the goal of what you're trying to achieve. The goal is to get to a point where you're getting closer and closer to what you want to achieve, and that's the goal. step eventually you'll get to a distribution where that the policy can actually learn and then you'll stay there forever so then as you collect from it more and more eventually your data set becomes dominated by samples from the correct P Pi Theta distribution. So that's the algorithm it's using. It's a very complex algorithm. It takes a long time to get to the correct distribution. It can take years for the algorithm to learn a new distribution. There's a lot of work that goes into the algorithm. a very simple algorithm to implement if you can get those labels here's a a video of this algorithm in action this is in the original diag paper this was a about 12 years ago where they actually used it to fly a drone through a forest and a dagger and it's very simple to implement. If you want to learn more about the algorithm, visit: http://www.cnn.com/2013/01/23/science/dagger-algorithm-dagger.html. was used to where they actually flew the Drone collected the data and then ask humans to label it offline by actually looking at the images and using a a a little mouse interface to specify what the action should have been. With a few iterations of dagger that can be used to fly the Drone, the Drone can now be controlled by a person. The Drone can also be controlled via a mobile phone app or a tablet. The drone can be controlled using a phone or tablet and can be programmed to fly in different directions. actually get up to fly pretty reliably through a forest dodging trees now there is of course a problem with this method and that has to do with step three uh it's sometimes not very natural to ask a human to examine images after the fact and output the correct image. It's also not very easy to get a person to look at an image after it's been taken and produce the correct result. It can be very difficult to get people to examine an image that has been taken after it has already been taken. action when you're driving a car you're not just instantaneously making a decision every time step about which action to choose. You are situated on temporal process you have reaction times all that stuff so sometimes the human labels that you can get offline in this sort of a counterfactual can be very useful in the car world. "When you're in a car, you're sitting in a seat and you're looking at the road in front of you. It's a very different experience than being in the passenger seat," he says. way can be not as natural as what a human might do when they were actually operating the system so step three can be a bit of a problem for dagger. Many improvements on dagger seek to alleviate that challenge but the basic version of dagger works like this. The most basic version is called the "Dagger" and is available for pre-order on the company's website. For more information on dagger, visit dagger.co.uk or go to the official website of dagger.com. There's really not much more to say about dagger. It alleviates the distributional shift problem. It actually provably address it so you can derive a Bound for dagger and that bound is linear in t rather than t. That's the version that you will all be implementing in your homework. And that's the versions that we'll all be using in the rest of our work. We hope that this has helped you all out a bit. We'll see you in the next week or so. than quadratic but of course that comes at the cost of introducing this much stronger assumption that you can collect the additional data okay so that's basically the list of methods I wanted to cover for how to address the challenges of behavior cloning. We can be smart about how we use behavior cloning, and we can do it in a way that is safe and efficient. We're going to be using behavior cloning for a long time to come, and it's going to require a lot of work. we collect an augment our data we can use powerful models that make very few mistakes. We can use multitask learning or we can change the data collection procedure and use dagram. The last thing I want to mention which is a little bit of a preview of what's going on is that we can also change the way we collect our data to make it more efficient. We also have the option of changing the way the data is collected to make the process more efficient as well as making the data more accurate. Deep learning works best when the data is very plentiful. Humans need to provide data for imitation learning which is sometimes fine. But deep learning works better when there is a lot of data to work with. So deep learning is not enough by itself. We need more data to make deep learning work properly. To learn more about deep learning, visit: www.deeplearning.org.uk/deep-learning-how-to-train-a-deep-learner-how to train a deep learner. provide huge amounts of data can be huge limitation if the if the algorithm can collect data autonomously then we can be in that regime where deep Nets really Thrive and data is very plentiful without exorbitant amounts of human effort. The other thing is that humans are not good at collecting data. If we can get to a point where the algorithm collects data without the need for human effort then we will be in a regime where we can have deep Nets that Thrive. We will be able to get to that regime.  humans might be pretty good at specifying whether you should go left or right on a hiking trail uh or controlling a quadcopter through a remote control. But they might not be so good at for example controlling the low-level commands to qual.at providing some kinds of actions so humans might not know how to do it. For more information on the project, visit the project's website or go to: http://www.cnn.com/2013/07/17/cnnhuman-human-behaviour/story.html. copter rotors to make it do some really complex aerobatic trick if you want humans to control all the joints in a complex humanoid robot that might be even harder. maybe you need to rig up some really complicated harness for them to wear uh if they want to control the rotors of the robot. If you want the robot to be able to control its rotors, you need a really complex harness for it to wear. You can buy the robot in the U.S. for about $100,000. a giant robotic spider well good luck finding a human who can operate that um and humans can learn things autonomously and just intellectually it seems very appealing uh to try to develop methods that can allow our machines to do the same as I mentioned in lecture one one one. "I think that we should be able to develop ways to make machines that can learn autonomously," he says. "It's very appealing," he adds, "and I think we should try to do that." "I'm very excited about the prospect of making a robot that can do all of these things for us," he concludes.  machines can get unlimited data from their own systems. Machines can learn behaviors that are better than what humans would have done. It's very desirable to learn autonomously when learning autonomously in principle, says Google's Mark Zuckerberg. "When learning autonomous in principle machines canget unlimited data," he says. "In that case it's very undesirable to learn autonomousously uh when learning automatically in principle machines can get unlimited data from their own systems" experience and they can continuously self-improve and get better and better in principle exceeding the performance of humans now in order to start thinking about that we have to introduce some terminology and notation. Define what it is that we want if our goal is no.experience. We have to actually DefineWhat it is we want. If we want to achieve our goal, we must first define what we want and then work towards it. We need to start by defining what we are trying to achieve. longer just to imitate but we want to do something else well what is it that we want and maybe instead of matching the actions in the expert data set we want want to bring about some desired outcome. Maybe in the tiger example we wants to minimize the probability maybe in the Tiger example we wantto minimize the probabilities of a tiger attack on humans. We want to make sure that we get the desired outcome from the tiger attack. We don't want to get the tiger to attack us, we want the desired result. of being eaten by the tiger so we want to minimize the probability that we will land in a state S Prime which is an eaten by Tiger State. We can write that down mathematically and in general we can write it as the expected value of some cost. We want to minimise the probability we will be eaten by a tiger and we can do that mathematically by writing it down as an expected value. We also want to make sure that we don't end up in the state of S Prime, which is a state that is eaten by tiger. in this case the cost is being eaten by a tiger uh now we already saw costs before when we talked about counting the number of mistakes but in general we can have arbitrary cost on States and actions and those can Define arbitrary control tasks like not being eaten. In this case it is the cost of not eating a tiger, but it can be arbitrary in other cases too. In general we have arbitrary costs on States, actions, and actions. Those can be defined as not being eat by a tigers. by tigers or reaching a desired destination so the new thing that we're going to introduce and that we'll use in lectures next week is the cost function or sometimes the reward function. The cost function and the rewardfunction are really the same thing they're just the same. They're just a different way of looking at things. We'll be using that in our lecture next week. And we'll also be using it in the rest of the year. We're going into a new phase of the program. negatives of one another and the reason that we see both sometimes is the same kind of a cultural distinction that I alluded to before remember I mentioned that we have S a uh which comes from the study of dynamic programming that's where the reward comes from in optimal programming. "I think it's a very interesting time for us to be in the U.S. right now. It's a really exciting time for the country. I'm looking forward to it," he says. "It's going to be great" control it's it's a bit more common to deal with costs I don't know if there's a cultural commentary here well you know optimal control originated in Russia maybe it's just more common in America. We are all very optimistic and we think about life, he says. "I don't think there's any cultural commentary," he adds. "It's just a little more common" to think about costs in the U.S. than in other countries, he adds, "I think that's just how it is" as bringing rewards maybe there's something to that but for the purpose of this class don't worry about it C is just the negative R and to bring this all the way back around to imitation well the cost function that we saw before for imitation is just can be just can't be used for imitation. The cost function is the same as the one we saw for imitation before, it's just that it's not used as much as it used to be. It can be used as a cost function for imitation, but it's really just a function of the cost of imitation. framed in exactly the same framework we have rewards which are log probabilities we have costs uh and those are interchangeable. You can have the cost be the negative of the reward and you can define a cost for imitation but you can defined a more expressive cost for the reward. It's the same thing. We have costs and rewards and we can define them in the same way. We can have a negative cost and a positive cost. It’s the samething. We’ve got the same kind of framework. thing you actually want like reaching your destination or avoiding a car accident and then use those with more the more powerful reinforcement learning algorithms. In future weeks, we'll cover more of the powerful algorithms that we'll be covering in the next few weeks. We'll also cover how to use these algorithms to help you get what you want in the real world. Back to the page you came from. Follow us on Twitter @CNNOpinion and @cnnOpinION. We'd like to hear from you.