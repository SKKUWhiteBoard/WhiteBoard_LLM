So I think I just spend like five minutes, just briefly review on the backpropagation last time. So I didn't have time to explain this figure, which I think probably would be useful as a high level summary of what's happening. So you start with some example x, and then you have some-- I guess, this is a matrix vector multiplication module, and you take x inner product multiplied with w and b. And then get some activation, the pre-activation. And you get some post activation. New example, x comma y, from some distribution D. And often, this is called test distribution. And then you evaluate what's the expected loss on this new test example. So what's important is that this x and y is not seen in a training. It's a new fresh example. And just to be clear, these test examples, you haven't seen them in aTraining set. They are something you draw. You can draw them in advance, but you cannot let them to be seen in the training process. or is much bigger than 0. So you want this gap to be as small as possible. So this one is something you can control in some sense. This is what you try to optimize for. But the generalization gap is something that is very hard to control. At least, you cannot directly control it. And the point of this lecture is to discuss in what cases you can somewhat know this is not too big? OK. And then before going to do more details, let me also define two notation-- two kind of like commonly used terminology. overfitting is that the training loss, j, is small, but the test loss is big. So you have this big generalization gap. So much worse-- becomes much worse. So this is a typical situation of overfitting. In some sense, you are saying that you fit the data very well, but you are overfitting in the sense that you kind of like forgot about the test performance. I will discuss why this will happen. I guess, you can probably guess, but this is so far, I'm just defining roughly what overfitting means. The bias is a decreasing function as the model complexity. So we say that the bias is large, it's because the model is not expressive enough. So that means that if your model is more expressive, then your bias should decrease. So this is the bias. And now let's think about how do you draw the in some sense. Because for example, suppose you believe that this is a U curve. Then should you try even bigger models, bigger family of models? Because you kind of believe that it will be even worse. So you should just try even more in the middle. bias will be smaller. And your sum of these two functions, which is a test error, will be. something like this. And then the best one will be something in the middle. So this is kind of the quick overview of what we're going to discuss. OK. So now, I'm going to define bias and variance in a little bit more formal ways. And I'll show some examples. So any questions so far? Why is the bias [INAUDIBLE]. Why is bias this crazy? Oh, squared, I mean. fifth-degree polynomial, and then I'm going to try quadratic. So linear model. I guess, you can probably see, guys, what will happen. So you can see that there's a large training error, training loss or training-- let's call it loss just for consistency. So what you really will fit, like if you minimize the error on the training data with this so many training examples, then what you will get is probably something like this. It's not like necessarily matching exactly the ground truth, but you have a small fluctuation. Mathematically, one way to define a bias is that you can say this is the-- So bias is-- I guess, actually, there's some approximation here, depending on what exactly your model is. But roughly speaking, it's the best error or loss, you can get with even infinite data. And you can kind of see that it's probably important for bias to be small because if bias is large, even with infinite data, you cannot do anything. And that's the problem with linear models. what I'm going to talk about. And this is an area of research productive in the last, probably, three or four years. So let me try to find out where should I erase. So this phenomenon that people observe, empirically, and then analyzed theoretically, this phenomenon is called double descent. If you are a historian, then I think actually this phenomenon actually dates back to something like 1990. But I think it just becomes popularized and more relevant these days. And what does this mean is that, so basically, I've told you that this is test error. This is model complexity. stochastic gradient descent for linear models. So the existing algorithms underperform dramatically when it's close to d. So both these two peaks are basically like this. So here, you are changing n, the number of data points. And you found that when n is close to. d, you had to pick. And here,you are changing the. number of parameters. And we realized when d is kind of above n, above the number. ofData points, you have a peak. going to discuss this more next time. So the high level thing is just that something else is driving the norm to be small. Thanks. Going to talk more about this in the next few days. Back to Mail Online home. back to the page you came from. Back To the pageYou came from: Back to thepage you camefrom. Back into the page You came from was from: The Daily Mail. Back onto the pageyou came from, the DailyMail.com page you were from. loss with respect to the output first. And this is often very easy. This is kind of like you are accessing this network in a backward fashion in some sense. But how do you do this, each of this arrow? So this is by the lemma that we discussed. So now in this lecture, and the lecture afterwards, we are talking about, I guess, a few concepts. One concept is called generalization, which is the main point of this lecture. And also, next lecture, we're going to talk about the concept of regularization.