So I think I just spend like five minutes, just briefly review on the backpropagation last time. So I didn't have time to explain this figure, which I think probably would be useful as a high level summary of what's happening. Like this is the forward path. This is how you define network and the loss function. So you start with some example x, and then you have some-- I guess, this is a matrix vector multiplication module, and you take x inner product multiplied with w and b. And then get some activation, the pre-activation. And you get some post activation. Just draw this from scratch. So this side is the model complexity. So we say that the bias is large, it's because the model is not expressive enough. So that means that if your model is more expressive, then your bias should decrease. And now let's think about how do you draw the variance on this thing. So the question you want to answer is that if you change the model. complexity, what is the best test error, right? So it means that it's somewhere in the middle.