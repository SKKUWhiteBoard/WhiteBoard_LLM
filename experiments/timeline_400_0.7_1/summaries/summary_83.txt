Coded imaging is a co-design between how you capture the image and how you process the image. The concept of a position or superposition applies to all three types, shadows- or refraction- or reflection-based techniques. We'll see how-- we already have some projects that are inspired by biological vision. And I believe Santiago-- where's Santiago? Oh, yeah, his triangle-- the piston, kind of-- so some really great ideas. It is going to be very popular. When you take a blurry photo, a lot of the high-frequency details are lost. If you try to apply some deblurring, you'll get a result that looks like this. The culprit here is really this box function, which is equivalent to opening the shutter, opening the-- release your shutter button-- opening it open for exposure duration and closing it. So what if you change that? What if you changed that? And instead of keeping the shutter open for the entire duration, you open and close it in a carefully chosen binary sequence. It's actually preserving all frequencies in the image. frequencies-- they're all preserved. Of course, they're attenuated. It's not as high as-- it's not 1.0.0, it's reduced. Maybe it's 0.1 or so. So there is still some hope to recover this photo back from this because, in the denominator, we will not have seen. This is a very simple idea. Instead of keeping the shutter open for the entire duration and getting a well-exposed photo, the shutter is open for only half of the time. And that's your 1010 inquiry. CNN's Ravi Agrawal is a senior producer at CNN.com. He is the founder and CEO of a company that uses artificial intelligence to improve camera technology. He says the technology can be used to blur images based on distance and speed of objects. He shows how the technology could be used in a mobile phone app to help people figure out license plate numbers of cars and cars on the street. The company is based in New York and has been around since the 1970s. of that object moves in a straight line, OK. It doesn't matter which direction and what speed. So the problem here really is the point spread function or the blurred function is very critical. And this is what we want to study about half of the class. And the concept is very interesting because light is linear. So eventually, it's very linear. What happens to a point happens to the rest of the object. So that's the same concept here. You just want to call leading the world, take a picture, and see how it works. to engineer activity of the camera. So in this particular case, a point that was moving created a blur like this. And by engineering the time point spread function, it stops looking a bit like that. And then it just turns out that this one is easier to deal with than this one. So this is very counterintuitive because you would say, let me just build the best lens and the best exposure time. And so that kind of mimics the human eye. But when it comes to actually extracting information from that scene, it turns out you need to strategically modify how the camera works. Coded imaging is to come up with clever mechanisms so that we can capture light. The circuit is very, very simple. You just take the hot shoe of the flash, and it triggers. When you lose the shutter, it triggers the circuit. And then you just cycle through the code that you care about. What can we do for defocus blur that is for motion blur? What can you do fordefocus blur? We, again, want to engineer the point spread function. system. It took almost two years to realize that to put this coded aperture in a camera, there are only a few places where you can put it to get good results. So out of that came this particular experiment. When you focus with this, it works in very interesting ways. It has to deal with chromatic aberration, geometric aberrations, such as radial distortion, and so on. So the multiple lenses are moving every time I move this. And they're moving because they're guided through these groups. But there's one particular location that does not change in this. When you think about a visual camera, you make this very simplistic assumption. That is a pinhole, and there's a sensor. When you put a lens, we assume that the center of the lens is the central projection. The center of production of this lens is very carefully designed by camera makers to be the same plane where you put your aperture. When it's in sharp focus, you just see the point. But let's say that your autofocus here. What will you see? You will see the same exact [INAUDIBLE].. In 1D, the Fourier transform of a 52-length vector is broadband. It has energy at all the frequency. In 2D, it's more distributed instead of just all being near the center. If I just take a square aperture, a traditional one, and a square transform, it will look something like this. So a Fouriertransform of 7 by 7 will have a peak in the middle. But the truly Fourier transforms will have the rest of the peak. the values will be constant. So if we're placing a broadband code, certainly we have an opportunity to recover all the information. It's much easier to think about convolution and deconvolution in frequency domain than in primal domain. The bokehs are-- it depends on your-- I mean, for your average consumer, I don't know whether this matters. But you're right. If you're looking at something that's-- we have bright lights in the scene, take our false photo. They will all look like this. motion case, we had to know how much the motion is. The size of the blur is dependent on what? AUDIENCE: Belt. RAMESH RASKAR: The belt. But not just depth-- depth from the plane of focus, right? So that's an extra parameter you would estimate somehow. Maybe you can use a rangefinder or something like that, or just a software. There are methods you can employ. We said, OK, let me try to refocus. When it comes into sharp focus, my edges, that must be the right depth. I said, by the time I come tomorrow morning, I'll find a really good code. And I came back next morning. Nothing had happened. I waited all day. It was still running. And it never came out of that. So 2 the 52 is pretty challenging. But even if you use a cluster, it's still a pretty big number. So you can do some approximation. So [? harder ?] [? mark ?] code, which we learned about a few weeks ago or so-called broadband codes, they all have polynomial solutions. filter to the beginning of the signal. This particular filter is actually not circular, but it's linear. So when you apply the filter here, when you start applying the filter at the end of the image, you don't go back to the front. It turns out, for circular convolution, the match is very clean and beautiful and smoother course work. Or for linear convolution,. there is no good mechanism. So we came up with our own code called RAT code, R-A-T, which is after three quarters. In astronomy, you have circular convolution because they use either two mirror tiles and one sensor or one mirror tile and two sensors. If you tile aperture, you'll get really horrible frequency response, unfortunately, because if you put two tiles, that means certain frequencies are lost. But that's because our eyes are not very good at thinking about what the original image could be, given either this one or the previous one. So given this, I can challenge you that you're not able to predict that it has all this structure, right? Ramesh Raskar: Coded imaging is elegant and beautiful and sometimes complicated. He says there are many ways of engineering the point spread function. RaskAR: For any continuous code, there is a corresponding binary code that will do an equally good job. "It's just one of those things. It's like we are sick of it, so we don't want to do it, but I think it's worth trying," he says of orthogonal motion blur. smart people who invented this. It's very sad Because that part is done. So they just wanted the technology. And it's in a lot of cameras. There's another company called Tessera, which has a very similar solution. But within this region, the thickness will be a bonus. So you can either think of it as adding small matchsticks on top of the main lens-- or the way they do it is they actually put one single sheet that looks like that, an additional layer of support, a face mask. the name. The solution is very similar. I'm sure they're fighting out in court right now. Same solution. Instead of putting this particular guy, that's just going to add some extra glass, but mostly in a minor form. It's just [INAUDIBLE] on that one. So basically the same solution but creating different focal length for different [? partners. ?] AUDIENCE: Yeah. Although you said, I mean, there's this portion there, where if you have another blur [INAudIBLE],, right? RAMESH RASKAR: Right. blur is only about 10 pixels, no matter where you [INAUDIBLE]. So maybe that was the matter. If you have a point of access, it's still going to create an image that's blurred 10 pixels. This is, again, very counterintuitive, where you go to make the image intentionally blurred. It's just that it's blurred everywhere. And then we also saw this one very early on, where the point spread function-- typically when something goes in and out of focus, it looks like a point. Some scientists have been able to reconstruct three-dimensional neuronal structures. The z-dimension of a microscope is now down to about 10 nanometers. A single-pixel camera was listed as one of the big things in 2005 by Technology Review. The idea is, instead of taking one single photo, what you're going to do is to take a single photodetector and aim it at a set of micrometers. It's a very cool idea, by the way, as a scientist, I really like it. Ramesh Raskar: Compressive sensing or compressed imaging comes up. He says it's taking the picture with a hardware and compressing the software. RaskAR: The theory of compressive sensing is that, using some basis, I can transform the image and measure in [? your ?] measurements. The claim is that by using this understanding that my image can be represented in some transform basis-- in this case, Fourier basis-- using very few coefficients, can be exploited while I'm sensing. a very, very active field. The secret of success for film, of film photography, is that if somebody had given you this problem before the invention of film, that there is a scene-- and I want to give you a sensation of the same scene-- time shifted or space shifted. If I can see the scene and understand it, I can just present that as is and let it go through. And this is how we have been treating photography all this time. It's a record of visual experience, which is great for humans, but not so great for computers. The debate about whether it's really better or not is photography? RAMESH RASKAR: Tomography, yeah. When you have to recover the sky, you want to take as few measurements as possible. So compressive sensing allows you to take less measurements. But the problem is you need to actually have more information about the scene before you take the measurement, which is another measurement. That's a different problem for the dual photography code for the camera. It's a very different thing. which is how to write a paper and wishlist for photography. Which isHow to Write a Paper and Wishlist for Photography: How to Write A Paper and Write A Wishlist For The Camera. For more information on writing a paper or wishlist, go to: http://www.cnn.com/2013/01/30/photography/how-to-write-a-paper-and-wishlist-for-photography-how- to- Write-A-Paper-And-Wishlist.html.