Transformer part is extremely extremely important so there's a debate a little bit what was the most influential part of making uh CHP and large language model possible uh Transformer is definitely a significant part of it and and I'll let you judge for yourself but uh I think it's less important than the actual modeling perspective that we've we've come up with. In order to understand the Transformer we're going to start to thinking about how we can process sequences so text is just sequence of words. possible to be as useful as possible okay so uh these are the the the three different things we want to address right what's good and bad dialogue. We want to be more robust and learn to solve correct and this is where we're going to do reinforcement learning from Human feedback. Open AI does on chtp and this was very very hyped for a long time but now people talk less about it uh okay so what do we do well we have a great model that's been fine tune on dialogue and it's able to generate really good answers. the World by looking at videos right you can even sort to understand how human beings work even better because you can see people being upset or sad or happy whatever right in in a video and start picking these cues up. You can connect the vision part to the text part and get a multimodality model that's able to do both in a really really sophisticated way uh also something that I think these these people are working on all right thank you thank you for your time and good luck with your book. The third lecture on Foundation mulative AI will cover chat GPT. We'll also talk about emerging Foundation models generative AI in the commercial space. And then we'll end with the lecture on AI ethics and regulation as well as a panel. The final lecture on January 16 will be on stable diffusion image generation. The course is being taught at the London School of Economics and Imperial College, London. For more information on the course visit www.lse.ac.uk or go to the course website.