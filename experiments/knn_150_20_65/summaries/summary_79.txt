In week four of CS224N, we take a break from learning more and more on neural network topics, and talk about final projects, but also some practical tips for building neural network systems. Today's lecture is the primary content for what you'll be using for building your assignment 4 systems. For assignment 4, we give you a mighty two extra days for it. And it's due on Thursday. So please be aware that assignment 4 is bigger and harder than the previous assignments. And then as I mentioned Thursday I'll turn to final projects. Machine translation was hyped as the solution to the Cold War obsession of keeping tabs on what the Russians were doing. Claims were made that the computer would replace most human translators. But despite the hype it ran into deep trouble. The idea was largely canned. Work then did revive in AI at doing rule based methods of machine translation in the 90s. But when things really became alive was once you got into the mid 90s, and when they were in the period of statistical NLP. We start with a source sentence. So this is a German sentence. And as is standard in German. You're getting this second position verb. So that's probably not in the right position for where the English translation is going to be. So we might need to rearrange the words. And in the process, I'll go through in more detail later when we do the neural equivalent. We sort of do this search where we explore likely translations and prune. And eventually, we've translated the whole of the input sentence and I've worked out a fairly likely translation. He does not go home. One French word gets translated as several English words. You can get the reverse, where you can have several French words that get translated as one English word. So here we sort of have four English words being translated as two French words. But they don't really break down and translate each other well. These things don't only happen across languages. They also happen within the language when you have different ways of saying the same thing. So another way you might have expressed the poor don't have any money is to say the poor are moneyless. That's much more similar to how the French is being rendered here. we have is based on the translation model. We have words or phrases that are reasonably likely translations of each German word, or sometimes a German phrase. And so then inside that, making use of this data, we're going to generate the translation piece by piece kind of like we did with our neural language models. And if we're guided by our fairly small, something like 5 to 10, we are going to keep track of the k most probable partial translation. So this is a heuristic method. It's not guaranteed to find the highest probability decoding. But at least, it gives you more of a shot than simply doing greedy decoding. The model is based on the idea that the final hidden state of the encode RNN is going to for instance, represent the source sentence. And we're going to feed it in directly as the initial hidden state for the decoder, or RNN. These models have also been applied not just to natural languages, but to other kinds of languages, including music, and also programming language code. So this is what the picture of a LSTM encoder-decoder neural machine translation system really looks like. Everywhere else as well. So you can do summarization. You can think of text summarization as translating a long text into a short text. But you can use them for other things that are in no way a translation whatsoever. So they're commonly used for neural dialogue systems. So the encoder will encode the previous two utterances, say. And then you will use the decoder to generate a next utterance. Some other uses are even freakier but have proven to be quite successful. So if you have any way of representing the parse of a sentence as a string. Stacked RNNs are more powerful. They require much less human effort to build. There's no feature engineering. You're using the same method for all language pairs. Next week, John is going to talk about transformer based networks. They're typically much deeper. But we'll leave discussing them until we get on further. So that was how we train the model. So let's just go a all parameters of the model end to end in a single large neural network has just proved to be a really powerful idea. stuck with it. And you have no way to undo decisions. So if these examples have been using this sentence about, he hit me with a pie going from translating from French to English. So we generate along and generate our whole sentence in this manner. And that's proven to be a very effective way of getting more information from the source sentence more flexibly to allow us to generate a good translation. I'll stop here for now and at the start of next time I'll finish this off by going through the actual equations for how attention works. In greedy decoding, we usually decode until the model produces an end token. In beam search decoding, different hypotheses may produce end tokens on different time steps. And so we don't want to stop as soon as one path through the search tree has generated end. So what we do is sort of put it aside as a complete hypothesis and continue exploring other hypotheses via our beam search. And then we'll look through the hypotheses that we've completed and say which is the best one of those. And that's the one we'll use. you have many languages don't distinguish gender. So the sentences are neutral between things masculine or feminine. But what happens when that gets translated into English by Google Translate is that the English language model just kicks in and applies stereotypical biases. So these gender neutral sentences get translated into, she works as a nurse. He works as an programmer. So if you want to help solve this problem, all of you can help by using singular they in all contexts when you're putting material online. And that could then change the distribution of what's generated. For languages for which there isn't much parallel data available, commonly the biggest place where you can get parallel data is from Bible translations. So this is a piece of parallel data that we can learn from. Cherokee is not a language that Google offers on Google Translate. So we can see how far we can get. But we have to be modest in our expectations because it's hard to build a very good MT system with only a fairly limited amount of data. There is a flipside, which is for you students doing the assignment. The advantage of not too much data is that your models will train relatively quickly.