So, last time we were starting to talk about the sort of the general overview of what reinforcement learning involves. We introduced the notion [NOISE] of a model, a value, and a policy. Can anybody remember off the top of their head what a model was in the context of reinforcement learning? So what we're gonna do today is, sort of, um, build up for Markov Processes, up to Markov Decision Processes. And this build, I think, is sort of a nice one because it allows one to think about what happens in the cases where you might not have control over the world. on sort of the contraction operator. So, if an operator is a contraction it means that if you apply it to two different things, you can think of these as value functions. The distance between them shrinks after, um, or at least is no bigger after you apply the operator compared to their distance before. So how do we prove this? Um, we prove it- for interest of time I'll show you the proof. Again, I'm happy to go through it,. um, I- or we can go throughIt in office hours et cetera.