Robotics is a really cool and important direction for the future. We are moving towards a world where so many routine tasks are taken off your plate. I really believe that in the future we will have AI assistance whether they are embodied or not to act as our guardian angels to ensure that we maximize and optimize our lives to live well and work effectively. And so today we can say that with AI we we will see such a wide breadth of applications for instance these technologies have the potential to reduce and eliminate car accidents. The future is really enabled by three interconnected fields and on one hand we have robots now robots I like to think of robots as as the machines that put computing in motion. We have artificial intelligence which enables machines to see to hear and to communicate and to make decisions like humans and then we have machine learning. In the context of robots we have three types of learning and you have seen different aspects of these methodologies throughout the course we have supervised learning, unsupervised learning and reinforcement learning. all objects on roads including Ducks cars people is really critical uh for autonomous driving and um so how does this work well let me let me give you a high level view of how a robot car can actually recognize the scene. In order to in order to use deep learning for the perception task of robots we use data this is manually labeled data that gets fed into a convolutional neural network and then the labels are used to classify what the data is so for instance for this image we may have classifications like car duck Road and we do this so that when the system when the car sees a new image the car could say oh this is a this is ducks on road now. kinds of extreme errors um or um the way um we we denote them Corner cases that we need to pay attention to when we train machine learning for safety critical applications like driving. MIT researchers led by George Tannenbaum and Boris Katz did an experiment a few years ago where they took regular objects and they put them in a different context. With this significant change in context the performance of the top performing imagenet algorithms dropped by as much as 40 to 50 percent. So keep this in mind as you think about deploying or building and deploying deep neural network Solutions. The images that get fed from the camera streams of cars are fed to the decision-making engine of the car. With all small perturbations you can turn the stop sign into a yield sign and you can imagine what kind of chaos this would create on a on a physical Road. Machine learning is very powerful for building perception systems for robots but as we employ machine learning in the context of robots it's important to keep in mind the scope when they work and when they don't work. agents ought to take action in an environment in order to maximize the notion of a cumulative reward and so um reinforcement learning is really about learning to act. This differs from supervised learning and not needing a labeled input or in not needing labeled input output pairs so in this example the agent has to learn to avoid fire and it's very simple it gets a negative reward if it goes to Fire and a positive reward when it gets to water and that's essentially what what what this approach is like you do trial and error and and eventually the positive rewards dominate the negative rewards. years ago and so these techniques that did not do so well back then all of a sudden are creating extraordinary possibilities and capabilities in our agents now this is a simple simulation in order to get the simulation to drive a real robot we actually need to think about the Dynamics of the robot and so in other words we have to take into account what the vehicle looks like what are its kinematics what are Its Dynamics and so here is a vehicle that is running the policy learned in simulation so it's really cool because really we are able to train in simulation. In 1995 a Carnegie Mellon project called nav lab built a car that was driven by a machine learning engine called Alvin and Alvin drove this car all the way from Washington DC to Los Angeles. The car was in autonomous mode for a large part of the highway driving but there was always a student there right ready to um to take control and the car did not did not drive inonomous mode when there were when it was raining or when there was a lot of congestion or when the car had to had to take exits. In 1986 German engineer Ernst Dickman started thinking about how he could turn his van into an autonomous vehicle. Dickman put computers and cameras on the van and began running tests on an empty section of the German Autobahn. computers needed about 10 minutes to analyze an image and they assumed that there were no obstacles in the world which made the problem much easier because all the car had to do was to stay on on the road so they developed some very fast solutions for paring down the image to only the the aspects that they needed to look at. It's really super interesting to think about how visual processing improved from one frame per 10 minutes to 100 frames per second and this has been a game changer for autonomous cars. The other thing that happened in autonomous driving was that the lidar sensors decreased the uncertainty and increased safety and today we have many companies and groups that are deploying self-driving cars. It's really exciting um okay now when we think about autonomous driving there are several key parameters that emerge as we Think about what the capabilities are. of these systems are one one question how complex is the environment where the car t Road like in the German case then the problem is much easier then we have to ask ourselves how how complex are the interactions between the car and the environment. We have very effective and Deployable solutions for robot cars that move safely in Easy environments where there aren't many static nor moving obstacles and you can you can see from this example this is this is an example of the MIT car and it'sYou can see this this car operating autonomously without any issues at Fort Devens. Many companies and research teams are deploying and developing self-driving cars and many of them follow a very simple solution which you can adopt and turn your car into a self- driving car. Here's what you have to do you take your car you extend it to drive by wires so that your computer can talk to um to the steering and the acceleration the throttle controls. Then you'll further extend this car with sensors and most of the sensors we use are cameras and lidars and then there are Suite of software models modules. computational units that you have to make you. There are so many works that address each of these subtasks that are involved in autonomous navigation. Alexander's PhD thesis his idea was to utilize a large data set to learn a representation of what humans did in similar situations and develop autonomous driving solutions that drive more like humans than than the traditional pipeline which is much more roboticy if you if so um so then the question is. If you if you don't if you do then the car is not going to work. how can we use machine learning to go directly from sensors to actuation in other words can we compress all the stuff in the middle and use learning to connect directly perception and action so the solutions that we employed build on things we have already talked about we can use deep learning and reinforcement learning to take us from from images of ofroads onto steering and and throttle onto what to do so this is really great because you can train on certain kinds of Roads and and you can then take your vehicle and put the vehicle in completely different driving environments and driving situations and you don't need new parameters. Alexander developed the Vista simulator. The Vista simulator can model multiple agents. It can simulate different physical sensing modalities that means including 2D cameras 3D lidar event cameras and and so forth. It's from this data we can learn to maximize the likelihood of particular control signals for particular situations. The solution also allows us to to localize the the vehicle so it's really super exciting okay so we can we can get this human-like control but assuming light control requires a lot of data. happens in the existing simulators in the state of the art so the Top Line shows crash locations in red and the bottom line shows mean trajectory variation in color. Here's the vehicle that is executing the learning based control and here's Alexander with his vehicle that was trained using data from Urban driving and now he's driving to the soccer field and you can see that he he's able to drive to to get this car to drive him without doing any training without ever having seen this road and explicitly providing data about this road. Liquid networks are Dynamic causal models and I want to show you some examples of how these models explain task of inside a wooded environment. Ramin introduced liquid networks and introduced neural circuit policies and so I just want to drill down a little bit more into this area because you can now compare you cannot understand how the the original engine worked and you can compare that against what we get from liquid Networks. The attention map is so much cleaner right the vehicle is looking at the road Horizon and at the sides of the road which is what we all do when we drive a vehicle. standard deep neural network and we have asked this network to solve this problem and the attention map of the network is really all over the place you can see that the network the the Deep neural network solution is very confused but check out something else the data that we collected was summertime data and now it's fall so the background is no longer green we have we don't have as many leaves on trees and so the context for this task has completely changed by comparison the the liquid network is able to focus on the task and is not confused. This kind of this kind of ability to transfer from one set of training data to completely different environments is truly transformational for the capabilities of machine learning. that yields models that generalize to unseen scenarios essentially addressing a challenge with today's neural networks that do not generalize well to unseen test scenarios because the models are so fast and and compact you can train them online. We in in our lab here at C cell we have one project that is looking at whether we can understand the lives of whales and so what do I mean by this so here is an example where we have used a robotic drone to find whales and look at what they do and track them. and mysterious creatures and so if we can use our Technologies to get better insights into their lives we will be able to to understand more about about other animals and other other creatures we share this beautiful planet with so we can study these whales from above from the air we can also study the whales from from within from from inside the uh the Ocean and here's a here's Sophie our soft robotic fish um which Joseph who is with us today has participated in in building and here is this beautiful beautiful very natural moving robot that can get close to aquatic creatures. friends and we would like to know what it's saying we have no idea but we can use machine learning to make progress. We are trying to understand the phonetics the semantics and the syntax and the discourse for whales. We have a big data set consisting of about 22 000 clicks uh the clicks get grouped into codas the codas are like the phonemes and using machine learning we can identify coded types. We can identify patterns for Coda exchanges and we can we can begin to really ask ourselves how is it that that that Wales Wales. There is so much opportunity for developing improved machine learning using existing models and inventing new models. If we can do this we can create an exciting work world where machines will really Empower us will really augment us and and enhance us in our cognitive abilities and in our physical abilities. If you're interested in this problem please come see us because we have a lot of projects that are very very exciting and important towards reverse engineering what this really extraordinary and majestic animal is capable of doing so let me close by saying that in this class you have looked at a number of really exciting machine learning algorithms. your dimensions and can create a bespoke shoe just for you and then all the all the clothing all the items in our environment can kind of awaken our clothing could become robots. Just-in-time Holograms could be used to make the virtual world much more much more realistic much more connected and so here they're discussing the the design of a new flying car and let's say we have these flying cars and then we can integrate these cars with the it infrastructure and the cars will know your needs so that they can tell you. the kind of future that machine learning artificial intelligence and robots are enabling. I'm personally very excited about this future with robots helping us with cognitive and physical work. This future is really dependent on very important new advancements that will come from all of you and so I'm so excited to see what you'll be doing in the next years in the years ahead so thank you very much and uh come come work with us. "Thank you verymuch and uhcome come work for us," he said.