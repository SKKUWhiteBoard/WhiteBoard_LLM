Sadhana sadhana is a machine learning scientist at Themis Ai and the lead TA of the course intro to deep learning at MIT. She'll be teaching us more about specifically the bias and the uncertainty of AI algorithms which are really two key or critical components towards achieving this Mission or this vision of safe and trustworthy deployment of AI all around us. Sadhana will talk about how we can build very modular and flexible methods for AI and building what we call asafe and trustworthy Ai. behalf of Themis so over the past decade we've seen some tremendous growth in artificial intelligence across safety critical domains in the Spheres of autonomy and Robotics. We now have models that can make critical decisions about things like self-driving at a second's notice and these are Paving the way for fully autonomous vehicles and robots. But there's another question that we need to ask which is where are these models in real life a lot of these Technologies were innovated five ten years ago but you and I don't see them in our daily lives. Themis believes that all of the problems on this slide are underlaid by two key Notions the first is bias bias is what happens when machine learning models do better on some demographics than others. The second notion that underlies a lot of these problems today is unmitigated and uncommunicated uncertainty. This is when models don't know when they can or can't be trusted and this results in scenarios such as self-driving cars continuing to operate in environments when they're not 100 confident. Themis is innovating in these areas in order to bring new algorithms in this space to Industries around the world. in the real world because American English is highly overrepresented as opposed to other demographics but that's not where bias and data stops. These biases can be propagated towards models training Cycles themselves which is what we'll focus on in the second half of this lecture. Once the model is actually deployed we may see further biases perpetuated that we haven't seen before. The distribution that the data is coming from would shift significantly across this decade and if we don't continue to update our models with this input stream of data we're going to have Obsolete and incorrect predictions. Commercial facial detection systems are everywhere you actually played around with some of them in lab two when you trained your vae on a facial detection data set. The biases in these models were only uncovered once an independent study actually constructed a data set that is specifically designed to uncover these sorts of biases by balancing across race and gender. There are other ways that data sets can be biased that we haven't yet talked about so so far we've assumed a pretty key assumption in our data set which is that the number of faces in ourData is the exact same as theNumber of non-faces in our Data set. accurate class boundary between the two classes so how can we mitigate this this is a really big problem. The first way that we can try to mitigate class imbalance is using sample re-weighting which is when instead of uniformly sampling from our data set we instead sample at a rate that is inversely proportional to the incidence of a class in the data set. The second way we can mitigate class and balance is through loss re- weightsing which means that samples from underrepresented classes contribute more to the loss function. yet is latent features so if you remember from Lab 2 and the last lecture latent features are the actual represent is the actual representation of this image according to the model. The bias present right now is in our latent features all of these images are labeled with the exact same label. We can't apply any of the previous approaches that we used to mitigate class imbalance because our classes are balanced but we have feature imbalance now. However we can adapt the previous methods to account for bias in latent features which we'll do in just a few slides. our biased features and then apply resampling so let's say in reality that this data set was biased on hair color most of the data set is made up of people with blonde hair with faces with black hair and red hair underrepresented. If we knew this information we could label the hair color of every single person in thisData set and we could apply either sample re-weading or loss relating just as we did previously. The question is what if we had a way to automatically learn latent features and use this learn feature representation to dbias a model. Reconstruction loss between the inputs and the outputs and continue to update their representation of the latent space. A de-biasing algorithm that automatically uses the latent features learned by a variational autoencoder to under sample and oversample from regions in our data set. We want samples that are similar to each other in the input to decode to latent vectors that are very close to eachOther in this latent space and samples that're far from each other or samples that's dissimilar to each each other should decode to should encode to latent vector that are far from Each other. good representation of what a face actually is so now that we have our latent structure we can use it to calculate a distribution of the inputs across every latent variable and we can estimate a probability distribution depending on that's based on the features of every item in this data set. This allows us to train in a fair and unbiased manner to dig in a little bit more into the math behind how this resampling works this approach basically approximates the latent space and then we can over sample denser or sparser areas of the data set and under sample from denser areas. via a joint histogram over the individual latent variables so we have a histogram for every latent variable Z sub I and what the histogram essentially does is it discretizes the continuous distribution. As Alpha increases this probability will tend to the uniform distribution and if Alpha decreases we tend to de-bias more strongly. This gives us the final weight of the sample in our data set that we can calculate on the Fly and use it to adaptively resample while training. Once we apply these this debiasing we have pretty remarkable results. lecture so so far we've been focusing mainly on facial recognition systems and a couple of other systems as canonical examples of bias however bias is actually far more widespread in machine learning. Consider the example of autonomous driving many data sets are comprised mainly of cars driving down straight and sunny roads in really good weather conditions with very high visibility. In some specific cases you're going to face adverse weather bad um bad visibility near Collision scenarios and these are actually the samples that are the most important for the model to learn. tend to amplify racial biases a paper from a couple years ago showed that black patients need to be significantly sicker than their white counterparts to get the same level of care. That's because of inherent bias in the data set of this model. In all of these examples we can use the above algorithmic bias mitigation method to try and solve these problems and more so we just went through how to mitigate some forms of bias in artificial intelligence and where these Solutions may be applied. We talked about a foundational algorithm that Themis uses that UL will also be developing today. the core idea behind uncertainty estimation so in the real world uncertainty estimation is useful for scenarios like this this is an example of a Tesla car driving behind a horse-drawn buggy which are very common in some parts of the United States. The exact same problem that resulted in that video has also resulted in numerous autonomous car crashes so let's go through why something like this might have happened there are multiple different types of uncertainty in neural networks which may cause incidents like the ones that we just saw we'll go through a simple example that illustrates the two main type of uncertainty that we'll focus on. model won't be able to compute outputs for the air points in this region accurately because very similar inputs have extremely different outputs which is the definition of data uncertainty we also have regions in this data set where we have no data so if we queried the model for a prediction in this part of the data set we should not really expect to see an accurate result because the model's never seen anything like this before and this is what is called Model uncertainty when the model hasn't seen enough data points or cannot estimate that area of the input distribution accurately enough to Output a correct prediction. names to the types of uncertainty that we just talked about the blue area or the area of high data uncertainty is known as aliatoric uncertainty. The goal of out estimating alliatoric uncertainty is to learn a set of variances that correspond to the input keep in mind that we are not looking at a data distribution and we are as humans are not estimating the variance we're training the model to do this task. The crucial thing to remember here is that this variance is not constant it depends on the value of x. with very low variance so our variance cannot be independent of the input and it depends on our input X so now that we have this model we have an extra layer attached to it in addition to predicting y hat we also predict a sigma squared how do we train this model our current loss function does not take into account variance at any point this is your typical mean squared error loss function that is used to train regression models. We want to generalize this loss function to when we don't have constant variance and the way we do this is by changing the lossfunction to the negative log likelihood. a data set called cityscapes and the inputs are RGB images of scenes the labels are pixel wise annotations of this entire image of which label every pixel belongs to and the outputs try to mimic the labels they're also predicted pixel wise masks. Why would we expect that this data set has high natural alliatoric uncertainty and which parts of this dataSet do you think would have aliatoric uncertainty? Because labeling every single Pixel of an image is such a labor-intensive task and it's also very hard to do accurately. An ensemble of networks can be used to estimate epistemic uncertainty. The key insight for ensembles is that by introducing some method of Randomness or stochasticity into our networks we're able to estimate epistemic uncertainty. To implement this what we have is a model with the exact one model. We add Dropout layers with a specific probability and then we run multiple forward passes and at every forward pass different layers get dropped different nodes in a layer get dropped out. We use the mean of these samples as the new prediction. epistemic uncertainty let's go back to our real world example let's say the again the input is the same as before it's a RGB image of some scene in a city. We can see why the areas of the sidewalk that are discolored have high levels of epistemic uncertainty. Once we're actually training a model if it's already been trained on a bias data set we can de-bias it adaptively during training using the methods that we talked about today and afterwards we can also verify or certify deployed machine learning models. guardian and that's essentially a layer between the artificial intelligence algorithm and the user and the way this works is this is the type of algorithm that if you're driving an autonomous vehicle would say hey the model doesn't actually know what is happening in the world around it right now as the user you should take control of this autonomous vehicle. We can apply this to spheres outside autonomy as well so you'll notice that I skipped one part of the cycle I skipped the part about building the model. We're going to focus a little bit on themes ai's product called capsa which is a model agnostic framework for risk estimation. then further analyze and so this is the one line that I've been talking about um after you build your model you can just create a wrapper or you can call a wrapper that capsa has a an extensive library of. Capsa wraps models for every uncertainty metric that we want to estimate we can apply and create the minimal model modifications as necessary while preserving the initial architecture and predictive capabilities. This could be adding a new layer in the case of a variational autoencoder this could be creating and training the decoder and calculating the Reconstruction loss on the Fly. Themis is a company that develops and deploys trustworthy AI across Industries and around the world. We're hiring for the upcoming summer and for full-time roles so if you're interested please send an email to careers themesai.io or apply by submitting your resume to the Deep learning resume drop and we'll see those resumes and get back to you thank you foreign thank you for joining us today. We'll use capsa in today's lab to thoroughly analyze a common facial detection data set that we've perturbed.