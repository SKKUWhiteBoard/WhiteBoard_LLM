GPT relies on a lot of tricks and Engineering insights and breakthroughs that we're not going to cover. We apply this self-supervised learning where we learn without label data so we can get as much data as we want because there's no human being in the loop. What we get from this you know by learning from observation and learning from the data directly is a very contextual and relational understanding of meaning. We'll talk about something that's extremely engineering heavy in you know chat GPT. then reduce all other ones so you know the next time it sees uh the same example or a similar example it actually does better and you know this is just one single example but you accumulate all of these directions and information across a batch of examples that you see at the same time. So it takes small small steps to getting a a better and better distribution and a more itic distribution of what word will come next given previous words and you do this in a batch on tons of examples and of course you know we have unlimited amount of data. Transformer part is extremely extremely important so there's a debate a little bit what was the most influential part of making uh CHP and large language model possible uh Transformer is definitely a significant part of it and and I'll let you judge for yourself but uh I think it's less important than the actual modeling perspective that we've we've come up with. In order to understand the Transformer we're going to start to thinking about how we can process sequences so text is just sequence of words. is that for every step here that's label with the same uh digit you know they can all be done in parallel. This is key because in in deep learning we use this uh um computer is called gpus. If we can make multiple steps into single step in parallel this is a single cost. We want to run things in parallel as much as possible so here basically you know this be a cost of four because all these different numbers can be run in parallel uh so this would just be acost of four and then of course processing this whole sequence will be a costs of nine. possible to be as useful as possible okay so uh these are the the the three different things we want to address right what's good and bad dialogue. We want to be more robust and learn to solve correct and this is where we're going to do reinforcement learning from Human feedback. Open AI does on chtp and this was very very hyped for a long time but now people talk less about it uh okay so what do we do well we have a great model that's been fine tune on dialogue and it's able to generate really good answers. the World by looking at videos right you can even sort to understand how human beings work even better because you can see people being upset or sad or happy whatever right in in a video and start picking these cues up. You can connect the vision part to the text part and get a multimodality model that's able to do both in a really really sophisticated way uh also something that I think these these people are working on all right thank you thank you for your time and good luck with your book. The third lecture on Foundation mulative AI will cover chat GPT. Chat GPT was the the tool or the the AI that really made people understand this is different now. Next time we'll talk about stable diffusion image generation. Then we'll cover emerging Foundation models basically Foundation models generative AI in the commercial space. And then we'll end with the lecture on AI ethics and regulation as well as a panel okay so what have we talked about before we started off H with an introduction a short high level intuitive answer to what is foundation MGenerative AI.