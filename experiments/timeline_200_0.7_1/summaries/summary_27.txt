MIT OpenCourseWare continues to offer high-quality educational resources for free. To make a donation or to view additional materials from hundreds of MIT courses, visit MIT OpenCourse Ware at ocw.mit.edu. The following content is provided under a Creative Commons license. Your support will help MIT Open courseWare continue to offer free, high- quality educational resources in the U.S. and around the world. For more information on MIT Open CourseWare, visit opencourseware.org. maybe you'll see the distinction between those things and understand why one version of the problem is much easier than another. But we try to respond as quickly as possible when we notice a typo like that so that we can set you guys on the right course. So we've got two lectures left discussing linear algebra before we move on to other topics. We're still going to talk about transformations of matrices. We looked at one type of transformation we could utilize for solving systems of equations. Today, we'll look at another one, the eigenvalue decomposition. This course moves at a pretty quick pace. We don't want anyone to get left behind. Speaking of getting left behind, we ran out of time a little bit at the end of lecture on Wednesday. That's OK. There were a lot of good questions that came up during class. And one topic that we didn't get to discuss is formal systems for doing reordering in systems of equations. We saw that reordering is important. In fact, it's essential for solving certain problems via Gaussian elimination. be stuck. It won't proceed after that. So it's the difference between getting a solution and writing a publication about the research problem you're interested in and not. So how do you do reordering? Well, we use a process called permutation. There's a certain class of matrix called a permutation matrix that can-- its action, multiplying another matrix, can swap rows or columns. So if I want to swap column 1 and 2, I multiply A from the right by P transpose. P permutation matrices are one class, maybe the simplest class, of unitary matrices. They're just doing row or column swaps, right? That's their job. And so if I have some reordering of the equations or rows of my system of equations that I want, that's going to be indicated by a permutation matrix-- say, P1. That would reorder the rows. If I swap the rows and then I swap them back, I get back what I had before. So there's a formal system for doing this sort of swapping. This is a form of preconditioning. It's always done via Gaussian elimination if we want an exact solution. You're studying one of them in your homework assignment now, where you know the matrix is banded with some bandwidth. So you don't do elimination on an entire full matrix. You do it on a sparse matrix whose structure you understand. We discussed sparse matrices and a little bit about reordering and now permutation. I feel like my diffusion example last time wasn't especially clear. So let me give a different example of diffusion. seen The Price Is Right? This is a game where you drop a chip into a board with pegs in it. It's a model of diffusion. The Plinko chip falls from level to level. It can go left or it can go right with equal probability. So the probability that I'm in a particular cell at level i is this Pi plus one. And there's some sparse matrix A which spreads that probability out. It splits it into my neighbors 50/50. And we'll see the simulation that tells us how probable it is to find the Plinka chip in a certain column. values for a couple of elements of this matrix. But this is a sparse matrix. It has a sparse structure. It models a diffusion problem, just like we saw before. Most of physics is local, like this, right? I just need to know what's going on with my neighbors. And I spread the probability out. I get this nice diffusion problem. Here's something to notice. This probability distribution always seems to flatten out. It becomes uniform. It turns out there are even special distributions for which A times A times that distribution is equal to that distribution. is one of the eigenvectors of this matrix A times A. It's a particular vector that when I multiply it by this matrix AA, I get that vector back. It happens to be unstretched. So this vector points in some direction. I transform it by the matrix. And I get back something that points in the same direction. That's the definition of this thing called an eigenvector. And this will be the subject that we focus on today. And finding the amount of stretch, which are complex numbers. eigenvector-eigenvalue pairs involves solving N equations. We'd like to know what these eigenvectors and eigenvalues are. They're non-linear because they depend on both the value and the vector, the product of the two, for N plus 1 unknowns. We don't know how to solve non- linear equations yet. So we're kind of-- might seem like we're in a rough spot. But I'll show you that we're not. understood them, then we can do a transformation. So I'll explain that in a minute. But how do you actually find these things, these eigenvalues? Well, I've got to solve an equation A times w equals Lambda times w. And so the solution set to this equation is either w is equal to 0. That's one possible solution to this problem or the eigenvector w belongs to the null space of this matrix. It's one of those special vectors that when it multiplies this matrix gives back 0. Determinant of a matrix like A minus Lambda I is a polynomial of degree N. The N roots of this characteristic polynomic are called the eigenvalues of the matrix. So if we can compute that determinant and solve for Lambda, then we'll know the Eigenvalue. There are all the possible amounts of stretch that can be imparted to particular eigenvectors, right? We don't know those vectors yet, but we'll find them in a second. do an example. Here's a matrix, minus 2, 1, 3. And it's 0's everywhere else. Can you work out the eigenvalues of this matrix? Let's take 90 seconds. You can work with your neighbors. Nobody's collaborating today. I'm going to do it myself. OK. What are you finding? Anyone want to guess what are the eigevalues? AUDIENCE: [INAUDIBLE] JAMES W. SWAN: Good. OK, so we need to know A minus lambda I and its determinant. The elements of a diagonal matrix are always the eigenvalues because the determinant is the product of the diagonal elements. So these diagonal values here are the roots of the secular characteristic polynomial. It turns out the diagonal Elements of a triangular matrix are eigen Values, too. This should seem familiar to you. We talked about easy-to-solve systems of equations, right? Diagonal systems of equation are easy to solve. It's also easy to find their eigen values. If a matrix is real-valued, then we know that we're going to have a polynomial of degree N. It can have no more than N roots, right? And so A can haveno more thanN distinct eigenvalues. But complex eigenvalue always appear as conjugate pairs. And here's a couple other properties. The trace of a matrix, which is the sum of its diagonal elements, is the product of the eigen values. It's possible that lambda 1 here is an eigen value twice. a matrix is also the sum of the eigenvalues. These can sometimes come in handy-- not often, but sometimes. Here's an example I talked about before-- so a series of chemical reactions. We want to know how the concentrations of A, B, C, and D vary as a function of time. And our conservation equation for material is here. This is a rate matrix. We'd like to understand what the characteristic polynomial of that is. The eigen values of that matrix are going to tell us something about how different rate processes evolve. The characteristic polynomial looks like this. What are the eigenvalues of the rate matrix? What is this eigenvalue 0 correspond to? What's that? OK. Physically, it's a rate process with 0 rate, steady state. What physical process does that represent? It's something evolving in time now, James Swan says. He asks the audience to guess what 0 is and what it means. The audience's guess is that it means 0 is a solution. It's 0. Minus k1 is another solution. The eigenvalues can be interpreted in terms of physical processes. This quadratic solution here has some eigenvalue. I don't know what it is. But it involves k2, k3, k4. And this is a typo. It should be k5. And so that says something about the interconversion between B, C, and D. Is that too fast? Do you want to write some more on this slide before I go on, or are you OK? Are there any questions about this? No. There's a null space to this matrix, right? We won't be able to eliminate everything. So let's try to find the eigenvectors of this matrix. They're minus 2, 1, and 3. So I want to solve this equation A minus this particular Lambda, which is minus 2,. times identity equals 0. It's already eliminated for me. I have one row which is all 0's, which says the first component of my eigen vector can be freely specified. The other two components have to be 0. This eigenvalue? 0, 0, 1, or anything proportional to it. All these eigenvectors have a geometric multiplicity of 1, right? I can just specify some scalar variant on them. And they'll transform into themselves. Can you do that? Can you find this eigenvector? Try it out with your neighbor. See if you can do it. And then we'll compare results. This will just be a quick test of understanding. Are you guys able to do this? Sort of, maybe? James W. Swan: Try this example out. See if you can work through the details of it. I think it's useful to be able to do these sorts of things quickly. Here's a matrix. It's not a very good matrix. But it's all 0's. So what are its eigenvalues? It's just 0, right? And they're 0. That eigenvalue has algebraic multiplicity 2. Can you give me the eigenvectors of this matrix? of equations or for transforming systems of ordinary differential equations. But we're only going to be able to do that when we have this complete set of eigenvectors. When we don't have that complete set, we're going to have to do other sorts of transformations. You have a problem in your homework now, I think, that has this sort of a hang-up associated with it. That's something to think about. A times a matrix W is equal to W times the matrix Lambda. made them the columns of a particular matrix. But it's nothing more than a restatement of the fundamental eigenvalue problem we posed at the beginning here. But what's nice is if I have this complete set of eigenvectors, then W has an inverse that I can write down. So another way to state this same equation is that the eigenvalues can be found from this matrix product. And under these circumstances, we say the matrix can be diagonalized. There's a transformation from A to a diagonal form. ways of writing this fundamental relationship up here when the inverse of W exists. So this is a useful sort of transformation to do. We haven't talked about how it's done in the computer. The computer won't do Gaussian elimination for each of those eigenvectors independently, right? Each elimination procedure is order N cubed, and you got to do that for N eigenvctors. That's pretty slow. There's an alternative way of doing it that's beyond the scope of this class called-- it's called the Lanczos algorithm. N cubed sort of calculation to find all the eigenvalues and eigenvectors solving a system of equations. Here's an example of how this eigendecomposition can be useful to you if you did it. So we know the matrix A can be represented as W Lambda W inverse times x equals b. But there are times when we deal with so-called symmetric matrices, ones for which they are equal to their transpose. So this becomes trivial to do then, this process of W inverse. in a lot of cases. You can prove-- I might ask you to show this some time-- that the eigenvectors of a symmetric matrix are orthogonal. They're also useful when analyzing systems of ordinary differential equations. So here, I've got a differential equation, a vector x dot. So the time derivative of x is equal to A times x. And if I substitute my eigendecomposition-- so W lambda W inverse-- and I define a new unknown y instead of x, then I can diagonalize that system of equations. There are many times when there's not a complete set of eigenvectors. And then the matrix can't be diagonalized in this way. So there's an almost diagonal form that you can transform into called the Jordan normal form. There are other transformations that one can do, like called, for example, Schur decomposition, which is a transformation into an upper. And you'll find out that this same sort of analysis can be quite useful in nonlinear systems of nonlinear equations. triangular form for this matrix. We'll talk next time about the singular value decomposition, which is another sort of transformation one can do when we don't have these complete sets of eigenvectors. You'll get a chance to practice these things on your next two homework assignments, actually. So it'll come up in a couple of different circumstances. I would really encourage you to try to solve some of these example problems that were in here. Solving by hand can be useful.