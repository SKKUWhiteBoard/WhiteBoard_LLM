In this module, I'm going to briefly introduce the idea of differentiable programming. Differentiable programming is closely related to deep learning. I've adopted the former term as an attempt to be more precise in terms of highlighting the mechanics of writing models as you would code. So let's begin with our familiar example, a simple neural network. And this is the programming part of differentable programming which allows you to build up an increasingly more sophisticated model without losing track of what's going on. Convolutional neural networks are a refinement of a fully connected neural network. ConvNets have two basic building blocks. Conv takes an image and the image is represented as a volume which is a collection of matrices. Each matrix has the same dimensionality as the image. Conv is going to compute another volume of a different size, usually the height and width of this volume. And at the end, you get a vector representing the probabilities of the different object categories. Click here for Andrej Karpathy's excellent demo where you can actually create and train ConvNet in your browser. be learned. The second thing is I also haven't specified the hyperparameters which is the number of channels, the filter sizes, and so on, which are actually pretty important for getting a good performance. But I just wanted to highlight the overarching structure and the idea that you can compose in a fairly effortless way. So now let's turn our attention to natural language processing. So here is a motivating example. Suppose we want to build a question answering system. We have a paragraph. It's from Wikipedia and we have a question. We want to select the answer from that passage, from the paragraph. In NLP, words are discrete objects and neural networks speak vectors. So whenever you're doing NLP with neural nets, you first have to embed words, or more generally, tokens. There's a lot of processing that needs to happen and it's hard to kind of specify in advance. A SequenceModel is going to be something that takes a sequence of input vectors and produces a corresponding sequence of output vectors where each vector in this sequence is a process with respect to the other elements. I'm going to talk about two implementations of the sequence models. One is recurrent neural networks and one is transformers. Here is one of the input vectors. x1, x2, x3, x4. I'm going to reduce its dimensionality to also 3 dimensions. And now I can take the dot product between these x's and y's. So that's an attention mechanism. You can think about this as a sequence model that just takes input sequence and contextualizes the input vector into output vectors. And if this is junk, then anything that I build on top of that is junk. processing each xi in context. Now we have enough that we can actually build up to BERT which was this complicated thing that I mentioned at the beginning. So BERT is this large unsupervised pretrained model which came out in 2018 which has really kind of transformed NLP. And the basic building block for generation is, I'm going to call it GenerateToken. And you take a vector x and you generate token y. And this is kind of the reverse of EmbedToken which takes a token and produces a vector. encourage you to consult the original source if you want kind of the actual, the full gory details. Another thing I haven't talked about is learning any of these models. It's going to be using some variant of stochastic gradient descent, but there's often various tricks that are needed to get it to work. But maybe the final thing I'll leave you with is the idea that all of differentiable programming is built out of modules. Even if you kind of don't understand or I didn't explain the details, I think it's really important to pay attention to the type signature of these functions.