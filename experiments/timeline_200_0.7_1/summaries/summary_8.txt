The final contest, which is due tonight, is to design an agent that plays together with another agent to try to collect food pellets while not getting eaten by ghosts. submissions for that, your last chance to submit are tonight at midnight. And on Thursday in lecture, we'll discuss the results. What else is left? I think there is a project due next week. There is still a section this week. And I think that homework is all wrapped up, but you would still have a self-assessment of your last homework that will be due next. And then there's a final exam the week after that. be mostly on advanced applications. The idea behind these two lectures is to look at advanced applications, where we have covered a good amount of the material in the ideas behind those applications. We will not quiz you on these application lectures, on the final exam, or anything like that. It's more meant to give you more perspective rather than extra study materials for the final. So far, I've looked at foundational methods for search, for acting adversarial environments, for learning to do new things, and for dealing with uncertainty. Today's state-of-the-art in Go is that there are computer players better than the best human players. But actually, if you went back to March 2016, that was not the case yet. So how do you make an AI for Go? Let's go back to what we were looking at in lecture on games, MiniMax. MiniMax is about solving games in adversarial environments. And you reason about it. And in what it did, we had to update this graph that you see at the very beginning of the course, where we had already checkers fully solved. For a game like Tic-Tac-Toe, you will find out that you can force a draw, and that means fully solving the game. For Go, this is actually pretty hard to do, and it's even much harder than chess. And why? Let's take a look at chess. It's a 19 by 19 board, so there's 19 times 19 positions to choose from in the first move. And then one less, of course, next move. But the branching factor is enormous. So if you tried to run an exhaustive search through this kind of game tree, it's not going to work. The evaluation function was learned. A deep neural network was trained to provide a good evaluation function at various stages of the game. But even then, because the branching factor is so large, the evaluation function, sure, if you want to look one step ahead, no problem. But many of the moves are not that useful. So the question you can ask, can we learn another neural network that can tell us how to reduce which moves to consider when we do our depth-limited search? And so policy network can be trained, which assigns probabilities to all possible moves. And then you might only consider the moves with high probability when running your search. DeepMind's AlphaGo is a computer program that can learn to play the game of Go without the help of humans. It uses deep neural networks and tree search, combined, to bring together two topics we've covered in this class. AlphaGo Zero has also been used to learn chess and, I believe, Shogi, or some other board game that's strategically similar to chess, but a little more complicated. Any questions about this? We'll cover a sequence of different applications one after the other, and this is the part for Go. When you play against yourself, initially, you're playing pretty poorly, of course, because you don't know how to play it. It's a random neural network, or a network that's randomly initialized, making decisions. And so it turns out that in terms of reinforcement learning problem, problems you can phrase as you playing against a version of yourself, and that kind of problem is much easier to learn than most other things. And the longer it keeps playing itself, the better it keeps getting. might overfit to your exact current way of playing. Sometimes the human knowledge put into it is looking at past games of humans, and training a value network and a pulse network to predict what happened in there. But maybe if you learn from scratch, you can find something even way better? I don't know the answer to that. But empirically, it seems that learning from scratch reaches levels beyond the levels reached with the previous methods. But yeah, who knows? There is no final answer yet. moment, ?] another [INAUDIBLE] PROFESSOR: There is something called fictitious play. In fictitious play, you kind of play against yourself in a slightly more complicated setting than this. So I believe it includes this, the result. By playing yourself, you're guaranteed to reach an equilibrium. Actually, what reaches the equilibrium is the average version of all your past selves. So rather than your actual current network being the equilibrium solution, it's the average of all past. AlphaGo Zero is compared with AlphaGo Lee, which is the version that played Lee Sedol. So AlphaGo Zero has no prior knowledge. It starts at a pretty, actually, negative elo rating. Loses all the time initially. It's playing itself, but then gets tested against a set of players to calibrate its elo. And the green line is we're Alpha go Lee. The blue line is Alpha go Zero. The red line isAlpha go Master. The yellow line isalpha go master. Sedol was. that blue dotted line, after 21 days, it goes past where AlphaGo Master, was which was an improved version of AlphaGo Lee Sedol. And then it was still creeping up after 40 days. Once you reach that level, essentially, there's no further to go, because you solved the game. And I don't know what elo rating would correspond to that. Any other questions about Go? Yes? STUDENT: [INAUDIBLE] PROFESSOR: Well, that's a good question. The question was, do we think we can solve the game with enough compute power? With reasonable compute power, it traverses the whole tree. Even with alpha beta pruning, I don't think that'll happen anytime soon. It's a really big tree. If you do a more kind of brute force style approach-- not 100% brute force. It might be. But pretty much no one would say less than 20, and most people would say 50 or more. And that's maybe when we reach human level play. But that's a given. Now, with infinite computing power, for sure. Then you can definitely solve it. even longer. But the more brute force style approach, where you don't have good value functions, you donâ€™t have good policy functions, was expected to take decades before we reached compute levels that can do that. It could be that by using human knowledge, you're in some kind of based enough attraction. For a local [? optim, ?] that maybe not as good as another one that might be out there. I don't know if that would be the case or not, but that's a possibility. Some examples in a few other problems settings soon. And then the reinforced learning on top of that to further improve. That's kind of the standard way to solve a problem. From a research point of view, it's very interesting to see where you can get from scratch. OK, switching gears to helicopters. Here's a motivating example. How do you get a helicopter to do this autonomously? And by the way, this is done autonomously here, but how do we get to something like this? like this? Well, what does it mean to fly a helicopter? What are the challenges? There's two key challenges. One is tracking where the helicopter is at any given time, because if you don't know where it is, you'll not be able to control it very reliably. And then the other thing is to decide what to do, what controls is sent to the helicopter. Typically, have a remote control, which has two joysticks. You send controls from that to the helicopters. We have four control channels, two in each joystick. A collective is the action for the main rotor collective page. It's the average angle of attack as the blade goes through the air. So that way, you can generate a torque that allows your helicopter to roll or pitch based on how much differential thrust you have from back, left to right. The tail rotor has a variable pitch also, and that pitch allows you to modulate how much thrust you get from the tail rotor. That's how you control a helicopter. You cannot directly ask it to fly forward or sideways. in a helicopter frame. vertically, the helicopter frame will now be partially forward and partially up, and up compensates for gravity and the forward is what lets you fly forward. So you can build a model for this from collecting data from your helicopter, let's say, and learning a bunch of parameters that predict the next state, given current state and action. For hover, we saw this. You can have a reward based on distance to target location, x star, y star, z star. And maybe penalizing for non-zero velocity, so you want it to hold still. not possible because when you are spinning and you're not able to generate direct vertical thrust, if you generate that vertical thrust in your helicopter frame, you move forward or backwards in some way. So maybe we decide as a target, for this helicopter, to do this kind of maneuver. But exactly what will be the details? It's not easy. Exactly how much do you push up? How fast do you spin? Difficult things to decide. And so we designed a trajectory. This is me, together with Adam Coates and Morgan Quigley. And then had reinforcement learning, learn a controller to follow the trajectory that we design. pretty good. And then we run it on our helicopter. So this is our helicopter, indeed, flipping, which is great. It's moving more than we want it to move. And it went into the trees. So let's think about what happened here. What happened in the real world is we're trying to follow this path that we design where the maximal reward is. And so we had asked it to fly a path that's not flyable. So it starts deviating from that path. As it deviates, what it learned in the simulator becomes less and less relevant. saw it making these wild motions, overcompensating. It pushed the controls so hard that the engine died. At that moment, the blades stopped spinning, or they slowed down. You lose control over your helicopter, more or less, at that point. Then what happened is our human pilot took back control to try to save the helicopter. And believe it or not, they actually saved this. Helicopter incredible. But he knew from past experience. His dynamics model was pretty good, I guess. was just phenomenal. It's behind the trees. He doesn't see it. He knows kind of what's happening. And right before he lands, he changes the collective control to not absorb energy into blades anymore, but to push it all back out. That kinetic energy that's in the spinning blades now goes into pushing air down to slow down the helicopter. It landed a little harder than you want to land, but it landed on its feet. So clearly, human level control, at that point, was much better than our autonomous controller. the helicopter to follow a path that's not flyable. So how do we get a specification of what we should be flying? Well, we could learn the trajectory from these as noisy observations. What methodology do we have for that? Hidden Markov models. If we collect paths from a human pilot and then ask the helicopter to fly those paths, they tend to be noisy. But if we collect many demonstrations, we figured, then, this set of demonstrations captures the essence of what it means. have something we don't know that evolves over time, but we have some noisy measurements of it, we can run an HMM to recover what we actually want. We use something called dynamic time warping. So what does that do? You can align two trajectories. After you do that, then you can run the inference and hidden Markov model, just standard based net inference, which, in this case, was just a guess. Then you run dynamic time Warping, which aligns each trajectory with the hidden one. But in the process, also aligns all the demonstrations, because they're all aligned with this one reference. is an extended common filter/smoother, which is a forward/backward path, similar to what we covered, but done for continuous variables rather than for discrete variables. Let's then re-infer, through probabilistic inference, what the hidden state might be. Keep repeating this till we reach some fixed point, and that will be our target for our helicopter to fly. What does this look like? Here is, in white, the target found through the hidden Markov model inference, and in color, still the demonstrations. We have collected data to learn a dynamics model for the helicopter. We can now penalize our award, penalized for deviating from the target. And then we can run reinforcement learning in simulation, let's say, in this learn simulator to find a good controller and run it on the real helicopter. The controller we learn in simulation is still a little optimistic about really following that path. So while we fly the helicopter, we'll do depth-limited search to improve what we have. able to look ahead only two seconds, rather than needing to look Ahead much further. A value function tells us, OK, how good is it to end up here? We also have a reward at each time tick. And our search over those two seconds is what results in the control we apply. Here's what we get. So fully autonomous. Takeoff, flipping over during takeoff. Hover. So this method can also learn to hover, no problem. Then it goes into forward flight and it's going to do something called split [? us, ?] where you do a half roll, half loop. The fastest we flew this helicopter was close to 55 miles per hour, so almost highway speeds. The algorithm's only this big, so it's pretty fast for something of this size. Inverted flight. Knife edge fall. Stall turn again, coming out tail first. Hurricanes are fast backward flying circles. And then now are actually some of the hardest maneuvers to execute on. Why is flying 55 miles an hour not harder than this? Well, when you do something like this and hear the tick tock, the hardest maneuver in this air show. Berkeley PhD student Woody Hoburg set up the helicopter to learn from scratch. Hoburg shut it off whenever it starts tilting itself, so it lands on some pretty wide landing gear so it's more stable. He was able to have it learn to hover reliably with the only human input being shut off when it looks like it might start doing something dangerous. But that was the onlyHuman input required. We did not push that further to flying those maneuvers. There is some work to be done. at OpenEye, there's been some work on robots learning to do back flips. And that was kind of one step further. It wasn't just shutting it off. You would watch your robot try things. And human input would be not specifying a reward function, which is very hard to do for things like back flips, just like it was hard here. But what they did is they said, the human watches it and says, which one is better or worse among a set of them? it has more time, and if it already has a recovery controller, then you can imagine that. And Claire Tomlin's group here at Berkeley has done some work in that direction, where they have a safe controller and a learned controller. And the learning controller is learning on its own while the safe controller keeps things in check so the helicopter doesn't crash. Yeah, the mass properties change a little bit. So it actually more power, until the very end, of course, when it has no power left. gets a little more power. Maybe a question related to that is, how much power does this thing actually have? This helicopter had inverted slide, where it has more power, 3 Gs. So it can generate three times the power of gravity. Of course, you need to generate one of them to even stay up in the air, but it still had two G's left to do other things with. And regular flight had about 2.5 G's maximum acceleration. OK, let's take a short break here. And after the break,let's do legged locomotion and manipulation. is a separate linear feedback controller for each time slice. And the way you learn that feedback controller is by doing a forward pass to see what your current sequence of controllers achieves. And then you can do a backward pass, which is, essentially, a value attrition pass over that same trajectory to find the optimal sequence of feedback controllers. So essentially, value iteration, but in a continuous space. A continuous space is always harder to represent things, so that's why you make a simplifying assumption. We assume that we are just going to use a sequence of linear controllers. follow a new path. Along that new path, we linear the dynamics, because dynamics' not really linear. We approximate it linearly. We do another backward pass along that, and keep repeating this until it's converged. Once we've done that, we have value functions everywhere and we have linear feedback controllers everywhere. If there is no wind, you can actually just run the linear feedback control. It will be fine. But if there's some wind gusts that could throw you off, you want to use the value functions and the two second look ahead against those value functions. Walking tends to be harder than flying for robotic control. If you're flying, you're up in the air. Everything is the same. Unless there's some weird air flows, that's the only thing that really changes. When you're walking, the surface could change all the time. A lot more change in your environment. Here's an example of how hard this can be. This is a video from 2015, there was the Doppler Robotics Challenge, which was held in Pomona, just east of Los Angeles. of Los Angeles. People had two years to work on this. And what did the robot have to do? It had to, essentially, drive a car or walk, but driving the car was recommended. Then get out of the car, walk a little bit, open a door, grab a drill, drill a hole, walk some more. So doesn't sound that complicated. But actually, it turned it's very complex to get a robot to do that. And here's some footage from the competition. very funny to watch the video, but it turns out at the competition, everybody was really sad for the robots when the robot fell. It was very interesting to see how people really connected with these robots and really felt for them and wanted them to succeed. Now, this is indicative of how hard this can be. And why is this so hard? Why did this not just work? Well, if you look underneath of a lot of what was going on in these approaches is they would build a model of the world. or not you're already making contact, and making contact or not. You can be very close, but not have contact. It's a very subtle thing. You don't have contact, you don't get to apply any forces. So it's very hard to do this. Now, what's changed recently in the past few years is that through advances in deep learning, it's been possible to better map from raw sensory information to controls you might want to apply. And ultimately, it learns to walk. Reinforcement learning algorithm can be reused directly onto other robots and can learn to control these other robots. The reward function is the closer your head is to standing head height, the better. So sitting is better than lying on the ground and standing is even better. At this point, it is possible. You run enough simulation to train up a very wide range of skills. So these are results from Jason Pang. He's the founder and CEO of reinforcement learning company, Pang Robotics. a student here at Berkeley. And at this point, if you have a motion you want a robot to master, Jason's methodology can pretty much make it happen. So this can work directly for building video games. You build video games, you want your main character to move in a realistic way. You can have it sequence together motions like these and dynamically simulate how they interact with the world. Same for animated movies. How about real robots? Let's say you want to get this robot across this terrain. And once I find a good path, I want to control the robot to follow that path. High level control problem, that's actually a star search. If you have a cost function for this terrain, what would the cost function be? Where do you want to place your feet? Well, you maybe don't want to be next to a big cliff. The three on the ground-- the support triangle of that-- if you project down the center of mass of the robot, she'll maybe fall within that support triangle, because then it won't fall over. So there's a bunch of considerations you might have. When we thought about this problem, we had 25 features we came up with that we thought matter. When you run the search, or the value iteration, which is, more or less, equivalent to find a path across this terrain. But if you choose the trade-off between the features differently, you'll find different paths. So reward learning. How you do reward learning, you demonstrate a path. Demonstrating doesn't mean just drawing a line. It actually means choosing a sequence of footsteps that it executes on, and assuming it does well. It assumes you have a low level controller, but that's well understood how to do that. First time it's ever been done, autonomous vehicles. During the first eight miles of the race, Highlander gains two minutes on Stanley. After months of tireless effort, there's a lot at stake. Each one leaves the chute with confidence, a far cry from the first Grand Challenge, where many faltered within sight of the start. No robot went beyond seven miles in last year's race, and no robot has ever gone further than three miles in the history of autonomous cars. Stanford robot becomes first vehicle in history to drive 132 miles by itself. Five hours after leaving the starting line, Stanley now leads the pack, and just five robots remain on the course. To finish, they must wind through a treacherous mountain pass. After driving six hours and 53 minutes at an average speed of 90 miles an hour, Stanley is about to become the first vehicle to drive all 132 miles in a single trip. It will take Stanley six hours to complete the 132-mile journey. work, and was very impressive that, in fact, four cars finished the 150 miles. That's a Berkeley entry. Only motorcycle in the race. Now, what goes onto those cars? There's a few different things. There is IMU, like right on a helicopter, a lot of computers. The GPS compass. Regular GPS to get position. Lasers, where you shoot out laser beams. And based on how long it takes them to get back, you know how far away the nearest obstacle is in that direction. An urban environment will have a lot more obstacles. With HMM, you get 0.02% false positives of where there might be obstacles. Cameras are important, too, of course. With a camera, you can often look further ahead. You might wonder, why do we need cameras if we have already have LIDAR? LIDar sends out a laser beam, measures how long it takes to get back. But usually, it doesn't work beyond 50 meters. You don't get enough signal back. A camera will be better at that than a LIDAR. Somebody needs to tell you what is road, what is not road. Self-supervision is a trick that's very widely used to reduce labeling efforts. So the camera now knows all the red here is road. In urban environments, there's even more need to recognize. A lot of progress has been made this is video from 2013. So after 2005, 2005 was a desert race,2005 was a Desert race, 2005. The Google Self-driving Car Project has been working on self-driving cars for a few years. This was before deep neural networks were heavily used for this kind of thing. It's only getting better to recognize what's in scenes, thanks toDeep neural networks. So what does it tell us? Well, the devil is really in the details, in the long term. The future is very bright for self- driving cars, but we have a long way to go before they're ready for prime time. tail of special events that can happen when you're driving. You can measure progress by just demo videos, which is one way, and it gives you some kind of feel for what's going on. But the 2013 video is already very impressive. So another way to measure progress is to see how are these cars doing relative to human drivers. So left and right are the same plots, but the ride is on a log scale so you can see more detail. It's a number of events per 1,000 miles driven. Red there is human fatalities. Then yellow is human injuries. is between 10 and negative 2 and 10 negative 3 per 1,000 miles of human driving. In green is the Google slash [? wave ?] mode disengagement. It's when the driver decides they want to take control because they don't trust the autonomous system right now to avoid an accident. And we see that it's going down how often that needs to happen, but still a bit removed from where humans are at. Where does this data come from? If you test in California, you have to report this data to the DMV. so many decisions. If they're gigantic, use a lot of power. That's a problem. Let's see what we can do to build smaller networks to make decisions. What else did we not cover yet? Personal robotics. I want to spend a little more than two minutes on that, so let's keep that for Thursday. that's it for today. Bye. [SIDE CONVERSATIONS] [Side CONversation] [sideconversation.com: Do you know more about this topic? Email us at jennifer.smith@cnn.com].