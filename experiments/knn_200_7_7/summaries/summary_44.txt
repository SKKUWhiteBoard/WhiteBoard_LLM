We are going to start talking about the optimization perspective in deep learning for two lectures. The main focus is to analyze what the functions you are optimizing look like so that you can use some trivial or some standard optimization algorithm for it. Note that it's not like these algorithms, like gradient descent or stochastic gradient descent, can work for all functions that you may optimize. There are some negative results about the NP-hardness of optimizing nonconvex functions. So the way that this is really reconciled is that the lower bound, the impossibility results is about worst case functions. Theory of gradient descent is that it cannot always find local mean or global minimum. Finding global minimum of general functions, general nonconvex functions, is NP-hard. But it doesn't really mean that there is no subset of functions that you can easily solve. And I guess the observation four is that objectives in deep learning are to find global minimum in polynomial time. That's what we're trying to do in this talk. We're going to talk about the goals of deep learning and how to achieve them. is nonconvex. It's almost trivial, but not entirely trivial. Most of the convex functions we know are somewhat kind of simple functions. And as soon as you go beyond two layers, it's not convex. And observation five, I think I mentioned it. Sort of like, gradient descent does work. Gradient descent or stochastic gradient descent finds-- let me be precise about this-- approximate, or even sometimes you can claim it's almost exactly, global minimum of loss functions in deep learning. Finding a local minimum is also NP-hard. So we have to consider these kind of pathological cases, which makes things harder. So the way to go beyond it is to remove some of the pathological cases as well so that you can find a localminimum in polynomial time. All note that number number works right, right? All right, cool, that's not entirely important because we are not going to be very quantitative about this. So everything is polynomially time. So I guess let me start with some definitions to formalize it. The problem here is that, when the gradient of fx is 0 and also the Hessian is not strictly positive semidefinite, it's just a positive semidrive. In that direction, you are pretty flat, right? So that makes it tricky because then the higher order gradients start to matter. And when you look at this-- and once it becomes about the third order derivative or fourth order derivative, things becomes much more complicated. So there's no set of cases in your function. This is strict-saddle. you still have to do the Scribe kind of from scratch in some sense. OK, cool. So the definition of strict-saddle, I'm citing this paper just because it's not like every paper is exactly the same definition. So we say f is alpha, beta, gamma strict-Saddle if, for every x in RD, it satisfies one of the following. The condition itself is not supposed for people to numerically check. So this condition is not something that you are supposed to check numerically. kind of like informal just because I'm not-- it's pretty formal in the sense that all the bounds are correct. It's just that I wouldn't specify some of the details. So suppose f is alpha, beta, and gamma strict-saddle. Then many optimizers, for example, GD, SGD, and many other articles, like cubic regularization, I guess many algorithms can do this. So far as can converge to a local min with epsilon error in Euclidean distance in time. exactly the thing that we did for the strict-saddle. But if you think about it, it's basically the same statement. OK. Anyway-- so cool. So we are basically done with the first part, so about identifying the subset of functions that are easy to optimize. And next, we are going to show some examples where these kind of properties can be proved rigorously for machine learning situations. But these examples are pretty simple. They are not deep learning. So these are still roughly the best that people can do in some sense. in machine learning, like especially if you think about nonlinear cases. And now, still I think it's used in the recommendation system. OK, cool. So any questions so far? I guess let's talk about PCA first. So we are assuming that we are given a matrix M in dimension d by d. And we want to find the best rank one case. So in this case, the bestrank one approximation is basically the eigen vector times eigenvector transpose up to some scaling. So how do we prove this? So as you can imagine, the proof is pretty simple. The plan is very simple. You first find out all stationary point, the first order stationary points. And then you prove that they are all global minimum. So basically, it's just more or less like we solve all of these equations and see what are the possible local minimum you can have, right? So let's firstly use the stationary points, a gradient condition. So gradient of g of x, I'm not going to give a detailed calculation here. This is equal to minus this times x. The methodology also applies when you talk about the Hessian. So the methodology, I'm not going to go to all the details. But roughly speaking, what you do is the following. So g of x plus some linear term in epsilon plus some quadratic term and so on and so forth, past the higher order term. And then if you have this, then this basically corresponds to something like v dot g square gxv. And so this is a very simple way to compute the Hessians. top one eigenvalue-- eigen vector with the right scaling. So the second case is that x has eigen value, let's say, lambda, which is strictly less than Lambda 1. And then because x is an eigenvector and also the eigen Value of x is orthogonal to v1, then x1 is Orthogonal. There is no guarantee that two eigenvectors are always orthogona because they could have the same eigenvalues and they are just in the same subspace. But if they have different eigen values, then they have to be orthogonial. Theory: P omega of A is the matrix obtained by zeroing out every entry outside omega. Top left corner is 1, so there is no way you can recover this matrix unless you observe that top left corner. Theory: If there's no other structure in this matrix M, there isNo way you. can recover the other entries because they can be arbitrary. Theorem: If p is something like poly mu and log d over d epsilon, then p omega of M is basically a random subset. by the users. Every user probably have an opinion about every item, right? Either they like it or not, so and so forth. So every user only buys a very small subset of the item. And that's why you only see some entries in this matrix. Amazon only sees some of the entries. And Amazon wants to understand what each user's preference is, want to know that each user likes which item. So the Amazon has an incentive to just fill in the entire table. it's unlikely it can work. So basically, that is saying that p is bigger than roughly 1 over d. And speaking of the objective functions, this is actually a pretty commonly used method in practice. So you just say I'm going to minimize this function that's called fx, which is defined to be that basically you have a parameterization called xx transpose. And you want to say this matrix actually faced all my observations, right? So you are taking a sum over all possible observed entries because these are the only cases you know what the entries are. no local minimum, all local minimum are global. So the infinity norm is less than mu over square root d. A mu is considered a constant or logarithmic in d. So what it's saying is that, this factor z, the norm is 1. And also, the entries are spread out. You cannot just have all the mass concentrated on one entry. So this is called incoherence assumption. And this assumption is necessary. People know it. So I guess we assume, for example-- first of all, we assume the ground truth has norm 1. assume that the input are linearly separable, then there is a proof for this. And there are a bunch of other cases where you can have some partial results. Next week, in the next lecture, maybe the second half of next lecture,. I'm also going to give another result, which is somewhat more general. It applies to many different architectures, but it has other kind of constraints. First of all, it doesn't really show exactly these kind of landscape properties. It shows that these kinds of properties holds for a region, for a special region in the parameter space.