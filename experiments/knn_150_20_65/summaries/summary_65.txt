Peter Solovits: I got a call from a committee of the National Academy of Science, Engineering, and Medicine. The committee is chaired by David Baltimore, who used to be an MIT professor until he went and became president of Caltech. Solovitz: They convened a meeting to talk about the set of topics that I've listed here. He says the group of us that talked about AI and decision making, I was a little bit surprised by the focus because Hank really is a law school professor. Trevor Noah: Algorithmic technologies may minimize harms that are the products of human judgment. Noah: We know that people are in fact prejudiced, and so there are prejudices by judges and by juries that play into the decisions made in the legal system. He says we should look for people with broad educations, like Trevor's father is white and his mother is African-American and his father is a Swiss guy. Trevor: What do you like about being able to defend a marriage that they were not allowed to marry six years ago? In California, the decision of whether you get bail or not is going to be made by a computer algorithm, not by a human being. The critique of these bail algorithms is based on a number of different factors. The data collection system is flawed in the same way as the judicial system itself, Peter says. Irene: If we're going to define the concept, what is fair? What characteristics would you like to have an algorithm have that judges you for some particular purpose? Yeah? When I was an undergraduate at Caltech, the Caltech faculty decided that they wanted to include student members of all the faculty committees. In those days, Caltech only took about 220, 230 students a year. So one day, one of the professors said here's what we ought to do. We ought to take the 230 people that we've just offered admission to and we should reject them all and take the next 230 people, and then see whether the faculty notices.look like they're a better bet. Peter Zolovits: People who are similar should be treated similarly. He says it's hard to define a universal notion of what it means to discriminate. Disparate treatment is about procedural fairness and equality, he says. Zolvits: There's plenty of evidence in literature that keeps persisting discrimination that keeps a lot of people out of the job. The question is can we change our hiring policies or whatever policies we're using in order to achieve the same goals, but with less of a disparity in the impact? In the US, there were tests of this sort done, but the problem was that a lot of African and African-American populations turned out to have this genetic variant frequently without developing this terrible disease. And it was only after years when people noticed that these people who were supposed to die genetically weren't dying that they said, maybe we misunderstood something. And what they misunderstood was that the population that was used to develop the model was a European ancestry population and not an African ancestry population. So you go, well, we must have learned that lesson. The study looked at how machine learning models can identify disparities in general medical and mental health. It found a racial bias in the data that we have and in the models that we're building. In psychiatry, when you look at the comparison populations, you see a fair amount of overlap. The models are not going to give us as accurate predictions, but you still see, still, a huge gap in the confidence intervals between them. The study was published in the American Medical Association's Journal of Ethics, which I didn't know existed. is choose some family of models to try to fit, and then I'm going to use some fitting technique, like stochastic gradient descent or something, that will find maybe a global optimum, but maybe not. And then there is noise. And so his observation is that if you count O as the optimal possible model over all possible model families, then the bias is essentially O minus L. The variance is like L minus A, it's the error that's due to the particular way in which you learned things. He had to pretend to be-- his mother was his caretaker rather than his mother in order to be able to go out in public, because otherwise, they would get arrested. So here are some of the legally recognized protected classes, race, color, sex, religion, national origin, citizenship, age, pregnancy, familial status, disability, veteran status, and more recently, sexual orientation. So if you want to create a stereotype, men are druggies and women are depressed, according to this data. scoring function is independent of the protected attribute. So that allows a little more wiggle room because it says that the protected. attribute can still predict something about the outcome, it's just that. you can't use it in the scoring function given the category of which. outcome category that individual belongs to. And then sufficiency is the inverse of that. It says that given the scoring. function, the outcome isIndependent of theprotected attribute. And so that says, can we build a fair scoring function that separates the outcome from the. protected attribute? Hiring is based on a good score in group A, but random in B? So for example, what if we know a lot more information about group A than we do about group B? Well, the outcomes are likely to be better for a group A rather than for group B. So it's a kind of nice graphical hack. Again, it'll be on the slides, and I urge you to check that out, but I'm not going to have time to go into it. does in other populations, and the FDA has actually approved the marketing of that drug to those subpopulations. And if you think about the personalized medicine idea, which we've talked about earlier, the populations that we're interested in becomes smaller and smaller until it may just be you. And so there might be a drug that works for you and not for anybody else in the class. But it's exactly the right drug for you, and we may get to the point where that will happen and where we can build such drugs. degree of calibration will give you a good approximation to this notion of sufficiency. These guys in the tutorial also point out that some data sets actually lead to good calibration without even trying very hard. So for example, this is the UCI census data set, and it's a binary prediction of whether somebody makes more than $50,000 a year if you have any income at all and if you're over 16 years old. It's almost exactly along the 45 degree line without having done anything particularly dramatic in order to achieve that. The probability that you visited the Grace Hopper Conference is dependent on your gender. Computer scientists are much more likely to be programmers than non-computer science majors. The optimal score is going to depend basically on whether you have a computer science degree or not. If you're a historian, you're not likely to have been interested in going to that conference. It's a really cool conference. Grace Murray Hopper invented the notion bug or the term bug and was a really famous computer scientist starting back in the 1940s. We used LDA, standard topic modeling framework. White patients have more topics that are enriched for anxiety and chronic pain. Black, Hispanic, and Asian patients had higher topic enrichment for psychosis. It's interesting. Male patients had more substance abuse problems. And so we said, what happens when you look at the different topics, how often the different topic arise in different subpopulations? And so what we found is that, for example, white patients haveMore topics enriched for Anxiety and Chronic pain. Public insurance patients often have multiple chronic conditions. Public insurance patients have atrial fibrillation, pacemakers, dialysis. Private insurance patients has higher topic enrichment values for fractures. The error rates on a zero-one loss metric are much lower for men than work here and embarrassing myself. So this is modeling mistrust in end-of-life care, and it's based on Willie's master's thesis and on some papers that came as a result of that. It could be any of a lot of different factors, but that's the case. or social differences, but to a difference in the degree of trust between the patient and their doctors? It's an interesting idea. And of course, I wouldn't be telling you about this if the answer were no. So there are red flags if you read the notes. For example, if a patient leaves the hospital against medical advice, that is a pretty good indication that they don't trust the medical system. If a person is in pain, that correlated with these mistrust measures as well.