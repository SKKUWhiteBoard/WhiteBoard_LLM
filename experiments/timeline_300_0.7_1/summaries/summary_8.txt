The final contest, which is due tonight, is to design an agent that plays together with another agent to try to collect food pellets while not getting eaten by ghosts. Submissions for that, your last chance to submit are tonight at midnight. And on Thursday in lecture, we'll discuss the results. The idea behind these two lectures is to look at advanced applications. We will not quiz you on these application lectures, on the final exam, or anything like that. It's more meant to give you more perspective rather than extra study materials. Search is very prevalent in language processing, and we'll cover some of that on Thursday. Adversarial games. Learning to acquire behaviors. We'll look at some real robot behaviors in flight and in legged locomotion. And dealing with uncertainty. So AlphaGo. Today's state-of-the-art in Go is that there are computer players better than the best human players. But actually, if you went back to March 2016, that was not the case yet. So how do you make an AI for Go? For a game like Tic-Tac-Toe, you will find out that you can force a draw, and that means fully solving the game. For Go, this is actually pretty hard to do, and it's even much harder than chess. Let's take a look at chess. The branching factor is enormous. So if you tried to run an exhaustive search through this kind of game tree, it's not going to work. And the ideal expansion scenario can reduce the size of the tree by square rooting it. Still too much. to think two deep instead of all the way till the end of the game. After two deep, you look at the evaluation function and it might be good enough. But many of the moves are not that useful. And so policy network can be trained, which assigns probabilities to all possible moves. And then you might only consider the moves with high probability when running your search, or you might just sample from that distribution to run your search. For AlphaGo, there was a bunch of data collected from human play. Then a supervised learning was run, so a deep neural network was trained. DeepMind's AlphaGo is a computer program that can learn to play Go without human input. It uses deep neural networks and tree search to learn how to play the game. AlphaGo Zero, which learns at a level beyond human level, was used to defeat the strongest player in the world at Go. The program is now being used to learn chess, Shogi, and other board games, as well as other complex games such as chess and checkers. For more on the AlphaGo program, visit the DeepMind website. AlphaGo Zero has no prior knowledge. It starts at a pretty, actually, negative elo rating. Loses all the time initially. It's playing itself, but then gets tested against a set of players. After 21 days, it goes past where AlphaGo Lee Sedol was. And then it was still creeping up after 40 days. It was still climbing, but let me see if I can pull something up that might showcase it. OK, so here's AlphaGo Zero. The cursor's somewhere here, yep. knows, from every situation, who's guaranteed to win or whether it's guaranteed. to be a draw, and then knows how to play that. Once you reach that level, essentially, there's no further to go, because you solved the game. With reasonable compute power, it traverses the whole tree. Even with alpha beta pruning, I don't think that'll happen anytime soon. It's a really big tree. And that'll add up to a very large number. If you do a more kind of brute force style approach-- not 100% brute force. even longer. It could be that by using human knowledge, you're in some kind of based enough attraction. I don't know if that would be the case or not, but that's a possibility. It also depends on how much randomness you have in your exploration. If you have enough randomness, then initialization will have much less effect than if you have limited randomness. Is it even possible to learn good Go players by just playing against yourself? That's something people did not have an answer. CNN's John Defterios: How do you get a helicopter to do something like this? He says it's hard to stabilize a helicopter when you only know where you are up to a couple meters. So how do we track the helicopter? None of these measurements are super precise, he says. Defterio: We set up a hidden Markov model, where we considered the state unknown, then the three angles, the velocity, and the angle rates. And the measurements are the measurements I just described. in angle from back and left right as the blade goes around. So that way, you can generate a torque that allows your helicopter to roll or pitch based on how much differential thrust you have from back, left to right. The tail rotor has a variable pitch also, and that pitch allows you to modulate how much thrust you get from the tail rotor. So you can build a model for this from collecting data from your helicopter, let's say, and learning a bunch of parameters that predict the next state. A helicopter can learn to flip and repeat by following a trajectory. But in the real world, the helicopter deviates from the path it learned in a simulator. The helicopter doesn't know how to control around that path. It started overcompensating. That's why you get a half flip, half spin and repeat. It's not easy. Exactly how much do you push up? How fast do you spin? Difficult things to decide. A few years ago as a PhD student, I tried to design this kind of thing. saw it making these wild motions, overcompensating. It pushed the controls so hard that the engine died. The engine just couldn't push it, died. At that moment, the blades stopped spinning, or they slowed down. Then what happened is our human pilot took back control to try to save the helicopter. And believe it or not, they actually saved this. It landed a little harder than you want to land, but it landed on its feet and it could be recovered from that, which was pretty crazy. that point, was much better than our autonomous controller. So we started looking at some human pilots. We said, well, clearly, the issue was that we asked the helicopter to follow a path that's not flyable. But it turns out, when we collect paths from a human pilot, they tend to be noisy. So how do we, from, this get a specification of what we should be flying? Well, we could learn the trajectory from these as noisy observations. What methodology do we have for that? Hidden Markov models. have something we don't know that evolves over time, but we have some noisy measurements of it, we can run an HMM to recover what we actually want. We use something called dynamic time warping. So what does that do? You can align two trajectories. Let's say you have many demonstrations. Then you have a hit injectory, which is just initialized with one of them. Then, you run dynamic time Warping, which aligns each trajectory with the hidden one-- which was just a guess. We have collected data to learn a dynamics model for the helicopter. We can now penalize our award, penalized for deviating from the target. And then we can run reinforcement learning in simulation, let's say, in this learn simulator to find a good controller and run it on the real helicopter. The controller we learn in simulation is still a little optimistic about really following that path. So while we fly the helicopter, we'll do depth-limited search to improve what we have. able to look ahead only two seconds, rather than needing to look further. A value function tells us, OK, how good is it to end up here? We also have a reward at each time tick. And our search over those two seconds is what results in the control we apply. The fastest we flew this helicopter was close to 55 miles per hour. The algorithm's only this big, so it's pretty fast for something of this size. And then now are actually some of the hardest maneuvers to execute on. Professor: With this methodology, it was possible to fly this helicopter at the level of the best human pilots. Professor: The hardest maneuver in this air show, the reason this is so hard, is that you're maneuvering in airflow. We did not push that further to flying those maneuvers. There is some work. If you look at Woody, Woody was shutting things off. He was able to have it learn to hover reliably with the only human input being shut it off when it looks like it might start doing something dangerous. at OpenEye, there's been some work on robots learning to do back flips. And that was kind of one step further. It wasn't just shutting it off. You would watch your robot try things. And human input would be not specifying a reward function, which is very hard to do for things like back flips, just like it was hard here. But what they did is they said, the human watches it and says, which one is better or worse among a set of them? And then from that, it learns a Reward function. The helicopter uses roughly a fixed amount of fuel, anyway, per time. So it's more that it has less weight to carry as it has used more fuel. This helicopter had inverted slide, where it has more power, 3 Gs. It can generate three times the power of gravity. OK, let's take a short break here. And after the break,. let's do legged locomotion and manipulation. All right, let't restart. Any questions about the first half? Yes? is a separate linear feedback controller for each time slice. So essentially, value iteration, but in a continuous space. If there is no wind, you can actually just run the linear feedback control. It will be fine. But if there's some wind gusts that could throw you off, you want to use the value functions and the two second look ahead. If you have a locally linear dynamics and quadratic reward function, it turns out that's the one continuous state action space scenario where you can run exact value iteration. against those value functions to do the controls. Can it be done with one unified network? Likely, it can. It might take some work, exactly, figuring out how to do it. Here's an example of how hard this can be. This is a video from 2015, there was the Doppler Robotics Challenge, which was held in Pomona, just east of Los Angeles. It's called the "Doppler Challenge" and it's a competition for roboticists. It was won by a team from the University of California, San Diego. of Los Angeles. People had two years to work on this. And what did the robot have to do? It had to, essentially, drive a car or walk, but driving the car was recommended. Then get out of the car, walk a little bit, open a door, grab a drill, drill a hole, walk some more. So doesn't sound that complicated. But actually, it turned it's very complex to get a robot to do that. And it's indicative of how hard it is to do walking with robots. build a model of the world. The thing is modeling these situations proved even harder than modeling helicopters, because your sensing needs to understand whether or not you're already making contact, and making contact or not. Now, what's changed recently in the past few years is that through advances in deep learning, it's been possible to better map from raw sensory information to controls you might want to apply. And so actually, you've seen this video before. It's in simulation, but this robot is, over time, learning to control itself to do walking. And ultimately, it learns to walk. The reinforcement learning algorithm can be reused directly onto other robots and can learn to control these other robots. The reward function is the closer your head is to standing head height, the better. This can work directly for building video games. You build video games, you want your main character to move in a realistic way. You can have it sequence together motions like these and dynamically simulate how they interact with the world, rather than key frame every little detail. But it shows, in simulation, at least, a lot of progress towards learning intelligent locomotion behaviors. There are two levels of control: low level and high level. Low level control is about placing your feet in the right next position. High level control problem is actually a star search. If you have a cost function for this terrain, what would the cost function be? Well, you maybe don't want to be next to a big cliff. Because if you're right next to it, then you slip a little bit, you'll slide your foot down and that's not good. So there's a bunch of considerations you might have. When you run the search, or the value iteration, which is, more or less, equivalent to find a path across this terrain. But if you choose the trade-off between the features differently, you'll find different paths. So reward learning. How you do reward learning, you demonstrate a path. Demonstrating doesn't mean just drawing a line. It actually means choosing a sequence of footsteps that it executes on, and assuming it does well. It assumes you have a low level controller, but that's well understood how to do that. Self-driving cars take part in Dartmouth Grand Challenge. First time autonomous vehicles have been used in a race. No robot has ever gone more than three miles in the race. The winner of the race will be crowned the winner of this year's MIT Robotics Competition. The final race will take place in May. The winners will be announced in May 2015. The prize money will be $1 million, with the winner taking home $500,000. The competition will be held every year until the winner is crowned. Stanley is about to become the first vehicle in history to drive 132 miles by itself. Cameras, radar, control screen, steering motor. Lasers, where you shoot out laser beams. The steering control, usually, you would have a high level planner choosing a path and then a low level controller that is good at following that path. And based on how long it takes them to get back, you know how far away the nearest obstacle is in that direction, assuming it's an obstacle that reflects back to where the laser came from. With a camera, you can often look further ahead. LIDAR sends out a laser beam, measures how long it takes to get back. With HMM, you get 0.02% false positives of where there might be obstacles. An HMM can de-noise those readings into a more reliable estimate of the geometry of the world around you. It would see that the readings are different and decide it needs to steer around that.an obstacle. If you're an urban environment, there'll be a lot more obstacles. A camera will be better at that than a LIDar. Self-supervision is a trick that's very widely used to reduce labeling efforts. So the camera now knows all the red here is road. In urban environments, there's even more need to recognize, not just road versus not road. A lot of progress has been made this is video from 2013. This was before deep neural networks were heavily used for this kind of thing. It's only getting better to recognize what's in scenes, thanks toDeep neural networks. Instead of classifying into which categories in the image, you would classify each pixel, as to what is in each pixel. tail of special events that can happen when you're driving. You can measure progress by just demo videos, which is one way, and it gives you some kind of feel for what's going on. Another way to measure progress is to see how are these cars doing relative to human drivers. If you test in California, you have to report this data to the DMV to see if your car is doing well or not. It's a number of events per 1,000 miles driven. Red there is human fatalities. Then yellow is human injuries. In green is the Google slash [? wave ?] mode disengagement. doing this. Early on, they were the only ones, as far as I know, but there's more companies now. And you can look at them on these plots and see which companies, how far along, in terms of how many disengagements they need per 1,000 miles driven. Still, so far, quite far removed from human accident rate levels. Another thing people have been pushing, as a consequence of all this, is lower power neural networks. So for example, Kurt Kuetzer at Berkeley has projects on this called SqueezeNet. If they're gigantic, use a lot of power. That's a problem. Let's see what we can do to build smaller networks to make decisions.