This week's lecture will focus on machine translation related topics. In the second half of the week, we take a break from learning more and more on neural network topics. We'll talk about final projects, but also some practical tips for building neural network systems. This is an important content full lecture. You can download the full lecture for free on the iReport app, which is free to download from the iTunes App Store and the Google Play Store. For confidential support, call the Samaritans on 08457 90 90 90, visit a local Samaritans branch or click here for details.  assignment 3 is due today, assignment 4 is out today. Today's lecture is the primary content for what you'll be using for building your assignment 4 systems. For assignment 4, we give you a mighty two extra days. So you get nine days for it. And it's due on Thursday. On the other hand, do please be aware that assignments 4 is bigger and harder than the previous assignments. So do make sure you get started on it early. And then as I mentioned Thursday I'll turn to final projects. In the early 1950s, there started to be work on machine translation. Machine translation is the task of translating a sentence x from one language to another language. So we start off with a source language sentence x. L'homme, and then we translate it and we get out the translation man is born free, but everywhere he is in chains. OK. So there's our machine translation, and now let's get to the neural machine translation part of the story. Get straight into this with machine translation and we'll go through the prehistory of machine translation in a bit. This video clip shows some of the earliest work in machine translation from 1954. One of the first non-numerical applications of computers. $500,000 simple calculator, most versatile electronic brain known, translates Russian into English. Instead of mathematical wizardry, a sentence in Russian, it could be -- instead of Mathematical wizardry,. It could be-- instead of mathematical Wizardry. It was hyped as a way of keeping tabs on what the Russians were doing during the Cold War. The computer will be able to do about with a modern commercial computer about one to two million words an hour. This will be quite an adequate speed to cope with the whole output of the Soviet Union in just a few hours of computer time a week. When do you hope to achieve this speed? I our experiments go well, then perhaps within five years or so. And finally, Mr. McDaniel, does this mean the end of human translators? I say yes for translators of scientific and technical material. But as regards to poetry and novels, no, Despite the hype it ran into deep trouble. So the experiments did not go well. And so in retrospect, it's not very surprising that the early work did not work out very well. I mean, this was in the sort of really beginning of the computer age in the 1950s. That it was also the beginning of people starting to understand the science of human languages, the field of linguistics. So really people had not much understanding of either side of what was happening. So what you had was people were trying to write systems on really incredibly primitive computers, right? using to translate. And so effectively, what you were getting were very simple rule based systems and word lookup. So it was sort like, dictionary look up a word and get its translation. But that just didn't work well. Because human languages are much more complex than that. Often words have many meanings and different senses as we've sort of discussed about a bit. Often there are idioms. You need to understand the grammar to rewrite the sentences. So for all sorts of reasons, it didn't working well. And this idea was largely canned. when they were in the period of statistical NLP that we've seen in other places in the course. And then the idea began, can we start with just data about translation i.e. sentences and their translations, and learn a probabilistic model that can predict the translations of fresh sentences? So suppose we're translating French into English. We can say, what's the probability of different English translations? And then we'll choose the most likely translation. And it's not immediately obvious as to why this should be because this is sort of just a trivial rewrite with Bayes' rule. The Rosetta Stone allowed the decoding of Egyptian hieroglyphs because it had the same piece of text in different languages. In the modern world, there are fortunately for people who build natural language processing systems quite a few places, where parallel data is produced in large quantities. The European Union produces a huge amount of parallel text across European languages. The Canadian Parliament conveniently produces parallel text between French and English, and even a limited amount in Inuktitut, Canadian Eskimo. And then the Hong Kong parliament produces English and Chinese. So there's a fair availability from different sources. And we can use that to build models. So how do we do it though? All we have is these sentences. And it's not quite obvious how to build a probabilistic model out of those. Well, as before, what we want to do is break this problem down. So in this case, what is an alignment variable. So a is the alignment variable, which is going to give a word level or sometimes phrase level correspondence between parts of the source sentence and the target sentence. is working out the correspondence between words that is capturing the grammatical differences between languages. So words will occur in different orders in different languages depending on whether it's a language that puts on the subject before the verb, or the subject after the verb. And the alignments will also capture something about differences about the ways that work languages do things. So you can have words that don't get translated at all in the other language. So in French, you put a definite article "the" before country names like Japon. So when that gets translated to English, you just get Japan. One French word gets translated as several English words. You can get the reverse, where you can have several French words that get translated as one English word. So here we sort of have four English words being translated as two French words. But they don't really break down and translate each other well. These things don't only happen across languages. They also happen within the language when you have different ways of saying the same thing. So another way you might have expressed the poor don't have any money is to say the poor are moneyless. That's much more similar to how the French is being rendered here. English to English, you have the same kind of alignment problem. So probabilistic or statistical machine translation is more commonly known. What we wanted to do is learn these alignments. And there's a bunch of sources of information you could use. If you start with parallel sentences, you can see how often words and phrases co-occur in parallel sentences. You can look at their positions in the sentence. And figure out what are good alignments for you. But alignments are a categorical thing. And so you need to use special learning algorithms like the expectation maximization algorithm. CS228 is a statistical machine translation system. It picks out the most likely why there's the translation of the sentence. The naive thing is to say, well, let's just enumerate every possible y and calculate its probability. But we can't possibly do that because there's a number of translation sentences in the target language. That's exponential in the length of the sentences. So we need to have some way to break it down more. See CS228.off and see CS228 if you want to know more about that. We had a simple way for language models, we just generated words one at a time and laid out the sentence. But here we need to deal with the fact that things occur in different orders in source languages and in translations. And so we do want to break it into pieces with an independence assumption like the language model. But then we want a way of breaking things apart and exploring it in what's called a decoding process. So we start with a source sentence. So this is a German sentence. And as is standard in German. So that's probably not in the right position for where the English translation is going to be. we have is based on the translation model. We have words or phrases that are reasonably likely translations of each German word, or sometimes a German phrase. And so then inside that, making use of this data, we're going to generate the translation piece by piece kind of like we did with our neural language models. So there's a search process. But one of the possible pieces is we could translate "er" with "he", or we could start the sentence with "are" translating the second word. And in the process, I'll go through in more detail later when we do the neural equivalent. In the period from about 1997 to around 2013, statistical machine translation was a huge research field. The best systems were extremely complex. And they had hundreds of details that I certainly haven't mentioned here. So Google Translate launched in the mid 2000s. And people thought wow, this is amazing. You could start to get sort of semi-decent automatic translations for different web pages. But that was chugging along well enough. And then we got Google. Translate, which is now one of the most popular online translation tools. Neural machine translation, well, it means you're using a neural network to do machine translation. But in practice, it's meant slightly more than that. It has meant that we're going to build one very large neural network, which completely does translation end to end. These neural network architectures are called sequence to sequence models. And they involve two neural networks. Here it says two RNNs. The version I'm presenting now has two Rnns. And it's a language model that's going to generate a target sentence conditioned on the final hidden state of the encoder. "he." And so then doing LSTM generation just like last class, we copy that down as the next input. And we've translated the sentence, right? So this is showing the test time behavior when we're generating the next sentence. For the training time behavior, when we have parallel sentences, we're still using the same kind of sequence to sequence model. But we're doing it with the decoder part just like training a language model, where we're wanting to do teacher forcing and predict each word. Everywhere else as well. So you can do summarization. You can think of text summarization as translating a long text into a short text. But you can use them for other things that are in no way a translation whatsoever. So they're commonly used for neural dialogue systems. So the encoder will encode the previous two utterances, say. And then you will use the decoder to generate a next utterance. Some other uses are even freakier but have proven to be quite successful. So if you have any way of representing the parse of a sentence as a string. arc, right arc, shifts like the transition system that you used for assignment 3. Feed the input sentence to the encoder and let it output the transition sequence of our dependency parser. These models have also been applied not just to natural languages, but to other kinds of languages, including music, and also programming language code. So you can train a seq2seq system, where it reads in pseudocode in natural language, and it generates out Python code. And if you have a good enough one, it can do the assignment for you. was just to start at the beginning of the sentence and generate a sentence based on nothing. But here we have something that is going to determine or partially determine. And that's going to strongly determine what is a good translation. So in neural machine translation we are directly calculating this conditional model probability of target language sentence given source language sentence. And so at at the end of the talk, we will reveal the results of our machine translation project, which will be shown in the next few weeks. each step, as we break down the word by word generation, that we're conditioning not only on previous words of the target language, but also each time on our source language sentence x. Because of this, we actually know a ton more about what our sentence that we generate should be. So if you look at the perplexities of these kind of conditional language models, you will find them like the numbers I showed last time. They usually have almost freakily low perplexities, that you will have models with perplexities that are something like 4 or even less. Both of those in a bit more detail. So the first step is we get a large parallel corpus. And we grab a lot of parallel English French data from the European parliament proceedings. So then once we have our parallel sentences, what we're going to do is take batches of source sentences and target sentences. We'll encode the source sentence with our encoder LSTM, and feed its final hidden state into a target L STM. And this one, we are now then going to train word by word by comparing what it predicts is the most likely word to be produced, versus what the actual first word, and then the actual second word is. generating the correct next word "he" and so on along the sentence. And the crucial thing about these sequence to sequence models that has made them extremely successful in practice is that the entire thing is optimized as a single system end to end. So starting with our final loss, we backpropagate it right through the system. So we not only update all the parameters of the decoder model, but we also update all of the parameters in the encoder model which in turn will influence what happens next. conditioning gets passed over from the encoder to the decoder. So this moment is a good moment for me to return to the three slides that I skipped. I'm running out of time at the end of last time, which is to mention multilayer RNNs. And having a multilayers RNN allows us the network to compute more complex representations. So simply put the lower Rnns tend to compute lower level features, and the higher RNN's should compute higher. level features. And just like in other neural networks, whether it's feed forward networks, or the kind of networks you see in vision systems, you get much greater power and success by having a stack on multiple layers of recurrent neural networks. And multilayer or stacked RNNs are more powerful. Can I ask you, there's a good student question here? What would lower level versus higher level features mean in this context? Sure. So I mean, in some sense, these are lower level features. are somewhat flimsy terms. The meaning isn't precise. But typically, what that's meaning is that lower level features and knowing sort of more basic things about words and phrases. So that commonly might be things like what part of speech is this word, or are these words the name of a person, or a company? Whereas higher level features refer to things that are at a higher semantic level. So knowing more about the overall structure of a sentence, knowing something about what it means, whether a phrase has positive or negative connotations. systems just don't work well. But you can build something that is no more complex than the model that I've just explained now. That does work pretty well by making it a multi-layer stacked LSTM neural machine translation system. And so our representation of the source sentence from our encoder is then this stack of three hidden layers, whoops. And then that we use to then feed in as the initial, as theinitial hidden layer into then sort of generating translations, or for training the model. become much less clear. Normally to do deeper LSTM models and get even better results. You have to be adding extra skip connections of the kind that I talked about at the very end of the last class. Next week, John is going to talk about transformer based networks. They're typically much deeper. But we'll leave discussing them until we get on further. So that was how we train the model. So let's just go a bit further and look at the data. So that we have our LSTM, we start, generate a hidden state. It has a probability distribution over words. And you choose the most probable one the argmax, and you say "he", and you copy it down and you repeat over. So doing this is referred to as greedy decoding. Taking the most likely word on each step. And it's sort of the obvious thing to do, and doesn't seem like it could be a bad thing toDo. But it turns out that it actually can be a fairly problematic thing todo. stuck with it. And you have no way to undo decisions. So if these examples have been using this sentence about, he hit me with a pie going from translating from French to English. But once you've generated it, there's no ways to go backwards. And so you just have to keep on going from there and you may not be able to generate the next word. And there are lots of reasons it could think so. Because after hit most commonly, there is a direct object now and then he hit a car, right? So that sounds pretty likely. translation you want. At best you can generate, he hit a pie, or something. And well, what could we do? Well, I sort of mentioned this before looking at the statistical empty models. Overall, what we'd like to do is find translations that maximize the probability of y given x, and at least if we know what the length of that translation is. And that's where that then requires generating an exponential number of translations. And it's far, far,far,far away. Far too expensive. So beyond greedy decoding, the most important method is something called beam search decoding. beam search's idea is that you're going to keep some hypotheses to make it more likely that you'll find a good generation while keeping the search tractable. So what we do is choose a beam size. And for neural MT, the beam size is normally around 1.5 to 2.5 times. It's not the only other decoding method. Once when we got on to the language generation class, we'll see a couple more. fairly small, something like 5 to 10. And at each step of the decoder, we're going to keep track of the k most probable partial translation. So what we want to do is search for high probability hypotheses. So this is a heuristic method. It's not guaranteed to find the highest probability decoding. But at least, it gives you more of a shot than simply doing greedy decoding. So let's go through the typical way using our conditional language model. So as written all of the scores are negative. And so the least negative one, i.e., thehighest probability one is the best one. an example to see how it works. So in this case, so I can fit it on a slide. The size of our beam is just 2. Though normally, it would actually be a bit bigger than that. And the blue numbers are the scores of the prefixes. So these are these log probabilities of a prefix. So we start off with our start symbol. And we're going to say, OK. What are the two most likely words, to generate first according to our language model? I was, I got. OK. So we have four partial hypotheses. We work out the scores of each of them. And then we can say, which of those two partial hypotheses? Because our beam size, k equals 2, have the highest score? And so they are, I was, and he hit. We keep those two and ignore the rest. And so then for those two, we are going to generate k hypotheses for the next word. And we can do that by taking the previous score that we have the partial hypothesis and adding on the log probability. the most likely following word. He struck me and I was, I don't know, he struck me. And he hit a. So we keep just those ones. And then for each of those, we generate the k most likely next words tart, pie, with, on. Then again, we filter back down to size k by saying, OK, the two most likely things here are pie or with. And at this point, we would generate end of string. And say, OK,. We've got a complete hypothesis. then trace back through the tree to obtain the full hypothesis for this sentence. So that's most of the algorithm. There's one more detail, which is the stopping criterion. So in greedy decoding, we usually decode until the model produces an end token. And when it produces the end token, we say we are done. In beam search decoding, different hypotheses may produce end tokens on different time steps. And so we don't want to stop as soon as one path through the search tree has generated end. n complete hypotheses. And then we'll look through the hypotheses that we've completed and say which is the best one of those. And that's the one we'll use. OK. So at that point, we have our list of completed hypotheses and we want to select the top one with the highest score. Well, that's exactly what we've been computing. But it turns out that we might not want to use that just so naively. Because that turns out to be a kind of a systematic problem, which is not as a theorem. In a newspaper, the median length of sentences is over 20. So you wouldn't want to be having a decoding model when translating news articles that says, huh, just generate two word sentences. They're just way high probability according to my language model. So the commonest way of dealing with that is that we normalize by length. So if we're working in log probabilities, that means taking dividing through by the length of the sentence. And then you have a per word log probability score. you can argue that this isn't quite right. In some theoretical sense, but in practice it works pretty well and it's very commonly used. Neural translation has proven to be much, much better. It has many advantages. It gives better performance. The translations are better. In particular, they're more fluent because neural language models produce much more fluent sentences. But also, they much better use context because neural. language models give us a very good way of conditioning on a lot of contexts. all parameters of the model end to end in a single large neural network has just proved to be a really powerful idea. The models are also actually great in other ways. They actually require much less human effort to build. There's no feature engineering. There're in general, no language specific components. You're using the same method for all language pairs. Of course, it's rare for things like this to happen, but it's not unheard of. We'll come back to the costs of that later in the course. Neural machine translation systems also have some disadvantages compared to the older statistical machine translation system. They're less interpretable. So they're hard to debug. They also tend to be sort of difficult to control. So there are various safety concerns. But at the end of the day, BLEU gives a score between 0 and 100 where your score is 100. If you are exactly producing one of the human written translations, and 0 if there's not even a single unigram that overlaps between the two. Machine translation with statistical models had been going on since the mid 2000s decade. But by the time you entered the 2010s, basically progress in statistical machine translation had stalled. And you were getting barely any increase over time. Most of the increase you weregetting over time was simply because you're training your models on more data. In those years, around the early 2010s,. the big hope that most people had was that machine translation was going to get better and better. And it didn't. In 2014, the first modern attempt to build a neural network from machine translations and encoded-decoder model. Within two years' time, Google had switched to using neural machine translation for most languages. Does that mean that machine translation is solved? No. There are still lots of difficulties which people continue to work on very actively. But there are lots of problems with out of vocabulary words. And domain mismatches between the training and test data. And hopefully, you'll even get a sense of this doing assignment 4. Even our best multilayer LSTMs aren't that great of capturing sentence meaning. There are particular problems such as interpreting what pronouns refer to. For languages that have lots of inflectional forms, these systems often get them wrong. So here's just sort of quick funny examples of the kind of things that go wrong, right? So if you asked to translate paper jam. Google Translate is deciding that this is a kind of jam just like this. And so this becomes a jam of paper. Many languages don't distinguish between things masculine or feminine. When that gets translated into English by Google Translate is that the English language model just kicks in and applies stereotypical biases. So if you want to help solve this problem, all of you can help by using singular they in all contexts when you're putting material online. And that could then change the distribution of what's generated. And people also work on modeling improvements to try and avoid this. Here's one more example that's kind of funny. People noticed a couple of years ago. That if you choose one of the rarer languages that Google will translate, that the gender neutral sentences get translated into, she works as a nurse. such as Somali, and you just write in some rubbish like ag ag ag. Freakily, it had produced out of nowhere prophetic and biblical texts, as the name of the Lord was written in the Hebrew language. As far as I can see, this problem is now fixed in 2021. So there are lots of ways to keep on doing research. NMT certainly is a flagship task for NLP and deep learning. And it was a place where many of the innovations of deep learning NLP were pioneered, and people continue to work hard on it. For assignment 4 this year, we've decided to do Cherokee English machine translation. Cherokee is an endangered Native American language that has about 2000 fluent speakers. And particularly, there's not a lot of parallel sentences between Cherokee and English. And here's the answer to Google's freaky prophetic translations.actually for the last bit of the class and the minute I'm going to present one huge improvement, which is so important that it's really come to dominate the whole of the recent field of neural networks for NLP. And that's the idea of attention.  Cherokee is not a language that Google offers on Google Translate. So we can see how far we can get. But we have to be modest in our expectations because it's hard to build a very good MT system with only a fairly limited amount of data. There is a flipside, which is for you students doing the assignment. The advantage of having not too much data is that your models will train relatively quickly. We'll actually have less trouble than we did last year with people's models taking hours to train as the assignment deadline closed in. Most Cherokee now live in Oklahoma. There are some that are in North Carolina. The writing system that I showed on this previous slide, it was invented by a Cherokee man, Sequoyah. So he started off illiterate and worked out how to produce a writing system. And given that it has this consonant-vowel structure, he chose a syllabary which turned out to be a good choice. So here's a neat historical fact. So in the 1830s and 1830s, a lot of the Native Americans from the Southeast of the US got forcibly shoved a long way further West. 1840s, the percentage of Cherokee that were literate in Cherokee written like this was actually higher than thepercent of white people in the southeastern United States at that point in time. And so we had this model of doing sequence to sequence models such as for neural machine translation. And the problem with this architecture is that we have this one hidden state, which has to encode all the information about the source sentence. So it acts as a kind of information bottleneck. And that's all the info that the generation gets. is conditioned on. The order of words is very important to preserve. It seems like we would do better, if somehow, we could get more information from the source sentence while we're generating the translation. If you're a human translator, you read the sentence that you're meant to translate. And you maybe start translating a few words. But then you look back at the source sentences to see what else was in it and translate some more. And in some sense, this just corresponds to what ahuman translator does, right? words. So very quickly after the first neural machine translation systems, people came up with the idea of maybe we could build a better neural empty MT that did that. So the core idea is on each step of the decoder, we're going to use a direct link between the encoder and the decoding that will allow us to focus on a particular word or words in the source sequence and use it to help us generate what words come next. I'll just go through now showing you the pictures of what attention does and then at the start of next time we'll go through the equations in more detail. The hidden representation is used to look back at the source to get information directly from it. So we'll be training the model here to be saying, well, probably you should translate the first word of the sentence first, so that's where the attention should be placed. And then based on this attention distribution, which is a probability distribution coming out of the softmax, we're going to generate a new attention output. And so this attention output is going to be an average of the hidden states of the encoder model. that can sometimes improve performance. And we actually have that trick in the assignment 4 system. And you can try it out. So we generate along and generate our whole sentence in this manner. And that's proven to be a very effective way of getting more information from the source sentence more flexibly to allow us to generate a good translation. I'll stop here for now and at the start of next time. I will finish this off by going through the actual equations for how attention works.