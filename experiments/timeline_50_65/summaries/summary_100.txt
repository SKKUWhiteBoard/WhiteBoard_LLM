Hello, so welcome to our lecture on the EM algorithm. So just to figure out where we are in the flow, because we kind of have this flow of looking through a bunch of these unsupervised algorithms. We've kind of got our hands dirty with k-means and GMM. And what we're going to try and do is kind of generalize what happened there so that we can use it in many different settings and move on from there. So last time we saw these two algorithms k-Means GMM, if you don't remember, the GMM algorithm was the one that we had these photons that we were trying to fit Gaussians to. Don't worry about if you don't remember the details, just roughly what we're dealing with. And the big idea we encountered was this idea that the universe is made up of photons that look like that. And so that's how we came up with the idea of the universe being made of photons, which we call Gaussian photons. And that's what we used to make the universe. of a latent variable. And the latent variable in this setting, if you remember, was this fraction of points that come from a source. So we didn't know how many points were coming from each one of those light sources that were out there. We had to estimate that. Once we knew that, we were able to use that information to figure out how much light was coming from which source. We could then use that to calculate how much power was being used at a given time. we estimated that, then we would be able to go back and fit all the different parameters that are there. So-- and the fraction of points, we also had to figure out the linkage, the probability that every source was coming from a point. And then we could do what we've done here. And it's been a lot of fun. It's been fun to see how far we've come, and how much further we have to go. We're looking forward to seeing what the future holds. the estimation. And the main thing that I wanted you to take from both K-means and GMM was this main idea that we kind of guess the latent variable. This is a great way to put it and how we have in the notes. You guess where they're probabilistically linked, and then you try to figure out how they're related. And that's what we do in the model. We try to guess where the variables are linked. And then we try to work out how to link them together. that is, what's the probability of these points belong to cluster one, this point belongs to cluster two, so on? And then once you have that, you then solve some estimation problem that looks like a traditional supervised learning. So the decomposition is quite important. And we're going to try to try and solve that in this article. We'll be updating this article as we get closer to the end of the interview. We hope you'll join us for the rest of the show. and kind of abstract that away. And then we would estimate the other parameters. That's what I mean by a kind of traditional supervised thing. Now, today what we are going to do is we're going to take a tour of EM in latent variable models and try and cast them. And we'll take a look at some of the key features of the EM model and how they can be applied to the real world. And that's what we'll be doing on this episode of The Daily Discussion. on a little bit more principled footing because last time, the calculations were, yeah, it kind of makes sense that that's the average, the weights in a cluster. We'll try to derive the action that we're doing there from a more principled framework, which is MLE. It doesn't mean that it doesn't make sense, it just means that it's a different way of looking at it. It's a bit of a different approach than what we've been doing in the past. it's right. Maximal likelihood is just a framework. It happens to be, though, the framework that we use throughout most of the class. There are others in machine learning by the way. But this is the one we're going to use. So before I get started on the rundown, any of you have any questions about the class? Send them to jennifer.smith@mailonline.co.uk and we'll try to get back to you in a week or so. We could guess randomly an assignment of every point to the cluster, the probability. Remember, there was this z(i)j. If you don't, I'll start to write.questions there. Oh, please. Yeah, for things we start by casting the views, what is the first step in GMM? What do we guess? We could guessing randomly an assignments of everyPoint to the Cluster. We could Guess randomly an Assignment of every Point to the Clusters. recall, we'll bring that up later. Don't worry about now. But we had to guess, for every point x, i, which of the k clusters it belonged to and with what probability. We could, for example,initialize that to uniform. We don't know anything, and that's something. We may have been wrong, but we may have found a way to get around it. We'll have to find out more later, but it's a start. some other heuristic guess. That was what was going on in the k-means++. We have a smarter initialization. But that's how we get the process started. Once the process is started, we just keep running those two loops again and again, and hopefully, it will improve. And we'll capture in the code that's going on. We'll capture it in the next version of this article, which will be published on Monday, November 14, 2014. We hope you'll all enjoy it. what sentence it improves. You'll see this weird picture of a curve that we go up, and that's going to be the loss function. Awesome. OK, so we're going to look at the EM for latent variable algorithms, and this is where it applies. This is what it's for is is to improve the efficiency of an algorithm. It's to make it easier for people to understand how to use it in a real-life situation. It can be used to help people understand the nature of a problem. dealing with various different notions of latent variables. And I'll say this right now-- may be a little bit cryptic, and I'll come back to it either at the end of today's lecture and the next lecture. When we pick these latent variables, there's a littlebit of an art. It's a bit of a mystery, but it's part of the process of thinking about the world around us. And it's a part of our job to try to make sense of it. What they're doing, basically, is they have that decoupling property. If we knew this thing that we couldn't have observed, then, all of a sudden, it becomes a really standard statistical estimation problem. And somehow, we are assuming structure, and that's what we're putting into the latent variable. So we're doing something that we wouldn't have done before, and it's a very useful tool for statistical estimation. It's a really good way to get a sense of what's going on in the world. to see walking through this today that structure-- we're assuming there's this probabilistic map out there that says how often, how likely every point is to go to every cluster. That's the real key. In other cases, we'll see more sophisticated variants of this idea, but it's actually fairly profound. The real key is to make sure that we don't get in the way of the flow of information in this way. The idea is that we should try to make the most of the information that we have. idea. We can abstract all the algorithmic details into EM the same way we did for the exponential family stuff, OK? Now, before we get started, I want to take a technical detour. And so it's really important that we have signposting here because you'll say, why is this guy drawing this stuff? And so, we have a signpost to help you understand what's going on. And we can do that in the following steps: 1. Draw the shape of a circle, 2. Draw a line through it, 3. Draw another line through the shape, 4. Draw it again. these weird pictures? The technical detail is I want to make sure that you understand this key result, which is convexity and Jensen's inequality. And the reason is-- I'll refer to this thing as we go through. We're going to use it. It's not like I'm just teaching you something, it's not. I want you to be able to see the result and understand what it means, and how it's going to be used in the future. And that's what I'm trying to do. for your health. This is actually going to be used in the next step. It will actually, in some sense, be the entire algorithm. Like, if you understand this in the simplest way, then understanding the algorithm will make a lot of sense. So it'll become a clear point of view. It's going to help you with your health, and it will help you in other ways, too. It'll help you get through the day, and in the long term, it'll help with your life. where we apply Jensen's inequality, where we make it tight. Those are the things that we're going to think about as we go through it, OK? So we'regoing to do this technical detail, right? I'm going to try and show it to you in pictures because I think it's really important. I'm trying to show it in pictures. I want to show you where we apply the inequality. I don't know if you've ever seen it, but it's very, very tight. the most intuitive way to understand the basic cases. If you already know it, don't worry. It's just another proof that you'll see. Then this will allow us to go to doing the EM algorithm as MLE. And what I mean is we're going to be able to write down the most basic cases that we can think of. And this is the way that we'll be using in the future for the first time in a number of ways. It will be a big step forward for us. a formal loss function, a likelihood function, right? That's what MLE is. We write down this loss function. Then we maximize the likelihood. And we're going to show that this actual algorithm is actually under the covers maximizing a likelihoodfunction, all right? Then I'm going to come back, and I'll show you what the algorithm is really all about, and why it's so good. And then I'll come back and show you how to use it yourself. I'm going to put GMM into this framework. And this will answer some of the questions that we kind of intuitively, kind of heuristically answered. Like, why are we estimating those parameters in such a way? And that will allow us to say, yep, GMM is an EM algorithm, and that will let us say, 'This is what GMM does. This is what EM algorithm does.' And that's what we're going to do with GMM. so it would give us a principle to solve for all the weights. If you remember, there was those cluster centers, the mus and the sigmas, the source centers, mus and sigmas and fractions, and we were going to solving for all of those. And this gives us a concept. And we can use it to solve all the other problems in the universe. We can solve all of the problems in a way that makes sense to us. We have a principle that we can apply to the whole universe. way to do it because we're in this MLE framework, OK? So we'll exercise it, basically exercise the notation. And then we almost certainly will not have time for this today, but I combine the notes, and we'll go continue to go through them on Wednesday. We'll go through what we'll do on Wednesday, and then we'll talk about what we've done so far this year. And we'll also talk about how we're going to go forward from here. we call factor analysis, OK? And factor analysis is another model. The reason I want to show it to you is it's different than GMMs, so it occupies a different space, and it will kind of force you to look at the kind of decisions you're making, right? What are factor analysis models? Factor analysis is a new type of model. It's called a factor analysis model. We call it factor analysis. We use factor analysis to make decisions. What are factors? you modeling here? And in particular, we'll model a situation where traditional Gaussians couldn't fit the bill because we're modeling something that's huge and high dimensional. And we have to assume some structure to be able to get the whole program to work. And by comparing these two and what's.you modeling, we can get an idea of what's going on in the system. We can get a sense of what the system is trying to do and how it's working. similar to them, hopefully, you get a pretty good sense of what EM is and all the different places that it runs. All right, OK, so far, so good? So if there are no questions, we're going to go right into our technical detour, which will lead, then, into the next part of the show. We'll be talking about how EM works, and then we'll go on to the next section, which is about how it works in the real world. EM algorithm as MLE.EM. This is convexity and Jensen. So this is a classical inequality. And what I want to show you is that Jensen's inequality really is like conveXity in another guise, OK? And it's a key result, so I Want to go. All right, so here's our detour. We're going to take a look at the MLE algorithm as a whole. We'll be back in a few minutes with the results. slowly. That's the only reason we're doing this, OK? So don't think that there's something super mysterious going on. There isn't. Hopefully, if I do my job well, you'll just look at the pictures and be like, oh, yeah, that makes sense. Here's the line. Let's see what's going on, OK?" she said. "That's the way I like to do it. I don't like to go too fast." A set is convex if for any a and b element of omega, the line between them is in omega. So what does that mean? So let's draw the picture first, and we'll draw the math. Here's the convex set. So it means no matter how I pick a-- here's a-- and no matterHow I pick b, the straight lineBetween them, the geodesic between them, is in Omega. So I'll write this in math so it's precise. the straight line, is in a set. This is convex. OK? In contrast, just to see it's not a trivial definition, this thing here-- which I drew very crappily, but that's OK. I'll draw it like this because, actually, you'll see why the bottom makes more sense to me in a second. Here, we have one example. So if I picked a here and I picked b here, yep, the line is in the set. But that doesn't prove it's convex, it has to be convex for all of those choices. And here, if I put it for b, lo lo. and behold, this would not be convex, OK? So let me write the math while I have the picture. So this is in symbols. For all Lambda, element of 0, 1-- so this is how I parameterize going on the line-- a, b element of omega Lambda a plus 1. And so on and so on until I get to the end of the line. And then I write it out. And it's a convex function. And the end result is convex. minus lambda-- oops, accidental stroke-- Lambda b is an element of omega, OK? Clear enough what that means? This is just the line between a and b, right? I'm just saying that no matter how I pick a and. b and lambda, this thing still remains in the set, which is the set of a, b, and lambda.minus subtracts. I'm not sure what the answer is, but I think it's something like this. is just capturing this picture. All right, now, we're going to apply this to functions. So given a function-- for right now,we make it one dimensional, Gf-- we'reGoing to find the graph of that function as a set of x, y such that y is greater than x. Is just capturing the picture. Cool? All right. Now, we are going to applied this to function Gf. Gf is a one-dimensional function. We are looking for a graph of Gf such that Gf's x and Y are greater than X and Y. So a function is going to be convex if its graph is, OK? As I said. OK, so let's draw an example of this. So here's my function. Here's 0.f of x, and I draw this character, OK. So a function can only be a convex function if it has a graph that is convex. OK? So here’s 0. f of x. Here’S minus 1.f, and this character is 1. this, OK? So the shaded region here-- so this function, by the way, is going to be f is equal to x squared. So if you're trying to correct for my artistic shortcomings, this is f equals x squared and it's a parabola, kind of. It's aParabola. This is a Parabola and this is a parbola. And this is the Parbola and the Shaded Region. And that's the way it's supposed to look. a bowl-shaped function, right? Now, no matter how I pick the points-- and clearly, I should really only have to worry about picking on the edge. So if I pick a point here, a, and IPick a point, f of a, pick a points b and pick aPoint, f, f f of b, f b, b and b, is a point. That's the way to pick points in a bowl. The bowl is shaped like a bowl, and the point is the edge of it. f of b, the line between them goes here, all right? It's not necessarily a straight line across. I just happened to pick it that way-- go up it, go down, do whatever it wants, OK? Now this function here, we'll imagine-- we'll talk about a point z later. So this line goes up, down, up and down, all the way to the top of the page. And it goes down, down and up to the bottom. I'll just say there's a point z that's going to live in the middle, and this is b lives here. Let me erase 0 and 1 because we don't really need them. Their values are kind of unimportant to us. It was just so you knew what I was drawing. I don't know why I'm doing it. It's just so I can show you what I'm drawing. It doesn't matter. I just want you to know what I've been doing. We'll draw a. All right, awesome. Allright, now, what is this function? Well, this is a-- I think it's x minus looks like this. And then its graph is everything up here. And this is not convex for the same reason. I could pick a point here-- I pick a here. I pick b here, and the line between them is below the set. This is not a convex function. OK, so let's look at this in symbols. said was a function graph, and the other one, you didn't. The function graph was open to the top, but that shouldn't be really disturbing. All right, so what does this mean? Means for all Lambda element of 0, 1, Lambda a, f of a plus. It means for all lambda element of 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 74, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 123, 120, 122, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 145, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157,157, 158, 159, 163, 164, 165, 168, 169, 170, 168. 1 minus lambda-- these are as tuples-- b, f of b is an element of omega. It means that if I take any z that's on the path, lambda a, to 1 minus lambda b-- so any character that comes. That's what omega is. It's a word that means "one" or "one of" the same thing as "one-to-one" and "one to one" is a word with the same meaning as the word " omega" It's an word that has the meaning of "one, two, three" in between here and here, then it had better be the case that f of a plus 1 minus f of b is greater than f of z, all right? So this is now z. This is z, f of Z. Does that mean that z is f of F of z? Yes, it does. So z is now f of f. F of Z is z. So f of  z is F of f of Z. F of f of Z is f  of z. make sense? Just translating the definitions directly. In more cryptic language, we usually just tell you every chord is below the function-- or, sorry, is above the function. What does that mean? Well, a chord is just anything that connects two. Sorry, I'm drawing the wrong way-- above thefunction. What do you think? Share your thoughts in the comments below or tweet us @CNNOpinion. We'll feature the best in our next column on Monday, November 14. points here. So this character would be another chord. It lies entirely above the graph of the function, where the function actually lives. Here, that's not case. I just found two points so that the chord between them is actually below the function. So it's not convex. And intuitively, the character would have to lie above the function as well. But here, that was not the case, so it's a different character. It's a character that lies above theGraph of the Function. reason I drew these shapes is that convexity for shapes probably makes more intuitive sense, 2D shapes. But now, hopefully, you see they're really the same thing. So your geometric intuition and the function intuition are the same, modulo that we change this definition here. OK? All right, now, let me let you know what I mean by that, because I don't want you to think I'm trying to make you feel bad about the way I drew the shapes. me see. All right, so one other bit here. So we're going to-- we'll actually prove this, I think, why not? Sounds like a fun thing to prove. If f is twice differentiable and, for all x, f double prime of x is greater than 0, then f is convex, and so on. And we'll prove this by proving that f is convex and that f is double prime of x. Second derivative being positive means that they have this kind of positive curvature that looks like the U's, right? Their first dimension-- first derivative goes up and down, but they're kind of always trending. That first derivative is always getting higher and higher. So this says these functions really are bowl-shaped, right?" "Yes," he says. "That's right. That's the way they are." "They're always trending," he adds. "Their first dimension is always trending." "Yes, that's right," she says. more positive, right? It's negative on the left-hand side, positive on the right-handside. That's what it means by bowl-shaped. OK, so this isn't super hard to prove, but just because I think it will stall for a little bit, in case you want to ask me questions. I'm writing more positive, more negative. More negative, more positive. Morenegative, morepositive. Morepositive, morenegative. MoreNegative, moreNegative. out a Taylor series for this. f double prime-- see the a, a minus z square. OK. Oop, plus. Let me drag that guy so it's a little clear. OK? And this a to a is just something in a, b. So maybe you remember this from your Taylor series. Or, if you don't, you can try this one out yourself. It's just a little more complicated than the first one. But it's not too bad. All I'm saying is I can write f of a at some point f of z plus some first derivative information plus some second derivative information. And then I'm using the second derivative remainder. So I'm say there's some point on the interval where this is true, OK? Same thing. Same thing, same thing, different way. Same way, same result. Same result. I'm not saying it's true, I'm just saying there's a point where it is true. for b. This isn't super important for your conceptual understanding, by the way. Like, this is just to show that you can do what you want to do here, that this makes sense to you. OK, minus z squared, handled it, OK? Now, I claim it's convex. So I just say it's a convex function. OK? That's what I'm going to do. I'm just going to say it. I don't care. I just want it to work. take what is the obvious thing to do. I'm going to multiply this by lambda. I have to make a statement about this, right? That's what's in my definition above, OK? Well, that's just the same as adding f of z for Lambda, plus 1 minus Lambda. So that's good. That appears. Notice here that I get 1 plus a times Lambda plus 1 plus the Lambda times b. That's just equal to z exactly. So I get 0 here plus 0. lambda a plus 1 minus lambda b equals z. Plus-- these things are all positive. Plus some constant that's greater than 0. So that shows that this thing, this inequality, holds, f of z. Please. Oh, so you've seen the double [INAUDIBLE],, is that [INAudIBLE]? Yeah, this is Taylor's theorem. Yeah, I've seen it. I'm sure you have, but I'm not sure if you've heard of it. Great question. So what's going on here if you remember Taylor's theorem is you can keep expanding, and then you have the last term, which is the remainder term. The remainder term says, there exists some point that lives in a to b such that this holds with equality. By the way, this is really not important for your conceptual understanding. I just want to show that it's one line, that this statement is sometimes mysterious about derivatives and causes people's heads to explode. derivatives connected to the convexity? It's because of this. This is all that's going on. Awesome. You can freely forget this and just use the fact, this fact, in the course, OK? OK, stalling done. Any more questions? Awesome. The real reason we want to go through the derivative thing is that it's the reason we're doing the course in the first place. That's why we're going to do it in this order. is, otherwise, this thing, which we actually do care about, is strongly convex. This definition feels like it comes from space aliens otherwise, if for on a domain, is if f prime of x is greater than 0, strictly greater than0. This is strict equality here, OK? That's where we're going with this. We're not going to get into a debate about the definition of convexity here. We'll get to that in a minute, though. the strongly convex comes from, whereas this is actually strictly convex. Doesn't really matter, but OK. So for example, f of x equals x squared, which I told you was in my head. Well, this gives me a simple test, right? Its second derivative is 2. That's greater than 0. That’s greater than 1! That's more than 2! I’m not sure where that came from, but it was in the head. It is the prototypical strongly convex function, OK? You also saw those This is to make this parameter sometimes with the curvature one. Doesn't really matter. The other function that I had, which you can check and graph yourself, is x squared x minus 1 squared. This is not the same thing. This isn't the same as the other function. It's a different kind of function, but it's the same in the sense that it's convex. It doesn't matter. Convex has two bumps, positive discriminant, OK? So at this point, what do I care that you know? Not too much, honestly, about this. Compute the derivative. You'll see. But it's the one that looks like the two bumps,. right? It's a quartix.convex. So that's what it looks like, has twobumps, OK?!? Awesome. That's what this is. It's the same thing, but with two bumps. What I care that you know is that there's some way that you are familiar with geometrically what convexity means. And you know that there are these tests in terms of the derivative. The second derivative being nonnegative is a good test for conveXity. And if you have a stronger derivative, that's a good sign that it's convex. I don't care what you know, I just want you to know that you're familiar with the concept of convexness. condition, you can get this strong or strict convexity, OK? All good. All right, now, what we actually need-- Jensen's inequality. Now, if I've done my job well, this mysterious-looking statement, once I show you the connection, you go, oh, OK, that makes sense. It's because it's actually just saying "I" and "Jensen" are the same thing, and it's a way of saying "we" or "they" something about convexity, but it's got a fancy name, and it's so useful. The expected value of f of x is greater than f of theexpected value of x so long as f is convex, OK? Why the heck would this happen? Let's take a look at a simple example of convexness. It's the following statement-- the expected value  of f is greater  than f  of the expected value of x, so long as f is convex. Expected value of f of x is equal to lambda times f of a plus 1 minus Lambda f of b. That's exactly the definition of convex, the inequality. This is convexity, OK? Now, the other thing I want to say for this is, notice that this does not matter how many Lambda there are. It doesn't matter how big the Lambda is, it just matters that it is a function of a Lambda. So, for example, suppose x takes value a with prob of lambdas and then takes value b with prob 1 minus lambda. Then what is f of the expected value of x? Well, it's f of Lambda aplus1 minus Lambdas b. I pick lambda. Later, I'm going to define a curve. And that curve is going to be as a result of sweeping some parameters in a high-dimensional weird space. But basically, it says, no matter how I pick the parameters of that curve, anywhere. It says, "No matter how you choose to define the curve, it will always be a straight line." And that's what the definition of a curve is all about. It's not about how you define it, it just is. that lives on this thing, that's a probability distribution, a bunch of numbers that sum to 1 in the discrete case. This inequality holds. And that's going to allow me to build a lower bound for my function, and I'm going to hillclimb using it. We'll see that in just a few minutes. We're going to go to the top of the mountain, and then we'll see how far we can climb it in a few more minutes. It's a long way down. a minute. That will become clear. Are there any questions about this piece here? All right. Now, you may look and say, OK, well, this is only in the case when there are two probabilities. What happens when there is more? You can just repeat by induction. You have to have more than two probabilities in a case. That is what we are trying to get at. We are not trying to say that there are no probabilities in any case. We just want to show that the probabilities are not the same. do something fancier if you want something that's a full probability distribution. This holds even if E is a continuous distribution. I won't show you that because we're not going to go too far off. We'll stop at kind of high school calculus. Sound good? All right. Allright. All right, so, so much as possible, let's get to the point. We're going to talk about probability distributions. Let's start with a simple one: E = 0.1. now you know Jensen's theorem, and hopefully, you'll always get the inequality the right way. And the reason you'llAlways get theequality the wrong way is you'll draw the picture of the function and see the chord is always above it. Which one must be z? Which one should be the chord above it? Which chord must be the one above the one below it? The chord must always be above the chord below it. The chord is the chord that is above the function that is being drawn. Be f of z must be below the chord of the function, and that's exactly this. All right. Now, everything is defined in the literature traditionally for convex. If you take convex analysis, it's the way we define things. We actually don't want to use a 'be f of' Be f of f of Z? Be f f of F of z? F of Z is a function of the number of points in a circle. Be f z of F. F of B. B. convex function here because we're maximizing likelihood. And this is just notational pain, right? Like, if we were-- maybe we should have minimized unlikelihood. I don't know what we should've done, but this is where we are. So we need concave functions. And what are concave Functions? g is g is. g is a function of likelihood, so we need a concave function to maximize likelihood. We need a g function to minimize unlikelihood, which is g. concave if and only f minus g is convex. All right? So we flip it upside down. OK? The prototypical one that we'll use if g of x, for example, is equal to log of x-- here's my picture of log of X, probably not very good-- is that's going to be the one we use. That's what we're going to use for this example. We'll use that example for the rest of the show. It's a very simple example. to look something like this, go off this way. And notice that if I take a chord of this function-- that's a chord-- it's below, right? If I flipped it upside down, the chord would be above. Cool. Now, also, also,. also, go ahead and take the next step and use the next function, which is to go to the bottom of the screen and look for a chord that is below. Which is what I should hope to do. there are functions that are concave and convex, right? So what if h of x is equal to a times x plus b? It's a line. Chords are both no longer above and below. It's actually concave or convex. OK, that ends the detour. Linear functions are concaved or concaved. They are convex or concave. They're a line or a convex line. That's what a line is. It doesn't have to be above or below to be concave, it just has to be a line, too. Let's get back to machine learning. So now we have the tools. Just to make sure, what do I care that you got there? You got this way that as long as we were dealing with probability distributions, no matter which probability distribution we took, we have this inequality. We don't care what distribution we're dealing with. We just want to know that we're getting the results we want. We want to get the results that we want, and that's what machine learning can do. can get lower bounds. We're going to use that in a second to draw some curves of a likelihood function that will hopefully be easier to optimize than the original function. And we'll try an iterative algorithm that will look exactly like we talked about before. And then we will try an algorithm that is exactly like the one we just used. We will then use that algorithm to try and find a way to get a lower bound on the likelihood of a certain outcome. We'll try and see if we can do it. conceptualize it is we solve for some hidden parameter. We solve, and that gives us an entire family of possible solutions. Let me draw the picture after I give you the formal set, OK? Oops. All right, so EM algorithm has max likelihood -- it has a max likelihood of 1 in 100,000,000. That's 1 in 1,100,000 in a million,000 million, or 1 in 10,000 billion, or something like that. It's a very, very big number. I'll actually put MLE. All right, so remember, this is the max likelihood formulation. There's some theta that lives out there. We have some data, i from 1 to n. These are our data points. We take a log of the probability that we assign to the data given our our data. And that's what we're going to do with the data. We'll use it to try to predict what the future will look like in the future. And we'll use the data to try and predict what that looks like. parameters. So this is a way for us to compare different parameters. And recall, these are the parameters, params, and this is the data. So far, so good? All right, now, we're working with latent variable models. So latent variables models mean that P has a little bit of extra. P is a latent variable model, so P has an extra variable called P. This is the model that we're using for this experiment. We're going to use P as a variable to test the model. structure. P(x; theta)-- this is a generic term, right? This is just one of the i terms-- says the function factors this way. Looks like a sum over z, where z is our hidden or latent variable. So remember, z was a latent variable, and theta was a hidden variable, so it's a function factor, not a variable. It's the same thing as a function that takes a variable into account, but it takes a function into account the hidden variable too. our GMM latent variable, the cluster probability, right? So we have to sum or marginalize over all the possible choices of z. This is basically saying, I don't know what z is. I have some probability distribution that I can compute over my data and z given theta. And this will get me back a probability for x, right?!? This is a sum over all possible z's. This will leave me with a probability of x, or. And since I don’t know whatZ is, the term is-- I marginalize it out-- means I sum overAll the possible values. sorry, probability for x. Is that clear? Yeah, ask a question, please. So where is the z going to go again? Like, is that property of the parameters [INAUDIBLE]?? Yeah, wonderful question. In a real sense, when we make a modeling decision, and we are looking at a set of parameters, the z is a property of that model. It's a property. of the model, and it's a real thing. It doesn't mean that it's always going to be true. say, there exists some structure out there. There exists a probabilistic assignment between photons and point sources. One version of the prior is, I tell you exactly where every photon comes from. That's clearly a very strong prior. If you knew that, Godspeed. Go do it. You just solved the mystery of the universe. It's a very, very big mystery. It takes a long time to solve, but once you've solved it, it's a done deal. GDA.GDA: If instead what you know is there exists some mapping that's out there, then that structure that you're putting into your function-- and what I'm saying is that mathematically comes down to baking exactly this in, OK? And this is the mathematical form for all of those latent variable. This is the Mathematical Form for All of Those latent variables. And it's the form that you bake this in to your function. And that's what you're going to do. models. So when we have that idea about latent structure, we'll eventually put it into this mathematical form. And we'll see a couple more examples. In GMM, this was exactly the z there. The notation isn't an accident. It's the same z. Go ahead. --example of this? In GMC, this is the z. And it's the z in GMM. And the z is in the GMM notation. And this is in GMC. Yeah, so the example that we had was basically the whole lecture where z was the probabilistic linking between sources and photons. Yeah, yeah. So that's one. We'll have more examples later. But I want to get through the algorithm in this abstract form, and we can shoehorn more things into the lecture. We can't get through it all in one sitting, but we can get through some of it in an abstract form. That's one of the things we want to do. into it. And what I'll do afterwards is put GMM right down in this language. We need a couple more things. Please. Yeah, correct me if I'm not understanding this correctly, but-- so z is the probability that a point is coming from a particular cluster. What is it? Like, like, in the game, it's the probability of a point being in a cluster. Like, in this case, the probability is that it's coming from that cluster. what is probability of x parameterized by theta actually represent in this case, in that photon example? Yeah, exactly. So remember, if you-- I think it was said yesterday by someone here on that side of the room. So I don't know if that's spatial recognition helps you in the case of the photon example. It's a very, very complex problem, but it can be solved with a bit of math and a little bit of luck, I think. I'm not sure if that helps. last lecture. But it was like, imagine I was guessing all the photon models that were out there. So each one was parameterized by some choice of z(i). And then what I'm thinking about is what I want over that is that, across all those thetas, no matter how I did it, it would always be the same. That's what I was trying to get across in this lecture. It was like I wanted to make sure that it was the same across all the models. P(x; theta), we've been using forever. We used.instantiate z, each one gives me a different probability distribution. I can sum them up, and that tells me, given this theta, no matter how z is assigned or marginalized across all ways that it's assigned, how likely is the data? So P(x, theta) is what we use to get the probability distribution of a given data point. We've used it for decades. that from the supervised days. We just inserted z and said, well, there's this wild z that we can't observe, but it somehow constrains x. It means that x-- like, the relationship between theta and x. And that's what the model does. Awesome question. Very cool. These are wonderful questions. It's great to have you on our show. We hope you'll join us next week for another episode of "This Is Life with Lisa Ling" I'd much rather answer these than badly draw the pictures that come next. We're going to get to those pictures no matter what, so there's really no saving us. All right, let's get to the bad pictures. I'll try and leave this more or less on the subject of bad pictures, but it's not going to be pretty. We'll get to them in the next few days, I'm sure. I've got a few more questions for you, so I'll leave it at that. screen. Here's the algorithm. This is a picture, which maybe won't make perfect sense to start with, but we'll get there. All right, so remember what a loss function looks like. I'm drawing everything in is in horrible high dimensions. My axis is theta. And then what I do is draw a circle around the screen, which is the shape of a circle. The shape of the circle is a circle, with a circle at the center of it, and a circle on the right. have-- and I apologize. I will use a bunch of colors. I hope this is OK for people to see. If not, let me know. Doesn't look like the most visible color but doesn't looks like the least visible, and I need a couple. This is my loss function, l(theta), and I'm using it to help people with their reading comprehension and vocabulary. I'm also using it as a tool to help me with my writing. If you have any questions, please send them to jennifer.smith@mailonline.co.uk. This is l(theta) here. Now, remember, it's not a nice concave or convex function, right? We wouldn't expect it to be. We would hope, because we're going to minimize it. OK? So this is my loss curve, OK? This is l('theta') here. This is the loss curve for l' (theta), OK? It's not concave, but it's convex. It's a convex loss curve. "We saw in a lot of the problems that we were after, it doesn't look like that," he says. "It has these kind of the kind of concave.it, that it's concave" "That would be nice. If it just looked like this-- like, oh, that'd be so great. We would just climb to the top," he adds. "That's not what we're doing." "We're trying to get to the bottom of the mountain," the team says. weird bends. That's all we're after. We settled for that in KMM, for k means, and we're going to settle forThat in GM, OK? So how does that work? We had to settle-- that's another way of copping out and saying, we had tosettle for these local iterative solutions. That was what we settled for in GM. So how do you do that? We're trying to figure it out. We'll let you know what it looks like. the algorithm work? We start with an initial guess. So theta, let's say, at time t-- so it could be time 0, right? This is just the initial guess, whatever it is. Then, again, you could ask-- these colors seem harder to read. You start with a guess, and then you try to guess what the color of the sky is at a given time. Then you try different colors, and so on, until you get the right color. what happens is this is mapped up to here, which is l of theta t. I'm just giving notation. This is just the value of the loss that I currently have. I suspect there's something up this way I'd like to get. So how do I get it? I haven't written anything. I just want to know how to get it. I don't know what it is, but I want to find out. I've never been able to do it before. do it? What's the algorithmic piece? What I'm going to do is I'mGoing to form-- so the problem is optimizing over all those z's seems daunting, directly optimizing the l's. So instead, what I'm Going to Do is Come up with a local curve, OK, and. And then I'll do it. And I'll be able to do it in a few seconds. And that's how you do it, right? That's what I do. That's how I do things. I'm going to call this curve Lt of theta. It's another function. I'm only drawing a piece of it, but it goes the whole distance, right? It's some curve. Now we'll pick Lt, usually, to be some nice convex function, something that's easy to optimize. So we're going to go with Lt of Theta. We'll call it Lt of Tta, which is a convex curve. We're not going to use the word convex here. try and get that kind of easy-to-optimize function. And then what we're going to do is we're Going to optimize that function. We'reGoing to find its local maximum. So it's local maximum, for the sake of writing, is, let's say, here. Then we'regoing to set that to the maximum of the function we're trying to optimize. And that's going to be the function that we want to optimize to the highest possible level. be theta t plus 1, OK? And this is now l of thetat plus 1. And we're going to, again, create some new curve, Lt plus 1 of theTA, based on that point. And the key aspects of the point that I'll write in a second is this. This is now theta l plus l. And it's now l t plus l t. And that's what we call the new shape of the theta. It's now Lt t plus Lt t. point is a lower bound. It's always below the loss, so it's kind of a surrogate that I'm not overestimating my progress, and it's tight. It meets at exactly that point. So if I did happen to have the actual optimal value, it would be at the top of the curve. It would always be above the loss. So it's a surrogate for what I would do if I didn't have the optimal value. And it's very close to the truth. meet at that point. So I wouldn't think and get fooled that there was a higher loss function somewhere else. Let me write those two things, OK? So first, Lt of theta is going to be less than l(theta). We'll call this the lower bound property. Our hope is Lt is easier to optimize than l. So this picture-- the content is we're picking these-- like, that was a really bad drawing of one, but these picking these concave. kinds of functions, which are easy to maximize, right-- that's what I mean by it kind of looks like a supervised thing. Then we maximize that, and this is formalizing the back and forth. We take that new maximum that we have, which is our new best effort of parameters. And then we take the new maximum and maximize that again, and so on and so forth. It's a kind of a back-and-forth process that goes on for a long time. Jensen: I'll sketch the algorithm. You don't have math to talk. And then, [MOUTH POP] we then do it again and create another curve. Now, the way we're going to create that curve-- you're going, to see in one minute. It's going to be Jensen's, and that's the whole algorithm. So I'll sketches the algorithm and then you'll see the result in a minute. That's Jensen's algorithm. And that's how you create a curve. about the algorithm, but hopefully, it's clear what's going on. Easy-to-train surrogate, and we kind of slowly hillclimb with that easy- to-trained surrogate, alternating back and forth. And this is what we were doing in K means. And just so it's just so clear, we did the same thing in GMMs as well. We used the same algorithm in K and GMMs, but we used a different surrogate for K. And we did it in GMM because it was easier to train. super clear, I want to make clear here, phi t plus 1-- this is nothing more than the argmax over theta of Lt of theta-- means I do the optimization on the surrogate curve that I created. All right. I think that this description, hopefully, gives you some intuition, hopefully to give you some insight into the algorithm. Cool. Now, let's go to the next part of the show. Back to the page you came from. The next part is about the next section, about the third section. of what's going on because, otherwise, the math is kind of bizarre-looking. But we'll see. So this is the rough algo. I'm just restating what's on the thing. This is going to be called, not surprisingly, E-step. This says, given phi t, find this. And that's what we're going to try to do in this experiment. But it's going to take some time to get to know each other. We'll see how it goes. curve, L of t. And then the M-step, and together, EM, given L of T, set phi t plus 1 equal to argmax phi Lt(phi). Cool. Just could you reiterate? Like, why are we not using gradients on the original turbulence? Right, so we could imagine doing some kind of turbulence-based algorithm. Right. Please. Like, how about using some sort of turbulence based algorithm? Right. So, let's try that. of gradient descent here, but it's not clear how to deal with this marginalization that happens in the middle. So if we did some marginalization or some sampling, we could do something that looked like that. But it's because we have this decomposition. Note we have-- you can also imagine we have a decomposition of gradient descent, but you can't imagine how to do that. So we have to do it in a different way, and it's a little bit more complicated. that we have a decent solver for the inner loop because it's this nice-to-solve thing. I would say, over time, this split of what's nice to solve and what's not-- right now, I'm pitching it to you as, it must be concave, and so it's nice. But this kind of kind of thing is not nice. It's not a nice thing to solve. But it's a good thing to be able to do. just means I have an internal solver that's fast and I kind of trust, and I have something on the outside that's a latent variable that I'm like splitting up the modeling. It's one of a number of decomposition strategies. Doesn't mean it's the only way to solve it, though. Don't use it if you don't like the way it looks, or if you're not sure you want to use it. It can be a lot of work. Wonderful question. Cool. All right, so the question is, how do we construct L of t? And I claim we know everything else. So we'll come back to that claim in a second. So let's look. It's going to go term by term. Solet's look at a single. Let's go term-by-term. We're going to start with the first one. We'll go to the second one. And then we'll go on to the third. term in our equation, OK? All right, so I'm going to grab one of these characters, just one, and work with that. So how do we construct it? So right now, we're trying to understand how to create this L of t from this function. And you should roughly roughly roughly figure out how to do it. And so we're going to work with one of those characters. OK? How do we do that? So I'm just going to take one of the characters, one of them, and I'll show you. be thinking because I told you that Jensen's will have something to do with this. Now, what we're going to do to put it in the form where Jensen's could be used looks wholly unmotivated. But it's to shoehorn into what we are doing, and there's some motivation, he says. "It's to be used in a way that makes sense," he adds. "I don't think it's going to be a big hit, but it's a start" but it's kind of opaque, let's say. What I'm going to do is something which, at first glance, seems strange. Now, this is true, formally, for any Q(z) that I pick, OK? Please. Would you just go a level [INAUDIBLE]?? Oh. Right? So here, I'm just introducing Q. This is Q(Z) This isQ(Q) Q(Q)(Q(R) Q (Q(Z), Q(R), Q (R, Q(S),Q(S,Q(T),Q (S,D,E,F,G,H,I,J,K,L,M,N,P,R,S,U,B,C,B,.Q(z),Q,Z,Z). true for any Q, right? Let's not worry about support issues, but I'm just putting in something that divides by 1-- seems sort of unmotivated to do this. Now, I'm going to only consider-- we get to pick Q's, so that I can use Q's. So I's going to pickQ's, and I'll only consider Q's if they divide by 1. So, I'll be using Q's to divide 1 by 1, and that's what I'll do. Jensen, such that it's a probability distribution over the states such that the sum over Q(z) equals 1. OK? And I'm going to call this property star, OK? So I am going to pick Q as the probability distribution. I'll write that Q(Z) is greater than or equal to 0. I will call it the Q property star. And I'll call it Q(S) if Q(s) is less than or greater than 0. in a different color. OK, why? Because now I can make my argument one line. That's the real reason. So how does it work? Yeah, good. So we have this character-- copy-- in here. This can also be written-- oops, I don't want to use blue. This could also be. red or blue. It can be any color. It could be black or white or red or yellow or blue or green. It's all about the color scheme. written as an expected value, where z is distributed like Q of this weird-looking quantity. Why is that? Well, it's just the definition of expectation. This is just symbol pushing. There is nothing deep going on. But it's important symbol pushing because it means Jensen's applies. Oops, log of this. It's a log of the expected value of this quantity, not the actual value of the quantity, so Jensen's doesn't apply to this quantity. It doesn't matter what the quantity is, it just matters that it's expected. thing, sorry. Dammit, I forgot a log. OK. I'm just transforming this thing internally into this notation. Yeah, please. What's Q? Q is this function that we picked up here. So Q is just some probability distribution. And this is going to define our curve. Just getting a little bit closer to the truth, I'm sure, but it's still a long way off. It's going to take a long time to get to the bottom of this. ahead of ourselves, we're going to allow-- the curve is going to be parameterized by whatever probability distribution we want. So it's our degree of freedom. I'm just telling you something that's going to hold no matter how I select the probability distribution. The tool that I have in my toolbox is the curve. It's not going to change, it's just going to get bigger and bigger. That's the way it's always going to work. I don't know why I'm telling you that. arsenal to do that is Jensen's inequality. Now I've turned this into an expectation, and in one line, I'm going to be able to turn it into a lower bound that works no matter how I pick it up. Graphically, what I'm doing-- sorry to confuse folks who are copying-- I'm saying that the lower bound is the same no matter what I do. I'm not trying to confuse anyone, I just want to make it clear that that's what I've done. is basically show how to construct this lower bound. We'll come back to this. So this is less than. I can pull the expectation out. P(x, z; theta) over Q(z). It's always a lower bound everywhere, and that's where I'm going to use Jensen's inequality. So let's see that next line. It's called the Jensen inequality. It says that the lower bound is always lower than the upper bound. And that's what we're going to do. This is Jensen, OK? Log is concave. This is equal to some Q(z) times log P(x, z; theta)Q(z)-- again, just symbol pushing. The key holds for any Q satisfying star. No matter how I pick the probability distribution, no matter what distribution I pick, the key holds. OK? OK? So there's only one content line here, OK?!? The key hold for anyQ satisfying star, OK! OK? this chain of reasoning goes through. Please. [INAUDIBLE] always the first-- the second value? Like, how did you convert the lower bound, that thing, and do certainly the expectations? Yeah, so this was exactly Jensen's inequality. So if I scroll back up, this was Jensen’s inequality. But because I was. in a position of power, I was able to use it to get a higher bound than Jensen could have used to get to a lower bound. applying it to the negative of it, it's exactly the same piece, but it reverses the inequality. And so I'm just directly applying that reasoning. Well, I mean, like before, I was like, how are you converging? Oh, this thing into this thing? Yeah. Yeah, sorry. So this is just applying the reasoning to the positive of it. And it's the same thing. It's just the opposite of what I was doing before. So I'm like, okay, I'm converging. because Q(z) is a discrete distribution, and the definition of expectation is, this is a bunch of numbers that sum to 1. So this is an expectation with respect to some distribution, in particular, the one where z(i) occurs with probability Q of z, z( i). That's it. It's, again, just the expectation of a distribution that has a discrete number of z. That's what we're looking at here. We're not looking at a distribution with a probability of any kind. symbol pushing. Please. distribution of z, it is going too far. Therefore, 1 plus Yeah, so you want to know how we ground it into an example. Is that what you're asking? But isn't phi completely [INAUDIBLE]?? No, no. So there's no phi here. So apologies if there's something wrong with the example. It's just that we don't know how to ground it in a way that doesn't make it a symbol. difficult to read. Q is something that I've artificially introduced. If you pick a Q that satisfies this, I have a way of saying it. There's a theta here. I'm just saying that all I've shown here is that I have an idea of what a Q is, and I can give you a way to say it. I can say it to you if you want to hear it, but it's not going to be easy to do. I don't know how to do it. lower bounding this function, getting a family of lower bounds to it. And I'm trying to give you the intuition of why I might want to do it. It's so that I can construct those curves that come later, because now this function is going to be much nicer to work with. And it's so I can get a better idea of how to use it in the future. And that's part of the reason why I'm doing it. I want to get a good idea of what to do with it. optimize, but we haven't quite gotten there yet. So you're right ahead of it. This gives a family of lower bounds. Namely, this is how I get Lt theta less than l(theta) OK? So this whole thing is-- this gives aFamily. This is just what I was saying there. So this is just how I got Lt thetas to be less than L(thetas) This is the way I get L-thetars to be smaller than L- thetars. because, term by term, it's going to be less than or equal to. Now, it doesn't satisfy all our requirements, because we have to make it tight. So how do we make it Tight? That's the next piece. But right now, I have a way of going term by Term. But I have to go term byTerm to get to the end of the project. That's what I'm trying to do right now. I'm not going to give up on it. from the likelihood function and getting lower bounds at a particular spot. And it'll be a lower bound no matter where I am, OK? But I have to pick a certain Q to make this operational. That's the piece. So I have freedom to pick Q, and I'm going to go with Q, OK?" he says. "So I have free choice to choose Q," he says, "and I'm not going to pick the same Q every time." "I'm going with Q," the computer responds. "And it's going to be a different Q each time." pick a very specific Q, and that's going to give me a lower bound. Would then start and then Q has to be greater than 0? Yeah, yeah. So I said I was going to ignore the support. Go ahead. You can do it. You've got to get the curve. And that's what I'm doing. I'm trying to get to the bottom of it. And I'm going to get there. You're going to have to go through the whole thing. We can imagine just for the sake of this lecture that it's strictly greater than 0, so I don't run into weird things about what I mean by divide by 0. Here, because I'm controlling the multiplication ahead-- ahead of time, it does make sense, but you're right to point out that it doesn't make sense in the real world. We can imagine that the number is greater than 1, so that we don't get into any weird math problems. We're going to divide by 1, and then divide by 2, and so on. that out. So just think about it as greater than 0. Yeah, wonderful question. Cool, all right. All right, so now how do we make it tight? So what we have to do-- the intuition here is that we want to make Jensen's inequality tight. And the idea is if you want to get rid of the inequality, you have to make it very tight. So that's what we're trying to do here. We're going to make the inequality as tight as possible. what's inside is constant-- imagine there was a constant inside, that this term was constant for all the different values of z-- then the expectation clearly doesn't matter, right? So if they were all the same, then these two would actually be equal to one another. This is some of the work we did to get to this point. It's a long way to go, but it's a step in the right direction, I think. We'll get there, I'm sure. value, alpha, and then you would get a sum over all the alphas that were there. They would sum to 1. Boom, done. If they all have the same value here, alpha,. they would be here in the log, and they would also sum in the same exact way. So if they all had the same alpha, they would all sum to the same amount. So that's how you do it. It's a very simple way to do it, I think. as long as this term is a constant-- that is, it doesn't depend on z-- I'm in business, all right? So what that means is I want to pick Q such that log P of x, z; theta over Q(z) equals C. Now, before, I had all kinds of freedoms. Now I'm not allowed to do that. I have to pick a constant. So I choose a constant that is a log of x and z. That's Q. to pick whatever Q I wanted. So Q(z) has to be related in some way to P x of z for this to work. Go ahead. What is c? Ah, what is z or c? Oh, c is some constant. This is. Q is the probability that Q is related to P X of z. Q has a probability of 1 in the range of 1 to 1,000,000. The probability of Q is 1 in 10,000 or 1 in 1,500,000 for each z. a constant independent of the-- just for some constant c. We don't care what its value is. We just care that it doesn't depend on z in any way, and then it will be exact equality. Then Jensen's will be equality, and so on. And so on, and on, until we get to the point where we know the answer to the question, "What is the value of the constant c," and then we can say the answer. And then the answer will be the same as Jensen's. OK, so what is the natural choice? Well it's that Q(z) should equal P of z given x; theta. Why is that? Well, this is also equal to-- so this is because P of x of z of theta equals p of z x;  P of x; Theta P of  x; Q of z; Z of z equals P of X; X of z, and so on. And so on and on. Q(z) does depend on theta. So if I plug these in, they cancel out. And c is equal to log P x of theta, OK? So let me make sure this is clear. Note-- this just means "note well," and B-- I just use it reflexively just to signal. Theta.theta. theta is the sign of the zodiac. The zodiac is the shape of a circle, with a central point at the center. and x. So we're going to have this notation, Q(i) of z because it depends on each different value. So each data point is going to get its own different Q, which is the log of how likely this thing is, OK? And we picked those for each i. So that's how we get to the Q of z. And that's what we're doing with the Qs for i, x, and z. We're doing that to get to a Q of i, z, and x. because we did this term by term, we can pick that Q-- Q1, Q2, Q3, all different. And we pick them all so they satisfy this equation. OK? This thing has a very famous name, so I'll write that while I kind of stall for more questions. So what we've done is we've picked a set of words that are all different, so that they satisfy the equation. So we can say that Q1 is Q1 and Q2 is Q2. defined here is called the Evidence-based Lower BOund, or the ELBO, which actually is a fairly-- like, if you say ELBO to a machine learning person, they actually know what it is. It's not something we're making up, it's a real thing. The ELBO of x, Q, z equals x, Y, Z, which is what we're trying to do here with our machine learning system. We want to make sure that the data is accurate and that it doesn't get corrupted. the sum over z Q(z) log P(x, z; theta) overQ(z), OK? And what we've shown is that l(theta) is less than or equal to-- or is greater than orequal to the sum, because we did this term by term, of the ELBO of x of i, Q of i. We've also shown that Q of x is the same as the sum of x and i, or the sum over x and z. The z can't appear there. Only thing that appears there, OK-- for any Q(i) satisfying star. That was just restating the lower bound. All I said is we went term by term. Sorry, this is incorrect notation. This is theta.i, theTA.i,. Sorry, the z is marginalized away. The z can’t appear there, the only thing that Appears there is the z. That’s it. Sound good? through this thing, so it holds for every term that we can pick Q(i). As long as it's a probability distribution, it is a lower bound. And then we also showed that l(theta) t equals some i 1 to n ELBO x(i), Q( i), theta(t) for the choice ofQ(i) above, OK? And then, for example, we showed that theta t is the lower bound for the probability distribution. So hopefully, that picture makes sense. Again, just to recap what's going on here, we have this opportunity to pick these bounds, and we'll use them in a second, so it'll hopefully become more clear exactly what we're kind of optimizing for here. What we're going to do is we'll pick a set of bounds and then we'll try to optimize for them. And we'll do it in a way that makes it more clear what we want to get out of these bounds. see how we pick the Q(i)'s and all the rest in a second. But this is basically saying that it satisfies the two properties that we had before. We're going to find this upside-down bowl-shaped thing. We's going to then pick where we are on the curve. And that's where we'll start our search for the answer to the question, "What is the best way to answer the question "What do you want to know about the world?"?" optimize that thing in a second, pick our new theta(t) then repeat and do another curve that's present. All right, so let's do the wrap-up and state the algorithm now with our newly hard-earned language. Yeah. [INAUDIBLE]? Then we need to find a lower bound. Oh, no. So these are these are the algorithms we use to find the lower bound on a certain number of degrees of freedom for a certain function. both on the original loss. These are just saying, this is the Lt here, capital L. Each one of these is capital L, basically, right? And then this one here is saying that, at that particular point for that t-th instantiation,this is where we are. OK. Yeah. All right, all right, All right. Allright, allright, Allright. We'll get to it. We're going to get to the point where we can talk about it. so the wrap-up is as follows-- this is how-- we can now write down the algorithm and the kind of full generality with mathematical precision, although it may still be a little bit opaque. We set Q(i) of z in the E-step equal to the probability that z(i, x) given x(i), x( i) is the same as z( i, x, y, z, x (i), y (i, z) and theta for i equals 1 to n, OK? So this says that you're going to pick the Q(i) distribution that says, what's the probability that's most informed or the exact probability that comes from your model knowing the data and the current guess of your parameters, right? So you pick the distribution that is most likely to be accurate. And then you try to predict the outcome of the experiment using that distribution. And if you get it right, you get the answer you want. have some theta at some time. You plug it in. You know the data point that you're looking at. You condition on that. And you say, what are the most likely values of the cluster linkage-- as we were talking about before, the source linkage-- for this particular point? You say, 'What is the best way to look at this point in time?' And then you try to figure out how to get to the right place in time to see it. get a probability distribution over those. You set them to Q(i)z. It's your estimate of how likely that is. Then you take an M-step. Theta t plus 1 equals argmax over theta of Lt(theta), which equals-- so Lt( theta)-- sorry, I'll write it like this. Lt(Theta) vs. Q( i)z is the probability distribution of a certain event. It can be used to predict the outcome of a game. equals this ELBO sum. x(i), Q(i) and theta, OK? Your current guess of parameters. I get the lower bound that's underneath the covers. Then I optimize that lower bound surrogate and get the theta t plus 1. That's what the ELBO is saying. It's saying, you give me a current estimate of parameters, and I get a lower bound. Then, I get that lowerbound surrogate and I optimize it. And that's how ELBO works. That gives me a new guess of parameters, which defines-- you get a new curve, a Q(i) for each one of what's going on, then I go back. Is it a ELBO down there-- is that the Q( i)? Q-- oh, sorry. Yeah, please. This isQ(i), and this is Q(a) and Q(b) andQ(c) And Q(d) is the same as Q(e), Q(f), Q('e'), Q('f') and Q('g', Q('h', Q ('i', Q' }); this is theta. And I'm inconsistent with the semicolons too. So you move this. So there's a good visual distance. More a notation question, I guess. Do you use that equation [INAUDIBLE]?? Right. So it's so it's an x. This is an x, This is a Q. this is a theta, this is an X. This. is an Q. and this. is a Theta. It's an X, a Q, and a Thea. just as before. t starts at 0. We have that initial guess, and then we go from there. Theta is our current guess. Cool. All right, why does this terminate? And it's basically for something that's kind of not very interesting or satisfying, but it does. This gives you a good idea of what to look for in the future. It gives you an idea of how the universe is going to change. And it gives you something to work with in the present. sequence that is monotonically increasing or nondecreasing, OK? So it's possible that it would grind to a halt. But eventually, it has to be strict. There are some other things about how fast it terminates. But it's a monotone sequence, so we'll have a convergence of subsequences. That's really all. It's a sequence of sequences that is Monotone. It is a sequence that isMonotonically Increasing or Non-Necessarily decreasing. that matters, OK? We don't say how fast it converges. It's a separate issue. Is it globally optimal? Well, no. Just look at the picture. And so to derive a counterexample, you would just find a likelihood function that had those two bumps. And you would run it in. And it would be in the same way that a probability function would be if it had two bumps in the middle of it. It wouldn't be globally optimal, but it could be in a certain way. that particular lower bound setting. And what it will do is it will gradually hillclimb. And this is actually not great. Like, it can't go back downhill, right? It's got to just continue to go up. If it gets locked inside one of those bumps, it's kind of toast. OK, OK, that's not great, but that's what happens when you set it to that lower bound. That's the way it goes. It's not good. so in summary, what we saw here is we derived EM as MLE as promised, OK? So just to recap what happened here, we started with this notion around Jensen's and convexity. So we looked at a pictures of convexness, and we got an intuition of what sets are convex. And then we used that intuition to get EM as a MLE. So that's what we did. And that's how we got EM as an MLE, as promised. and what sets are not. We wanted to use concave functions, which are these kind of downward-facing things. The chords are always below them, OK? Those are the loss functions because we wanted to maximize them. The reason that was important is we had to do this back-and-forth iteration. Given a set, you can see what it is and what it isn't, and how it's going to change over time. That's what we tried to do with this set. a set of parameters, we were going to find a surrogate. That surrogate was going to be concave in our setting. We would use Jensen's inequality as a way of constructing that entire curve. We needed the concave part of the curve to be a nice function that we were after. It's one of those nice functions that we're after. We'd like to use it to construct a nice curve. It would be one of the nice functions we're looking for. It'd be a concave concave function. We need to be able to use that concave curvature. entire curve because we wanted to optimize it. So it wasn't enough to find a point in a lower bound. We needed to find the whole thing that was underneath it so we could run our argmax step. And that was the setting where we would learn all of the setting of the curve. We wanted to learn all the settings of the entire curve so we would be able to run the argmax. And we were able to do that by looking at the whole curve, not just one point. parameters and estimate that in a way that was hopefully nice and easy to do, which was like, estimate the means and the variance of the data that we're given. We'll run through an example of that. This is a necessarily kind of abstract and confusing algorithm. The best way to go about it is to try and estimate the mean and variance of a set of data points and try to get a good estimate of what that means. That's what we're going to try to do here. to understand it is just to run it through a couple of the different examples. EM and the next one-factor analysis-- and by the end, you'd be like, oh, OK, that makes sense. It's a lot of notation because we're abstracting out a huge number of things that we're doing," he says. "It's a huge amount of notation," he adds, "but it's all very important to the goal of what we're trying to do" But in the end, it's not so bad, right? You take the Q(i)'s. And this way, set the thetas, do a descent on them, or ascent in this case. Do argmax. OK. All right, so let's see it for our Gaussian mixture model. Please. For this, [INAUDIBLE] this termination condition for this, for this. Please, please, please. Allright, so we're going to try it out. event. The termination condition is not really important, or in the classical sense. The thing is that it's nondecreasing so that, eventually, there's a convergent subsequence of it. And it converges for the same reasons that GMMs converge. It's not telling you how fast it converging, for example, but it's converging for the reason that GMM converges. It is converging so that there is a subsequence that is convergent with the previous one. that we were kind of going downhill, if you remember it, every time. Like, there was some loss functional that was decreasing. And this is just saying there's something that's continually increasing. When do you terminate it operationally, like you're running this algorithm, when do you decide? You look and look and you look and it's like, 'What do I do?' You know, it's kind of like a game. It's like a chess game. You have to play the game. see if the loss of the likelihood function is not changing too much. What does too much depend on? Depends on your data, depends on the problem. Like, sometimes if you have a huge amount of data, and you're averaging over billions of examples. Sometimes if you've got only a few examples, you don't need to worry too much about it. If you have thousands of examples, it may not be a big deal. If it's just one example, it's probably not a big problem. small amount of data, you want to get to machine precision and 10 to the minus 16. And so that's the way you decide when to do it. This just says that it's not going to oscillate wildly. It's a very weak statement I'm making. Yeah, please. Can you explain it to me? I'm sorry, I don't know what I'm doing. I just want to know what's going on. I'm trying to figure out what's happening. what specific part of this is linked to the MLE sort of aspect of it? Oh, awesome. Yeah, so we're going to see the M LE when we actually do the computation here. The reason it's linked to MLE comes from a very simple piece, which is we started in this. We started with a simple piece of code that was in the code base. It's a very small part of the codebase, but it's a big part of what we're doing. model, where we were saying, the way we're going to think about the world was to maximize the likelihood. And that was how we think about our data. That's less disturbing to this group than it is to, I guess, generally worldwide who think about this, because this is the world we live in, he says. "This is the way I think about my life," he says, "and that's how I want to live my life. I don't want to have to worry about what other people think about me." only framework we've used in the course, but that's what I mean. We started with l(theta) as what we were optimizing, and then we derived this as a set of concerns. We didn't get to a global optimum. So I don't mean that we definitely guaranteed that we got the.only framework. But that's where we were going with it, and that's the way we're going to use it in the rest of the course. We'll see how it goes from there. maximum likelihood estimation, just that you can phrase what's going on as MLE. And so when you get into other estimation problems and the subproblems, you just apply the MLE stuff you learned from the first half of the class. And we'll see that in an example. Does that make that make sense? We'll see in a second example how to do that. We'll show you how to use that in a third example. And then we'll go on to a fourth example. sense? Yeah, yeah. Awesome. Thank you for the question. Why is this tight? Which one? Oh, it's tight because we went through this small piece here, which was that if we selected it as a constant in this particular way-- so this is tight. This is tight because it's a constant that we selected as a particular way, and that's why it's so tight. So this is the tightest of all the ELBOs that we've ever come up with. before we could pick any Q and it was a lower bound, as long as we did this, then actually this line was no longer an inequality but was actually exact equality. And it depended, though-- that selection of Q depends on theta and x. [INAUDIBLE] Awesome. Great question. Great answer. Great point. Great idea. I'll be back in a few minutes with the rest of the team. Thank you so much for your time. Back to the page you came from. Yeah, and that's just making sure that the picture in your head is exactly right. We go up to the loss curve. We get something that's underneath it that touches at that one point. And then any optimization we do there is actually also optimization on the loss curves itself. It's all part of the same process, so it's not just about making sure you get the picture right in the first place, it's also about getting it right on the other side as well. Cool. EM for mixtures of Gaussians, or we call them GMMs, sorry. All right, so what's the E-step? Huh? Yeah, I'm just going to copy down the thing. So let's get the generic algorithms. Let me get thegeneric algorithm. Allright, just so all right. All Right, just just so All right. Just so All Right. All OK. All Okay. All okay. All. OK. Now, let's try it out. we have it on the screen. So here's our warm-up-- not really a warm- up because we're almost out of time, but here's-- remember, if we saw how this worked-- P x(i) and z(i). And remember, we factored it as the following-- this is just Bayes' rule, nothing crafty going on. We're going to try to figure out a way to use this to our advantage in the next few minutes. here, not tricky. z(i) was a multinomial. This means phi i greater than And this was, remember, our N cluster j. And so then once we knew-- given z( i) equals j-- that every cluster had a different shape-- so we had adifferent mean, mu j, and a different mean. And this is how we got the shape of the N cluster, which is a N-shaped cluster. It's a very simple structure. size or variance, mu j. And I'm doing everything in one dimension, but in two dimensions, you would have actually the whole covariance would be different. These are the cluster size descriptions, cluster means, OK? All right, z(i) is our latent variable. All right. So let's take a look. What do you think? Share your thoughts in the comments below or on our Facebook page, @cnnbayarea. We'll feature the best in our weekly Newsquiz. does EM actually do here? So what is EM? EM is very general. You can instantiate it, right? So Q(i) of z is going to be equal to P z( i) equals j given x(i), and so forth, OK? Now, what actually happened here when EM actually happened? Well, that's what EM is supposed to do. So what does it mean here? It's going to say that P z is equal to Q z, and Q z is the same as P z, so P z equals j. we wanted to understand-- what was the probability? This says, the probability that i, the i-th component, belongs in j given what we've observed about x(i) and what we know about the cluster shapes and their frequencies. So if you remember, we had this diagram that I drew quite poorly. I drew it quite poorly because I didn't know what I was talking about. We wanted to know what the probability was that i belonged in j, and what it would look like if it did. the last time that said we had these two bumps, which were our two Gaussians, let's say, in one dimensions that looked like this. This was mu 2 sigma 2 square. And the question is, you give me a point here. This is mu 1 sigma 1 square. The last time we had this, it was mu 1 1 1/2 square. This time, it's mu 2 1/4 square. It's mu 1 2/3 square. my x(i) How likely is it to belong to 1 or 2, to cluster 1 or2? Right? That's basically what we're asking. What's the probability that at this point, this i-th point here, comes from1 or 2? Now, remember, if we just looked at this, and these two, how likely are they to be one or two? We want to know if they're the same person, or if there's a difference between the two. distributions were-- or the phis were equal-- that is, both sources were generating the same amount of information. But if we knew, say, on the other hand, then we would say, oh, it's probably much more likely it belongs to this function, cluster 1, than cluster 2. But we don't know this, so we can't say for sure which function it is in the first place. It could be cluster 1 or cluster 2, or it could be both. phi 2 was hugely bigger than phi 1, right-- a billion points came from the second source, and only one point from the first source. We'd probably say that it's more likely that it would go to this, right? It would certainly boost its probability. So now the question is, what will phi 2 do? We'll have to wait and see, but we think it would be a good idea for it to go to the right place. We'll see. is, how do we automate that reasoning? And that is Bayes' rule. More likely in 2. It just weighs those two probabilities and tells us what should happen. That's it. We ran through it. This is all we did last time, Bayes’ rule. And that’s it. That’ll be the next part of the series. We’re going to go through it again and see if we can figure out how to make it even better. exactly those calculations last time. All right, let's take a look at the M-step now. We have to compute derivatives. I want to highlight only one thing here because it's something that causes people pain when they do their homeworks. So we're going to do the same thing as we did in the first step. We're doing the same calculations as we were in the previous step. The result is that we're doing exactly what we did last time in the second step. maximizing here over all the parameters, phi and mu and sigma, sigmas. So these are the sigmas, lowercase. And the notation above-- these are all theta or all theTA, right? So theta refers to all theparameters of the problem. We were breaking it out into phi, mu, sigma and sigmas -- theta, theta and theta -- and we were maximizing over all of them. That's what we're doing here. mus and sigmas and phis. So those are all the things we're observing. Everything that's nonlatent, that's observed, not hidden to us. And what we are maximizing over from our ELBO lower bound was this, sum over z(i) Q(i), theta over Q( i) of i, z( i), log P x of i,. Z(i, i, i), P(x) of x, P(X) of y, Y, Z(x, y, z, y) This whole thing, we're going to call fi.z(i), OK? This is fi of theta. It hides a ton in our notation, all right? So this thing is-- let's write it out because the gory details will help us. Oh, please. You have a question. Do you mind defining what it is? Do you want to know what it means to be a member of the family of the square root of the Greek word "fibre"? Do you know what that is? is latent and what is not? Yeah, so in our terminology, z is just latent. So I'm giving you the intuition that it's something that's hidden or not observed. But formally, it's just going to be anything that's a z. z is latent. That's our definition. Yeah. Please. So the concept of a z is that something that is latent is not observed or not visible. And that's what we're going to focus on here. We're not going to get into the specifics of that. This is exactly the instantiation of what we had above. And to make it concrete, I am.fi is just like the ELBO [INAUDIBLE].. Exactly right. We reasoned about this through ad-hoc reasons last time, but it is exactly that ELBO that we're now going to minimize with derivatives. We're going to use derivatives to minimize the amount of data we have to work with. We are going to be using derivatives to reduce the size of the data that we have. "I'm going to write out exactly what fi(theta) is so that you can see what the derivatives are that you will compute on this thing, because right now you're either going to waste a bunch of your time or something will snap in your head and see how these things put together" "Right now, you're going to wasted a bunchof your time. I'm going to write out what fi( theta is so that you can see what the derivatives are that you will compute on this thing" now, it's probably pretty mysterious to you. Like, there's ELBOs, and there's P's, andthere's Q's. And you can just write this thing down and compute its derivatives, and that's what you do. I mean, that's how this whole method works, just abstracted three orders of magnitude more than it used to be. That's how it works now. It's just a different way of looking at it, but it's the same thing. should be, OK? So let's see that piece. Oh, please. Sorry. The z(i) that said we're summing over? Yeah, that's going to be-- so I'm just using that notation to make sure it's clear that it depends on the i. It's actually just a z that you'resumming over. And that's what we're going to do here. We're just going to sum it over. We don't have to do it all at once. it's summing over, for example-- like, we're imagining that it's discrete to make our notation a little bit nicer. It would be Summing over all of the different clusters that are possible there, all the different sources. How likely are you to be in cluster 1, 2, 3, 4, 5, and so on? That's what we're trying to figure out. We want to know how likely it is that we're in that particular cluster, and how many others we could be in. so on? You could also-- we'll see later-- replace it with an integral if you had something really fancy that was there. Yeah. [INAUDIBLE] z, to similar to what phi does, like if phi is greater than [INAudIBLE],, what what Phi does. So, if you have a continuous distribution over the hidden states, you could use that. Yeah, you can use that, too. You can use it to replace an integral. sorts of [INAUDIBLE]? Ah, yeah. So it is, in fact, because of this right here, which I kind of glossed over. Q(i) is exactly setting that-- is setting this function. So I glossed. over this really, really quickly because it was the same calculation we did last time. P z(i), P z (i), Q z( i), P (i) P(i, P z) P z, P (p) P (z), Q (p, z) j-- to compute that, remember, we expanded it by Bayes' rule. We had, if you knew you were in a cluster, how likely is the data point? And then we had a term that said, how Likely is the cluster? And those were the two parts of the equation. It's a very complex problem, but we think it's a good one to try to solve. We'll have to wait and see how the results pan out, but I think we'll have a pretty good idea of what it means. functions that we put in and broke down by Bayes' rule. It's exactly the same. You've got it perfectly. Yeah. All right, so let me write out this monstrosity just because it will be potentially-- it has been in the past educational. Who knows if it's educational in the future, though? I don't think it's going to be educational. I think we're going to have a lot of fun with it. We're not going to get very far with it, though. and the future being, like, two seconds from now? All right, I'm going to use a notation, and hopefully it doesn't confuse you-- Q(i) equals z j. So this is the piece there. This is the weight. This w(i), is the same w( i) we had before. I'm sure you're sure you've heard this before, but it's a good way to start a conversation. It's a great way to get to know people. intimately familiar with all the notation I used in the GMM lecture, but it's the same w(i)j that we had before. It's the weight that summarizes this probability, just so I don't have to write that whole thing out, OK? All right, so fi of theta is going to be. The weight is the sum of the probabilities of different things happening at the same time. That's what we're going to call the probability of a certain thing happening at a certain time. equal to the sum over j-- because now I'm summing over the cluster centers, right? The z(i) notation was still very abstract-- wj(i), which was summing. over this part here, log-- and help us all, 1 over 2 pi-- this is a covariance, 1/2. This is the exp of Oh, Oh, oh, oh. Oh,oh, oh! Oh,Oh,oh. Oh! Oh! oh,oh! I decided to write this in four general things. Why do I care about that? Oh, I see why. OK. Transpose sigma inverse x(i) mu j times phi j. Oh, that hurts. All right, let me scoot. So much better here. On a whiteboard, that's on a white board, that’s on a blackboard. On the blackboard, it's on the black board, it’re on the whiteboard. really catastrophic-- phi j, OK? Let me make sure the brackets are clear. I'm going to highlight the brackets like it's a syntax editor. So make sure they're all there where they're supposed to be. This blue goes with that one, so, OK, great. Ah, and that means I'm missing. Ah,. OK, Great, that means he's missing. And that means it's really catastrophic -- phi J. OK, that's what I was going for. This is the Gaussian, remember, from our model. Let's go back up here. Oh, and this whole thing is unfortunately-- snap 2? Yeah-- over w(i)j, right? That's just this piece is this piece. This piece here-- this is the probability. Perfect. All right. Not so bad. Sorry for the log.a log. A log is a log of the probability of an event occurring. It is the sum of the probabilities of events that occur. all the scrolling. This is a Gaussian distribution with center j. I did use a higher dimensional covariance because it's something you're going to have to compute. So I've gone from 1D to higher dimensions. The notation doesn't change except for this is what the distribution looks like in 2D. The distribution is the same in both 2D and 3D. It's the same as the distribution in 1D and 2D with a different center j and a different covariance. Gaussian looks like instead of a square. You know that, right? And then there's the phi j, which is just multiplied times this horrible expression. And this exp parentheses is so I don't have to write it in superscript. Just expo the function, just a bad habit that I have. I'm sorry, I'm so bad at this. I'll try not to do it again, though. I promise I won't do that again. I swear. always use brackets for this. It's historical, and I would love to beat it out of myself if it were possible. Please. Does the covariance depend on j? Right now, the covariance does not depend on z. In our model, it depends on which of the two variables is the "covarianance" of the whole system. It is not the same thing as saying that the vacuum of the entire system is the same as the corvance of that particular variable. cluster, right? So it depends on j. Sorry, I just want make sure I understand what you-- That's it. Yeah, so I think if you-- it means it depend on j, yes, the covariance could have different shapes. Some could be long and skinny, some could be shorten and round. It could be a lot of different things, but that's what it would look like if it was a cluster. It would be a big, big cluster, but it would be different. Yeah, those depend on j. So this thing here is a very polite way of saying, this guy here should depend onj. Yeah, good catch. All right, so now we can compute some fun derivatives, OK? So let's compute mu j of fi of theta. We have to estimate it. We've got to estimate. We're going to do it now. OK? We're doing it. So now we have to compute mu J of fi. Let's do it. the mean, right? Now. And I'm going to do it-- actually, I'mgoing to do something slightly harder. So apologies if you wrote that down. Let's do this. It'll be just one extra line because it's all linear. I'mGoing to sum over all the data, 1 to n. OK. Now. The mean is 1 to 1, or 1 to 2, or 2 to 3, or 3 to 4, or 4 to 5, or 5 to 6, or 6 to 7. So what this becomes is sum equals 1 to n. This is over all the data. I get mu j here, mu j, times-- and then it's going to be wj, and I'm going to drop terms inside the log that obviously have nothing to do with mu j. sorry. I'm not sure why I'm doing that, but I'm just going to do it. I don't know what the purpose of it is, but it's not going to make any difference. T sigma inverse j x(i) mu j. All right, and so just so you're clear what's going on here, the log turns these multiplications into additions. So when I take derivatives, this doesn't show up anywhere because sigma doesn't depend on it-- sorry it doesn'tdepend on mu. And this is what happens when I add two numbers together. It's called a "log" and it turns the multiplication into an addition. The log turns the multiplications in this case into additions, and this is how it works. doesn't depend on mu either, so I'm left with these terms. Please, go ahead. Oh, what is the physical meaning of the fi here? fi? fi is just the term here in a a function. It is the likelihood function after we've picked Q at the particular iteration. So it's fi is the chance that Q will be Q at that point in the iteration. It's the likelihood that Q won't be Q when Q is Q, and so on. just notation so I don't have to write this monstrosity every time. OK. So when wj has something to do with mu? Actually, it doesn't. It is exactly the ELBO. fi is the i-th ELBO and mu is the j-th. OK, so wj is wj, mu is mu, and fi is fi. It's exactly the elBO. It has exactly the same name. I'm not sure why I kept it. Yeah, w doesn't have anything to do with lambda j. So it should be crossed out. Is that true? Let me show-- let me see something crazy here. No, it shouldn't. But it will be-- sorry, I see what's going on. This is what's happening. It will be a mistake. It won't be a problem. It's a mistake that will be fixed. It'll be fixed in a few minutes. It doesn't matter what it is, it's just that it's not the same. going on. This is 1/2, and this is a minus. w(i)j is multiplied by it. It's going to be this times this thing plus. Yeah, sorry. Thank you for the notational issue. Yeah. Cool. All right, we're in business. So what? So what is w( i)j? That's take the derivative of the log. That's what we're going to call it. That is what we are going to say. happens now? Well, some mechanics that almost certainly will introduce bugs and you will catch, and it'll be great. That's learning happening there and me making mistakes, OK? So when we actually compute this, this is going to be sigma j x(i) minus mu j. You computed this a bunch. You compute this a lot. You will catch the bugs and it will be great, right? Right? OK? That's OK? Right?" "Yes, that's right," he says. "That's right." of times, all right? So, yeah, all good. So when can we pull this thing out that's repeated? Because it's full rank, we can pull it out, and it's linear. And it doesn't change anything. So we want to set this to 0 and use that sigma j inverse. Sigma j inverse is the inverse of the sigma J inverse. It's a linear inverse, so we can use it to pull out the full rank. We can pull this out and it will be repeated. j is full rank. And that will become clear in a second why that matters so much, because when we pull it out, what do we get? We get here sigma j inverse times sum, which is an unfortunate collision, i equals 1 to n w(i)j x( i) minus mu j. i is the number of a person in a group of people in a certain order. The number of people that are in the same order as each other in the group is known as the rank of that person. equals 0, OK? But then because this is full rank, the only way that this thing is 0 is if it's identically 0, right? If this were non-full rank-- sorry, the j is in the wrong spot. That's extraordinarily confusing. Since this matrix is fullRank, for this thing to be 0, it would have to be identically0. But it's not. It's 0 because it's in the right place. It has to be. Be 0 means that this blue part is identically 0. And so what that tells us is mu j should be equal to sum i w(i)j x(i), over sum w( i)j. That was before, OK? So, so far, nothing happened. We estimated the means by simply averaging their weighted averages, which is how we arrived at mu j. mu j is the number of points that are equal to the sum of the squares of their weights. and we computed this before, and it's just a matter of computing the derivatives. The one that I actually care about showing you, by the way, is phi j, so let me just jump to that because we only have a minute or two left. And I want to show it to you. And it's the one I really want to talk about, so I'm going to jump to it now. I'll be back in a minute to show you how to do it. you what happens in phi j. Would you mind showing [INAUDIBLE] scrolling up? Sure. No, wait. I just want the last one. OK, sure. Also I would say, ahead of time, I do post all the notes online. Please feel free to take a look at my notes online before you go. I'll be back in a week or so to talk about my new book, which is due out in October. I hope you'll join me. our reference to those notes too. They will have potentially fewer typos than me trying to answer questions, draw, and generally be distracted. They still do have typos, though, so always look at the notes. Can't focus that long. I have to read the notes, though. All right, let me just let you just know that I am still alive and well. I'm fine. I just need to get back to work. I'll be back in a few days. show this one thing, phi j is constrained, OK? So phiJ is constrained. I just want to remind you of something that you probably learned in high school or freshman year in calculus. I don't actually know when anyone learns anything. Anytime I say something like that, my brain goes into overdrive and I start thinking about what I just said. It's like I'm trying to remember something that I learned in calculus, and I'm like, "Oh my God, I just learned something!" students always get upset with me, so I should just stop. But I assume you've seen it before this moment, how about that? You need a Lagrangian, OK? No, you haven't seen it? That's fine too. If you want, I'll post notes about how to compute Lagrangians as well. If not, I will just say that you need to know how to do it. And that's OK too. I'll just stop now. you haven't seen this before, this will trip you up in some way. So when you compute the derivative with respect to phi j, what happens is you're going to get something that says you have this weighted sum w(i)j times the derivative of phiJ log phij plus--. It's a very simple formula. It doesn't make any sense, but it's a good way to work out how to do it. It will help you with your math. so if you just take this and compute the derivative, it doesn't account for the constraint. So you have a bunch of numbers that must sum to 1. So if you think about you're on a line-- let's say that you're optimizing on a lines, right? If the gradient-- like, if the gradient is like, like, 1/2 or 1/3, it's not enough to just compute a derivative. It's got to be at least 1/4 of the way. let's say that your points are on this line, and you're saying, I want to optimize here. This condition that you could imagine for an optimal solution is the gradient's identically 0, right? It vanishes. That's good. But what if the gradient is perpendicular to the line? That's a point. And what if it's not? Well, that's another point. It's the same thing, but it has a different shape. And that's the key. Like, its wants to push you only perpendicular and has no component moving you along the line, right? In that case, this is still a critical point. It's still potentially a minimum. Does that make sense? Because it's not telling you that there's a minimum to your left and right. Because it doesn't tell you that that's what it wants to do. It just wants to get you to a point where it can push you. It doesn't want you to go further than that. It's along the line, OK? So the question is, how do you encode that information that you want to kind of screen off information that's orthogonal to the line? And I'll write up a little note to show this whole thing. What you do is you introduce this thing called a "line" and then you screen off that information. It's along a line, and it's in the middle of a line. So you screen that information off. And then you use a line to screen off other information. And you do it in this way. Lagrange multipliers. And if you haven't seen them, don't worry. These are super easy to teach. And this multiplier-- it's not obvious in this formulation what it's doing, but this multiplier is basically the thing that screening off. It's just an extra term here. And it's basically the same thing that's going on in the background of the video. It just looks like a different way of looking at it, but it's really the same. And that's what this multiplier does. things that are orthogonal to these constraints, OK? So this constraint here says, theta j is equal to 1. And you set this constraint equal to 0, this term equal to0. And it says, if you're going off in a direction that would not change any of their values, you're not going to change them at all. So, for example, you could go off in the direction of 0, and you'd be going in the opposite direction. That's what we're doing here. that's OK. You get to screen that off. And I'll make that geometric intuition. I'll just post a one-page write-up for you. Please remind me in the thread, and I will definitely do that. If you don't do that, you'll get the wrong answer. That's also a motivation to learn. I'm sorry, I didn't mean to offend you. I was trying to help you out. I don't want to offend anyone. I just don't know how to do it. It's just telling you, you have to normalize them in some way, right? Now, since-- in this case, we can do it in an ad-hoc way. And so what ends up happening here is you get something that says, I get sum i goes from 1 to n w(i)j over phi j plus lambda equals 0. And this implies that phi of j is equal to 1 over Lambda sum i. And that's the correct normalization. out the-- divide everything by 1 since this is just-- this sum is equal to 1. I divideEverything by 1 here, and that tells me that Lambda must beequal to 1 over negative n. This implies Lambda equals negative 1/n. It's just normalizing. It doesn't have to be negative. It just has to be equal to one over negative 1. That's all that's needed to make it work. The rest is just a matter of adding up the numbers. It's just doing the average, which was weird to look at before. But that allows us to compute all the things in the way we would expect. Here, it's totally natural. So if you don't get the general rule that I'm trying to tell you, the reason I'm Trying to. Tell us what you think in the comments below or on Twitter @CNNOpinion. Back to the page you came from. Click here to read the full interview with CNN's John Sutter. tell you is I think we make you use it at some point, this rule. So just check. It'll come up on a homework. I don't think comes on an exam. But just flag something when you have a constraint. When you've got a constrained probability distribution, you have to have to use it. You have to be able to do it. It's not easy, but it's a good way to start. You can do it if you want to. But it's not always easy. use a Lagrange multiplier. That's all I care about that you understand. In this case, it makes total sense, though, because these numbers have to sum to 1. So if you don't have a normalization constant here, you're adding up a bunch of numbers which individually sum up to n, and you're not getting the full effect of the Lagrange Multiplier. If you have any questions, please contact us at jennifer.smith@mailonline.com. right? The sum over all of them is n. And this is just the principle that tells you, you have to normalize them by this n factor, OK? So all I care that you take away, if you've seen this a thousand times, I don't care. I just want you to know that it's a good idea to try and normalize these things in some way. And I hope that you'll take away some of the lessons from this, as well. When you minimize a function that's constrained to make sure you use Lagrange multipliers. I will put up a little tutorial about them. You do not need to use them. If you've never seen this before, I just want to flag for you. I hope you had a nice rest. I'll be back in a few days with a new post about how to use the new features in your game. Back to Mail Online home. Back To the page you came from. Click here for more information. to spend a bunch of time on them. It's just if you see some challenge where you're actually supposed to do it, just have a little light bulb to go off that says, OK, I've got to look up how toDo it in this. If you're not sure how to do something, just look it up on the Internet. It'll help you get the hang of it. If it doesn't help, just read the instructions on the back of the book. case. That's all I care about, OK? And you'll trace through it in the notes. Please. So I have two questions. [INAUDIBLE] minus [INAudIBLE] equals to 1 because that is Lambda equals to minus [inaudible].. Yeah, it equals tominus-- sorry, equals minus 1/n because there's a negative sign everywhere. So this minus Lambda is going to be equal to 1/ n. Maybe that would be less-- yeah. just swapped them, and even that's backwards from how I should do it. I didn't see that. Thank you for the catch. It's just supposed to average out so it looks the same. Any questions about this? All right. So also, why is it [INAUDIBLE]? Because. Because it's supposed to look the same, and it doesn't look like it does. It looks like it's going to look a little bit different, but it's actually the other way around. it's a probability distribution. So again, the issue here is phi j is constrained by the model. So if we go back to this model, this is a constraint onphi j. So whenever you have a probability. distribution, a multinomial probability distribution, it's not just that the. phi i's are constrained. It's that the phi. i's can't be the same as the other phi i's in the same way. That's the problem. are nonnegative, which the constraint-- we're almost ignoring-- but it's that the phi i's equal 1. And so you couldn't, for example, set your probabilities to be 0.5 and 0.8, right? They have to add up to 1 here because it's a multinomial. So that means these phi j's-- we have constrained them to equal to 1. So now the question is, when we do the gradient descent, which is exactly the same.  gradient descent we did before, where does that show up? And it shows up in this extra term here, which is the Lagrange multiplier. This thing is called a Lagrangemultiplier. The multiplier itself is called that, OK? And this is the constraint put into the normal form. If you want to see the full effect, go to the bottom of the page and click on the image for a larger version of the image. The full image will be available in the next few days. haven't seen this before, it'll look quite mysterious. But what I was trying to do is I'm not going to teach you the Lagrange multipliers in this class. I'll put up something. But the piece is here that it gets you back to an expression which makes sense in this. It's a bit of a mystery, but it's a very good mystery. It'll get you back in the mood. It will get you into the mood for the rest of the day. setting. And you needed something to average over because these numbers sum up to something that looks like n. If you just compute it naively, you'll get something that doesn't make any sense. Yeah, please. I think the problem is that, in a lot of sense, phi j equals 1, and you need to make sure that's not the case in this case. And so you need a number that's bigger than 1 to make it seem like a bigger number. then the [INAUDIBLE] sigma of phi j's. No, no, there's no sigmoid here. A sigma for summation. In this setting, the sigma is out here. Yeah, but the other one, like since phiJ equals 1-- I think what's confusing you maybe is this, that these are not not sigmoids, they're phi J's. I think that's what's wrong with you. I'm sorry. the same j's. No, but [INAUDIBLE]. Which line? Just says phi j i. Oh, oh,Oh, I see. I see, I See, Isee. Sorry, sorry, sorry. Yeah, I was talking while I was saying. Thank you for the clarification. That was really helpful. Apologies for that. Yes, it's this. It's this and this and that and this. And that. And this. and that. constraint here. Sorry, this is the constraint that was in our head. Yeah, and it just makes a mysterious reappearance here, all right? All right, awesome. OK, so what is the message that I want you to take away from this? Two things. First message is GMM is an EM, and the second message is that GMM can be used in a variety of ways, and that it is possible to use GMM in a number of different ways. algorithm, OK? That's really one of the pieces that is there. In the next lecture, we're going to see a different example, which is called factor analysis, where z has a very different form. And it's meant to constrain the problem. And this is interesting because we'll see that in a different way in this lecture. We'll be looking at factor analysis in a new way, as well as factor analysis with a different type of algorithm. And that's going to be very interesting. in a different way. We went through in this lecture a couple of different steps. We started with that convexity piece so we could get an intuition for what these functions look like. And then we went through the EM algorithm, which is an algorithm for solving problems in a computer program. We didn't want to use convexities. We used concavity. In this lecture, we didn't useconvexity. We use concavities. In the next lecture we'll go through a different part of the algorithm. which we formalized as kind of back and forth with using these curves over time. Once we had those curves, what was happening was we would pick and optimize on those curves. The Q(i)'s played a starring role. Those became our w's here, those became our s's here. We're going to continue to work on this until we find a way to make it even better. We'll keep working on it until we know what we want to do, and then we'll go from there. and they kind of add nastiness to all of the equations but not a tremendous amount of nastier, right? They just add little weights and expectations everywhere. And then we ran exactly the kind of standard supervised machine learning, if you like, or the stuff that you've been doing for years. And we found that it worked just as well as we thought it would. And so, we're going to use it for the next few years. We'll see how it works. MLE for the entire quarter on those properties. Then we introduced a lot of typos to keep you on your toes. No, I introduced a ton of tyPOS because I was talking while I was writing, and I shouldn't have done that. And so then we saw the two things that we had been working on for a long time, and we were able to work on both of them at the same time. It was a good test for us to see what we could do. that I cared about to highlight. One is you know how to find means, and these are just weighted means. You should run through that calculation to make sure you know what you're doing. I'm almost positive we will ask you to do it at some point, and I want you to be prepared. I want to know how you do it, because I'm going to have to ask you in the future. I don't want to be surprised if I have to do that, because it's part of the job. in the near future. And then the second thing that I would tell you to do is when you have constraints, you have to know how to optimize them. You don't need to know the general theory of how you optimize against nonlinear constraints, but you should review how to. You should reviewHow to Optimize against Non-Linear Constraints. How to optimize against Nonlinear Constrained Constrains. How To Optimize Against Non-Lunar Constraint. do this when you have something that sums to 1. It's not more complicated than what I wrote here, but make sure independently you go through it and ask questions. I'm happy to point you to different resources. In the next class, as I said, we'll talk about how to use this information to help you with your work. We'll also talk about the best ways to use it in the real world, and how to apply it to your own work. I'll be happy to answer any questions you have. what we're going to see is this notion of factor analysis. And that is going to tell us how to apply EM to a different kind of setting, which, at first glance, will look kind of impossible to do without a latent variable model. It's a pretty interesting scenario, and it's going to be pretty interesting to watch. It will be interesting to see how EM works in a different setting, and how we can apply it to that setting. It'll be pretty exciting to see. And I think that's all I want to say. Any last questions before we head out? I'll stick around for a couple of minutes as usual. Thanks so much for your time and attention. I'll be back in a few minutes to talk to you about the next episode of "This Is Life with Lisa Ling" on Sunday at 8 p.m. ET on CNN.com/This is Life With Lisa Ling. Follow Lisa Ling on Twitter @LisaLing and @CNNOpinion.