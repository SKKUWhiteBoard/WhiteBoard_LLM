Machine learning is about how to acquire a model and acquire the parameters of a model, from data and experience. In the next few lectures, we're going to work through a sequence of different takes on machine learning that are going to highlight different subsets of the big ideas on this topic. We'll see today exactly how data kind of goes through the mill and gets turned into a model. And we'll see more in-depth examples and more structured examples of these kinds of problems later on when we talk about applications. Machine learning can be used to detect diseases, fraud, and other problems. It can also be used for classification tasks like spam or ham classifications. The most widely-used application of machine learning is classification, but there are a lot of other applications, too. The next episode of Machine Learning with John Defterios will be on Monday at 10 p.m. ET on CNN. For more, go to www.cnn.com/machine-learning and follow John on Twitter @JohnDefterios. We're going to talk about model-based and model-free learning. Model-based learning involves building a model from our data, and then doing inference in that model to make predictions in the world. After today we'll look at the model- free methods. We'll also talk about how to learn the parameters of a models from data once we've decided it's structure. And then, the thing we'll mostly talk about today is how to teach a model to recognize a person. Spam detection is, in some ways, a very poor example of a canonical classification problem. Over-fitting means fitting the training data very closely, but not generalizing well. Under-fitting, where you're just like, I don't know what's going on, I'm just guessing randomly everywhere, is not going to work very well. Spam is being generated by people who are trying to defeat spam filters. And now you have spammers who are primarily looking at information or contact information. the thing we're trying to do is to fit a curve to this data. You can always fit your data more. It's about fitting your data to the point where the patterns that you are capturing are ones which generalize to test. This is definitely over-fitting. Over-fitting shows up not just on these continuous functions. It also shows up, for example, in a hypothetical digit classification, we might say, here is an image I've never seen before. Let's use Naive Bayes to classify it. So what would we do? We'd do our running total. over-fitting usually shows up as sampling variance. To do better, we need to smooth, or regularize, our estimates. How are we going to figure out what a random variable is? We can ask a human. You can go to a doctor and say, hey, I'm building a classifier. What fraction of people with meningitis will present with a fever? And a doctor can give you a guess, right? It may be qualitative. You could use training data. And this is basically what learning does.