OK, cool. So I guess last time we have talked about some of the kind of the bigger questions, the conceptual kind of bigger questions in deep learning theory. And today, we are going to start talking about the optimization perspective. So, let's talk about the materials today. We will start by talking about deep learning from the perspective of a machine learning perspective. And then, we will move on to the next phase of the discussion, which is about the theory of machine learning from a mathematical perspective. in deep learning for two lectures. And here, I guess I'm going to explain what optimization landscape means. It really means the surface of the loss function, but I guess you will see. So we are going to introduce some very basic things about optimization, but the main focus is in deep learning. We will be talking about optimization landscape for the next two lectures, which will be on November 14 and November 15, in New York and New York City. The lectures will be held at the New York University School of Computer Science. The more focus is to analyze what the functions you are optimizing look like so that you can use some trivial or some standard optimization algorithm for it. So you don't need any background about optimization. You probably not about how do you update. How do you design algorithm. The most important thing is to make sure that the function you are trying to optimize looks like it should look like the function it is supposed to be optimizing for. So that you are able to make the right decisions for the right function. need to know what gradient descent is. There's no any kind of concrete requirement about the details like what momenta look like, what stochastic gradient descent exactly is. You probably need to know the algorithm is, optimization algorithm is. But you don't have to know all the details about the algorithm, just the basic idea of what it is to make a gradient descent algorithm. The algorithm is called gradient descent and it is a type of optimization. It can be applied to any type of algorithm, but not all of them are the same. don't really necessarily have to know them. OK, cool. So I guess the question we are trying to address-- just to quickly review to come back to the last lecture is that why many optimization algorithm. So many Optimization Algorithms are so difficult to understand. The bigger question is why do we need to know so many optimization algorithms? The answer is that we don't really have to. We just need to be able to understand them. We need to have a better understanding of them. designed for convex functions. But why they can still work for nonconvex functions? So why do they still work? It's not like gradient descent or stochastic gradient descent can work for all functions. It's actually, pretty well in practice for non-conveX functions in deep learning. So why does it still work in practice? We'll have to find out in the next few weeks. Back to Mail Online home. Back To the page you came from. that you may optimize. Definitely, there are many functions that they cannot optimize. And there are these examples in many areas of research. But in machine learning, typically, people assume that you can-- people assume and also somewhat observe thatyou can optimize your function pretty well even though you can't optimize it. That's not always the case, though, and that's what we're trying to find out in this project. It's a step in the right direction, but it's a long way from there. Even in machine learning, there are atypical case or outliers, whatever you call it, especially if your parameterization of your model is very complex or kind of somewhat weird. So you could face some difficulties. For example, one simple example is that, if you.function is not convex, it could mean that your model. is not very good at predicting the outcome of a particular event, such as an earthquake or a tsunami. If you have a very complex model, you may have to change some of the parameters. have a very deep network, like a feedforward standard deep networks, then it's actually pretty hard to optimize. Because sometimes you have vanishing gradients. Sometimes you have exploding gradients, so on and so forth. However, some of these are solved by changing the architecture, which changes the optimization landscape. Anyway, anyway, Anyway, Anyway,. Anyway, this is a very, very long list of things to do with deep networks. If you have any questions, please email us at jennifer.smith@mailonline.co.uk. Nonconvex functions in machine learning can be optimized pretty well by gradient descent or stochastic gradient descent. We are trying to understand why we can optimize reasonably well. So that's the question. We're trying to figure out how to do it better. And we're going to try to figure that out in the next few weeks. We'll let you know what we come up with in the coming days and weeks. Back to Mail Online home. Back To the page you came from. And maybe just before talking about more details, let's first quickly review kind of like what gradient descent is just in case. This is very quick. I'm just going to define some notations here. So suppose g theta is the loss function. Like I said, I'm using g here just for the purposes of this article. And I'm going to use g here for the duration of the article. So let's go to the next part of the talk. The next part is the next section of the presentation. The last section is the final section. because I want to use a generic letter instead of use l, right? So l probably should be a better letter, but here I'm using something that is more generic. And the algorithm is just something like sets 0 is some initialization. And you have something like theta t plus t plus T plus T, which is like sets t and t plus sets 0. So that's what I'm going to do here. I'm just going to use sets 0 and T and T. 1 is equals to theta t minus eta times the gradient of g of theta T. This is gradient decent. And you can have stochastic versions of it. Many of you probably know them. And I'm going to list a few facts just to kind of motivate the discussions here. I'll be back in a few days with a new post on the topic of "Theory of Stochastic Gradients" and "Stochastic Stochastics in a Non-Stochastics World" I'm looking forward to it. The first fact is a fact, or it's just some observation. So maybe let's call it observation. The first observation is that so GD cannot always find local mean or global. So the first fact of a nonconvex function is the fact that it can't find global or local mean. That's a fact. And the second fact is that it's not possible to find a global mean. So that's an observation. And then the third is that there's no global mean, so it's impossible to find global global. minimum, right? This is for continuous functions. This is kind of obvious. Because depending on where you initialize, depending on how the function look like. And gradient descent will go rightward. And maybe it overshoots a little bit, and they overshoot. And so on and so on, until you get to the right place, which is the minimum. And then it will go leftward, and then rightward, until it reaches it. And that's what happens in this case. go back, so and so forth. But at the end of the day, it converges to this local minimum if your gradient is somewhat small, right? So it will stuck at this local Minimum and stay there. And even you have stochasticity, if your stochasticallyity is not big enough, you will not be able to get out of this hole. And so you will be stuck there for a long time, and it will get worse and worse until you get out. are not going to go to another local minimum-- or go to a global minimum, right? So clearly, you cannot hope that gradient descent work, you know, in the worst case for all possible nonconvex functions. So finding global minimum of general functions is NP-hard. This is just really saying that it's computationally intractable to find global minimum. But just to clarify what does that mean, that means that there are no global minimums for general functions. exists a function that you cannot solve. But it doesn't really mean that there is no subset of functions that you can easily solve, right? So for example, a convex subset of function can be solved in polynomial time. And I guess the observation four is that objectives in deep learning are to solve convex functions, as I said. And obviously, this is opposite observation. So gradient descent can solve convex functions, asI said. But gradient descent cannot solve all possible functions with gradient descent. is nonconvex. You have to-- it's probably kind of trivial to see that you cannot prove that they are convex, but you probably need a little bit of calculation or some kind of constructions. And generally, they are not convex just because there are so many nonlinearities. Most of the convex functions we know are somewhat kind of simple functions. And as soon as you go beyond two, they become convex. It's almost trivial, but not entirely trivial. layers, it's not convex. OK? And observation five, I think I mentioned it. Gradient descent or stochastic gradient descent does work, so finds approximate, or even sometimes you can claim it's almost exactly, global minimum of loss. OK, so that's observation five. And observation six: gradient descent can be used to find a convex layer. So that's also observation six, and observation seven. And it's observation eight. And then observation nine. And so on. functions in deep learning. Of course, this is not a 100% rigorous statement because it depends on which loss function you are talking about, what [INAUDIBLE] what you have, and what architectures you have. But I'm just saying, for most of the cases indeep learning, forMost of the case in deeplearning, it's a good idea to have a loss function that can be applied to a large number of data points in a big data set. It's a very powerful tool. stochastic descent or gradient descent seems to work pretty well. And why they are finding the global minimum? So you know that because you know the loss function is nonactive, right? So suppose you run ImageNet or you do some kind of vision experiments. And you knowThe global minimum is the nonactive loss function of the image. The global minimum can be found by running ImageNet with a loss function that is not active. The loss function can then be used to find the globalminimum of an image. has-- at least the loss function is always inactive. And you can see how small the loss SGD or GD can get you. Or depending on whether you use regularization sometimes it could be a lot smaller or a lot bigger than you think. And often, the lossfunction is pretty small, something like y minus 2 or something like that. or depending upon whether you used regularizationsometimes it could have been a lot larger or smaller than you thought. or even a lot less than you expected. be y minus 4, y minus 5 depending on the situation. So you kind of believe that you get at least an approximate global minimum. Cool. So it sounds like there are some positive results, empirical observations. There are some negative results about the NP-hardness. So what's going on here? We're trying to figure out what's the difference between the two. We'll let you know when we know the answer. Back to Mail Online home. Back To the page you came from. or the intractability of optimizing nonconvex functions. The way that can reconcile this is really just that the lower bound, the impossibility results, is about worst case functions. So I guess, in my mind, the kind of view is that we are not optimizing worst case function, right? So, I think, the way to reconcile it is to think of it in terms of worst-case functions, not best-case ones. I think that's the right way to look at it. that you have a family of all functions, right? So in this family, the functions are super hard to solve, right, so like very hard functions. And there's also a small subset of functions that's called convex functions. These are not, we call them convex. And these are easy to solve. So gradient descent can solve them. But actually, we didn't identify all the functions that we can solve. There are actually more functions than convex function that gradient descent or some other algorithms can solve, and that's a slightly larger family in between. We cannot identify all the functions that are benign enough for us to solve, but we are going to identify a subset of the bigger than convex subset. So these are nonconvex. But some would say some would be more benign than others. And today, we're going to talk about these kind of functions. We'll start with a set of functions that is benign enough to be solved by a computer program. And we'll move on to a set that is not benign enough. have benign properties, some benign properties. And kind of the task is to figure out what properties that make them somewhat nice and easy to optimize. All right, so here's our plan for this lecture and maybe the first part of the next lecture. So the first step is that we need to find out what the properties are that make it easier to optimize a system. We need to know what the benign properties are, and then figure out how to optimize them. We'll talk about that in the next part of this lecture. we can identify a large set of functions that SGD or GD can solve up to global optimality. And two, we're going to prove that, as for some special-- some of the loss function, in machine learning problems belongs to this set we just identified, we can solve them. We can solve some special loss function in machinelearning problems. And we can prove that we can do this for many other types of loss function. We'll show that this is true for a large number of loss functions. this larger set of functions we just identified. So most of the effort will be spent on the second bullet. The first bullet, you do need to show why I SGD can solve this set of function. But I guess I would tell you what people can show in the first bullet. It's a lot of work, but I think it's worth it in the end. I hope it will be a useful exercise for some people in the future. I'm looking forward to hearing from you. The results are intuitive, but require a lot of backgrounds to talk about details. "I wouldn't tell you anything about all the details. The results, they're actually, in some sense, kind of kind of intuitive," he says. "But they do require a lots of backgrounds," he adds. "So you need to know a lot more than just the first bullet, right? ... I wouldn't say anything about the details, but I would say that the results are actually intuitive" "It's like a first bullet. It's kind of along the line of first bullet" about how to analyze these iterative optimization algorithms. So that's why we don't focus on that. We mostly focus on the second part, which is more about the statistical properties of those functions used in machine learning. OK, cool. So the very basic idea is the following. So it's very basic. It's very simple. It doesn't have to be complicated. It just has to be more or less accurate. And that's what we're trying to do here. We're not trying to make a big deal out of it. Simple.simple.simple: So basically, you say that-- so you know that gradient descent can find local minimum. This is somewhat easy to believe. But actually, there are some caveats about it. Simple.simple.: If all local minimum of f are also global, then this, too, means that GD can find global minimum, right? SoBasically, the first statement is about the first-- so basically, the set of f is the local minimum, and the second is the global minimum. functions that we are going to identify to be solvable by GD and SGD is just a set of functions with the property that all local minimum are also global minimum. And then we need to characterize when you show that the functions we are actually using in machine learning are solvable. We need to show that all the functions that are being used in the machine learning process are also solvable in the same way as the global minimums. We then need to identify when we are using these functions. have this property, right? Of course, not all problems have this property. We're going to show some very, actually, simple cases where we can prove this. But I guess, as I mentioned, there is some caveat about whether you can even converge to a local minimum. So this is actually somewhat nuanced. So I'm going to formalize this converging to aLocal Minimum. But, of course, I'm not going to prove any of the theorems here. So the next part is the convergence to local Minimum. start with some definitions to formalize it. So let f be twice differentiable. For simplicity, sometimes you can extend this to maybe only differentiable, maybe not twice Differentiable. And what's the definition of local minimum? I guess you have seen it already, but let's say f is twice Differentiated. AndWhat's the Definition of Local Minimum? Let's Say f is Twice Differentiable, And What's The Definition Of LocalMinimum? And What Is The definition of Local minimum? this in calculus class, right? So x is a local min of the function f if there exists an open neighborhood-- let's call it n-- around x such that, in this neighborhood n, the function value-- I'm using a lot of text here just to make it easier to understand. So in the neighborhoodn, all the function values in neighborhood n are at least fx. So that's the definition of local min. And so I guess, from the calculus class,. you probably know that, if x is the local min, it means that the gradient of fx is. Cool. But actually, it's not a local minimum, as you can see, right? Because if you change x2, you can make the function smaller in the neighborhood. Because x2 is the cubic function. So you can always make it smaller than 0 in the vicinity of 0. But you can't make it bigger than 0. You can only make it larger than 0 if you have a bigger cubic function than x2. So that's what we're doing here. We're changing x2 to make it a smaller cubic function, so that it's smaller than 1. The problem here is that, when the gradient of fx is 0 and also the Hessian is not strictly strictly 0.0, OK? So basically, from this example, you can kind of see that what happens is the following. So why this is a problem, right? So fundamentally, what's the problem here? The problem is that when the gradients are 0 and the Hessians are not strictly 0, it's a problem. It's a very, very serious problem. positive semidefinite. So suppose Hessian vanishes in some direction. So that means, in that direction, you don't have the first order descent. You don’t have the second order gradient. In that direction you are pretty flat, right? So if Hessian. vanishes, it’s just a positive semideFinite, it's just a. positive semidefinite. It's not a negative semidiffinite. it's a positive. semidifinite. that makes it tricky because then the higher order gradients start to matter. Because if your second order derivative is actually non-zero, it's curved, then the third derivative is always cubed by a second order derivatives. However, if your neighborhood is small enough, it doesn't matter as long as it's within a certain distance from the center of the curve. If the neighborhood is too big, it can't be used as a basis for any kind of curve. It can only be used if it's in a neighborhood that has a certain number of degrees of freedom. if your neighborhood-- but if your second order derivative is literally 0 in some direction, then a third order derivative starts to matter. So that's why local minimum is not only always a property of the first and second order derivatives. And when you look at this-- and once it happens, it's a big deal. It's a very, very important thing to keep in mind. It can have a huge impact on the way you live in your neighborhood, and it can also have a very big impact on how much you earn. becomes about the third order derivative or fourth order derivative, things becomes much more complicated. And actually, if you look at the hard instance of the NP-hardness or the intractability results about optimization, once you-- so basically, all the hard cases happens when you have to deal with the high-order derivatives, like fourth order derivatives. And then, basically, I know this is probably not making that much sense for all of you if you're not familiar with how you prove NP- hardness. But basically, you can invite hard instances of some kind of set instance to the fourth order of derivatives. so that knowing whether the first order derivative is a positive operator is equivalent to knowing whether you solve the set problem. Anyway, so this is only for those people who know a little bit about the computational intractability results. But the intuition is that higher order derivatives is higher than lower order derivatives, so it's possible to solve set problems with a higher order derivative than a lower order derivative. So that knowing if a higher-order derivative is positive is the same as knowing if you can solve a setproblem. just hard to deal with, especially higher than fourth order. And there's a theorem, which is the following. So the theorem is that verifying if x is a local minimum without any assumption of the local minimum of f is actually NP-hard, so that finding a localMinimum is also NP- hard. And so on. And on. and so on, and on and on, until we get to the point where we can't find a local Minimum, and then we can find a minimum. NP-hard. So I've told you that finding a global minimum is NP-hard, but actually, finding a local minimum is also NP- hard. And this is the caveat I was referring to. Because in most of the cases, if we talk to someone at random, like if you talk to me about the world, I don't know what the world is like. I'm not sure what it's like to live in a world that doesn't include the world that I live in. research, you know, you're making-- we will think about finding a local minimum is easy, right? In general, that's the right conclusion. But we have to consider these kind of pathological cases, which makes things harder. So how do we proceed? So if finding aLocal minimum is hard, then this plan doesn't work, right?" "There is a way to also remove some of the pathological cases as well so that you can find a local Minimum in polynomial time," he says. execute our prime. So this is what will happen. So here is a condition called strict set of conditions. And if you certify the condition, then you remove those pathological cases which requires high order derivatives. So-- sorry. Strict-saddle condition-- so in some sense, I guess I'm not sure whether it's a good thing or a bad thing, but it's the way it's going to be. I mean, it's just the way things are going to go. this makes sense before I define it. But generally, you are basically saying that you want to rule out. You want to say that you assume your function doesn't have this kind of somewhat subtle possible candidate of local minimum, right? So every point, whether it's a local minimum or not, is a rule-out point. It's a rule of thumb. It doesn't mean you have to be perfect. It just means that you don't want to do something that you're not sure about. not, can be told from examine only the first order gradient and the second order gradient, second order directives. So there's no set of cases in your function. So how do we formalize this? This is strict-saddle. I guess the paper to cite is Lee, et al. By the way, by theway, I'm not sure if you've ever heard of this, but it's a great idea. It's like a new way of thinking about programming. I think I wrote a book chapter about this kind of optimization thing for our book. So I can send that to the person who take the Scribe notes. And that probably help you to have some references. But the materials are not exactly the same as the book, so the book is not the same. It's a different kind of process. I think it's a good thing to have references. It helps you to know what you're doing. But it's also a little bit of a pain. you still have to do the Scribe kind of from scratch in some sense. OK, cool. So the definition of strict-saddle, I'm citing this paper just because it's not like every paper is exactly the same definition. The very, very original paper that introduced this term and this notion is 'Strict-Saddle' and it was published in the journal 'The American Journal of Letters and Science' in 1903. It was the first use of the term 'strict saddle' in the world. by Rong Ge, et al. in '15. So that paper has a slightly different definition, but I think this one, the definition in Lee, et. al., is a little more kind of easier to use for the future of research. So I think people are somewhat converging to this. So here here is the definition that I think is more appropriate for the next generation of research, and I think it's going to be used a lot more in the future. is the definition. So we say f is alpha, beta, gamma strict-saddle if, for every x in RD, it satisfies one of the following. So the first one is that, for some of the x, it just satisfies that fx, the true norm of x is larger than alpha, right? So we says f isalpha,beta, Gamma strict-Saddle if fx just satisfies fx. So, for example, fx is alpha-beta-gamma, gamma-strict-saddle. you see that, of course, some of the x satisfies this, right? So we have gradient space. And alpha, beta, gamma is not positive numbers. So these are not stationary points. They are not local minimum. OK? By the way, by stationary point I mean first order stationary, meaning those points with gradient 0. So if you can satisfy number one here, you cannot be a localminimum. You can think of alpha,beta, and gamma to be something super small or even close to 0. We just require them to be strictly bigger than 0 just for technical purposes. And the third condition, the third possibility, is that x is gamma-close to a local minimum, to aLocal min. Let's call it x star in Euclidean distance. The distance in matrix here, probably not entirely not entirely here, is the distance in square root of the square of the number of stars in the matrix. It's the same as the distance from a point to the center of the universe. important because we are not going to be very quantitative about this. So everything is polynomial. So it's not that important. So basically, this is saying that number one rules are some kind of local minimum. Number two rules aresome other kind oflocal minimum. But note that number number is not very important because it is just a number. It's just a way of saying that there are some rules that need to be followed, but they are not very quantitative. That's the way it works. one, number two doesn't rule out all possible local minimums because there are even local minimum which has gradient 0, which has 0 gradient and positive semidefinite Hessian. So there are also-- my bad. So one, two doesn’t tell you exactly whether a point is local minimum per se if you don't have this assumption. Because you can have a point that does not satisfy one or two. For example, if you have a points with gradient 0 and Hessian is PSD, right, it doesn't satisfy 1, 2. But it could still be not a local minimum, right? That's the pathological case. And this assumption, this definition, is basically saying that, if you don't satisfy one, two, then there's no this pathological case. You have to be close to a real local minimum, right? So that's what this strict-saddle condition is saying. Maybe let me take pause for a moment, maybe let's take a moment to think about this. I think it's a very, very bad assumption. I don't think it makes any sense. see whether there are any questions. Is alpha and beta positive? Yes, that's right. So this definition only makes sense when alpha, beta, and gamma are positive. I think-- right, I think right, right, Right, I'm looking at the query bar. I know it's empty. But if you have any questions, feel free to ask. I'll be back in a minute to answer them. Back to Mail Online home. back to the page you came from. yes, exactly. So alpha, beta-- cool. The third strict-saddle condition sounds hard to check. Yes, that's a great point. So you cannot check. There is no way you can check whether empirically your function satisfies this condition. So I think it's even-- I'm not 100% sure about this one, but it's a good one. It's even harder to check than the first two conditions. So Alpha, beta -- cool. Any other questions? If you are just given an arbitrary function, differentiable functions, you should then be able to check whether it satisfies strict-saddle. It should be as hard as finding a local minimum in some sense, right? So this condition is not something you need to prove. But I think you can prove that, if you are given a differentiable function, you can check if it satisfies this condition. This. is not a proof, but I think it's a good starting point. that you are supposed to check numerically. Of course, I know in many cases you cannot, but nobody can do. But I think the condition itself is not supposed for people to numerically check. This is something that you're supposed to prove theoretically in some sense if you can. But you can't, and I don't think you should try to do it. I think it's a mistake to think that you can do it if you're not sure you can, and if you are not sure, then you shouldn't try it. That's a good question. OK, cool. So let's see, what's-- OK, we have the condition, right? So now, what you can do is this. So here's a theorem. The theorem is somewhat different from what you might think. Theorem is somewhat similar to the one I just gave you, but it's not quite the same thing. So that's what I'm going to show you. And by the way, always feel free to ask questions just even as I'm speaking. kind of like informal just because I'm not-- it's pretty formal in the sense that all the bounds are correct. It's just that I wouldn't specify some of the details. So suppose f is alpha, beta, and gamma strict-saddle. Then many optimizers, for example, GD, SGD, if you use the GD,SGD, then you have a very different type of optimizer than you would have if you'd used a different type. That's what I'm trying to get at. written word correctly, and many other articles, like cubic regularization, I guess many algorithms can do this. So far as can converge to a local min with epsilon error in Euclidean distance in time poly d-- d is dimension-- 1 over alpha, 1 over beta, 1over gamma, and 1 over gamma. I'm not sure how this works, but it seems to be a good starting point. I've seen many other algorithms that can do it, too. over epsilon, all right? So this theorem is very coarse-grained. Of course, different optimizers have different convergence rate. But at least for the purpose of this course and this lecture, we are not interested in which one is faster. We are mostly just interested in whether it's polynomial time versus epsilon time, and that's what we're going to focus on in this lecture. We're not going to talk about which one's faster. exponential time. And the point is that, if you have the strict-saddle condition, then you can converge to a local minimum. You don't have the pathological local minimum, pathological cases. You can converge. to aLocal minimum in this polynomial time. All right. Now, let's go to the other side of the room and look at the other end of the spectrum. The other side is the other half of the scale, and we can see how the two sides interact. Strict-saddle is just because the pathological case is a saddle point, right? So when you have these kind of cases where the gradient is 0 and the Hessian is PSD, but not strictly positive, it's a strict saddle point.OK. So by the way, I think just to explain the name of strict-s saddle,. I think this is justBecause the pathological cases is a Saddle point. So when they have this kind of case, it is a strict Saddle Point. semidefinite-- so you have some direction where you can potentially have a negative curvature. This is called cubic regularization. What is the third optimizer inside the parentheses? That's a good question. I guess there are so many others. Cubic regularization is the condition that says that, if you are a saddle point, you can tell it's a saddle points from the negative curvatures. The condition is saying that, If you are. a saddlePoint, and you can. tell it’s a saddlepoint from thenegative curvature, then you are saddlePoint. one of the early work in 2006 by Nesterov. Many other people published papers on this. I think I can add more references in the Scribe notes, in the final Scribes notes, to cite some of the recent work on the subject. But there are many other optimizers. I published a paper on this, and many other people have done the same. There are many more references to be added in the notes, if you want to see them. I'm looking forward to seeing them. works. OK, cool. All right. And now, we can converge to a local minimum with these conditions. And if you make additional assumption to say that all the local minimum are global, then we are good, right? So basically, the next theorem is trying to says that-- and now, let's go to the next step. We'll see how that works out in the next few minutes. Back to Mail Online home. Back To the page you came from. Click here for more information. are global and you have the strict-saddle set of condition, then this means that optimizers can converge to global min. So here is a theorem that formalizes this. I guess I'm writing it a slightly different way not because-- just in some sense, I unpack it a little bit. I think it's a good idea to have a set of rules that can be applied to a function of a certain size, and then try to get it to converge to that size. thought that this either provides a slightly different way of thinking about this, or it's just more explicit. So basically, you say that you assume the strict-saddle condition, but let's rephrase all local minimum global strict-Saddle condition like this. So you say there exist epsilon 0, and tau 0, but there also exist tau 1, and Tau 2, and so on. And so on and so forth. It's just a way of saying that there are two conditions that have to be met. and c such that, if x in RD satisfies, the gradient is small at the epsilon and the Hessian is larger than minus tau 0. So basically, this is a-- what does these two conditions mean? This is saying that you are a somewhat approximate local minimum, right? So you are you are an approximation to a local minimum. And so on and so on, until you get to a point where you are at a point that you can't go any further. have not passed the sanity check for local minimum. But of course, you haven't passed the-- you cannot rule out the pathological cases, but at least you pass the first order condition. The first order gradient is small. You somewhat pass the second order condition approximately because the Hessian is about 1.5. It's not a perfect test, but it's a good one. It doesn't mean you can't do it. It just means you have to be a little bit more careful. somewhat big, almost kind of larger than 0. And then this is saying that, suppose you pass these two condition, then x is epsilon to the power of a c close to a global minimum. The power c is just to relax the conditions so that you can have square. It's just a way of saying that you have to have a power close to the global minimum to get a square. And that's what we're trying to do here. We want to get the power c to be close to 0. root epsilon-- for example, close or something like that. So then, actually, it's close to a global minimum of the function f, right? So this condition is just a slight different way to say that you have all local minimum global and strict-saddle together, all right? And then I know what the condition is, because I've been doing this for a long time. I know how to do it. I've done it before. I'll do it again. Many optimizers can converge to a global min of f up to, say, delta-error and Euclidean distance in time poly 1 over delta, 1 over tau 0, and d. Then optimizers-- again, the same set of optimizers which can converge. to local minimum. All right, so it's not all that simple, but it's a good starting point for thinking about how best to solve a problem. It's also a good way to learn more about how to solve problems. exactly the thing that we did for the strict-saddle. But if you think about it, it's basically the same statement. Anyway-- so cool. So we are basically done with the first part, so about identifying the subset of functions that are easy to optimize. But these are all local, so we don't have to worry about them in the same way. We just have to make sure that we have the right type of function to optimize for. We don't need to worry too much about it in the first place. minimum, global minimum functions. And next, we are going to show some examples where these kind of properties can be proved rigorously for machine learning situations. But these examples are pretty simple. They are not deep learning. So these are still roughly the best that people can do in some cases. But they are not as good as the deep learning examples we have seen so far. We will show some more examples in the next few weeks. We hope you will join us for the next part of the series. sense. So this is just to give some examples for which these kind of properties can hold. OK. So next, we have two examples. The first one is the PCA or matrix factorization. And fundamentally, this is more or less the same as the linearized network. Even though, if you want to, you can do it in a different way. You can do the same thing in a more complex way, for example, by using a different type of matrix or a different kind of matrix. do linearized network, there is a little bit more things to do beyond that. And the second example I'm going to give is matrix completion. This is an important machine learning question by itself as well, right? So before deep learning, this was one of the most important topic maybe maybe before deeplearning. It's a very important topic. It is a very powerful tool. It can be used for a lot of different things. It has a huge impact on the way we think about the world. in machine learning, like especially if you think about nonlinear cases. And now, still I think it's used in the recommendation system. So we're going to talk about that. OK, cool. So any questions so far? I guess let's talk about PCA first. So I guess I'll maybe more precisely say that PCA is used in a number of different ways, but it's also used in other ways as well, like in recommendation systems, for example. So that's what we'll talk about first. say matrix factorization. So we are assuming that we are given a matrix M in dimension d by d. And we want to find the best-- let's talk about the rank one case. So in this case, the best rank one approximation is basically the eigenvector times eigen vector transpose up to some scaling. So this is not a hard problem, right? So you can just run any eigen vectors solver to find it and then scale it properly. Then you get the bestrank one approximation. But just for the purpose of this class, we are interested in this nonconvex objective function, right, which is literally interpreting this in the most straightforward way. So you say I'm just literally finding this back best rank one approximation. I know the best rankOne approximation should be symmetric. But for the purposes of the class, I know it should be non-symmetric. So I'm trying to find this back back to a symmetric non-asymmetric approximation. So I'm just trying to find M. Let's find a vector x such that M minus xx transpose in Frobenius norm is the smallest. So you are approximating M, and the matrix M was the matrix xx transposing, right? So xx Transpose is the rank one approximation. And you are looking for M. You can find M by looking for the ratio of M to the square root of the square of the number of transposons in the matrix, or the ratio between M and xx. measure the error by Frobenius norm. OK? And this becomes a nonconvex objective function because you have a quadratic term here. And then you take the square of that. It becomes degree four polynomial, and it's non Convex. Right. And our goal is to show that, even though it'sNonConvex, all of it is still true. All of it still is true, even if it's not a convex function. local minimum of this g are global minimum under the assumptions that we have mentioned, so like rank one, PSD, so and so forth. OK? So I guess I think I forgot to pass a figure here. So if you look at the one-dimensional case, this function looks like this, so one dimension. So d is 1. Then you just have a scalar, m minus x squared squared. This is our function, g of x. And there are two local minimum. And they are both global. minimum because there is some symmetry here. And if you have a higher dimension, it becomes a little bit more complicated because you have actually, in some cases, not only necessarily one, not necessarily only two local minimum. Or I guess if it's rank one, there are only twolocal minimums if you're in rank one. If you're rank two or three or four or five or six or seven or eight or nine or ten, you have local minimums that are not necessarily one or two. minimum, but it looks more complicated. But generally, you have some kind of rotational kind of symmetry here to make this happen. OK, so let's talk about the proof. So how do we prove this? So as you can imagine, the proof is pretty simple. The plan is very simple. It's very simple to prove. But it's not easy to prove, and it's very difficult to understand. But we'll get to that in a minute. We'll show you how to do it. You first find out all stationary point, the first order stationary points. And then you find all local minimum, and you prove that they are all global minimum. So basically, it's just more or less like we solve all of these equations and see what are the possible local. local points, and then you prove they're all global. minimum. It's just like solving all of the equations and seeing what are possible local minimums, which are global minimums. That's basically how it works. minimum you can have, right? So let's firstly use the stationary point, a gradient condition. So gradient of x is 0. And what is the gradient of g of x,right? So gradient of g of X, I'm not going to give a detailed calculation here. But believe me, I'll give it to you in a second or three. I'll show you how to do it in a few seconds. I'm sure you'll find it very easy. this is equal to minus this times x. I think this is actually a question in homework 0, maybe question 2 or question 3 on homework 0. And you have the gradient. And then you say, let's write out what this 0 means, right? And then, of course, you write it out in the form of a number. It's a very simple question, but it can be very difficult to understand, especially if you don't know what a number is. It can be a very difficult question to understand. So this means m times x is equal to 2 norm fx squared times x, right? Because the three things together, the last two things, becomes the 2 norm of x squared. And that's a scalar. You can switch the sign. You get 2 norm  x squared times -- you get 2norm of x square times -- and that's what we're talking about. We're not talking about a straight line here. We are talking about the sign of the norm of the square root of the squared norm. can switch the order. You get this. OK, cool. And this is a matrix vector application. So basically, this is saying that x is an eigen vector. So x is eigenvector, and x squared corresponds to eigenvalue. SoBasically, you have to-- maybe one way to maybe one. way to do it is to use a scalar, which is a vector, and a matrix. vector. Which is a linear combination of two. scalars, which are linear combinations of two vectors. think about this is that you first find out the unit eigen vector. So the eigenvector doesn't have a scale, right? So you firstfind out theUnit eigen Vector. And then if you have unique eigenvectors, so suppose, let's say-- let me just specify all this. So suppose eigenvalues are distinct even though we don't have to assume this. Then you have unit eigenevectors v1 up to vd. And you have Lambda 1 up to Lambda d. And these are the Eigenvalues. And this part just follows some intuition. basically, all the stationary points are of the form that x is equal to plus minus square root lambda i times the eigenvectors. Because if you measure the norm of x squared, then you get a Lambda i. And that is the corresponding eigenvalue, right? So these are all the all the points that are of that form, and they are all stationary points of the same form. So they're all the same shape, and all of them are stationary points. stationary points the first order stationary points of this problem. And now, let's look at which of these is a local minimum. And then we say OK, all the local minimum are global, right? So ideally, we just want to say that only vi, the v1 thing, is the local. It's a global problem, but it's a local problem in a way that makes it easier for people to work on it. The solution is to make the problem global, so that all the global points are local. minimum because that one is also a global minimum. So square root lambda 1 v1 is the global minimum, OK? So how do we do this? And also, we don't necessarily want to assume all the eigenvalues are distinct. So there's the small thing to be done regarding that as well as the other things we need to do to make sure we have the right eigenvalue for a given eigen value. So that's the first thing we have to do, and then we can get to the second thing. well. So let's compute Hessian, right? So we need to use the Hessian. So I think this is, actually, a typical question I got when people start to think about these kind of optimization problems. The Hessian sometimes can be very hard to be written down. So here, it's actually not that hard because the Hessians is in dimension d by d because you have d parameters. Sometimes your parameters is a matrix, and the Hessia becomes a fourth. order tensor. And it's kind of very complex to be even just written down to just write down the Hessian. So here is a kind of a very useful trick and which actually also has some fundamental reasons that this is useful. So the useful thing is that, if you're a mathematician, you can use this trick to work out how to write down a Hessian, which is a very complex number. And so it's very useful to be able to do that. look at the quadratic form regarding the Hessian and you look at v transpose Hessian v or v in the part that was Hessian times v, this is the quadRatic form related to Hessian. And this is much easier to compute. So why this is more difficult to compute? Because it's not the same as the original Hessian, it's just a different form of it. So this is a better way to compute it, and it's easier to work out. the methodology is the following. So this is, in some sense, in the homework solution of that homework that asks you to find a gradient of this function, right? So I guess we have the homework. Homework 0 has this question, to ask the gradient ofthis function. The same in the same way in homework 1. The homework solution is the same as the homework 0 question. It's the same for the homework 1 and the homework 2 questions. So the same thing happens. methodology also applies here when you talk about the Hessian. So the methodology, I'm not going to go to all the details. But roughly speaking, what you do is the following. You say, I am going to consider gx plus epsilon. And I'm going to Taylor expand this. So I'm just going to say that I'm considering gx and I'm expanding this. And that's the way it works. It's not a perfect system, but it's a good one. going to iteratively Taylor expand here. Whatever this g is, g needs to have analytical form. Maybe it's a composition of several functions. You just iteratively expand it, Taylor expand it. So, something like g of x plus some epsilon times some vector plus someepsilontimes some matrix. That would be a function of x and a matrix of some kind. That's a function called g. And that's what we're going to do here. We're just going to Taylor expand that. OK, I guess maybe I shouldn't even write this. So g of x plus some linear term in epsilon plus some quadratic term and so on and so forth, past the higher order term. And then if you have this, then this basically corresponds to the homework 0 solutions. So I guess I'm not sure whether this is too abstract when I say this. But if you didn't get exactly what it means, so just look at the homework0 solutions. It's basically doing this. this is a very simple way to compute the Hessian, the quadratic form over the Hessians. So if you apply these kind of techniques, you can get the Hessia like this. So the quadRatic form of the HessIAN is equals to something like this, right? But for many other cases, actually, it's very hard to write out that matrix of thehessian, right?" "Yes," he says. "Because even you are given, for example, a Hessian which is kind of complex, there's not much things you can do with it" you'll still be looking at the different specific quadratic form. And we know that the Hessian is larger than 0 is equivalent to that, for every v. So here is what we have to do. We have to have this quadratics form. So we have a Hessian that is bigger than 0, and we have this form. We can then use this form to work out a formula for a cube of size 2, for example, or a square root of 2, where v is 2. If you plug in different v's, you get the same thing. But which we want to plug in? Shall we plug in all of the v's or shall we just use some? This is why you only care about quadratic form. This is just because youonly care about-- basically, if you Plug in different V's, You Get The Same Thing. That's what we're trying to do here. We're not trying to figure out how to do it. We just want to get it. specific v's? It turns out, in many cases, you only care about a few special v's because some of v's are much more informative than the others. So you want to choose some informative v's to evaluate this formula so that you get some important information about what x can do for you. For more information on how to use this formula, go to: http://www.cnn.com/2013/01/27/how-to-use-this-formula-and-how-much-it-can-do-for-you/ The v's that are informative here is the top eigenvector. How do you know this? It requires some intuition. It also probably makes sense because the topeigenvector direction is the global minimum. So you try whether you can move in the direction of the globalminimum to see. The top eigens are the most important. They are the ones that determine what x is because you are using these to pin down what are the local minimum. The most important v's are the informative v's. whether your function value can increase your second order sense. And to some extent, it's intuitive. But anyway, so v is equal to v1 is a good choice. Because if you plug it in, you get v1 times the Hessian of x. If you plug in v1 and x you get x times v1, or v1 + x, or x + v, then x is v1 - v1. If x is x and v is v, v is x, then v1 = x. times v1. When you plug into this formula, you get 2 times x, v1 squared minus v1 transpose Mv1 plus x2 norm squared. And you say this is larger than 0. So it's the hardest test, and you can probably see why this is informative because this term is negative. It's a negative transpose, so it's not a positive transpose. It means the term is bigger than 0, and that's why it's negative. in some sense, because the negative term is maximized. So we only care about the Hessian for the stationary point, in the first order stationary point. And now, let's look at what we can get from this equation. So realize that we don't care aboutThe Hessian. for every point, right? So we don’t care about that at all. We just want to know what the Hessians are for each of the stationary points. So, for example, let’s look at the equation for a stationary point and see what it says. point. Because only those points can be possibly our local minimum. So we want to look at x as a eigenvector because we are only filtering local minimums from the stationary points. So because x is an eigen vector, we have two cases. So the first case is that x has x has a local minimum of some kind. The second case is when x has no local minimum at all, so we have no eigenvectors at all. We can then use the eigenvalues to find the local minimum for a given point. In PCA, the best one-to-one approximation is the top one eigenvalue. Different eigenvectors with different eigenvalues will be orthogonal. There is no guarantee that two eigenevectors are always Orthogonal because they could have the same eigen Value and they are just in the same subspace. But if they have differenteigenvalues, then they have to be orthosurround. So that's why x1 is orthogona to b1. If you evaluate 2, this equation 2, the 2 means what? So 2 means that the first term goes away. So you get just x2 norm square is bigger than v1 transpose in Mvi. And we have a contradiction because this is contradictory with the assumption that Lambda is less than Lambda 1. So basically, we have Lambdas bigger than Lambds, which is contradictory to the first order condition that Lambds are less than lambds. So this is a contradiction. So we have to go back to the original condition of Lambds being smaller than Lambdos. write that. OK, any questions about this? So, guys, maybe just a very quick summary-- so basically, this is saying that, if x is stationary-- by stationary point, it always means first order stationary point. So I'm not going to clarify that in the future. So if x was stationary, it was at a first order point. If x was not stationary, then it was a second order point, so it was stationary at a third order point and so on. point and is x is not global min then moving in v1 direction would not change our function very much. Because you have stationary point, that means your point is flat. So changing inv1 direction wouldn't change it by a lot. It would lead to a flat point, not a point with a point on top of it. That would change the function by a small amount, but not much. It wouldn't affect our function by much, but it would change how we see the world. a second order improvement. And that's why it's not a local minimum. Because if you are local minimum, moving in v1 direction shouldn't give you any second order improvements either. So that's basically the gist of the analysis. All right-- so cool. So now, let's talk about matrix completion. OK. so now,let's talkAbout matrix completion, and we'll start with the first step. The first step is to create a matrix. The second step is the second step, which is the third. which is kind of like an upgraded version of PCA. And as I said, this is actually a pretty important question in machine learning. So let me define the question first, and then I can briefly talk about why people care about it. And let's also talk about rank one, which is also kind of a new way of thinking about rank in a ranking system. And then we'll go on to the next question: What is the best way to rank a person in the world? version just for simplicity. So question is that, so let's say suppose-- and also, we are assuming that we have-- I guess, we assume the ground truth matrix M is also a rank one matrix. Let M be a rankOne matrix, and symmetric, and PSD just for Simplicity. And let PSD be a symmetric version of M, and let M be the groundtruth matrix. And so on and so on, until we get to the answer we want. Z is kind of the ground truth. And z is in dimension d. So we are given random entries of M. And we pick some random indices of M, and M is a transpose of z. So in other words, you can assume M equals something like zz transpose, right? And z  is kind of a ground truth, and z is in dimension d. And M  is a Transpose of Z. So M equals zz transpose of zz. and you review the corresponding entries. That's the only thing you know about M. And then the goal is to recover the rest of the entries, right? More formally, so you say that there is omega, which is a subset of the. entries, subset of. the indices of d times. The goal is then to recover all of the other entries, including omega, to find out what omega is. That is the goal of the search for omega. The search is called the omega search. Each entry is included with some probability P.d. And you say this is actually a random subset in a sense that every entry is chosen including omega uniformly randomly. So what we see, we observe, is that the entries are chosen uniformly randomly-- independently with probability P, P. So each entry is part of a larger set of entries that are chosen with some probability P, P, and P. And so on and so on, until you get to the end of the program. P omega of A is the matrix obtained by zeroing out every entry outside omega. Everything that is not in omega, you'll make those entries 0. So we observe P omega of M. And our goal is to recover M, which is called the "p omega" matrix. It's a very complex matrix, but it has a very simple structure. The answer is that it's a matrix with a simple structure, and that structure is called a "p Omega" matrix, which has a simple form. with a recommendation system. So here, I'm assuming it's a symmetric matrix, so and so forth. But if you relax those a little bit-- which doesn't necessarily change the essence of the problem. So suppose you think of we have a matrix, and in one side, the columns are indexed. And in one part of the matrix, the rows are indexed, and so on. And that's the way the recommendation system works. It's a little different, but it's the same thing. by the users. So let's say suppose this is a matrix that Amazon maintains. And each entry is the rating of the user to the item. And every user probably have an opinion about every item, right? Either they like it or not, so and so forth. But it's not like every user buys every item. So every user only buys a very small subset of the item, and that's why you only see some entries in this matrix. Amazon only sees some of the entries. And Amazon wants to understand what each user's preference is. to recommend items to users. So that's why you have to recover all the rest of the entries to serve the users better in the future. And that was why this problem was important. And it's still kind of important these days, but I guess there are many already existing methods to do it. I guess it's just a matter of finding the right way to go about it. But it's a good idea to try and get it right as soon as possible, I think. to solve this. And the most used method to solve this is basically nonconvex optimization to find this ground truth matrix M using the fact that you have a low rank structure. If there's no other structure in this matrix M, there is no way you can recover the other entries because they can be arbitrary. So that's why you have to assume that the matrix M has somelow rank structure or some other structures. So maybe just to give you a quick kind of sense about how. does this structure matters here-- so if you count the number of parameters, we have d parameters, right, to describe a rank one matrix of dimension d by d because you can just write it as xx transpose. And so how many entries you observe should be probably bigger than d, right? So if you look at the structure, you can see that it's a very complex structure. So how many parameters do you have? How many entries do you observe? And so on and so on. the degree of freedom here, right? So the number of observations, it is roughly equal to p times d squared because each entry is observed with probability p. And this should be bigger than d. If this is not bigger than D, then there is a problem. So you have p times D squared entries, and if this is bigger thand, then the problem is solved. If it isn't, then you have a problem with the degree of freedom of the system. it's unlikely it can work. So basically, that is saying that p is bigger than roughly 1 over d. And this is actually the regime we are going to work with, right? So we're going to try to get it to work, but it's not going to be easy. It's going to take a long time, but we'll get there, I'm sure, and it'll be worth it in the long run. I think we'll see a lot of progress in the next year. This is actually a pretty commonly used method in practice. So you just say I'm going to minimize this function that's called fx, which is defined fx.by, for example, log factor or something like that. So that's the setting we're going to be in. And speaking of the objective functions, this is actually an extremely common way to do it. It's called the minimize-by-objective-function method. It can be used to get a more accurate estimate of a function's value. to be that basically you have a parameterization called xx transpose. This is the parameterization of a target matrix. And you want to say this matrix actually faced all my observations, right? So you are taking a sum over all possible observed entries because these are your target. So these are the observations that you are trying to capture. And so you are looking at a matrix that is the sum of all the possible observations that could have been made in that matrix. That's what you're looking at. the only cases you know what the entries are. You know this Mij, and you minus this with xi times xj. And you take the square. So this is our prediction. This is our observation. AndYou take thesquare and take the sum over all the observed entries. And And And You Take the Sum Over All The Observed Entries. And So This is Our Prediction. And You take the Square and Take The sum over All The observed Entries And And So And So On. just to follow future notational easiness, actually you can write this as P omega of M minus xx transpose. You're looking at error matrix, right, and then you zero out all of those that you don't know, right? Because offset omega you have no no offset omega. You have to write it as M minusxx transpose, right. Because this is the matrix, you have to transpose M plus xx. That's the matrix. You can do that. information, you zero out all of those. And you take the sum of squares of the rest of the entries, right? So that's another way to write this function. And just a side note, I think, actually, there are many other methods that can solve matrix completion. So there are other ways to do it, and I'm not sure if this is the best one yet, but it's a good one. I'll let you know when I find one that works. convex transition methods and so and so forth. But in practice, just because the convex transition takes too long time, people actually are using objective functions or methods like this. However, those methods actually often have stronger guarantees. For example, they have tighter sample complexity bounds, so. and so. forth, and so on. It's a very complex problem, but it can be solved with the help of objective functions and other techniques. It can be done with a lot of work, and it's not always easy. And then they just use gradient descent to optimize these kind of functions. All right, so our main goal is to prove that this objective function has this property. And that's why it's kind of also practically relevant to analyze these kinds of objective functions because they are, indeed, used in practice. It's a very useful tool. It can be used in a lot of different ways, and it can be very useful in a number of different situations, for example, when you're trying to solve a complex problem. no local minimum, all local minimum are global. There is one assumption that I have to specify, but it's not going to be used much in this lecture in the proof. Because we are going to sweep some of these things under the rug. But I do have to mention that I'm going to make a few assumptions about the global minimums, which will be used later in the lecture. We're going to talk about some of those assumptions in the next few weeks, and then we'll go on to other things. this assumption. It may not sound very intuitive. I wouldn't spend too much time on it, but just let me mention it. So this is called incoherence assumption. And this assumption is necessary. People know it. I guess we assume, for example-- first of all, we assume the ground is the ground. And we assume that if it's not the ground, then there's something wrong with it. And so on and so on. It's a very common assumption. truth has norm 1. This is with all this generality, just which is for convenience fix of scale. And then after you fix the scale, you assume that the ground truth vector z-- so we call that [INAUDIBLE] zz transpose. So z is the groundtruth. So the infinity norm is infinity.truth hasnorm 1. Truth has norm 2. Truth is infinity norm.truth.truth is infinity truth. truth has norm 3.Truth has norm 4. truth is infinitytruth. truth. Truth can be infinity truth or ground truth.Truth can be ground truth or z. is less than mu over square root d. A mu is considered a constant or logarithmic in d. So what it's saying is that, this factor z, the norm is 1. And also, the entries are spread out. You cannot just have all the mass concentrated on one entry. So you can't have all of the mass in one place. You have to spread it out. And that's what we're doing here. We're spreading the mass around. So it's not just concentrated in one spot. the reason why you don't want that is because, for example, a counterexample is that, if z is just e1, then your M is just  e1e1 transpose, which is just at the top left corner of the matrix. We just have a very, very sparse matrix. And top right corner is the transpose. Top left corner is just the matrix's transpose to the z-axis. We don't need to have a transpose in the matrix, we just need a matrix. is 1. And then there is no way you can recover this matrix unless you observe that top left corner. So basically, all bets are off. You have to see enough entries. So this incoherence condition is, in some sense, trying to rule out these kind of pathological cases. But in the end, it doesn't work. It doesn't make any difference. It just means you have to look at the right side of the matrix and see what's going on. And that's what we do. I'm not going to talk too much about it. It's just for the formality. OK, cool. So here is the theorem. And I guess I'm going to stop after I state the theorem and then prove it next time. So this is just for. the rigorous of the proof. Sohere is the. theorem and I guess i'll stop after. I state it and then. prove it. So next time I'll talk about it a little bit more. OK. the theorem is that suppose p is something like poly mu and log d over d epsilon. Recall that we are in a regime that p is roughly 1 over d. And this is the same regime, where e Epsilon is somethinglike larger than 0, kind of like a constant. Theorem: If p is 1 over 1, then log d is a constant, and if d is 0, then the theorem is true. The theorem is based on the fact that p can be written as a function of size. And this is a poly factor in mu and also poly log in d, OK? So suppose p is on this order, and then we assume the incoherence. And then our local of f are when we are-- so actually, you can prove that they are all exactly global minimum. And so we have a global minimum of p, f, and a global incoherence of d, which is p-f-d. And we can say that this is the global minimum for all of these. But for the moment, we only prove that they are of square root epsilon-close to either z or minus z. And z and minus z are clearly a global minimum because the error will be exactly 0. All right, so that's the statement. And also, just to mention, you can also be of squareroot epsilon-close. You can be. of squareRootEpsilon close to z, for example, or to z or to minus z, or even to z and plus z. also have strict-saddle conditions. You can also prove that. It's just that I didn't include it just for the sake of simplicity. And you do have to prove that to have the rigorous result. And if you don't prove it, you just prove it. You don't have to say it just to prove it - you just have to do it. That's the only way to get the result you want. And that's what I'm trying to do. that all local minimum are global. Sometimes you may get somewhat misleading results. So I think there is a paper that shows that actually, in somewhat weird cases, you can show very strong looking results. The results look very strong in a sense that all local Minimums are global, but they are not. But they are very strong, in the sense that they show that all minimums in the world are the same. That is a very strong result. But it is not always the case. actually, the reason why they are so strong is because somehow, in that setting, you ignore the strict-saddle, which is problematic. All right, so I guess so the proof is obviously too long to cover in 1 minute. So I guess I'll leave it to the next lecture, I can. I can't believe I'm doing this, but I'm so happy to be back in the saddle. I've missed so much. I love being back on the horse. take some questions if anybody has any questions. Otherwise, I think we are good today. OK, there's a question. So are there any network models where these are known to be hold? The answer is no especially if you look for a global property, like globally, all local. If you have any questions, please contact us on 0203 615 9091 or email jennifer.smith@mailonline.co.uk. For confidential support call the Samaritans in the UK on 08457 90 90 90, visit a local Samaritans branch or click here for details. minimum are global. I don't think we have any proofs for any real neural network models. I guess there is a proof for linearized network models, like all the activations are linear. And actually, in that case, if you have more than two layers, you don't have strict-saddle conditions. You don't need to have a global minimum to be a neural network. You just have to have global activations that are global, and you can use that to create a network. have a lot of [INAUDIBLE] points. So basically, the short answer is that I don't think there are any real cases, like satisfactory cases, where we know how to prove this. But there are for two-layer networks, if you assume some conditions on the input-- for example, if it's a two-layered network, then you can do it. But it's very difficult to do it in the real world, as we've seen in the past. assume that the input are linearly separable, then there is a proof for this. Yeah. And there are a bunch of other cases where you can have some partial results. Next week, in the next lecture, maybe the second half of next lectures, I'm also going to give another result, and I'll show you how to do it in a graph. I'll give you the results in the second part of the lecture, and you can show them in the third part. which is somewhat more general. It applies to many different architectures, but it has other kind of constraints. First of all, it doesn't really show exactly these kind of landscape properties. It shows that these kinds of properties holds for a region, for a special region in the parameter space. It's a more general approach to the problem than the previous one, which is a little bit more specific to a particular region of the system. It doesn't show exactly the same kind of properties for all regions. And that's so-called NTK approach. I'm going to specify more details, but there are also other kind of problems with those kind of approach as well as-- I'mgoing to talk more about it next lecture. And that's also so- called "NTK approach" and it's a way of looking at the world from a very different perspective. And it's very different from the way we look at it now. And there are other problems with that, too, as well.