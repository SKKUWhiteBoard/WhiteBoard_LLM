The final contest, which is due tonight, is to design an agent that plays together with another agent to try to collect food pellets while not getting eaten by ghosts. So submissions for that, your last chance to submit are tonight at midnight. And on Thursday in lecture, we'll discuss the results. So today's lecture, as well as Thursday's lecture,. will be mostly on advanced applications. We will not quiz you on these application lectures, on the final exam, or anything like that. It's more meant to give you more perspective. Today's state-of-the-art in Go is that there are computer players better than the best human players. But for a game like Go, this is actually pretty hard to do. And why? Let's take a look at chess. It's much bigger than Tic-Tac-Toe. You're not going to easily compute all possible scenarios in chess. Hasn't been done yet. For Go, here's what it looks like. The brushing factor is much larger. And the ideal expansion scenario can reduce the size of the tree by square rooting it, making it half as deep. Still too much. With reasonable compute power, it traverses the whole tree. Even with alpha beta pruning, I don't think that'll happen anytime soon. It could be that by using human knowledge, you're in some kind of based enough attraction. For a local [? optim, ?] that maybe not as good as another one that might be out there. It also depends on how much randomness you have in your exploration. If you have enough randomness, then initialization will have much less effect than if you have limited randomness. Researchers have developed a way to teach helicopters to fly autonomously. The system is based on a hidden Markov model. It can be used to learn new skills, such as how to fly a helicopter in a certain way. The researchers hope to use the system to teach Go players to play the game without knowing any prior knowledge of the game or how to do certain maneuvers in a particular way. Back to Mail Online home.Back to the page you came from.. The team has developed a system that can teach a helicopter to fly in a specific way, using a hidden model. the helicopter to follow a path that's not flyable. So what if we collect paths from a human pilot and then ask the helicopter to fly those paths? Well, we could learn the trajectory from these as noisy observations. What methodology do we have for that? Hidden Markov models. We can now penalize our award, penalized for deviating from the target. And then we can run reinforcement learning in simulation, let's say, in this learn simulator to find a good controller and run it on the real helicopter. is a separate linear feedback controller for each time slice. If there is no wind, you can actually just run the linear feedback control. It will be fine. But if there's some wind gusts that could throw you off, you want to use the value functions and the two second look ahead against those value functions to do the controls. Training a unified policy across the entire space might work. It might take some work, exactly, figuring out how to do it. It could be interesting to revisit that now and see what the current understanding of how to train these networks. In 2015, there was the Doppler Robotics Challenge, which was held in Pomona, just east of Los Angeles. The robot had to, essentially, drive a car or walk, but driving the car was recommended. It turned it's very complex to get a robot to do that. The thing is modeling these situations proved even harder than modeling helicopters, because your sensing needs to understand whether or not you're already making contact, and making contact or not. You can be very close, but not have contact. It's a very subtle thing. Four cars finished the 150-mile Berkeley autonomous car race in 2005. What goes onto the cars? There is IMU, like right on a helicopter, a lot of computers. Lasers, where you shoot out laser beams. And based on how long it takes them to get back, you know how far away the nearest obstacle is in that direction. Cameras, radar, control screen, steering motor, usually, you would have a high level planner choosing a path and then a low level controller following that path. A camera will be better at that than a LIDAR. Self-supervision is a trick that's very widely used to reduce labeling efforts. In urban environments, there's even more need to recognize, not just road versus not road. A lot of progress has been made this is video from 2013. This was before deep neural networks were heavily used for this kind of thing. It's only getting better to recognize what's in scenes, thanks toDeep neural networks. The devil is really in the details, in the long term. tail of special events that can happen when you're driving. You can measure progress by just demo videos, which is one way, and it gives you some kind of feel for what's going on. Another way to measure progress is to see how are these cars doing relative to human drivers. If you test in California, you have to report this data to the DMV. It's a number of events per 1,000 miles driven. Red there is human fatalities. Then yellow is human injuries. In green is the Google slash [? wave ?] mode disengagement. so many decisions. If they're gigantic, use a lot of power. That's a problem. Let's see what we can do to build smaller networks to make decisions. What else did we not cover yet? Personal robotics. I want to spend a little more than two minutes on that, so let's keep that for Thursday. that's it for today. Bye. [SIDE CONVERSATIONS] [Side CONversation] [sideconversation.com: Do you know more about this topic? Email us at jennifer.smith@cnn.com].