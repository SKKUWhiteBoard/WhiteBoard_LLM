presenting okay share your screen that's what I'm doing oh you are okay hopefully it is yeah stop sharing yeah you should be sharing my screen under your camera until I can decide if I click on slideshow this is still show my camera uh it does I guess I can minimize it do screen sharing are you recording tooYeah great baseball back yeah I mean it's my first time giving my lecture so I'm as good as I can be do you want that cheers I mean I have to write something to hear me okay okay you know that it works. this week will probably go live tomorrow uh not quite sure yet but you'll try to get it up as soon as possible so I guess like without further Ado let's Jump Right In. I'm gonna go somewhat into detail into what representation learning is and I think this should sort of cap out the last few weeks of deep learning um and probably give you a more comprehensive understanding of what deep learning actually is doing. We'll also talk a little bit about what transfer learning is. I'll cover convolutions and CNN's in greater depth very soon. The machine learning pipeline you start with an input X you extract all the relevant features from it and then you push those into a machine learning algorithm should get an output Y and you sort of optimize based on that. The process of choosing the right features can get really complicated really fast and this is also kind of a compromise solution in the sense that you are learning the weights of your model but you're still hand programming the feature extractor yourself. Deep learning says that hey we don't need to hand program feature extractors we can all learn those as well in fact we can learn the entire pipeline from feature extraction to um training and you could you just need to pass in this like raw image input and it will spit out an output. There are special feature extractors for images so this is sort of what classical machine classical CV look like. Once you train this model you can take um can you see my cursor you can once this model has been trained these three like set up neurons in the middle of the model can then be 16b. Even without any labels we can still learn something meaningful about the structure of the raw data how many have you guys taken 16b before so yeah you might have you might recall something called PCA from the class of principle component analysis it's actually one of the most common unsupervised learning algorithms. but luckily huge models have already been trained before so the sort of question is can we leverage them in some way and the answer is yes absolutely. There's an idea called freezing so that's basically what you guys both mentioned one idea is to freeze certain layers of our neural network so basically we use our pre-trained Network which is like the already trained resnet classifier this card a few of the layer layers and then freeze the remaining earlier layers and so we can add and train the later layers to basically customize them. The layers are all network but a lot of the times they'll be like 100 layers for example depending on how deep we want it to be. In this example we start sort of abstracting in layer three um where we see objects and humans and then once we reach layer five draw like decision boundaries based on that. Even if you don't have any labels the model can still learn that okay these points are dripping up together they're forming clusters and this is still like meaningful information that themodel can learn so yeah hopefully it's sort of this picture makes clear the difference between unsupervised and supervised learning. in the past few lectures can be basically avoided so the speed of our model increases by a lot by doing this. transfer learning we basically apply the knowledge that we've learned from one pre-trained network and apply that to the second task. transforming learning is especially huge in NLP as you can imagine with natural language once you've trained on a large large amount of text there's no reason for you to relearn all of that um so you already know the meaning so there's like some semantic meaning. um and the similarity of the new data set to the original data set so for example in the case one where you have um a small small data set or a lot a large data set. Since it's larger we have more confidence that we won't overfit if we fine-tune. On the other hand if we have a smaller data set even though it's similar to theOriginal model it's not a good idea sometimes to fine tune because you can definitely overfit. And then in the third case in which you have like a smallsmall data set and then it's pretty different from the first task um. In terms of neural networks embeddings are pretty important so they are often described as lower dimensional learns continuous Vector representations of discrete variables. Embeddings are useful because they can reduce the dimensionality of your categorical variables for example and meaningfully represent them in the transform space. The idea is that if we can directly work with the significant somehow find a way to represent this line using just one variable instead of three that's going to be better. We often prefer to work with lower dimensional data so a common task is transforming them in from high dimensional data into lower dimensional instructors. Using pre-trained networks and using transfer learning is a really really good idea but not only does that apply to NLP it applies to almost other domains every other domain xcp. Downstream tasks can be something like you learn the representations from some pretext tasks and you'll use those representations for image classification or object detection or semantic segmentation or whatever for NLP. The next part of the lecture is going to be on self-supervised free training so before I tell you what self- supervised is let's clarify some terminology. notion of a loss that sort of Compares how far apart this prediction is from the ground truth label y right and your goal is to optimize the network such that this error decreases and your model is trying to output something that is very close to the actual labels y . so in a sense your training process is receiving supervision from the labels your super your labels are guiding what the model must learn. Some examples of supervised learning can be your typical classification problem like the one that we just showed. It can also be something like regression if you have kick in say 16 a or means. using that and turns out this is a pretty good idea also but it turns out that these large data sets are usually not labeled you can you could like scrap text or image or images from the web but you can't really label them automatically right so a lot of the time you're working with unlabeled data. We want to see if we could use unsupervised learning techniques to the sum level data sets and learn representations using that and this is also appealing because labeling in general is a very time consuming and tedious process. anymore but it's also trying to understand what's going on in the image it's it's going to learn that an image can be made up of different parts and those parts are going to be related to each other. There are some more technical details on the slides um I won't go into those but something that the authors actually did I actually go mention this is when they sample the patches and they divide it into a grid instead of taking the grid directly they actually generate each patch a bit.  SSL can also be applied to audio it's a very broad sort of paradigm and I think the currency of the art and audio classification is Wave 2 Vector Q which I think came out a few years ago okay so what if we go back to this idea of where to work we are predicting a single word from some surrounding context right. What if you predict a word from the entire sentence that it is a part of and as I think I think you might imagine that this white might work better because the sentence will give you more context than just like those two surrounding words. the slide is it turns out that the current state of the art for CV is actually very similar to Burke so that's a teaser for the lecture that we discuss Advanced Techniques and as software CV. There is a homework for this entire cluster which is the high Crush notebook that should be due next Tuesday even though this lecture doesn't have homework I mentioned before that there will be a lecture on on Advanced SSL for CV and that will have a homework so to work on that homework this slide deck should be up on the website again if you feel free to do that that is it for today a second pause.