So in the next portion of today's lecture we're going to talk about how we can modify the policy gradient calculation to reduce its variance. In this way we'll obtain a version of the policy gradients that can be used as a practical reinforcement learning algorithm. The first trick that we'll start with is going to exploit a property that is always true in our universe which is causality causality says that the policy at time t prime can't affect the reward at another time step t if t is less than t prime. Policy gradients are a way to formalize trial and error learning as a grain ascend procedure. We can derive the expression for the optimal baseline to minimize variance of a policy gradient. The optimal baseline is not used very much in practical policy grading algorithms but it's perhaps instructive to understand some of the mathematical tools that go to studying variants so that's what we're going to do in the next portion the next part will go through a mathematical calculation where we'll actually derive the equation for optimal baseline. variance we just uh sorry we often don't use the optimal baseline we typically just use the expected reward but if you want the optimal baseline this is how you would get it all right so to review what we've covered so far we talked about the high variance of policy gradients algorithms. We talked about how we can lower that variance by exploiting the fact that present actions don't affect past rewards and we talked about how we can use baselines which are also unbiased.