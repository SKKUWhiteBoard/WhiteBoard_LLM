The configuration of a particle is given by, or described by, a wave function psi of x. In 3D, the wave function would be a function of all three positions x, y and z. If there are two possible configurations the system can be in, it's possible to find the system in a superposition of those two psi is equal to some arbitrary linear combination alpha psi 1 plus beta psi 2 of x, OK? So some things to note-- so questions about those before we move on? No questions? Nothing? You're going to make he threaten you with something. All reasonable functions are equally reasonable as wave functions. There's no primacy in wave functions or in states. Some wave functions are more equal than others. A state with a definite momentum has the property that, when you hit it with the operation associated with momentum, you get back the same function times a constant, and that constant is exactly that momentum we ascribe to that plane wave function. The probability that I'm going to fall over in 10 seconds is not equal to 1% or 3%. It's one of those. Hopefully is much lower than that. Any function can be expressed as a superposition of wave functions with a definite momentum. Fourier didn't think about it that way, but from quantum mechanics, this is the way we want toThink about it. We want some good definition of p given that we're working a wave function. Hint the first is that a wave with a wave number is associated, according to de Broglie and Davisson, to a particle-- a particle having a particle with momentum p. But if I measure it to be anything else, that is one and the same. a general statement that any state can be expressed as a superposition of states with a well defined observable quantity for any observable quantity you want. In 2D, this is a perfectly good vector, right? Now here's a question I want to ask you. Is that a superpositional? Yeah. I mean every vector can be written as the sum of other vectors. And it can be done in an infinite number of ways. So there's no such thing as a state which is not asuperposition. Fourier: In order to reproduce that as a superposition of states with definite momentum, I need arbitrarily high wavelength. To construct or detect an arbitrarily small feature, you need arbitrarily large momentum modes. Fourier: What should be true of the Fourier transformable wave function does that mean they're not supposed to be transformable? That's usually a condition of a Fourier wave function, and we don't know exactly what that is, but it's a good question to ask. say this. When you have a sine wave, what can you say about it's-- we know that a sines wave is continuous, and it's continuous everywhere. Its derivative is continuous and differentiable everywhere, because it's a cosine, right? So if yo you take a superposition of sines and cosines, do you ever get a discontinuity? No. So how would you ever reproduce a thing with a discontinuous using sines or cosines? Well, you'd need some infinite sum. quantum physicist: Average age is the sum over all possible ages of the number of people with that age times the age divided by the total number. Average need not be an observable value, professor says. Average value of the square of ages is, well, I'm going to do exactly the same thing. It's just a squared, right? 14 squared, 15 squared, 16 square, 16 squares, 16 squared. The expected value of a is equal to the sum of a times the probability of measuring that value. The average value of a number is 0. The standard deviation is 0, as long as there's no width, which is why it's a good measure of width or uncertainty. The square root of the standard deviation squared is the uncertainty in a given probability distribution. Different probability distributions are going to give me different delta a's, so we can either write it in this fashion or this fashion, and the notation for this is delta a squared. It's a little odd, because really you'd want to call it thestandard deviation squared. But whatever. notation that says which distribution you were talking about. When you have multiple distributions, or multiple possible probability distributions, sometimes it's useful to just put given the probability distribution p of a. This is not very often used, but sometimes it is very helpful when you're doing calculations just to keep track. Another notation that will come back-- you'll see why this is a useful notation later in the semester-- is this notation, psi. And we define the uncertainty in x is equal to the expectation value of x squared minus the expected value of X quantity squared. of x, how do we get the probability that you measure p? Do I want to do this now? Yeah, OK I do. And we need a guess. Question mark. We made a guess at the end of last lecture that, in quantum mechanics, this should be dp minus infinity to infinity of the Fourier transform. Psi tilde of p up to an h bar factor. OK, so we're guessing that the Fouriers transform norm squared is equal to the probability of measuring the associated momentum. And so on your problem set you're going to prove it. ways. You can either say the expectation value of x, or the expectation of x in the state psi. And this would be pronounced one of two ways. Why is there a double notation of psi? Yeah, we'll see later. Roughly speaking, it's but I don't want to take the time to do it, so ask in office hours. OK, good. The second part of your question was why does the Heisenberg relation work out nicely in terms of these guys? We'll see that. because in computing this expectation value, there's a psi squared. And so this is to remind you of that. Other questions? Terminology is one of the most annoying features of quantum mechanics. Yeah? AUDIENCE: So it seems like this [INAUDIBLE] variance is a really convenient way of doing it. How is it the Heisenberg uncertainty works exactly as it does for this definition of variance? PROFESSOR: That's a very good question. In order to answer that question, we need to actually work out the He Eisenberg uncertainty relation. And one answer is, indeed, the uncertainty relation works out quite nicely. a wave, a plane wave e to the iks, how do I get h bar k out of it? Note the following, the derivative with respect to x. The units of h bar are momentum times length. Momentum is about velocities, which is like derivatives. So if this is supposed to be true in some sense, what is momentum have to do with a derivative? We've been led to the idea that using wave functions that there's some relationship between the momentum, the observable quantity that you measure with sticks, and meters, and stuff. Noether's theorem underlies an enormous amount of classical mechanics, but also of quantum mechanics. To every symmetry is associated a conserved quantity. Conservation of momentum is associated with time translational symmetry. Rotational symmetries. x, as a vector, goes to some rotation times x. What's conserved by virtue of force? What's p dot? Yep. Noetherâ€™s theorem is solid. It's not shocking that in quantum mechanics, where we're not interested in the action of things on positions, but on functions of position, it's very interesting. of rotational symmetry? AUDIENCE: Angular momentum. PROFESSOR: Rock on. OK So quickly, I'm not going to prove to you Noether's theorem. It's one of the most beautiful and important theorems in physics. But let me just convince you quickly that it's true in classical mechanics. And this was observed long before Noether pointed out why it was true in general. What does it mean to have transitional symmetry? It means that, if I do an experiment here and I do it here, I get exactly the same results. translate by L. And what translate by L does is it takes f of x and it maps it to f of X minus L. So this is a thing that affects the translation. And why do I say that's a translation by L rather than minus L? Well, the point-- if you have some function like this, and it has a peak at 0, then after the translation, the peak is when x is equal to L. OK? So just to get the signs straight, define this operation. It's a Taylor expansion for a particular function. a priori did the world have to look like. Physics tells you this is a good model. And to the degree that it doesn't fit the data, it's wrong. This isn't something we derive. This is something we declare. We call it our model, and then we use it to calculate stuff, and we see if it fits the real world. Out, please, please leave. Thank you. [LAUGHTER] I love MIT. I really do. We live in a world governed by probabilities. The poem was written in the 19th century. It was written by a poet-turned-professor. The poem is called "The Poem of Love and Poetry" It is written in rhyme with the words "Love, Love, Love" and "Poem, Poem, Love." The poem was published in the early 20th century by the poet-in-chief, Christopher Smith, and was published by the publisher, Macmillan, on Valentine's Day, 1913.