Causality is a property that is always true in our universe which is causality causality says that the policy at time t prime can't affect the reward at another time step t if t is less than t prime. This is another way of saying that what you do now is not going to change the reward that you've got in the past. The only way this would not be true is if you had time travel and you could take an action or travel back into the past and change your action. that we're going to see this is we're Going to rewrite the policy grading equation i've i've not changed it anyway i've simply rewritten it and what i've done here is i use the distributive property to distribute the sum over rewards into the sum of grad log pies. At every time step i multiply the grand log probability of the action at that time step t by the sum. of rewards over all time steps in the past present and future. If we generate enough samples eventually we should see that all the rewards at time steps t prime less than t will average out to a multiplier of zero. the proof is somewhat involved so i won't go through it here but once we show that this is true then we can simply change the summation of rewards. Instead of summing from t prime equals one to capital t simply sum from t Prime equals t to capitalt basically discard all the rewards in the past because we know the current policy can't affect them. For a finite sample size removing all those rewards from the past will actually change your estimator but it will still be unbiased so this is the only change that we made. in the previous lecture we will get much more into this in the next lecture when we talk about extra critical algorithms but for now we'll just use a similar symbol with a hat on top to note that it's a single sample estimate all right now the causality trick that i described before you can always use it you'll use it in homework two it reduces your variance there's another slightly more involved trick that we can use that also turns out to be very important to make policy gradients practical and it's something called a baseline. grad log p by r of tau we multiply by r  where b is the average reward this would cause policy gradients to align with our intuition. subtracting a constant b from your rewards in policy gradient will not actually change the gradient in expectation although it will change its variance meaning that for any b doing this trick will keep your grading estimator unbiased. The average reward which is what i'm using here turns out to not actually be the best baseline but it's actually pretty good. In many cases when we just need a quick and dirty baseline we'll use average reward however we can actually derive the optimal baseline. baseline to optimally minimize variance so to start with we're going to write down variance. The variance of the policy gradient is equal to the expected value of the quantity inside the bracket squared minus the whole expected value squared. The first term in the variance doesn't depend on b but the first term does so then in order to find the optimal b i'm going towrite down the derivative d var db and solve for the best b. The baseline actually depends on the gradient which means that if the gradient is a vector with multiple dimensions if you have multiple parameters you like to have a different baseline for every entry in the gradient. different policy parameters you'll have one value of the baseline for parameter one a different value for parameter two. In practice we often don't use the optimal variance we just uh sorry we typically just use the expected reward but if you want the optimal baseline this is how you would get it all right so to review what we've covered so far we talked about the high variance of policy gradients algorithms. We talked about how we can lower that variance by exploiting the fact that present actions don't affect past rewards and we can use baselines which are also unbiased.