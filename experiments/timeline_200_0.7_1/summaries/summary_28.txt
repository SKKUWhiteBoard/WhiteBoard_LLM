A random variable is a number that's produced by a random process. The number of faulty pixels in a monitor is also produced from an unpredictable randomness in the manufacturing process. One example is a system that you're watching and you're going to time it to see when the next crash comes, if it crashes. That number is produced by this random process of whether the system works or not. It's called a random variable because it's unpredictable that it happens in some random way. It can also be called a non-random variable, because it can't be predicted. that really is modeled in physics as random is when you have a Geiger counter, you're measuring alpha particles. And if I flip coins then the number of heads in a given number of flips-- let's say I flip a coin n times. OK what is abstractly a random variable? Let's look at that example of three fair coins. So each coin has a probability of being heads that's a half and tails being a half. Or alternatively you could think of flipping the same coin three times. heads is a number that comes out of this random process of flipping the three coins. Another one is simply a [? 0-1 ?] valued random variable where it signals 1 if all 3 coins match in what they come up with, and 0 if they don't match. One of the things that's a convenient use of random variables is to use them to define various kinds of events. The event that C equals 1, that's an event that-- it's a set of outcomes where the count is 1 and it has a certain probability. We think of the outcomes in the sample space as the results of a random experiment. They are an outcome and they have a probability. And when the outcome is translated into a real number that you think of as being produced as a result of that outcome, that's what the random variable does. So formally, a random variable is not a variable. Or it's a function that maps the sampleSpace to the real numbers. And it's got to be total, by the way. It's a total function. Usually this would be a real valued random variable. I look at the event that R is equal to a, that's an interesting event. And it's one of the basic events that R puts together. If you knew the answer to all of these R equals a's, then you really know a lot about R. That's why this little topic of introducing random variables is also about independence because the definition of independence carries right over. Namely, a bunch of random variables are mutually independent if the events that they define are all mutually independent. could say explicitly where it comes from as an equation. It means that the probability that R1 is equal to a1 and R2 isequal to a2 and Rn is Equal to an. And the definition then of mutual independence of the random variables R1 through n, Rn holds is that this equation it holds for all possible values, little a1 through little an. So let's just practice. Are the variables C, which is the count of the number of heads when you flip three coins, and M, [? the 0-1 ?] valued random variable that tells you whether there's a match, are they independent? Well certainly not. 2 tails and there's no match. So without thinking very hard about what the probabilities are we can immediately see that the product is not equal to the probability of the conjunction or the and, and therefore they're not independent. Well here's one that's a little bit more interesting. In order to explain it I've got to set up the idea of an indicator variable, which itself is a very important concept. So if I have an event A, I can package A into a random variable. And it means that really I can think of events as special cases of random variables. of random variables one way. We have another concept of independence that holds for events. The definition for random variable was motivated by the definition for events but it's a different definition of independence of different kinds of objects. Now if this correspondence between events and indicator variables is going to make sense and not confuse us it should be the case that two events are independent if and only if their indicator variables are independent. And this is a lovely little exercise. It's like a three-line proof for you to verify. number of heads we can ask whether the event M, which is the indicator variable for a match-- the random variable M-- and the indicatorvariable IO are dependent or not. Now both of these depend on all the three coins. IO is looking at all 3 coins to see if there are an odd number of heads, M is looking to see whether they're all heads or all tails. And it's not immediately obvious that they're independent, but as a matter of fact they are. this can have value 0 and 1. If R is independent of S then R is really independent of any information at all that you have about S. And of course the notion of k-way independence carries right over from the event case. If I have k random-- if I have a bunch of random variables, a large number much more than k, they're k- way independent if every set of k of them are mutually independent. of f of S, any transformation of S by a fixed non-random function. course as with events we use the 2-way case to call them pairwise independent. If we have k coins and Hi is the indicator variable for how coin I came out, whether or not there's a head, now O can be nicely expressed. The notion that there's an odd number of heads is simply the mod 2 sum of the Hi's. And this by the way, is a trick that we'll be using regularly that events can be defined rather nicely in terms of doing operations on the arithmetic values of indicator variables. makes the k plus 1-- k plus first. And the reason why any k of them were independent was discussed in the previous slide when we were looking at the events of there being an odd number of heads and a head coming up on the i flip. For a bunch of major applications this pairwise independence is sufficient. It's harder to check mutual independence. You've got a lot more equations to check. We'll be making use of it in an application later when we look at sampling and the law of large numbers.