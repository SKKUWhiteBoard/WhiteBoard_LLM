In this module, I'm going to briefly introduce the idea of differentiable programming. Differentiable programming is closely related to deep learning. I've adopted the former term as an attempt to be more precise in terms of highlighting the mechanics of writing models as you would code. There's really enough to say here, to fill up an entire course at least. So I'll try to keep things pretty high level but I will try to highlight the power of composition. So let's begin with our familiar example, the three layer neural network. example, the ReLU or logistic. Now I have a vector and now I can do the same thing again. I apply a matrix, add a bias term, apply an activation function. Apply a matrix which happens to be a vector. And I get a score which then I can happily drive regression or take the sign to drive classification. So what I want to do now is to factor out this kind of complex looking expression into a reusable component which I'm going to call FeedForward. But we're going to see a lot of these box diagrams which are going to represent functions that we can reuse. you want to do image classification. We need some way of representing images. The FeedForward function that we just introduced, takes a vector as input and we can represent an image as a long vector by, for example, adding all the rows. But then we would have this huge matrix that we would need to be able to transform this vector resulting in a lot of parameters which may make life difficult. To fix this problem, we introduce convolutional neural networks which is a refinement of a fully connected neural network. first is Conv. Conv takes an image and the image is going to be represented as a volume which is a collection of matrices, one for each channel, red, green, blue. Each matrix has the same dimensionality as the image, height by width. The way that Conv is. going to compute this volume is via a sequence of filters, and intuitively. what it's going to do is try to detect local patterns with [AUDIO OUT] So here is one filter and how it works is I'm going to slide this filter across the image. going to slide a little max operation over every 2x2 or 3x3 region. So the max over these four numbers is going to be used to build this [INAUDIBLE] and so on. That's all I'm going to say about MaxPool. If you want to go into the details, you can check out this demo or you can learn more in 231. But again, I want to highlight that there's these two modules. One for detecting patterns and one for aggregating. And with these two functions along with FeedForward, now we can define AlexNet. In NLP, words are discrete objects and neural networks speak vectors. So whenever you're doing NLP with neural nets, you first have to embed words, or more generally, tokens. We're going to define an EmbedToken function that takes a word or a token x and maps it into a vector. And all this function is going to do is it's going to look up vector in a dictionary that has a static set of vectors associated with particular tokens. So this representation of the sentence is not going to be a particularly sophisticated one. is something that has an interface but not an implementation. So a SequenceModel is going to be something that takes a sequence of input vectors and produces a corresponding sequence of output vectors. I'm going to talk about two implementations of the sequence models. One is recurrent neural networks and one is transformers. So an RNN, or a recurrent neural network, can be thought of as reading a sentence left to right. So the intuition again is reading left-to-right, updating the hidden state as you go along. Collapse takes a sequence of vectors and returns a single vector. There's three common things you can do. If you're doing text classification, you probably want to pick the average to not privilege any individual word. But as we'll see later if you're trying to do language modeling, you want to take the last. These types of functions where the input and output have the same type signature are really handy because then you can compose them with each other and get multiple steps of computation. before actually defining it. So the core part of a transformer is the attention mechanism. The attention mechanism takes in a collection of input vectors and a query vector and it outputs a single vector. mathematically what this is doing is you start with the query vector. I'm going to multiply a matrix to reduce its dimensionality, in this case from 6 to 3. And now I can take the dot product between these x's and y's. So that's going to give me a four-dimensional vector of dot products. In contrast with the RNN, you have representations that have to kind of proceed step by step. And the number of steps is the length of a sequence which causes these long chains which prevents kind of fast propagation. So that's an attention mechanism. You can think about this as a sequence model that just takes input sequence and contextualizes the input vectors into output vectors. There's two other pieces I need to talk about before I can fully define the transformer. Layer normalization and residual connections. These are really kind of technical devices to make the final neural network easier to train. was this complicated thing that I mentioned at the beginning. So BERT is this large unsupervised pretrained model which came out in 2018 which has really kind of transformed NLP. Before there were a lot of specialized architectures for different tasks, but BERT was a single model architecture that worked well across the many tasks. So this is the way it works for question answering. You take the question, you concatenate it with the paragraph, that gives you just a sequence of tokens. We can use language models and we can build on top of them to create what is known as a sequence-to-sequence model. This is perhaps one of my kind of favorite interfaces because it's so versatile. So this is by and large how a lot of the state of the art methods for, for example, machine translation works. We're generating a translated sentence, given the input sentence or a document summarization or semantic parsing. Each of these sequence can be framed as sequence- to-sequence tasks based on, usually these days, basically BERT and Transformers. encourage you to consult the original source if you want kind of the actual, the full gory details. Another thing I haven't talked about is learning any of these models. It's going to be using some variant of stochastic gradient descent, but there's often various tricks that are needed to get it to work. But maybe the final thing I'll leave you with is the idea that all of differentiable programming is built out of modules. Even if you kind of don't understand or I didn't explain the details, I think it's really important to pay attention to the type signature of these functions.