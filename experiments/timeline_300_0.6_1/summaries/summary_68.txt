Today we're gonna talk about learning in the setting of games. Can you still be optimal if you reveal your strategy? It's actually not the size that matters. It's the type of strategy that you play that matters, so just to give you an idea. And then, er, towards the end of the lecture, we wanna talk a little [NOISE] bit about variations of the game- the games we have talked about. So, uh, how about if you have- how about the cases where we have simultaneous games. going to pick bucket A, bucket B, or bucket C. And then the opponent is going to pick a number from these buckets. They can either pick minus 50 or 50, 1 or 3 or minus 5 or 15. So if you want to maximize your, your utility as an agent, then you can potentially think that your opponent [NOISE] is trying to, trying to minimize your utility, and you can have this minimax game, kind of, playing against each other. game of chess. And if you think about the game of chess, the branching factor is huge. The depth is really large. It's not practical to u- to do the recurrence. So we, we started talking about ways to- for speeding things up, and, and one way to speed things up was this idea of using an evaluation function. So instead of the usual recurrence, what we did was we decided to add this D here, um, this D right here which is the depth that un- until which we are exploring. In chess, you have this evaluation function that can depend on the number of pieces you have, the mobility of your pieces. Maybe the safety of your king, central control, all these various things that you might care about. So, so the hand- like you can actually hand-design these things and, and write down these weights about how much you care about these features.and figure out what is a good evaluation function. So to do that, I can write my evaluation function, eval of S, as, as this V as a function of state parameters. how learning is applied to these game settings. And specifically the way we are using learning for these game. settings is to just get a better sense of what this evaluation function should be from some data. And, and that kind of introduces to this, this, um, temporal difference learning which we're gonna discuss in a second. It's very similar to Q-learning. Uh, and then towards the end of the class, we will talk about simultaneous games and non-zero-sum games. at a simplified version of it. So, so what are some features that you think might be useful? Remember the learning lecture. How did we come up with, like, feature templates? Yes. Currently, still bound with the [inaudible]. So maybe like the location of the X's and O's. The number of them. Yeah. All right. So okay, so that was my model. So now, the question is where do I get data? Like where and because if I'm doing learning, I got to get data from somewhere. So one idea that we can use here is we can try to generate data based on our current policy pi agent or pi opponent. So, so that's kind of how we do it. We call these policies. We get a bunch of episodes. We go over them to make things better and better. So then we generate episodes and then from these episodes, we want to learn. These episodes look like state action reward states and then they keep going until we get a full episode. And then we had a target that you're trying to get to. And my target, which is kind- kind of acts as a label, is going to be equal to my reward, the reward that I'm getting. Is it possible to have, an end state and not end state have the same feature vector, or no? If you use like, uh, initialize rates do not be zeros which you update throughout instead of just to the end. If there were kind of the same and have same sort of characteristics, it's fine to have feature that gives the same value. If it is always 0, it doesn't matter like what the weight of that entry is. So in general, you wanna have features that are differentiating and you're using it in some way. The idea of learning in games is old. People have been using it. In the case of Backgammon, um, this was around '90s when Tesauro came up with, with an algorithm to solve the game. And he was able to reach human expert play. And then more recently we have been looking at the game of Go. So in 2016, we had AlphaGo, uh, which was using a lot of expert knowledge in addition to ideas from a Monte Carlo tree search and then, in 2017, we have AlphaGo Zero, which wasn't using even expert knowledge. Minimax sca- strategy seemed to be pretty okay when it comes to solving these turn-based games. But not all games are turn- based, right? Like an example of it is rock-paper-scissors. You're all playing at the same time, everyone is playing simultaneously. The question is, how do we go about solving simultaneously, okay? So let's start with, um, a game that is a simplified version of rock- Paper-Scissors. This is called a two-finger Morra game. So, we have player A and player B. We have these possible actions of showing 1 or 2. And then, we're gonna use this, this payoff matrix which, which represents A's utility. If A chooses action A and B chooses action B. And this is called the pay-off matrix. So, uh, so now we need to talk about what does a solution mean in this setting? So, so what is a policy in the setting? And, and then the way we refer to them in this case are as strategies. Someone tells me it's pi A and pi B, I can evaluate it. I can know how good pi A is, from the perspective of agent A. But with the challenge here is we are playing simultaneously, so we can't really use the minimax tree. So what should we do? So I'm going to assume we can play sequentially. So that's what I wanna do for now. So right now I'm gonna focus only on pure strategies. I will just consider a setting- very limited setting and see what happens. that's the question we're trying to answer. Okay? So, so let's say Player A comes in, and Player A says, "Well, I'm gonna reveal my strategy to you" So the value of the game, uh, would be, maybe I'll write it here. Pi A is already this mixed strategy of one-half, one- half, right? It's going to be equal to Pi B times 1. So what I'm really trying to do as Agent B is to minimize this, because I don't want Agent A to get anything. The idea of the Prisoner's dilemma is that you have a prosecutor who has to decide whether or not to convict someone. So, next 10 minutes, I want to spend a little bit of time talking about non-zero-sum games. So so far, we have talked about zero- sum games, where it's either I get the negative reward or I get some negative reward. There are also these other things called collaborative games where we are just both maximizing something out of both of us. It's a single optimization that's kinda like a single maxim. look at them, would be useful for projects. And with that, I'll see you guys next time. Back to the page you came from! Back to Mail Online home. Back into the fold! Back To the pageYou came from: Back To The Page You Came From. Back from the Page you Came From: back to the Page You came from.back to thepage you came From: Back to The Page you cameFrom: The PageYou Came from: ThePageYouComingBack to: ThepageYouComing Back to: