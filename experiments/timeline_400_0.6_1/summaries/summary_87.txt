Differentiable programming is closely related to deep learning. It allows you to build up an increasingly more sophisticated model without losing track of what's going on. So let's begin with our familiar example, the three layer neural network. We're going to see a lot of these box diagrams which are going to represent functions that we can reuse and have a nice interpretation. To fix this problem, we introduce convolutional neural networks which is a refinement of fully connected neural networks. So here is an example of ConvNet in action. Play with ConvNets, you can actually click here for Andrej Karpathy's excellent demo. You can actually create and train ConvNet in your browser. So Conv takes an image and the image is going to be represented as a volume which is a collection of matrices, one for each channel, red, green, blue. Each matrix has the same dimensionality as the image, height by width. And what the Conv is. going to do is it's going to compute another volume of a slightly different size, usually the height and width of this volume is going. to be equal or maybe slightly smaller. going to slide a little max operation over every 2x2 or 3x3 region. So the max over these four numbers is going to be used to build this [INAUDIBLE] and so on. If you want to go into the details, you can check out this demo or you can learn more in 231. But again, I want to highlight that there's these two modules. One for detecting patterns and one for aggregating, to kind of reduce the dimensionality. And with these two functions along with FeedForward, now we can define AlexNet. In NLP, words are discrete objects and neural networks speak vectors. So whenever you're doing NLP with neural nets, you first have to embed words, or more generally, tokens. There's a lot of processing that needs to happen and it's hard to kind of specify in advance. A SequenceModel is going to be something that takes a sequence of input vectors and produces a corresponding sequence of output vectors where each vector in this sequence is a process with respect to the other elements. I'm going to talk about two implementations of the sequence models. One is recurrent neural networks and one is transformers. Self attention takes a sequence of input vectors and then it's going to output the same sequence of output vectors where the first vector is, I'm going to stick x1 into the query vector for y and compute the attention. So that's an attention mechanism. You can think about this as a sequence model that just takes input sequence and contextualizes the input vectors into output vectors. Layer normalization and residual connections are really kind of technical devices to make the final neural network easier to train. encourage you to consult the original source if you want kind of the actual, the full gory details. Another thing I haven't talked about is learning any of these models. It's going to be using some variant of stochastic gradient descent, but there's often various tricks that are needed to get it to work. But maybe the final thing I'll leave you with is the idea that all of differentiable programming is built out of modules. Even if you kind of don't understand or I didn't explain the details, I think it's really important to pay attention to the type signature of these functions.