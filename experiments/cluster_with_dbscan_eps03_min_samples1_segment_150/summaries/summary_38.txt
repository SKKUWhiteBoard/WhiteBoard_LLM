So I think I just spend like five minutes, just briefly review on the backpropagation last time. So I didn't have time to explain this figure, which I think probably would be useful as a high level summary of what's happening. So you start with some example x, and then you have some-- I guess, this is a matrix vector multiplication module, and you take x inner product multiplied with w and b. And then get some activation, the pre-activation. And you get some post activation. loss with respect to the output first. This is like just take the-- because the loss is something like y minus tau squared, times a half. And this one is just a very simple formula. And then you compute the derivative of the loss withrespect to-- here, I only have three layers. So I think we have three lemmas or three abstractions. So and each of these arrow is using a different way to compute the loss. It's kind of like you are accessing this network in a backward fashion in some sense. one of those three lemmas. If you know how to compute the derivative with respect to the output of some module, suppose this is a module, tau is the output. And now you can see what this does kind of like lemma are for. Those lemma, basically, are saying that if you know dg over the d tau, how do you computedg over da? And there's another lemma which says that ifYou know how. to compute dgover da, howDo you compute dG over dz? All of those lemma is about this kind of relationship. If you know this quantity, then how to compute the derivative with back to the last layer weight. And from this quantity,. you know how to complete a derivative with respect to w1. And also the same thing for b's. And this kind of-- the last row, this quantity. So they don't depend on, for example, after you get this, you can convert this, right? And after you've got this quantity you can come up with a derivative. We don't have enough time to review again. these two quantities. But this row, the derivative with respect to activations, you can only do it sequentially. You can not say, you compute this before you do this. So this arrow is kind of the orders of the dependencies between these quantities. And each of these arrow is basically done by one of the lemma that we discussed last time. Any questions? This is just an extension of the last five minutes of the previous lecture. I didn't have enough time to elaborate on this. Some of the practical viewpoint of ML, like how do you relate to your model, what you have to do in this whole process, right? So like you start with the data process, and then you has to tune the model. So basically, in these two lectures, I think we're going to discuss this concept. I think that generalization is probably the main thing that we are talking about here. It's really just about how well your model is performing, and syntax examples. And we fit some model on them. So now, what we care about whether this model will work for future unseen examples. And we are going to discuss a bunch of concepts, the balance variance trade-off, which is a kind of a principle when you think about how test error changes as you change model complexity. OK. So I guess, that's just a very high level overview. I use a lot of buzzwords. I'm not expecting everyone to follow everything. So let me maybe be concrete. it means. Training loss or sometimes, it's called training error. Sometimes, we use the word cost. So they all mean the similar type of concepts. This is the loss function you care about when you have square loss. And other loss could be cross-entropy loss. It could be like MLE, the maximum likelihood estimator. I think we have to write down-- we have written down this equation a lot of times. It's actually one principle to derive the training loss. You derive the maximum likelihood estimator for data sets. And that you use that as your training loss. So this is basically, so far, what we have focused on in the last few weeks. How do you get a training loss, and how do you really implement this, and optimize this, right? So there are many ways to optimize it. So like for example, like in deep learning, we are using stochastic gradient descent. And then we have talked about Newton's method, so and so forth. is this loss function when we try to find the minimizer of this lossfunction. So ideally, you want the model to not only perform well on the training data because for the trainingdata, you already know the prediction. So that's why the test loss is defined on unseen examples. And I'm going to use this notion. So suppose, say, you draw-- the process is that you draw some data and then try to predict it. Why you care about letting themodel to predict something you already knows? New example, x comma y, from some distribution D. And often, this is called test distribution. And then you evaluate what's the expected loss on this new test example. So what's important is that this x and y is not seen in a training. It's a new fresh example. And just to be clear, these test examples, you haven't seen them in aTraining set. They are something you draw. You can draw them in advance, but you cannot let them to be seen in the training process. is a notion called generalization gap. When you test, you find that your model is not as good as you thought before on the training set. Sometimes, it's probably a little worse, sometimes a lot worse. But generally, you shouldn't expect that your test performance is dramatically better than the training performance. In extreme cases, you can design the set, such that this happens. But I think in realistic practical situations, I don't think you should expect that at all. The gap is either very close to 0, or maybe a slightly negative, slightly positive, or is much bigger than 0. So you want this gap to be as small as possible. So this one is something you can control in some sense. This is what you try to optimize for. But the generalization gap is something that is very hard to control. At least, you cannot directly control it. And the point of this lecture is to discuss in what cases you can somewhat know this is not too big? OK. And then before going to do more details, let me also define two notation-- two kind of like commonly used terminology. When l theta is big, the question is, what do we do to change it? Like if you observe that your test loss is very big, then what you can do to make it smaller? That's the question you want to study. And typically, there are two failure mode in some sense. One of the failure mode is called overfitting. And so overfitting, I'm going to discuss a lot about. But the first other bit is that the typical situation of overfitting is not supposed to be comprehensive. But I think, typically, you are in either one of these two failure modes. overfitting is that the training loss, j, is small, but the test loss is big. So you have this big generalization gap. So much worse-- becomes much worse. So this is a typical situation of overfitting. In some sense, you are saying that you fit the data very well, but you are overfitting in the sense that you kind of like forgot about the test performance. I will discuss why this will happen. I guess, you can probably guess, but this is so far, I'm just defining roughly what overfitting means. like this, maybe this, and something like this. You see these four blue points, and you want to fit a line to it or fit some curve to it. And the question is what curve you are going to fit? So suppose, you fit something crazy like this, let me try to see what color I'm using for this. One moment. Let me think about how do I use the color in a consistent way. So I guess if you fit-- I'm going to use black for the model you fit. that you are not-- you fit the training data, but you don't generalize. Another notion is called underfitting. An underfitting, basically, just means that you face something like this. And whether you are in the overfitting regime or the underfitting regime, depends a lot on different things. And one kind of decision we are trying to discuss today is that what is the right model complexity. So like what are we going to use? Linear model, maybe use quadratic, or fifth degree polynomial, or neural network, so on. and so forth. So we're going to discuss what will happen if you change your model complexity, and whether in what cases, you may underfit. In what cases you may overfit, and what is the best response. Any questions so far? And kind of like as a spoiler, in some sense, like we'regoing to discuss two-- we are going to decompose the test error, l theta. The test error is the test loss, and l theTA. The bias is a decreasing function as the model complexity. So we say that the bias is large, it's because the model is not expressive enough. So that means that if your model is more expressive, then your bias should decrease. So this is the bias. And now let's think about how do you draw the in some sense. Because for example, suppose you believe that this is a U curve. Then should you try even bigger models, bigger family of models? Because you kind of believe that it will be even worse. So you should just try even more in the middle. bias will be smaller. And your sum of these two functions, which is a test error, will be. something like this. And then the best one will be something in the middle. So this is kind of the quick overview of what we're going to discuss. OK. So now, I'm going to define bias and variance in a little bit more formal ways. And I'll show some examples. So any questions so far? Why is the bias [INAUDIBLE]. Why is bias this crazy? Oh, squared, I mean. I drew above. These training examples are something like yi is equals close to a quadratic function. Sometimes, they are called this h star xi. Just for the sake of terminology, I think, sometimes, they call this the ground truth. And sometimes, the true function you are trying to find out. But of course, you don't know it. You want to try to recover it. And I'm going to do a thought experiment first. I's going to start with linear model, and then I'm Going to try a few experiments. fifth-degree polynomial, and then I'm going to try quadratic. So linear model. I guess, you can probably see, guys, what will happen. So you can see that there's a large training error, training loss or training-- let's call it loss just for consistency. So what you really will fit, like if you minimize the error on the training data with this so many training examples, then what you will get is probably something like this. It's not like necessarily matching exactly the ground truth, but you have a small fluctuation. the training data set. This is your prediction for this x. And you look at the distance between the prediction and the true label. You see that the distance is pretty big. So this is underfitting, by our definition of underfitting because the tuning is already big. And now let's think about so what you should blame. Why the training is big? What's the culprit? The culprit, I would argue, is that it's just because no any linear model can fit your data. well. So when in this kind of settings things happens, like you have the bias. So the bias is basically like it's saying that the reason why-- I don't know exactly why people call it bias in the very first time. But I think you can-- see kind of the relationship. So you are imposing a linear structure, but the true data is not linear. So it doesn't matter how many data you see, as long you impose this, you just insist that I just believe that this thing is linear, you're going to fail. cannot be mitigated by more data, as I said. And actually, it can also not beMitigated by less noise, even though there is-- and by less Noise data. So suppose, you see a little more data. Suppose you see some more data as training data. And maybe let's say, you just-- suppose in extreme case,you just see everything exactly on this quadratic line without any noise, still, if you think about what's the best fit. The best fit probably would change a little bit. Mathematically, one way to define a bias is that you can say this is the-- So bias is-- I guess, actually, there's some approximation here, depending on what exactly your model is. But roughly speaking, it's the best error or loss, you can get with even infinite data. And you can kind of see that it's probably important for bias to be small because if bias is large, even with infinite data, you cannot do anything. And that's the problem with linear models. you mean, like I don't know. Within the same model. You're not changing the model, right? You're only using linear. Like you cannot-- OK. So bias would be the best [INAUDIBLE] linear model. Exactly. So this is the bias. And here, there is-- I'll come back to the variance for this model. But here, the variance is, in some sense, you can say, it's not very important. Only the bias is the important. The model is something like h theta x is some theta 5x to the 5 plus up to theta 0. But recall that we can do this with linear regression because you just-- this is still linear in the theta. So we are able to fit this. And now, I'm going to show cases where the variance is the culprit to blame for. So I guess, I's going to redraw this. So you have-- [INAUDIBLE] four points. So if you fit the fifth-degree polynomial, so probably, you're going to get-- a fifth-degree polynomial can go up and down so many times, several times. And actually, if you really look, you can probably do something like this. So the exact details here don't matter. So just the point is that if you have high degree polynomials,. you can be more flexible. And then if you fit the data-- if youFit the polynomic to the data, then possible, you's going to getting something kind of pretty flexible. up some-- like this is not required for this course, but if you look up the book for the calculus of like polynomials, you know that if you have four points, there's always a fifth-degree polynomial with a path for all of them. So in this case, the training error is literally 0. So I guess, this is expected. And the thing is that this is overfitting. So what's the problem here? Why it's overfitting? So why the test is not good? model tries to explain all of this small perturbations, small noise. And because it overexpressed the small noise, at loss, it kind of like didn't pay enough attention to the more important stuff. So that doesn't sounds right. So like how come your model can explain like everything and anything like a random. pattern? So basically, you are looking at-- you are kind of overfitting to the spurious patterns, but instead of the big pattern. The big pattern is this. spurious patterns are the fluctuations in some sense. And so in other words, I think you are explaining the noise instead of the ground truth. How do I formulate this? Like one way to kind of formulate this a little bit more mathematically is that you can consider to redraw the samples. So you redraw some new samples with different spurious patterns. They are spurious because they are noise. If your model is specific to the noise, you are sensitive to it. And again, how do you make this intuition a more formal? OK. spurious patterns, that means that if you redraw, you are going to-- you're going to learn the new spurious patterns. And you aregoing to have a different model. And if you are not specific or sensitive to spurious patterns, even you have a new data set, you probably shouldn't change much, but you should still be somewhat the same. You should still opt for the same model. If you have the 5-degree polynomial , you redrawn the data sets, then you will find a new model. this, maybe they're like point like this. Maybe we want to keep this. And I'm going to try to make the pattern rather different. Then, maybe you're going to get something different. Maybe, I don't know. You try to find out what the degree of the polynomial. Maybe you want to getSomething like this, maybe. OK. Actually, these two are still similar, but I can't draw anything. Empirically, you will see that they will be different, just because any small perturbations of this would change a lot. that you draw the same number of samples with similar ground truth-- the same ground truth and the solution. But just their randomness are different. And that's a good question. That's exactly what I'm going to talk about next. OK. One moment before that. So basically, OK, just to summarize here, if you redraw all the examples and you find that a large variation between-- so suppose, you have a-- so you so you have -- so you call this-- that means you have large variance. And if you don't see a lot of differences, then you don’t have a large variance, that's the somewhat formal definition of this. We will have a little more formal version of this, but this is the idea. So maybe, for example, if you get a new data set, you get something like maybe. maybe here, here, Here, Here and here, and maybe you're going to learn something very different, maybe something like this. variance is caused by lack of data. And it can be mitigated if you have more data. Sometimes there are two reasons. One thing is like you have lack ofData, and the other is you have too expressive models. So if you've got a very expressive model, but your data is really, really big, then probably, it's OK. It's probably, you cannot say this is only caused by loss of data because you have a different model of a variance. On the other hand, if you have not too many data, but you have very, very simple model, then it's probably still OK. The mitigation is that either you get more data or you have simpler model. So technically, you don't have more data. If you have moreData, you should already use them already. But for the understanding, let's see, for example, what happens if you've got a million data, roughly. There's a little bit fluctuation, of course. So now you want minima [INAUDIBLE] So the question is that another possibility is that a failure mode is that you just couldn't find this degree 5 polynomial because some optimization issue. So this is something that we don't discuss, at least, in the scope of this lecture. So in this lecture, we are assuming that you can just-- optimization always works. You always find the best model. So if it exists, then you can find it. So that's why I'm like in this case, even have a lot of data, and even you have a very complex model as a degree 5 polynomial or even degree there's always exist one model that works, which is like something like this, like the ground truth. And we'll find it. For this case, definitely, we will find it because it's a linear regression problem. You will find the best model. OK. Cool. So any other questions? What happens to the [INAUDIBLE] You got more data. You're getting moreData. Yeah. So here, when I say more data, I really mean that you have-- you just collect-- you have more data from the same distribution. same distribution. Yeah. So like if you collect more data from-- yeah. So in some sense, you kind of like the mindset-- I'm not saying this is universally applicable to every situation, but the mindset we are in is that, for example, you have-- how do I say that? You have a lot of like medical images. So probably one thing is that I can just sample more data. But these four images are samples from this big population. And now, I'm asking I found out my variance is very big. So howDo I mitigate that? from the same-- I have like 1 million and label examples. I had four labeled ones, and now I say, I'm going to collect more labels. So I sample like another like 100 examples from the same distribution, and then I label them. And then I run the algorithm, and the variance will be smaller. Actually, [INAUDIBLE] to ground truths of the data. When you don't know ground truth, so all of these are so far are for analysis purpose. the ground truth, I think you cannot exactly compute the bias. Because the definition of the bias, actually, requires you to sample a lot of data. So there is no way you can evaluate the bias exactly. So typically, what you do is you say, you fit the data on the training set. And you see you're underfitting. And that's when you say-- underfitting means you have a large training error. And when you start to believe that you've got a large bias. I didn't even tell you what this is. I'll go back to come back to this. Are we [INAUDIBLE] for highly imbalanced data set? So maybe let's discuss this offline. I'm not sure whether this-- I think, it probably requires more-- the imbalancedData set is pretty often. Like we have research on that. But maybe it's not exactly related to the context here. Maybe we can discuss offline. Any other questions? OK. I think I do have something to say about the variance. is not expressive enough. Doesn't depend too much on the data. For linear models, you can just say, doesn't depend on the [INAUDIBLE] of data for non-linear models. There is some technicality, which you don't have to make-- like the only reason why I had much is just because there's some technicalities that prevented me to say this is exactly irrelevant to number of data. But you should basically just believe that it's intuitive. It's really about how expressive your model is. variance on this thing. So we said the variance is caused because you have too complex of model. That means if your model is more and more complex, then you should have bigger and bigger variance. So the test error is the sum of these two. And so the question you want to answer is that if you change the model complexity, what is the best test error, right? So it means that it's somewhere in the middle. But suppose you believe in this, then what the conclusion, the implication of this is that you should somehow kind of find a sweet spot. the model complexity. So for example, maybe at the beginning you find that your training error is very low, which means our bias is very high. And the bias is high, it means you are underfitting. So basically, you increase the model complexity to some extent until your bias and variance has a right trade-off. And at some point, you. find that you are in other regime, where the variance is too. high, then you should stop. One of the reasons for this is that when you see the training. error is big, you kind of see your biases. You kind of believe that your bias is too high, so that's why you should increase themodel complexity. of bias and variance first change, did you use different type of model [INAUDIBLE] So I think this figure, so this is the-- OK. You ask a good question. So probably, the best thing is to use quadratic. Quadratic is, in principle, expressive enough to express our data. So that's why quadRatic has small bias. And also, quadratics is probably, among all models, the most expressive of all models. But where the trade-off comes from, where the sweet spot is would depend on the ground truth. the models with small bias, among all the models that can express your function, quadratic is the least complex. They don't necessarily have to match each other because it also depends on the data, how many data. For example, suppose you are-- maybe let's give you an example. But it's somewhat look like a linear function. So suppose your ground truth is almost linear, but with a little of a kind of like small fluctuation. But you don't have a lot of data. You just have like five. data points. So the bias, the trade-off, depends on, for example, how many data you have as well. And all of this, all of what we discussed today is more about some internal understanding. So this bias and various is not a problem. We just [INAUDIBLE] for the loss function. So how can [INAudIBLE]. That's a good question. And the answer to that is that no, you cannot compute the bias and variance. But if you use a linear, your bias is not zero, but still small enough, right? something you can-- at least, in some case you, can estimate them a little bit. But typically, you probably shouldn't really actively estimate the bias and variance in your-- these are mostly just for-- its internal understanding for our research, for ourselves, but not necessarily something you, empirically, evaluate. So the variance and bias are just for understanding. Empirically, what you really do is that you try a lot of different models. And you select based on a validation set. Don't think it's required for the exam or anything, but it's a relatively simple word if you're interested. And also, just this kind of bias and variance trade-off, it's not that always easy to achieve, mathematically. So that's why in the lecture notes, we only talk about square loss. But the intuition is still kind of fun. So if you don't care about what exactly definition of bias is. But like how do you do the mathematical decomposition is actually pretty challenging. So I will spend the next 20 minutes to talk about a new-- something that is actually challenging this picture. So this kind of like a U curve test error and bias-variance trade-off. This has been like discovered or like analyzed for I don't know how many years, maybe like 40 years or something like that. But this is like a very classic. However, people realize that there are some issues with this understanding, especially we realize that in deep learning, like you-- actually, people start to realize this in deep, learning. what I'm going to talk about. And this is an area of research productive in the last, probably, three or four years. So let me try to find out where should I erase. So this phenomenon that people observe, empirically, and then analyzed theoretically, this phenomenon is called double descent. If you are a historian, then I think actually this phenomenon actually dates back to something like 1990. But I think it just becomes popularized and more relevant these days. And what does this mean is that, so basically, I've told you that this is test error. This is model complexity. In some cases, like in many cases, you will see a second descent. Is this because we are [INAUDIBLE] the one with much more data? Not directly, I say. Because this is-- at least, on the surface, if you look at this, so this regime is the regime, where the parameters is bigger than the number of data points. So if you want to want to know what's going on, you have to look at the data yourself, and you can see what's happening. to find the right course, I'm not saying-- like you probably will say, at least, to be in this regime, probably, you need to compute. You need a lot of compute because probably, like 10 years ago or 20 years ago, you cannot even afford to run experiments. But of course, nowadays, we also have more data points. And this is the so-called double descent phenomenon. It's about less mysterious these days like after people have studied this in the last five years very carefully. the explanations, intuitions. But before that, let me also give another related phenomenon, which is also called double descent, but it's called data wise double descent. So on the x-axis, I'm going to change the number of data points. So here, the y-axis is still a test error. And the x -axis is the amount of data. OK. What this curve should look like? As you have more data points, how does the test error change? Right. if you believe in that, then you should say that OK, the test should look at this. And it should continue to decrease as you have more and more data. But it turns out that, actually, in many cases, what happens is that the test error will look like this, or increase, at some point, and it will decrease again. And this peak here is kind of similar to the peak here. So this peak is often happening when-- it's roughly equal to d. I guess, by the way, here, like there-- this is active research area, so I'm not being very precise in every places. d or not is in 2d or the relationship is less clear. But let's suppose, when you think about relatively simple models, then when n, the number of data points, is closer to the number. of parameters, then in this case, you're going to see a peak. And then after that, you have more data. It actually helps. I saw some questions. So the original double descent, does that like continue to decrease or does it eventually increase again? So in the first figure. This is a good question. this function. This was like this has been for a while. This phenomenon. This one, I think, is also-- actually, the paper that first systematically discussed this is like 2020. About that peak, when was that discovered? The peak? Yeah. This is discovered in the same paper, the peak. It's not monotone. The fact that there exists a peak was also discovered right, essentially. Yeah. But at least I would say, at least, it's only until 2020 that most people start to realize this. And because of that paper. are changing the number of data points. OK. This sounds like a mysterious enough. So like a very, very interesting. And what's the explanation? In the last few years, people try to explain what happens, and try to reconcile with our old understanding about this. And also, this is an important question because this regime, this blue regime, is actually-- actually, it's not clear whether when you run like a classical linear, models I don't think necessarily, you are in this regime. people really care about it. And what I mean by that is that even within linear models, you can try to change the model complexity. So what that means is that you try to decide how many features you use. So you can start with only using one feature or two features like for example, in the house price, where you can use the square foot as the single feature, or you can collect a bunch of other features. So keep adding more and more features. That means you have more. and more parameters. So even within linear models, you can still change the complexity, just to clarify that. Most of this theoretical study, I think, are for linear models. And they are pretty precise these days. And I'm going to try to kind of roughly summarize the intuition from the study of this double descent. So I think the first thing to realize is that this peak, so you can argue what is the most exciting or surprising thing about this graph. But let's first talk about a peak, this peak in the middle. stochastic gradient descent for linear models. So the existing algorithms underperform dramatically when it's close to d. So both these two peaks are basically like this. So here, you are changing n, the number of data points. And you found that when n is close to. d, you had to pick. And here,you are changing the. number of parameters. And we realized when d is kind of above n, above the number. ofData points, you have a peak. Algorithm that you use to produce this graph, it really underperforms very dramatically. If you change your algorithm, you probably wouldn't see this peak. What goes wrong is that a norm of the theta, the linear models you learned, is very big. It's very big, so that's why the peak shows up. OK. So for linear models, maybe this is-- I say, this is forlinear models. So what goes wrong with the so-called existing algorithm? So this basically gradient descent algorithms. have more parameters, maybe sometimes, you have lower smaller norm. So the norm when n is close to d, for some reason, it is very, very big. So in some sense, this is saying that your model is actually very complex. So very complex on [INAUDIBLE] to the norm. This model, it seem it doesn't have a lot of parameters compared to, for example, this model. So if you compare this model to the classifier theta, you can argue that if the norm is too big, then your models is too complex. and this model. So this model seems to have less parameter than this. That's by definition. The norm is actually very big. So in some sense, if you use the norm as the complexity, actually, these peaks have large complexity. So what is the right measure for complexity? So this is a very difficult question. Like for different situations, you have different answers. But there is no universal answer. But norm could be one complex measure. And it's also a way to describe how many choices to fit your data. options, in some sense, to fit your data. So that's restrict the complexity. And which norm, that's actually, for different situations, you can argue which norm is the right complexity. Actually, there's probably no universal answer. But I guess what I'm trying to say here is that the number of parameters is also not necessarily theright complexity measure. Even you have more parameters, suppose all the parameters are very, very close to 0. That's probably also very simple model because those parameters are not working. happens that for mathematical reasons, I think l2 norm behaves really nice. It seems to relate to a lot of fundamental properties. And actually, if you-- and you can test this hypothesis in some sense. So you can say that OK, I'm saying here the existing algorithm underperforms. But if you have a new algorithm, that's regularized, suppose you recognize the norm. OK. So here, it's just saying that at least for this case, it sounds like norm seems to be a slightly better complex measurement. mean is that you try to find a model such that the norm is small. So you don't only train on the training loss, but also youtry to make the norm smaller. Then you're going to see something like this. So regularization would mitigate this to some extent. I would discuss more about regularization in the next lecture. But here, it really just means that you don’t only care about training loss. You try to finding a model with small norms. And you have some kind of balance between them. Your algorithm didn't use the right complexity measure. And you can fix that peak by adding norm. But there's one more question, which is there is no peak, but why there's no ascent? So suppose you just see this. Actually here, you will also see this, something like this. So this figure is actually pretty reasonable. Because if your data point is increasing, you probably should just have one decrease, like you just keep decreasing. So why, when you use, for example, a million parameters, and you just have five examples, why you can still generalize? Why you don't have ascent, eventually? In many cases, the best one is just you have more and more parameters. And actually, for example, another question is when number of parameter is bigger than the number of data points. Sometimes, you are thinking this is the-- you have too many degree of freedom to fit all the specifics of the data set. But actually, empirically, you do work pretty well. points. So the thing is that even though it sounds like you are supposed to overfit, but actually, the norm is small. The reason is that your optimization algorithm has some implicit encouragement to make the norm small, which is not used. And that's something I'm going to discuss, I think, more next time. So for this lecture, I Think I'm just-- so we're just going to focus on the loss function, and then we'll move on to other topics. going to discuss this more next time. So the high level thing is just that something else is driving the norm to be small. Thanks. Going to talk more about this in the next few days. Back to Mail Online home. back to the page you came from. Back To the pageYou came from: Back to thepage you camefrom. Back into the page You came from was from: The Daily Mail. Back onto the pageyou came from, the DailyMail.com page you were from.