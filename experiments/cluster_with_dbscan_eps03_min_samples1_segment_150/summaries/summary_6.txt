foreign and I should turn off the zoom background blur Ry options like this oh it does show up uhYeah there's like speaker notes on your screen but there's be careful because I accidentally just put something else in the first longer okay I don't have too many but yeah there's an interview oh this is where it has speaker notes and stuff I think it's fine. I just think I was just too ready to go I usually uh yeah as our slides were they and put which is the product describing to replace the names of it. yeah party on pictures are good first time we got foreign foreign foreign things okay let me make sure it's it was obviously I think it is for the most part of it that's something the green and the yellow look identical we can go to something different oh my gosh it's mine started oh do you want the mic um I think you should be fine though what hello hello all right so I think we can get about started here I want to start today uh before we get into like talking about uh more advanced CNN architectures I wantto start by sort of recapping what we talked about last Tuesday. review um convolutions and and the architecture of a CNN to make this more clear and put it put it into perspective how it relates to um just standard um dense neural networks I think it's fine um so we talked I think I think most people felt okay about um the actual mechanics of doing a convolution um and I just wanted to sort of clarify that when we do a convolutions operation um we treat it like a layer like with standard dense neural network uh we treated matrix multiplication as sort of like a layers where all the Learned parameters were all the values in our Matrix. and we also have a bias term that gets added to the output of moving each window on each location of our input we refer to it as a volume simply because it sort of looks like a cube. If you have a whole bunch of filters then you're going to end up stacking up all of the different outputs from taking each one of your different filters and running it over your input and you get something that has a number of channels in the output. Think of it like that I I don't really think of it as like a layer. again you can kind of think of this as like a layer of a neural network so after we're done doing this we're still going to add our activation function like array Loop or or something else. We can still take partial derivatives of our loss with respect to our parameters and do gradient descent. When you have images and a CNN you can simply just do convolutions instead of your normal matrix multiplication. which is demonstrated up here. You do convolution followed by your activation functions you have pooling layers. CNN is where you have like an image like a person in it and you need to Output another image except each pixel has basically been like labeled with a person. In that case your output is the same size as your input which gets a little bit weird and we'll talk about that more later how you because at face value that would be like super inefficient all of your convolutions are on these just huge inputs. If you want to decrease the size of this volume because it can get quite unwieldy you can do pooling simply just looking at each. regular matrix multiplication um at the end yeah yeah no really um it's just there because like if we if we have like a huge input that's like 256 by 256 I think it's like huge. The output assuming you don't do a strided convolution which is just taking your filter overlapping in this case a three by three area and instead of moving over one to overlap in the next era you move it over two. That just immediately cuts the whole thing in half on your height and width axis so you have a quarter as many values now. different activation Maps one from each filter that we obtained so your your Channel's Dimension uh would be 10 Deep um yeah it just corresponds to like if if if this filter is corresponds to horizontal edges and this filter corresponds to edges like this. You can just sort of look along the channel and see like okay like was there an edge that went this way was there a feature that went in this way or this way. They just correspond to what the filter what that filter in the previous layer picked up on does that gonna make sense? Resnet is the only architecture that you really need to take away from here which is going to be resnet we'll get to that. If it goes over your head like don't worry it's it's fine this is just more for people who who want to know um resnets are the only one we should really like feel free to ask like tons of questions because we want to spend the most time on that and all that.any more questions then I will hand it over to Rohan who's going to talk about um more advanced CNN architectures. a little timeline here starting from uh alexnet and moving forward. laneet is something that was created quite a while ago. Alexnet in 2012 was a a big groundbreaking feat in that it proposed stacking layers of convolutions Max poolings following it up by fully connected layers and coming up with not a very deep network. This kind of uh turned a lot of heads when it achieved a around 17 error rate on imagenet. Inception Nets were proposed in 1990 by Yen lacun and he he kind of pioneered that pattern for today. The next slide should have an updated drawing that's hopefully a lot easier to understand than the previous one. This has a convolutional layer followed by a Max pooling layer. Three dense layers are following this last stack of convolutions and Max pooled. The motivation behind this is you want a deeper neural network. It computes a little bit better because computers evolve CPU power is able to handle these high order Matrix computations. The next slide is a visualization of how you have multiple layers that are stacked they're pulled together. up quite a bit this also means that the number of computations that you need to do stacks up. Only having five convolutional layers and three dense layers really limits the amount of information we could synthesize by our feature Maps. We want to be able to have our model learn higher order feature maps and by that I mean low level features um like edges things like that but as we go higher we're starting to see the correlation between features and computations get higher and higher. soft Max will scale more logarithmically um and it'll give you a final like probability map. If you don't specify a certain type of padding valid padding is going to be applied to make sure that as you're sliding your kernel across an image uh you're left with the same dimension is there anything you want to add Jake or no that's I I should have mentioned adding two yeah I mean you did a good job we basically just had a bunch of zeros on the outside. one by one convolutions can actually be used as a form of padding and dimensionality addition and reduction. We do want a higher accuracy we want a deeper Network we want something that is able to effectively both space and time effectively compute these Matrix productsYeah so yeah and you're also adding pixels to uh the the top and bottom as you're sliding if that makes sense there is a yeah there's a picture on the next slide I believe uh uh okay yeah yeah so a one by one conclusion. discrimination in lower stages um increase the gradient signal that gets propagated back and provide additional regularization um the motivation behind this is that again we want to learn low level features. There are a couple of common issues that can uh kind of result from just blindly stacking layers um so you might ask uh we saw Alex Ned and we saw bdg which was like basically the same thing but we added a couple more layers. Why can't we just infinitely stack these layers um and there are a lot of answers to that. problems that come with that that Inception and resonance um try to fix and that is adding residuals yeah uh what do you mean by branched uh yeah the classifier is at the end uh by this uh again there's there's a drawing for it but uh essentially you have a yeah yeah you've a multi-headed yeah so you're you're taking whatever your input is in a certain step and applying it to the output of another step. This maintains kind of a about it's a backwards way of maintaining a residual value. about before adding more layers shouldn't hurt because the layer can learn the identity transform which is essentially how you're transforming a certain input in every step. Vanishing gradients is a common problem as you add a bunch of layers stacked together and that the learning signal or the gradient computation becomes extremely weak the model struggles to learn. The other side of this problem is explode ingredients which isn't as applicable to this but another problem a third problem is shatter ingredients and the the the point of shatter. ingredients is that as I go deeper into an extremely deep convolutional neural network my gradients actually start resembling white noise so there's no pattern my model isn't actually learning anything because of the depth of the network. This is where residuals come into play that's why it's called resnet the solution is make it easier to learn at least the identity so keep information from previous stages into future computation that's like the the the key motivation behind residuals so yeah this is what I was talking about earlier. relief activation function um as your X goes through a weight layer the function is applied you go through another weight layer this f of x kind of encompasses that process this is the function that you've applied to X now your output is whatever f of X is the motivation behind residuals is that after your f ofx has been applied you add X back into your network um so by multiplying or by adding the resultant X by whatever your original identity was and using that as the input for the next layer you've maintained a semblance of what you had prior to whatever function you'd applied. residual that's being computed. A 34 layer residual will have jumps between every two. This is a process known as bottlenecking versus if it was after every layer. Adding residuals will increase the time to convergence because you're increasing the number of backwards considering computations that you have so if you're if your bottleneck isn't as big your time to converge will be smaller so as your uh as your residual skips more and more levels your time of convergence will also be smaller but your results may also not be as good. because you're not negating the problem this is highly dependent on what system you're using to compute these as well as where the model is eventually running so yeah um yeah dude that was an example of a very long uh residual net adding skip connections makes the identity easier to learn because you're quite literally adding a previous identity to the resultant of a transformation. As you can see this is a gradient map uh the Lost surface of resnet with and without skip connections with and with skip connections. event like a low dimensional projection s yeah yeah this is like probably like really important thing for today but like this idea of like why it'd be important to sort of be able to learn the identity like it's sort of a weird thing um are there any questions or comments or concerns about that yes yeah for sure right so like if you have a dent snail Network like like let's just ignore convolutions right now if you like a dense neural network trivially you have the identity Matrix which is just ones along the diagonal and it spits out the exact same thing that it took in. so like if I have a neural network and I just trivially add so I have like a whole bunch of like layers right like this this new connection to the next layer right like just a dense neural network. If I just make each layer like the identity Matrix if I make all the weights correspond to the identityMatrix like there's no reason I shouldn't be able to make like a million length neural network which is kind of absurd. But like in practice we've observed that like if you add if you put a million layers on a dense Neural network it's going to just learn like garbage like it's not going to work at all. took in it's just kind of curious that it was observed that deeper networks don't work and this makes it super easy so if your weights are literally all zeros and your biases are literallyall zeros you're going to spit out exactly what you took in so it makes it really really easy for the network to just say like hey okay we've got enough information at this point in the network like we don't need to learn more complicated features I could just send the weights to zero and just ship exactly what I have about halfway through the network. like okay like we've got enough information to like make a good classification but we're only about like halfway through the network with with this it's just super easy for it to learn. "It's just something you can tune and there's like foreign so like if you're doing the chain rule it just results in a lot of multiplications right like the more like um like we talked in the third lecture about applying like the chainRule um to deep neural networks" "If you know exactly like how many years to have in your block here yeah so I mean resnet just used two uh two is a fine Choice" step in your network if you just multiply the partial derivatives of all of those steps you can find the the derivative of your loss with respect to a given parameter just with the chain rule. If all of these different things that you're multiplying are even a little bit smaller than one immediately like at a certain point at acertain number of multiplications your partial derivative uh your chain rule that you've gotten as a result of many many multiplications just gets sent straight to zero. That's just not helpful. is like sort of uh sort of regular a little bit more consistent um so that our weights aren't either just exploding because the gradient steps are huge or they're just literally never going to change. If you're stacking like a bunch of sub 1 multiplications you're going to be left like a super small number as you go back. It doesn't necessarily mean you're close to a minimum either like because your lost surface can be like a little Plateau it just means that for some of our our parameters really early in our Network. well um and it's probably like a bigger contributor to like stopping the vanishing gradient problem then uh and residuals but like residuals help I think residuals are more for like shattered gradients so that's when like you introduce a bunch of like meaningless noise into your gradients. It comes from again like the the depth of the matrix multiplication that you're doing uh so without like by losing form of the identity. That is the reason that stacking a lot of layers doesn't result in like better performance or strictly better performance. shouldn't produce accuracy when in reality like if this is scaled it can yeah awesome group wise uh your Dimensions so yeah this is a really good point um so if your layer is a convolution um the dimension can change which is why often this is result like kind of viewed as f of x plus W of X where W is a transformation that you do on X two to make it the same Dimension exactly. To make sure your Matrix addition stays the same like like basically you have like the X number of layers and things like during the state. you're not necessarily zeroing out uh learned weights uh this like Jake was saying like other ways of uh normalizing your data as you go through like Bachelor affect the vanishing gradient problem more than residuals do the main point of this is that you want to maintain a semblance of identity as you going through your network if that makes sense. This is often viewed as plus W of x there any other questions about resnet all right dope uh so the next thing to talk about is global average pooling which is designed to replace fully connected layers in cnns. the vector that you want to classify your classification Vector you're generating a feature map for each of those averaging those and then that is fed into the softmax layer um this uh the typical dense layer that you previously had that's facilitating these connections does not enforce correspondences between feature maps and categories. As you go through the three dense layers at the end of the network there's no parameter to optimizing global average as well which saves overfitting time. In dense layers this often results in if I have a very deep neural network that is trained on a certain subset of data. at the end it's very easy to overfit to the data that I have provided uh for training um so this kind of prevents that. This comes more into play also in Mobile nuts and uh the efficient that's that will be talked about as well. There any questions on the previous kind of topics all right all right that was kind of the meat of this lecture uh but mobile nuts are very cool in that you're you're using depth wise convolutions and point-wise convolutions to reduce the number of computations. efficiently uh and there's a little example that will hopefully show how the math behind this works so essentially yeah if you're involved by 12 by 3 image and a like five five three feature you would have to do and this result in an eight by eight by one at the end of your process. This would be times you're 64. um and then this would then be multiplied 256 times or however many channels that you're doing um on the other hand uh if we did the other metric which is instead of a five by five by three we have a uh five byFive by one uh which then we still have uh 25. generally work I'm running a thin one by one by three layer here so this is 64 times three it's 192. and this is what is being multiplied by the 256 and added to our previous product which is 74 times 64. so these two are being added together to end up with your your final computation for how many I guess multiplication parameters you have other questions about this yeah. So mobilenet has a lot fewer parameters which results in a lot faster convergence time um and it matches Inception of D3 accuracy just by using depth and point wise convolutions and combining. those so you can think about it instead of doing one step that results in one map and multiplying that by the number of filters you have you're not doing that. You're applying this one step to every channel and then your next step is applying a different sized convolution to do your filter multiplication so instead of one you have two steps that are being combined which reduces complexity quite a bit alrighty I guess we can quickly go over like squeezing anxiety networks basically uh you squeeze you apply this through a couple of dense layers and then you rescale so we talked about global average pooling. connected layer passing it through relu and with a fully connected layer you can also expand this back to whatever Dimension you originally had um rescaling according to the layer output is also not as computationally intensive. I think the slides are pretty good and compressed in a very visual way uh the remainder of the piano more impressive art it's actually the other way. I I hope the main takeaways are that you'll like understand those rules and that you you see that we've like adding all of these different sort of like tools to your tool belt now. CNN.swap out if you're very well CNN building plus. I think understanding like the the base of how optimizations are held and the problems that certain optimizations face and others don't really sets the stage for like future networks like the efficient net in 2020. We can straight up just go by what we've already learned in that we know we can pass through a one by one convolution a depthwise convolution which is where we apply this filter to each Channel individually and recombine them using our squeeze and exide networks. so these are some things that these models wanted to optimize over time accuracy performance and model size um model size is something that has a trade-off if you get too big you lose out on other metrics like accuracy. performance is something directly corresponds to depthwise convolutions and mobile nuts for Edge Computing and things like that. You want to drastically reduce the number of computations that you want to do yep that is basically everything for today thank you guys for coming oh and there will also be a quiz.